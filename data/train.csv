"Graph neural networks (GNNs) have been widely used to learn vector representation of graph-structured data and achieved better task performance than conventional methods. The foundation of GNNs is the message passing procedure, which propagates the information in a node to its neighbors. Since this procedure proceeds one step per layer, the range of the information propagation among nodes is small in the lower layers, and it expands toward the higher layers. Therefore, a GNN model has to be deep enough to capture global structural information in a graph. On the other hand, it is known that deep GNN models suffer from performance degradation because they lose nodes' local information, which would be essential for good model performance, through many message passing steps. In this study, we propose multi-level attention pooling (MLAP) for graph-level classification tasks, which can adapt to both local and global structural information in a graph. It has an attention pooling layer for each message passing step and computes the final graph representation by unifying the layer-wise graph representations. The MLAP architecture allows models to utilize the structural information of graphs with multiple levels of localities because it preserves layer-wise information before losing them due to oversmoothing. Results of our experiments show that the MLAP architecture improves the graph classification performance compared to the baseline architectures. In addition, analyses on the layer-wise graph representations suggest that aggregating information from multiple levels of localities indeed has the potential to improve the discriminability of learned graph representations.",0
"The use of graph neural networks (GNNs) has become widespread as they have proven to be more effective than traditional methods in learning vector representations of data with graph structures. GNNs rely on message passing to transfer information between nodes and this process progresses layer by layer. As a result, information is transmitted over a shorter distance in the lower layers and expands in the higher ones, requiring a deep GNN structure to capture the global structural information of a graph. However, deep GNN models may suffer from performance degradation as they lose local information through numerous message passing steps. This study introduces multi-level attention pooling (MLAP) for graph-level classification tasks, which adapts to both local and global structural information by preserving layer-wise information to avoid oversmoothing. MLAP comprises an attention pooling layer for each message passing step and consolidates the layer-wise graph representations for the final graph representation. Results of the experiments show that MLAP enhances graph classification performance and the layer-wise graph representations analysis indicates that utilizing information from multiple levels of localities can potentially improve the learned graph representations' discriminability.",1
"Deep networks and decision forests (such as random forests and gradient boosted trees) are the leading machine learning methods for structured and tabular data, respectively. Many papers have empirically compared large numbers of classifiers on one or two different domains (e.g., on 100 different tabular data settings). However, a careful conceptual and empirical comparison of these two strategies using the most contemporary best practices has yet to be performed. Conceptually, we illustrate that both can be profitably viewed as ""partition and vote"" schemes. Specifically, the representation space that they both learn is a partitioning of feature space into a union of convex polytopes. For inference, each decides on the basis of votes from the activated nodes. This formulation allows for a unified basic understanding of the relationship between these methods. Empirically, we compare these two strategies on hundreds of tabular data settings, as well as several vision and auditory settings. Our focus is on datasets with at most 10,000 samples, which represent a large fraction of scientific and biomedical datasets. In general, we found forests to excel at tabular and structured data (vision and audition) with small sample sizes, whereas deep nets performed better on structured data with larger sample sizes. This suggests that further gains in both scenarios may be realized via further combining aspects of forests and networks. We will continue revising this technical report in the coming months with updated results.",0
"Structured and tabular data are best analyzed using deep networks and decision forests like random forests and gradient boosted trees, respectively. However, a comprehensive comparative analysis of these two methods is lacking. Previous studies have compared numerous classifiers in one or two domains, but not using modern best practices. Both deep networks and decision forests function as ""partition and vote"" schemes, partitioning the feature space into convex polytopes and making decisions based on votes from activated nodes. We conducted an empirical comparison on hundreds of tabular data settings, vision and auditory settings, focusing on datasets with a maximum of 10,000 samples, which are common in scientific and biomedical datasets. Our findings show that decision forests are more effective for tabular and structured data with small sample sizes, while deep networks perform better on structured data with larger sample sizes. Combining aspects of both methods could lead to further improvements in both scenarios. We will continue updating our technical report with new results.",1
"Graph convolutional networks (GCNs) are powerful tools for graph-structured data. However, they have been recently shown to be vulnerable to topological attacks. To enhance adversarial robustness, we go beyond spectral graph theory to robust graph theory. By challenging the classical graph Laplacian, we propose a new convolution operator that is provably robust in the spectral domain and is incorporated in the GCN architecture to improve expressivity and interpretability. By extending the original graph to a sequence of graphs, we also propose a robust training paradigm that encourages transferability across graphs that span a range of spatial and spectral characteristics. The proposed approaches are demonstrated in extensive experiments to simultaneously improve performance in both benign and adversarial situations.",0
"GCNs are effective tools for processing data with graph structures, but they have recently been found to be susceptible to topological attacks. To increase their resilience to such attacks, we move beyond spectral graph theory and adopt robust graph theory. Our new convolution operator, which challenges the traditional graph Laplacian, is designed to be dynamically robust in the spectral domain and is integrated into the GCN architecture to enhance its interpretability and expressive power. We also introduce a robust training technique that expands the original graph into a sequence of graphs, promoting transferability across a range of spatial and spectral characteristics. Our proposed methods are evaluated through extensive experiments and are found to improve performance in both benign and adversarial scenarios.",1
"Graph neural networks (GNNs) have been widely used to learn vector representation of graph-structured data and achieved better task performance than conventional methods. The foundation of GNNs is the message passing procedure, which propagates the information in a node to its neighbors. Since this procedure proceeds one step per layer, the range of the information propagation among nodes is small in the lower layers, and it expands toward the higher layers. Therefore, a GNN model has to be deep enough to capture global structural information in a graph. On the other hand, it is known that deep GNN models suffer from performance degradation because they lose nodes' local information, which would be essential for good model performance, through many message passing steps. In this study, we propose multi-level attention pooling (MLAP) for graph-level classification tasks, which can adapt to both local and global structural information in a graph. It has an attention pooling layer for each message passing step and computes the final graph representation by unifying the layer-wise graph representations. The MLAP architecture allows models to utilize the structural information of graphs with multiple levels of localities because it preserves layer-wise information before losing them due to oversmoothing. Results of our experiments show that the MLAP architecture improves the graph classification performance compared to the baseline architectures. In addition, analyses on the layer-wise graph representations suggest that aggregating information from multiple levels of localities indeed has the potential to improve the discriminability of learned graph representations.",0
"The use of graph neural networks (GNNs) has become widespread due to their ability to create vector representations of graph-structured data, resulting in better performance than conventional methods. GNNs rely on a message passing procedure to disseminate information from a node to its neighbors. However, as this process occurs one layer at a time, the range of information propagation is limited in the lower layers and expands in the higher ones. Therefore, to capture the global structural information in a graph, a GNN model needs to be sufficiently deep. However, deep GNN models can suffer from performance degradation, since they lose the nodes' local information through many message passing steps. To address this, we propose multi-level attention pooling (MLAP) for graph-level classification tasks. This method utilizes an attention pooling layer for each message passing step to adapt to both local and global structural information in a graph. Additionally, the MLAP architecture allows for the preservation of layer-wise information, thus enabling models to leverage the structural information of graphs with multiple levels of localities. Our experiments demonstrate that the MLAP architecture improves graph classification performance compared to baseline architectures. Furthermore, layer-wise graph representations analysis suggests that aggregating information from multiple levels of localities has the potential to improve the discriminability of learned graph representations.",1
"Deep networks and decision forests (such as random forests and gradient boosted trees) are the leading machine learning methods for structured and tabular data, respectively. Many papers have empirically compared large numbers of classifiers on one or two different domains (e.g., on 100 different tabular data settings). However, a careful conceptual and empirical comparison of these two strategies using the most contemporary best practices has yet to be performed. Conceptually, we illustrate that both can be profitably viewed as ""partition and vote"" schemes. Specifically, the representation space that they both learn is a partitioning of feature space into a union of convex polytopes. For inference, each decides on the basis of votes from the activated nodes. This formulation allows for a unified basic understanding of the relationship between these methods. Empirically, we compare these two strategies on hundreds of tabular data settings, as well as several vision and auditory settings. Our focus is on datasets with at most 10,000 samples, which represent a large fraction of scientific and biomedical datasets. In general, we found forests to excel at tabular and structured data (vision and audition) with small sample sizes, whereas deep nets performed better on structured data with larger sample sizes. This suggests that further gains in both scenarios may be realized via further combining aspects of forests and networks. We will continue revising this technical report in the coming months with updated results.",0
"The top machine learning methods for structured and tabular data are deep networks and decision forests like random forests and gradient boosted trees, respectively. While many papers have compared numerous classifiers on a few domains, no comprehensive conceptual and empirical comparison has been conducted using contemporary best practices. Both methods can be viewed as ""partition and vote"" schemes, as they learn a representation space that partitions the feature space into convex polytopes and decide based on votes from activated nodes. We compare the two strategies on hundreds of tabular data settings, as well as vision and auditory settings, focusing on datasets with at most 10,000 samples. We found that forests perform better on tabular and structured data with small sample sizes, including vision and audition, while deep nets outperform forests on structured data with larger sample sizes. Combining aspects of both methods could lead to further improvements in both scenarios, and we will continue updating this technical report with new findings.",1
"Graph convolutional networks (GCNs) are powerful tools for graph-structured data. However, they have been recently shown to be vulnerable to topological attacks. To enhance adversarial robustness, we go beyond spectral graph theory to robust graph theory. By challenging the classical graph Laplacian, we propose a new convolution operator that is provably robust in the spectral domain and is incorporated in the GCN architecture to improve expressivity and interpretability. By extending the original graph to a sequence of graphs, we also propose a robust training paradigm that encourages transferability across graphs that span a range of spatial and spectral characteristics. The proposed approaches are demonstrated in extensive experiments to simultaneously improve performance in both benign and adversarial situations.",0
"GCNs, which are useful for data structured in graphs, have been found to be susceptible to attacks on topology. In order to increase resilience to such attacks, we move beyond spectral graph theory and instead employ robust graph theory. By introducing a new convolution operator that is proven to be robust in the spectral domain and is integrated into the GCN architecture, we are able to improve both the expressiveness and interpretability of the system. Additionally, we extend the original graph into a sequence of graphs and develop a training method that promotes transferability across graphs with varying spatial and spectral characteristics. Our experiments demonstrate that these approaches enhance performance in both benign and adversarial scenarios.",1
"Graph neural networks (GNNs) have been widely used to learn vector representation of graph-structured data and achieved better task performance than conventional methods. The foundation of GNNs is the message passing procedure, which propagates the information in a node to its neighbors. Since this procedure proceeds one step per layer, the range of the information propagation among nodes is small in the lower layers, and it expands toward the higher layers. Therefore, a GNN model has to be deep enough to capture global structural information in a graph. On the other hand, it is known that deep GNN models suffer from performance degradation because they lose nodes' local information, which would be essential for good model performance, through many message passing steps. In this study, we propose multi-level attention pooling (MLAP) for graph-level classification tasks, which can adapt to both local and global structural information in a graph. It has an attention pooling layer for each message passing step and computes the final graph representation by unifying the layer-wise graph representations. The MLAP architecture allows models to utilize the structural information of graphs with multiple levels of localities because it preserves layer-wise information before losing them due to oversmoothing. Results of our experiments show that the MLAP architecture improves the graph classification performance compared to the baseline architectures. In addition, analyses on the layer-wise graph representations suggest that aggregating information from multiple levels of localities indeed has the potential to improve the discriminability of learned graph representations.",0
"The use of Graph neural networks (GNNs) has become widespread for learning vector representations of data with a graph structure. These networks have shown a better performance in tasks compared to traditional methods. The foundation of GNNs lies in the message passing procedure, which involves the transmission of information from a node to its neighbors. This process occurs one step per layer, which means that information propagation is limited in lower layers and expands towards higher ones. To capture the global structural information of a graph, GNN models must be deep enough. However, deep GNN models suffer from performance degradation as they lose nodes' local information through many message passing steps. To address this issue, this study proposes a multi-level attention pooling (MLAP) approach that adapts to both local and global structural information in a graph. It has an attention pooling layer for each message passing step and unifies the layer-wise graph representations to compute the final graph representation. The MLAP architecture enables models to utilize the structural information of graphs with multiple levels of localities by preserving layer-wise information before oversmoothing. The experiments conducted in this study show that the MLAP architecture improves graph classification performance compared to baseline architectures. Furthermore, analyses on the layer-wise graph representations suggest that aggregating information from multiple levels of localities has the potential to enhance the discriminability of learned graph representations.",1
"Deep networks and decision forests (such as random forests and gradient boosted trees) are the leading machine learning methods for structured and tabular data, respectively. Many papers have empirically compared large numbers of classifiers on one or two different domains (e.g., on 100 different tabular data settings). However, a careful conceptual and empirical comparison of these two strategies using the most contemporary best practices has yet to be performed. Conceptually, we illustrate that both can be profitably viewed as ""partition and vote"" schemes. Specifically, the representation space that they both learn is a partitioning of feature space into a union of convex polytopes. For inference, each decides on the basis of votes from the activated nodes. This formulation allows for a unified basic understanding of the relationship between these methods. Empirically, we compare these two strategies on hundreds of tabular data settings, as well as several vision and auditory settings. Our focus is on datasets with at most 10,000 samples, which represent a large fraction of scientific and biomedical datasets. In general, we found forests to excel at tabular and structured data (vision and audition) with small sample sizes, whereas deep nets performed better on structured data with larger sample sizes. This suggests that further gains in both scenarios may be realized via further combining aspects of forests and networks. We will continue revising this technical report in the coming months with updated results.",0
"Structured and tabular data are best analyzed with deep networks and decision forests, respectively. Numerous studies have compared classifiers on a limited number of domains, but a comprehensive comparison using the latest techniques has not been conducted. Both strategies can be considered as ""partition and vote"" schemes, where the learned representation space is a partitioning of feature space into a union of convex polytopes. Each method decides based on votes from the activated nodes. We compared the two approaches on hundreds of tabular data settings, as well as vision and auditory settings, with a focus on datasets with up to 10,000 samples. Our findings suggest that forests are more effective for small sample sies, while deep nets perform better on larger structured data. Combining aspects of both methods may lead to further gains in both scenarios. The technical report will be updated with new results in the future.",1
"Graph convolutional networks (GCNs) are powerful tools for graph-structured data. However, they have been recently shown to be vulnerable to topological attacks. To enhance adversarial robustness, we go beyond spectral graph theory to robust graph theory. By challenging the classical graph Laplacian, we propose a new convolution operator that is provably robust in the spectral domain and is incorporated in the GCN architecture to improve expressivity and interpretability. By extending the original graph to a sequence of graphs, we also propose a robust training paradigm that encourages transferability across graphs that span a range of spatial and spectral characteristics. The proposed approaches are demonstrated in extensive experiments to simultaneously improve performance in both benign and adversarial situations.",0
"GCNs are a useful tool for analyzing data structured as a graph, but recent research has revealed that they are susceptible to topological attacks. To address this weakness and improve adversarial robustness, we have developed a new convolution operator based on robust graph theory rather than spectral graph theory. This operator enhances both the expressivity and interpretability of the GCN architecture. In addition, we have created a robust training paradigm that encourages transferability across a sequence of graphs with varying spatial and spectral characteristics. Our experiments show that these approaches improve performance in both benign and adversarial scenarios.",1
"With the increasing popularity of Graph Neural Networks (GNNs) in several sensitive applications like healthcare and medicine, concerns have been raised over the privacy aspects of trained GNNs. More notably, GNNs are vulnerable to privacy attacks, such as membership inference attacks, even if only blackbox access to the trained model is granted. To build defenses, differential privacy has emerged as a mechanism to disguise the sensitive data in training datasets. Following the strategy of Private Aggregation of Teacher Ensembles (PATE), recent methods leverage a large ensemble of teacher models. These teachers are trained on disjoint subsets of private data and are employed to transfer knowledge to a student model, which is then released with privacy guarantees. However, splitting graph data into many disjoint training sets may destroy the structural information and adversely affect accuracy. We propose a new graph-specific scheme of releasing a student GNN, which avoids splitting private training data altogether. The student GNN is trained using public data, partly labeled privately using the teacher GNN models trained exclusively for each query node. We theoretically analyze our approach in the R\`{e}nyi differential privacy framework and provide privacy guarantees. Besides, we show the solid experimental performance of our method compared to several baselines, including the PATE baseline adapted for graph-structured data. Our anonymized code is available.",0
"The rise in popularity of Graph Neural Networks (GNNs) in sensitive applications such as healthcare and medicine has raised concerns about privacy issues related to trained GNNs. GNNs are particularly susceptible to privacy attacks, such as membership inference attacks, even with only blackbox access to the model. Differential privacy has emerged as a mechanism to protect sensitive data in training datasets. Recent methods have utilized a large ensemble of teacher models trained on disjoint subsets of private data to transfer knowledge to a student model, which is then released with privacy guarantees. However, this approach may compromise the accuracy of GNNs by splitting graph data into many disjoint training sets, thereby destroying structural information. In response, we propose a novel graph-specific approach that avoids splitting private training data. Our method trains the student GNN using public data, which is partly labeled privately using the teacher GNN models trained exclusively for each query node. We provide privacy guarantees for our approach using the R\`{e}nyi differential privacy framework and demonstrate our method's superior experimental performance compared to several baselines, including the PATE baseline adapted for graph-structured data. Our anonymized code is available for review.",1
"Machine learning solutions for pattern classification problems are nowadays widely deployed in society and industry. However, the lack of transparency and accountability of most accurate models often hinders their safe use. Thus, there is a clear need for developing explainable artificial intelligence mechanisms. There exist model-agnostic methods that summarize feature contributions, but their interpretability is limited to predictions made by black-box models. An open challenge is to develop models that have intrinsic interpretability and produce their own explanations, even for classes of models that are traditionally considered black boxes like (recurrent) neural networks. In this paper, we propose a Long-Term Cognitive Network for interpretable pattern classification of structured data. Our method brings its own mechanism for providing explanations by quantifying the relevance of each feature in the decision process. For supporting the interpretability without affecting the performance, the model incorporates more flexibility through a quasi-nonlinear reasoning rule that allows controlling nonlinearity. Besides, we propose a recurrence-aware decision model that evades the issues posed by unique fixed points while introducing a deterministic learning method to compute the tunable parameters. The simulations show that our interpretable model obtains competitive results when compared to the state-of-the-art white and black-box models.",0
"Nowadays, there is a widespread use of machine learning solutions in society and industry for pattern classification problems. However, many of the most accurate models lack transparency and accountability, which can make their use unsafe. Therefore, there is a clear need for the development of explainable mechanisms for artificial intelligence. Although there are model-agnostic methods available that can summarize feature contributions, their interpretability is limited to predictions made by black-box models. The challenge now is to create models that have intrinsic interpretability and can generate their own explanations, even for traditionally considered black-box models such as (recurrent) neural networks. In this paper, we present a Long-Term Cognitive Network that provides its mechanism for providing explanations by quantifying the relevance of each feature in the decision-making process. Our model incorporates more flexibility through a quasi-nonlinear reasoning rule that allows controlling nonlinearity without affecting performance. Additionally, we propose a recurrence-aware decision model that avoids unique fixed points' issues and introduces a deterministic learning method to compute the tunable parameters. Our simulations show that our interpretable model can obtain competitive results when compared to the state-of-the-art white and black-box models.",1
"Graph neural networks (GNNs) are powerful models for many graph-structured tasks. Existing models often assume that a complete structure of a graph is available during training, however, in practice, graph-structured data is usually formed in a streaming fashion, so that learning a graph continuously is often necessary. In this paper, we aim to bridge GNN to lifelong learning by converting a graph problem to a regular learning problem, so that GNN is able to inherit the lifelong learning techniques developed for convolutional neural networks (CNNs). To this end, we propose a new graph topology based on feature cross-correlation, called the feature graph. It takes features as new nodes and turns nodes into independent graphs. This successfully converts the original problem of node classification to graph classification, in which the increasing nodes are turned into independent training samples. In the experiments, we demonstrate the efficiency and effectiveness of feature graph networks (FGN) by continuously learning a sequence of classical graph datasets. We also show that FGN achieves superior performance in 0 action recognition with distributed streaming signals for wearable devices.",0
"Graph neural networks (GNNs) are robust models for various tasks involving graph structures. However, current models assume that the complete graph structure is available for training. In reality, graph data is often created continuously, making lifelong learning essential. This paper aims to connect GNN with lifelong learning by transforming a graph problem into a regular learning problem. This enables GNN to adopt lifelong learning techniques developed for convolutional neural networks (CNNs). To achieve this, a new graph topology called the feature graph is proposed. This topology uses feature cross-correlation to transform nodes into independent graphs with features as new nodes. This conversion changes the original problem of node classification to graph classification, where increasing nodes are treated as independent training samples. The proposed feature graph network (FGN) is tested on a sequence of classical graph datasets, demonstrating efficiency and effectiveness. FGN also outperforms other models in 0 action recognition using distributed streaming signals from wearable devices.",1
"Deep learning models, such as convolutional neural networks, have long been applied to image and multi-media tasks, particularly those with structured data. More recently, there has been more attention to unstructured data that can be represented via graphs. These types of data are often found in health and medicine, social networks, and research data repositories. Graph convolutional neural networks have recently gained attention in the field of deep learning that takes advantage of graph-based data representation with automatic feature extraction via convolutions. Given the popularity of these methods in a wide range of applications, robust uncertainty quantification is vital. This remains a challenge for large models and unstructured datasets. Bayesian inference provides a principled approach to uncertainty quantification of model parameters for deep learning models. Although Bayesian inference has been used extensively elsewhere, its application to deep learning remains limited due to the computational requirements of the Markov Chain Monte Carlo (MCMC) methods. Recent advances in parallel computing and advanced proposal schemes in MCMC sampling methods has opened the path for Bayesian deep learning. In this paper, we present Bayesian graph convolutional neural networks that employ tempered MCMC sampling with Langevin-gradient proposal distribution implemented via parallel computing. Our results show that the proposed method can provide accuracy similar to advanced optimisers while providing uncertainty quantification for key benchmark problems.",0
"Convolutional neural networks, like those based on deep learning, have been used widely for image and multimedia tasks that involve structured data. However, there has been a growing interest in using these models for unstructured data that can be represented through graphs, such as medical and research data, as well as social networks. Graph convolutional neural networks, which can extract features automatically through convolutions, have emerged as a popular approach for deep learning with graph-based data. Nevertheless, uncertainty quantification remains a significant challenge, especially for large models and unstructured datasets. Bayesian inference offers a systematic method for quantifying uncertainty, but it has not been widely used in deep learning due to the computational demands of Markov Chain Monte Carlo (MCMC) methods. However, recent advances in MCMC sampling methods, such as parallel computing and advanced proposal schemes, have paved the way for Bayesian deep learning. In this study, we introduce a Bayesian graph convolutional neural network that employs tempered MCMC sampling with Langevin-gradient proposal distribution, implemented via parallel computing. Our results demonstrate that this method can provide comparable accuracy to advanced optimizers, while also providing uncertainty quantification for key benchmark issues.",1
"A Graph Convolutional Network (GCN) stacks several layers and in each layer performs a PROPagation operation (PROP) and a TRANsformation operation (TRAN) for learning node representations over graph-structured data. Though powerful, GCNs tend to suffer performance drop when the model gets deep. Previous works focus on PROPs to study and mitigate this issue, but the role of TRANs is barely investigated. In this work, we study performance degradation of GCNs by experimentally examining how stacking only TRANs or PROPs works. We find that TRANs contribute significantly, or even more than PROPs, to declining performance, and moreover that they tend to amplify node-wise feature variance in GCNs, causing variance inflammation that we identify as a key factor for causing performance drop. Motivated by such observations, we propose a variance-controlling technique termed Node Normalization (NodeNorm), which scales each node's features using its own standard deviation. Experimental results validate the effectiveness of NodeNorm on addressing performance degradation of GCNs. Specifically, it enables deep GCNs to outperform shallow ones in cases where deep models are needed, and to achieve comparable results with shallow ones on 6 benchmark datasets. NodeNorm is a generic plug-in and can well generalize to other GNN architectures. Code is publicly available at https://github.com/miafei/NodeNorm.",0
"The process of learning node representations over graph-structured data involves stacking multiple layers of a Graph Convolutional Network (GCN), each layer performing a PROPagation operation (PROP) and a TRANsformation operation (TRAN). However, as the model gets deeper, GCNs may experience a decline in performance. While previous research has focused on PROPs to address this issue, the role of TRANs has been overlooked. To investigate this further, we conducted experiments to examine the impact of stacking only TRANs or PROPs on GCN performance. Our findings revealed that TRANs can contribute significantly, and sometimes even more than PROPs, to performance degradation. Furthermore, we identified that TRANs have a tendency to amplify node-wise feature variance in GCNs, which is a key factor for causing performance drop. Based on these observations, we propose a variance-controlling technique called Node Normalization (NodeNorm), which scales each node's features using its own standard deviation. Our experimental results show that NodeNorm is effective in addressing the performance degradation of GCNs. Specifically, it enables deep GCNs to outperform shallow ones when deep models are needed, and to achieve comparable results with shallow ones on six benchmark datasets. NodeNorm is a generic plug-in that can be applied to other GNN architectures. The code for NodeNorm is publicly available at https://github.com/miafei/NodeNorm.",1
"Transformer neural networks have achieved state-of-the-art results for unstructured data such as text and images but their adoption for graph-structured data has been limited. This is partly due to the difficulty of incorporating complex structural information in the basic transformer framework. We propose a simple yet powerful extension to the transformer - residual edge channels. The resultant framework, which we call Edge-augmented Graph Transformer (EGT), can directly accept, process and output structural information as well as node information. It allows us to use global self-attention, the key element of transformers, directly for graphs and comes with the benefit of long-range interaction among nodes. Moreover, the edge channels allow the structural information to evolve from layer to layer, and prediction tasks on edges/links can be performed directly from the output embeddings of these channels. In addition, we introduce a generalized positional encoding scheme for graphs based on Singular Value Decomposition which can improve the performance of EGT. Our framework, which relies on global node feature aggregation, achieves better performance compared to Convolutional/Message-Passing Graph Neural Networks, which rely on local feature aggregation within a neighborhood. We verify the performance of EGT in a supervised learning setting on a wide range of experiments on benchmark datasets. Our findings indicate that convolutional aggregation is not an essential inductive bias for graphs and global self-attention can serve as a flexible and adaptive alternative.",0
"While transformer neural networks have produced impressive results for text and image data, their application to graph-structured data has been limited. This is due in part to the challenge of incorporating complex structural information into the basic transformer framework. To address this issue, we propose a straightforward yet effective extension to the transformer model, called residual edge channels. Our resulting framework, the Edge-augmented Graph Transformer (EGT), can process and output both structural and node information, utilizing global self-attention for graphs and enabling long-range interaction among nodes. Additionally, our edge channels allow for structural information to evolve from layer to layer and enable direct prediction of edges/links from channel output embeddings. We also introduce a generalized positional encoding scheme based on Singular Value Decomposition to enhance EGT's performance. Our approach, which relies on global node feature aggregation, outperforms Convolutional/Message-Passing Graph Neural Networks that focus on local feature aggregation within a neighborhood. We demonstrate the effectiveness of EGT on a variety of benchmark datasets in a supervised learning setting, highlighting the versatility and adaptability of global self-attention as a substitute for convolutional aggregation.",1
"Link prediction is one of the key problems for graph-structured data. With the advancement of graph neural networks, graph autoencoders (GAEs) and variational graph autoencoders (VGAEs) have been proposed to learn graph embeddings in an unsupervised way. It has been shown that these methods are effective for link prediction tasks. However, they do not work well in link predictions when a node whose degree is zero (i.g., isolated node) is involved. We have found that GAEs/VGAEs make embeddings of isolated nodes close to zero regardless of their content features. In this paper, we propose a novel Variational Graph Normalized AutoEncoder (VGNAE) that utilize L2-normalization to derive better embeddings for isolated nodes. We show that our VGNAEs outperform the existing state-of-the-art models for link prediction tasks. The code is available at https://github.com/SeongJinAhn/VGNAE.",0
"Graph-structured data presents a significant challenge in link prediction. Recent advancements in graph neural networks have led to the development of graph autoencoders (GAEs) and variational graph autoencoders (VGAEs) for unsupervised learning of graph embeddings, which have proven effective in link prediction. However, when an isolated node with zero degree is present, GAEs/VGAEs produce embeddings that tend to be close to zero, regardless of the content features. In this paper, we introduce a new model, the Variational Graph Normalized AutoEncoder (VGNAE), which utilizes L2-normalization to generate superior embeddings for isolated nodes. We demonstrate that our VGNAEs outperform existing state-of-the-art models in link prediction tasks, and the code is available at https://github.com/SeongJinAhn/VGNAE.",1
"Data augmentation has been widely used in image data and linguistic data but remains under-explored on graph-structured data. Existing methods focus on augmenting the graph data from a global perspective and largely fall into two genres: structural manipulation and adversarial training with feature noise injection. However, the structural manipulation approach suffers information loss issues while the adversarial training approach may downgrade the feature quality by injecting noise. In this work, we introduce the local augmentation, which enhances node features by its local subgraph structures. Specifically, we model the data argumentation as a feature generation process. Given the central node's feature, our local augmentation approach learns the conditional distribution of its neighbors' features and generates the neighbors' optimal feature to boost the performance of downstream tasks. Based on the local augmentation, we further design a novel framework: LA-GNN, which can apply to any GNN models in a plug-and-play manner. Extensive experiments and analyses show that local augmentation consistently yields performance improvement for various GNN architectures across a diverse set of benchmarks. Code is available at https://github.com/Soughing0823/LAGNN.",0
"While data augmentation has been commonly utilized for image and linguistic data, its application to graph-structured data has been relatively unexplored. Existing methods for augmenting graph data have focused on a global perspective, with two main approaches: structural manipulation and adversarial training with feature noise injection. However, these approaches suffer from information loss and reduced feature quality. In this study, we propose local augmentation, which enhances node features by utilizing local subgraph structures. Our approach models data augmentation as a feature generation process, where the conditional distribution of neighbors' features is learned to generate optimal features for downstream tasks. Using local augmentation, we introduce LA-GNN, a novel framework that can be applied to any GNN models. Extensive experiments show that local augmentation consistently improves performance across various GNN architectures and benchmarks. Code is available at https://github.com/Soughing0823/LAGNN.",1
"Graph Neural Network (GNN) research is rapidly growing thanks to the capacity of GNNs in learning distributed representations from graph-structured data. However, centralizing a massive amount of real-world graph data for GNN training is prohibitive due to privacy concerns, regulation restrictions, and commercial competitions. Federated learning (FL), a trending distributed learning paradigm, provides possibilities to solve this challenge while preserving data privacy. Despite recent advances in vision and language domains, there is no suitable platform for the FL of GNNs. To this end, we introduce FedGraphNN, an open FL benchmark system that can facilitate research on federated GNNs. FedGraphNN is built on a unified formulation of graph FL and contains a wide range of datasets from different domains, popular GNN models, and FL algorithms, with secure and efficient system support. Particularly for the datasets, we collect, preprocess, and partition 36 datasets from 7 domains, including both publicly available ones and specifically obtained ones such as hERG and Tencent. Our empirical analysis showcases the utility of our benchmark system, while exposing significant challenges in graph FL: federated GNNs perform worse in most datasets with a non-IID split than centralized GNNs; the GNN model that attains the best result in the centralized setting may not maintain its advantage in the FL setting. These results imply that more research efforts are needed to unravel the mystery behind federated GNNs. Moreover, our system performance analysis demonstrates that the FedGraphNN system is computationally efficient and secure to large-scale graphs datasets. We maintain the source code at https://github.com/FedML-AI/FedGraphNN.",0
"The popularity of Graph Neural Network (GNN) research is increasing because of their ability to learn distributed representations from graph-structured data. However, it is not feasible to centralize a large amount of real-world graph data for GNN training due to privacy concerns, regulation restrictions, and commercial competition. To solve this problem, Federated learning (FL) is a promising distributed learning paradigm that preserves data privacy. Unfortunately, there is no suitable platform for the FL of GNNs despite recent advances in vision and language domains. Therefore, we have developed FedGraphNN, an open FL benchmark system that can facilitate research on federated GNNs. FedGraphNN is built on a unified formulation of graph FL and contains a wide range of datasets from different domains, popular GNN models, and FL algorithms with secure and efficient system support. We have collected, preprocessed, and partitioned 36 datasets from 7 domains, including publicly available ones and specifically obtained ones such as hERG and Tencent. Our empirical analysis shows that federated GNNs perform worse than centralized GNNs in most datasets with a non-IID split, and the GNN model that performs best in the centralized setting may not maintain its advantage in the FL setting. Our system performance analysis demonstrates that the FedGraphNN system is computationally efficient and secure for large-scale graph datasets. The source code is available at https://github.com/FedML-AI/FedGraphNN. More research is needed to uncover the mystery behind federated GNNs.",1
"Interpreting deep neural networks from the ordinary differential equations (ODEs) perspective has inspired many efficient and robust network architectures. However, existing ODE based approaches ignore the relationship among data points, which is a critical component in many problems including few-shot learning and semi-supervised learning. In this paper, inspired by the diffusive ODEs, we propose a novel diffusion residual network (Diff-ResNet) to strengthen the interactions among data points. Under the structured data assumption, it is proved that the diffusion mechanism can decrease the distance-diameter ratio that improves the separability of inter-class points and reduces the distance among local intra-class points. This property can be easily adopted by the residual networks for constructing the separable hyperplanes. The synthetic binary classification experiments demonstrate the effectiveness of the proposed diffusion mechanism. Moreover, extensive experiments of few-shot image classification and semi-supervised graph node classification in various datasets validate the advantages of the proposed Diff-ResNet over existing few-shot learning methods.",0
"The perspective of interpreting deep neural networks through ordinary differential equations (ODEs) has inspired the creation of many network architectures that are efficient and robust. However, current ODE-based approaches disregard the relationship among data points, which is an essential element in various problems, including semi-supervised learning and few-shot learning. This paper proposes a novel Diffusion Residual Network (Diff-ResNet) that strengthens the interactions among data points by drawing inspiration from diffusive ODEs. Under the assumption of structured data, it is proven that the diffusion mechanism can improve the separability of inter-class points and reduce the distance among local intra-class points, thus decreasing the distance-diameter ratio. This property can be easily incorporated into residual networks to construct separable hyperplanes. Synthetic binary classification experiments validate the effectiveness of the proposed diffusion mechanism. Furthermore, extensive experiments in various datasets for few-shot image classification and semi-supervised graph node classification demonstrate the superiority of the proposed Diff-ResNet over existing few-shot learning methods.",1
"The success of machine learning stems from its structured data representation. Similar data have close representation as compressed codes for classification or emerged labels for clustering. We observe that the frequency of the internal representation follows power laws in both supervised and unsupervised learning. The scale-invariant distribution implies that machine learning largely compresses frequent typical data, and at the same time, differentiates many atypical data as outliers. In this study, we derive how the power laws can naturally arise in machine learning. In terms of information theory, the scale-invariant representation corresponds to a maximally uncertain data grouping among possible representations that guarantee pre-specified learning accuracy.",0
"Machine learning owes its success to the way it represents structured data. The representation of similar data is done through compressed codes for classification or labels for clustering. It has been noticed that the frequency of the internal representation follows power laws in both supervised and unsupervised learning. The distribution being scale-invariant indicates that machine learning compresses the frequently occurring typical data while identifying many atypical data as outliers. This study delves into how power laws can arise naturally in machine learning. From an information theory perspective, the scale-invariant representation corresponds to an uncertain grouping of data that maximizes the chances of achieving pre-defined learning accuracy.",1
"Sensitive medical data is often subject to strict usage constraints. In this paper, we trained a generative adversarial network (GAN) on real-world electronic health records (EHR). It was then used to create a data-set of ""fake"" patients through synthetic data generation (SDG) to circumvent usage constraints. This real-world data was tabular, binary, intensive care unit (ICU) patient diagnosis data. The entire data-set was split into separate data silos to mimic real-world scenarios where multiple ICU units across different hospitals may have similarly structured data-sets within their own organisations but do not have access to each other's data-sets. We implemented federated learning (FL) to train separate GANs locally at each organisation, using their unique data silo and then combining the GANs into a single central GAN, without any siloed data ever being exposed. This global, central GAN was then used to generate the synthetic patients data-set. We performed an evaluation of these synthetic patients with statistical measures and through a structured review by a group of medical professionals. It was shown that there was no significant reduction in the quality of the synthetic EHR when we moved between training a single central model and training on separate data silos with individual models before combining them into a central model. This was true for both the statistical evaluation (Root Mean Square Error (RMSE) of 0.0154 for single-source vs. RMSE of 0.0169 for dual-source federated) and also for the medical professionals' evaluation (no quality difference between EHR generated from a single source and EHR generated from multiple sources).",0
"The use of sensitive medical data is often restricted, so we developed a generative adversarial network (GAN) to generate fake patient data from real electronic health records (EHR) using synthetic data generation (SDG) to bypass these constraints. The EHR data we used was binary and related to ICU patient diagnoses. To simulate real-world scenarios where different hospitals have similar datasets but cannot access each other's data, we divided the data into separate silos. We used federated learning (FL) to train GANs locally at each organization, combining them into a central GAN that produced the synthetic patient data. We evaluated the synthetic data using statistical measures and feedback from medical professionals, finding no significant quality difference between a single central model and separate models trained on individual silos before being combined.",1
"Message-Passing Neural Networks (MPNNs), the most prominent Graph Neural Network (GNN) framework, celebrate much success in the analysis of graph-structured data. Concurrently, the sparsification of Neural Network models attracts a great amount of academic and industrial interest. In this paper, we conduct a structured study of the effect of sparsification on the trainable part of MPNNs known as the Update step. To this end, we design a series of models to successively sparsify the linear transform in the Update step. Specifically, we propose the ExpanderGNN model with a tuneable sparsification rate and the Activation-Only GNN, which has no linear transform in the Update step. In agreement with a growing trend in the literature, the sparsification paradigm is changed by initialising sparse neural network architectures rather than expensively sparsifying already trained architectures. Our novel benchmark models enable a better understanding of the influence of the Update step on model performance and outperform existing simplified benchmark models such as the Simple Graph Convolution. The ExpanderGNNs, and in some cases the Activation-Only models, achieve performance on par with their vanilla counterparts on several downstream tasks while containing significantly fewer trainable parameters. In experiments with matching parameter numbers, our benchmark models outperform the state-of-the-art GNN models. Our code is publicly available at: https://github.com/ChangminWu/ExpanderGNN.",0
"The analysis of graph-structured data has become increasingly successful with the use of Message-Passing Neural Networks (MPNNs), which are the most prominent framework for Graph Neural Networks (GNNs). Simultaneously, there is a growing interest in the sparsification of Neural Network models. This paper presents a structured examination of the impact of sparsification on the Update step of MPNNs, which is the trainable part. We introduce two models, the ExpanderGNN and the Activation-Only GNN, which progressively sparsify the linear transform in the Update step. We adopt a new approach to sparsification by initializing sparse neural network architectures rather than sparsifying already trained architectures. Our benchmark models provide a better understanding of the effect of the Update step on model performance and outperform existing simplified benchmark models like Simple Graph Convolution. The ExpanderGNNs and, in some cases, the Activation-Only models achieve comparable performance to their vanilla counterparts on several downstream tasks, but with significantly fewer trainable parameters. In experiments with matching parameter numbers, our benchmark models surpass the state-of-the-art GNN models. The code for our models is publicly available at: https://github.com/ChangminWu/ExpanderGNN.",1
"Graph feature extraction is a fundamental task in graphs analytics. Using feature vectors (graph descriptors) in tandem with data mining algorithms that operate on Euclidean data, one can solve problems such as classification, clustering, and anomaly detection on graph-structured data. This idea has proved fruitful in the past, with spectral-based graph descriptors providing state-of-the-art classification accuracy on benchmark datasets. However, these algorithms do not scale to large graphs since: 1) they require storing the entire graph in memory, and 2) the end-user has no control over the algorithm's runtime. In this paper, we present single-pass streaming algorithms to approximate structural features of graphs (counts of subgraphs of order $k \geq 4$). Operating on edge streams allows us to avoid keeping the entire graph in memory, and controlling the sample size enables us to control the time taken by the algorithm. We demonstrate the efficacy of our descriptors by analyzing the approximation error, classification accuracy, and scalability to massive graphs. Our experiments showcase the effect of the sample size on approximation error and predictive accuracy. The proposed descriptors are applicable on graphs with millions of edges within minutes and outperform the state-of-the-art descriptors in classification accuracy.",0
"The extraction of graph features is a crucial task in graphs analytics. By using feature vectors, or graph descriptors, in conjunction with data mining algorithms, which work on Euclidean data, one can solve problems such as classification, clustering, and anomaly detection on graph-structured data. In the past, spectral-based graph descriptors have been successful in achieving state-of-the-art classification accuracy on benchmark datasets. However, these algorithms are not scalable to large graphs due to the need to store the entire graph in memory, and the lack of control over the algorithm's runtime. This paper presents single-pass streaming algorithms that approximate the structural features of graphs, such as counts of subgraphs of order $k \geq 4$. Operating on edge streams enables us to avoid storing the entire graph in memory, and controlling the sample size allows us to manage the time taken by the algorithm. Our approach's efficacy is demonstrated by analyzing the approximation error, classification accuracy, and scalability to massive graphs. Our experiments show the impact of the sample size on approximation error and predictive accuracy. Our proposed descriptors can be applied to graphs with millions of edges within minutes and outperform state-of-the-art descriptors in classification accuracy.",1
"Networks are ubiquitous in the real world such as social networks and communication networks, and anomaly detection on networks aims at finding nodes whose structural or attributed patterns deviate significantly from the majority of reference nodes. However, most of the traditional anomaly detection methods neglect the relation structure information among data points and therefore cannot effectively generalize to the graph structure data. In this paper, we propose an end-to-end model of Deep Dual Support Vector Data description based Autoencoder (Dual-SVDAE) for anomaly detection on attributed networks, which considers both the structure and attribute for attributed networks. Specifically, Dual-SVDAE consists of a structure autoencoder and an attribute autoencoder to learn the latent representation of the node in the structure space and attribute space respectively. Then, a dual-hypersphere learning mechanism is imposed on them to learn two hyperspheres of normal nodes from the structure and attribute perspectives respectively. Moreover, to achieve joint learning between the structure and attribute of the network, we fuse the structure embedding and attribute embedding as the final input of the feature decoder to generate the node attribute. Finally, abnormal nodes can be detected by measuring the distance of nodes to the learned center of each hypersphere in the latent structure space and attribute space respectively. Extensive experiments on the real-world attributed networks show that Dual-SVDAE consistently outperforms the state-of-the-arts, which demonstrates the effectiveness of the proposed method.",0
"In the real world, networks like social and communication networks are everywhere. Anomaly detection on networks involves identifying nodes that differ significantly from the majority of reference nodes in terms of structural or attributed patterns. However, traditional anomaly detection methods often overlook the relationship structure among data points, which makes it difficult to generalize to graph structure data. In this paper, we present the Deep Dual Support Vector Data description based Autoencoder (Dual-SVDAE), an end-to-end model for detecting anomalies in attributed networks that takes both structure and attribute into account. The Dual-SVDAE comprises a structure autoencoder and an attribute autoencoder that learn the node's latent representation in the structure and attribute spaces, respectively. A dual-hypersphere learning mechanism helps to learn two hyperspheres of normal nodes from the structure and attribute viewpoints. To achieve joint learning between the network's structure and attribute, the structure embedding and attribute embedding are fused as the final input to the feature decoder to produce the node attribute. Finally, abnormal nodes are detected by measuring the distance between the nodes and the learned center of each hypersphere in the latent structure and attribute spaces. The effectiveness of the proposed method is demonstrated by extensive experiments on real-world attributed networks, where Dual-SVDAE consistently outperforms state-of-the-art methods.",1
"Graph neural networks (GNNs) have shown great power in modeling graph structured data. However, similar to other machine learning models, GNNs may make biased predictions w.r.t protected sensitive attributes, e.g., skin color and gender. This is because the training data often contains historical bias towards sensitive attributes. In addition, we empirically show that the discrimination in GNNs can be magnified by graph structures and the message-passing mechanism of GNNs. As a result, the applications of GNNs in high-stake domains such as crime rate prediction would be largely limited. Though extensive studies of fair classification have been conducted on i.i.d data, methods to address the problem of discrimination on non-i.i.d data are rather limited. Generally, learning fair models require abundant sensitive attributes to regularize the model. However, for many graphs such as social networks, users are reluctant to share sensitive attributes. Thus, only limited sensitive attributes are available for fair GNN training in practice. Moreover, directly collecting and applying the sensitive attributes in fair model training may cause privacy issues, because the sensitive information can be leaked in data breach or attacks on the trained model. Therefore, we study a novel and crucial problem of learning fair GNNs with limited and private sensitive attribute information. In an attempt to address these problems, FairGNN is proposed to eliminate the bias of GNNs whilst maintaining high accuracy by leveraging graph structures and limited sensitive information. We further extend FairGNN to NT-FairGNN which can achieve both fairness and privacy on sensitive attributes by using limited and private sensitive attributes. Theoretical analysis and extensive experiments on real-world datasets demonstrate the effectiveness of FairGNN and NT-FairGNN in achieving fair and high-accurate classification.",0
"The modeling of graph structured data has been greatly enhanced by Graph Neural Networks (GNNs). However, like other machine learning models, GNNs may generate biased predictions with respect to protected sensitive attributes, such as skin color and gender, due to historical bias in the training data. Moreover, GNNs' discrimination can be amplified by their graph structures and message-passing mechanism, which limits their applications in high-stake domains like crime rate prediction. Although fair classification has been extensively studied on i.i.d data, addressing discrimination on non-i.i.d data is challenging, especially when there are only limited and private sensitive attributes available. FairGNN is proposed to tackle this issue by leveraging graph structures and limited sensitive information to eliminate the bias of GNNs while maintaining high accuracy. Additionally, NT-FairGNN extends FairGNN by achieving both fairness and privacy on sensitive attributes using limited and private information. The effectiveness of FairGNN and NT-FairGNN is demonstrated through theoretical analysis and extensive experiments on real-world datasets.",1
Learning distributions over graph-structured data is a challenging task with many applications in biology and chemistry. In this work we use an energy-based model (EBM) based on multi-channel graph neural networks (GNN) to learn permutation invariant unnormalized density functions on graphs. Unlike standard EBM training methods our approach is to learn the model via minimizing adversarial stein discrepancy. Samples from the model can be obtained via Langevin dynamics based MCMC. We find that this approach achieves competitive results on graph generation compared to benchmark models.,0
Acquiring knowledge about distributions over graph-structured data is a daunting undertaking that has numerous applications in the fields of biology and chemistry. Our study utilizes an energy-based model (EBM) that relies on multi-channel graph neural networks (GNN) to learn unnormalized density functions that are permutation invariant on graphs. Our approach differs from the usual EBM training methods as we aim to learn the model by minimizing adversarial stein discrepancy. Samples can be procured from the model through Langevin dynamics based Markov Chain Monte Carlo (MCMC). Our research demonstrates that this technique produces competitive outcomes in graph generation in comparison to benchmark models.,1
"UMAP is a non-parametric graph-based dimensionality reduction algorithm using applied Riemannian geometry and algebraic topology to find low-dimensional embeddings of structured data. The UMAP algorithm consists of two steps: (1) Compute a graphical representation of a dataset (fuzzy simplicial complex), and (2) Through stochastic gradient descent, optimize a low-dimensional embedding of the graph. Here, we extend the second step of UMAP to a parametric optimization over neural network weights, learning a parametric relationship between data and embedding. We first demonstrate that Parametric UMAP performs comparably to its non-parametric counterpart while conferring the benefit of a learned parametric mapping (e.g. fast online embeddings for new data). We then explore UMAP as a regularization, constraining the latent distribution of autoencoders, parametrically varying global structure preservation, and improving classifier accuracy for semi-supervised learning by capturing structure in unlabeled data. Google Colab walkthrough: https://colab.research.google.com/drive/1WkXVZ5pnMrm17m0YgmtoNjM_XHdnE5Vp?usp=sharing",0
"UMAP is an algorithm that utilizes applied Riemannian geometry and algebraic topology to reduce the dimensionality of structured data. It is a non-parametric graph-based method that involves two steps: (1) Creating a graphical representation of the data (fuzzy simplicial complex), and (2) Optimizing a low-dimensional embedding of the graph through stochastic gradient descent. In this study, we enhance the second step of UMAP by incorporating parametric optimization of neural network weights to establish a relationship between data and embedding. Our results demonstrate that Parametric UMAP performs similarly to its non-parametric counterpart while offering the advantage of a learned parametric mapping, such as rapid online embeddings for new data. We also investigate the use of UMAP as a regularization technique, which can help constrain the latent distribution of autoencoders, parametrically alter global structure preservation, and improve classifier accuracy for semi-supervised learning by capturing the structure of unlabeled data. The Google Colab walkthrough for this project can be found at: https://colab.research.google.com/drive/1WkXVZ5pnMrm17m0YgmtoNjM_XHdnE5Vp?usp=sharing.",1
"Graph Neural Networks (GNNs), which generalize the deep neural networks to graph-structured data, have achieved great success in modeling graphs. However, as an extension of deep learning for graphs, GNNs lack explainability, which largely limits their adoption in scenarios that demand the transparency of models. Though many efforts are taken to improve the explainability of deep learning, they mainly focus on i.i.d data, which cannot be directly applied to explain the predictions of GNNs because GNNs utilize both node features and graph topology to make predictions. There are only very few work on the explainability of GNNs and they focus on post-hoc explanations. Since post-hoc explanations are not directly obtained from the GNNs, they can be biased and misrepresent the true explanations. Therefore, in this paper, we study a novel problem of self-explainable GNNs which can simultaneously give predictions and explanations. We propose a new framework which can find $K$-nearest labeled nodes for each unlabeled node to give explainable node classification, where nearest labeled nodes are found by interpretable similarity module in terms of both node similarity and local structure similarity. Extensive experiments on real-world and synthetic datasets demonstrate the effectiveness of the proposed framework for explainable node classification.",0
"The success of Graph Neural Networks (GNNs) in modeling graphs is well-established. However, due to their lack of explainability, their adoption is limited in scenarios that require transparent models. While many efforts have been made to improve the explainability of deep learning, they mainly focus on i.i.d data and cannot be applied directly to GNNs. The few existing works on the explainability of GNNs are post-hoc explanations, which can be biased and misrepresent true explanations. To address this issue, we propose a novel framework for self-explainable GNNs that simultaneously provides predictions and explanations. Our framework finds $K$-nearest labeled nodes for each unlabeled node using an interpretable similarity module that considers both node similarity and local structure similarity. Extensive experiments on real-world and synthetic datasets demonstrate the effectiveness of our framework for explainable node classification.",1
"Representation learning on static graph-structured data has shown a significant impact on many real-world applications. However, less attention has been paid to the evolving nature of temporal networks, in which the edges are often changing over time. The embeddings of such temporal networks should encode both graph-structured information and the temporally evolving pattern. Existing approaches in learning temporally evolving network representations fail to capture the temporal interdependence. In this paper, we propose Toffee, a novel approach for temporal network representation learning based on tensor decomposition. Our method exploits the tensor-tensor product operator to encode the cross-time information, so that the periodic changes in the evolving networks can be captured. Experimental results demonstrate that Toffee outperforms existing methods on multiple real-world temporal networks in generating effective embeddings for the link prediction tasks.",0
"Many practical applications have shown that representation learning on static graph-structured data has a significant impact. However, less attention has been given to the changing nature of temporal networks where edges change over time. Embeddings of temporal networks should include both graph-structured information and the evolving patterns over time. Existing approaches for learning temporal network representations have failed to consider the temporal interdependence. In this paper, we introduce Toffee, a novel approach for temporal network representation learning that uses tensor decomposition. Our method uses the tensor-tensor product operator to encode cross-time information, allowing it to capture periodic changes in evolving networks. Experimental results demonstrate that Toffee performs better than existing methods on multiple real-world temporal networks, generating effective embeddings for link prediction tasks.",1
"Graph structured data have enabled several successful applications such as recommendation systems and traffic prediction, given the rich node features and edges information. However, these high-dimensional features and high-order adjacency information are usually heterogeneous and held by different data holders in practice. Given such vertical data partition (e.g., one data holder will only own either the node features or edge information), different data holders have to develop efficient joint training protocols rather than directly transfer data to each other due to privacy concerns. In this paper, we focus on the edge privacy, and consider a training scenario where Bob with node features will first send training node features to Alice who owns the adjacency information. Alice will then train a graph neural network (GNN) with the joint information and release an inference API. During inference, Bob is able to provide test node features and query the API to obtain the predictions for test nodes. Under this setting, we first propose a privacy attack LinkTeller via influence analysis to infer the private edge information held by Alice via designing adversarial queries for Bob. We then empirically show that LinkTeller is able to recover a significant amount of private edges, outperforming existing baselines. To further evaluate the privacy leakage, we adapt an existing algorithm for differentially private graph convolutional network (DP GCN) training and propose a new DP GCN mechanism LapGraph. We show that these DP GCN mechanisms are not always resilient against LinkTeller empirically under mild privacy guarantees ($\varepsilon>5$). Our studies will shed light on future research towards designing more resilient privacy-preserving GCN models; in the meantime, provide an in-depth understanding of the tradeoff between GCN model utility and robustness against potential privacy attacks.",0
"The use of graph structured data has led to successful applications, such as traffic prediction and recommendation systems, due to the abundance of node features and edge information. However, these features and adjacency information are often heterogeneous and belong to different data holders, resulting in a vertical data partition. To address privacy concerns, data holders must develop efficient joint training protocols instead of directly sharing data. This paper focuses on edge privacy and proposes a training scenario where Bob sends his node features to Alice, who owns the adjacency information. Alice trains a graph neural network (GNN) with the joint information and releases an inference API. During inference, Bob can query the API for predictions of test nodes. The paper introduces a privacy attack called LinkTeller, which utilizes influence analysis to infer Alice's private edge information by designing adversarial queries for Bob. Empirical results show that LinkTeller outperforms existing baselines in recovering private edges. The paper also evaluates the privacy leakage of differentially private graph convolutional network (DP GCN) training mechanisms and proposes a new mechanism called LapGraph. The study highlights the need for more resilient privacy-preserving GCN models and provides insights into the tradeoff between GCN model utility and robustness against potential privacy attacks.",1
"Spatio-temporal forecasting is of great importance in a wide range of dynamical systems applications from atmospheric science, to recent COVID-19 spread modeling. These applications rely on accurate predictions of spatio-temporal structured data reflecting real-world phenomena. A stunning characteristic is that the dynamical system is not only driven by some physics laws but also impacted by the localized factor in spatial and temporal regions. One of the major challenges is to infer the underlying causes, which generate the perceived data stream and propagate the involved causal dynamics through the distributed observing units. Another challenge is that the success of machine learning based predictive models requires massive annotated data for model training. However, the acquisition of high-quality annotated data is objectively manual and tedious as it needs a considerable amount of 0 intervention, making it infeasible in fields that require high levels of expertise. To tackle these challenges, we advocate a spatio-temporal physics-coupled neural networks (ST-PCNN) model to learn the underlying physics of the dynamical system and further couple the learned physics to assist the learning of the recurring dynamics. To deal with data-acquisition constraints, an active learning mechanism with Kriging for actively acquiring the most informative data is proposed for ST-PCNN training in a partially observable environment. Our experiments on both synthetic and real-world datasets exhibit that the proposed ST-PCNN with active learning converges to near optimal accuracy with substantially fewer instances.",0
"Accurate predictions of spatio-temporal structured data are crucial in various applications such as atmospheric science and COVID-19 spread modeling. These systems are impacted not only by physics laws but also by localized factors in spatial and temporal regions. Inferring underlying causes and propagating causal dynamics through distributed observing units are major challenges. Machine learning based predictive models require large amounts of annotated data for training, which is often difficult to acquire manually. To address these challenges, we suggest using a spatio-temporal physics-coupled neural networks (ST-PCNN) model to learn the underlying physics of the dynamical system and assist with learning recurring dynamics. An active learning mechanism with Kriging is proposed to acquire the most informative data for ST-PCNN training in a partially observable environment. Our experiments demonstrate that the proposed ST-PCNN with active learning achieves near-optimal accuracy with substantially fewer instances on both synthetic and real-world datasets.",1
"Graph neural networks (GNNs) have been popularly used in analyzing graph-structured data, showing promising results in various applications such as node classification, link prediction and network recommendation. In this paper, we present a new graph attention neural network, namely GIPA, for attributed graph data learning. GIPA consists of three key components: attention, feature propagation and aggregation. Specifically, the attention component introduces a new multi-layer perceptron based multi-head to generate better non-linear feature mapping and representation than conventional implementations such as dot-product. The propagation component considers not only node features but also edge features, which differs from existing GNNs that merely consider node features. The aggregation component uses a residual connection to generate the final embedding. We evaluate the performance of GIPA using the Open Graph Benchmark proteins (ogbn-proteins for short) dataset. The experimental results reveal that GIPA can beat the state-of-the-art models in terms of prediction accuracy, e.g., GIPA achieves an average test ROC-AUC of $0.8700\pm 0.0010$ and outperforms all the previous methods listed in the ogbn-proteins leaderboard.",0
"The use of graph neural networks (GNNs) has become increasingly popular in the analysis of data structured as graphs. They have shown promising results in various applications, including node classification, link prediction, and network recommendation. In this paper, we propose a novel GNN called GIPA, which is designed for learning attributed graph data. GIPA is composed of three main components: attention, feature propagation, and aggregation. The attention component utilizes a multi-layer perceptron based multi-head to generate better non-linear feature mapping and representation than traditional dot-product methods. The propagation component considers both node and edge features, which is a departure from existing GNNs that only consider node features. The aggregation component utilizes a residual connection to produce the final embedding. We evaluate GIPA's performance using the Open Graph Benchmark proteins dataset (ogbn-proteins) and find that it outperforms all previous methods listed on the ogbn-proteins leaderboard, achieving an average test ROC-AUC of $0.8700\pm 0.0010$.",1
"Structured text understanding on Visually Rich Documents (VRDs) is a crucial part of Document Intelligence. Due to the complexity of content and layout in VRDs, structured text understanding has been a challenging task. Most existing studies decoupled this problem into two sub-tasks: entity labeling and entity linking, which require an entire understanding of the context of documents at both token and segment levels. However, little work has been concerned with the solutions that efficiently extract the structured data from different levels. This paper proposes a unified framework named StrucTexT, which is flexible and effective for handling both sub-tasks. Specifically, based on the transformer, we introduce a segment-token aligned encoder to deal with the entity labeling and entity linking tasks at different levels of granularity. Moreover, we design a novel pre-training strategy with three self-supervised tasks to learn a richer representation. StrucTexT uses the existing Masked Visual Language Modeling task and the new Sentence Length Prediction and Paired Boxes Direction tasks to incorporate the multi-modal information across text, image, and layout. We evaluate our method for structured text understanding at segment-level and token-level and show it outperforms the state-of-the-art counterparts with significantly superior performance on the FUNSD, SROIE, and EPHOIE datasets.",0
"Document Intelligence relies heavily on the understanding of Structured Text in Visually Rich Documents (VRDs). However, it has been a challenging task to extract structured data from VRDs due to the complexity of content and layout. Existing studies have separated the problem into two sub-tasks, namely entity labeling and entity linking, which require a complete understanding of the context of documents at both token and segment levels. Unfortunately, there has been limited research on solutions that can efficiently extract structured data from different levels. To address this, we propose a flexible and effective unified framework called StrucTexT, which handles both sub-tasks. We introduce a segment-token aligned encoder based on the transformer to deal with entity labeling and entity linking tasks at different granularities, and we design a novel pre-training strategy with three self-supervised tasks to learn a more robust representation. StrucTexT uses the existing Masked Visual Language Modeling task and the new Sentence Length Prediction and Paired Boxes Direction tasks to incorporate multi-modal information across text, image, and layout. We evaluate our method on the FUNSD, SROIE, and EPHOIE datasets for structured text understanding at both the segment and token levels, and show that it outperforms state-of-the-art counterparts with significantly superior performance.",1
"One intriguing property of deep neural networks (DNNs) is their inherent vulnerability to backdoor attacks -- a trojan model responds to trigger-embedded inputs in a highly predictable manner while functioning normally otherwise. Despite the plethora of prior work on DNNs for continuous data (e.g., images), the vulnerability of graph neural networks (GNNs) for discrete-structured data (e.g., graphs) is largely unexplored, which is highly concerning given their increasing use in security-sensitive domains. To bridge this gap, we present GTA, the first backdoor attack on GNNs. Compared with prior work, GTA departs in significant ways: graph-oriented -- it defines triggers as specific subgraphs, including both topological structures and descriptive features, entailing a large design spectrum for the adversary; input-tailored -- it dynamically adapts triggers to individual graphs, thereby optimizing both attack effectiveness and evasiveness; downstream model-agnostic -- it can be readily launched without knowledge regarding downstream models or fine-tuning strategies; and attack-extensible -- it can be instantiated for both transductive (e.g., node classification) and inductive (e.g., graph classification) tasks, constituting severe threats for a range of security-critical applications. Through extensive evaluation using benchmark datasets and state-of-the-art models, we demonstrate the effectiveness of GTA. We further provide analytical justification for its effectiveness and discuss potential countermeasures, pointing to several promising research directions.",0
"Deep neural networks (DNNs) possess an intriguing characteristic, namely their inherent susceptibility to backdoor attacks. Trojan models respond predictably to embedded triggers without any unusual activity otherwise. While there have been numerous studies on DNNs for continuous data, such as images, the vulnerability of graph neural networks (GNNs) for discrete-structured data, like graphs, is largely unexplored. This is especially concerning because of their increasing use in security-sensitive domains. To address this issue, we introduce GTA, the first backdoor attack on GNNs. Our approach differs significantly from previous work in several ways, including being graph-oriented, input-tailored, downstream model-agnostic, and attack-extensible. We can launch the attack without prior knowledge of downstream models or fine-tuning strategies, and it can be instantiated for both transductive and inductive tasks. This makes it a severe threat to various security-critical applications. We have conducted extensive evaluations using benchmark datasets and state-of-the-art models to demonstrate the effectiveness of GTA. We also provide analytical justification for its effectiveness and suggest potential countermeasures, highlighting promising research directions.",1
"Deep convolutional neural networks (CNNs) have shown outstanding performance in the task of semantically segmenting images. Applying the same methods on 3D data still poses challenges due to the heavy memory requirements and the lack of structured data. Here, we propose LatticeNet, a novel approach for 3D semantic segmentation, which takes raw point clouds as input. A PointNet describes the local geometry which we embed into a sparse permutohedral lattice. The lattice allows for fast convolutions while keeping a low memory footprint. Further, we introduce DeformSlice, a novel learned data-dependent interpolation for projecting lattice features back onto the point cloud. We present results of 3D segmentation on multiple datasets where our method achieves state-of-the-art performance. We also extend and evaluate our network for instance and dynamic object segmentation.",0
"The performance of deep convolutional neural networks (CNNs) in semantically segmenting images is remarkable. However, the same methods for 3D data segmentation face challenges due to high memory requirements and lack of structured data. In this study, we introduce LatticeNet, a new approach for 3D semantic segmentation that uses raw point clouds as input. The local geometry is described by a PointNet and embedded into a sparse permutohedral lattice to allow for fast convolutions with low memory requirements. Additionally, we present DeformSlice, a novel learned data-dependent interpolation that projects lattice features back onto the point cloud. Our method achieves state-of-the-art performance on multiple datasets for 3D segmentation and is extended and evaluated for instance and dynamic object segmentation.",1
"Combinatorial optimization problems (COPs) on the graph with real-life applications are canonical challenges in Computer Science. The difficulty of finding quality labels for problem instances holds back leveraging supervised learning across combinatorial problems. Reinforcement learning (RL) algorithms have recently been adopted to solve this challenge automatically. The underlying principle of this approach is to deploy a graph neural network (GNN) for encoding both the local information of the nodes and the graph-structured data in order to capture the current state of the environment. Then, it is followed by the actor to learn the problem-specific heuristics on its own and make an informed decision at each state for finally reaching a good solution. Recent studies on this subject mainly focus on a family of combinatorial problems on the graph, such as the travel salesman problem, where the proposed model aims to find an ordering of vertices that optimizes a given objective function. We use the security-aware phone clone allocation in the cloud as a classical quadratic assignment problem (QAP) to investigate whether or not deep RL-based model is generally applicable to solve other classes of such hard problems. Extensive empirical evaluation shows that existing RL-based model may not generalize to QAP.",0
"Canonical challenges in Computer Science involve Combinatorial optimization problems (COPs) on graphs with real-life applications. However, the difficulty of finding quality labels for problem instances has prevented the use of supervised learning for combinatorial problems. To address this challenge, Reinforcement learning (RL) algorithms have been recently used to automatically solve COPs. This approach uses a graph neural network (GNN) to encode the local information of nodes and graph-structured data to capture the current environment state. The actor then learns problem-specific heuristics to make informed decisions for finding a solution. Previous studies have focused on a family of combinatorial problems, such as the travel salesman problem. We investigate whether deep RL-based models can solve other classes of hard problems, using the security-aware phone clone allocation in the cloud as a classical quadratic assignment problem (QAP). However, our empirical evaluation shows that existing RL-based models may not generalize to QAP.",1
"Deep learning's performance has been extensively recognized recently. Graph neural networks (GNNs) are designed to deal with graph-structural data that classical deep learning does not easily manage. Since most GNNs were created using distinct theories, direct comparisons are impossible. Prior research has primarily concentrated on categorizing existing models, with little attention paid to their intrinsic connections. The purpose of this study is to establish a unified framework that integrates GNNs based on spectral graph and approximation theory. The framework incorporates a strong integration between spatial- and spectral-based GNNs while tightly associating approaches that exist within each respective domain.",0
"There has been a significant acknowledgement of deep learning's performance recently, and in response, graph neural networks (GNNs) have been developed to handle graph-structural data that classical deep learning cannot efficiently manage. However, due to their varied theoretical foundations, making direct comparisons between different GNNs is challenging. Previous research has mainly focused on categorizing these models, with little attention given to their inherent connections. Therefore, this study aims to create a unified framework that merges GNNs based on spectral graph and approximation theory, tightly integrating methods from both spatial- and spectral-based GNNs within their respective domains.",1
"Decision forests (Forests), in particular random forests and gradient boosting trees, have demonstrated state-of-the-art accuracy compared to other methods in many supervised learning scenarios. In particular, Forests dominate other methods in tabular data, that is, when the feature space is unstructured, so that the signal is invariant to a permutation of the feature indices. However, in structured data lying on a manifold (such as images, text, and speech) deep networks (Networks), specifically convolutional deep networks (ConvNets), tend to outperform Forests. We conjecture that at least part of the reason for this is that the input to Networks is not simply the feature magnitudes, but also their indices. In contrast, naive Forest implementations fail to explicitly consider feature indices. A recently proposed Forest approach demonstrates that Forests, for each node, implicitly sample a random matrix from some specific distribution. These Forests, like some classes of Networks, learn by partitioning the feature space into convex polytopes corresponding to linear functions. We build on that approach and show that one can choose distributions in a manifold-aware fashion to incorporate feature locality. We demonstrate the empirical performance on data whose features live on three different manifolds: a torus, images, and time-series. Moreover, we demonstrate its strength in multivariate simulated settings and also show superiority in predicting surgical outcome in epilepsy patients and predicting movement direction from raw stereotactic EEG data from non-motor brain regions. In all simulations and real data, Manifold Oblique Random Forest (MORF) algorithm outperforms approaches that ignore feature space structure and challenges the performance of ConvNets. Moreover, MORF runs fast and maintains interpretability and theoretical justification.",0
"Decision forests, specifically random forests and gradient boosting trees, have been shown to achieve exceptional accuracy in many supervised learning scenarios compared to other methods. In unstructured tabular data, where the feature space does not follow a specific pattern, decision forests outperform other approaches. However, in structured data that lies on a manifold, such as images, text, and speech, deep networks like convolutional deep networks tend to outperform decision forests. This may be due to the fact that deep networks consider not only the feature magnitudes but also their indices, whereas decision forests do not explicitly consider feature indices. A new approach to decision forests suggests that these forests implicitly sample a random matrix from a specific distribution for each node. These forests learn by partitioning the feature space into convex polytopes that correspond to linear functions. By choosing distributions in a manifold-aware way to incorporate feature locality, we demonstrate the empirical performance of the Manifold Oblique Random Forest (MORF) algorithm in data with features that live on different manifolds. MORF outperforms other approaches that ignore feature space structure and challenges the performance of convolutional deep networks in both simulated and real-world settings. Additionally, MORF is fast, interpretable, and theoretically justified.",1
"Graph neural networks (GNN) have shown great success in learning from graph-structured data. They are widely used in various applications, such as recommendation, fraud detection, and search. In these domains, the graphs are typically large, containing hundreds of millions of nodes and several billions of edges. To tackle this challenge, we develop DistDGL, a system for training GNNs in a mini-batch fashion on a cluster of machines. DistDGL is based on the Deep Graph Library (DGL), a popular GNN development framework. DistDGL distributes the graph and its associated data (initial features and embeddings) across the machines and uses this distribution to derive a computational decomposition by following an owner-compute rule. DistDGL follows a synchronous training approach and allows ego-networks forming the mini-batches to include non-local nodes. To minimize the overheads associated with distributed computations, DistDGL uses a high-quality and light-weight min-cut graph partitioning algorithm along with multiple balancing constraints. This allows it to reduce communication overheads and statically balance the computations. It further reduces the communication by replicating halo nodes and by using sparse embedding updates. The combination of these design choices allows DistDGL to train high-quality models while achieving high parallel efficiency and memory scalability. We demonstrate our optimizations on both inductive and transductive GNN models. Our results show that DistDGL achieves linear speedup without compromising model accuracy and requires only 13 seconds to complete a training epoch for a graph with 100 million nodes and 3 billion edges on a cluster with 16 machines. DistDGL is now publicly available as part of DGL:https://github.com/dmlc/dgl/tree/master/python/dgl/distributed.",0
"Learning from graph-structured data has seen a significant improvement with Graph neural networks (GNN), which are now widely used in various domains such as recommendation, fraud detection, and search. These applications usually involve large graphs containing millions of nodes and billions of edges, making it a challenge to train GNNs. To address this issue, we have developed DistDGL, a system that trains GNNs in a mini-batch fashion on a cluster of machines. Based on the popular GNN development framework, Deep Graph Library (DGL), DistDGL distributes the graph and its associated data across machines and derives a computational decomposition using an owner-compute rule. To minimize distributed computation overheads, DistDGL employs a high-quality and lightweight min-cut graph partitioning algorithm along with multiple balancing constraints. It also reduces communication overhead by replicating halo nodes and using sparse embedding updates. DistDGL achieves high parallel efficiency, memory scalability, and linear speedup without compromising model accuracy. We demonstrate our optimizations on both inductive and transductive GNN models, and DistDGL is now publicly available as part of DGL on Github.",1
"Data selection methods, such as active learning and core-set selection, are useful tools for improving the data efficiency of deep learning models on large-scale datasets. However, recent deep learning models have moved forward from independent and identically distributed data to graph-structured data, such as social networks, e-commerce user-item graphs, and knowledge graphs. This evolution has led to the emergence of Graph Neural Networks (GNNs) that go beyond the models existing data selection methods are designed for. Therefore, we present Grain, an efficient framework that opens up a new perspective through connecting data selection in GNNs with social influence maximization. By exploiting the common patterns of GNNs, Grain introduces a novel feature propagation concept, a diversified influence maximization objective with novel influence and diversity functions, and a greedy algorithm with an approximation guarantee into a unified framework. Empirical studies on public datasets demonstrate that Grain significantly improves both the performance and efficiency of data selection (including active learning and core-set selection) for GNNs. To the best of our knowledge, this is the first attempt to bridge two largely parallel threads of research, data selection, and social influence maximization, in the setting of GNNs, paving new ways for improving data efficiency.",0
"Methods for selecting data, such as active learning and core-set selection, can enhance the efficiency of deep learning models on extensive datasets. Nevertheless, the advancement of deep learning models from independent and identically distributed data to graph-structured data, including social networks, e-commerce user-item graphs, and knowledge graphs, has created a need for Graph Neural Networks (GNNs). These networks exceed the capabilities of existing data selection methods. In light of this, Grain presents a new framework that connects data selection in GNNs with social influence maximization. By capitalizing on the typical patterns of GNNs, Grain introduces a new concept of feature propagation, diverse influence maximization objectives with unique influence and diversity functions, and a greedy algorithm with an approximating guarantee. Empirical studies demonstrate that Grain significantly enhances the performance and efficiency of data selection, including active learning and core-set selection, for GNNs. This is the first endeavor to combine two distinct threads of research, data selection, and social influence maximization, in the context of GNNs, paving the way for improved data efficiency.",1
"The geometric structure of an optimization landscape is argued to be fundamentally important to support the success of deep neural network learning. A direct computation of the landscape beyond two layers is hard. Therefore, to capture the global view of the landscape, an interpretable model of the network-parameter (or weight) space must be established. However, the model is lacking so far. Furthermore, it remains unknown what the landscape looks like for deep networks of binary synapses, which plays a key role in robust and energy efficient neuromorphic computation. Here, we propose a statistical mechanics framework by directly building a least structured model of the high-dimensional weight space, considering realistic structured data, stochastic gradient descent training, and the computational depth of neural networks. We also consider whether the number of network parameters outnumbers the number of supplied training data, namely, over- or under-parametrization. Our least structured model reveals that the weight spaces of the under-parametrization and over-parameterization cases belong to the same class, in the sense that these weight spaces are well-connected without any hierarchical clustering structure. In contrast, the shallow-network has a broken weight space, characterized by a discontinuous phase transition, thereby clarifying the benefit of depth in deep learning from the angle of high dimensional geometry. Our effective model also reveals that inside a deep network, there exists a liquid-like central part of the architecture in the sense that the weights in this part behave as randomly as possible, providing algorithmic implications. Our data-driven model thus provides a statistical mechanics insight about why deep learning is unreasonably effective in terms of the high-dimensional weight space, and how deep networks are different from shallow ones.",0
"The success of deep neural network learning is highly dependent on the geometric structure of the optimization landscape. Unfortunately, computing this landscape beyond two layers is challenging. To overcome this, an interpretable model of the weight space is required to capture the global view of the landscape, but such a model is currently lacking. Additionally, the landscape of deep networks with binary synapses, which are crucial for robust and energy-efficient neuromorphic computation, remains unknown. To address these issues, a statistical mechanics framework is proposed that incorporates a least structured model of the weight space, stochastic gradient descent training, and the computational depth of neural networks while considering over- or under-parametrization. The proposed model reveals that under- and over-parametrization cases belong to the same class, and the weight spaces are well-connected. In contrast, shallow networks have a broken weight space with a discontinuous phase transition. This model also shows that there exists a liquid-like central part of the architecture in deep networks, where the weights behave randomly, providing algorithmic implications. Therefore, the proposed data-driven model provides a statistical mechanics insight into the effectiveness of deep learning in the high-dimensional weight space and the differences between deep and shallow networks.",1
"Despite the remarkable success of deep learning, optimal convolution operation on point cloud remains indefinite due to its irregular data structure. In this paper, we present Cubic Kernel Convolution (CKConv) that learns to voxelize the features of local points by exploiting both continuous and discrete convolutions. Our continuous convolution uniquely employs a 3D cubic form of kernel weight representation that splits a feature into voxels in embedding space. By consecutively applying discrete 3D convolutions on the voxelized features in a spatial manner, preceding continuous convolution is forced to learn spatial feature mapping, i.e., feature voxelization. In this way, geometric information can be detailed by encoding with subdivided features, and our 3D convolutions on these fixed structured data do not suffer from discretization artifacts thanks to voxelization in embedding space. Furthermore, we propose a spatial attention module, Local Set Attention (LSA), to provide comprehensive structure awareness within the local point set and hence produce representative features. By learning feature voxelization with LSA, CKConv can extract enriched features for effective point cloud analysis. We show that CKConv has great applicability to point cloud processing tasks including object classification, object part segmentation, and scene semantic segmentation with state-of-the-art results.",0
"The irregular data structure of point clouds makes it difficult to determine the optimal convolution operation, despite the success of deep learning. To address this issue, we introduce Cubic Kernel Convolution (CKConv), which combines continuous and discrete convolutions to voxelize local point features. Our continuous convolution uses a 3D cubic kernel weight representation to divide features into voxels in embedding space. By applying discrete 3D convolutions in a spatial manner, the preceding continuous convolution is forced to learn spatial feature mapping. This approach allows for detailed encoding of geometric information with subdivided features, without suffering from discretization artifacts. Additionally, we propose a Local Set Attention (LSA) module to enhance structure awareness within the local point set and extract enriched features for effective point cloud analysis. We demonstrate the applicability of CKConv to various tasks, including object classification, object part segmentation, and scene semantic segmentation, achieving state-of-the-art results.",1
"Graph Neural Networks (GNNs) have exploded onto the machine learning scene in recent years owing to their capability to model and learn from graph-structured data. Such an ability has strong implications in a wide variety of fields whose data is inherently relational, for which conventional neural networks do not perform well. Indeed, as recent reviews can attest, research in the area of GNNs has grown rapidly and has lead to the development of a variety of GNN algorithm variants as well as to the exploration of groundbreaking applications in chemistry, neurology, electronics, or communication networks, among others. At the current stage of research, however, the efficient processing of GNNs is still an open challenge for several reasons. Besides of their novelty, GNNs are hard to compute due to their dependence on the input graph, their combination of dense and very sparse operations, or the need to scale to huge graphs in some applications. In this context, this paper aims to make two main contributions. On the one hand, a review of the field of GNNs is presented from the perspective of computing. This includes a brief tutorial on the GNN fundamentals, an overview of the evolution of the field in the last decade, and a summary of operations carried out in the multiple phases of different GNN algorithm variants. On the other hand, an in-depth analysis of current software and hardware acceleration schemes is provided, from which a hardware-software, graph-aware, and communication-centric vision for GNN accelerators is distilled.",0
"In recent years, Graph Neural Networks (GNNs) have gained popularity in the machine learning community due to their ability to model and learn from graph-structured data, which is particularly useful in fields where data is inherently relational. However, while research in this area has grown rapidly and led to groundbreaking applications in various fields, efficient processing of GNNs remains a challenge due to their dependence on input graphs, combination of dense and sparse operations, and need to scale to large graphs. In light of these challenges, this paper aims to contribute two main aspects: first, a computing-focused review of the evolution of GNNs over the last decade, including a tutorial on GNN fundamentals and a summary of operations in different GNN algorithm variants. Second, an in-depth analysis of current software and hardware acceleration schemes, which highlights the need for a graph-aware, communication-centric, and hardware-software vision for GNN accelerators.",1
"Graph neural networks (GNNs) have achieved remarkable success as a framework for deep learning on graph-structured data. However, GNNs are fundamentally limited by their tree-structured inductive bias: the WL-subtree kernel formulation bounds the representational capacity of GNNs, and polynomial-time GNNs are provably incapable of recognizing triangles in a graph. In this work, we propose to augment the GNN message-passing operations with information defined on ego graphs (i.e., the induced subgraph surrounding each node). We term these approaches Ego-GNNs and show that Ego-GNNs are provably more powerful than standard message-passing GNNs. In particular, we show that Ego-GNNs are capable of recognizing closed triangles, which is essential given the prominence of transitivity in real-world graphs. We also motivate our approach from the perspective of graph signal processing as a form of multiplex graph convolution. Experimental results on node classification using synthetic and real data highlight the achievable performance gains using this approach.",0
"GNNs have been successful in deep learning on graph-structured data, but their tree-structured inductive bias limits their representational capacity. The WL-subtree kernel formulation restricts GNNs, and polynomial-time GNNs cannot recognize triangles in a graph. Our study proposes augmenting GNN message-passing operations with information from ego graphs, or the subgraph around each node. These new approaches, called Ego-GNNs, are more powerful than standard message-passing GNNs and can recognize closed triangles, which is essential in real-world graphs. We also explain our approach as a form of multiplex graph convolution in graph signal processing. Our experiments on node classification using synthetic and real data demonstrate the potential performance gains of Ego-GNNs.",1
"Graph neural networks (GNNs) is widely used to learn a powerful representation of graph-structured data. Recent work demonstrates that transferring knowledge from self-supervised tasks to downstream tasks could further improve graph representation. However, there is an inherent gap between self-supervised tasks and downstream tasks in terms of optimization objective and training data. Conventional pre-training methods may be not effective enough on knowledge transfer since they do not make any adaptation for downstream tasks. To solve such problems, we propose a new transfer learning paradigm on GNNs which could effectively leverage self-supervised tasks as auxiliary tasks to help the target task. Our methods would adaptively select and combine different auxiliary tasks with the target task in the fine-tuning stage. We design an adaptive auxiliary loss weighting model to learn the weights of auxiliary tasks by quantifying the consistency between auxiliary tasks and the target task. In addition, we learn the weighting model through meta-learning. Our methods can be applied to various transfer learning approaches, it performs well not only in multi-task learning but also in pre-training and fine-tuning. Comprehensive experiments on multiple downstream tasks demonstrate that the proposed methods can effectively combine auxiliary tasks with the target task and significantly improve the performance compared to state-of-the-art methods.",0
"Graph neural networks (GNNs) are commonly utilized to acquire a powerful representation of data structured in graphs. Recent research has revealed that transferring knowledge from self-supervised tasks to downstream tasks can enhance graph representation even further. However, there exists a gap between self-supervised tasks and downstream tasks concerning optimization objectives and training data. Conventional pre-training techniques may not be effective in knowledge transfer as they do not adapt to downstream tasks. To address these issues, we propose a novel transfer learning approach for GNNs that effectively employs self-supervised tasks as auxiliary tasks to aid the target task. Our method selects and combines different auxiliary tasks with the target task in the fine-tuning stage, utilizing an adaptive auxiliary loss weighting model to learn the weights of auxiliary tasks, quantifying their consistency with the target task. Furthermore, the weighting model is learned through meta-learning. Our approach can be applied to various transfer learning approaches, and it performs well in multi-task learning, pre-training, and fine-tuning. Our comprehensive experiments on multiple downstream tasks demonstrate that our proposed method can effectively combine auxiliary tasks with the target task and significantly improve performance compared to state-of-the-art methods.",1
"Graph representation learning plays a vital role in processing graph-structured data. However, prior arts on graph representation learning heavily rely on labeling information. To overcome this problem, inspired by the recent success of graph contrastive learning and Siamese networks in visual representation learning, we propose a novel self-supervised approach in this paper to learn node representations by enhancing Siamese self-distillation with multi-scale contrastive learning. Specifically, we first generate two augmented views from the input graph based on local and global perspectives. Then, we employ two objectives called cross-view and cross-network contrastiveness to maximize the agreement between node representations across different views and networks. To demonstrate the effectiveness of our approach, we perform empirical experiments on five real-world datasets. Our method not only achieves new state-of-the-art results but also surpasses some semi-supervised counterparts by large margins. Code is made available at https://github.com/GRAND-Lab/MERIT",0
"The processing of graph-structured data heavily relies on graph representation learning, which is crucial to its success. However, previous methods have relied heavily on labeling information, posing a problem. To address this issue, we propose a novel self-supervised approach that builds on the success of graph contrastive learning and Siamese networks in visual representation learning. Our method involves generating two enhanced views of the input graph based on local and global perspectives and using two objectives, cross-view and cross-network contrastiveness, to maximize agreement between node representations across various views and networks. We demonstrate the effectiveness of our approach through empirical experiments on five real-world datasets, achieving new state-of-the-art results and surpassing some semi-supervised counterparts by significant margins. Our code is available at https://github.com/GRAND-Lab/MERIT.",1
"Recent achievements in end-to-end deep learning have encouraged the exploration of tasks dealing with highly structured data with unified deep network models. Having such models for compressing audio signals has been challenging since it requires discrete representations that are not easy to train with end-to-end backpropagation. In this paper, we present an end-to-end deep learning approach that combines recurrent neural networks (RNNs) within the training strategy of variational autoencoders (VAEs) with a binary representation of the latent space. We apply a reparametrization trick for the Bernoulli distribution for the discrete representations, which allows smooth backpropagation. In addition, our approach allows the separation of the encoder and decoder, which is necessary for compression tasks. To our best knowledge, this is the first end-to-end learning for a single audio compression model with RNNs, and our model achieves a Signal to Distortion Ratio (SDR) of 20.54.",0
"The progress made in end-to-end deep learning has motivated researchers to explore the use of unified deep network models for tasks that involve highly structured data. However, developing models for compressing audio signals has been challenging due to the need for discrete representations that are difficult to train using end-to-end backpropagation. This study presents a novel approach that combines variational autoencoders (VAEs) with recurrent neural networks (RNNs) to achieve end-to-end deep learning for a single audio compression model. Our approach involves using a binary representation of the latent space and a reparametrization trick for the Bernoulli distribution to enable smooth backpropagation. The encoder and decoder can also be separated, which is essential for compression tasks. Our model achieves a Signal to Distortion Ratio (SDR) of 20.54, making it the first successful end-to-end learning approach for audio compression with RNNs.",1
"Graph convolutional networks are becoming indispensable for deep learning from graph-structured data. Most of the existing graph convolutional networks share two big shortcomings. First, they are essentially low-pass filters, thus the potentially useful middle and high frequency band of graph signals are ignored. Second, the bandwidth of existing graph convolutional filters is fixed. Parameters of a graph convolutional filter only transform the graph inputs without changing the curvature of a graph convolutional filter function. In reality, we are uncertain about whether we should retain or cut off the frequency at a certain point unless we have expert domain knowledge. In this paper, we propose Automatic Graph Convolutional Networks (AutoGCN) to capture the full spectrum of graph signals and automatically update the bandwidth of graph convolutional filters. While it is based on graph spectral theory, our AutoGCN is also localized in space and has a spatial form. Experimental results show that AutoGCN achieves significant improvement over baseline methods which only work as low-pass filters.",0
"Graph convolutional networks have become an essential aspect of deep learning when dealing with graph-structured data. However, there are two major issues with most existing graph convolutional networks. Firstly, they are limited to low-pass filters which disregard the potentially valuable middle and high frequency band of graph signals. Secondly, the bandwidth of current graph convolutional filters is rigid, meaning that any parameters only affect the transformation of graph inputs and not the curvature of the filter. Without expert domain knowledge, it is difficult to determine whether to maintain or eliminate certain frequencies. To overcome these problems, we present Automatic Graph Convolutional Networks (AutoGCN) which can capture the full spectrum of graph signals and automatically adjust the bandwidth of graph convolutional filters. Even though AutoGCN is grounded in graph spectral theory, it is also spatially localized. Our experimental findings demonstrate that AutoGCN outperforms baseline methods that only function as low-pass filters.",1
"Multi-relational graph is a ubiquitous and important data structure, allowing flexible representation of multiple types of interactions and relations between entities. Similar to other graph-structured data, link prediction is one of the most important tasks on multi-relational graphs and is often used for knowledge completion. When related graphs coexist, it is of great benefit to build a larger graph via integrating the smaller ones. The integration requires predicting hidden relational connections between entities belonged to different graphs (inter-domain link prediction). However, this poses a real challenge to existing methods that are exclusively designed for link prediction between entities of the same graph only (intra-domain link prediction). In this study, we propose a new approach to tackle the inter-domain link prediction problem by softly aligning the entity distributions between different domains with optimal transport and maximum mean discrepancy regularizers. Experiments on real-world datasets show that optimal transport regularizer is beneficial and considerably improves the performance of baseline methods.",0
"The multi-relational graph is a significant and widespread data structure that offers a versatile representation of various types of relationships and interactions between entities. Link prediction, a crucial task for knowledge completion, is also essential for multi-relational graphs, similar to other graph-structured data. When multiple interconnected graphs exist, integrating them to form a larger graph can be advantageous. However, this integration requires predicting hidden relational connections between entities from different graphs, which is a challenge for existing methods designed for intra-domain link prediction. In this research, we propose a new approach to address the inter-domain link prediction problem by aligning the entity distributions between different domains using optimal transport and maximum mean discrepancy regularizers. Our experiments on real-world datasets demonstrate that the optimal transport regularizer improves the performance of baseline methods significantly.",1
"Graph representation learning has attracted a surge of interest recently, whose target at learning discriminant embedding for each node in the graph. Most of these representation methods focus on supervised learning and heavily depend on label information. However, annotating graphs are expensive to obtain in the real world, especially in specialized domains (i.e. biology), as it needs the annotator to have the domain knowledge to label the graph. To approach this problem, self-supervised learning provides a feasible solution for graph representation learning. In this paper, we propose a Multi-Level Graph Contrastive Learning (MLGCL) framework for learning robust representation of graph data by contrasting space views of graphs. Specifically, we introduce a novel contrastive view - topological and feature space views. The original graph is first-order approximation structure and contains uncertainty or error, while the $k$NN graph generated by encoding features preserves high-order proximity. Thus $k$NN graph generated by encoding features not only provide a complementary view, but is more suitable to GNN encoder to extract discriminant representation. Furthermore, we develop a multi-level contrastive mode to preserve the local similarity and semantic similarity of graph-structured data simultaneously. Extensive experiments indicate MLGCL achieves promising results compared with the existing state-of-the-art graph representation learning methods on seven datasets.",0
"Lately, there has been a growing interest in graph representation learning, which aims to learn distinctive embeddings for each node in a graph. However, most of the current representation methods require labeled information and rely heavily on supervised learning. This poses a challenge as annotating graphs can be costly, particularly in specialized domains such as biology where domain knowledge is necessary to label the graph. To address this issue, self-supervised learning offers a viable solution for graph representation learning. This article proposes a Multi-Level Graph Contrastive Learning (MLGCL) framework that learns robust representation of graph data by contrasting space views of graphs. The framework introduces a novel contrastive view - topological and feature space views - where the latter is more suitable for GNN encoder to extract discriminant representation. Additionally, the framework employs a multi-level contrastive mode to preserve both the local and semantic similarity of graph-structured data. The experimental results demonstrate that MLGCL outperforms the current state-of-the-art graph representation learning methods on seven datasets.",1
"Various non-trivial spaces are becoming popular for embedding structured data such as graphs, texts, or images. Following spherical and hyperbolic spaces, more general product spaces have been proposed. However, searching for the best configuration of product space is a resource-intensive procedure, which reduces the practical applicability of the idea. We generalize the concept of product space and introduce an overlapping space that does not have the configuration search problem. The main idea is to allow subsets of coordinates to be shared between spaces of different types (Euclidean, hyperbolic, spherical). As a result, parameter optimization automatically learns the optimal configuration. Additionally, overlapping spaces allow for more compact representations since their geometry is more complex. Our experiments confirm that overlapping spaces outperform the competitors in graph embedding tasks. Here, we consider both distortion setup, where the aim is to preserve distances, and ranking setup, where the relative order should be preserved. The proposed method effectively solves the problem and outperforms the competitors in both settings. We also perform an empirical analysis in a realistic information retrieval task, where we compare all spaces by incorporating them into DSSM. In this case, the proposed overlapping space consistently achieves nearly optimal results without any configuration tuning. This allows for reducing training time, which can be significant in large-scale applications.",0
"Structured data, such as graphs, texts, and images, are increasingly being embedded in various non-trivial spaces. While spherical and hyperbolic spaces have been proposed, more general product spaces require a resource-intensive configuration search and limit practical applicability. To address this, we introduce an overlapping space that allows subsets of coordinates to be shared between different space types, automatically learning the optimal configuration and enabling more compact representations. Our experiments show that overlapping spaces outperform competitors in both distortion and ranking setups for graph embedding tasks. We also demonstrate the effectiveness of the proposed method in a realistic information retrieval task, achieving nearly optimal results without configuration tuning and reducing training time for large-scale applications.",1
"Relational databases are the de facto standard for storing and querying structured data, and extracting insights from structured data requires advanced analytics. Deep neural networks (DNNs) have achieved super-0 prediction performance in particular data types, e.g., images. However, existing DNNs may not produce meaningful results when applied to structured data. The reason is that there are correlations and dependencies across combinations of attribute values in a table, and these do not follow simple additive patterns that can be easily mimicked by a DNN. The number of possible such cross features is combinatorial, making them computationally prohibitive to model. Furthermore, the deployment of learning models in real-world applications has also highlighted the need for interpretability, especially for high-stakes applications, which remains another issue of concern to DNNs.   In this paper, we present ARM-Net, an adaptive relation modeling network tailored for structured data, and a lightweight framework ARMOR based on ARM-Net for relational data analytics. The key idea is to model feature interactions with cross features selectively and dynamically, by first transforming the input features into exponential space, and then determining the interaction order and interaction weights adaptively for each cross feature. We propose a novel sparse attention mechanism to dynamically generate the interaction weights given the input tuple, so that we can explicitly model cross features of arbitrary orders with noisy features filtered selectively. Then during model inference, ARM-Net can specify the cross features being used for each prediction for higher accuracy and better interpretability. Our extensive experiments on real-world datasets demonstrate that ARM-Net consistently outperforms existing models and provides more interpretable predictions for data-driven decision making.",0
"Storing and retrieving structured data is commonly done using relational databases, and analyzing this data requires advanced analytics. While deep neural networks (DNNs) have shown exceptional predictive ability in certain data types, such as images, they may not be effective when applied to structured data. This is because correlations and dependencies exist across attribute values in a table that do not follow simple additive patterns that can be replicated by a DNN. Additionally, the number of possible cross features is vast and computationally intensive to model, and interpretability remains a concern in high-stakes applications. In this paper, we introduce ARM-Net, an adaptive relation modeling network designed for structured data, and ARMOR, a lightweight framework based on ARM-Net for relational data analysis. ARM-Net utilizes a selective and dynamic approach to model feature interactions with cross features, transforming input features into exponential space and determining the interaction order and weights adaptively for each cross feature. A novel sparse attention mechanism is proposed to generate interaction weights dynamically, enabling the explicit modeling of cross features of any order with selectively filtered noisy features. During model inference, ARM-Net can specify the cross features used for each prediction, leading to higher accuracy and better interpretability. Our experiments on real-world datasets demonstrate that ARM-Net outperforms existing models and provides more interpretable predictions for data-driven decision making.",1
We propose a model for hierarchical structured data as an extension to the stochastic temporal convolutional network. The proposed model combines an autoregressive model with a hierarchical variational autoencoder and downsampling to achieve superior computational complexity. We evaluate the proposed model on two different types of sequential data: speech and handwritten text. The results are promising with the proposed model achieving state-of-the-art performance.,0
"Our suggestion is to extend the stochastic temporal convolutional network with a model for hierarchical structured data. This proposed model merges an autoregressive model with a hierarchical variational autoencoder and downsampling to attain exceptional computational complexity. We examine the proposed model's effectiveness on two types of sequential data, namely speech and handwritten text, and the outcomes are encouraging, with the proposed model exhibiting state-of-the-art performance.",1
"Graph neural networks have recently achieved remarkable success in representing graph-structured data, with rapid progress in both the node embedding and graph pooling methods. Yet, they mostly focus on capturing information from the nodes considering their connectivity, and not much work has been done in representing the edges, which are essential components of a graph. However, for tasks such as graph reconstruction and generation, as well as graph classification tasks for which the edges are important for discrimination, accurately representing edges of a given graph is crucial to the success of the graph representation learning. To this end, we propose a novel edge representation learning framework based on Dual Hypergraph Transformation (DHT), which transforms the edges of a graph into the nodes of a hypergraph. This dual hypergraph construction allows us to apply message passing techniques for node representations to edges. After obtaining edge representations from the hypergraphs, we then cluster or drop edges to obtain holistic graph-level edge representations. We validate our edge representation learning method with hypergraphs on diverse graph datasets for graph representation and generation performance, on which our method largely outperforms existing graph representation learning methods. Moreover, our edge representation learning and pooling method also largely outperforms state-of-the-art graph pooling methods on graph classification, not only because of its accurate edge representation learning, but also due to its lossless compression of the nodes and removal of irrelevant edges for effective message passing.",0
"Recent advancements in graph neural networks have been remarkable, particularly in representing graph-structured data through node embedding and graph pooling methods. However, these methods have mostly focused on capturing information from the nodes and not much attention has been given to representing the edges, which are crucial components of a graph. Accurately representing edges is important for tasks such as graph reconstruction and generation, as well as graph classification tasks that require discrimination. To address this issue, we propose a novel edge representation learning framework based on Dual Hypergraph Transformation (DHT), which transforms edges into nodes of a hypergraph, allowing for message passing techniques to be applied. We then cluster or drop edges to obtain holistic graph-level edge representations. Our method outperforms existing graph representation learning methods on diverse graph datasets for graph representation and generation performance. Additionally, it outperforms state-of-the-art graph pooling methods on graph classification due to its accurate edge representation learning and effective message passing.",1
"In the world where big data reigns and there is plenty of hardware prepared to gather a huge amount of non structured data, data acquisition is no longer a problem. Surveillance cameras are ubiquitous and they capture huge numbers of people walking across different scenes. However, extracting value from this data is challenging, specially for tasks that involve 0 images, such as face recognition and person re-identification. Annotation of this kind of data is a challenging and expensive task. In this work we propose a domain adaptation workflow to allow CNNs that were trained in one domain to be applied to another domain without the need for new annotation of the target data. Our method uses AlignedReID++ as the baseline, trained using a Triplet loss with batch hard. Domain adaptation is done by using pseudo-labels generated using an unsupervised learning strategy. Our results show that domain adaptation techniques really improve the performance of the CNN when applied in the target domain.",0
"In a world inundated with big data and abundant hardware, acquiring data is no longer a problem. Surveillance cameras are omnipresent, capturing vast numbers of individuals moving through various environments. However, extracting valuable insights from the data, particularly for tasks that involve 0 images like face recognition and person re-identification, presents a challenge, as annotating this data is both difficult and costly. This study proposes a domain adaptation workflow that enables CNNs trained in one domain to be applied in another domain without the need for new annotation. The approach utilizes AlignedReID++ as the baseline, which is trained using a Triplet loss with batch hard. Domain adaptation is achieved by generating pseudo-labels using an unsupervised learning strategy. The results indicate that domain adaptation techniques significantly enhance the CNN's performance in the target domain.",1
"In representation learning on the graph-structured data, under heterophily (or low homophily), many popular GNNs may fail to capture long-range dependencies, which leads to their performance degradation. To solve the above-mentioned issue, we propose a graph convolutional networks with structure learning (GCN-SL), and furthermore, the proposed approach can be applied to node classification. The proposed GCN-SL contains two improvements: corresponding to node features and edges, respectively. In the aspect of node features, we propose an efficient-spectral-clustering (ESC) and an ESC with anchors (ESC-ANCH) algorithms to efficiently aggregate feature representations from all similar nodes. In the aspect of edges, we build a re-connected adjacency matrix by using a special data preprocessing technique and similarity learning, and the re-connected adjacency matrix can be optimized directly along with GCN-SL parameters. Considering that the original adjacency matrix may provide misleading information for aggregation in GCN, especially the graphs being with a low level of homophily. The proposed GCN-SL can aggregate feature representations from nearby nodes via re-connected adjacency matrix and is applied to graphs with various levels of homophily. Experimental results on a wide range of benchmark datasets illustrate that the proposed GCN-SL outperforms the stateof-the-art GNN counterparts.",0
"When dealing with graph-structured data in representation learning, low homophily (or heterophily) can cause issues for many popular GNNs, as they may not be able to capture long-range dependencies, leading to decreased performance. To address this problem, we present a graph convolutional network with structure learning (GCN-SL) that can be used for node classification. The GCN-SL approach involves two improvements: one for node features and one for edges. For node features, we propose two algorithms (efficient-spectral-clustering and ESC with anchors) to efficiently aggregate feature representations from similar nodes. For edges, we use a special data preprocessing technique and similarity learning to build a re-connected adjacency matrix, which can be optimized directly along with GCN-SL parameters. This is necessary because the original adjacency matrix may provide misleading information for aggregation in GCN, particularly for graphs with low homophily. The proposed GCN-SL is able to aggregate feature representations from nearby nodes via the re-connected adjacency matrix and can be applied to graphs with varying levels of homophily. Our experimental results on numerous benchmark datasets demonstrate that GCN-SL outperforms current state-of-the-art GNNs.",1
"Self-supervised learning on graph-structured data has drawn recent interest for learning generalizable, transferable and robust representations from unlabeled graphs. Among many, graph contrastive learning (GraphCL) has emerged with promising representation learning performance. Unfortunately, unlike its counterpart on image data, the effectiveness of GraphCL hinges on ad-hoc data augmentations, which have to be manually picked per dataset, by either rules of thumb or trial-and-errors, owing to the diverse nature of graph data. That significantly limits the more general applicability of GraphCL. Aiming to fill in this crucial gap, this paper proposes a unified bi-level optimization framework to automatically, adaptively and dynamically select data augmentations when performing GraphCL on specific graph data. The general framework, dubbed JOint Augmentation Optimization (JOAO), is instantiated as min-max optimization. The selections of augmentations made by JOAO are shown to be in general aligned with previous ""best practices"" observed from handcrafted tuning: yet now being automated, more flexible and versatile. Moreover, we propose a new augmentation-aware projection head mechanism, which will route output features through different projection heads corresponding to different augmentations chosen at each training step. Extensive experiments demonstrate that JOAO performs on par with or sometimes better than the state-of-the-art competitors including GraphCL, on multiple graph datasets of various scales and types, yet without resorting to any laborious dataset-specific tuning on augmentation selection. We release the code at https://github.com/Shen-Lab/GraphCL_Automated.",0
"Recently, there has been interest in self-supervised learning on graph-structured data to learn generalizable, transferable, and robust representations from unlabeled graphs. Graph contrastive learning (GraphCL) is a popular method that has shown promising results in representation learning. However, unlike its counterpart on image data, GraphCL's effectiveness relies on manually selecting ad-hoc data augmentations that vary based on the dataset's diverse nature. This limits GraphCL's general applicability. To address this issue, this paper proposes JOint Augmentation Optimization (JOAO), a unified bi-level optimization framework that automatically and adaptively selects data augmentations for GraphCL on specific graph data. JOAO performs on par with or better than state-of-the-art competitors, including GraphCL, on multiple graph datasets of various scales and types without requiring any dataset-specific tuning on augmentation selection. The code is available at https://github.com/Shen-Lab/GraphCL_Automated.",1
"Data augmentation is a key element of deep learning pipelines, as it informs the network during training about transformations of the input data that keep the label unchanged. Manually finding adequate augmentation methods and parameters for a given pipeline is however rapidly cumbersome. In particular, while intuition can guide this decision for images, the design and choice of augmentation policies remains unclear for more complex types of data, such as neuroscience signals. Moreover, label independent strategies might not be suitable for such structured data and class-dependent augmentations might be necessary. This idea has been surprisingly unexplored in the literature, while it is quite intuitive: changing the color of a car image does not change the object class to be predicted, but doing the same to the picture of an orange does. This paper aims to increase the generalization power added through class-wise data augmentation. Yet, as seeking transformations depending on the class largely increases the complexity of the task, using gradient-free optimization techniques as done by most existing automatic approaches becomes intractable for real-world datasets. For this reason we propose to use differentiable data augmentation amenable to gradient-based learning. EEG signals are a perfect example of data for which good augmentation policies are mostly unknown. In this work, we demonstrate the relevance of our approach on the clinically relevant sleep staging classification task, for which we also propose differentiable transformations.",0
"Deep learning pipelines rely heavily on data augmentation to teach the network about input data transformations that preserve the label. However, manually determining appropriate augmentation techniques and parameters for a given pipeline can quickly become tedious. While it is relatively straightforward to choose augmentation methods for images based on intuition, this process is less clear for more complex data types like neuroscience signals. Additionally, label-independent strategies may not be suitable for structured data, which may require class-dependent augmentations. Despite the intuitive nature of this idea, it remains largely unexplored in the literature. This paper seeks to address this gap by proposing class-specific data augmentation to enhance generalization power. However, this task's complexity increases significantly when seeking transformations based on the class, making gradient-free optimization techniques impractical for real-world datasets. To overcome this limitation, the authors propose using differentiable data augmentation that can be optimized using gradient-based learning. EEG signals are a prime example of data lacking well-defined augmentation policies, and the proposed approach is demonstrated in the clinically relevant sleep staging classification task, for which the authors also suggest differentiable transformations.",1
"Temporal graph signals are multivariate time series with individual components associated with nodes of a fixed graph structure. Data of this kind arises in many domains including activity of social network users, sensor network readings over time, and time course gene expression within the interaction network of a model organism. Traditional matrix decomposition methods applied to such data fall short of exploiting structural regularities encoded in the underlying graph and also in the temporal patterns of the signal. How can we take into account such structure to obtain a succinct and interpretable representation of temporal graph signals?   We propose a general, dictionary-based framework for temporal graph signal decomposition (TGSD). The key idea is to learn a low-rank, joint encoding of the data via a combination of graph and time dictionaries. We propose a highly scalable decomposition algorithm for both complete and incomplete data, and demonstrate its advantage for matrix decomposition, imputation of missing values, temporal interpolation, clustering, period estimation, and rank estimation in synthetic and real-world data ranging from traffic patterns to social media activity. Our framework achieves 28% reduction in RMSE compared to baselines for temporal interpolation when as many as 75% of the observations are missing. It scales best among baselines taking under 20 seconds on 3.5 million data points and produces the most parsimonious models. To the best of our knowledge, TGSD is the first framework to jointly model graph signals by temporal and graph dictionaries.",0
"Temporal graph signals refer to multivariate time series that have individual components linked to nodes in a fixed graph structure. Such data is commonplace across various domains, including social network user activity, readings from sensor networks over time, and gene expression patterns in model organisms' interaction networks. Conventional matrix decomposition methods applied to this data fall short of utilizing the structural regularities encoded in the underlying graph and temporal signal patterns. To address this, we present a dictionary-based framework for temporal graph signal decomposition (TGSD), which involves learning a low-rank, joint encoding of the data using a combination of graph and time dictionaries. Our approach offers a highly scalable decomposition algorithm for both complete and incomplete data, which we demonstrate in synthetic and real-world data. The framework proves advantageous for matrix decomposition, imputation of missing values, temporal interpolation, clustering, period estimation, and rank estimation. With a 28% reduction in RMSE compared to baselines for temporal interpolation, even when 75% of observations are absent, our framework scales best among baselines, taking less than 20 seconds on 3.5 million data points, and produces the most parsimonious models. To the best of our knowledge, TGSD is the first framework to jointly model graph signals by temporal and graph dictionaries.",1
"We introduce the Graph Mixture Density Networks, a new family of machine learning models that can fit multimodal output distributions conditioned on graphs of arbitrary topology. By combining ideas from mixture models and graph representation learning, we address a broader class of challenging conditional density estimation problems that rely on structured data. In this respect, we evaluate our method on a new benchmark application that leverages random graphs for stochastic epidemic simulations. We show a significant improvement in the likelihood of epidemic outcomes when taking into account both multimodality and structure. The empirical analysis is complemented by two real-world regression tasks showing the effectiveness of our approach in modeling the output prediction uncertainty. Graph Mixture Density Networks open appealing research opportunities in the study of structure-dependent phenomena that exhibit non-trivial conditional output distributions.",0
"We present the Graph Mixture Density Networks, a novel group of machine learning models that can accommodate multimodal output distributions dependent on graphs of any topology. Through a combination of mixture models and graph representation learning, we can tackle a wider range of difficult conditional density estimation issues that require structured data. We evaluate our technique on a new benchmark application that employs random graphs for stochastic epidemic simulations, demonstrating a significant increase in the probability of epidemic outcomes by considering both multimodality and structure. We also conduct two real-world regression tasks to demonstrate the effectiveness of our method in modeling output prediction uncertainty. The Graph Mixture Density Networks offer exciting research opportunities in the study of structure-dependent phenomena that exhibit complex conditional output distributions.",1
"Hypergraphs are used to model higher-order interactions amongst agents and there exist many practically relevant instances of hypergraph datasets. To enable efficient processing of hypergraph-structured data, several hypergraph neural network platforms have been proposed for learning hypergraph properties and structure, with a special focus on node classification. However, almost all existing methods use heuristic propagation rules and offer suboptimal performance on many datasets. We propose AllSet, a new hypergraph neural network paradigm that represents a highly general framework for (hyper)graph neural networks and for the first time implements hypergraph neural network layers as compositions of two multiset functions that can be efficiently learned for each task and each dataset. Furthermore, AllSet draws on new connections between hypergraph neural networks and recent advances in deep learning of multiset functions. In particular, the proposed architecture utilizes Deep Sets and Set Transformer architectures that allow for significant modeling flexibility and offer high expressive power. To evaluate the performance of AllSet, we conduct the most extensive experiments to date involving ten known benchmarking datasets and three newly curated datasets that represent significant challenges for hypergraph node classification. The results demonstrate that AllSet has the unique ability to consistently either match or outperform all other hypergraph neural networks across the tested datasets. Our implementation and dataset will be released upon acceptance.",0
"There are numerous practical applications for hypergraphs, which are utilized to represent higher-order interactions among agents. However, existing hypergraph neural network platforms that have been developed for efficient processing of hypergraph data are limited in their performance due to their reliance on heuristic propagation rules. To address this issue, we introduce a novel hypergraph neural network paradigm called AllSet, which employs a highly general framework for (hyper)graph neural networks. Our approach implements hypergraph neural network layers as compositions of two multiset functions that can be efficiently learned for each task and dataset. Additionally, our architecture utilizes Deep Sets and Set Transformer architectures, which allow for increased modeling flexibility and expressive power. We evaluate the performance of AllSet using an extensive set of experiments on ten benchmarking datasets and three newly curated datasets, which pose significant challenges for hypergraph node classification. Our results demonstrate that AllSet consistently matches or outperforms all other hypergraph neural networks across all tested datasets. We will release our implementation and dataset upon acceptance.",1
"Deep neural networks, while generalize well, are known to be sensitive to small adversarial perturbations. This phenomenon poses severe security threat and calls for in-depth investigation of the robustness of deep learning models. With the emergence of neural networks for graph structured data, similar investigations are urged to understand their robustness. It has been found that adversarially perturbing the graph structure and/or node features may result in a significant degradation of the model performance. In this work, we show from a different angle that such fragility similarly occurs if the graph contains a few bad-actor nodes, which compromise a trained graph neural network through flipping the connections to any targeted victim. Worse, the bad actors found for one graph model severely compromise other models as well. We call the bad actors ``anchor nodes'' and propose an algorithm, named GUA, to identify them. Thorough empirical investigations suggest an interesting finding that the anchor nodes often belong to the same class; and they also corroborate the intuitive trade-off between the number of anchor nodes and the attack success rate. For the dataset Cora which contains 2708 nodes, as few as six anchor nodes will result in an attack success rate higher than 80\% for GCN and other three models.",0
"While deep neural networks are generally effective, they are also vulnerable to small changes that can be exploited by adversaries. This creates a serious security risk and highlights the need for a thorough investigation of the resilience of deep learning models. With the advent of neural networks designed for graph-based data, it is imperative to study their robustness as well. Recent research has shown that even minor alterations to the graph structure or node features can significantly degrade model performance. This study takes a different approach by demonstrating that the presence of only a few ""bad-actor"" nodes in a graph can compromise a trained graph neural network. These nodes, which we refer to as ""anchor nodes,"" are identified using our proposed algorithm, GUA. Our empirical investigations reveal that anchor nodes tend to belong to the same class and that there is a trade-off between the number of anchor nodes and the success rate of the attack. In fact, for the Cora dataset, consisting of 2708 nodes, as few as six anchor nodes can result in an attack success rate higher than 80% for GCN and other models.",1
"Graph Neural Networks (GNNs) have been extensively used for mining graph-structured data with impressive performance. However, traditional GNNs suffer from over-smoothing, non-robustness and over-fitting problems. To solve these weaknesses, we design a novel GNN solution, namely Graph Attention Network with LSTM-based Path Reweighting (PR-GAT). PR-GAT can automatically aggregate multi-hop information, highlight important paths and filter out noises. In addition, we utilize random path sampling in PR-GAT for data augmentation. The augmented data is used for predicting the distribution of corresponding labels. Finally, we demonstrate that PR-GAT can mitigate the issues of over-smoothing, non-robustness and overfitting. We achieve state-of-the-art accuracy on 5 out of 7 datasets and competitive accuracy for other 2 datasets. The average accuracy of 7 datasets have been improved by 0.5\% than the best SOTA from literature.",0
"Graph Neural Networks (GNNs) have been used extensively to mine graph-structured data and have shown impressive performance. However, traditional GNNs face issues such as over-smoothing, non-robustness, and over-fitting. To address these shortcomings, we have developed a new GNN solution called Graph Attention Network with LSTM-based Path Reweighting (PR-GAT). PR-GAT automatically aggregates multi-hop information, highlights important paths, and filters out noise. Additionally, we use random path sampling in PR-GAT for data augmentation, which is used to predict the distribution of corresponding labels. Our results demonstrate that PR-GAT mitigates the issues of over-smoothing, non-robustness, and overfitting. We achieved state-of-the-art accuracy on 5 out of 7 datasets and competitive accuracy on the other 2 datasets. The average accuracy of 7 datasets has improved by 0.5% compared to the best SOTA reported in the literature.",1
"This paper presents a new approach for assembling graph neural networks based on framelet transforms. The latter provides a multi-scale representation for graph-structured data. We decompose an input graph into low-pass and high-pass frequencies coefficients for network training, which then defines a framelet-based graph convolution. The framelet decomposition naturally induces a graph pooling strategy by aggregating the graph feature into low-pass and high-pass spectra, which considers both the feature values and geometry of the graph data and conserves the total information. The graph neural networks with the proposed framelet convolution and pooling achieve state-of-the-art performance in many node and graph prediction tasks. Moreover, we propose shrinkage as a new activation for the framelet convolution, which thresholds high-frequency information at different scales. Compared to ReLU, shrinkage activation improves model performance on denoising and signal compression: noises in both node and structure can be significantly reduced by accurately cutting off the high-pass coefficients from framelet decomposition, and the signal can be compressed to less than half its original size with well-preserved prediction performance.",0
"A novel method for constructing graph neural networks utilizing framelet transforms is presented in this research paper. The approach uses multi-scale representation to decompose graphs into low-pass and high-pass frequencies coefficients for network training, leading to a framelet-based graph convolution. The framelet decomposition naturally induces a graph pooling strategy that aggregates graph features into low-pass and high-pass spectra, considering both feature values and graph data geometry while preserving total information. The proposed framelet convolution and pooling in graph neural networks yield state-of-the-art results in various node and graph prediction tasks. Additionally, the research proposes shrinkage activation as an alternative to ReLU activation in framelet convolution, which improves model performance in denoising and signal compression. The high-pass coefficients from framelet decomposition are accurately cut off, reducing noise in both node and structure while maintaining prediction performance. The signal can be compressed to less than half its original size while preserving prediction accuracy.",1
"The Transformer architecture has become a dominant choice in many domains, such as natural language processing and computer vision. Yet, it has not achieved competitive performance on popular leaderboards of graph-level prediction compared to mainstream GNN variants. Therefore, it remains a mystery how Transformers could perform well for graph representation learning. In this paper, we solve this mystery by presenting Graphormer, which is built upon the standard Transformer architecture, and could attain excellent results on a broad range of graph representation learning tasks, especially on the recent OGB Large-Scale Challenge. Our key insight to utilizing Transformer in the graph is the necessity of effectively encoding the structural information of a graph into the model. To this end, we propose several simple yet effective structural encoding methods to help Graphormer better model graph-structured data. Besides, we mathematically characterize the expressive power of Graphormer and exhibit that with our ways of encoding the structural information of graphs, many popular GNN variants could be covered as the special cases of Graphormer.",0
"Although the Transformer architecture is widely used in natural language processing and computer vision, it has not been able to perform as well as mainstream GNN variants in graph-level prediction leaderboards. This raises questions about how Transformers can effectively learn graph representations. Our paper introduces Graphormer, which is based on the standard Transformer architecture and achieves outstanding results on a variety of graph representation learning tasks, particularly the OGB Large-Scale Challenge. Our key insight is that encoding the structural information of a graph into the model is crucial for effectively utilizing the Transformer in graphs. Therefore, we propose several simple yet effective methods for encoding structural information to improve Graphormer's ability to model graph-structured data. Additionally, we mathematically characterize Graphormer's expressive power and demonstrate that our structural encoding methods cover many popular GNN variants as special cases of Graphormer.",1
"Graphs are versatile tools for representing structured data. As a result, a variety of machine learning methods have been studied for graph data analysis. Although many such learning methods depend on the measurement of differences between input graphs, defining an appropriate distance metric for graphs remains a controversial issue. Hence, we propose a supervised distance metric learning method for the graph classification problem. Our method, named interpretable graph metric learning (IGML), learns discriminative metrics in a subgraph-based feature space, which has a strong graph representation capability. By introducing a sparsity-inducing penalty on the weight of each subgraph, IGML can identify a small number of important subgraphs that can provide insight into the given classification task. Because our formulation has a large number of optimization variables, an efficient algorithm that uses pruning techniques based on safe screening and working set selection methods is also proposed. An important property of IGML is that solution optimality is guaranteed because the problem is formulated as a convex problem and our pruning strategies only discard unnecessary subgraphs. Furthermore, we show that IGML is also applicable to other structured data such as itemset and sequence data, and that it can incorporate vertex-label similarity by using a transportation-based subgraph feature. We empirically evaluate the computational efficiency and classification performance of IGML on several benchmark datasets and provide some illustrative examples of how IGML identifies important subgraphs from a given graph dataset.",0
"Graphs are a useful way to represent structured data and various machine learning methods have been developed for analyzing graph data. However, defining an appropriate distance metric for graphs is a controversial issue for many learning methods that rely on measuring differences between input graphs. To address this, we propose a supervised distance metric learning method called interpretable graph metric learning (IGML) for the graph classification problem. IGML learns discriminative metrics in a subgraph-based feature space that is highly effective for graph representation. By introducing a sparsity-inducing penalty on the weight of each subgraph, IGML can identify a small number of important subgraphs that offer valuable insights into the classification task. To efficiently solve the problem, our algorithm uses pruning techniques based on safe screening and working set selection methods. IGML is a convex problem, ensuring solution optimality, and our pruning strategies only discard unnecessary subgraphs. In addition, we demonstrate that IGML can be used for other structured data such as itemset and sequence data and can incorporate vertex-label similarity using a transportation-based subgraph feature. We evaluate the computational efficiency and classification performance of IGML on multiple benchmark datasets and provide examples of how IGML identifies important subgraphs from graph datasets.",1
"An outlier is an observation or a data point that is far from rest of the data points in a given dataset or we can be said that an outlier is away from the center of mass of observations. Presence of outliers can skew statistical measures and data distributions which can lead to misleading representation of the underlying data and relationships. It is seen that the removal of outliers from the training dataset before modeling can give better predictions. With the advancement of machine learning, the outlier detection models are also advancing at a good pace. The goal of this work is to highlight and compare some of the existing outlier detection techniques for the data scientists to use that information for outlier algorithm selection while building a machine learning model.",0
"An outlier refers to a data point or observation that is significantly distant from the other data points in a particular dataset. This means that the outlier is situated far from the center of mass of observations. When present, outliers can distort both statistical measures and data distributions, causing a misrepresentation of the underlying data and relationships. However, it has been observed that removing outliers from the training dataset before modeling can lead to improved predictions. With the development of machine learning techniques, outlier detection models are also advancing rapidly. Therefore, the aim of this research is to highlight and compare various existing outlier detection techniques. This way, data scientists can make informed decisions when selecting an outlier algorithm for machine learning model construction.",1
"The spatio-temporal graph learning is becoming an increasingly important object of graph study. Many application domains involve highly dynamic graphs where temporal information is crucial, e.g. traffic networks and financial transaction graphs. Despite the constant progress made on learning structured data, there is still a lack of effective means to extract dynamic complex features from spatio-temporal structures. Particularly, conventional models such as convolutional networks or recurrent neural networks are incapable of revealing the temporal patterns in short or long terms and exploring the spatial properties in local or global scope from spatio-temporal graphs simultaneously. To tackle this problem, we design a novel multi-scale architecture, Spatio-Temporal U-Net (ST-UNet), for graph-structured time series modeling. In this U-shaped network, a paired sampling operation is proposed in spacetime domain accordingly: the pooling (ST-Pool) coarsens the input graph in spatial from its deterministic partition while abstracts multi-resolution temporal dependencies through dilated recurrent skip connections; based on previous settings in the downsampling, the unpooling (ST-Unpool) restores the original structure of spatio-temporal graphs and resumes regular intervals within graph sequences. Experiments on spatio-temporal prediction tasks demonstrate that our model effectively captures comprehensive features in multiple scales and achieves substantial improvements over mainstream methods on several real-world datasets.",0
"The study of spatio-temporal graph learning is becoming increasingly important as many areas of application rely on dynamic graphs with temporal information, such as traffic networks and financial transaction graphs. Despite progress in learning structured data, there is still a lack of effective methods to extract complex features from spatio-temporal structures. Conventional models like convolutional networks or recurrent neural networks are unable to reveal temporal patterns and explore spatial properties simultaneously. To address this issue, we propose a multi-scale architecture, Spatio-Temporal U-Net (ST-UNet), for graph-structured time series modeling. Our U-shaped network employs paired sampling operations (ST-Pool and ST-Unpool) to coarsen and restore the original structure of spatio-temporal graphs while abstracting multi-resolution temporal dependencies through dilated recurrent skip connections. Our experiments on spatio-temporal prediction tasks show that our model effectively captures comprehensive features in multiple scales and outperforms mainstream methods on several real-world datasets.",1
"The Wasserstein distance provides a notion of dissimilarities between probability measures, which has recent applications in learning of structured data with varying size such as images and text documents. In this work, we analyze the $k$-nearest neighbor classifier ($k$-NN) under the Wasserstein distance and establish the universal consistency on families of distributions. Using previous known results on the consistency of the $k$-NN classifier on infinite dimensional metric spaces, it suffices to show that the families is a countable union of finite dimension sets. As a result, we show that the $k$-NN classifier is universally consistent on spaces of finitely supported measures, the space of Gaussian measures, and the space of measures with finite wavelet densities. In addition, we give a counterexample to show that the universal consistency does not hold on $\mathcal{W}_p((0,1))$.",0
"Recently, the Wasserstein distance has found applications in learning structured data of differing sizes, such as text documents and images, as it offers a way to compare the dissimilarities between probability measures. This study focuses on the $k$-nearest neighbor classifier ($k$-NN) under the Wasserstein distance and establishes its universal consistency on a range of distribution families. To do so, previous results on the consistency of the $k$-NN classifier in infinite dimensional metric spaces have been used, and it is sufficient to demonstrate that the families are a countable union of finite dimension sets. As a result, the $k$-NN classifier is found to be universally consistent on finitely supported measures, Gaussian measures, and measures with finite wavelet densities. However, a counterexample is provided to show that universal consistency is not upheld on $\mathcal{W}_p((0,1))$.",1
"Recent years have witnessed tremendous interest in deep learning on graph-structured data. Due to the high cost of collecting labeled graph-structured data, domain adaptation is important to supervised graph learning tasks with limited samples. However, current graph domain adaptation methods are generally adopted from traditional domain adaptation tasks, and the properties of graph-structured data are not well utilized. For example, the observed social networks on different platforms are controlled not only by the different crowd or communities but also by the domain-specific policies and the background noise. Based on these properties in graph-structured data, we first assume that the graph-structured data generation process is controlled by three independent types of latent variables, i.e., the semantic latent variables, the domain latent variables, and the random latent variables. Based on this assumption, we propose a disentanglement-based unsupervised domain adaptation method for the graph-structured data, which applies variational graph auto-encoders to recover these latent variables and disentangles them via three supervised learning modules. Extensive experimental results on two real-world datasets in the graph classification task reveal that our method not only significantly outperforms the traditional domain adaptation methods and the disentangled-based domain adaptation methods but also outperforms the state-of-the-art graph domain adaptation algorithms.",0
"In recent times, there has been a surge of interest in deep learning on graph-structured data. However, obtaining labeled graph-structured data is a costly affair, making domain adaptation a crucial aspect of supervised graph learning tasks with limited samples. Unfortunately, current graph domain adaptation methods tend to be derived from traditional domain adaptation tasks, with little regard for the unique properties of graph-structured data. For instance, social networks observed on different platforms are controlled not only by the varying crowd or communities but also by domain-specific policies and background noise. To address these issues, we propose a disentanglement-based unsupervised domain adaptation method for graph-structured data. We assume that the graph-structured data generation process is controlled by three independent types of latent variables, namely the semantic latent variables, domain latent variables, and random latent variables. Our method leverages variational graph auto-encoders to recover these latent variables and disentangles them via three supervised learning modules. Our extensive experiments on two real-world datasets in the graph classification task demonstrate that our method outperforms traditional domain adaptation methods, disentangled-based domain adaptation methods, and state-of-the-art graph domain adaptation algorithms.",1
"While the advent of Graph Neural Networks (GNNs) has greatly improved node and graph representation learning in many applications, the neighborhood aggregation scheme exposes additional vulnerabilities to adversaries seeking to extract node-level information about sensitive attributes. In this paper, we study the problem of protecting sensitive attributes by information obfuscation when learning with graph structured data. We propose a framework to locally filter out pre-determined sensitive attributes via adversarial training with the total variation and the Wasserstein distance. Our method creates a strong defense against inference attacks, while only suffering small loss in task performance. Theoretically, we analyze the effectiveness of our framework against a worst-case adversary, and characterize an inherent trade-off between maximizing predictive accuracy and minimizing information leakage. Experiments across multiple datasets from recommender systems, knowledge graphs and quantum chemistry demonstrate that the proposed approach provides a robust defense across various graph structures and tasks, while producing competitive GNN encoders for downstream tasks.",0
"Graph Neural Networks (GNNs) have significantly enhanced the learning of node and graph representations in many applications, but the neighborhood aggregation scheme has increased the vulnerability of sensitive attributes to adversarial attacks. This study focuses on safeguarding sensitive attributes through information obfuscation when using graph structured data. The proposed framework uses adversarial training with the total variation and the Wasserstein distance to locally filter out predetermined sensitive attributes. This method provides robust protection against inference attacks while only slightly reducing task performance. The framework's effectiveness against a worst-case adversary is analyzed, and a trade-off between maximizing predictive accuracy and minimizing information leakage is identified. Experiments across multiple datasets demonstrate the framework's robustness and competitive GNN encoders for downstream tasks.",1
"Link prediction is an important learning task for graph-structured data. In this paper, we propose a novel topological approach to characterize interactions between two nodes. Our topological feature, based on the extended persistent homology, encodes rich structural information regarding the multi-hop paths connecting nodes. Based on this feature, we propose a graph neural network method that outperforms state-of-the-arts on different benchmarks. As another contribution, we propose a novel algorithm to more efficiently compute the extended persistence diagrams for graphs. This algorithm can be generally applied to accelerate many other topological methods for graph learning tasks.",0
"The task of learning link prediction for graph-structured data is crucial. Our paper introduces an innovative approach to characterizing node interactions using topology. Our topological feature, which utilizes extended persistent homology, captures extensive structural information about multi-hop paths between nodes. We apply this feature to develop a graph neural network technique that surpasses current benchmarks. Additionally, we present a new algorithm that streamlines the computation of extended persistence diagrams for graphs, facilitating faster and more efficient topological methods for graph learning tasks.",1
"Deep neural networks have revolutionized many machine learning tasks in power systems, ranging from pattern recognition to signal processing. The data in these tasks is typically represented in Euclidean domains. Nevertheless, there is an increasing number of applications in power systems, where data are collected from non-Euclidean domains and represented as graph-structured data with high dimensional features and interdependency among nodes. The complexity of graph-structured data has brought significant challenges to the existing deep neural networks defined in Euclidean domains. Recently, many publications generalizing deep neural networks for graph-structured data in power systems have emerged. In this paper, a comprehensive overview of graph neural networks (GNNs) in power systems is proposed. Specifically, several classical paradigms of GNNs structures (e.g., graph convolutional networks) are summarized, and key applications in power systems, such as fault scenario application, time series prediction, power flow calculation, and data generation are reviewed in detail. Furthermore, main issues and some research trends about the applications of GNNs in power systems are discussed.",0
"The use of deep neural networks has drastically transformed various machine learning tasks within power systems, including signal processing and pattern recognition. Typically, these tasks involve data represented in Euclidean domains. However, as more data is collected from non-Euclidean domains in power systems, with graph-structured data featuring high dimensional features and interdependency among nodes, deep neural networks defined in Euclidean domains face significant challenges. To address this issue, numerous publications have recently emerged, generalizing deep neural networks for graph-structured data in power systems. This paper offers a comprehensive overview of graph neural networks (GNNs) in power systems. It summarizes several classical paradigms of GNNs structures, such as graph convolutional networks, and reviews key applications in power systems, including fault scenario application, time series prediction, power flow calculation, and data generation. Additionally, this paper discusses main issues and research trends regarding the use of GNNs in power systems.",1
"HyperGraph Convolutional Neural Networks (HGCNNs) have demonstrated their potential in modeling high-order relations preserved in graph structured data. However, most existing convolution filters are localized and determined by the pre-defined initial hypergraph topology, neglecting to explore implicit and long-ange relations in real-world data. In this paper, we propose the first learning-based method tailored for constructing adaptive hypergraph structure, termed HypERgrAph Laplacian aDaptor (HERALD), which serves as a generic plug-in-play module for improving the representational power of HGCNNs. Specifically, HERALD adaptively optimizes the adjacency relationship between hypernodes and hyperedges in an end-to-end manner and thus the task-aware hypergraph is learned. Furthermore, HERALD employs the self-attention mechanism to capture the non-local paired-nodes relation. Extensive experiments on various popular hypergraph datasets for node classification and graph classification tasks demonstrate that our approach obtains consistent and considerable performance enhancement, proving its effectiveness and generalization ability.",0
"The effectiveness of HyperGraph Convolutional Neural Networks (HGCNNs) in modeling high-order relations present in graph structured data has been established. However, the existing convolution filters are localized and depend on the pre-defined initial hypergraph topology. This limitation hampers the exploration of implicit and long-range relations in real-world data. In this research, we introduce HypERgrAph Laplacian aDaptor (HERALD), a learning-based method that constructs an adaptive hypergraph structure. HERALD is a plug-in-play module that enhances the representational power of HGCNNs. Our method optimizes the adjacency relationship between hypernodes and hyperedges in an end-to-end manner, allowing the task-aware hypergraph to be learned. HERALD also utilizes the self-attention mechanism to capture the non-local paired-nodes relation. Based on extensive experiments on various popular hypergraph datasets for node classification and graph classification tasks, our approach consistently and considerably enhances performance, demonstrating its effectiveness and generalization ability.",1
"Supervised machine learning has several drawbacks that make it difficult to use in many situations. Drawbacks include: heavy reliance on massive training data, limited generalizability and poor expressiveness of high-level semantics. Low-shot Learning attempts to address these drawbacks. Low-shot learning allows the model to obtain good predictive power with very little or no training data, where structured knowledge plays a key role as a high-level semantic representation of 0. This article will review the fundamental factors of low-shot learning technologies, with a focus on the operation of structured knowledge under different low-shot conditions. We also introduce other techniques relevant to low-shot learning. Finally, we point out the limitations of low-shot learning, the prospects and gaps of industrial applications, and future research directions.",0
"The use of supervised machine learning can be challenging in various situations due to its limitations. These limitations include the need for extensive training data, a restricted ability to generalize, and a low level of expressiveness in high-level semantics. Nonetheless, Low-shot Learning is a technique that aims to overcome these drawbacks by enabling models to make accurate predictions with little or no training data. This technique relies on structured knowledge as a representation of 0 high-level semantics. This article will explore the key factors of low-shot learning, with a particular focus on how structured knowledge operates under various low-shot conditions. Additionally, we will introduce other techniques relevant to low-shot learning. Finally, we will identify the limitations of low-shot learning, and discuss the potential for industrial applications, as well as future research directions.",1
"Graph Neural Networks (GNNs) have been widely applied to various fields due to their powerful representations of graph-structured data. Despite the success of GNNs, most existing GNNs are designed to learn node representations on the fixed and homogeneous graphs. The limitations especially become problematic when learning representations on a misspecified graph or a heterogeneous graph that consists of various types of nodes and edges. To address this limitations, we propose Graph Transformer Networks (GTNs) that are capable of generating new graph structures, which preclude noisy connections and include useful connections (e.g., meta-paths) for tasks, while learning effective node representations on the new graphs in an end-to-end fashion. We further propose enhanced version of GTNs, Fast Graph Transformer Networks (FastGTNs), that improve scalability of graph transformations. Compared to GTNs, FastGTNs are 230x faster and use 100x less memory while allowing the identical graph transformations as GTNs. In addition, we extend graph transformations to the semantic proximity of nodes allowing non-local operations beyond meta-paths. Extensive experiments on both homogeneous graphs and heterogeneous graphs show that GTNs and FastGTNs with non-local operations achieve the state-of-the-art performance for node classification tasks. The code is available: https://github.com/seongjunyun/Graph_Transformer_Networks",0
"Due to their exceptional ability to represent graph-structured data, Graph Neural Networks (GNNs) have found extensive application in various fields. However, existing GNNs are mostly designed to learn node representations on fixed and homogeneous graphs, which poses limitations when learning representations on misspecified or heterogeneous graphs composed of different types of nodes and edges. To overcome these limitations, we present Graph Transformer Networks (GTNs), which can generate new graph structures that exclude noisy connections and include useful connections (such as meta-paths) for tasks, while learning effective node representations on the new graphs end-to-end. Additionally, we propose an enhanced version of GTNs, Fast Graph Transformer Networks (FastGTNs), that improve scalability of graph transformations. Compared to GTNs, FastGTNs are 230 times faster and use 100 times less memory while allowing identical graph transformations. Furthermore, we extend graph transformations to the semantic proximity of nodes, enabling non-local operations beyond meta-paths. Through extensive experiments on both homogeneous and heterogeneous graphs, we demonstrate that GTNs and FastGTNs with non-local operations achieve state-of-the-art performance for node classification tasks. The code is available at https://github.com/seongjunyun/Graph_Transformer_Networks.",1
"Graph neural networks (GNNs) are one of the most popular approaches to using deep learning on graph-structured data, and they have shown state-of-the-art performances on a variety of tasks. However, according to a recent study, a careful choice of pooling functions, which are used for the aggregation or readout operation in GNNs, is crucial for enabling GNNs to extrapolate. Without the ideal combination of pooling functions, which varies across tasks, GNNs completely fail to generalize to out-of-distribution data, while the number of possible combinations grows exponentially with the number of layers. In this paper, we present GNP, a $L^p$ norm-like pooling function that is trainable end-to-end for any given task. Notably, GNP generalizes most of the widely-used pooling functions. We verify experimentally that simply replacing all pooling functions with GNP enables GNNs to extrapolate well on many node-level, graph-level, and set-related tasks; and GNP sometimes performs even better than optimal combinations of existing pooling functions.",0
"GNNs are a popular method for using deep learning on data structured as graphs, and they have achieved impressive results on various tasks. However, a recent study has shown that the choice of pooling functions, which are crucial for the aggregation or readout operation in GNNs, plays a significant role in enabling GNNs to extrapolate. Without the correct combination of pooling functions, which varies depending on the task, GNNs fail to generalize to out-of-distribution data, and the number of possible combinations increases exponentially with the number of layers. This paper introduces GNP, a trainable end-to-end $L^p$ norm-like pooling function for any given task that generalizes widely-used pooling functions. Our experiments demonstrate that replacing all pooling functions with GNP enables GNNs to extrapolate well on many node-level, graph-level, and set-related tasks, and sometimes even outperforms optimal combinations of existing pooling functions.",1
"HyperGraph Convolutional Neural Networks (HGCNNs) have demonstrated their potential in modeling high-order relations preserved in graph structured data. However, most existing convolution filters are localized and determined by the pre-defined initial hypergraph topology, neglecting to explore implicit and long-ange relations in real-world data. In this paper, we propose the first learning-based method tailored for constructing adaptive hypergraph structure, termed HypERgrAph Laplacian aDaptor (HERALD), which serves as a generic plug-in-play module for improving the representational power of HGCNNs. Specifically, HERALD adaptively optimizes the adjacency relationship between hypernodes and hyperedges in an end-to-end manner and thus the task-aware hypergraph is learned. Furthermore, HERALD employs the self-attention mechanism to capture the non-local paired-nodes relation. Extensive experiments on various popular hypergraph datasets for node classification and graph classification tasks demonstrate that our approach obtains consistent and considerable performance enhancement, proving its effectiveness and generalization ability.",0
"The potential of HyperGraph Convolutional Neural Networks (HGCNNs) in modeling high-order relations preserved in graph structured data has been demonstrated. However, the existing convolution filters are localized and determined by the pre-defined initial hypergraph topology, which neglects to explore implicit and long-range relations in real-world data. This paper proposes the first learning-based method, HERALD, for constructing adaptive hypergraph structure, which serves as a generic plug-in-play module to improve the representational power of HGCNNs. HERALD adaptively optimizes the adjacency relationship between hypernodes and hyperedges in an end-to-end manner, resulting in a task-aware hypergraph. Additionally, HERALD uses the self-attention mechanism to capture non-local paired-nodes relation. The effectiveness and generalization ability of our approach are proved through extensive experiments on various popular hypergraph datasets for node classification and graph classification tasks, where consistent and considerable performance enhancements are obtained.",1
"Graph is an usual representation of relational data, which are ubiquitous in manydomains such as molecules, biological and social networks. A popular approach to learningwith graph structured data is to make use of graph kernels, which measure the similaritybetween graphs and are plugged into a kernel machine such as a support vector machine.Weisfeiler-Lehman (WL) based graph kernels, which employ WL labeling scheme to extract subtree patterns and perform node embedding, are demonstrated to achieve great performance while being efficiently computable. However, one of the main drawbacks of ageneral kernel is the decoupling of kernel construction and learning process. For moleculargraphs, usual kernels such as WL subtree, based on substructures of the molecules, consider all available substructures having the same importance, which might not be suitable inpractice. In this paper, we propose a method to learn the weights of subtree patterns in the framework of WWL kernels, the state of the art method for graph classification task [14]. To overcome the computational issue on large scale data sets, we present an efficient learning algorithm and also derive a generalization gap bound to show its convergence. Finally, through experiments on synthetic and real-world data sets, we demonstrate the effectiveness of our proposed method for learning the weights of subtree patterns.",0
"A graph is a common way of representing relational data, which can be found in various domains like molecules, social networks, and biology. Graph kernels are a popular method for learning with graph structured data, as they measure the similarity between graphs and are used in kernel machines like support vector machines. The Weisfeiler-Lehman (WL) based graph kernels are efficient and achieve high performance by using WL labeling scheme to extract subtree patterns and perform node embedding. However, a general kernel has a disadvantage of separating kernel construction and learning process. The usual kernels like WL subtree for molecular graphs consider all substructures equally important, which may not be practical. This article presents a method to learn the weights of subtree patterns using WWL kernels, which are state-of-the-art for graph classification. To address the computational issue, an efficient learning algorithm is introduced, and a generalization gap bound is derived to demonstrate its convergence. Experiments on synthetic and real-world data sets prove the effectiveness of this proposed method for learning the weights of subtree patterns.",1
"Event forecasting is a challenging, yet important task, as 0s seek to constantly plan for the future. Existing automated forecasting studies rely mostly on structured data, such as time-series or event-based knowledge graphs, to help predict future events. In this work, we aim to formulate a task, construct a dataset, and provide benchmarks for developing methods for event forecasting with large volumes of unstructured text data. To simulate the forecasting scenario on temporal news documents, we formulate the problem as a restricted-domain, multiple-choice, question-answering (QA) task. Unlike existing QA tasks, our task limits accessible information, and thus a model has to make a forecasting judgement. To showcase the usefulness of this task formulation, we introduce ForecastQA, a question-answering dataset consisting of 10,392 event forecasting questions, which have been collected and verified via crowdsourcing efforts. We present our experiments on ForecastQA using BERT-based models and find that our best model achieves 60.1% accuracy on the dataset, which still lags behind 0 performance by about 19%. We hope ForecastQA will support future research efforts in bridging this gap.",0
"Predicting future events is a difficult but significant task for 0s who wish to plan ahead. Past automated forecasting studies have primarily used structured data, like time-series or event-based knowledge graphs, to help anticipate future occurrences. However, in this study, we strive to establish a task, create a dataset, and offer benchmarks for devising techniques for event prediction using vast amounts of unstructured text data. To simulate the forecasting scenario on temporal news documents, we present the problem as a limited-domain, multiple-choice, question-answering (QA) task. Unlike previous QA tasks, our task restricts accessible information, requiring a model to make a forecasting decision. To demonstrate the effectiveness of this task formulation, we present ForecastQA, a QA dataset containing 10,392 event forecasting questions, which have been gathered and verified through crowdsourcing. We perform experiments on ForecastQA using BERT-based models and discover that our best model attains 60.1% accuracy on the dataset, which still falls short of 0 performance by about 19%. We hope ForecastQA will facilitate future research efforts in narrowing this gap.",1
"Graph Neural Network (GNN) has been demonstrated its effectiveness in dealing with non-Euclidean structural data. Both spatial-based and spectral-based GNNs are relying on adjacency matrix to guide message passing among neighbors during feature aggregation. Recent works have mainly focused on powerful message passing modules, however, in this paper, we show that none of the message passing modules is necessary. Instead, we propose a pure multilayer-perceptron-based framework, Graph-MLP with the supervision signal leveraging graph structure, which is sufficient for learning discriminative node representation. In model-level, Graph-MLP only includes multi-layer perceptrons, activation function, and layer normalization. In the loss level, we design a neighboring contrastive (NContrast) loss to bridge the gap between GNNs and MLPs by utilizing the adjacency information implicitly. This design allows our model to be lighter and more robust when facing large-scale graph data and corrupted adjacency information. Extensive experiments prove that even without adjacency information in testing phase, our framework can still reach comparable and even superior performance against the state-of-the-art models in the graph node classification task.",0
"The effectiveness of Graph Neural Network (GNN) in dealing with non-Euclidean structural data has been demonstrated. Both spatial-based and spectral-based GNNs use adjacency matrix to guide message passing among neighbors during feature aggregation. While recent works have focused on powerful message passing modules, this paper proposes a pure multilayer-perceptron-based framework, Graph-MLP, which uses graph structure as a supervision signal to learn discriminative node representation. The Graph-MLP model only includes multilayer perceptrons, activation function, and layer normalization. The loss level utilizes a neighboring contrastive (NContrast) loss to bridge the gap between GNNs and MLPs by implicitly using adjacency information. Our framework is lighter and more robust when facing large-scale graph data and corrupted adjacency information. Extensive experiments show that even without adjacency information in testing phase, Graph-MLP still achieves comparable and even superior performance against state-of-the-art models in the graph node classification task.",1
"Various classes of Graph Neural Networks (GNN) have been proposed and shown to be successful in a wide range of applications with graph structured data. In this paper, we propose a theoretical framework able to compare the expressive power of these GNN architectures. The current universality theorems only apply to intractable classes of GNNs. Here, we prove the first approximation guarantees for practical GNNs, paving the way for a better understanding of their generalization. Our theoretical results are proved for invariant GNNs computing a graph embedding (permutation of the nodes of the input graph does not affect the output) and equivariant GNNs computing an embedding of the nodes (permutation of the input permutes the output). We show that Folklore Graph Neural Networks (FGNN), which are tensor based GNNs augmented with matrix multiplication are the most expressive architectures proposed so far for a given tensor order. We illustrate our results on the Quadratic Assignment Problem (a NP-Hard combinatorial problem) by showing that FGNNs are able to learn how to solve the problem, leading to much better average performances than existing algorithms (based on spectral, SDP or other GNNs architectures). On a practical side, we also implement masked tensors to handle batches of graphs of varying sizes.",0
"A broad range of applications utilizing graph structured data have demonstrated the success of Graph Neural Networks (GNN) in various classes. This paper presents a theoretical framework capable of comparing the expressive power of these GNN architectures. Although current universality theorems only apply to intractable classes of GNNs, we establish the first approximation guarantees for practical GNNs, facilitating a better comprehension of their generalization. Our theoretical outcomes pertain to two types of GNNs: invariant GNNs computing a graph embedding and equivariant GNNs computing an embedding of the nodes. We demonstrate that Folklore Graph Neural Networks (FGNN), tensor-based GNNs augmented with matrix multiplication, are the most expressive architectures proposed thus far for a given tensor order. We illustrate our findings using the Quadratic Assignment Problem, a challenging combinatorial problem, by demonstrating that FGNNs can learn to solve the problem, resulting in significantly better average performances than existing algorithms based on spectral, SDP, or other GNNs architectures. On a practical level, we employ masked tensors to handle batches of graphs of varying sizes.",1
"This paper presents Gem, a model-agnostic approach for providing interpretable explanations for any GNNs on various graph learning tasks. Specifically, we formulate the problem of providing explanations for the decisions of GNNs as a causal learning task. Then we train a causal explanation model equipped with a loss function based on Granger causality. Different from existing explainers for GNNs, Gem explains GNNs on graph-structured data from a causal perspective. It has better generalization ability as it has no requirements on the internal structure of the GNNs or prior knowledge on the graph learning tasks. In addition, Gem, once trained, can be used to explain the target GNN very quickly. Our theoretical analysis shows that several recent explainers fall into a unified framework of additive feature attribution methods. Experimental results on synthetic and real-world datasets show that Gem achieves a relative increase of the explanation accuracy by up to $30\%$ and speeds up the explanation process by up to $110\times$ as compared to its state-of-the-art alternatives.",0
"Gem is a novel approach that provides interpretable explanations for any GNNs on various graph learning tasks. The paper formulates the problem of explaining the decisions of GNNs as a causal learning task and trains a causal explanation model with a loss function based on Granger causality. Unlike existing explainers for GNNs, Gem explains GNNs on graph-structured data from a causal perspective, which enhances its generalization ability with no requirements on the internal structure of the GNNs or prior knowledge on the graph learning tasks. Additionally, Gem can be employed quickly once trained to explain the target GNN. The theoretical analysis reveals that several recent explainers belong to a unified framework of additive feature attribution methods. The experimental results on synthetic and real-world datasets demonstrate that Gem improves the explanation accuracy by up to $30\%$ and accelerates the explanation process by up to $110\times$ when compared to its state-of-the-art alternatives.",1
"Learning the representation of data with hierarchical structures in the hyperbolic space attracts increasing attention in recent years. Due to the constant negative curvature, the hyperbolic space resembles tree metrics and captures the tree-like properties naturally, which enables the hyperbolic embeddings to improve over traditional Euclidean models. However, many real-world hierarchically structured data such as taxonomies and multitree networks have varying local structures and they are not trees, thus they do not ubiquitously match the constant curvature property of the hyperbolic space. To address this limitation of hyperbolic embeddings, we explore the complex hyperbolic space, which has the variable negative curvature, for representation learning. Specifically, we propose to learn the embeddings of hierarchically structured data in the unit ball model of the complex hyperbolic space. The unit ball model based embeddings have a more powerful representation capacity to capture a variety of hierarchical structures. Through experiments on synthetic and real-world data, we show that our approach improves over the hyperbolic embedding models significantly.",0
"In recent years, there has been growing interest in learning the representation of data with hierarchical structures in the hyperbolic space. This is because the hyperbolic space, with its constant negative curvature, resembles tree metrics and naturally captures tree-like properties. As a result, hyperbolic embeddings outperform traditional Euclidean models. However, real-world hierarchically structured data, such as taxonomies and multitree networks, have varying local structures that do not always match the constant curvature property of the hyperbolic space. To overcome this limitation, we investigate the complex hyperbolic space, which has variable negative curvature for representation learning. Our proposal is to learn the embeddings of hierarchically structured data in the unit ball model of the complex hyperbolic space, which has a more powerful representation capacity for capturing a diverse range of hierarchical structures. Through experiments on both synthetic and real-world data, we demonstrate that our approach significantly outperforms hyperbolic embedding models.",1
"Imbalanced classification on graphs is ubiquitous yet challenging in many real-world applications, such as fraudulent node detection. Recently, graph neural networks (GNNs) have shown promising performance on many network analysis tasks. However, most existing GNNs have almost exclusively focused on the balanced networks, and would get unappealing performance on the imbalanced networks. To bridge this gap, in this paper, we present a generative adversarial graph network model, called ImGAGN to address the imbalanced classification problem on graphs. It introduces a novel generator for graph structure data, named GraphGenerator, which can simulate both the minority class nodes' attribute distribution and network topological structure distribution by generating a set of synthetic minority nodes such that the number of nodes in different classes can be balanced. Then a graph convolutional network (GCN) discriminator is trained to discriminate between real nodes and fake (i.e., generated) nodes, and also between minority nodes and majority nodes on the synthetic balanced network. To validate the effectiveness of the proposed method, extensive experiments are conducted on four real-world imbalanced network datasets. Experimental results demonstrate that the proposed method ImGAGN outperforms state-of-the-art algorithms for semi-supervised imbalanced node classification task.",0
"The problem of imbalanced classification on graphs is a common challenge in various real-world applications, including fraudulent node detection. Graph neural networks (GNNs) have shown promising results in network analysis tasks. However, most existing GNNs have primarily focused on balanced networks and struggle to perform well on imbalanced ones. To address this gap, we propose a generative adversarial graph network model, ImGAGN, in this paper. ImGAGN introduces a GraphGenerator, which generates synthetic minority nodes that simulate both the minority class nodes' attribute distribution and network topological structure distribution. This allows for the balancing of the number of nodes in different classes. A graph convolutional network (GCN) discriminator is also trained to differentiate between real and generated nodes and between minority and majority nodes on the balanced network. We conduct extensive experiments on four real-world imbalanced network datasets to validate the effectiveness of the proposed method. The results show that ImGAGN outperforms state-of-the-art algorithms for the semi-supervised imbalanced node classification task.",1
"Graph Neural Networks (GNNs) are the first choice methods for graph machine learning problems thanks to their ability to learn state-of-the-art level representations from graph-structured data. However, centralizing a massive amount of real-world graph data for GNN training is prohibitive due to user-side privacy concerns, regulation restrictions, and commercial competition. Federated Learning is the de-facto standard for collaborative training of machine learning models over many distributed edge devices without the need for centralization. Nevertheless, training graph neural networks in a federated setting is vaguely defined and brings statistical and systems challenges. This work proposes SpreadGNN, a novel multi-task federated training framework capable of operating in the presence of partial labels and absence of a central server for the first time in the literature. SpreadGNN extends federated multi-task learning to realistic serverless settings for GNNs, and utilizes a novel optimization algorithm with a convergence guarantee, Decentralized Periodic Averaging SGD (DPA-SGD), to solve decentralized multi-task learning problems. We empirically demonstrate the efficacy of our framework on a variety of non-I.I.D. distributed graph-level molecular property prediction datasets with partial labels. Our results show that SpreadGNN outperforms GNN models trained over a central server-dependent federated learning system, even in constrained topologies. The source code is publicly available at https://github.com/FedML-AI/SpreadGNN",0
"Due to concerns regarding user privacy, regulations, and commercial competition, it is challenging to centralize a significant amount of real-world graph data for Graph Neural Network (GNN) training. Therefore, Federated Learning has become the preferred method for collaborative training of machine learning models on distributed edge devices without centralization. However, training GNNs in a federated setting presents statistical and system challenges that have not been thoroughly defined. This paper introduces SpreadGNN, a novel multi-task federated training framework that is capable of training GNNs in a serverless setting with partial labels, and without a central server. SpreadGNN extends federated multi-task learning to realistic serverless settings for GNNs, and uses the Decentralized Periodic Averaging SGD (DPA-SGD) optimization algorithm, which guarantees convergence, to solve decentralized multi-task learning problems. Our empirical results demonstrate the effectiveness of SpreadGNN on various non-I.I.D. distributed graph-level molecular property prediction datasets with partial labels. SpreadGNN outperforms GNN models trained on a central server-dependent federated learning system, even in constrained topologies. The source code for SpreadGNN is publicly available at https://github.com/FedML-AI/SpreadGNN.",1
"Graph Neural Networks (GNNs) are widely used deep learning models that learn meaningful representations from graph-structured data. Due to the finite nature of the underlying recurrent structure, current GNN methods may struggle to capture long-range dependencies in underlying graphs. To overcome this difficulty, we propose a graph learning framework, called Implicit Graph Neural Networks (IGNN), where predictions are based on the solution of a fixed-point equilibrium equation involving implicitly defined ""state"" vectors. We use the Perron-Frobenius theory to derive sufficient conditions that ensure well-posedness of the framework. Leveraging implicit differentiation, we derive a tractable projected gradient descent method to train the framework. Experiments on a comprehensive range of tasks show that IGNNs consistently capture long-range dependencies and outperform the state-of-the-art GNN models.",0
"Graph Neural Networks (GNNs) are extensively utilized deep learning models that acquire significant representations from data structured as graphs. However, current GNN techniques may encounter difficulties in capturing long-range dependencies in the underlying graphs due to the limited nature of the recurrent structure. To address this issue, we propose Implicit Graph Neural Networks (IGNN), a graph learning framework that utilizes fixed-point equilibrium equations involving ""state"" vectors to make predictions. We employ the Perron-Frobenius theory to ensure the framework's well-posedness and develop a feasible projected gradient descent method utilizing implicit differentiation to train the model. Our experiments demonstrate that IGNNs consistently capture long-range dependencies and outperform the state-of-the-art GNN models across a range of tasks.",1
"Graph neural networks (GNNs) have been successfully employed in a myriad of applications involving graph-structured data. Theoretical findings establish that GNNs use nonlinear activation functions to create low-eigenvalue frequency content that can be processed in a stable manner by subsequent graph convolutional filters. However, the exact shape of the frequency content created by nonlinear functions is not known, and thus, it cannot be learned nor controlled. In this work, node-variant graph filters (NVGFs) are shown to be capable of creating frequency content and are thus used in lieu of nonlinear activation functions. This results in a novel GNN architecture that, although linear, is capable of creating frequency content as well. Furthermore, this new frequency content can be either designed or learned from data. In this way, the role of frequency creation is separated from the nonlinear nature of traditional GNNs. Extensive simulations are carried out to differentiate the contributions of frequency creation from those of the nonlinearity.",0
"GNNs have been successful in various applications involving graph data. Research indicates that GNNs generate low-eigenvalue frequency content using nonlinear activation functions, which can be processed stably by graph convolutional filters. However, the shape of this frequency content is unknown and cannot be controlled or learned. This study uses node-variant graph filters (NVGFs) to generate frequency content and replace nonlinear activation functions. Although it is linear, this new GNN architecture can produce frequency content that can be either designed or learned from data. This approach separates the frequency creation from the nonlinearity of traditional GNNs. The study conducts extensive simulations to distinguish between the contributions of frequency creation and nonlinearity.",1
"Networks are ubiquitous in the real world. Link prediction, as one of the key problems for network-structured data, aims to predict whether there exists a link between two nodes. The traditional approaches are based on the explicit similarity computation between the compact node representation by embedding each node into a low-dimensional space. In order to efficiently handle the intensive similarity computation in link prediction, the hashing technique has been successfully used to produce the node representation in the Hamming space. However, the hashing-based link prediction algorithms face accuracy loss from the randomized hashing techniques or inefficiency from the learning to hash techniques in the embedding process. Currently, the Graph Neural Network (GNN) framework has been widely applied to the graph-related tasks in an end-to-end manner, but it commonly requires substantial computational resources and memory costs due to massive parameter learning, which makes the GNN-based algorithms impractical without the help of a powerful workhorse. In this paper, we propose a simple and effective model called #GNN, which balances the trade-off between accuracy and efficiency. #GNN is able to efficiently acquire node representation in the Hamming space for link prediction by exploiting the randomized hashing technique to implement message passing and capture high-order proximity in the GNN framework. Furthermore, we characterize the discriminative power of #GNN in probability. The extensive experimental results demonstrate that the proposed #GNN algorithm achieves accuracy comparable to the learning-based algorithms and outperforms the randomized algorithm, while running significantly faster than the learning-based algorithms. Also, the proposed algorithm shows excellent scalability on a large-scale network with the limited resources.",0
"The presence of networks is widespread in reality. One of the main challenges for network-structured data is link prediction, which aims to determine whether a connection exists between two nodes. The traditional method involves calculating explicit similarity between compact node representations created by embedding each node into a low-dimensional space. Hashing has been used successfully to handle the intensive similarity computation required for link prediction by producing node representations in the Hamming space. However, hashing-based link prediction algorithms can experience accuracy loss due to randomized hashing techniques or inefficiency in the embedding process from learning to hash techniques. Although the Graph Neural Network (GNN) framework has been applied to graph-related tasks in an end-to-end manner, it often requires significant computational resources and memory costs due to massive parameter learning, making it impractical without a powerful computer. This study proposes a simple and effective model called #GNN that balances accuracy and efficiency. #GNN uses the randomized hashing technique to implement message passing and capture high-order proximity in the GNN framework, allowing it to efficiently acquire node representation in the Hamming space for link prediction. Additionally, the discriminative power of #GNN is characterized in probability. The extensive experimental results show that #GNN achieves accuracy comparable to learning-based algorithms and outperforms randomized algorithms while running significantly faster than learning-based algorithms. Furthermore, the proposed algorithm exhibits excellent scalability on large-scale networks with limited resources.",1
"Estimating the amount of electricity that can be produced by rooftop photovoltaic systems is a time-consuming process that requires on-site measurements, a difficult task to achieve on a large scale. In this paper, we present an approach to estimate the solar potential of rooftops based on their location and architectural characteristics, as well as the amount of solar radiation they receive annually. Our technique uses computer vision to achieve semantic segmentation of roof sections and roof objects on the one hand, and a machine learning model based on structured building features to predict roof pitch on the other hand. We then compute the azimuth and maximum number of solar panels that can be installed on a rooftop with geometric approaches. Finally, we compute precise shading masks and combine them with solar irradiation data that enables us to estimate the yearly solar potential of a rooftop.",0
"The process of determining the electricity output of rooftop photovoltaic systems is a lengthy task that involves on-site measurements, which is challenging to accomplish on a large scale. This article proposes a novel approach to evaluate the solar potential of rooftops by considering their location, architectural features, and the amount of solar radiation they receive annually. Our method employs computer vision to segment roof sections and objects and a structured building features-based machine learning model to predict roof pitch. We use geometric methods to calculate the azimuth and maximum number of solar panels that can be installed on a rooftop, followed by the computation of precise shading masks and the integration of solar irradiation data to estimate the yearly solar potential of a rooftop.",1
"Functional connectivity (FC) between regions of the brain can be assessed by the degree of temporal correlation measured with functional neuroimaging modalities. Based on the fact that these connectivities build a network, graph-based approaches for analyzing the brain connectome have provided insights into the functions of the 0 brain. The development of graph neural networks (GNNs) capable of learning representation from graph structured data has led to increased interest in learning the graph representation of the brain connectome. Although recent attempts to apply GNN to the FC network have shown promising results, there is still a common limitation that they usually do not incorporate the dynamic characteristics of the FC network which fluctuates over time. In addition, a few studies that have attempted to use dynamic FC as an input for the GNN reported a reduction in performance compared to static FC methods, and did not provide temporal explainability. Here, we propose STAGIN, a method for learning dynamic graph representation of the brain connectome with spatio-temporal attention. Specifically, a temporal sequence of brain graphs is input to the STAGIN to obtain the dynamic graph representation, while novel READOUT functions and the Transformer encoder provide spatial and temporal explainability with attention, respectively. Experiments on the HCP-Rest and the HCP-Task datasets demonstrate exceptional performance of our proposed method. Analysis of the spatio-temporal attention also provide concurrent interpretation with the neuroscientific knowledge, which further validates our method. Code is available at https://github.com/egyptdj/stagin",0
"Measuring the functional connectivity (FC) between different regions of the brain involves assessing the degree of temporal correlation using functional neuroimaging modalities. These connectivities form a network, and graph-based approaches have been used to analyze the brain connectome and gain insights into the functions of the 0 brain. Graph neural networks (GNNs) have been developed to learn the graph representation of the brain connectome, but they often do not incorporate the dynamic characteristics of the FC network or provide temporal explainability. To address these limitations, we propose STAGIN, a method for learning the dynamic graph representation of the brain connectome with spatio-temporal attention. STAGIN uses a temporal sequence of brain graphs as input and utilizes novel READOUT functions and a Transformer encoder to provide spatial and temporal explainability with attention. Our experiments on the HCP-Rest and HCP-Task datasets show exceptional performance, and the spatio-temporal attention analysis provides concurrent interpretation with neuroscientific knowledge. Our code is available at https://github.com/egyptdj/stagin.",1
"Kernel methods on discrete domains have shown great promise for many challenging data types, for instance, biological sequence data and molecular structure data. Scalable kernel methods like Support Vector Machines may offer good predictive performances but do not intrinsically provide uncertainty estimates. In contrast, probabilistic kernel methods like Gaussian Processes offer uncertainty estimates in addition to good predictive performance but fall short in terms of scalability. While the scalability of Gaussian processes can be improved using sparse inducing point approximations, the selection of these inducing points remains challenging. We explore different techniques for selecting inducing points on discrete domains, including greedy selection, determinantal point processes, and simulated annealing. We find that simulated annealing, which can select inducing points that are not in the training set, can perform competitively with support vector machines and full Gaussian processes on synthetic data, as well as on challenging real-world DNA sequence data.",0
"Kernel methods applied to discrete domains have demonstrated significant potential in handling various complex types of data, such as biological sequence and molecular structure data. Although scalable kernel methods like Support Vector Machines offer favorable predictive performance, they do not inherently provide uncertainty estimates. In contrast, probabilistic kernel methods like Gaussian Processes provide both uncertainty estimates and good predictive performance, but they lack scalability. However, sparse inducing point approximations can enhance the scalability of Gaussian processes, albeit selecting these inducing points proves to be challenging. We investigate different techniques for selecting inducing points on discrete domains, which include greedy selection, determinantal point processes, and simulated annealing. Our findings show that simulated annealing, which can select inducing points that are not in the training set, can perform comparably to support vector machines and full Gaussian processes on synthetic data and challenging real-world DNA sequence data.",1
"This paper presents Sparse Tensor Classifier (STC), a supervised classification algorithm for categorical data inspired by the notion of superposition of states in quantum physics. By regarding an observation as a superposition of features, we introduce the concept of wave-particle duality in machine learning and propose a generalized framework that unifies the classical and the quantum probability. We show that STC possesses a wide range of desirable properties not available in most other machine learning methods but it is at the same time exceptionally easy to comprehend and use. Empirical evaluation of STC on structured data and text classification demonstrates that our methodology achieves state-of-the-art performances compared to both standard classifiers and deep learning, at the additional benefit of requiring minimal data pre-processing and hyper-parameter tuning. Moreover, STC provides a native explanation of its predictions both for single instances and for each target label globally.",0
"In this paper, we introduce the Sparse Tensor Classifier (STC), which is a categorical data classification algorithm based on the concept of superposition of states in quantum physics. By considering observations as a combination of features, we propose a machine learning framework that combines classical and quantum probability, and we refer to this as wave-particle duality. STC offers several advantages that are not available in most other machine learning approaches, yet it is straightforward to understand and use. We conducted an empirical evaluation of STC on structured data and text classification, and the results indicate that our method outperforms both standard classifiers and deep learning algorithms while requiring minimal data pre-processing and hyper-parameter tuning. Additionally, STC provides a natural explanation of its predictions for single instances and each target label globally.",1
"Over the last few years, we have seen increasing data generated from non-Euclidean domains, which are usually represented as graphs with complex relationships, and Graph Neural Networks (GNN) have gained a high interest because of their potential in processing graph-structured data. In particular, there is a strong interest in exploring the possibilities in performing convolution on graphs using an extension of the GNN architecture, generally referred to as Graph Convolutional Neural Networks (GCNN). Convolution on graphs has been achieved mainly in two forms: spectral and spatial convolutions. Due to the higher flexibility in exploring and exploiting the graph structure of data, recently, there is an increasing interest in investigating the possibilities that the spatial approach can offer. The idea of finding a way to adapt the network behaviour to the inputs they process to maximize the total performances has aroused much interest in the neural networks literature over the years. This paper presents a novel method to adapt the behaviour of a GCNN to the input proposing two ways to perform spatial convolution on graphs using input-based filters which are dynamically generated. Our model also investigates the problem of discovering and refining relations among nodes. The experimental assessment confirms the capabilities of the proposed approach, which achieves satisfying results using simple architectures with a low number of filters.",0
"In recent years, there has been an increase in data generated from non-Euclidean domains, often represented as complex graphs with intricate relationships. As a result, Graph Neural Networks (GNN) have become popular due to their potential in processing graph-structured data. One area of interest is exploring the possibilities of performing convolution on graphs using Graph Convolutional Neural Networks (GCNN). Convolution on graphs has been achieved through spectral and spatial convolutions, with a growing interest in the latter due to its flexibility in exploiting the graph structure of data. Adapting network behavior to input for improved performance has been a topic of interest in neural network literature. This paper proposes a novel method for adapting the behavior of a GCNN to the input by dynamically generating input-based filters for spatial convolution on graphs. The model also investigates the problem of discovering and refining relations among nodes. The proposed approach achieves satisfactory results using simple architectures with a low number of filters as confirmed by experimental assessment.",1
"Graph convolutional networks (GCNs) have achieved great success in dealing with data of non-Euclidean structures. Their success directly attributes to fitting graph structures effectively to data such as in social media and knowledge databases. For image processing applications, the use of graph structures and GCNs have not been fully explored. In this paper, we propose a novel encoder-decoder network with added graph convolutions by converting feature maps to vertexes of a pre-generated graph to synthetically construct graph-structured data. By doing this, we inexplicitly apply graph Laplacian regularization to the feature maps, making them more structured. The experiments show that it significantly boosts performance for image restoration tasks, including deblurring and super-resolution. We believe it opens up opportunities for GCN-based approaches in more applications.",0
"The success of graph convolutional networks (GCNs) in managing non-Euclidean data structures is well-documented. This success is due to their ability to effectively fit graph structures to data, particularly in social media and knowledge databases. However, the use of GCNs and graph structures in image processing applications has not been fully explored. To address this gap, we propose a new encoder-decoder network that includes graph convolutions. We do this by converting feature maps to vertexes of a pre-generated graph to create synthetic graph-structured data. This approach applies graph Laplacian regularization to the feature maps implicitly, resulting in greater structure. Our experiments demonstrate that this method significantly improves performance in image restoration tasks, such as deblurring and super-resolution. We anticipate that this approach will expand the scope of GCN-based techniques to other applications.",1
"Traditional approaches for data anonymization consider relational data and textual data independently. We propose rx-anon, an anonymization approach for heterogeneous semi-structured documents composed of relational and textual attributes. We map sensitive terms extracted from the text to the structured data. This allows us to use concepts like k-anonymity to generate a joined, privacy-preserved version of the heterogeneous data input. We introduce the concept of redundant sensitive information to consistently anonymize the heterogeneous data. To control the influence of anonymization over unstructured textual data versus structured data attributes, we introduce a modified, parameterized Mondrian algorithm. The parameter $\lambda$ allows to give different weight on the relational and textual attributes during the anonymization process. We evaluate our approach with two real-world datasets using a Normalized Certainty Penalty score, adapted to the problem of jointly anonymizing relational and textual data. The results show that our approach is capable of reducing information loss by using the tuning parameter to control the Mondrian partitioning while guaranteeing k-anonymity for relational attributes as well as for sensitive terms. As rx-anon is a framework approach, it can be reused and extended by other anonymization algorithms, privacy models, and textual similarity metrics.",0
"Conventional methods for anonymizing data treat relational and textual data separately. Our proposed approach, rx-anon, addresses the anonymization of heterogeneous semi-structured documents that contain both relational and textual attributes. By mapping sensitive terms extracted from the text to the structured data, we can use concepts like k-anonymity to create a privacy-preserved version of the combined data input. To ensure consistent anonymization of the heterogeneous data, we introduce the concept of redundant sensitive information. To control the impact of anonymization on unstructured textual data versus structured data attributes, we introduce a modified, parameterized Mondrian algorithm. The parameter $\lambda$ can be used to assign different weights to the relational and textual attributes during the anonymization process. We evaluated our approach using two real-world datasets and a Normalized Certainty Penalty score, which was adapted to jointly anonymize relational and textual data. The results demonstrate that our approach can minimize information loss by adjusting the Mondrian partitioning while ensuring k-anonymity for relational attributes and sensitive terms. As rx-anon is a framework approach, it can be extended and reused by other anonymization algorithms, privacy models, and textual similarity metrics.",1
"We propose an approach to learning with graph-structured data in the problem domain of graph classification. In particular, we present a novel type of readout operation to aggregate node features into a graph-level representation. To this end, we leverage persistent homology computed via a real-valued, learnable, filter function. We establish the theoretical foundation for differentiating through the persistent homology computation. Empirically, we show that this type of readout operation compares favorably to previous techniques, especially when the graph connectivity structure is informative for the learning problem.",0
"Our proposal involves a learning approach for graph classification that uses graph-structured data. We introduce a unique readout operation that aggregates node features into a representation at the graph level. This is accomplished through the use of a real-valued and learnable filter function that computes persistent homology. We establish the ability to differentiate through the persistent homology computation. Our empirical findings demonstrate that this readout operation is superior to previous methods, particularly when the learning problem is informed by the graph's connectivity structure.",1
"We study the multiple manifold problem, a binary classification task modeled on applications in machine vision, in which a deep fully-connected neural network is trained to separate two low-dimensional submanifolds of the unit sphere. We provide an analysis of the one-dimensional case, proving for a simple manifold configuration that when the network depth $L$ is large relative to certain geometric and statistical properties of the data, the network width $n$ grows as a sufficiently large polynomial in $L$, and the number of i.i.d. samples from the manifolds is polynomial in $L$, randomly-initialized gradient descent rapidly learns to classify the two manifolds perfectly with high probability. Our analysis demonstrates concrete benefits of depth and width in the context of a practically-motivated model problem: the depth acts as a fitting resource, with larger depths corresponding to smoother networks that can more readily separate the class manifolds, and the width acts as a statistical resource, enabling concentration of the randomly-initialized network and its gradients. The argument centers around the neural tangent kernel and its role in the nonasymptotic analysis of training overparameterized neural networks; to this literature, we contribute essentially optimal rates of concentration for the neural tangent kernel of deep fully-connected networks, requiring width $n \gtrsim L\,\mathrm{poly}(d_0)$ to achieve uniform concentration of the initial kernel over a $d_0$-dimensional submanifold of the unit sphere $\mathbb{S}^{n_0-1}$, and a nonasymptotic framework for establishing generalization of networks trained in the NTK regime with structured data. The proof makes heavy use of martingale concentration to optimally treat statistical dependencies across layers of the initial random network. This approach should be of use in establishing similar results for other network architectures.",0
"In this study, we examine the multiple manifold problem, which involves a binary classification task used in machine vision. We use a deep fully-connected neural network to separate two low-dimensional submanifolds of the unit sphere. Specifically, we focus on the one-dimensional case and prove that if the network depth $L$ is large relative to certain geometric and statistical properties of the data, the network width $n$ grows as a sufficiently large polynomial in $L$, and the number of i.i.d. samples from the manifolds is polynomial in $L$, then randomly-initialized gradient descent can rapidly learn to classify the two manifolds perfectly with high probability. Our analysis demonstrates the importance of depth and width in this model problem, with depth acting as a fitting resource and width acting as a statistical resource. We use the neural tangent kernel to analyze the nonasymptotic training of overparameterized neural networks, and we contribute essentially optimal rates of concentration for the neural tangent kernel of deep fully-connected networks. Our proof employs martingale concentration to optimally treat statistical dependencies across layers of the initial random network, and we believe this approach can be useful in establishing similar results for other network architectures.",1
"In this paper, we propose a two-stage deep learning framework called VoxelContext-Net for both static and dynamic point cloud compression. Taking advantages of both octree based methods and voxel based schemes, our approach employs the voxel context to compress the octree structured data. Specifically, we first extract the local voxel representation that encodes the spatial neighbouring context information for each node in the constructed octree. Then, in the entropy coding stage, we propose a voxel context based deep entropy model to compress the symbols of non-leaf nodes in a lossless way. Furthermore, for dynamic point cloud compression, we additionally introduce the local voxel representations from the temporal neighbouring point clouds to exploit temporal dependency. More importantly, to alleviate the distortion from the octree construction procedure, we propose a voxel context based 3D coordinate refinement method to produce more accurate reconstructed point cloud at the decoder side, which is applicable to both static and dynamic point cloud compression. The comprehensive experiments on both static and dynamic point cloud benchmark datasets(e.g., ScanNet and Semantic KITTI) clearly demonstrate the effectiveness of our newly proposed method VoxelContext-Net for 3D point cloud geometry compression.",0
"The paper presents a two-stage deep learning framework named VoxelContext-Net for compressing both static and dynamic point clouds. The approach combines the strengths of octree-based and voxel-based methods and utilizes voxel context to compress octree-structured data. The first stage involves extracting a local voxel representation that encodes spatial neighboring context information for each node in the octree. The second stage proposes a deep entropy model based on voxel context to compress symbols of non-leaf nodes in a lossless way. For dynamic point cloud compression, local voxel representations from temporal neighboring point clouds are incorporated to exploit temporal dependency. Additionally, a voxel context-based 3D coordinate refinement method is suggested to reduce distortion from octree construction and produce more accurate reconstructed point clouds. The effectiveness of the proposed VoxelContext-Net method is demonstrated through comprehensive experiments on static and dynamic point cloud benchmark datasets such as ScanNet and Semantic KITTI for 3D point cloud geometry compression.",1
"Graph neural networks (GNNs) have achieved outstanding performance in learning graph-structured data and various tasks. However, many current GNNs suffer from three common problems when facing large-size graphs or using a deeper structure: neighbors explosion, node dependence, and oversmoothing. Such problems attribute to the data structures of the graph itself or the designing of the multi-layers GNNs framework, and can lead to low training efficiency and high space complexity. To deal with these problems, in this paper, we propose a general subgraph-based training framework, namely Ripple Walk Training (RWT), for deep and large graph neural networks. RWT samples subgraphs from the full graph to constitute a mini-batch, and the full GNN is updated based on the mini-batch gradient. We analyze the high-quality subgraphs to train GNNs in a theoretical way. A novel sampling method Ripple Walk Sampler works for sampling these high-quality subgraphs to constitute the mini-batch, which considers both the randomness and connectivity of the graph-structured data. Extensive experiments on different sizes of graphs demonstrate the effectiveness and efficiency of RWT in training various GNNs (GCN & GAT).",0
"Learning graph-structured data and various tasks have been accomplished with exceptional performance by Graph Neural Networks (GNNs). However, when dealing with larger graphs or using a deeper structure, many present GNNs suffer from three common problems, namely neighbors explosion, node dependence, and oversmoothing. These issues are caused by the graph's data structures or the multi-layered GNNs framework design, leading to low training efficiency and high space complexity. To address these problems, we propose a general subgraph-based training framework, called Ripple Walk Training (RWT), for deep and large graph neural networks. RWT uses high-quality subgraphs sampled from the full graph to form a mini-batch, and the full GNN is updated based on the mini-batch gradient. We theoretically analyze the subgraphs' quality for GNN training. Ripple Walk Sampler is a new sampling method that works to sample these high-quality subgraphs to form the mini-batch, which considers both the randomness and connectivity of the graph-structured data. RWT's effectiveness and efficiency in training various GNNs (GCN & GAT) are demonstrated through extensive experiments on different sizes of graphs.",1
"Graph neural networks (GNNs) have emerged as the standard method for numerous tasks on graph-structured data such as node classification. However, real-world graphs are often evolving over time and even new classes may arise. We model these challenges as an instance of lifelong learning, in which a learner faces a sequence of tasks and may take over knowledge acquired in past tasks. Such knowledge may be stored explicitly as historic data or implicitly within model parameters. In this work, we systematically analyze the influence of implicit and explicit knowledge. Therefore, we present an incremental training method for lifelong learning on graphs and introduce a new measure based on $k$-neighborhood time differences to address variances in the historic data. We apply our training method to five representative GNN architectures and evaluate them on three new lifelong node classification datasets. Our results show that no more than 50% of the GNN's receptive field is necessary to retain at least 95% accuracy compared to training over the complete history of the graph data. Furthermore, our experiments confirm that implicit knowledge becomes more important when fewer explicit knowledge is available.",0
"Graph neural networks (GNNs) have become the standard approach for various tasks involving graph-structured data, such as node classification. However, real-world graphs are often dynamic and may introduce new classes over time. To address this challenge, we view the problem as a lifelong learning instance, where a learner is presented with a sequence of tasks and can leverage past knowledge. This knowledge can be stored explicitly or implicitly in the model parameters. In this study, we examine the impact of implicit and explicit knowledge and propose an incremental training method for lifelong learning on graphs. We introduce a new measure based on $k$-neighborhood time differences to deal with variances in historical data. We apply our method to five common GNN architectures and test them on three new lifelong node classification datasets. Our results indicate that retaining no more than 50% of the GNN's receptive field is sufficient to achieve 95% accuracy compared to training on the complete graph data history. Additionally, our experiments show that implicit knowledge becomes more important when explicit knowledge is limited.",1
"\textit{Graph Neural Network} (GNN) is a promising approach for analyzing graph-structured data that tactfully captures their dependency information via node-level message passing. It has achieved state-of-the-art performances in many tasks, such as node classification, graph matching, clustering, and graph generation. As GNNs operate on non-Euclidean data, their irregular data access patterns cause considerable computational costs and overhead on conventional architectures, such as GPU and CPU. Our analysis shows that GNN adopts a hybrid computing model. The \textit{Aggregation} (or \textit{Message Passing}) phase performs vector additions where vectors are fetched with irregular strides. The \textit{Transformation} (or \textit{Node Embedding}) phase can be either dense or sparse-dense matrix multiplication. In this work, We propose \textit{VersaGNN}, an ultra-efficient, systolic-array-based versatile hardware accelerator that unifies dense and sparse matrix multiplication. By applying this single optimized systolic array to both aggregation and transformation phases, we have significantly reduced chip sizes and energy consumption. We then divide the computing engine into blocked systolic arrays to support the \textit{Strassen}'s algorithm for dense matrix multiplication, dramatically scaling down the number of multiplications and enabling high-throughput computation of GNNs. To balance the workload of sparse-dense matrix multiplication, we also introduced a greedy algorithm to combine sparse sub-matrices of compressed format into condensed ones to reduce computational cycles. Compared with current state-of-the-art GNN software frameworks, \textit{VersaGNN} achieves on average 3712$\times$ speedup with 1301.25$\times$ energy reduction on CPU, and 35.4$\times$ speedup with 17.66$\times$ energy reduction on GPU.",0
"The Graph Neural Network (GNN) is a promising method for analyzing data with a graph structure by using node-level message passing to capture their dependency information. GNNs have shown to outperform other approaches in various tasks, including clustering, graph generation, node classification, and graph matching. However, GNNs operate on non-Euclidean data, which causes significant computational costs and overhead on conventional architectures like GPU and CPU due to their irregular data access patterns. Our analysis indicates that GNN adopts a hybrid computing model, with the Aggregation phase performing vector additions that require fetching vectors with irregular strides, and the Transformation phase using either dense or sparse-dense matrix multiplication. To address this issue, we propose VersaGNN, an ultra-efficient hardware accelerator that uses a single optimized systolic array to perform both dense and sparse matrix multiplication, resulting in a significant reduction in chip sizes and energy consumption. We also divide the computing engine into blocked systolic arrays to support Strassen's algorithm for dense matrix multiplication, reducing the number of multiplications and enabling high-throughput computation of GNNs. To balance the workload of sparse-dense matrix multiplication, we introduce a greedy algorithm to combine sparse sub-matrices of compressed format into condensed ones, reducing computational cycles. Compared with current state-of-the-art GNN software frameworks, VersaGNN achieves an average speedup of 3712x with 1301.25x energy reduction on CPU and 35.4x speedup with 17.66x energy reduction on GPU.",1
"Graph neural networks are a popular variant of neural networks that work with graph-structured data. In this work, we consider combining graph neural networks with the energy-based view of Grathwohl et al. (2019) with the aim of obtaining a more robust classifier. We successfully implement this framework by proposing a novel method to ensure generation over features as well as the adjacency matrix and evaluate our method against the standard graph convolutional network (GCN) architecture (Kipf & Welling (2016)). Our approach obtains comparable discriminative performance while improving robustness, opening promising new directions for future research for energy-based graph neural networks.",0
"The energy-based approach of Grathwohl et al. (2019) can be combined with graph neural networks to create a more reliable classifier. Our study focuses on this combination and introduces a new technique to guarantee generation over both features and the adjacency matrix. We compare our method to the conventional graph convolutional network (GCN) architecture (Kipf & Welling (2016)), and find that our approach provides equivalent discriminative performance while enhancing robustness. This opens up exciting possibilities for further exploration of energy-based graph neural networks.",1
"Network-structured data becomes ubiquitous in daily life and is growing at a rapid pace. It presents great challenges to feature engineering due to the high non-linearity and sparsity of the data. The local and global structure of the real-world networks can be reflected by dynamical transfer behaviors among nodes. This paper proposes a network embedding framework to capture the transfer behaviors on structured networks via deep prediction models. We first design a degree-weight biased random walk model to capture the transfer behaviors on the network. Then a deep network embedding method is introduced to preserve the transfer possibilities among the nodes. A network structure embedding layer is added into conventional deep prediction models, including Long Short-Term Memory Network and Recurrent Neural Network, to utilize the sequence prediction ability. To keep the local network neighborhood, we further perform a Laplacian supervised space optimization on the embedding feature representations. Experimental studies are conducted on various datasets including social networks, citation networks, biomedical network, collaboration network and language network. The results show that the learned representations can be effectively used as features in a variety of tasks, such as clustering, visualization, classification, reconstruction and link prediction, and achieve promising performance compared with state-of-the-arts.",0
"Data that is structured in a network format has become increasingly common in day-to-day life and is rapidly expanding. This type of data poses significant challenges for feature engineering due to its high level of non-linearity and sparsity. The dynamic transfer of information between nodes in real-world networks reflects their local and global structures. This study proposes a network embedding framework that employs deep prediction models to capture these transfer behaviors on structured networks. Initially, a degree-weight biased random walk model is used to capture the transfer behaviors. Then, a deep network embedding method is introduced to preserve the transfer possibilities among the nodes. A network structure embedding layer is added to conventional deep prediction models, such as Long Short-Term Memory Network and Recurrent Neural Network, to utilize sequence prediction ability. To maintain the local network neighborhood, the embedding feature representations undergo a Laplacian supervised space optimization. The effectiveness of the learned representations as features in various tasks, including clustering, visualization, classification, reconstruction, and link prediction, is demonstrated through experimental studies on different datasets, such as social networks, citation networks, biomedical networks, collaboration networks, and language networks. The results indicate that the proposed method performs well compared to state-of-the-art approaches.",1
"Efficient video processing is a critical component in many IoMT applications to detect events of interest. Presently, many window optimization techniques have been proposed in event processing with an underlying assumption that the incoming stream has a structured data model. Videos are highly complex due to the lack of any underlying structured data model. Video stream sources such as CCTV cameras and smartphones are resource-constrained edge nodes. At the same time, video content extraction is expensive and requires computationally intensive Deep Neural Network (DNN) models that are primarily deployed at high-end (or cloud) nodes. This paper presents VID-WIN, an adaptive 2-stage allied windowing approach to accelerate video event analytics in an edge-cloud paradigm. VID-WIN runs parallelly across edge and cloud nodes and performs the query and resource-aware optimization for state-based complex event matching. VID-WIN exploits the video content and DNN input knobs to accelerate the video inference process across nodes. The paper proposes a novel content-driven micro-batch resizing, queryaware caching and micro-batch based utility filtering strategy of video frames under resource-constrained edge nodes to improve the overall system throughput, latency, and network usage. Extensive evaluations are performed over five real-world datasets. The experimental results show that VID-WIN video event matching achieves ~2.3X higher throughput with minimal latency and ~99% bandwidth reduction compared to other baselines while maintaining query-level accuracy and resource bounds.",0
"Detecting events of interest is crucial for many IoMT applications, and efficient video processing plays a significant role in achieving this goal. However, videos pose a challenge due to their complexity and lack of structured data models, making video content extraction expensive and computationally intensive. Additionally, video stream sources such as CCTV cameras and smartphones are resource-constrained edge nodes, while DNN models are primarily deployed at high-end nodes. To address these issues, this paper proposes an adaptive 2-stage windowing approach called VID-WIN, which optimizes video event analytics in an edge-cloud paradigm. VID-WIN runs in parallel across edge and cloud nodes and leverages video content and DNN input knobs to accelerate the video inference process. The approach also employs content-driven micro-batch resizing, query-aware caching, and utility filtering of video frames under resource-constrained edge nodes to improve system throughput, latency, and network usage. Experimental results demonstrate that VID-WIN achieves ~2.3X higher throughput with minimal latency and ~99% bandwidth reduction compared to other baselines while maintaining query-level accuracy and resource bounds.",1
"Graph neural networks (GNNs), an emerging deep learning model class, can extract meaningful representations from highly expressive graph-structured data and are therefore gaining popularity for wider ranges of applications. However, current GNNs suffer from the poor performance of their sparse-dense matrix multiplication (SpMM) operator, even when using powerful GPUs. Our analysis shows that 95% of the inference time could be spent on SpMM when running popular GNN models on NVIDIA's advanced V100 GPU. Such SpMM performance bottleneck hinders GNNs' applicability to large-scale problems or the development of more sophisticated GNN models. To address this inference time bottleneck, we introduce ES-SpMM, a cache-first edge sampling mechanism and codesigned SpMM kernel. ES-SpMM uses edge sampling to downsize the graph to fit into GPU's shared memory. It thus reduces the computation cost and improves SpMM's cache locality. To evaluate ES-SpMM's performance, we integrated it with a popular GNN framework, DGL, and tested it using representative GNN models and datasets. Our results show that ES-SpMM outperforms the highly optimized cuSPARSE SpMM kernel by up to 4.35x with no accuracy loss and by 45.3x with less than a 1% accuracy loss.",0
"GNNs are gaining popularity for their ability to extract useful information from complex graph-structured data. However, the SpMM operator used in current GNNs has poor performance, even with advanced GPUs, and can account for 95% of inference time. This bottleneck limits the applicability of GNNs to larger problems and more sophisticated models. To address this issue, we introduce ES-SpMM, a cache-first edge sampling mechanism and codesigned SpMM kernel that downsizes the graph to fit into the GPU's shared memory. This reduces computation cost and improves cache locality, resulting in improved performance. We integrated ES-SpMM with DGL, a popular GNN framework, and tested it on various models and datasets. Our results show that ES-SpMM outperforms the highly optimized cuSPARSE SpMM kernel by up to 4.35x without accuracy loss and by 45.3x with less than 1% accuracy loss.",1
"Tensor networks are a powerful modeling framework developed for computational many-body physics, which have only recently been applied within machine learning. In this work we utilize a uniform matrix product state (u-MPS) model for probabilistic modeling of sequence data. We first show that u-MPS enable sequence-level parallelism, with length-n sequences able to be evaluated in depth O(log n). We then introduce a novel generative algorithm giving trained u-MPS the ability to efficiently sample from a wide variety of conditional distributions, each one defined by a regular expression. Special cases of this algorithm correspond to autoregressive and fill-in-the-blank sampling, but more complex regular expressions permit the generation of richly structured data in a manner that has no direct analogue in neural generative models. Experiments on sequence modeling with synthetic and real text data show u-MPS outperforming a variety of baselines and effectively generalizing their predictions in the presence of limited data.",0
"Tensor networks were originally developed for computational many-body physics, but have recently been applied to machine learning. Our research utilizes a uniform matrix product state (u-MPS) model to probabilistically model sequence data. Our study demonstrates that u-MPS permits sequence-level parallelism, allowing for evaluation of length-n sequences at a depth of O(log n). Additionally, we introduce a novel generative algorithm that enables trained u-MPS to sample from various conditional distributions defined by regular expressions. This algorithm can generate richly structured data in a unique way that is not seen in neural generative models. Our experiments on synthetic and real text data demonstrate that u-MPS outperforms various baselines and effectively generalizes predictions with limited data.",1
"Bayesian optimization (BO) is a popular paradigm for global optimization of expensive black-box functions, but there are many domains where the function is not completely black-box. The data may have some known structure, e.g. symmetries, and the data generation process can yield useful intermediate or auxiliary information in addition to the value of the optimization objective. However, surrogate models traditionally employed in BO, such as Gaussian Processes (GPs), scale poorly with dataset size and struggle to incorporate known structure or auxiliary information. Instead, we propose performing BO on complex, structured problems by using Bayesian Neural Networks (BNNs), a class of scalable surrogate models that have the representation power and flexibility to handle structured data and exploit auxiliary information. We demonstrate BO on a number of realistic problems in physics and chemistry, including topology optimization of photonic crystal materials using convolutional neural networks, and chemical property optimization of molecules using graph neural networks. On these complex tasks, we show that BNNs often outperform GPs as surrogate models for BO in terms of both sampling efficiency and computational cost.",0
"Bayesian optimization (BO) is a widely used method for globally optimizing expensive black-box functions. However, in many domains, the function has some known structure, such as symmetries, and the data generation process can provide useful intermediate or auxiliary information in addition to the optimization objective. Surrogate models, such as Gaussian Processes (GPs), are typically used in BO but encounter difficulties incorporating known structure or auxiliary information and scaling with dataset size. To address this, we propose using Bayesian Neural Networks (BNNs) as surrogate models in BO for complex, structured problems. BNNs are scalable, have the flexibility to handle structured data, and can exploit auxiliary information. We demonstrate the effectiveness of this approach on several realistic problems in physics and chemistry, such as topology optimization of photonic crystal materials and chemical property optimization of molecules. Our results show that BNNs outperform GPs in terms of both sampling efficiency and computational cost on these complex tasks.",1
"The mean shift (MS) algorithm is a nonparametric method used to cluster sample points and find the local modes of kernel density estimates, using an idea based on iterative gradient ascent. In this paper we develop a mean-shift-inspired algorithm to estimate the modes of regression functions and partition the sample points in the input space. We prove convergence of the sequences generated by the algorithm and derive the non-asymptotic rates of convergence of the estimated local modes for the underlying regression model. We also demonstrate the utility of the algorithm for data-enabled discovery through an application on biomolecular structure data. An extension to subspace constrained mean shift (SCMS) algorithm used to extract ridges of regression functions is briefly discussed.",0
"The nonparametric technique known as the mean shift (MS) algorithm is utilized to cluster sample points and determine the local modes of kernel density estimates. This is achieved by employing iterative gradient ascent. The present study introduces a mean-shift-inspired algorithm that can estimate the modes of regression functions and segregate sample points in the input space. The sequences generated by the algorithm are proven to converge, and we determine the non-asymptotic convergence rates of the estimated local modes for the underlying regression model. The algorithm's effectiveness for data-enabled discovery is demonstrated through an application on biomolecular structure data. Additionally, we briefly touch upon an extension to the subspace constrained mean shift (SCMS) algorithm, which is used to extract ridges of regression functions.",1
"Recently, there has been great success in applying deep neural networks on graph structured data. Most work, however, focuses on either node- or graph-level supervised learning, such as node, link or graph classification or node-level unsupervised learning (e.g. node clustering). Despite its wide range of possible applications, graph-level unsupervised learning has not received much attention yet. This might be mainly attributed to the high representation complexity of graphs, which can be represented by n! equivalent adjacency matrices, where n is the number of nodes. In this work we address this issue by proposing a permutation-invariant variational autoencoder for graph structured data. Our proposed model indirectly learns to match the node ordering of input and output graph, without imposing a particular node ordering or performing expensive graph matching. We demonstrate the effectiveness of our proposed model on various graph reconstruction and generation tasks and evaluate the expressive power of extracted representations for downstream graph-level classification and regression.",0
"The application of deep neural networks on graph structured data has been successful in recent times. However, the majority of the existing research has concentrated on either node- or graph-level supervised learning, which includes tasks such as node, link or graph classification, or node-level unsupervised learning such as node clustering. Graph-level unsupervised learning, which has a wide range of potential applications, has not received much attention. This is primarily due to the high representation complexity of graphs, which can have numerous equivalent adjacency matrices, n!, where n is the number of nodes. To address this issue, we present a permutation-invariant variational autoencoder for graph structured data. Our proposed model learns indirectly to match the node ordering of input and output graphs, without imposing a specific node ordering or performing expensive graph matching. We demonstrate the effectiveness of our proposed model on various graph reconstruction and generation tasks and evaluate the expressive power of extracted representations for downstream graph-level classification and regression.",1
"Graph neural networks (GNNs) have been successfully applied in many structured data domains, with applications ranging from molecular property prediction to the analysis of social networks. Motivated by the broad applicability of GNNs, we propose the family of so-called RankGNNs, a combination of neural Learning to Rank (LtR) methods and GNNs. RankGNNs are trained with a set of pair-wise preferences between graphs, suggesting that one of them is preferred over the other. One practical application of this problem is drug screening, where an expert wants to find the most promising molecules in a large collection of drug candidates. We empirically demonstrate that our proposed pair-wise RankGNN approach either significantly outperforms or at least matches the ranking performance of the naive point-wise baseline approach, in which the LtR problem is solved via GNN-based graph regression.",0
"The use of graph neural networks (GNNs) has been successful in various areas of structured data, such as molecular properties and social network analysis. To further utilize the versatility of GNNs, we introduce RankGNNs, which combine neural Learning to Rank (LtR) techniques and GNNs. RankGNNs are trained using a set of pairwise preferences between graphs, where one graph is favored over another. This approach is useful in drug screening, where experts need to identify the most promising molecules from a vast collection of drug candidates. Our empirical results demonstrate that our proposed RankGNN approach outperforms, or at least matches, the ranking performance of the traditional point-wise baseline approach, which solves the LtR problem through GNN-based graph regression.",1
"Graph neural networks (GNN) have been proven to be mature enough for handling graph-structured data on node-level graph representation learning tasks. However, the graph pooling technique for learning expressive graph-level representation is critical yet still challenging. Existing pooling methods either struggle to capture the local substructure or fail to effectively utilize high-order dependency, thus diminishing the expression capability. In this paper we propose HAP, a hierarchical graph-level representation learning framework, which is adaptively sensitive to graph structures, i.e., HAP clusters local substructures incorporating with high-order dependencies. HAP utilizes a novel cross-level attention mechanism MOA to naturally focus more on close neighborhood while effectively capture higher-order dependency that may contain crucial information. It also learns a global graph content GCont that extracts the graph pattern properties to make the pre- and post-coarsening graph content maintain stable, thus providing global guidance in graph coarsening. This novel innovation also facilitates generalization across graphs with the same form of features. Extensive experiments on fourteen datasets show that HAP significantly outperforms twelve popular graph pooling methods on graph classification task with an maximum accuracy improvement of 22.79%, and exceeds the performance of state-of-the-art graph matching and graph similarity learning algorithms by over 3.5% and 16.7%.",0
"Graph neural networks (GNNs) have reached a level of maturity that enables them to handle graph-structured data for node-level graph representation learning tasks. However, the graph pooling technique for learning expressive graph-level representation remains a critical and challenging task. Current pooling methods fail to capture local substructure or effectively utilize high-order dependency, limiting their expression capability. In this paper, we introduce a novel framework called HAP, which is adaptively sensitive to graph structures and clusters local substructures while incorporating high-order dependencies. HAP utilizes a cross-level attention mechanism, MOA, to focus on close neighborhoods and capture higher-order dependency, thereby extracting crucial information. It also learns a global graph content, GCont, which extracts graph pattern properties and maintains stable pre- and post-coarsening graph content, enabling generalization across graphs with the same features. Evaluation on fourteen datasets shows that HAP outperforms twelve popular graph pooling methods by up to 22.79% in graph classification and exceeds state-of-the-art graph matching and similarity learning algorithms by over 3.5% and 16.7%, respectively.",1
"In recent years, graph neural networks (GNNs) have been widely adopted in the representation learning of graph-structured data and provided state-of-the-art performance in various applications such as link prediction, node classification, and recommendation. Motivated by recent advances of self-supervision for representation learning in natural language processing and computer vision, self-supervised learning has been recently studied to leverage unlabeled graph-structured data. However, employing self-supervision tasks as auxiliary tasks to assist a primary task has been less explored in the literature on graphs. In this paper, we propose a novel self-supervised auxiliary learning framework to effectively learn graph neural networks. Moreover, this work is the first study showing that a meta-path prediction is beneficial as a self-supervised auxiliary task for heterogeneous graphs. Our method is learning to learn a primary task with various auxiliary tasks to improve generalization performance. The proposed method identifies an effective combination of auxiliary tasks and automatically balances them to improve the primary task. Our methods can be applied to any graph neural network in a plug-in manner without manual labeling or additional data. Also, it can be extended to any other auxiliary tasks. Our experiments demonstrate that the proposed method consistently improves the performance of node classification and link prediction.",0
"Graph neural networks (GNNs) have gained widespread adoption in recent years for representation learning of graph-structured data and have demonstrated state-of-the-art performance in applications such as recommendation, node classification, and link prediction. Self-supervised learning has been studied as a way to leverage unlabeled graph-structured data, inspired by advances in self-supervision for representation learning in natural language processing and computer vision. However, the use of self-supervision tasks as auxiliary tasks to assist a primary task has received less attention in graph literature. This paper presents a novel self-supervised auxiliary learning framework for effective learning of graph neural networks, and demonstrates that meta-path prediction is a beneficial self-supervised auxiliary task for heterogeneous graphs. Our method learns a primary task with various auxiliary tasks and identifies an effective combination of tasks that can improve generalization performance. The proposed approach can be applied to any graph neural network without manual labeling or additional data, and can be extended to other auxiliary tasks. Experimental results show that our method consistently improves node classification and link prediction performance.",1
"Graph Neural Networks (GNNs) have proved to be an effective representation learning framework for graph-structured data, and have achieved state-of-the-art performance on many practical predictive tasks, such as node classification, link prediction and graph classification. Among the variants of GNNs, Graph Attention Networks (GATs) learn to assign dense attention coefficients over all neighbors of a node for feature aggregation, and improve the performance of many graph learning tasks. However, real-world graphs are often very large and noisy, and GATs are prone to overfitting if not regularized properly. Even worse, the local aggregation mechanism of GATs may fail on disassortative graphs, where nodes within local neighborhood provide more noise than useful information for feature aggregation. In this paper, we propose Sparse Graph Attention Networks (SGATs) that learn sparse attention coefficients under an $L_0$-norm regularization, and the learned sparse attentions are then used for all GNN layers, resulting in an edge-sparsified graph. By doing so, we can identify noisy/task-irrelevant edges, and thus perform feature aggregation on most informative neighbors. Extensive experiments on synthetic and real-world graph learning benchmarks demonstrate the superior performance of SGATs. In particular, SGATs can remove about 50\%-80\% edges from large assortative graphs, while retaining similar classification accuracies. On disassortative graphs, SGATs prune majority of noisy edges and outperform GATs in classification accuracies by significant margins. Furthermore, the removed edges can be interpreted intuitively and quantitatively. To the best of our knowledge, this is the first graph learning algorithm that shows significant redundancies in graphs and edge-sparsified graphs can achieve similar or sometimes higher predictive performances than original graphs.",0
"The effectiveness of Graph Neural Networks (GNNs) for graph-structured data representation learning has been established, and they have outperformed other methods on various predictive tasks, such as graph classification, node classification, and link prediction. Graph Attention Networks (GATs) are a variant of GNNs that employ attention coefficients to aggregate features across all neighboring nodes, resulting in improved performance on graph learning tasks. However, GATs are susceptible to overfitting on large and noisy real-world graphs, and their local aggregation mechanism may not work well on disassortative graphs. In this study, we introduce Sparse Graph Attention Networks (SGATs) that learn sparse attention coefficients using L0-norm regularization. The learned sparse attentions are used across all GNN layers, which sparsifies the graph's edges. This enables us to identify noisy and irrelevant edges, aggregate features on the most informative neighbors, and outperform GATs on disassortative graphs. We conducted extensive experiments on synthetic and real-world graph learning benchmarks, and SGATs exhibited superior performance. Specifically, SGATs removed 50%-80% of edges from large assortative graphs, while maintaining similar classification accuracies. Additionally, the removed edges can be intuitively and quantitatively interpreted. To our knowledge, this is the first graph learning algorithm that demonstrates significant redundancies in graphs, and edge-sparsified graphs can achieve similar or, in some cases, higher predictive performances than the original graphs.",1
"Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics systems, learning molecular fingerprints, predicting protein interface, and classifying diseases demand a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures (like the dependency trees of sentences and the scene graphs of images) is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are neural models that capture the dependence of graphs via message passing between the nodes of graphs. In recent years, variants of GNNs such as graph convolutional network (GCN), graph attention network (GAT), graph recurrent network (GRN) have demonstrated ground-breaking performances on many deep learning tasks. In this survey, we propose a general design pipeline for GNN models and discuss the variants of each component, systematically categorize the applications, and propose four open problems for future research.",0
"There is a high demand for models that can learn from graph data, which contains complex relationships between elements. Various tasks, such as modeling physics systems, predicting protein interface, and classifying diseases, require a model that can handle graph inputs. Even in domains like text and image analysis, there is a need for graph reasoning models to handle extracted structures. Graph neural networks (GNNs) are neural models that use message passing between nodes to understand the dependence of graphs. Recent variants of GNNs, such as graph convolutional network (GCN), graph attention network (GAT), and graph recurrent network (GRN), have shown impressive results in deep learning tasks. In this survey, we propose a general design pipeline for GNN models, categorize their applications, and highlight four areas for future research.",1
"Deep learning has recently demonstrated its promising performance for vision-based parking-slot detection. However, very few existing methods explicitly take into account learning the link information of the marking-points, resulting in complex post-processing and erroneous detection. In this paper, we propose an attentional graph neural network based parking-slot detection method, which refers the marking-points in an around-view image as graph-structured data and utilize graph neural network to aggregate the neighboring information between marking-points. Without any manually designed post-processing, the proposed method is end-to-end trainable. Extensive experiments have been conducted on public benchmark dataset, where the proposed method achieves state-of-the-art accuracy. Code is publicly available at \url{https://github.com/Jiaolong/gcn-parking-slot}.",0
"Recently, deep learning has displayed its potential for detecting parking slots based on visual data. However, most current techniques do not consider the relationship between the marking points, leading to complicated post-processing and inaccurate detection. In this study, we introduce an attentional graph neural network approach for parking-slot detection that treats the marking points in a surrounding image as graph-structured data and uses graph neural network to combine information between adjacent marking points. Our proposed method is fully trainable without any manual post-processing. We conducted extensive experiments on a public benchmark dataset, and our approach achieved state-of-the-art accuracy. Our code is publicly available at \url{https://github.com/Jiaolong/gcn-parking-slot}.",1
"Generalizable, transferrable, and robust representation learning on graph-structured data remains a challenge for current graph neural networks (GNNs). Unlike what has been developed for convolutional neural networks (CNNs) for image data, self-supervised learning and pre-training are less explored for GNNs. In this paper, we propose a graph contrastive learning (GraphCL) framework for learning unsupervised representations of graph data. We first design four types of graph augmentations to incorporate various priors. We then systematically study the impact of various combinations of graph augmentations on multiple datasets, in four different settings: semi-supervised, unsupervised, and transfer learning as well as adversarial attacks. The results show that, even without tuning augmentation extents nor using sophisticated GNN architectures, our GraphCL framework can produce graph representations of similar or better generalizability, transferrability, and robustness compared to state-of-the-art methods. We also investigate the impact of parameterized graph augmentation extents and patterns, and observe further performance gains in preliminary experiments. Our codes are available at https://github.com/Shen-Lab/GraphCL.",0
"Current graph neural networks (GNNs) still face difficulties in achieving generalizable, transferable, and robust representation learning for graph-structured data. Unlike convolutional neural networks (CNNs) for image data, self-supervised learning and pre-training have not been extensively explored for GNNs. This study proposes a framework called Graph Contrastive Learning (GraphCL) for unsupervised representation learning of graph data. The study includes the design of four types of graph augmentations to incorporate different priors, followed by a systematic study of the effects of various combinations of graph augmentations on multiple datasets in four different settings: semi-supervised, unsupervised, transfer learning, and adversarial attacks. Results show that, even without tuning augmentation extents or using sophisticated GNN architectures, the GraphCL framework can produce graph representations with similar or better generalizability, transferability, and robustness compared to state-of-the-art methods. The study also investigates the impact of parameterized graph augmentation extents and patterns and observes further performance gains in preliminary experiments. The codes used in this study are available at https://github.com/Shen-Lab/GraphCL.",1
"Generative modeling of set-structured data, such as point clouds, requires reasoning over local and global structures at various scales. However, adopting multi-scale frameworks for ordinary sequential data to a set-structured data is nontrivial as it should be invariant to the permutation of its elements. In this paper, we propose SetVAE, a hierarchical variational autoencoder for sets. Motivated by recent progress in set encoding, we build SetVAE upon attentive modules that first partition the set and project the partition back to the original cardinality. Exploiting this module, our hierarchical VAE learns latent variables at multiple scales, capturing coarse-to-fine dependency of the set elements while achieving permutation invariance. We evaluate our model on point cloud generation task and achieve competitive performance to the prior arts with substantially smaller model capacity. We qualitatively demonstrate that our model generalizes to unseen set sizes and learns interesting subset relations without supervision. Our implementation is available at https://github.com/jw9730/setvae.",0
"Generating models for set-structured data, such as point clouds, involves analyzing both local and global structures at varying scales. However, adapting multi-scale frameworks for sequential data to set-structured data is challenging because it must be unaffected by the element's permutation. This article introduces SetVAE, a hierarchical variational autoencoder for sets. SetVAE utilizes attention modules that partition the set and then project it back to the original cardinality to learn latent variables at different scales, capturing the set elements' coarse-to-fine dependence while ensuring permutation invariance. We assessed our model's performance in point cloud generation and achieved competitive results with less model capacity than previous approaches. We also demonstrated that our model generalizes to different set sizes and learns interesting subset relationships without supervision. Our implementation is available at https://github.com/jw9730/setvae.",1
"Graph Neural Networks (GNNs) draw their strength from explicitly modeling the topological information of structured data. However, existing GNNs suffer from limited capability in capturing the hierarchical graph representation which plays an important role in graph classification. In this paper, we innovatively propose hierarchical graph capsule network (HGCN) that can jointly learn node embeddings and extract graph hierarchies. Specifically, disentangled graph capsules are established by identifying heterogeneous factors underlying each node, such that their instantiation parameters represent different properties of the same entity. To learn the hierarchical representation, HGCN characterizes the part-whole relationship between lower-level capsules (part) and higher-level capsules (whole) by explicitly considering the structure information among the parts. Experimental studies demonstrate the effectiveness of HGCN and the contribution of each component.",0
"Graph Neural Networks (GNNs) are effective at modeling the topological information of structured data, but they have limited ability to capture the hierarchical graph representation necessary for graph classification. This paper introduces the hierarchical graph capsule network (HGCN), which can learn node embeddings and extract graph hierarchies simultaneously. HGCN uses disentangled graph capsules to identify heterogeneous factors underlying each node, and characterizes the part-whole relationship between lower-level and higher-level capsules by considering the structure information among the parts. Experimental studies show that HGCN is effective and each component contributes significantly.",1
"Graph Neural Networks (GNNs) have attracted increasing attention due to its successful applications on various graph-structure data. However, recent studies have shown that adversarial attacks are threatening the functionality of GNNs. Although numerous works have been proposed to defend adversarial attacks from various perspectives, most of them can be robust against the attacks only on specific scenarios. To address this shortage of robust generalization, we propose to defend the adversarial attacks on GNN through applying the Spatio-Temporal sparsification (called ST-Sparse) on the GNN hidden node representation. ST-Sparse is similar to the Dropout regularization in spirit. Through intensive experiment evaluation with GCN as the target GNN model, we identify the benefits of ST-Sparse as follows: (1) ST-Sparse shows the defense performance improvement in most cases, as it can effectively increase the robust accuracy by up to 6\% improvement; (2) ST-Sparse illustrates its robust generalization capability by integrating with the existing defense methods, similar to the integration of Dropout into various deep learning models as a standard regularization technique; (3) ST-Sparse also shows its ordinary generalization capability on clean datasets, in that ST-SparseGCN (the integration of ST-Sparse and the original GCN) even outperform the original GCN, while the other three representative defense methods are inferior to the original GCN.",0
"The success of Graph Neural Networks (GNNs) in various graph-structure data applications has resulted in an increase in their popularity. However, studies indicate that GNNs are vulnerable to adversarial attacks. Despite several defense mechanisms proposed to counter these attacks, most are only robust in specific scenarios. To address this limitation, we suggest using Spatio-Temporal sparsification (ST-Sparse) to defend against adversarial attacks on GNN. ST-Sparse is similar to Dropout regularization and has been evaluated through experiments with GCN as the target GNN model. Our results indicate that ST-Sparse improves defense performance and generalization capability. In most cases, ST-Sparse increases robust accuracy by up to 6%, integrates with existing defense methods, and performs better than the original GCN on clean datasets.",1
"Heart failure (HF) is a major cause of mortality. Accurately monitoring HF progress and adjust therapies are critical for improving patient outcomes. An experienced cardiologist can make accurate HF stage diagnoses based on combination of symptoms, signs, and lab results from the electronic health records (EHR) of a patient, without directly measuring heart function. We examined whether machine learning models, more specifically the XGBoost model, can accurately predict patient stage based on EHR, and we further applied the SHapley Additive exPlanations (SHAP) framework to identify informative features and their interpretations. Our results indicate that based on structured data from EHR, our models could predict patients' ejection fraction (EF) scores with moderate accuracy. SHAP analyses identified informative features and revealed potential clinical subtypes of HF. Our findings provide insights on how to design computing systems to accurately monitor disease progression of HF patients through continuously mining patients' EHR data.",0
"Monitoring and adjusting therapies for heart failure (HF) is crucial for improving patient outcomes as it is a significant cause of mortality. A cardiologist can diagnose the stage of HF accurately by combining symptoms, signs, and lab results from electronic health records (EHR), without directly measuring heart function. Our study aimed to determine whether machine learning models, specifically the XGBoost model, can predict patient stage accurately using EHR data. We also used the SHapley Additive exPlanations (SHAP) framework to identify informative features and their interpretations. Our results demonstrate moderate accuracy in predicting ejection fraction (EF) scores based on structured data from EHR. Additionally, the SHAP analyses identified informative features and revealed potential clinical subtypes of HF. Our study provides insights into designing computing systems that can accurately monitor the disease progression of HF patients by continuously mining their EHR data.",1
"The emergence of Graph Convolutional Network (GCN) has greatly boosted the progress of graph learning. However, two disturbing factors, noise and redundancy in graph data, and lack of interpretation for prediction results, impede further development of GCN. One solution is to recognize a predictive yet compressed subgraph to get rid of the noise and redundancy and obtain the interpretable part of the graph. This setting of subgraph is similar to the information bottleneck (IB) principle, which is less studied on graph-structured data and GCN. Inspired by the IB principle, we propose a novel subgraph information bottleneck (SIB) framework to recognize such subgraphs, named IB-subgraph. However, the intractability of mutual information and the discrete nature of graph data makes the objective of SIB notoriously hard to optimize. To this end, we introduce a bilevel optimization scheme coupled with a mutual information estimator for irregular graphs. Moreover, we propose a continuous relaxation for subgraph selection with a connectivity loss for stabilization. We further theoretically prove the error bound of our estimation scheme for mutual information and the noise-invariant nature of IB-subgraph. Extensive experiments on graph learning and large-scale point cloud tasks demonstrate the superior property of IB-subgraph.",0
"The development of graph learning has been greatly enhanced by the introduction of Graph Convolutional Network (GCN). However, the progress of GCN is impeded by two problematic factors: noise and redundancy in graph data, and the inability to interpret prediction results. One solution to address these issues is to identify a predictive and compressed subgraph that eliminates noise and redundancy, and provides an interpretable portion of the graph. This subgraph setting is similar to the information bottleneck (IB) principle, which has been less explored for graph-structured data and GCN. Drawing inspiration from the IB principle, we propose a novel subgraph information bottleneck (SIB) framework that recognizes such subgraphs, called IB-subgraph. However, the optimization of SIB is notoriously difficult due to the intractability of mutual information and the discrete nature of graph data. Therefore, we introduce a bilevel optimization scheme with a mutual information estimator for irregular graphs, and a continuous relaxation for subgraph selection with a connectivity loss for stabilization. We also theoretically prove the error bound of our mutual information estimation scheme and the noise-invariant nature of IB-subgraph. Extensive experiments on graph learning and large-scale point cloud tasks demonstrate the superior performance of IB-subgraph.",1
"Random forests on the one hand, and neural networks on the other hand, have met great success in the machine learning community for their predictive performance. Combinations of both have been proposed in the literature, notably leading to the so-called deep forests (DF) (Zhou \& Feng,2019). In this paper, our aim is not to benchmark DF performances but to investigate instead their underlying mechanisms. Additionally, DF architecture can be generally simplified into more simple and computationally efficient shallow forest networks. Despite some instability, the latter may outperform standard predictive tree-based methods. We exhibit a theoretical framework in which a shallow tree network is shown to enhance the performance of classical decision trees. In such a setting, we provide tight theoretical lower and upper bounds on its excess risk. These theoretical results show the interest of tree-network architectures for well-structured data provided that the first layer, acting as a data encoder, is rich enough.",0
"Both random forests and neural networks have been highly successful in the machine learning community due to their predictive abilities. The combination of the two has been proposed in the literature as deep forests (DF) and while our paper does not aim to benchmark their performance, we investigate their underlying mechanisms. DF architecture can be simplified into more computationally efficient shallow forest networks that, despite some instability, may perform better than standard predictive tree-based methods. We present a theoretical framework that shows how a shallow tree network can improve the performance of classical decision trees. Our results provide both lower and upper bounds on its excess risk, demonstrating the usefulness of tree-network architectures for well-structured data, provided the first layer functions as a rich data encoder.",1
"Graph-structured data are ubiquitous. However, graphs encode diverse types of information and thus play different roles in data representation. In this paper, we distinguish the \textit{representational} and the \textit{correlational} roles played by the graphs in node-level prediction tasks, and we investigate how Graph Neural Network (GNN) models can effectively leverage both types of information. Conceptually, the representational information provides guidance for the model to construct better node features; while the correlational information indicates the correlation between node outcomes conditional on node features. Through a simulation study, we find that many popular GNN models are incapable of effectively utilizing the correlational information. By leveraging the idea of the copula, a principled way to describe the dependence among multivariate random variables, we offer a general solution. The proposed Copula Graph Neural Network (CopulaGNN) can take a wide range of GNN models as base models and utilize both representational and correlational information stored in the graphs. Experimental results on two types of regression tasks verify the effectiveness of the proposed method.",0
"Data represented as graphs are commonly encountered, but the information contained within graphs can vary widely and serve different purposes in data representation. This study aims to differentiate between the roles of graphs in node-level prediction tasks, namely the representational and correlational roles. Furthermore, the study explores how Graph Neural Network (GNN) models can effectively utilize both types of information. The representational information guides the model in constructing better node features, while the correlational information indicates the correlation between node outcomes based on node features. A simulation study found that many popular GNN models are not proficient in utilizing correlational information. To address this, the study proposes Copula Graph Neural Network (CopulaGNN), which uses the copula concept to describe the dependence among multivariate random variables in a principled manner. CopulaGNN can function with a wide range of GNN models as base models and utilize both representational and correlational information within graphs. The study's experimental results on two types of regression tasks demonstrate the effectiveness of the proposed method.",1
"Graph convolutional neural networks (GCNNs) have received much attention recently, owing to their capability in handling graph-structured data. Among the existing GCNNs, many methods can be viewed as instances of a neural message passing motif; features of nodes are passed around their neighbors, aggregated and transformed to produce better nodes' representations. Nevertheless, these methods seldom use node transition probabilities, a measure that has been found useful in exploring graphs. Furthermore, when the transition probabilities are used, their transition direction is often improperly considered in the feature aggregation step, resulting in an inefficient weighting scheme. In addition, although a great number of GCNN models with increasing level of complexity have been introduced, the GCNNs often suffer from over-fitting when being trained on small graphs. Another issue of the GCNNs is over-smoothing, which tends to make nodes' representations indistinguishable. This work presents a new method to improve the message passing process based on node transition probabilities by properly considering the transition direction, leading to a better weighting scheme in nodes' features aggregation compared to the existing counterpart. Moreover, we propose a novel regularization method termed DropNode to address the over-fitting and over-smoothing issues simultaneously. DropNode randomly discards part of a graph, thus it creates multiple deformed versions of the graph, leading to data augmentation regularization effect. Additionally, DropNode lessens the connectivity of the graph, mitigating the effect of over-smoothing in deep GCNNs. Extensive experiments on eight benchmark datasets for node and graph classification tasks demonstrate the effectiveness of the proposed methods in comparison with the state of the art.",0
"Recently, there has been a lot of interest in Graph Convolutional Neural Networks (GCNNs) due to their ability to handle data with graph structures. Many existing GCNNs use a neural message passing approach where features of nodes are passed around their neighbors, aggregated, and transformed to improve node representations. However, these methods often ignore node transition probabilities, which are useful for exploring graphs. Additionally, when transition probabilities are considered, their direction is often not properly considered in the feature aggregation step, resulting in an inefficient weighting scheme. Furthermore, many GCNN models suffer from over-fitting, particularly when trained on small graphs, and over-smoothing, which makes nodes' representations indistinguishable. This study proposes a new method that improves the message passing process based on node transition probabilities and considers their direction, resulting in a better weighting scheme. The study also introduces a regularization method called DropNode, which randomly discards a portion of the graph to create multiple deformed versions of the graph, leading to data augmentation regularization, and lessening the connectivity of the graph, mitigating the effect of over-smoothing in deep GCNNs. Extensive experiments on eight benchmark datasets for node and graph classification tasks demonstrate the effectiveness of these proposed methods compared to the state of the art.",1
"Graph neural networks (GNNs) have demonstrated strong performance on a wide variety of tasks due to their ability to model non-uniform structured data. Despite their promise, there exists little research exploring methods to make them more efficient at inference time. In this work, we explore the viability of training quantized GNNs, enabling the usage of low precision integer arithmetic during inference. We identify the sources of error that uniquely arise when attempting to quantize GNNs, and propose an architecturally-agnostic method, Degree-Quant, to improve performance over existing quantization-aware training baselines commonly used on other architectures, such as CNNs. We validate our method on six datasets and show, unlike previous attempts, that models generalize to unseen graphs. Models trained with Degree-Quant for INT8 quantization perform as well as FP32 models in most cases; for INT4 models, we obtain up to 26% gains over the baselines. Our work enables up to 4.7x speedups on CPU when using INT8 arithmetic.",0
"The ability of graph neural networks (GNNs) to model non-uniform structured data has led to their impressive performance on various tasks. However, little research has been conducted on improving their efficiency during inference. Therefore, we investigate the possibility of training quantized GNNs that use low precision integer arithmetic for inference. Our proposed approach, Degree-Quant, is not architecture-specific and outperforms existing quantization-aware training techniques used for other architectures like CNNs. We identify the errors specific to quantizing GNNs and validate our method on six datasets, demonstrating that models generalize to previously unseen graphs. Our INT8 quantization models perform as well as FP32 models in most cases, while our INT4 models yield up to 26% improvement over baselines. Our work enables up to 4.7x faster inference on CPU with INT8 arithmetic.",1
"In this thesis, we develop various techniques for working with sets in machine learning. Each input or output is not an image or a sequence, but a set: an unordered collection of multiple objects, each object described by a feature vector. Their unordered nature makes them suitable for modeling a wide variety of data, ranging from objects in images to point clouds to graphs. Deep learning has recently shown great success on other types of structured data, so we aim to build the necessary structures for sets into deep neural networks.   The first focus of this thesis is the learning of better set representations (sets as input). Existing approaches have bottlenecks that prevent them from properly modeling relations between objects within the set. To address this issue, we develop a variety of techniques for different scenarios and show that alleviating the bottleneck leads to consistent improvements across many experiments.   The second focus of this thesis is the prediction of sets (sets as output). Current approaches do not take the unordered nature of sets into account properly. We determine that this results in a problem that causes discontinuity issues with many set prediction tasks and prevents them from learning some extremely simple datasets. To avoid this problem, we develop two models that properly take the structure of sets into account. Various experiments show that our set prediction techniques can significantly benefit over existing approaches.",0
"This thesis explores different methods for working with sets in machine learning. Rather than using images or sequences, the inputs and outputs are sets - collections of objects described by feature vectors. Sets have the advantage of being able to model a wide range of data, from images to graphs. The goal is to incorporate set structures into deep neural networks, building better set representations and improving set prediction. The first part of the thesis focuses on improving set representations, as existing approaches struggle to model object relationships within sets. To address this issue, the thesis proposes various techniques that consistently improve performance. The second part of the thesis is focused on predicting sets, which is challenging due to the unordered nature of sets. Existing approaches have difficulty with set prediction tasks and simple datasets. To overcome this problem, the thesis presents two models that take set structures into account and show significant improvements over existing approaches.",1
"An increasing number of machine learning tasks deal with learning representations from set-structured data. Solutions to these problems involve the composition of permutation-equivariant modules (e.g., self-attention, or individual processing via feed-forward neural networks) and permutation-invariant modules (e.g., global average pooling, or pooling by multi-head attention). In this paper, we propose a geometrically-interpretable framework for learning representations from set-structured data, which is rooted in the optimal mass transportation problem. In particular, we treat elements of a set as samples from a probability measure and propose an exact Euclidean embedding for Generalized Sliced Wasserstein (GSW) distances to learn from set-structured data effectively. We evaluate our proposed framework on multiple supervised and unsupervised set learning tasks and demonstrate its superiority over state-of-the-art set representation learning approaches.",0
"The number of machine learning tasks focusing on learning representations from set-structured data is increasing. To solve these problems, a combination of permutation-equivariant modules (such as self-attention or processing via feed-forward neural networks) and permutation-invariant modules (such as global average pooling or pooling by multi-head attention) is required. This paper introduces a framework for learning representations from set-structured data that is rooted in the optimal mass transportation problem and is geometrically-interpretable. The proposed framework treats elements of a set as samples from a probability measure and suggests an exact Euclidean embedding for Generalized Sliced Wasserstein (GSW) distances to effectively learn from set-structured data. The proposed framework is evaluated on multiple supervised and unsupervised set learning tasks and demonstrates its superiority over state-of-the-art set representation learning approaches.",1
"Graph neural networks (GNNs) have achieved state-of-the-art performance for node classification on graphs. The vast majority of existing works assume that genuine node labels are always provided for training. However, there has been very little research effort on how to improve the robustness of GNNs in the presence of label noise. Learning with label noise has been primarily studied in the context of image classification, but these techniques cannot be directly applied to graph-structured data, due to two major challenges -- label sparsity and label dependency -- faced by learning on graphs. In this paper, we propose a new framework, UnionNET, for learning with noisy labels on graphs under a semi-supervised setting. Our approach provides a unified solution for robustly training GNNs and performing label correction simultaneously. The key idea is to perform label aggregation to estimate node-level class probability distributions, which are used to guide sample reweighting and label correction. Compared with existing works, UnionNET has two appealing advantages. First, it requires no extra clean supervision, or explicit estimation of the noise transition matrix. Second, a unified learning framework is proposed to robustly train GNNs in an end-to-end manner. Experimental results show that our proposed approach: (1) is effective in improving model robustness against different types and levels of label noise; (2) yields significant improvements over state-of-the-art baselines.",0
"Node classification on graphs has achieved state-of-the-art performance with Graph Neural Networks (GNNs). However, current research has mostly assumed that accurate node labels are always available for training. This has resulted in little exploration of how to enhance the robustness of GNNs when faced with label noise. Learning with label noise has been studied previously in the context of image classification, but applying these techniques to graph data presents two significant challenges: label sparsity and label dependency. In this study, we present a new framework called UnionNET for learning with noisy labels on graphs in a semi-supervised setting. Our approach offers a unified solution by training GNNs robustly and correcting labels simultaneously. We achieve this by performing label aggregation to estimate node-level class probability distributions, which guide sample reweighting and label correction. UnionNET has two distinct advantages over existing works: it requires no extra clean supervision or explicit estimation of the noise transition matrix, and it provides an end-to-end learning framework for GNNs. Our experimental results demonstrate that our proposed approach is effective in improving model robustness and outperforms current state-of-the-art baselines.",1
"While most neural generative models generate outputs in a single pass, the 0 creative process is usually one of iterative building and refinement. Recent work has proposed models of editing processes, but these mostly focus on editing sequential data and/or only model a single editing pass. In this paper, we present a generic model for incremental editing of structured data (i.e., ""structural edits""). Particularly, we focus on tree-structured data, taking abstract syntax trees of computer programs as our canonical example. Our editor learns to iteratively generate tree edits (e.g., deleting or adding a subtree) and applies them to the partially edited data, thereby the entire editing process can be formulated as consecutive, incremental tree transformations. To show the unique benefits of modeling tree edits directly, we further propose a novel edit encoder for learning to represent edits, as well as an imitation learning method that allows the editor to be more robust. We evaluate our proposed editor on two source code edit datasets, where results show that, with the proposed edit encoder, our editor significantly improves accuracy over previous approaches that generate the edited program directly in one pass. Finally, we demonstrate that training our editor to imitate experts and correct its mistakes dynamically can further improve its performance.",0
"Although most neural generative models produce outputs in a single attempt, the 0 creative process typically involves iterative development and refinement. Recent research has introduced models of editing processes, but these models mainly concentrate on editing sequential data or only account for a single editing pass. This study presents a universal model for incremental editing of structured data, specifically ""structural edits."" We focus on tree-structured data, using abstract syntax trees of computer programs as an example. Our editor learns to iteratively generate tree edits, such as adding or deleting a subtree, and applies them to partially edited data. This allows the entire editing process to be formulated as consecutive, incremental tree transformations. We propose a novel edit encoder to learn to represent edits and an imitation learning method that enhances the editor's robustness to demonstrate the unique benefits of modeling tree edits directly. We evaluate our editor on two source code edit datasets, demonstrating that with the proposed edit encoder, our editor significantly improves accuracy over previous approaches that generate the edited program directly in one pass. Finally, we show that training our editor to imitate experts and correct its mistakes dynamically can further enhance its performance.",1
"Graph Neural Networks (GNNs) are widely used for analyzing graph-structured data. Most GNN methods are highly sensitive to the quality of graph structures and usually require a perfect graph structure for learning informative embeddings. However, the pervasiveness of noise in graphs necessitates learning robust representations for real-world problems. To improve the robustness of GNN models, many studies have been proposed around the central concept of Graph Structure Learning (GSL), which aims to jointly learn an optimized graph structure and corresponding representations. Towards this end, in the presented survey, we broadly review recent progress of GSL methods for learning robust representations. Specifically, we first formulate a general paradigm of GSL, and then review state-of-the-art methods classified by how they model graph structures, followed by applications that incorporate the idea of GSL in other graph tasks. Finally, we point out some issues in current studies and discuss future directions.",0
"Graph Neural Networks (GNNs) are frequently used for analyzing data structured as graphs. However, most GNN methods are sensitive to the quality of graph structures and require a perfect graph structure for effective learning. Due to the presence of noise in real-world graphs, there is a need for GNN models to learn robust representations. To address this issue, Graph Structure Learning (GSL) has been proposed as a central concept, which involves jointly learning an optimized graph structure and corresponding representations. In this survey, we review recent progress in GSL methods to improve the robustness of GNN models. We first introduce a general paradigm of GSL, followed by a review of state-of-the-art methods categorized by their approaches to modeling graph structures. We also explore applications that incorporate the idea of GSL in other graph tasks. Finally, we identify some current issues and discuss future directions.",1
"Graph convolutional networks (GCNs) and their variants have achieved great success in dealing with graph-structured data. However, it is well known that deep GCNs will suffer from over-smoothing problem, where node representations tend to be indistinguishable as we stack up more layers. Although extensive research has confirmed this prevailing understanding, few theoretical analyses have been conducted to study the expressivity and trainability of deep GCNs. In this work, we demonstrate these characterizations by studying the Gaussian Process Kernel (GPK) and Graph Neural Tangent Kernel (GNTK) of an infinitely-wide GCN, corresponding to the analysis on expressivity and trainability, respectively. We first prove the expressivity of infinitely-wide GCNs decaying at an exponential rate by applying the mean-field theory on GPK. Besides, we formulate the asymptotic behaviors of GNTK in the large depth, which enables us to reveal the dropping trainability of wide and deep GCNs at an exponential rate. Additionally, we extend our theoretical framework to analyze residual connection-resemble techniques. We found that these techniques can mildly mitigate exponential decay, but they failed to overcome it fundamentally. Finally, all theoretical results in this work are corroborated experimentally on a variety of graph-structured datasets.",0
"Graph convolutional networks (GCNs) and their variations have been successful in handling data structured as graphs. However, deep GCNs suffer from over-smoothing, where node representations become indistinguishable as more layers are added. Although prior research confirms this understanding, few theoretical analyses have been conducted to study the expressivity and trainability of deep GCNs. In this study, we examine the Gaussian Process Kernel (GPK) and Graph Neural Tangent Kernel (GNTK) of an infinitely-wide GCN to demonstrate these characteristics, corresponding to expressivity and trainability analyses. We show that infinitely-wide GCNs decay exponentially, as proven by applying mean-field theory to GPK. We also formulate the asymptotic behavior of GNTK in large depth, revealing the exponential rate of trainability drop in wide and deep GCNs. Furthermore, we extend our theoretical framework to analyze residual connection-resembling techniques, finding that they only mildly mitigate exponential decay. Finally, our theoretical results are experimentally validated on various graph-structured datasets.",1
"Most existing set encoding algorithms operate under the assumption that all the elements of the set are accessible during training and inference. Additionally, it is assumed that there are enough computational resources available for concurrently processing sets of large cardinality. However, both assumptions fail when the cardinality of the set is prohibitively large such that we cannot even load the set into memory. In more extreme cases, the set size could be potentially unlimited, and the elements of the set could be given in a streaming manner, where the model receives subsets of the full set data at irregular intervals. To tackle such practical challenges in large-scale set encoding, we go beyond the usual constraints of invariance and equivariance and introduce a new property termed Mini-Batch Consistency that is required for large scale mini-batch set encoding. We present a scalable and efficient set encoding mechanism that is amenable to mini-batch processing with respect to set elements and capable of updating set representations as more data arrives. The proposed method respects the required symmetries of invariance and equivariance as well as being Mini-Batch Consistent for random partitions of the input set. We perform extensive experiments and show that our method is computationally efficient and results in rich set encoding representations for set-structured data.",0
"Many set encoding algorithms assume that all set elements are available during training and inference, and that there are enough computational resources to process large sets simultaneously. However, when the set's cardinality is extremely high, it may not be possible to load the set into memory, and the set size may be unlimited, with elements arriving in a streaming fashion. To address these challenges, we introduce Mini-Batch Consistency, which is crucial for mini-batch set encoding at scale. Our method is scalable and efficient, and can update set representations as new data arrives. It respects invariance and equivariance symmetries, and is Mini-Batch Consistent for random partitions of the input set. We demonstrate through extensive experimentation that our approach is computationally efficient and produces rich set encoding representations for set-structured data.",1
"\textit{Attention} computes the dependency between representations, and it encourages the model to focus on the important selective features. Attention-based models, such as Transformer and graph attention network (GAT), are widely utilized for sequential data and graph-structured data. This paper suggests a new interpretation and generalized structure of the attention in Transformer and GAT. For the attention in Transformer and GAT, we derive that the attention is a product of two parts: 1) the RBF kernel to measure the similarity of two instances and 2) the exponential of $L^{2}$ norm to compute the importance of individual instances. From this decomposition, we generalize the attention in three ways. First, we propose implicit kernel attention with an implicit kernel function instead of manual kernel selection. Second, we generalize $L^{2}$ norm as the $L^{p}$ norm. Third, we extend our attention to structured multi-head attention. Our generalized attention shows better performance on classification, translation, and regression tasks.",0
"The concept of \textit{Attention} involves the assessment of interdependence between representations, with a focus on pertinent, significant features. Attention-based models like Transformer and graph attention network (GAT) are commonly used for graph-structured and sequential data. This study presents a novel interpretation and expanded structure for the attention mechanism in Transformer and GAT. Specifically, we demonstrate that attention can be broken down into two components: 1) an RBF kernel for assessing similarity between instances and 2) the exponential function of $L^{2}$ norm for determining the significance of individual instances. This decomposition enables us to generalize attention in three ways: firstly, we introduce implicit kernel attention using an implicit kernel function instead of manual kernel selection; secondly, we expand $L^{2}$ norm to $L^{p}$ norm; and thirdly, we extend our attention to structured multi-head attention. Our generalized attention mechanism outperforms traditional attention approaches in a variety of classification, translation, and regression tasks.",1
"As a new approach to train generative models, \emph{generative adversarial networks} (GANs) have achieved considerable success in image generation. This framework has also recently been applied to data with graph structures. We propose labeled-graph generative adversarial networks (LGGAN) to train deep generative models for graph-structured data with node labels. We test the approach on various types of graph datasets, such as collections of citation networks and protein graphs. Experiment results show that our model can generate diverse labeled graphs that match the structural characteristics of the training data and outperforms all alternative approaches in quality and generality. To further evaluate the quality of the generated graphs, we use them on a downstream task of graph classification, and the results show that LGGAN can faithfully capture the important aspects of the graph structure.",0
"Generative adversarial networks (GANs) have had notable success in image generation and are now being applied to data with graph structures. To train deep generative models for graph-structured data with node labels, we propose labeled-graph generative adversarial networks (LGGAN). We evaluate our approach on various types of graph datasets, including citation networks and protein graphs. Our results demonstrate that our model can generate diverse labeled graphs that match the structural characteristics of the training data and surpasses all other approaches in quality and generality. To further assess the quality of the generated graphs, we use them in a downstream task of graph classification, and our findings reveal that LGGAN can accurately capture the critical aspects of the graph structure.",1
"This paper introduces CloudLSTM, a new branch of recurrent neural models tailored to forecasting over data streams generated by geospatial point-cloud sources. We design a Dynamic Point-cloud Convolution (DConv) operator as the core component of CloudLSTMs, which performs convolution directly over point-clouds and extracts local spatial features from sets of neighboring points that surround different elements of the input. This operator maintains the permutation invariance of sequence-to-sequence learning frameworks, while representing neighboring correlations at each time step -- an important aspect in spatiotemporal predictive learning. The DConv operator resolves the grid-structural data requirements of existing spatiotemporal forecasting models and can be easily plugged into traditional LSTM architectures with sequence-to-sequence learning and attention mechanisms. We apply our proposed architecture to two representative, practical use cases that involve point-cloud streams, i.e., mobile service traffic forecasting and air quality indicator forecasting. Our results, obtained with real-world datasets collected in diverse scenarios for each use case, show that CloudLSTM delivers accurate long-term predictions, outperforming a variety of competitor neural network models.",0
"CloudLSTM is a novel type of recurrent neural model that is specifically designed to forecast data streams generated by geospatial point-cloud sources. The core component of CloudLSTMs is the Dynamic Point-cloud Convolution (DConv) operator, which performs convolution directly over point-clouds and extracts local spatial features from sets of neighboring points. This operator preserves the permutation invariance of sequence-to-sequence learning frameworks, while representing neighboring correlations at each time step. The DConv operator resolves the grid-structural data requirements of existing spatiotemporal forecasting models and can be easily integrated into traditional LSTM architectures with sequence-to-sequence learning and attention mechanisms. To evaluate the effectiveness of our proposed architecture, we applied it to two practical use cases involving point-cloud streams: mobile service traffic forecasting and air quality indicator forecasting. Our results demonstrate that CloudLSTM provides highly accurate long-term predictions, outperforming a range of other neural network models, and is therefore highly effective for spatiotemporal predictive learning.",1
"Our interest is in scientific problems with the following characteristics: (1) Data are naturally represented as graphs; (2) The amount of data available is typically small; and (3) There is significant domain-knowledge, usually expressed in some symbolic form. These kinds of problems have been addressed effectively in the past by Inductive Logic Programming (ILP), by virtue of 2 important characteristics: (a) The use of a representation language that easily captures the relation encoded in graph-structured data, and (b) The inclusion of prior information encoded as domain-specific relations, that can alleviate problems of data scarcity, and construct new relations. Recent advances have seen the emergence of deep neural networks specifically developed for graph-structured data (Graph-based Neural Networks, or GNNs). While GNNs have been shown to be able to handle graph-structured data, less has been done to investigate the inclusion of domain-knowledge. Here we investigate this aspect of GNNs empirically by employing an operation we term ""vertex-enrichment"" and denote the corresponding GNNs as ""VEGNNs"". Using over 70 real-world datasets and substantial amounts of symbolic domain-knowledge, we examine the result of vertex-enrichment across 5 different variants of GNNs. Our results provide support for the following: (a) Inclusion of domain-knowledge by vertex-enrichment can significantly improve the performance of a GNN. That is, the performance VEGNNs is significantly better than GNNs across all GNN variants; (b) The inclusion of domain-specific relations constructed using ILP improves the performance of VEGNNs, across all GNN variants. Taken together, the results provide evidence that it is possible to incorporate symbolic domain knowledge into a GNN, and that ILP can play an important role in providing high-level relationships that are not easily discovered by a GNN.",0
"We are interested in scientific problems that possess the following characteristics: (1) Data is presented in graph form naturally; (2) The quantity of data accessible is generally limited; and (3) There is a substantial amount of domain-specific knowledge that is often conveyed in symbolic form. These types of problems have historically been tackled efficiently by Inductive Logic Programming (ILP), primarily due to two important attributes: (a) The usage of a representation language that captures the connection encoded in graph-structured data effortlessly, and (b) The integration of pre-existing information encoded as domain-specific relations, which can alleviate data scarcity issues and facilitate the construction of new relationships. Recently, Graph-based Neural Networks (GNNs) have been developed to handle graph-structured data more effectively. However, there has been less exploration into the inclusion of domain-specific knowledge. In this study, we investigate this feature of GNNs empirically by introducing a new technique called ""vertex-enrichment,"" and we refer to the resulting GNNs as ""VEGNNs."" By employing over 70 real-world datasets and a significant amount of symbolic domain-knowledge, we analyze the impact of vertex-enrichment on 5 different variations of GNNs. Our findings indicate that: (a) The inclusion of domain-knowledge through vertex-enrichment can considerably enhance a GNN's performance, with VEGNNs outperforming GNNs across all GNN variations; (b) The incorporation of domain-specific relations generated using ILP improves VEGNNs' performance across all GNN variations. In conclusion, our results demonstrate that it is feasible to integrate symbolic domain knowledge into GNNs and that ILP can play a critical role in identifying high-level connections that may not be readily apparent to a GNN.",1
"Graph-structured data arise in a variety of real-world context ranging from sensor and transportation to biological and social networks. As a ubiquitous tool to process graph-structured data, spectral graph filters have been used to solve common tasks such as denoising and anomaly detection, as well as design deep learning architectures such as graph neural networks. Despite being an important tool, there is a lack of theoretical understanding of the stability properties of spectral graph filters, which are important for designing robust machine learning models. In this paper, we study filter stability and provide a novel and interpretable upper bound on the change of filter output, where the bound is expressed in terms of the endpoint degrees of the deleted and newly added edges, as well as the spatial proximity of those edges. This upper bound allows us to reason, in terms of structural properties of the graph, when a spectral graph filter will be stable. We further perform extensive experiments to verify intuition that can be gained from the bound.",0
"Graph-structured data can be found in a variety of real-world contexts, including sensor and transportation networks, as well as biological and social networks. Spectral graph filters are a commonly used tool for processing graph-structured data, as they can be used for tasks like denoising and anomaly detection, and for designing deep learning architectures like graph neural networks. However, there is a lack of theoretical understanding around the stability properties of spectral graph filters, which is important for creating robust machine learning models. This paper explores filter stability by providing a new upper bound on the change of filter output. This bound is based on the endpoint degrees of deleted and newly added edges, as well as the spatial proximity of those edges. With this new upper bound, we can determine when a spectral graph filter will be stable in terms of the graph's structural properties. The paper also includes experimental evidence to support the intuitions gained from the bound.",1
"Machine learning on graph-structured data has attracted high research interest due to the emergence of Graph Neural Networks (GNNs). Most of the proposed GNNs are based on the node homophily, i.e neighboring nodes share similar characteristics. However, in many complex networks, nodes that lie to distant parts of the graph share structurally equivalent characteristics and exhibit similar roles (e.g chemical properties of distant atoms in a molecule, type of social network users). A growing literature proposed representations that identify structurally equivalent nodes. However, most of the existing methods require high time and space complexity. In this paper, we propose VNEstruct, a simple approach, based on entropy measures of the neighborhood's topology, for generating low-dimensional structural representations, that is time-efficient and robust to graph perturbations. Empirically, we observe that VNEstruct exhibits robustness on structural role identification tasks. Moreover, VNEstruct can achieve state-of-the-art performance on graph classification, without incorporating the graph structure information in the optimization, in contrast to GNN competitors.",0
"Due to the emergence of Graph Neural Networks (GNNs), there has been a considerable amount of research interest in machine learning on graph-structured data. While most GNNs are based on node homophily, where neighboring nodes share similar characteristics, many complex networks have nodes that share structurally equivalent characteristics despite being distant from each other. Existing methods for identifying these nodes require high time and space complexity. This paper proposes VNEstruct, a time-efficient and robust approach based on entropy measures of the neighborhood's topology for generating low-dimensional structural representations. Empirical evidence shows that VNEstruct is robust in identifying structural roles and can achieve state-of-the-art performance in graph classification without incorporating graph structure information in the optimization, unlike GNN competitors.",1
"Online real estate platforms have become significant marketplaces facilitating users' search for an apartment or a house. Yet it remains challenging to accurately appraise a property's value. Prior works have primarily studied real estate valuation based on hedonic price models that take structured data into account while accompanying unstructured data is typically ignored. In this study, we investigate to what extent an automated visual analysis of apartment floor plans on online real estate platforms can enhance hedonic rent price appraisal. We propose a tailored two-staged deep learning approach to learn price-relevant designs of floor plans from historical price data. Subsequently, we integrate the floor plan predictions into hedonic rent price models that account for both structural and locational characteristics of an apartment. Our empirical analysis based on a unique dataset of 9174 real estate listings suggests that current hedonic models underutilize the available data. We find that (1) the visual design of floor plans has significant explanatory power regarding rent prices - even after controlling for structural and locational apartment characteristics, and (2) harnessing floor plans results in an up to 10.56% lower out-of-sample prediction error. We further find that floor plans yield a particularly high gain in prediction performance for older and smaller apartments. Altogether, our empirical findings contribute to the existing research body by establishing the link between the visual design of floor plans and real estate prices. Moreover, our approach has important implications for online real estate platforms, which can use our findings to enhance user experience in their real estate listings.",0
"Real estate platforms are important for users searching for properties, but accurately appraising a property's value is still difficult. Previous studies have focused on hedonic price models using structured data, ignoring unstructured data. This study explores how an automated visual analysis of apartment floor plans can improve hedonic rent price appraisal. A two-staged deep learning approach is proposed to learn price-relevant designs from historical price data and integrate floor plan predictions into hedonic rent price models. Empirical analysis of a dataset of 9174 real estate listings shows that current hedonic models underutilize available data. The visual design of floor plans explains rent prices, even when accounting for structural and locational characteristics, and harnessing floor plans results in up to 10.56% lower prediction error. Floor plans have a high impact on the prediction performance of older and smaller apartments. This study contributes to existing research by linking the visual design of floor plans and real estate prices, and has implications for enhancing user experience on online real estate platforms.",1
"Graph neural networks (GNNs) are popular to use for classifying structured data in the context of machine learning. But surprisingly, they are rarely applied to regression problems. In this work, we adopt GNN for a classic but challenging nonlinear regression problem, namely the network localization. Our main findings are in order. First, GNN is potentially the best solution to large-scale network localization in terms of accuracy, robustness and computational time. Second, proper thresholding of the communication range is essential to its superior performance. Simulation results corroborate that the proposed GNN based method outperforms all state-of-the-art benchmarks by far. Such inspiring results are theoretically justified in terms of data aggregation, non-line-of-sight (NLOS) noise removal and low-pass filtering effect, all affected by the threshold for neighbor selection. Code is available at https://github.com/Yanzongzi/GNN-For-localization.",0
"Graph neural networks (GNNs) are widely used in machine learning to classify structured data, but their use in regression problems is surprisingly rare. This study focuses on the challenging nonlinear regression problem of network localization and applies GNNs to it. The study reveals that GNNs offer the best solution in terms of accuracy, robustness, and computational time for large-scale network localization. The study also highlights the importance of proper thresholding of the communication range for superior performance. Simulation results demonstrate that the proposed GNN-based method outperforms all state-of-the-art benchmarks. Theoretical justification for these results includes data aggregation, non-line-of-sight (NLOS) noise removal, and low-pass filtering effect, all of which are affected by the threshold for neighbor selection. The code for this study is available at https://github.com/Yanzongzi/GNN-For-localization.",1
"The von Mises-Fisher (vMF) is a well-known density model for directional random variables. The recent surge of the deep embedding methodologies for high-dimensional structured data such as images or texts, aimed at extracting salient directional information, can make the vMF model even more popular. In this article, we will review the vMF model and its mixture, provide detailed recipes of how to train the models, focusing on the maximum likelihood estimators, in Python/PyTorch. In particular, implementation of vMF typically suffers from the notorious numerical issue of the Bessel function evaluation in the density normalizer, especially when the dimensionality is high, and we address the issue using the MPMath library that supports arbitrary precision. For the mixture learning, we provide both minibatch-based large-scale SGD learning, as well as the EM algorithm which is a full batch estimator. For each estimator/methodology, we test our implementation on some synthetic data, while we also demonstrate the use case in a more realistic scenario of image clustering. Our code is publicly available in https://github.com/minyoungkim21/vmf-lib.",0
"The von Mises-Fisher (vMF) model is widely used for directional random variables, and its popularity is expected to increase with the growing interest in deep embedding methodologies for high-dimensional structured data. This article provides an overview of the vMF model and its mixture, and presents detailed instructions on how to train the models using maximum likelihood estimators in Python/PyTorch. The implementation of vMF is known to have numerical issues due to the Bessel function evaluation in the density normalizer, especially in high-dimensional settings. To address this issue, we use the MPMath library, which supports arbitrary precision. We also provide both minibatch-based large-scale SGD learning and the EM algorithm for mixture learning. We demonstrate the performance of our implementation on synthetic data and show its application to image clustering. Our code is publicly available on https://github.com/minyoungkim21/vmf-lib.",1
"Graph neural networks have shown superior performance in a wide range of applications providing a powerful representation of graph-structured data. Recent works show that the representation can be further improved by auxiliary tasks. However, the auxiliary tasks for heterogeneous graphs, which contain rich semantic information with various types of nodes and edges, have less explored in the literature. In this paper, to learn graph neural networks on heterogeneous graphs we propose a novel self-supervised auxiliary learning method using meta-paths, which are composite relations of multiple edge types. Our proposed method is learning to learn a primary task by predicting meta-paths as auxiliary tasks. This can be viewed as a type of meta-learning. The proposed method can identify an effective combination of auxiliary tasks and automatically balance them to improve the primary task. Our methods can be applied to any graph neural networks in a plug-in manner without manual labeling or additional data. The experiments demonstrate that the proposed method consistently improves the performance of link prediction and node classification on heterogeneous graphs.",0
"Graph neural networks have proven to be highly effective in a broad range of applications, offering a robust representation of graph-structured data. Recent research has revealed that auxiliary tasks can further enhance this representation. However, little attention has been paid in the literature to auxiliary tasks for heterogeneous graphs, which contain diverse types of nodes and edges and rich semantic information. To address this gap, we propose a novel self-supervised auxiliary learning approach using meta-paths, which are composite relations of multiple edge types, to train graph neural networks on heterogeneous graphs. Our method, which involves predicting meta-paths as auxiliary tasks, is a form of meta-learning that can effectively identify and balance the most effective combination of auxiliary tasks to enhance the primary task. Importantly, our method can be applied in a plug-in manner to any graph neural network without the need for manual labeling or additional data. Our experiments demonstrate that our approach consistently improves the performance of link prediction and node classification on heterogeneous graphs.",1
"Graph attention networks (GATs) have been recognized as powerful tools for learning in graph structured data. However, how to enable the attention mechanisms in GATs to smoothly consider both structural and feature information is still very challenging. In this paper, we propose Graph Joint Attention Networks (JATs) to address the aforementioned challenge. Different from previous attention-based graph neural networks (GNNs), JATs adopt novel joint attention mechanisms which can automatically determine the relative significance between node features and structural coefficients learned from graph topology, when computing the attention scores. Therefore, representations concerning more structural properties can be inferred by JATs. Besides, we theoretically analyze the expressive power of JATs and further propose an improved strategy for the joint attention mechanisms that enables JATs to reach the upper bound of expressive power which every message-passing GNN can ultimately achieve, i.e., 1-WL test. JATs can thereby be seen as most powerful message-passing GNNs. The proposed neural architecture has been extensively tested on widely used benchmarking datasets, and has been compared with state-of-the-art GNNs for various downstream predictive tasks. Experimental results show that JATs achieve state-of-the-art performance on all the testing datasets.",0
"While graph attention networks (GATs) have been acknowledged as valuable tools for learning in graph structured data, effectively incorporating attention mechanisms that consider both structural and feature information remains a significant challenge. This paper introduces Graph Joint Attention Networks (JATs) as a solution to this challenge. Unlike previous attention-based graph neural networks (GNNs), JATs employ novel joint attention mechanisms that automatically determine the relative significance between node features and structural coefficients learned from graph topology when computing attention scores. Consequently, JATs can infer representations that account for more structural properties. The paper also provides a theoretical analysis of the expressive power of JATs and presents an improved strategy for the joint attention mechanisms that enables JATs to achieve the upper bound of expressive power, i.e., the 1-WL test, making them the most powerful message-passing GNNs. Extensive testing on benchmarking datasets and comparisons with state-of-the-art GNNs demonstrate that JATs perform exceptionally well on various downstream predictive tasks.",1
"Convolutional Neural Networks(CNNs) has achieved remarkable performance breakthrough in Euclidean structure data. Recently, aggregation-transformation based Graph Neural networks(GNNs) gradually produce a powerful performance on non-Euclidean data. In this paper, we propose a cross-correlation based graph convolution method allowing to naturally generalize CNNs to non-Euclidean domains and inherit the excellent natures of CNNs, such as local filters, parameter sharing, flexible receptive field, etc. Meanwhile, it leverages dynamically generated convolution kernel and cross-correlation operators to address the shortcomings of prior methods based on aggregation-transformation or their approximations. Our method has achieved or matched popular state-of-the-art results across three established graph benchmarks: the Cora, Citeseer, and Pubmed citation network datasets.",0
"CNNs have shown impressive advancements in handling Euclidean structured data, while GNNs that are based on aggregation-transformation have been successful in non-Euclidean data. This paper proposes a new approach - a cross-correlation based graph convolution method, which combines the advantages of CNNs and GNNs. By utilizing dynamically generated convolution kernels and cross-correlation operators, our method overcomes the limitations of previous methods. The proposed method has been tested on three established graph benchmarks - Cora, Citeseer, and Pubmed citation network datasets, achieving or matching state-of-the-art results.",1
"The medical field is creating large amount of data that physicians are unable to decipher and use efficiently. Moreover, rule-based expert systems are inefficient in solving complicated medical tasks or for creating insights using big data. Deep learning has emerged as a more accurate and effective technology in a wide range of medical problems such as diagnosis, prediction and intervention. Deep learning is a representation learning method that consists of layers that transform the data non-linearly, thus, revealing hierarchical relationships and structures. In this review we survey deep learning application papers that use structured data, signal and imaging modalities from cardiology. We discuss the advantages and limitations of applying deep learning in cardiology that also apply in medicine in general, while proposing certain directions as the most viable for clinical use.",0
"Physicians are faced with a challenge in efficiently interpreting the vast amount of data generated by the medical field. Traditional rule-based expert systems are insufficient in solving complex medical tasks and analyzing big data. However, deep learning technology has proven to be a more accurate and effective solution for various medical issues such as diagnosis, prediction, and intervention. This method involves layers that nonlinearly transform data, revealing hierarchical relationships and structures. This review explores the use of deep learning on structured data, signal, and imaging modalities in cardiology, highlighting its advantages and limitations. We also propose potential directions for clinical use in cardiology and medicine as a whole.",1
"Summarization of long sequences into a concise statement is a core problem in natural language processing, requiring non-trivial understanding of the input. Based on the promising results of graph neural networks on highly structured data, we develop a framework to extend existing sequence encoders with a graph component that can reason about long-distance relationships in weakly structured data such as text. In an extensive evaluation, we show that the resulting hybrid sequence-graph models outperform both pure sequence models as well as pure graph models on a range of summarization tasks.",0
"The process of condensing lengthy sequences into a brief statement is a fundamental issue in natural language processing. This task demands a substantial comprehension of the input. Building on the successful outcomes of graph neural networks when dealing with highly organized data, our team has created a structure that expands current sequence encoders with a graph element. This component can analyze distant connections in text that lacks strong structure. Through a comprehensive analysis, we demonstrate that the hybrid sequence-graph models that result are superior to pure sequence and pure graph models in multiple summarization tasks.",1
"Graph-structured data ubiquitously appears in science and engineering. Graph neural networks (GNNs) are designed to exploit the relational inductive bias exhibited in graphs; they have been shown to outperform other forms of neural networks in scenarios where structure information supplements node features. The most common GNN architecture aggregates information from neighborhoods based on message passing. Its generality has made it broadly applicable. In this paper, we focus on a special, yet widely used, type of graphs -- DAGs -- and inject a stronger inductive bias -- partial ordering -- into the neural network design. We propose the \emph{directed acyclic graph neural network}, DAGNN, an architecture that processes information according to the flow defined by the partial order. DAGNN can be considered a framework that entails earlier works as special cases (e.g., models for trees and models updating node representations recurrently), but we identify several crucial components that prior architectures lack. We perform comprehensive experiments, including ablation studies, on representative DAG datasets (i.e., source code, neural architectures, and probabilistic graphical models) and demonstrate the superiority of DAGNN over simpler DAG architectures as well as general graph architectures.",0
"The presence of graph-structured data is widespread in science and engineering. Graph neural networks (GNNs) utilize the relational inductive bias present in graphs and have proven to be more effective than other neural network types in situations where node features are supplemented by structural information. The most commonly used GNN architecture is based on message passing to aggregate information from neighborhoods and has a broad range of applications. This paper focuses on directed acyclic graphs (DAGs), a type of graph that is widely used, and introduces a stronger inductive bias, partial ordering, into the neural network design. The resulting architecture, called DAGNN, processes information according to the flow defined by the partial order. DAGNN is a framework that includes earlier works as special cases, but it includes several essential components that previous architectures lack. Comprehensive experiments, including ablation studies, were conducted on representative DAG datasets, such as source code, neural architectures, and probabilistic graphical models. The results show that DAGNN outperforms simpler DAG architectures and general graph architectures.",1
"A big mystery in deep learning continues to be the ability of methods to generalize when the number of model parameters is larger than the number of training examples. In this work, we take a step towards a better understanding of the underlying phenomena of Deep Autoencoders (AEs), a mainstream deep learning solution for learning compressed, interpretable, and structured data representations. In particular, we interpret how AEs approximate the data manifold by exploiting their continuous piecewise affine structure. Our reformulation of AEs provides new insights into their mapping, reconstruction guarantees, as well as an interpretation of commonly used regularization techniques. We leverage these findings to derive two new regularizations that enable AEs to capture the inherent symmetry in the data. Our regularizations leverage recent advances in the group of transformation learning to enable AEs to better approximate the data manifold without explicitly defining the group underlying the manifold. Under the assumption that the symmetry of the data can be explained by a Lie group, we prove that the regularizations ensure the generalization of the corresponding AEs. A range of experimental evaluations demonstrate that our methods outperform other state-of-the-art regularization techniques.",0
"The challenge of deep learning is still the ability to generalize with more model parameters than training examples. This study aims to gain a better understanding of Deep Autoencoders (AEs), a popular deep learning approach for creating compressed and organized data representations. Our analysis focuses on how AEs use their continuous piecewise affine structure to approximate the data manifold. By redefining AEs, we gain new insights into their mapping and reconstruction guarantees, as well as their regularization techniques. Our research has led to two new regularizations that can capture the inherent symmetry in the data. These regularizations leverage recent advances in transformation learning to help AEs better approximate the data manifold without explicitly defining the underlying group. We prove that these regularizations ensure the generalization of AEs by assuming that the symmetry of the data can be explained by a Lie group. Our experiments show that our regularizations outperform other state-of-the-art techniques.",1
"Graph Neural Networks (GNNs) have received considerable attention on graph-structured data learning for a wide variety of tasks. The well-designed propagation mechanism which has been demonstrated effective is the most fundamental part of GNNs. Although most of GNNs basically follow a message passing manner, litter effort has been made to discover and analyze their essential relations. In this paper, we establish a surprising connection between different propagation mechanisms with a unified optimization problem, showing that despite the proliferation of various GNNs, in fact, their proposed propagation mechanisms are the optimal solution optimizing a feature fitting function over a wide class of graph kernels with a graph regularization term. Our proposed unified optimization framework, summarizing the commonalities between several of the most representative GNNs, not only provides a macroscopic view on surveying the relations between different GNNs, but also further opens up new opportunities for flexibly designing new GNNs. With the proposed framework, we discover that existing works usually utilize naive graph convolutional kernels for feature fitting function, and we further develop two novel objective functions considering adjustable graph kernels showing low-pass or high-pass filtering capabilities respectively. Moreover, we provide the convergence proofs and expressive power comparisons for the proposed models. Extensive experiments on benchmark datasets clearly show that the proposed GNNs not only outperform the state-of-the-art methods but also have good ability to alleviate over-smoothing, and further verify the feasibility for designing GNNs with our unified optimization framework.",0
"Graph Neural Networks (GNNs) have gained significant attention for their ability to learn from graph-structured data for a wide range of tasks. The propagation mechanism is the most crucial part of GNNs and has been proven to be effective. However, little effort has been made to analyze the essential relationships between different propagation mechanisms. In this study, we establish a connection between various propagation mechanisms through a unified optimization problem. Our proposed framework summarizes the commonalities between several of the most representative GNNs, providing a macroscopic view of the relations between different GNNs, and opening up new opportunities for designing GNNs. We discover that existing works usually utilize naive graph convolutional kernels for feature fitting function, and we further develop two novel objective functions. Moreover, we provide convergence proofs and expressive power comparisons for the proposed models. The proposed GNNs not only outperform state-of-the-art methods but also have good ability to alleviate over-smoothing, demonstrating the feasibility of designing GNNs with our unified optimization framework.",1
"Kernel methods have been among the most popular techniques in machine learning, where learning tasks are solved using the property of reproducing kernel Hilbert space (RKHS). In this paper, we propose a novel data analysis framework with reproducing kernel Hilbert $C^*$-module (RKHM) and kernel mean embedding (KME) in RKHM. Since RKHM contains richer information than RKHS or vector-valued RKHS (vv RKHS), analysis with RKHM enables us to capture and extract structural properties in multivariate data, functional data and other structured data. We show a branch of theories for RKHM to apply to data analysis, including the representer theorem, and the injectivity and universality of the proposed KME. We also show RKHM generalizes RKHS and vv RKHS. Then, we provide concrete procedures for employing RKHM and the proposed KME to data analysis.",0
"One of the most widely used machine learning techniques is kernel methods, which leverage the capabilities of reproducing kernel Hilbert space (RKHS) to solve learning tasks. In this article, we introduce a new framework for data analysis that employs reproducing kernel Hilbert C*-module (RKHM) and kernel mean embedding (KME) within RKHM. By utilizing RKHM, which contains more extensive information than vector-valued RKHS (vv RKHS) or RKHS, we can identify and extract structural properties in multivariate data, functional data, and other structured data. We also present a range of theories for using RKHM in data analysis, such as the representer theorem and the injectivity and universality of KME. Moreover, we demonstrate how RKHM generalizes vv RKHS and RKHS and provide practical guidelines for utilizing RKHM and KME in data analysis.",1
"Graph representation of structured data can facilitate the extraction of stereoscopic features, and it has demonstrated excellent ability when working with deep learning systems, the so-called Graph Neural Networks (GNNs). Choosing a promising architecture for constructing GNNs can be transferred to a hyperparameter optimisation problem, a very challenging task due to the size of the underlying search space and high computational cost for evaluating candidate GNNs. To address this issue, this research presents a novel genetic algorithm with a hierarchical evaluation strategy (HESGA), which combines the full evaluation of GNNs with a fast evaluation approach. By using full evaluation, a GNN is represented by a set of hyperparameter values and trained on a specified dataset, and root mean square error (RMSE) will be used to measure the quality of the GNN represented by the set of hyperparameter values (for regression problems). While in the proposed fast evaluation process, the training will be interrupted at an early stage, the difference of RMSE values between the starting and interrupted epochs will be used as a fast score, which implies the potential of the GNN being considered. To coordinate both types of evaluations, the proposed hierarchical strategy uses the fast evaluation in a lower level for recommending candidates to a higher level, where the full evaluation will act as a final assessor to maintain a group of elite individuals. To validate the effectiveness of HESGA, we apply it to optimise two types of deep graph neural networks. The experimental results on three benchmark datasets demonstrate its advantages compared to Bayesian hyperparameter optimization.",0
"The utilization of graph representation for structured data can enhance the identification of stereoscopic features, particularly when working with Graph Neural Networks (GNNs), a type of deep learning system. However, selecting an appropriate architecture for constructing GNNs can pose a challenge due to the extensive search space and high computational cost required for assessing candidate GNNs. To overcome this hurdle, a novel genetic algorithm with a hierarchical evaluation strategy (HESGA) has been proposed. HESGA combines full evaluation of GNNs with a fast evaluation approach, and RMSE is used to measure the quality of the GNN represented by the set of hyperparameters. The fast evaluation process involves interrupting training at an early stage and using the difference in RMSE values between the starting and interrupted epochs as a fast score to assess the potential of a GNN. The hierarchical strategy coordinates both types of evaluations, with the fast evaluation recommending candidates to the higher level for full evaluation. HESGA has been applied to two types of deep graph neural networks, and experimental results on three benchmark datasets demonstrate its superiority over Bayesian hyperparameter optimization.",1
"Deep generative models, since their inception, have become increasingly more capable of generating novel and perceptually realistic signals (e.g., images and sound waves). With the emergence of deep models for graph structured data, natural interests seek extensions of these generative models for graphs. Successful extensions were seen recently in the case of learning from a collection of graphs (e.g., protein data banks), but the learning from a single graph has been largely under explored. The latter case, however, is important in practice. For example, graphs in financial and healthcare systems contain so much confidential information that their public accessibility is nearly impossible, but open science in these fields can only advance when similar data are available for benchmarking.   In this work, we propose an approach to generating a doppelganger graph that resembles a given one in many graph properties but nonetheless can hardly be used to reverse engineer the original one, in the sense of a near zero edge overlap. The approach is an orchestration of graph representation learning, generative adversarial networks, and graph realization algorithms. Through comparison with several graph generative models (either parameterized by neural networks or not), we demonstrate that our result barely reproduces the given graph but closely matches its properties. We further show that downstream tasks, such as node classification, on the generated graphs reach similar performance to the use of the original ones.",0
"The capability of deep generative models to produce realistic signals has improved over time, especially with the development of models for graph structured data. Although successful extensions have been made in learning from multiple graphs, there has been limited exploration in learning from a single graph. This is important in fields such as finance and healthcare, where sensitive information cannot be made public but is necessary for benchmarking. In this study, we propose a method for generating a graph that resembles a given graph in various properties but cannot be used to reverse engineer the original one. Our approach involves graph representation learning, generative adversarial networks, and graph realization algorithms. Our results show that our model reproduces the properties of the given graph closely but with minimal edge overlap. Furthermore, downstream tasks such as node classification on the generated graphs perform similarly to those on the original ones.",1
"A fundamental problem on graph-structured data is that of quantifying similarity between graphs. Graph kernels are an established technique for such tasks; in particular, those based on random walks and return probabilities have proven to be effective in wide-ranging applications, from bioinformatics to social networks to computer vision. However, random walk kernels generally suffer from slowness and tottering, an effect which causes walks to overemphasize local graph topology, undercutting the importance of global structure. To correct for these issues, we recast return probability graph kernels under the more general framework of density of states -- a framework which uses the lens of spectral analysis to uncover graph motifs and properties hidden within the interior of the spectrum -- and use our interpretation to construct scalable, composite density of states based graph kernels which balance local and global information, leading to higher classification accuracies on a host of benchmark datasets.",0
"The quantification of similarity between graphs is a significant challenge in graph-structured data. Graph kernels are a well-known approach that addresses this issue. Random walk and return probability-based kernels have been particularly successful in a variety of applications, such as bioinformatics, computer vision, and social networks. However, random walk kernels tend to be slow and unstable, resulting in local graph topology receiving too much emphasis, while the significance of global structure is diminished. To solve these problems, we have redefined return probability graph kernels under the more comprehensive framework of density of states. This framework uses spectral analysis to reveal hidden graph motifs and properties within the spectrum. We have used this interpretation to create scalable, composite density of states based graph kernels that balance local and global information, resulting in improved classification accuracy for various benchmark datasets.",1
"Lots of neural network architectures have been proposed to deal with learning tasks on graph-structured data. However, most of these models concentrate on only node features during the learning process. The edge features, which usually play a similarly important role as the nodes, are often ignored or simplified by these models. In this paper, we present edge-featured graph attention networks, namely EGATs, to extend the use of graph neural networks to those tasks learning on graphs with both node and edge features. These models can be regarded as extensions of graph attention networks (GATs). By reforming the model structure and the learning process, the new models can accept node and edge features as inputs, incorporate the edge information into feature representations, and iterate both node and edge features in a parallel but mutual way. The results demonstrate that our work is highly competitive against other node classification approaches, and can be well applied in edge-featured graph learning tasks.",0
"Numerous neural network structures have been suggested to address learning tasks involving graph-structured data. Nevertheless, most of these models primarily focus on node features during the learning process. Edge features, which are equally significant as nodes, are frequently disregarded or simplified by these models. This paper introduces Edge-Featured Graph Attention Networks, or EGATs, which broaden the scope of graph neural networks to tasks involving graphs with both node and edge features. These models are essentially extensions of Graph Attention Networks, or GATs. By modifying the model structure and learning process, the new models can take in both node and edge features as inputs, merge edge information into feature representations, and iterate both node and edge features in a parallel but interdependent manner. The findings indicate that our work is highly competitive compared to other node classification methods and can be effectively utilized in edge-featured graph learning tasks.",1
"There has been a surge of recent interest in learning representations for graph-structured data. Graph representation learning methods have generally fallen into three main categories, based on the availability of labeled data. The first, network embedding (such as shallow graph embedding or graph auto-encoders), focuses on learning unsupervised representations of relational structure. The second, graph regularized neural networks, leverages graphs to augment neural network losses with a regularization objective for semi-supervised learning. The third, graph neural networks, aims to learn differentiable functions over discrete topologies with arbitrary structure. However, despite the popularity of these areas there has been surprisingly little work on unifying the three paradigms. Here, we aim to bridge the gap between graph neural networks, network embedding and graph regularization models. We propose a comprehensive taxonomy of representation learning methods for graph-structured data, aiming to unify several disparate bodies of work. Specifically, we propose a Graph Encoder Decoder Model (GRAPHEDM), which generalizes popular algorithms for semi-supervised learning on graphs (e.g. GraphSage, Graph Convolutional Networks, Graph Attention Networks), and unsupervised learning of graph representations (e.g. DeepWalk, node2vec, etc) into a single consistent approach. To illustrate the generality of this approach, we fit over thirty existing methods into this framework. We believe that this unifying view both provides a solid foundation for understanding the intuition behind these methods, and enables future research in the area.",0
"Recently, there has been an increased interest in acquiring knowledge about representations for graph-structured data. These methods can be categorized into three groups, based on their reliance on labeled data. The first group, network embedding, concentrates on unsupervised learning of relational structures, using shallow graph embedding or graph auto-encoders. The second group, graph regularized neural networks, utilizes graphs to augment neural network losses for semi-supervised learning. The third group, graph neural networks, attempts to learn differentiable functions over discrete topologies with arbitrary structure. However, there has been little attempt to unify these three paradigms. The aim of this study is to bridge the gap between graph neural networks, network embedding, and graph regularization models. A comprehensive taxonomy of representation learning methods for graph-structured data is proposed, with the Graph Encoder Decoder Model (GRAPHEDM) as the centerpiece. This model generalizes several algorithms for semi-supervised learning on graphs and unsupervised learning of graph representations into a single coherent approach. Through this framework, over thirty existing methods can be fitted, and we believe that this unifying view provides a solid foundation for understanding these methods and enables future research in this area.",1
"Standard convolutional neural networks assume a grid structured input is available and exploit discrete convolutions as their fundamental building blocks. This limits their applicability to many real-world applications. In this paper we propose Parametric Continuous Convolution, a new learnable operator that operates over non-grid structured data. The key idea is to exploit parameterized kernel functions that span the full continuous vector space. This generalization allows us to learn over arbitrary data structures as long as their support relationship is computable. Our experiments show significant improvement over the state-of-the-art in point cloud segmentation of indoor and outdoor scenes, and lidar motion estimation of driving scenes.",0
"The use of standard convolutional neural networks is restricted to input that is grid structured, making it unsuitable for various real-world applications. To address this issue, we introduce a new learnable operator called Parametric Continuous Convolution, which operates over non-grid structured data. This is achieved by utilizing parameterized kernel functions that cover the entire continuous vector space. With this approach, we are able to learn from arbitrary data structures as long as their support relationship can be computed. Our experiments demonstrate that this method significantly improves upon the current state-of-the-art in point cloud segmentation for indoor and outdoor scenes, as well as lidar motion estimation for driving scenes.",1
"Graph convolutional networks have achieved great success on graph-structured data. Many graph convolutional networks can be regarded as low-pass filters for graph signals. In this paper, we propose a new model, BiGCN, which represents a graph neural network as a bi-directional low-pass filter. Specifically, we not only consider the original graph structure information but also the latent correlation between features, thus BiGCN can filter the signals along with both the original graph and a latent feature-connection graph. Our model outperforms previous graph neural networks in the tasks of node classification and link prediction on most of the benchmark datasets, especially when we add noise to the node features.",0
"Great success has been achieved by graph convolutional networks on data with graph structures. These networks can function as low-pass filters for graph signals. In this paper, we present a new model called BiGCN, which represents a graph neural network as a bi-directional low-pass filter. Our model considers not only the original graph structure information but also the latent correlation between features. Thus, BiGCN has the ability to filter signals along with both the original graph and a latent feature-connection graph. We have found that our model performs better than previous graph neural networks in the tasks of node classification and link prediction on most benchmark datasets, especially when noise is added to the node features.",1
"Graph convolutional networks (GCN) have recently demonstrated their potential in analyzing non-grid structure data that can be represented as graphs. The core idea is to encode the local topology of a graph, via convolutions, into the feature of a center node. In this paper, we propose a novel GCN model, which we term as Shortest Path Graph Attention Network (SPAGAN). Unlike conventional GCN models that carry out node-based attentions within each layer, the proposed SPAGAN conducts path-based attention that explicitly accounts for the influence of a sequence of nodes yielding the minimum cost, or shortest path, between the center node and its higher-order neighbors. SPAGAN therefore allows for a more informative and intact exploration of the graph structure and further {a} more effective aggregation of information from distant neighbors into the center node, as compared to node-based GCN methods. We test SPAGAN on the downstream classification task on several standard datasets, and achieve performances superior to the state of the art. Code is publicly available at https://github.com/ihollywhy/SPAGAN.",0
"Recently, Graph convolutional networks (GCN) have shown promise in analyzing non-grid structure data, represented as graphs. The main concept is to use convolutions to encode the local topology of a graph into a center node's feature. In this article, we introduce a new GCN model, Shortest Path Graph Attention Network (SPAGAN). Unlike conventional GCN models that carry out node-based attentions, SPAGAN uses path-based attention to explicitly consider the influence of a sequence of nodes that result in the minimum cost or shortest path between the center node and its higher-order neighbors. As a result, SPAGAN allows more informative exploration of the graph structure and more effective information aggregation from distant neighbors into the center node than node-based GCN methods. We tested SPAGAN on standard datasets for downstream classification tasks and achieved superior performance compared to state-of-the-art methods. We have made the code publicly available at https://github.com/ihollywhy/SPAGAN.",1
"A scalable semi-supervised node classification method on graph-structured data, called GraphHop, is proposed in this work. The graph contains attributes of all nodes but labels of a few nodes. The classical label propagation (LP) method and the emerging graph convolutional network (GCN) are two popular semi-supervised solutions to this problem. The LP method is not effective in modeling node attributes and labels jointly or facing a slow convergence rate on large-scale graphs. GraphHop is proposed to its shortcoming. With proper initial label vector embeddings, each iteration of GraphHop contains two steps: 1) label aggregation and 2) label update. In Step 1, each node aggregates its neighbors' label vectors obtained in the previous iteration. In Step 2, a new label vector is predicted for each node based on the label of the node itself and the aggregated label information obtained in Step 1. This iterative procedure exploits the neighborhood information and enables GraphHop to perform well in an extremely small label rate setting and scale well for very large graphs. Experimental results show that GraphHop outperforms state-of-the-art graph learning methods on a wide range of tasks (e.g., multi-label and multi-class classification on citation networks, social graphs, and commodity consumption graphs) in graphs of various sizes. Our codes are publicly available on GitHub (https://github.com/TianXieUSC/GraphHop).",0
"In this work, a method called GraphHop is presented for scalable semi-supervised node classification on graph-structured data. The graph consists of attributes for all nodes, but only a few nodes have labels. The traditional label propagation (LP) method and the more recent graph convolutional network (GCN) are two popular semi-supervised approaches to this problem. However, LP is not effective at modeling node attributes and labels simultaneously, and it has a slow convergence rate on large-scale graphs. To address this, GraphHop is proposed. With appropriate initial label vector embeddings, each iteration of GraphHop involves two steps: label aggregation and label update. In Step 1, each node aggregates its neighbors' label vectors from the previous iteration. In Step 2, a new label vector is predicted for each node based on its own label and the aggregated label information from Step 1. This iterative process takes advantage of neighborhood information and allows GraphHop to perform well even with a very small label rate and scale up for very large graphs. Experimental results demonstrate that GraphHop outperforms state-of-the-art graph learning methods on a variety of tasks (e.g., multi-label and multi-class classification on citation networks, social graphs, and commodity consumption graphs) for graphs of different sizes. The codes for GraphHop are publicly available on GitHub (https://github.com/TianXieUSC/GraphHop).",1
"Graph Neural Networks (graph NNs) are a promising deep learning approach for analyzing graph-structured data. However, it is known that they do not improve (or sometimes worsen) their predictive performance as we pile up many layers and add non-lineality. To tackle this problem, we investigate the expressive power of graph NNs via their asymptotic behaviors as the layer size tends to infinity. Our strategy is to generalize the forward propagation of a Graph Convolutional Network (GCN), which is a popular graph NN variant, as a specific dynamical system. In the case of a GCN, we show that when its weights satisfy the conditions determined by the spectra of the (augmented) normalized Laplacian, its output exponentially approaches the set of signals that carry information of the connected components and node degrees only for distinguishing nodes. Our theory enables us to relate the expressive power of GCNs with the topological information of the underlying graphs inherent in the graph spectra. To demonstrate this, we characterize the asymptotic behavior of GCNs on the Erd\H{o}s -- R\'{e}nyi graph. We show that when the Erd\H{o}s -- R\'{e}nyi graph is sufficiently dense and large, a broad range of GCNs on it suffers from the ""information loss"" in the limit of infinite layers with high probability. Based on the theory, we provide a principled guideline for weight normalization of graph NNs. We experimentally confirm that the proposed weight scaling enhances the predictive performance of GCNs in real data. Code is available at https://github.com/delta2323/gnn-asymptotics.",0
"Graph Neural Networks (graph NNs) offer a promising approach to deep learning for analyzing data structured in graphs. However, increasing the number of layers and adding non-linearity does not necessarily improve their predictive performance and can even make it worse. To address this issue, we examine the expressive power of graph NNs by studying their behavior as the layer size approaches infinity. Our approach involves generalizing the forward propagation of a popular graph NN variant, Graph Convolutional Network (GCN), as a specific dynamical system. By examining the spectra of the (augmented) normalized Laplacian, we show that a GCN's output can exponentially approach the set of signals that carry information on connected components and node degrees for distinguishing nodes. This theory enables us to relate GCN's expressive power to the topological information of the underlying graphs inherent in the graph spectra. We demonstrate this by characterizing the behavior of GCNs on the Erd\H{o}s -- R\'{e}nyi graph. We show that many GCNs on this graph suffer from ""information loss"" in the limit of infinite layers, especially when the graph is sufficiently dense and large. Based on our theory, we provide a principled guideline for weight normalization of graph NNs. We also confirm experimentally that our proposed weight scaling improves the predictive performance of GCNs on real data. Our code is available at https://github.com/delta2323/gnn-asymptotics.",1
"In recent years, ride-hailing services have been increasingly prevalent as they provide huge convenience for passengers. As a fundamental problem, the timely prediction of passenger demands in different regions is vital for effective traffic flow control and route planning. As both spatial and temporal patterns are indispensable passenger demand prediction, relevant research has evolved from pure time series to graph-structured data for modeling historical passenger demand data, where a snapshot graph is constructed for each time slot by connecting region nodes via different relational edges (e.g., origin-destination relationship, geographical distance, etc.). Consequently, the spatiotemporal passenger demand records naturally carry dynamic patterns in the constructed graphs, where the edges also encode important information about the directions and volume (i.e., weights) of passenger demands between two connected regions. However, existing graph-based solutions fail to simultaneously consider those three crucial aspects of dynamic, directed, and weighted (DDW) graphs, leading to limited expressiveness when learning graph representations for passenger demand prediction. Therefore, we propose a novel spatiotemporal graph attention network, namely Gallat (Graph prediction with all attention) as a solution. In Gallat, by comprehensively incorporating those three intrinsic properties of DDW graphs, we build three attention layers to fully capture the spatiotemporal dependencies among different regions across all historical time slots. Moreover, the model employs a subtask to conduct pretraining so that it can obtain accurate results more quickly. We evaluate the proposed model on real-world datasets, and our experimental results demonstrate that Gallat outperforms the state-of-the-art approaches.",0
"Ride-hailing services have become increasingly popular due to their convenience for passengers. However, predicting passenger demand in different regions is crucial for effective traffic flow control and route planning. Research has evolved from time series to graph-structured data, where a snapshot graph is constructed for each time slot by connecting region nodes via different relational edges. The graphs carry dynamic patterns, and the edges encode important information about the directions and volume of passenger demands. Existing graph-based solutions fail to consider the dynamic, directed, and weighted (DDW) graphs simultaneously, leading to limited expressiveness. To address this, we propose Gallat, a spatiotemporal graph attention network that comprehensively incorporates the intrinsic properties of DDW graphs. Gallat builds three attention layers to capture spatiotemporal dependencies among different regions across all historical time slots and employs pretraining to obtain accurate results more quickly. Experimental results on real-world datasets demonstrate that Gallat outperforms state-of-the-art approaches.",1
"The development of Graph Neural Networks (GNNs) has led to great progress in machine learning on graph-structured data. These networks operate via diffusing information across the graph nodes while capturing the structure of the graph. Recently there has also seen tremendous progress in quantum computing techniques. In this work, we explore applications of multi-particle quantum walks on diffusing information across graphs. Our model is based on learning the operators that govern the dynamics of quantum random walkers on graphs. We demonstrate the effectiveness of our method on classification and regression tasks.",0
"Significant advancements have been made in machine learning on graph-structured data due to the emergence of Graph Neural Networks (GNNs). These networks effectively capture the graph structure by diffusing information across its nodes. Furthermore, remarkable improvements in quantum computing techniques have been observed lately. In this study, we investigate the use of multi-particle quantum walks for information diffusion on graphs. Our approach involves acquiring knowledge of the operators that dictate the dynamics of quantum random walkers on graphs. We validate the efficacy of our approach by performing classification and regression tasks.",1
"Graph Neural Networks (GNNs) are the subject of intense focus by the machine learning community for problems involving relational reasoning. GNNs can be broadly divided into spatial and spectral approaches. Spatial approaches use a form of learned message-passing, in which interactions among vertices are computed locally, and information propagates over longer distances on the graph with greater numbers of message-passing steps. Spectral approaches use eigendecompositions of the graph Laplacian to produce a generalization of spatial convolutions to graph structured data which access information over short and long time scales simultaneously. Here we introduce the Spectral Graph Network, which applies message passing to both the spatial and spectral domains. Our model projects vertices of the spatial graph onto the Laplacian eigenvectors, which are each represented as vertices in a fully connected ""spectral graph"", and then applies learned message passing to them. We apply this model to various benchmark tasks including a graph-based variant of MNIST classification, molecular property prediction on MoleculeNet and QM9, and shortest path problems on random graphs. Our results show that the Spectral GN promotes efficient training, reaching high performance with fewer training iterations despite having more parameters. The model also provides robustness to edge dropout and outperforms baselines for the classification tasks. We also explore how these performance benefits depend on properties of the dataset.",0
"The machine learning community is intently focused on Graph Neural Networks (GNNs) for problems that require relational reasoning. These networks can be classified into two main approaches: spatial and spectral. Spatial approaches employ a form of learned message-passing, where interactions among vertices are calculated locally, and information propagates over longer distances on the graph with more message-passing steps. Spectral approaches, on the other hand, use eigendecompositions of the graph Laplacian to create a spatial convolution that accesses information over short and long time scales simultaneously. The Spectral Graph Network, which we introduce in this study, applies message passing to both the spatial and spectral domains. The vertices of the spatial graph are projected onto the Laplacian eigenvectors, which are each represented as vertices in a fully connected ""spectral graph,"" and then message passing is applied. We evaluate the model on various benchmark tasks, including a graph-based variant of MNIST classification, molecular property prediction on MoleculeNet and QM9, and shortest path problems on random graphs. Our results demonstrate that the Spectral GN facilitates effective training, achieving high performance with fewer training iterations, despite having more parameters. Additionally, the model is robust to edge dropout and outperforms baselines for the classification tasks. We also investigate how the performance benefits depend on the dataset properties.",1
"Graph Neural Networks (GNNs) have been shown to be effective models for different predictive tasks on graph-structured data. Recent work on their expressive power has focused on isomorphism tasks and countable feature spaces. We extend this theoretical framework to include continuous features - which occur regularly in real-world input domains and within the hidden layers of GNNs - and we demonstrate the requirement for multiple aggregation functions in this context. Accordingly, we propose Principal Neighbourhood Aggregation (PNA), a novel architecture combining multiple aggregators with degree-scalers (which generalize the sum aggregator). Finally, we compare the capacity of different models to capture and exploit the graph structure via a novel benchmark containing multiple tasks taken from classical graph theory, alongside existing benchmarks from real-world domains, all of which demonstrate the strength of our model. With this work, we hope to steer some of the GNN research towards new aggregation methods which we believe are essential in the search for powerful and robust models.",0
"Different predictive tasks on graph-structured data have been effectively modeled using Graph Neural Networks (GNNs). Recent studies have explored the expressive power of GNNs in isomorphism tasks and countable feature spaces. In this study, we expand the theoretical framework to include continuous features that are commonly found in real-world input domains and hidden layers of GNNs. We establish the necessity for multiple aggregation functions in this context and introduce a new architecture called Principal Neighbourhood Aggregation (PNA), which combines multiple aggregators with degree-scalers. We evaluate the model's ability to capture and utilize graph structure by introducing a novel benchmark that includes tasks from classical graph theory and existing benchmarks from real-world domains. Our results highlight the effectiveness and strength of our model. We aim to encourage GNN research to explore new aggregation methods that are crucial to developing powerful and robust models.",1
"When compared to unimodal systems, multimodal biometric systems have several advantages, including lower error rate, higher accuracy, and larger population coverage. However, multimodal systems have an increased demand for integrity and privacy because they must store multiple biometric traits associated with each user. In this paper, we present a deep learning framework for feature-level fusion that generates a secure multimodal template from each user's face and iris biometrics. We integrate a deep hashing (binarization) technique into the fusion architecture to generate a robust binary multimodal shared latent representation. Further, we employ a hybrid secure architecture by combining cancelable biometrics with secure sketch techniques and integrate it with a deep hashing framework, which makes it computationally prohibitive to forge a combination of multiple biometrics that pass the authentication. The efficacy of the proposed approach is shown using a multimodal database of face and iris and it is observed that the matching performance is improved due to the fusion of multiple biometrics. Furthermore, the proposed approach also provides cancelability and unlinkability of the templates along with improved privacy of the biometric data. Additionally, we also test the proposed hashing function for an image retrieval application using a benchmark dataset. The main goal of this paper is to develop a method for integrating multimodal fusion, deep hashing, and biometric security, with an emphasis on structural data from modalities like face and iris. The proposed approach is in no way a general biometric security framework that can be applied to all biometric modalities, as further research is needed to extend the proposed framework to other unconstrained biometric modalities.",0
"Multimodal biometric systems offer several advantages over unimodal systems, such as higher accuracy, lower error rates, and wider population coverage. However, they also require increased privacy and integrity measures as they store multiple biometric traits per user. In this research, we propose a deep learning framework that employs feature-level fusion to generate a secure multimodal template from a user's face and iris biometrics. Our approach integrates a deep hashing technique to create a robust binary multimodal shared latent representation. We also utilize cancelable biometrics and secure sketch techniques to enhance security and privacy. Our proposed method improves matching performance and offers cancelability and unlinkability of templates while preserving privacy of biometric data. We evaluate our approach on a multimodal database of face and iris and demonstrate improved performance. Although our approach emphasizes structural data from modalities such as face and iris, it is not a general biometric security framework and requires further research to extend it to other modalities. We also test our proposed hashing function for an image retrieval application using a benchmark dataset.",1
"In this paper, we propose a novel hierarchical representation via message propagation (HRMP) method for robust model fitting, which simultaneously takes advantages of both the consensus analysis and the preference analysis to estimate the parameters of multiple model instances from data corrupted by outliers, for robust model fitting. Instead of analyzing the information of each data point or each model hypothesis independently, we formulate the consensus information and the preference information as a hierarchical representation to alleviate the sensitivity to gross outliers. Specifically, we firstly construct a hierarchical representation, which consists of a model hypothesis layer and a data point layer. The model hypothesis layer is used to remove insignificant model hypotheses and the data point layer is used to remove gross outliers. Then, based on the hierarchical representation, we propose an effective hierarchical message propagation (HMP) algorithm and an improved affinity propagation (IAP) algorithm to prune insignificant vertices and cluster the remaining data points, respectively. The proposed HRMP can not only accurately estimate the number and parameters of multiple model instances, but also handle multi-structural data contaminated with a large number of outliers. Experimental results on both synthetic data and real images show that the proposed HRMP significantly outperforms several state-of-the-art model fitting methods in terms of fitting accuracy and speed.",0
"In this paper, we introduce the Hierarchical Representation via Message Propagation (HRMP) method for robust model fitting. The HRMP method utilizes both consensus and preference analysis to estimate parameters of multiple model instances from data corrupted by outliers. Rather than analyzing each data point or model hypothesis independently, the HRMP method formulates consensus and preference information as a hierarchical representation to reduce sensitivity to gross outliers. This representation consists of a model hypothesis layer and a data point layer, which work together to remove insignificant model hypotheses and gross outliers. Additionally, the HRMP method includes an effective Hierarchical Message Propagation (HMP) algorithm and an improved Affinity Propagation (IAP) algorithm to cluster remaining data points and prune insignificant vertices. The HRMP method accurately estimates the number and parameters of multiple model instances and can handle multi-structural data contaminated with a large number of outliers. Experimental results on synthetic data and real images demonstrate that the HRMP method outperforms several state-of-the-art model fitting methods in terms of fitting accuracy and speed.",1
"Semi-supervised node classification on graph-structured data has many applications such as fraud detection, fake account and review detection, user's private attribute inference in social networks, and community detection. Various methods such as pairwise Markov Random Fields (pMRF) and graph neural networks were developed for semi-supervised node classification. pMRF is more efficient than graph neural networks. However, existing pMRF-based methods are less accurate than graph neural networks, due to a key limitation that they assume a heuristics-based constant edge potential for all edges. In this work, we aim to address the key limitation of existing pMRF-based methods. In particular, we propose to learn edge potentials for pMRF. Our evaluation results on various types of graph datasets show that our optimized pMRF-based method consistently outperforms existing graph neural networks in terms of both accuracy and efficiency. Our results highlight that previous work may have underestimated the power of pMRF for semi-supervised node classification.",0
"The use of semi-supervised node classification on graph-structured data has numerous practical applications, including the detection of fraud, fake accounts, and reviews, as well as the inference of private attributes in social networks and community detection. Several methods, such as pairwise Markov Random Fields (pMRF) and graph neural networks, have been developed for this purpose. While pMRF is more efficient than graph neural networks, existing pMRF-based approaches are less accurate due to the assumption of a constant edge potential for all edges. Our research aims to overcome this limitation by proposing a method that learns edge potentials for pMRF. Our experiments on various graph datasets demonstrate that our optimized pMRF-based method consistently outperforms existing graph neural networks in terms of both accuracy and efficiency. This suggests that previous research may have underestimated the potential of pMRF for semi-supervised node classification.",1
"Mesh is a powerful data structure for 3D shapes. Representation learning for 3D meshes is important in many computer vision and graphics applications. The recent success of convolutional neural networks (CNNs) for structured data (e.g., images) suggests the value of adapting insight from CNN for 3D shapes. However, 3D shape data are irregular since each node's neighbors are unordered. Various graph neural networks for 3D shapes have been developed with isotropic filters or predefined local coordinate systems to overcome the node inconsistency on graphs. However, isotropic filters or predefined local coordinate systems limit the representation power. In this paper, we propose a local structure-aware anisotropic convolutional operation (LSA-Conv) that learns adaptive weighting matrices for each node according to the local neighboring structure and performs shared anisotropic filters. In fact, the learnable weighting matrix is similar to the attention matrix in the random synthesizer -- a new Transformer model for natural language processing (NLP). Comprehensive experiments demonstrate that our model produces significant improvement in 3D shape reconstruction compared to state-of-the-art methods.",0
"The use of Mesh as a data structure for 3D shapes is highly effective. In computer vision and graphics, the ability to learn representations for 3D meshes is crucial. While convolutional neural networks (CNNs) have proven successful for structured data such as images, the irregularity of 3D shape data poses a challenge due to the unordered nature of each node's neighbors. Various graph neural networks have been developed to address this issue, but they are limited in their ability to represent data. In this study, we introduce a local structure-aware anisotropic convolutional operation (LSA-Conv) that utilizes adaptive weighting matrices for each node and shared anisotropic filters. This weighting matrix is similar to the attention matrix in the random synthesizer, a Transformer model for natural language processing. Our experiments show that our model is highly effective in reconstructing 3D shapes and outperforms current state-of-the-art methods.",1
"Spectral graph convolutional networks are generalizations of standard convolutional networks for graph-structured data using the Laplacian operator. A common misconception is the instability of spectral filters, i.e. the impossibility to transfer spectral filters between graphs of variable size and topology. This misbelief has limited the development of spectral networks for multi-graph tasks in favor of spatial graph networks. However, recent works have proved the stability of spectral filters under graph perturbation. Our work complements and emphasizes further the high quality of spectral transferability by benchmarking spectral graph networks on tasks involving graphs of different size and connectivity. Numerical experiments exhibit favorable performance on graph regression, graph classification, and node classification problems on two graph benchmarks. The implementation of our experiments is available on GitHub for reproducibility.",0
"Spectral graph convolutional networks use the Laplacian operator to generalize standard convolutional networks for graph-structured data. A common misconception is that spectral filters are unstable and cannot be transferred between graphs of varying size and topology. This has hindered the development of spectral networks for multi-graph tasks, with spatial graph networks being preferred instead. However, recent research has shown that spectral filters are stable under graph perturbation, contradicting the misconception. Our study further emphasizes the transferability of spectral filters, demonstrating their quality through benchmarking spectral graph networks on tasks involving graphs of different sizes and connectivity. Our numerical experiments show favorable performance on graph regression, graph classification, and node classification problems on two graph benchmarks. We have made the implementation of our experiments available on GitHub for reproducibility purposes.",1
"Named Entity Recognition has been extensively investigated in many fields. However, the application of sensitive entity detection for production systems in financial institutions has not been well explored due to the lack of publicly available, labeled datasets. In this paper, we use internal and synthetic datasets to evaluate various methods of detecting NPI (Nonpublic Personally Identifiable) information commonly found within financial institutions, in both unstructured and structured data formats. Character-level neural network models including CNN, LSTM, BiLSTM-CRF, and CNN-CRF are investigated on two prediction tasks: (i) entity detection on multiple data formats, and (ii) column-wise entity prediction on tabular datasets. We compare these models with other standard approaches on both real and synthetic data, with respect to F1-score, precision, recall, and throughput. The real datasets include internal structured data and public email data with manually tagged labels. Our experimental results show that the CNN model is simple yet effective with respect to accuracy and throughput and thus, is the most suitable candidate model to be deployed in the production environment(s). Finally, we provide several lessons learned on data limitations, data labelling and the intrinsic overlap of data entities.",0
"Numerous investigations have been conducted on Named Entity Recognition in various fields, but the detection of sensitive entities for financial institutions has not been thoroughly explored due to the absence of publicly available labeled data. This study utilizes internal and synthetic data sets to assess the effectiveness of different methods for identifying Nonpublic Personally Identifiable (NPI) information found in structured and unstructured financial data. The study examines character-level neural network models such as CNN, LSTM, BiLSTM-CRF, and CNN-CRF on two prediction tasks: entity detection in multiple formats and column-wise entity prediction in tabular data sets. The models are compared to standard approaches on real and synthetic data with regards to F1-score, precision, recall, and throughput. The study utilizes both real and synthetic data sets, including internally structured data and publicly available email data with manually tagged labels. The study concludes that the CNN model is the most effective and efficient model for deployment in a production environment. The study also highlights data limitations, labeling issues, and the overlap of data entities as important takeaways.",1
"Graph representation learning has many real-world applications, from super-resolution imaging, 3D computer vision to drug repurposing, protein classification, social networks analysis. An adequate representation of graph data is vital to the learning performance of a statistical or machine learning model for graph-structured data. In this paper, we propose a novel multiscale representation system for graph data, called decimated framelets, which form a localized tight frame on the graph. The decimated framelet system allows storage of the graph data representation on a coarse-grained chain and processes the graph data at multi scales where at each scale, the data is stored at a subgraph. Based on this, we then establish decimated G-framelet transforms for the decomposition and reconstruction of the graph data at multi resolutions via a constructive data-driven filter bank. The graph framelets are built on a chain-based orthonormal basis that supports fast graph Fourier transforms. From this, we give a fast algorithm for the decimated G-framelet transforms, or FGT, that has linear computational complexity O(N) for a graph of size N. The theory of decimated framelets and FGT is verified with numerical examples for random graphs. The effectiveness is demonstrated by real-world applications, including multiresolution analysis for traffic network, and graph neural networks for graph classification tasks.",0
"The use of graph representation learning has many practical applications, such as in super-resolution imaging, 3D computer vision, drug repurposing, protein classification, and social network analysis. It is crucial to have an appropriate representation of graph data to ensure optimal learning performance by statistical or machine learning models for graph-structured data. In this study, we introduce a novel system for multiscale graph data representation, called decimated framelets, which form a localized tight frame on the graph. By storing the graph data representation on a coarse-grained chain and processing it at multiple scales, the decimated framelet system allows for subgraph storage at each scale. We then establish decimated G-framelet transforms for the decomposition and reconstruction of the graph data at multiple resolutions, using a constructive data-driven filter bank. The graph framelets are built on a chain-based orthonormal basis that supports fast graph Fourier transforms. We provide a fast algorithm for the decimated G-framelet transforms, or FGT, which has linear computational complexity O(N) for a graph of size N. The theory of decimated framelets and FGT is supported by numerical examples for random graphs. The effectiveness of this approach is demonstrated through real-world applications, including multiresolution analysis for traffic networks and the use of graph neural networks for graph classification tasks.",1
"Most of the existing coastal flood Forecast and Early-Warning Systems do not model the flood, but instead, rely on the prediction of hydrodynamic conditions at the coast and on expert judgment. Recent scientific contributions are now capable to precisely model flood events, even in situations where wave overtopping plays a significant role. Such models are nevertheless costly-to-evaluate and surrogate ones need to be exploited for substantial computational savings. For the latter models, the hydro-meteorological forcing conditions (inputs) or flood events (outputs) are conveniently parametrised into scalar representations. However, they neglect the fact that inputs are actually functions (more precisely, time series), and that floods spatially propagate inland. Here, we introduce a multi-output Gaussian process model accounting for both criteria. On various examples, we test its versatility for both learning spatial maps and inferring unobserved ones. We demonstrate that efficient implementations are obtained by considering tensor-structured data and/or sparse-variational approximations. Finally, the proposed framework is applied on a coastal application aiming at predicting flood events. We conclude that accurate predictions are obtained in the order of minutes rather than the couples of days required by dedicated hydrodynamic simulators.",0
"At present, many coastal flood Forecast and Early-Warning Systems do not model floods themselves, but rely on expert judgment and predictions of hydrodynamic conditions at the coast. However, recent scientific advancements have made it possible to accurately model flood events, even when wave overtopping is a significant factor. However, these models can be expensive to evaluate, and surrogate models can be used to save computational resources. These surrogate models use scalar representations of hydro-meteorological inputs and flood outputs, but do not account for the fact that inputs are actually time series and floods propagate inland. To address this, we propose a multi-output Gaussian process model that takes these factors into account. We test its versatility on various examples and demonstrate that efficient implementations can be achieved through the use of tensor-structured data and/or sparse-variational approximations. Finally, we apply our framework to a coastal application and find that it can accurately predict flood events in minutes, rather than the days required by dedicated hydrodynamic simulators.",1
"Class imbalance is a challenging issue in practical classification problems for deep learning models as well as for traditional models. Traditionally successful countermeasures such as synthetic over-sampling have had limited success with complex, structured data handled by deep learning models. In this work, we propose to use a Generative Adversarial Network (GAN) equipped with a generator network G, a discriminator network D and a classifier network C to remove the class-imbalance in visual data sets. The generator network is initialized with auto-encoder to make it stable. The discriminator D ensures that G adheres to class distribution of imbalanced class. In conventional methods, where Generator G competes with discriminator D in a min-max game, we propose to further add an additional classifier network to the original network. Now, the generator network tries to compete in a min-max game with Discriminator as well as the new classifier that we have introduced. An additional condition is enforced on generator network G to produce points in the convex hull of desired imbalanced class. Further the contention of adversarial game with classifier C, pushes conditional distribution learned by G towards the periphery of the respective class, compensating the problem of class imbalance. Experimental evidence shows that this initialization results in stable training of the network. We achieve state of the art performance on extreme visual classification task on the FashionMNIST, MNIST, SVHN, ExDark, MVTec Anomaly Detection dataset, Chest X-Ray dataset and others.",0
"Addressing class imbalance is a difficult problem for both traditional and deep learning models when dealing with practical classification problems. While synthetic over-sampling has been successful in the past, it has limited success when working with complex, structured data utilized by deep learning models. Our proposed solution is to use a Generative Adversarial Network (GAN) that includes a generator network (G), a discriminator network (D), and a classifier network (C) to balance class-imbalance in visual data sets. To ensure stability, we initialize the generator network with an auto-encoder. The discriminator network ensures that the generator network adheres to the class distribution of the imbalanced class. We also add an extra classifier network to the original network, forcing the generator network to compete in a min-max game against both the discriminator and the new classifier. Additionally, we enforce a condition on the generator network to produce points in the convex hull of the desired imbalanced class. By pushing the conditional distribution learned by the generator network towards the periphery of the respective class, the contention of the adversarial game with classifier C compensates for the problem of class-imbalance. Our experimental evidence demonstrates that this initialization leads to stable training of the network, ultimately achieving state-of-the-art performance on extreme visual classification tasks on the FashionMNIST, MNIST, SVHN, ExDark, MVTec Anomaly Detection dataset, Chest X-Ray dataset, and others.",1
"Deep Neural Network (DNN) models have vulnerabilities related to security concerns, with attackers usually employing complex hacking techniques to expose their structures. Data poisoning-enabled perturbation attacks are complex adversarial ones that inject false data into models. They negatively impact the learning process, with no benefit to deeper networks, as they degrade a model's accuracy and convergence rates. In this paper, we propose an attack-agnostic-based defense method for mitigating their influence. In it, a Defensive Feature Layer (DFL) is integrated with a well-known DNN architecture which assists in neutralizing the effects of illegitimate perturbation samples in the feature space. To boost the robustness and trustworthiness of this method for correctly classifying attacked input samples, we regularize the hidden space of a trained model with a discriminative loss function called Polarized Contrastive Loss (PCL). It improves discrimination among samples in different classes and maintains the resemblance of those in the same class. Also, we integrate a DFL and PCL in a compact model for defending against data poisoning attacks. This method is trained and tested using the CIFAR-10 and MNIST datasets with data poisoning-enabled perturbation attacks, with the experimental results revealing its excellent performance compared with those of recent peer techniques.",0
"DNN models are vulnerable to security threats, such as complex hacking techniques used by attackers to expose their structures. Among these adversarial attacks is the data poisoning-enabled perturbation attack, which injects false data into models and negatively impacts the learning process. To address this issue, we propose a defense method that integrates a Defensive Feature Layer (DFL) with a well-known DNN architecture to neutralize the effects of illegitimate perturbation samples in the feature space. Furthermore, we improve the method's robustness and trustworthiness by regularizing the hidden space of a trained model with a discriminative loss function known as Polarized Contrastive Loss (PCL). By improving discrimination among samples in different classes while maintaining the resemblance of those in the same class, this approach effectively defends against data poisoning attacks. We validate our method using the CIFAR-10 and MNIST datasets, and experimental results demonstrate its superior performance compared to recent peer techniques.",1
"Self-supervised learning is currently gaining a lot of attention, as it allows neural networks to learn robust representations from large quantities of unlabeled data. Additionally, multi-task learning can further improve representation learning by training networks simultaneously on related tasks, leading to significant performance improvements. In this paper, we propose three novel self-supervised auxiliary tasks to train graph-based neural network models in a multi-task fashion. Since Graph Convolutional Networks are among the most promising approaches for capturing relationships among structured data points, we use them as a building block to achieve competitive results on standard semi-supervised graph classification tasks.",0
"At present, self-supervised learning is receiving considerable focus due to its ability to enable neural networks to acquire strong representations from vast amounts of unlabeled data. Moreover, multi-task learning can enhance representation learning by training networks on correlated tasks concurrently, resulting in substantial performance enhancements. This paper puts forth three innovative self-supervised auxiliary tasks to train graph-based neural network models in a multi-task approach. To attain competitive outcomes on prevalent semi-supervised graph classification tasks, we employ Graph Convolutional Networks as a fundamental mechanism since they are among the most promising strategies for capturing connections among structured data points.",1
"Understanding the reasons for the success of deep neural networks trained using stochastic gradient-based methods is a key open problem for the nascent theory of deep learning. The types of data where these networks are most successful, such as images or sequences of speech, are characterised by intricate correlations. Yet, most theoretical work on neural networks does not explicitly model training data, or assumes that elements of each data sample are drawn independently from some factorised probability distribution. These approaches are thus by construction blind to the correlation structure of real-world data sets and their impact on learning in neural networks. Here, we introduce a generative model for structured data sets that we call the hidden manifold model (HMM). The idea is to construct high-dimensional inputs that lie on a lower-dimensional manifold, with labels that depend only on their position within this manifold, akin to a single layer decoder or generator in a generative adversarial network. We demonstrate that learning of the hidden manifold model is amenable to an analytical treatment by proving a ""Gaussian Equivalence Property"" (GEP), and we use the GEP to show how the dynamics of two-layer neural networks trained using one-pass stochastic gradient descent is captured by a set of integro-differential equations that track the performance of the network at all times. This permits us to analyse in detail how a neural network learns functions of increasing complexity during training, how its performance depends on its size and how it is impacted by parameters such as the learning rate or the dimension of the hidden manifold.",0
"A significant issue for the emerging theory of deep learning is comprehending why deep neural networks trained using stochastic gradient-based methods are successful. The data types where these networks are most effective, such as images or speech sequences, have complex correlations, but theoretical work on neural networks often does not factor in training data or assumes that each element of a data sample is drawn independently from a factorised probability distribution. These methods are thus ignorant of the correlation structure of real-world data sets and their influence on learning in neural networks. In this study, we introduce a generative model called the hidden manifold model (HMM) for structured data sets. The HMM creates high-dimensional inputs that exist on a lower-dimensional manifold with labels that rely only on their position within this manifold. We prove a ""Gaussian Equivalence Property"" (GEP) that allows us to examine in-depth the learning process of two-layer neural networks trained using one-pass stochastic gradient descent. We can observe how a neural network learns functions of increasing complexity during training, how its performance depends on its size, and how it is affected by parameters like the learning rate or the dimension of the hidden manifold.",1
"Many modern data analyses benefit from explicitly modeling dependence structure in data -- such as measurements across time or space, ordered words in a sentence, or genes in a genome. A gold standard evaluation technique is structured cross-validation (CV), which leaves out some data subset (such as data within a time interval or data in a geographic region) in each fold. But CV here can be prohibitively slow due to the need to re-run already-expensive learning algorithms many times. Previous work has shown approximate cross-validation (ACV) methods provide a fast and provably accurate alternative in the setting of empirical risk minimization. But this existing ACV work is restricted to simpler models by the assumptions that (i) data across CV folds are independent and (ii) an exact initial model fit is available. In structured data analyses, both these assumptions are often untrue. In the present work, we address (i) by extending ACV to CV schemes with dependence structure between the folds. To address (ii), we verify -- both theoretically and empirically -- that ACV quality deteriorates smoothly with noise in the initial fit. We demonstrate the accuracy and computational benefits of our proposed methods on a diverse set of real-world applications.",0
"Explicitly modeling dependence structure in data is beneficial for modern data analyses, such as measurements across time or space, ordered words in a sentence, or genes in a genome. A gold standard evaluation technique is structured cross-validation (CV), where a data subset (such as data within a time interval or data in a geographic region) is left out in each fold. However, CV can be too slow due to the need to re-run expensive learning algorithms several times. Previous work has shown that approximate cross-validation (ACV) methods are a fast and accurate alternative, but they are restricted to simpler models that assume independent data across CV folds and an exact initial model fit. In structured data analyses, these assumptions are often false. This work extends ACV to CV schemes with dependence structure between the folds and addresses the deterioration of ACV quality with noise in the initial fit. The proposed methods are demonstrated to be accurate and computationally efficient on a diverse set of real-world applications.",1
"We consider the problem of learning a manifold from a teacher's demonstration. Extending existing approaches of learning from randomly sampled data points, we consider contexts where data may be chosen by a teacher. We analyze learning from teachers who can provide structured data such as individual examples (isolated data points) and demonstrations (sequences of points). Our analysis shows that for the purpose of teaching the topology of a manifold, demonstrations can yield remarkable decreases in the amount of data points required in comparison to teaching with randomly sampled points. We also discuss the implications of our analysis for learning in 0s and machines.",0
"The focus of our study is on acquiring knowledge about a manifold through a teacher's guidance. In contrast to current methods of gathering information from randomly chosen data points, our investigation examines situations where the teacher selects the data. We explore the effectiveness of learning from teachers who can provide structured data in the form of individual examples or sequences of points. Through our evaluation, we observe that using demonstrations can considerably reduce the number of data points required to teach the manifold's topology, compared to learning with randomly sampled points. We also discuss the impact of our findings on both 0 and machine learning.",1
"Graph-structured data exist in numerous applications in real life. As a state-of-the-art graph neural network, the graph convolutional network (GCN) plays an important role in processing graph-structured data. However, a recent study reported that GCNs are also vulnerable to adversarial attacks, which means that GCN models may suffer malicious attacks with unnoticeable modifications of the data. Among all the adversarial attacks on GCNs, there is a special kind of attack method called the universal adversarial attack, which generates a perturbation that can be applied to any sample and causes GCN models to output incorrect results. Although universal adversarial attacks in computer vision have been extensively researched, there are few research works on universal adversarial attacks on graph structured data. In this paper, we propose a targeted universal adversarial attack against GCNs. Our method employs a few nodes as the attack nodes. The attack capability of the attack nodes is enhanced through a small number of fake nodes connected to them. During an attack, any victim node will be misclassified by the GCN as the attack node class as long as it is linked to them. The experiments on three popular datasets show that the average attack success rate of the proposed attack on any victim node in the graph reaches 83% when using only 3 attack nodes and 6 fake nodes. We hope that our work will make the community aware of the threat of this type of attack and raise the attention given to its future defense.",0
"Graph-structured data is ubiquitous in real-world applications, and the graph convolutional network (GCN) is a cutting-edge graph neural network that plays a vital role in processing such data. However, a recent study revealed that GCNs are susceptible to adversarial attacks, meaning that malicious alterations to the data may cause GCN models to suffer. One type of attack that poses a significant threat to GCNs is the universal adversarial attack, which generates a perturbation that can be applied to any sample and cause GCN models to produce incorrect results. Although universal adversarial attacks in computer vision have been extensively researched, little research has been conducted on universal adversarial attacks on graph-structured data. Therefore, we propose a targeted universal adversarial attack against GCNs in this paper. Our method enhances the attack nodes' attack capabilities by connecting them to a small number of fake nodes. During an attack, any victim node connected to the attack nodes will be misclassified by the GCN as the attack node class. The results of our experiments on three popular datasets demonstrate that the proposed attack achieves an average attack success rate of 83% on any victim node in the graph using only three attack nodes and six fake nodes. We hope that our work will raise awareness of the threat posed by this type of attack and encourage further research into its defense.",1
"Graph Neural Networks (GNNs) are a popular approach for predicting graph structured data. As GNNs tightly entangle the input graph into the neural network structure, common explainable AI approaches are not applicable. To a large extent, GNNs have remained black-boxes for the user so far. In this paper, we show that GNNs can in fact be naturally explained using higher-order expansions, i.e. by identifying groups of edges that jointly contribute to the prediction. Practically, we find that such explanations can be extracted using a nested attribution scheme, where existing techniques such as layer-wise relevance propagation (LRP) can be applied at each step. The output is a collection of walks into the input graph that are relevant for the prediction. Our novel explanation method, which we denote by GNN-LRP, is applicable to a broad range of graph neural networks and lets us extract practically relevant insights on sentiment analysis of text data, structure-property relationships in quantum chemistry, and image classification.",0
"Graph Neural Networks (GNNs) have become a popular method for predicting data with graph structures. However, due to the integration of the input graph into the neural network structure, conventional explainable AI methods cannot be used with GNNs. This has resulted in GNNs being viewed as black-boxes by users. In this study, we demonstrate that higher-order expansions can be used to explain GNNs naturally. This involves identifying groups of edges that contribute jointly to a prediction. We have developed a novel explanation technique called GNN-LRP, which uses a nested attribution approach to extract relevant information from the input graph. This method is broadly applicable to various graph neural networks and has been used to extract insights on text data sentiment analysis, quantum chemistry's structure-property relationships, and image classification.",1
"Trust and credibility in machine learning models is bolstered by the ability of a model to explain itsdecisions. While explainability of deep learning models is a well-known challenge, a further chal-lenge is clarity of the explanation itself, which must be interpreted by downstream users. Layer-wiseRelevance Propagation (LRP), an established explainability technique developed for deep models incomputer vision, provides intuitive 0-readable heat maps of input images. We present the novelapplication of LRP for the first time with structured datasets using a deep neural network (1D-CNN),for Credit Card Fraud detection and Telecom Customer Churn prediction datasets. We show how LRPis more effective than traditional explainability concepts of Local Interpretable Model-agnostic Ex-planations (LIME) and Shapley Additive Explanations (SHAP) for explainability. This effectivenessis both local to a sample level and holistic over the whole testing set. We also discuss the significantcomputational time advantage of LRP (1-2s) over LIME (22s) and SHAP (108s), and thus its poten-tial for real time application scenarios. In addition, our validation of LRP has highlighted features forenhancing model performance, thus opening up a new area of research of using XAI as an approachfor feature subset selection",0
"The trustworthiness and credibility of machine learning models can be enhanced by their ability to justify their decisions. Although explaining deep learning models is a well-known challenge, another problem is ensuring the clarity of the explanation that downstream users must interpret. Layer-wise Relevance Propagation (LRP) is a well-established technique for explaining deep models in computer vision by generating 0-readable heat maps of the input images. In this study, we introduce the novel application of LRP to structured datasets using a deep neural network (1D-CNN) for predicting Credit Card Fraud and Telecom Customer Churn. Our results demonstrate that LRP is more effective than traditional explainability concepts, such as Local Interpretable Model-agnostic Explanations (LIME) and Shapley Additive Explanations (SHAP), for explaining model decisions both locally and holistically. Moreover, LRP has a considerable computational advantage over LIME and SHAP, making it suitable for real-time applications. Additionally, our study demonstrates the potential of using XAI as an approach for feature subset selection, which can enhance model performance.",1
"Deep Learning architectures, albeit successful in most computer vision tasks, were designed for data with an underlying Euclidean structure, which is not usually fulfilled since pre-processed data may lie on a non-linear space. In this paper, we propose a geometry aware deep learning approach for skeleton-based action recognition. Skeleton sequences are first modeled as trajectories on Kendall's shape space and then mapped to the linear tangent space. The resulting structured data are then fed to a deep learning architecture, which includes a layer that optimizes over rigid and non rigid transformations of the 3D skeletons, followed by a CNN-LSTM network. The assessment on two large scale skeleton datasets, namely NTU-RGB+D and NTU-RGB+D 120, has proven that proposed approach outperforms existing geometric deep learning methods and is competitive with respect to recently published approaches.",0
"Despite their success in most computer vision tasks, Deep Learning architectures were originally intended for data with a Euclidean structure. However, pre-processed data often exists on a non-linear space, which makes these architectures inappropriate. To address this, we present a geometry-aware deep learning approach for skeleton-based action recognition. Firstly, we model skeleton sequences as trajectories on Kendall's shape space and then map them to the linear tangent space. The resulting structured data is then processed by a deep learning architecture, which includes a layer that optimizes for rigid and non-rigid transformations of the 3D skeletons, followed by a CNN-LSTM network. Our assessment on two large scale skeleton datasets, NTU-RGB+D and NTU-RGB+D 120, demonstrates that our approach outperforms existing geometric deep learning methods and is competitive with recently published methods.",1
"Time series are all around in real-world applications. However, unexpected accidents for example broken sensors or missing of the signals will cause missing values in time series, making the data hard to be utilized. It then does harm to the downstream applications such as traditional classification or regression, sequential data integration and forecasting tasks, thus raising the demand for data imputation. Currently, time series data imputation is a well-studied problem with different categories of methods. However, these works rarely take the temporal relations among the observations and treat the time series as normal structured data, losing the information from the time data. In recent, deep learning models have raised great attention. Time series methods based on deep learning have made progress with the usage of models like RNN, since it captures time information from data. In this paper, we mainly focus on time series imputation technique with deep learning methods, which recently made progress in this field. We will review and discuss their model architectures, their pros and cons as well as their effects to show the development of the time series imputation methods.",0
"Time series are prevalent in various real-world applications, but unexpected occurrences like broken sensors or signal loss can lead to missing values in the data, making it challenging to utilize. This can adversely affect downstream applications such as traditional classification or regression, sequential data integration, and forecasting tasks, necessitating the need for data imputation. Although time series data imputation has been extensively studied, the existing methods generally overlook the temporal relationships among observations, treating time series as regular structured data and losing valuable time data information. Recently, deep learning models like RNN have gained significant attention in time series imputation methods as they capture time information from data. This paper focuses on reviewing and discussing the model architectures, pros and cons, and effects of deep learning-based time series imputation techniques that have recently made progress in this field.",1
"Graph neural networks (GNNs) are a popular class of parametric model for learning over graph-structured data. Recent work has argued that GNNs primarily use the graph for feature smoothing, and have shown competitive results on benchmark tasks by simply operating on graph-smoothed node features, rather than using end-to-end learned feature hierarchies that are challenging to scale to large graphs. In this work, we ask whether these results can be extended to heterogeneous graphs, which encode multiple types of relationship between different entities. We propose Neighbor Averaging over Relation Subgraphs (NARS), which trains a classifier on neighbor-averaged features for randomly-sampled subgraphs of the ""metagraph"" of relations. We describe optimizations to allow these sets of node features to be computed in a memory-efficient way, both at training and inference time. NARS achieves a new state of the art accuracy on several benchmark datasets, outperforming more expensive GNN-based methods",0
"A prevalent type of parametric model for learning over graph-structured data is Graph neural networks (GNNs). Recent research suggests that GNNs primarily employ the graph for feature smoothing, leading to competitive results on standard tasks by solely operating on graph-smoothed node features instead of relying on challenging-to-scale end-to-end learned feature hierarchies. This study aims to extend these results to heterogeneous graphs, which contain multiple types of relationships between various entities. To achieve this, we propose Neighbor Averaging over Relation Subgraphs (NARS), which trains a classifier on neighbor-averaged features for randomly-sampled subgraphs of the ""metagraph"" of relations. We also present optimizations that enable these sets of node features to be computed memory-efficiently during both training and inference. NARS surpasses more expensive GNN-based methods and achieves a new state of the art accuracy on several benchmark datasets.",1
"A model involving Gaussian processes (GPs) is introduced to simultaneously handle multi-task learning, clustering, and prediction for multiple functional data. This procedure acts as a model-based clustering method for functional data as well as a learning step for subsequent predictions for new tasks. The model is instantiated as a mixture of multi-task GPs with common mean processes. A variational EM algorithm is derived for dealing with the optimisation of the hyper-parameters along with the hyper-posteriors' estimation of latent variables and processes. We establish explicit formulas for integrating the mean processes and the latent clustering variables within a predictive distribution, accounting for uncertainty on both aspects. This distribution is defined as a mixture of cluster-specific GP predictions, which enhances the performances when dealing with group-structured data. The model handles irregular grid of observations and offers different hypotheses on the covariance structure for sharing additional information across tasks. The performances on both clustering and prediction tasks are assessed through various simulated scenarios and real datasets. The overall algorithm, called MagmaClust, is publicly available as an R package.",0
"Introducing a model that utilizes Gaussian processes (GPs) to address multi-task learning, clustering, and prediction for multiple functional data simultaneously. This approach serves as a model-based clustering technique for functional data and a learning step to predict new tasks. The model comprises a mixture of multi-task GPs with common mean processes, and a variational EM algorithm is utilized to optimize the hyper-parameters and estimate the hyper-posteriors of latent variables and processes. Explicit formulas are developed to incorporate the mean processes and latent clustering variables into a predictive distribution, which accounts for uncertainty in both aspects. This distribution is defined as a mixture of cluster-specific GP predictions, which enhances performance when handling group-structured data. The model accommodates irregular observation grids and proposes various hypotheses on the covariance structure to share additional information across tasks. The model's performance is evaluated through simulated and real datasets, and the overall algorithm, named MagmaClust, is publicly available as an R package.",1
"Graph neural networks~(GNNs) apply deep learning techniques to graph-structured data and have achieved promising performance in graph representation learning. However, existing GNNs rely heavily on enough labels or well-designed negative samples. To address these issues, we propose a new self-supervised graph representation method: deep graph bootstrapping~(DGB). DGB consists of two neural networks: online and target networks, and the input of them are different augmented views of the initial graph. The online network is trained to predict the target network while the target network is updated with a slow-moving average of the online network, which means the online and target networks can learn from each other. As a result, the proposed DGB can learn graph representation without negative examples in an unsupervised manner. In addition, we summarize three kinds of augmentation methods for graph-structured data and apply them to the DGB. Experiments on the benchmark datasets show the DGB performs better than the current state-of-the-art methods and how the augmentation methods affect the performances.",0
"The application of deep learning techniques to graph-structured data has led to the development of Graph neural networks (GNNs) which have shown great potential in graph representation learning. However, the current GNNs heavily rely on sufficient labeling or negative samples that are well-designed. To resolve these challenges, we propose a novel self-supervised graph representation approach known as deep graph bootstrapping (DGB). DGB comprises two neural networks, the online and target networks, which receive different augmented views of the initial graph as inputs. The online network is trained to predict the target network while the target network is updated with a slow-moving average of the online network. This facilitates learning from each other, enabling the DGB to learn graph representation unsupervised without negative examples. Moreover, we present three augmentation techniques for graph-structured data and apply them to the DGB. Our experiments on benchmark datasets demonstrate that DGB outperforms current state-of-the-art methods and how the augmentation methods influence the performances.",1
"Discrete structures play an important role in applications like program language modeling and software engineering. Current approaches to predicting complex structures typically consider autoregressive models for their tractability, with some sacrifice in flexibility. Energy-based models (EBMs) on the other hand offer a more flexible and thus more powerful approach to modeling such distributions, but require partition function estimation. In this paper we propose ALOE, a new algorithm for learning conditional and unconditional EBMs for discrete structured data, where parameter gradients are estimated using a learned sampler that mimics local search. We show that the energy function and sampler can be trained efficiently via a new variational form of power iteration, achieving a better trade-off between flexibility and tractability. Experimentally, we show that learning local search leads to significant improvements in challenging application domains. Most notably, we present an energy model guided fuzzer for software testing that achieves comparable performance to well engineered fuzzing engines like libfuzzer.",0
"The role of discrete structures is significant in various applications, such as software engineering and program language modeling. To predict complex structures, autoregressive models are commonly used due to their tractability despite their limited flexibility. However, energy-based models (EBMs) offer a more flexible and powerful approach to modeling such distributions, but require partition function estimation. This paper introduces ALOE, a novel algorithm for learning conditional and unconditional EBMs for discrete structured data. The algorithm estimates parameter gradients using a learned sampler that imitates local search. The energy function and sampler are trained efficiently using a new variational form of power iteration, which achieves a better balance between flexibility and tractability. The experimental results demonstrate that learning local search results in significant improvements, particularly in challenging domains. Notably, the paper presents an energy model guided fuzzer for software testing that performs comparably to well-engineered fuzzing engines such as libfuzzer.",1
"Graph-structured data are an integral part of many application domains, including chemoinformatics, computational biology, neuroimaging, and social network analysis. Over the last two decades, numerous graph kernels, i.e. kernel functions between graphs, have been proposed to solve the problem of assessing the similarity between graphs, thereby making it possible to perform predictions in both classification and regression settings. This manuscript provides a review of existing graph kernels, their applications, software plus data resources, and an empirical comparison of state-of-the-art graph kernels.",0
"Many fields rely on graph-structured data, such as social network analysis, neuroimaging, computational biology, and chemoinformatics. To evaluate the similarity between graphs and enable predictions in both classification and regression settings, graph kernels or kernel functions have been suggested. These have been proposed over the past 20 years, and this article offers an overview of existing graph kernels, their applicability, data resources, software, and a comparative study of advanced graph kernels.",1
"In recent years, developing a speech understanding system that classifies a waveform to structured data, such as intents and slots, without first transcribing the speech to text has emerged as an interesting research problem. This work proposes such as system with an additional constraint of designing a system that has a small enough footprint to run on small micro-controllers and embedded systems with minimal latency. Given a streaming input speech signal, the proposed system can process it segment-by-segment without the need to have the entire stream at the moment of processing. The proposed system is evaluated on the publicly available Fluent Speech Commands dataset. Experiments show that the proposed system yields state-of-the-art performance with the advantage of low latency and a much smaller model when compared to other published works on the same task.",0
"The development of a speech understanding system that can classify waveforms into structured data, like intents and slots, without first transcribing speech to text has recently become an intriguing research problem. This study introduces a system with an additional requirement of being small enough to run on micro-controllers and embedded systems with minimal latency. The proposed system can process a streaming input speech signal in segments, without requiring the entire stream at the time of processing. The system is evaluated using the Fluent Speech Commands dataset and experiments demonstrate that it achieves state-of-the-art performance while also having low latency and a much smaller model than other published works on this task.",1
"Optimal Transport is a theory that allows to define geometrical notions of distance between probability distributions and to find correspondences, relationships, between sets of points. Many machine learning applications are derived from this theory, at the frontier between mathematics and optimization. This thesis proposes to study the complex scenario in which the different data belong to incomparable spaces. In particular we address the following questions: how to define and apply Optimal Transport between graphs, between structured data? How can it be adapted when the data are varied and not embedded in the same metric space? This thesis proposes a set of Optimal Transport tools for these different cases. An important part is notably devoted to the study of the Gromov-Wasserstein distance whose properties allow to define interesting transport problems on incomparable spaces. More broadly, we analyze the mathematical properties of the various proposed tools, we establish algorithmic solutions to compute them and we study their applicability in numerous machine learning scenarii which cover, in particular, classification, simplification, partitioning of structured data, as well as heterogeneous domain adaptation.",0
"The theory of Optimal Transport enables the definition of geometrical concepts of distance between probability distributions and the establishment of relationships between sets of points, which has led to numerous machine learning applications at the intersection of mathematics and optimization. This thesis aims to explore the intricate scenario where different data exist in incomparable spaces. Specifically, it seeks to address questions such as: how to apply Optimal Transport to graphs and structured data, and how to adapt it to diverse data types that are not inherently embedded in the same metric space? To this end, the thesis proposes a range of Optimal Transport tools for these varied cases, with a significant focus on the Gromov-Wasserstein distance, which can define interesting transport problems on incomparable spaces. The thesis delves into the mathematical properties of these tools, provides algorithmic solutions for their computation, and investigates their applicability in various machine learning scenarios, such as classification, simplification, partitioning of structured data, and heterogeneous domain adaptation.",1
"Modern machine learning applications should be able to address the intrinsic challenges arising over inference on massive real-world datasets, including scalability and robustness to outliers. Despite the multiple benefits of Bayesian methods (such as uncertainty-aware predictions, incorporation of experts knowledge, and hierarchical modeling), the quality of classic Bayesian inference depends critically on whether observations conform with the assumed data generating model, which is impossible to guarantee in practice. In this work, we propose a variational inference method that, in a principled way, can simultaneously scale to large datasets, and robustify the inferred posterior with respect to the existence of outliers in the observed data. Reformulating Bayes theorem via the $\beta$-divergence, we posit a robustified pseudo-Bayesian posterior as the target of inference. Moreover, relying on the recent formulations of Riemannian coresets for scalable Bayesian inference, we propose a sparse variational approximation of the robustified posterior and an efficient stochastic black-box algorithm to construct it. Overall our method allows releasing cleansed data summaries that can be applied broadly in scenarios including structured data corruption. We illustrate the applicability of our approach in diverse simulated and real datasets, and various statistical models, including Gaussian mean inference, logistic and neural linear regression, demonstrating its superiority to existing Bayesian summarization methods in the presence of outliers.",0
"To effectively handle the challenges involved in analyzing large real-world datasets, modern machine learning applications need to be scalable and robust to outliers. While Bayesian methods offer many advantages, such as incorporating expert knowledge and providing uncertainty-aware predictions, their effectiveness depends on whether the observed data aligns with the assumed data generating model, which is often not the case in practical scenarios. In this study, we introduce a variational inference method that can scale to large datasets and strengthen the inferred posterior against the presence of outliers in the observed data. By using the $\beta$-divergence to reformulate Bayes theorem, we develop a robustified pseudo-Bayesian posterior for inference. Additionally, we leverage recent advancements in Riemannian coresets to propose a sparse variational approximation of the robustified posterior and an efficient stochastic black-box algorithm to construct it. Our approach generates cleansed data summaries that can be broadly applied to scenarios involving structured data corruption. We demonstrate the effectiveness of our method in various statistical models, including Gaussian mean inference, logistic and neural linear regression, on simulated and real datasets, highlighting its superiority over existing Bayesian summarization methods in the presence of outliers.",1
"The ability to detect and count certain substructures in graphs is important for solving many tasks on graph-structured data, especially in the contexts of computational chemistry and biology as well as social network analysis. Inspired by this, we propose to study the expressive power of graph neural networks (GNNs) via their ability to count attributed graph substructures, extending recent works that examine their power in graph isomorphism testing and function approximation. We distinguish between two types of substructure counting: induced-subgraph-count and subgraph-count, and establish both positive and negative answers for popular GNN architectures. Specifically, we prove that Message Passing Neural Networks (MPNNs), 2-Weisfeiler-Lehman (2-WL) and 2-Invariant Graph Networks (2-IGNs) cannot perform induced-subgraph-count of substructures consisting of 3 or more nodes, while they can perform subgraph-count of star-shaped substructures. As an intermediary step, we prove that 2-WL and 2-IGNs are equivalent in distinguishing non-isomorphic graphs, partly answering an open problem raised in Maron et al. (2019). We also prove positive results for k-WL and k-IGNs as well as negative results for k-WL with a finite number of iterations. We then conduct experiments that support the theoretical results for MPNNs and 2-IGNs. Moreover, motivated by substructure counting and inspired by Murphy et al. (2019), we propose the Local Relational Pooling model and demonstrate that it is not only effective for substructure counting but also able to achieve competitive performance on molecular prediction tasks.",0
"The ability to identify and quantify certain substructures within graphs is crucial for solving various tasks involving graph-structured data, particularly in computational chemistry, biology, and social network analysis. In light of this, we aim to explore the expressive capabilities of graph neural networks (GNNs) by investigating their capacity to count attributed graph substructures. This builds on recent research that has focused on their power in graph isomorphism testing and function approximation. We differentiate between two forms of substructure counting - induced-subgraph-count and subgraph-count - and provide both positive and negative outcomes for popular GNN architectures. Our findings reveal that Message Passing Neural Networks (MPNNs), 2-Weisfeiler-Lehman (2-WL), and 2-Invariant Graph Networks (2-IGNs) cannot perform induced-subgraph-count of substructures comprising 3 or more nodes, but can perform subgraph-count of star-shaped substructures. We also demonstrate that 2-WL and 2-IGNs are equivalent in distinguishing non-isomorphic graphs, partially answering an open question raised in Maron et al. (2019). Additionally, we present positive outcomes for k-WL and k-IGNs, as well as negative results for k-WL with a finite number of iterations. We support our theoretical results through experiments with MPNNs and 2-IGNs. Furthermore, we introduce the Local Relational Pooling model, which is effective for substructure counting, and demonstrates competitive performance on molecular prediction tasks, inspired by substructure counting and motivated by Murphy et al. (2019).",1
"We propose a flexible framework for clustering hypergraph-structured data based on recently proposed random walks utilizing edge-dependent vertex weights. When incorporating edge-dependent vertex weights (EDVW), a weight is associated with each vertex-hyperedge pair, yielding a weighted incidence matrix of the hypergraph. Such weightings have been utilized in term-document representations of text data sets. We explain how random walks with EDVW serve to construct different hypergraph Laplacian matrices, and then develop a suite of clustering methods that use these incidence matrices and Laplacians for hypergraph clustering. Using several data sets from real-life applications, we compare the performance of these clustering algorithms experimentally against a variety of existing hypergraph clustering methods. We show that the proposed methods produce higher-quality clusters and conclude by highlighting avenues for future work.",0
"Our proposal introduces a flexible framework to cluster hypergraph-structured data through random walks that utilize edge-dependent vertex weights. By assigning a weight to each vertex-hyperedge pair, we create a weighted incidence matrix for the hypergraph, which is commonly used in term-document representations of text data sets. We demonstrate how random walks with these weights can construct multiple hypergraph Laplacian matrices, and from there, we develop a set of clustering methods that leverage these matrices for hypergraph clustering. Through experiments with several real-life data sets, we compare the performance of these methods against existing hypergraph clustering techniques and find that our methods yield higher-quality clusters. We conclude by identifying potential areas for future research.",1
"The recent introduction of Graph Neural Networks (GNNs) and their growing popularity in the past few years has enabled the application of deep learning algorithms to non-Euclidean, graph-structured data. GNNs have achieved state-of-the-art results across an impressive array of graph-based machine learning problems. Nevertheless, despite their rapid pace of development, much of the work on GNNs has focused on graph classification and embedding techniques, largely ignoring regression tasks over graph data. In this paper, we develop a Graph Mixture Density Network (GraphMDN), which combines graph neural networks with mixture density network (MDN) outputs. By combining these techniques, GraphMDNs have the advantage of naturally being able to incorporate graph structured information into a neural architecture, as well as the ability to model multi-modal regression targets. As such, GraphMDNs are designed to excel on regression tasks wherein the data are graph structured, and target statistics are better represented by mixtures of densities rather than singular values (so-called ``inverse problems""). To demonstrate this, we extend an existing GNN architecture known as Semantic GCN (SemGCN) to a GraphMDN structure, and show results from the 03.6M pose estimation task. The extended model consistently outperforms both GCN and MDN architectures on their own, with a comparable number of parameters.",0
"The increasing popularity of Graph Neural Networks (GNNs) has made it possible to employ deep learning algorithms in processing non-Euclidean, graph-based data. GNNs have achieved impressive results in various machine learning problems that use graph as their basis. However, most of the research on GNNs has concentrated on graph classification and embedding techniques, neglecting regression tasks that involve graph data. This paper introduces a new approach called Graph Mixture Density Network (GraphMDN), which combines graph neural networks with mixture density network (MDN) outputs. The GraphMDN technique is advantageous in integrating graph-based information into a neural architecture and modeling multi-modal regression targets. It is designed to perform better in regression tasks where the data has a graph structure and the target statistics are better represented by mixtures of densities rather than singular values. The paper extends the existing Semantic GCN (SemGCN) architecture to a GraphMDN structure and demonstrates the results of 03.6M pose estimation task. The extended model outperforms both GCN and MDN architectures independently while having a comparable number of parameters.",1
"Graph Neural Networks achieve remarkable results on problems with structured data but come as black-box predictors. Transferring existing explanation techniques, such as occlusion, fails as even removing a single node or edge can lead to drastic changes in the graph. The resulting graphs can differ from all training examples, causing model confusion and wrong explanations. Thus, we argue that explicability must use graphs compliant with the distribution underlying the training data. We coin this property Distribution Compliant Explanation (DCE) and present a novel Contrastive GNN Explanation (CoGE) technique following this paradigm. An experimental study supports the efficacy of CoGE.",0
"Although Graph Neural Networks excel in solving problems with structured data, they act as opaque predictors. The traditional explanation methods, including occlusion, are not effective because the elimination of a single node or edge can cause significant changes in the graph. Consequently, the resulting graphs may differ from the training examples, leading to model confusion and inaccurate explanations. Therefore, we propose that graphs adhering to the distribution of the training data must be used for explicability. We refer to this property as Distribution Compliant Explanation (DCE) and introduce a new Contrastive GNN Explanation (CoGE) methodology that aligns with this principle. Our experimental study validates the effectiveness of CoGE.",1
"Graph, as an important data representation, is ubiquitous in many real world applications ranging from social network analysis to biology. How to correctly and effectively learn and extract information from graph is essential for a large number of machine learning tasks. Graph embedding is a way to transform and encode the data structure in high dimensional and non-Euclidean feature space to a low dimensional and structural space, which is easily exploited by other machine learning algorithms. We have witnessed a huge surge of such embedding methods, from statistical approaches to recent deep learning methods such as the graph convolutional networks (GCN). Deep learning approaches usually outperform the traditional methods in most graph learning benchmarks by building an end-to-end learning framework to optimize the loss function directly. However, most of the existing GCN methods can only perform convolution operations with node features, while ignoring the handy information in edge features, such as relations in knowledge graphs. To address this problem, we present CensNet, Convolution with Edge-Node Switching graph neural network, for learning tasks in graph-structured data with both node and edge features. CensNet is a general graph embedding framework, which embeds both nodes and edges to a latent feature space. By using line graph of the original undirected graph, the role of nodes and edges are switched, and two novel graph convolution operations are proposed for feature propagation. Experimental results on real-world academic citation networks and quantum chemistry graphs show that our approach achieves or matches the state-of-the-art performance in four graph learning tasks, including semi-supervised node classification, multi-task graph classification, graph regression, and link prediction.",0
"Graphs are widely used in various fields, such as social network analysis and biology, to represent data. The ability to effectively extract information from graphs is crucial for numerous machine learning tasks. Graph embedding is a method that transforms data structures into a low-dimensional and structural space, making it easier for other machine learning algorithms to analyze them. While traditional statistical approaches have been used for graph embedding, recent developments in deep learning, such as the Graph Convolutional Network (GCN), have shown better performance in graph learning benchmarks. However, most GCN methods only consider node features and ignore edge features, such as relationships in knowledge graphs. To address this issue, we propose CensNet, a general graph embedding framework that embeds both nodes and edges into a latent feature space. CensNet uses the line graph of the original undirected graph to switch the roles of nodes and edges, which allows for two novel graph convolution operations for feature propagation. Our approach achieves or matches state-of-the-art performance in four graph learning tasks, including semi-supervised node classification, multi-task graph classification, graph regression, and link prediction, as demonstrated on real-world academic citation networks and quantum chemistry graphs.",1
"Representation learning of graph-structured data is challenging because both graph structure and node features carry important information. Graph Neural Networks (GNNs) provide an expressive way to fuse information from network structure and node features. However, GNNs are prone to adversarial attacks. Here we introduce Graph Information Bottleneck (GIB), an information-theoretic principle that optimally balances expressiveness and robustness of the learned representation of graph-structured data. Inheriting from the general Information Bottleneck (IB), GIB aims to learn the minimal sufficient representation for a given task by maximizing the mutual information between the representation and the target, and simultaneously constraining the mutual information between the representation and the input data. Different from the general IB, GIB regularizes the structural as well as the feature information. We design two sampling algorithms for structural regularization and instantiate the GIB principle with two new models: GIB-Cat and GIB-Bern, and demonstrate the benefits by evaluating the resilience to adversarial attacks. We show that our proposed models are more robust than state-of-the-art graph defense models. GIB-based models empirically achieve up to 31% improvement with adversarial perturbation of the graph structure as well as node features.",0
"Learning representations of graph-structured data is difficult because the graph structure and node features are both important. Graph Neural Networks (GNNs) can combine information from both, but they are vulnerable to attacks. The Graph Information Bottleneck (GIB) principle aims to balance expressiveness and robustness by finding the minimal sufficient representation for a task. GIB maximizes mutual information between the representation and target, while constraining mutual information between the representation and input data. GIB also regulates structural and feature information. We introduce GIB-Cat and GIB-Bern models and show they are more robust than other graph defense models, achieving up to a 31% improvement against adversarial attacks on graph structure and node features.",1
"Graph neural networks (GNNs) have been widely used to analyze the graph-structured data in various application domains, e.g., social networks, molecular biology, and anomaly detection. With great power, the GNN models, usually as valuable Intellectual Properties of their owners, also become attractive targets of the attacker. Recent studies show that machine learning models are facing a severe threat called Model Extraction Attacks, where a well-trained private model owned by a service provider can be stolen by the attacker pretending as a client. Unfortunately, existing works focus on the models trained on the Euclidean space, e.g., images and texts, while how to extract a GNN model that contains a graph structure and node features is yet to be explored. In this paper, we explore and develop model extraction attacks against GNN models. Given only black-box access to a target GNN model, the attacker aims to reconstruct a duplicated one via several nodes he obtained (called attacker nodes). We first systematically formalise the threat modeling in the context of GNN model extraction and classify the adversarial threats into seven categories by considering different background knowledge of the attacker, e.g., attributes and/or neighbor connectives of the attacker nodes. Then we present the detailed methods which utilize the accessible knowledge in each threat to implement the attacks. By evaluating over three real-world datasets, our attacks are shown to extract duplicated models effectively, i.e., more than 89% inputs in the target domain have the same output predictions as the victim model.",0
"Graph neural networks (GNNs) have become prevalent in analyzing graph-structured data across various domains such as social networks, molecular biology, and anomaly detection. While these models are valuable intellectual properties of their owners, they pose a risk of being targeted by attackers due to their power. Model extraction attacks are a severe threat to machine learning models, where a well-trained private model can be stolen by an attacker posing as a client. However, current research focuses on models trained on Euclidean space, such as images and texts, and does not explore extracting GNN models that contain graph structures and node features. In this paper, we investigate and develop model extraction attacks against GNN models. By only having black-box access to a target GNN model, the attacker aims to reconstruct a duplicated one using several nodes (attacker nodes) obtained. We systematically formalize the threat modeling, classify the adversarial threats into seven categories based on the attacker's background knowledge, and present detailed methods for implementing attacks using accessible knowledge. Our attacks are shown to effectively extract duplicated models, with over 89% of inputs in the target domain having the same output predictions as the victim model, as demonstrated by evaluating over three real-world datasets.",1
"Graph embeddings are a ubiquitous tool for machine learning tasks, such as node classification and link prediction, on graph-structured data. However, computing the embeddings for large-scale graphs is prohibitively inefficient even if we are interested only in a small subset of relevant vertices. To address this, we present an efficient graph coarsening approach, based on Schur complements, for computing the embedding of the relevant vertices. We prove that these embeddings are preserved exactly by the Schur complement graph that is obtained via Gaussian elimination on the non-relevant vertices. As computing Schur complements is expensive, we give a nearly-linear time algorithm that generates a coarsened graph on the relevant vertices that provably matches the Schur complement in expectation in each iteration. Our experiments involving prediction tasks on graphs demonstrate that computing embeddings on the coarsened graph, rather than the entire graph, leads to significant time savings without sacrificing accuracy.",0
"Graph embeddings are commonly used in machine learning applications involving graph-structured data, such as node classification and link prediction. However, computing these embeddings for large-scale graphs can be highly inefficient, even when focusing solely on a small subset of relevant vertices. To solve this problem, we propose an efficient graph coarsening technique based on Schur complements, which allows for the computation of embeddings for relevant vertices. We demonstrate that these embeddings are accurately preserved by the Schur complement graph obtained through Gaussian elimination on non-relevant vertices. While computing Schur complements can be costly, we present a nearly-linear time algorithm that generates a coarsened graph on relevant vertices, which matches the Schur complement in expectation in each iteration. Our experiments show that computing embeddings on the coarsened graph, rather than the entire graph, results in significant time savings without sacrificing accuracy in prediction tasks on graphs.",1
"Graph convolution is a fundamental building block for many deep neural networks on graph-structured data. In this paper, we introduce a simple, yet very effective graph convolutional network with skip connections for semi-supervised anomaly detection. The proposed multi-layer network architecture is theoretically motivated by the concept of implicit fairing in geometry processing, and comprises a graph convolution module for aggregating information from immediate node neighbors and a skip connection module for combining layer-wise neighborhood representations. In addition to capturing information from distant graph nodes through skip connections between the network's layers, our approach exploits both the graph structure and node features for learning discriminative node representations. The effectiveness of our model is demonstrated through extensive experiments on five benchmark datasets, achieving better or comparable anomaly detection results against strong baseline methods.",0
"Many deep neural networks on graph-structured data rely on graph convolution as a crucial component. Our paper proposes a graph convolutional network with skip connections for semi-supervised anomaly detection, which is both simple and highly effective. The multi-layer network architecture is motivated by the concept of implicit fairing in geometry processing and comprises a graph convolution module for aggregating information from immediate node neighbors and a skip connection module for combining layer-wise neighborhood representations. Our approach captures information from distant graph nodes through skip connections and leverages both the graph structure and node features to learn discriminative node representations. We demonstrate the efficacy of our model through extensive experiments on five benchmark datasets, achieving better or comparable anomaly detection results against strong baseline methods.",1
"This paper presents and characterizes an Open Application Repository for Federated Learning (OARF), a benchmark suite for federated machine learning systems. Previously available benchmarks for federated learning have focused mainly on synthetic datasets and use a very limited number of applications. OARF includes different data partitioning methods (horizontal, vertical and hybrid) as well as emerging applications in image, text and structured data, which represent different scenarios in federated learning. Our characterization shows that the benchmark suite is diverse in data size, distribution, feature distribution and learning task complexity. We have developed reference implementations, and evaluated the important aspects of federated learning, including model accuracy, communication cost, differential privacy, secure multiparty computation and vertical federated learning.",0
"In this paper, the Open Application Repository for Federated Learning (OARF) is introduced and analyzed as a benchmark for federated machine learning systems. Existing benchmarks for federated learning have been limited to synthetic datasets and only a few applications. OARF, on the other hand, consists of various data partitioning methods (horizontal, vertical, and hybrid) and includes emerging applications in image, text, and structured data, representing a range of scenarios in federated learning. Our analysis demonstrates that OARF is diverse in terms of data size, distribution, feature distribution, and learning task complexity. We have also provided reference implementations and assessed crucial aspects of federated learning, such as model accuracy, communication cost, differential privacy, secure multiparty computation, and vertical federated learning.",1
"Graph neural network models have been extensively used to learn node representations for graph structured data in an end-to-end setting. These models often rely on localized first order approximations of spectral graph convolutions and hence are unable to capture higher-order relational information between nodes. Probabilistic Graphical Models form another class of models that provide rich flexibility in incorporating such relational information but are limited by inefficient approximate inference algorithms at higher order. In this paper, we propose to combine these approaches to learn better node and graph representations. First, we derive an efficient approximate sum-product loopy belief propagation inference algorithm for higher-order PGMs. We then embed the message passing updates into a neural network to provide the inductive bias of the inference algorithm in end-to-end learning. This gives us a model that is flexible enough to accommodate domain knowledge while maintaining the computational advantage. We further propose methods for constructing higher-order factors that are conditioned on node and edge features and share parameters wherever necessary. Our experimental evaluation shows that our model indeed captures higher-order information, substantially outperforming state-of-the-art $k$-order graph neural networks in molecular datasets.",0
"The use of graph neural network models has been widespread in the learning of node representations for graph structured data in an end-to-end scenario. However, these models rely heavily on first order approximations, which limit their ability to capture higher-order relational information between nodes. Alternatively, Probabilistic Graphical Models offer increased flexibility in incorporating such relational information, but are hindered by inefficient approximate inference algorithms at higher order. To address these limitations, we propose a combination of both approaches to yield better node and graph representations. Our approach involves deriving an efficient approximate sum-product loopy belief propagation inference algorithm for higher-order PGMs, incorporating message passing updates into a neural network for inductive bias, and constructing higher-order factors conditioned on node and edge features. Our experimental evaluation indicates that this approach effectively captures higher-order information, outperforming state-of-the-art $k$-order graph neural networks in molecular datasets, while still being flexible and computationally efficient.",1
"Fairness in machine learning is crucial when individuals are subject to automated decisions made by models in high-stake domains. Organizations that employ these models may also need to satisfy regulations that promote responsible and ethical A.I. While fairness metrics relying on comparing model error rates across subpopulations have been widely investigated for the detection and mitigation of bias, fairness in terms of the equalized ability to achieve recourse for different protected attribute groups has been relatively unexplored. We present a novel formulation for training neural networks that considers the distance of data points to the decision boundary such that the new objective: (1) reduces the average distance to the decision boundary between two groups for individuals subject to a negative outcome in each group, i.e. the network is more fair with respect to the ability to obtain recourse, and (2) increases the average distance of data points to the boundary to promote adversarial robustness. We demonstrate that training with this loss yields more fair and robust neural networks with similar accuracies to models trained without it. Moreover, we qualitatively motivate and empirically show that reducing recourse disparity across groups also improves fairness measures that rely on error rates. To the best of our knowledge, this is the first time that recourse capabilities across groups are considered to train fairer neural networks, and a relation between error rates based fairness and recourse based fairness is investigated.",0
"When it comes to automated decisions made by machine learning models in high-stakes domains, fairness is of utmost importance. Organizations using these models must adhere to regulations promoting ethical and responsible A.I. While measuring model error rates across different subpopulations has been extensively studied to detect and mitigate bias, fairness in terms of equal recourse for various protected attribute groups has been largely unexplored. In this study, we introduce a new approach to training neural networks that considers the distance between data points and the decision boundary. This approach aims to reduce the average distance to the decision boundary for individuals experiencing negative outcomes in each group, promoting fairness in the ability to obtain recourse, while also increasing the average distance to the boundary to enhance adversarial robustness. Our results demonstrate that implementing this approach produces fairer and more robust neural networks with comparable accuracies to those trained without it. Additionally, we show that reducing recourse disparities across groups also leads to improved fairness measures based on error rates. This study is the first to consider recourse capabilities across groups in training fairer neural networks, and it investigates the relationship between error rates and recourse-based fairness.",1
"Exploiting the rapid advances in probabilistic inference, in particular variational Bayes and variational autoencoders (VAEs), for anomaly detection (AD) tasks remains an open research question. Previous works argued that training VAE models only with inliers is insufficient and the framework should be significantly modified in order to discriminate the anomalous instances. In this work, we exploit the deep conditional variational autoencoder (CVAE) and we define an original loss function together with a metric that targets hierarchically structured data AD. Our motivating application is a real world problem: monitoring the trigger system which is a basic component of many particle physics experiments at the CERN Large Hadron Collider (LHC). In the experiments we show the superior performance of this method for classical machine learning (ML) benchmarks and for our application.",0
"The use of probabilistic inference advancements, such as variational Bayes and variational autoencoders (VAEs), for detecting anomalies (AD) is currently an unresolved research issue. Previous studies have suggested that training VAE models solely with inliers is inadequate and that the framework must be significantly adjusted to distinguish anomalous instances. In this study, we utilize the deep conditional variational autoencoder (CVAE) and introduce a novel loss function and metric that target hierarchically structured data AD. Our study is motivated by a practical issue: monitoring the trigger system, which is a fundamental component in many particle physics experiments at the CERN Large Hadron Collider (LHC). Our experiments demonstrate the superior performance of this method for both classical machine learning (ML) benchmarks and our application.",1
"Recently, graph neural networks have attracted great attention and achieved prominent performance in various research fields. Most of those algorithms have assumed pairwise relationships of objects of interest. However, in many real applications, the relationships between objects are in higher-order, beyond a pairwise formulation. To efficiently learn deep embeddings on the high-order graph-structured data, we introduce two end-to-end trainable operators to the family of graph neural networks, i.e., hypergraph convolution and hypergraph attention. Whilst hypergraph convolution defines the basic formulation of performing convolution on a hypergraph, hypergraph attention further enhances the capacity of representation learning by leveraging an attention module. With the two operators, a graph neural network is readily extended to a more flexible model and applied to diverse applications where non-pairwise relationships are observed. Extensive experimental results with semi-supervised node classification demonstrate the effectiveness of hypergraph convolution and hypergraph attention.",0
"In various research fields, graph neural networks have gained significant attention and achieved notable performance. However, these algorithms usually assume pairwise relationships between objects, which may not be the case in many real-world applications where higher-order relationships exist. To enable efficient learning of deep embeddings on high-order graph-structured data, we introduce two operators to the graph neural network family: hypergraph convolution and hypergraph attention. Hypergraph convolution performs convolution on a hypergraph, while hypergraph attention enhances representation learning by utilizing an attention module. With these operators, the graph neural network becomes more flexible and can be applied to various applications where non-pairwise relationships are present. Extensive experiments with semi-supervised node classification show that hypergraph convolution and hypergraph attention are effective.",1
"Graphs are the most ubiquitous form of structured data representation used in machine learning. They model, however, only pairwise relations between nodes and are not designed for encoding the higher-order relations found in many real-world datasets. To model such complex relations, hypergraphs have proven to be a natural representation. Learning the node representations in a hypergraph is more complex than in a graph as it involves information propagation at two levels: within every hyperedge and across the hyperedges. Most current approaches first transform a hypergraph structure to a graph for use in existing geometric deep learning algorithms. This transformation leads to information loss, and sub-optimal exploitation of the hypergraph's expressive power. We present HyperSAGE, a novel hypergraph learning framework that uses a two-level neural message passing strategy to accurately and efficiently propagate information through hypergraphs. The flexible design of HyperSAGE facilitates different ways of aggregating neighborhood information. Unlike the majority of related work which is transductive, our approach, inspired by the popular GraphSAGE method, is inductive. Thus, it can also be used on previously unseen nodes, facilitating deployment in problems such as evolving or partially observed hypergraphs. Through extensive experimentation, we show that HyperSAGE outperforms state-of-the-art hypergraph learning methods on representative benchmark datasets. We also demonstrate that the higher expressive power of HyperSAGE makes it more stable in learning node representations as compared to the alternatives.",0
"Machine learning commonly employs graphs as a structured form of data representation. However, graphs only account for pairwise node relations and are inadequate for encoding higher-order relations observed in real-world datasets. Hypergraphs, on the other hand, provide a natural representation for modeling complex relations. Learning node representations in hypergraphs is more intricate because it involves information propagation within and across hyperedges. Most existing methods convert hypergraphs to graphs for use in geometric deep learning algorithms, leading to information loss and suboptimal use of the hypergraph's expressive power. To address this issue, we present HyperSAGE, a novel hypergraph learning framework that uses a two-level neural message passing strategy for efficient and accurate information propagation. HyperSAGE's flexible design allows for different methods of aggregating neighborhood information. Unlike transductive approaches, HyperSAGE is inductive and can be used on unseen nodes, making it suitable for evolving or partially observed hypergraphs. Our extensive experimentation shows that HyperSAGE outperforms state-of-the-art hypergraph learning methods on representative benchmark datasets. Additionally, HyperSAGE's higher expressive power makes it more stable in learning node representations compared to other alternatives.",1
"Graph neural networks have become an important tool for modeling structured data. In many real-world systems, intricate hidden information may exist, e.g., heterogeneity in nodes/edges, static node/edge attributes, and spatiotemporal node/edge features. However, most existing methods only take part of the information into consideration. In this paper, we present the Co-evolved Meta Graph Neural Network (CoMGNN), which applies meta graph attention to heterogeneous graphs with co-evolution of node and edge states. We further propose a spatiotemporal adaption of CoMGNN (ST-CoMGNN) for modeling spatiotemporal patterns on nodes and edges. We conduct experiments on two large-scale real-world datasets. Experimental results show that our models significantly outperform the state-of-the-art methods, demonstrating the effectiveness of encoding diverse information from different aspects.",0
"Structured data modeling has been enhanced by the use of graph neural networks. In some systems, there may be complex underlying information such as heterogeneous nodes/edges, static node/edge attributes, and spatiotemporal node/edge features. However, prevailing approaches only take some of this information into account. This study introduces the Co-evolved Meta Graph Neural Network (CoMGNN), which incorporates meta graph attention to heterogeneous graphs with co-evolution of node and edge states. Additionally, the authors propose the ST-CoMGNN for modeling spatiotemporal patterns on nodes and edges. The models were tested on two large-scale real-world datasets, and results prove that they outperform current methods, highlighting their ability to encode various information from diverse perspectives.",1
"Understanding customer behavior is fundamental for many use-cases in industry, especially in accelerated growth areas such as fin-tech and e-commerce. Structured data are often expensive, time-consuming and inadequate to analyze and study complex customer behaviors. In this paper, we propose a multi-graph embedding approach for creating a non-linear representation of customers in order to have a better knowledge of their characteristics without having any prior information about their financial status or their interests. By applying the current method we are able to predict users' future behavior with a reasonably high accuracy only by having the information of their friendship network. Potential applications include recommendation systems and credit risk forecasting.",0
"In various industries, particularly in rapidly growing sectors like fin-tech and e-commerce, it is crucial to comprehend customer behavior. Analyzing intricate customer behavior through structured data can be expensive, time-consuming, and insufficient. To address this, we suggest a multi-graph embedding technique to create a non-linear depiction of customers, enabling us to learn about their traits without any prior knowledge of their financial status or interests. By utilizing this approach, we can accurately anticipate users' future actions through their friendship networks. This method can be utilized for prediction systems and credit risk forecasting, among other things.",1
"Graph Neural Networks (GNNs) have risen to prominence in learning representations for graph structured data. A single GNN layer typically consists of a feature transformation and a feature aggregation operation. The former normally uses feed-forward networks to transform features, while the latter aggregates the transformed features over the graph. Numerous recent works have proposed GNN models with different designs in the aggregation operation. In this work, we establish mathematically that the aggregation processes in a group of representative GNN models including GCN, GAT, PPNP, and APPNP can be regarded as (approximately) solving a graph denoising problem with a smoothness assumption. Such a unified view across GNNs not only provides a new perspective to understand a variety of aggregation operations but also enables us to develop a unified graph neural network framework UGNN. To demonstrate its promising potential, we instantiate a novel GNN model, ADA-UGNN, derived from UGNN, to handle graphs with adaptive smoothness across nodes. Comprehensive experiments show the effectiveness of ADA-UGNN.",0
"Learning representations for graph structured data has become increasingly popular with the emergence of Graph Neural Networks (GNNs). A single GNN layer involves a feature transformation and feature aggregation operation, with feed-forward networks being used for the former and the latter involving the aggregation of transformed features over the graph. Recent studies have proposed different aggregation operation designs for GNN models. This study establishes mathematically that a group of representative GNN models including GCN, GAT, PPNP, and APPNP can be viewed as (approximately) solving a graph denoising problem with a smoothness assumption. This unified view provides new insights into understanding aggregation operations and enables the development of a unified graph neural network framework UGNN. Using UGNN, a novel GNN model, ADA-UGNN, has been instantiated to handle graphs with adaptive smoothness across nodes, and comprehensive experiments prove its effectiveness.",1
"The Large Scale Visual Recognition Challenge based on the well-known Imagenet dataset catalyzed an intense flurry of progress in computer vision. Benchmark tasks have propelled other sub-fields of machine learning forward at an equally impressive pace, but in healthcare it has primarily been image processing tasks, such as in dermatology and radiology, that have experienced similar benchmark-driven progress. In the present study, we performed a comprehensive review of benchmarks in medical machine learning for structured data, identifying one based on the Medical Information Mart for Intensive Care (MIMIC-III) that allows the first direct comparison of predictive performance and thus the evaluation of progress on four clinical prediction tasks: mortality, length of stay, phenotyping, and patient decompensation. We find that little meaningful progress has been made over a 3 year period on these tasks, despite significant community engagement. Through our meta-analysis, we find that the performance of deep recurrent models is only superior to logistic regression on certain tasks. We conclude with a synthesis of these results, possible explanations, and a list of desirable qualities for future benchmarks in medical machine learning.",0
"The Imagenet dataset's Large Scale Visual Recognition Challenge triggered a notable surge of advancement in computer vision. Benchmark tasks have also stimulated progress in other sub-fields of machine learning, but in healthcare, mainly image processing tasks, such as dermatology and radiology, have seen similar benchmark-driven progress. This study conducted a comprehensive review of benchmarks in medical machine learning for structured data and identified one based on the Medical Information Mart for Intensive Care (MIMIC-III). This benchmark enabled the first direct comparison of predictive performance and progress evaluation on four clinical prediction tasks: mortality, length of stay, phenotyping, and patient decompensation. Despite significant community engagement, our meta-analysis revealed little meaningful progress on these tasks over a 3-year period. We discovered that the performance of deep recurrent models was only superior to logistic regression on certain tasks. We conclude with a synthesis of these findings, possible explanations, and a list of desirable qualities for future medical machine learning benchmarks.",1
"Graph Neural Networks (GNNs), a generalization of neural networks to graph-structured data, are often implemented using message passes between entities of a graph. While GNNs are effective for node classification, link prediction and graph classification, they are vulnerable to adversarial attacks, i.e., a small perturbation to the structure can lead to a non-trivial performance degradation. In this work, we propose Uncertainty Matching GNN (UM-GNN), that is aimed at improving the robustness of GNN models, particularly against poisoning attacks to the graph structure, by leveraging epistemic uncertainties from the message passing framework. More specifically, we propose to build a surrogate predictor that does not directly access the graph structure, but systematically extracts reliable knowledge from a standard GNN through a novel uncertainty-matching strategy. Interestingly, this uncoupling makes UM-GNN immune to evasion attacks by design, and achieves significantly improved robustness against poisoning attacks. Using empirical studies with standard benchmarks and a suite of global and target attacks, we demonstrate the effectiveness of UM-GNN, when compared to existing baselines including the state-of-the-art robust GCN.",0
"The use of Graph Neural Networks (GNNs) has become popular for processing graph-structured data, as they allow for message passing between entities of a graph. However, GNNs are susceptible to adversarial attacks, which can cause a drop in performance when there is a small perturbation to the structure. To address this issue, we introduce Uncertainty Matching GNN (UM-GNN), which aims to increase the robustness of GNN models against poisoning attacks by utilizing epistemic uncertainties. We achieve this by creating a surrogate predictor that does not directly access the graph structure, but extracts reliable knowledge from a standard GNN through an uncertainty-matching strategy. This approach makes UM-GNN resistant to evasion attacks and improves its robustness against poisoning attacks. We demonstrate the effectiveness of UM-GNN through empirical studies using standard benchmarks and a variety of attacks, showing that it outperforms existing baselines, including the state-of-the-art robust GCN.",1
"Graph structured data has wide applicability in various domains such as physics, chemistry, biology, computer vision, and social networks, to name a few. Recently, graph neural networks (GNN) were shown to be successful in effectively representing graph structured data because of their good performance and generalization ability. GNN is a deep learning based method that learns a node representation by combining specific nodes and the structural/topological information of a graph. However, like other deep models, explaining the effectiveness of GNN models is a challenging task because of the complex nonlinear transformations made over the iterations. In this paper, we propose GraphLIME, a local interpretable model explanation for graphs using the Hilbert-Schmidt Independence Criterion (HSIC) Lasso, which is a nonlinear feature selection method. GraphLIME is a generic GNN-model explanation framework that learns a nonlinear interpretable model locally in the subgraph of the node being explained. More specifically, to explain a node, we generate a nonlinear interpretable model from its $N$-hop neighborhood and then compute the K most representative features as the explanations of its prediction using HSIC Lasso. Through experiments on two real-world datasets, the explanations of GraphLIME are found to be of extraordinary degree and more descriptive in comparison to the existing explanation methods.",0
"The use of graph structured data is widespread across numerous fields, including physics, chemistry, biology, computer vision, and social networks. Graph neural networks (GNN) have proven to be a successful method for effectively representing this type of data due to their strong performance and generalization abilities. However, as with other deep learning models, explaining the effectiveness of GNNs can be challenging due to complex nonlinear transformations. To address this, we introduce GraphLIME, a framework for locally interpreting GNN models using the Hilbert-Schmidt Independence Criterion (HSIC) Lasso, a nonlinear feature selection method. GraphLIME generates a nonlinear interpretable model from the $N$-hop neighborhood of a node, and uses HSIC Lasso to compute the K most representative features as explanations for its prediction. Our experiments on two real-world datasets demonstrate that GraphLIME provides highly descriptive and informative explanations compared to existing methods.",1
"Graph Neural Networks (GNNs) have attracted considerable attention and have emerged as a new promising paradigm to process graph-structured data. GNNs are usually stacked to multiple layers and the node representations in each layer are computed through propagating and aggregating the neighboring node features with respect to the graph. By stacking to multiple layers, GNNs are able to capture the long-range dependencies among the data on the graph and thus bring performance improvements. To train a GNN with multiple layers effectively, some normalization techniques (e.g., node-wise normalization, batch-wise normalization) are necessary. However, the normalization techniques for GNNs are highly task-relevant and different application tasks prefer to different normalization techniques, which is hard to know in advance. To tackle this deficiency, in this paper, we propose to learn graph normalization by optimizing a weighted combination of normalization techniques at four different levels, including node-wise normalization, adjacency-wise normalization, graph-wise normalization, and batch-wise normalization, in which the adjacency-wise normalization and the graph-wise normalization are newly proposed in this paper to take into account the local structure and the global structure on the graph, respectively. By learning the optimal weights, we are able to automatically select a single best or a best combination of multiple normalizations for a specific task. We conduct extensive experiments on benchmark datasets for different tasks, including node classification, link prediction, graph classification and graph regression, and confirm that the learned graph normalization leads to competitive results and that the learned weights suggest the appropriate normalization techniques for the specific task. Source code is released here https://github.com/cyh1112/GraphNormalization.",0
"The emergence of Graph Neural Networks (GNNs) as a promising paradigm for processing graph-structured data has garnered significant attention. GNNs are typically composed of multiple layers, with node representations in each layer computed through the propagation and aggregation of neighboring node features with respect to the graph. By stacking multiple layers, GNNs can capture long-range dependencies and improve performance. However, effective training of GNNs with multiple layers requires normalization techniques, which are highly task-dependent and difficult to determine in advance. To address this issue, this paper proposes learning graph normalization by optimizing a weighted combination of four different normalization techniques, including node-wise, adjacency-wise, graph-wise, and batch-wise normalization. The proposed approach automatically selects the best or combination of normalizations for a specific task by learning optimal weights. Extensive experiments on benchmark datasets for various tasks confirm the effectiveness of the learned graph normalization and suggest appropriate normalization techniques for specific tasks. The source code is available at https://github.com/cyh1112/GraphNormalization.",1
"While existing predictive frameworks are able to handle Euclidean structured data (i.e, brain images), they might fail to generalize to geometric non-Euclidean data such as brain networks. Besides, these are rooted the sample selection step in using Euclidean or learned similarity measure between vectorized training and testing brain networks. Such sample connectomic representation might include irrelevant and redundant features that could mislead the training sample selection step. Undoubtedly, this fails to exploit and preserve the topology of the brain connectome. To overcome this major drawback, we propose Residual Embedding Similarity-Based Network selection (RESNets) for predicting brain network evolution trajectory from a single timepoint. RESNets first learns a compact geometric embedding of each training and testing sample using adversarial connectome embedding network. This nicely reduces the high-dimensionality of brain networks while preserving their topological properties via graph convolutional networks. Next, to compute the similarity between subjects, we introduce the concept of a connectional brain template (CBT), a fixed network reference, where we further represent each training and testing network as a deviation from the reference CBT in the embedding space. As such, we select the most similar training subjects to the testing subject at baseline by comparing their learned residual embeddings with respect to the pre-defined CBT. Once the best training samples are selected at baseline, we simply average their corresponding brain networks at follow-up timepoints to predict the evolution trajectory of the testing network. Our experiments on both healthy and disordered brain networks demonstrate the success of our proposed method in comparison to RESNets ablated versions and traditional approaches.",0
"Current predictive frameworks are effective in dealing with structured Euclidean data, such as brain images, but they may not be suitable for non-Euclidean geometric data, like brain networks. Additionally, the sample selection step using Euclidean or learned similarity measures between vectorized training and testing brain networks could be hindered by irrelevant and redundant features that mislead the training sample selection step, thus failing to utilize and preserve the topology of the brain connectome. To address this issue, we introduce Residual Embedding Similarity-Based Network selection (RESNets), a method for predicting brain network evolution trajectory from a single timepoint. RESNets first utilizes an adversarial connectome embedding network to learn a compact geometric embedding of each training and testing sample. This reduces the high-dimensionality of brain networks while retaining their topological properties via graph convolutional networks. To calculate the similarity between subjects, we introduce the notion of a connectional brain template (CBT), a fixed network reference. We also represent each training and testing network as a deviation from the reference CBT in the embedding space. We then select the most similar training subjects to the testing subject at baseline by comparing their learned residual embeddings with respect to the pre-defined CBT. After selecting the best training samples at baseline, we simply average their corresponding brain networks at follow-up timepoints to predict the evolution trajectory of the testing network. Our experiments on healthy and disordered brain networks demonstrate the effectiveness of our proposed method in comparison to RESNets ablated versions and traditional approaches.",1
"Graph neural networks (GNNs) have achieved high performance in analyzing graph-structured data and have been widely deployed in safety-critical areas, such as finance and autonomous driving. However, only a few works have explored GNNs' robustness to adversarial attacks, and their designs are usually limited by the scale of input datasets (i.e., focusing on small graphs with only thousands of nodes). In this work, we propose, SAG, the first scalable adversarial attack method with Alternating Direction Method of Multipliers (ADMM). We first decouple the large-scale graph into several smaller graph partitions and cast the original problem into several subproblems. Then, we propose to solve these subproblems using projected gradient descent on both the graph topology and the node features that lead to considerably lower memory consumption compared to the conventional attack methods. Rigorous experiments further demonstrate that SAG can significantly reduce the computation and memory overhead compared with the state-of-the-art approach, making SAG applicable towards graphs with large size of nodes and edges.",0
"Graph neural networks (GNNs) have been highly successful in analyzing graph-structured data and have been widely implemented in safety-critical areas such as finance and autonomous driving. However, research on GNNs' resilience to adversarial attacks is limited, with most designs focusing on small graphs with only a few thousand nodes. To address this gap, we propose SAG, the first scalable adversarial attack method that employs the Alternating Direction Method of Multipliers (ADMM). To achieve scalability, we divide the large-scale graph into several smaller partitions and solve the resulting subproblems using projected gradient descent on both the graph topology and the node features. This approach significantly reduces memory consumption compared to conventional attack methods, making SAG suitable for graphs with large numbers of nodes and edges. Our rigorous experiments demonstrate that SAG can substantially reduce computation and memory overheads compared to state-of-the-art methods.",1
"Despite substantial progress in signal source separation, results for richly structured data continue to contain perceptible artifacts. In contrast, recent deep generative models can produce authentic samples in a variety of domains that are indistinguishable from samples of the data distribution. This paper introduces a Bayesian approach to source separation that uses generative models as priors over the components of a mixture of sources, and noise-annealed Langevin dynamics to sample from the posterior distribution of sources given a mixture. This decouples the source separation problem from generative modeling, enabling us to directly use cutting-edge generative models as priors. The method achieves state-of-the-art performance for MNIST digit separation. We introduce new methodology for evaluating separation quality on richer datasets, providing quantitative evaluation of separation results on CIFAR-10. We also provide qualitative results on LSUN.",0
"While signal source separation has made significant advancements, there are still noticeable imperfections in the results when dealing with complex data. In contrast, recent deep generative models have the ability to create authentic samples across various domains that are virtually indistinguishable from the original data distribution. This paper presents a Bayesian approach to source separation that utilizes generative models as priors for the components of a source mixture. By using noise-annealed Langevin dynamics to sample from the posterior distribution, the source separation problem is separated from generative modeling, allowing for the use of state-of-the-art generative models as priors. In terms of MNIST digit separation, this method achieves the best performance to date. We also introduce a novel evaluation method for assessing separation quality on more complex datasets and provide quantitative analysis of separation results on CIFAR-10, as well as qualitative results on LSUN.",1
"Axis-aligned decision forests have long been the leading class of machine learning algorithms for modeling tabular data. In many applications of machine learning such as learning-to-rank, decision forests deliver remarkable performance. They also possess other coveted characteristics such as interpretability. Despite their widespread use and rich history, decision forests to date fail to consume raw structured data such as text, or learn effective representations for them, a factor behind the success of deep neural networks in recent years. While there exist methods that construct smoothed decision forests to achieve representation learning, the resulting models are decision forests in name only: They are no longer axis-aligned, use stochastic decisions, or are not interpretable. Furthermore, none of the existing methods are appropriate for problems that require a Transfer Learning treatment. In this work, we present a novel but intuitive proposal to achieve representation learning for decision forests without imposing new restrictions or necessitating structural changes. Our model is simply a decision forest, possibly trained using any forest learning algorithm, atop a deep neural network. By approximating the gradients of the decision forest through input perturbation, a purely analytical procedure, the decision forest directs the neural network to learn or fine-tune representations. Our framework has the advantage that it is applicable to any arbitrary decision forest and that it allows the use of arbitrary deep neural networks for representation learning. We demonstrate the feasibility and effectiveness of our proposal through experiments on synthetic and benchmark classification datasets.",0
"For a long time, axis-aligned decision forests have been the most popular machine learning algorithms for modeling tabular data. They have been particularly successful in learning-to-rank applications and are known for their interpretability. However, decision forests have not been able to handle raw structured data like text, nor have they been effective in learning representations for them. Smoothed decision forests have been used to achieve representation learning, but they are no longer axis-aligned, use stochastic decisions, or are not interpretable. Additionally, none of these methods are appropriate for transfer learning. This paper presents a new approach to achieve representation learning for decision forests without imposing new restrictions or structural changes. The proposed model is a decision forest, trained using any forest learning algorithm, on top of a deep neural network. By approximating the gradients of the decision forest through input perturbation, the decision forest directs the neural network to learn or fine-tune representations. This approach is applicable to any arbitrary decision forest and allows the use of arbitrary deep neural networks for representation learning. The feasibility and effectiveness of this proposal are demonstrated through experiments on synthetic and benchmark classification datasets.",1
"Urban ride-hailing demand prediction is a crucial but challenging task for intelligent transportation system construction. Predictable ride-hailing demand can facilitate more reasonable vehicle scheduling and online car-hailing platform dispatch. Conventional deep learning methods with no external structured data can be accomplished via hybrid models of CNNs and RNNs by meshing plentiful pixel-level labeled data, but spatial data sparsity and limited learning capabilities on temporal long-term dependencies are still two striking bottlenecks. To address these limitations, we propose a new virtual graph modeling method to focus on significant demand regions and a novel Deep Multi-View Spatiotemporal Virtual Graph Neural Network (DMVST-VGNN) to strengthen learning capabilities of spatial dynamics and temporal long-term dependencies. Specifically, DMVST-VGNN integrates the structures of 1D Convolutional Neural Network, Multi Graph Attention Neural Network and Transformer layer, which correspond to short-term temporal dynamics view, spatial dynamics view and long-term temporal dynamics view respectively. In this paper, experiments are conducted on two large-scale New York City datasets in fine-grained prediction scenes. And the experimental results demonstrate effectiveness and superiority of DMVST-VGNN framework in significant citywide ride-hailing demand prediction.",0
"The task of predicting urban ride-hailing demand is important for building intelligent transportation systems, but it is also challenging. Accurate predictions can improve vehicle scheduling and dispatch on online car-hailing platforms. While conventional deep learning methods can use pixel-level labeled data to make predictions using hybrid CNN and RNN models, they still face limitations due to sparse spatial data and limited learning capabilities on long-term temporal dependencies. To overcome these limitations, we propose a new virtual graph modeling method focused on significant demand regions and a novel Deep Multi-View Spatiotemporal Virtual Graph Neural Network (DMVST-VGNN) that strengthens learning capabilities of spatial dynamics and temporal dependencies. DMVST-VGNN integrates 1D Convolutional Neural Network, Multi Graph Attention Neural Network, and Transformer layer structures for short-term temporal dynamics, spatial dynamics, and long-term temporal dynamics, respectively. We conducted experiments on two New York City datasets and showed the effectiveness and superiority of DMVST-VGNN in predicting significant citywide ride-hailing demand.",1
"Representation learning over graph structure data has been widely studied due to its wide application prospects. However, previous methods mainly focus on static graphs while many real-world graphs evolve over time. Modeling such evolution is important for predicting properties of unseen networks. To resolve this challenge, we propose SGRNN, a novel neural architecture that applies stochastic latent variables to simultaneously capture the evolution in node attributes and topology. Specifically, deterministic states are separated from stochastic states in the iterative process to suppress mutual interference. With semi-implicit variational inference integrated to SGRNN, a non-Gaussian variational distribution is proposed to help further improve the performance. In addition, to alleviate KL-vanishing problem in SGRNN, a simple and interpretable structure is proposed based on the lower bound of KL-divergence. Extensive experiments on real-world datasets demonstrate the effectiveness of the proposed model. Code is available at https://github.com/StochasticGRNN/SGRNN.",0
"The potential applications of representation learning for graph structure data have led to extensive research. However, most existing methods have focused on static graphs, despite the fact that many real-world graphs evolve over time. To address this issue, we introduce SGRNN, a new neural architecture that uses stochastic latent variables to model node attributes and topology evolution simultaneously. SGRNN separates deterministic states from stochastic states during the iterative process to minimize mutual interference. We also propose a non-Gaussian variational distribution by integrating semi-implicit variational inference to improve performance. To mitigate the KL-vanishing problem in SGRNN, we introduce a simple and interpretable structure based on the lower bound of KL-divergence. Our experiments on real-world datasets demonstrate the effectiveness of SGRNN, and the code is available at https://github.com/StochasticGRNN/SGRNN.",1
"Recently, the surge in popularity of Internet of Things (IoT), mobile devices, social media, etc. has opened up a large source for graph data. Graph embedding has been proved extremely useful to learn low-dimensional feature representations from graph structured data. These feature representations can be used for a variety of prediction tasks from node classification to link prediction. However, existing graph embedding methods do not consider users' privacy to prevent inference attacks. That is, adversaries can infer users' sensitive information by analyzing node representations learned from graph embedding algorithms. In this paper, we propose Adversarial Privacy Graph Embedding (APGE), a graph adversarial training framework that integrates the disentangling and purging mechanisms to remove users' private information from learned node representations. The proposed method preserves the structural information and utility attributes of a graph while concealing users' private attributes from inference attacks. Extensive experiments on real-world graph datasets demonstrate the superior performance of APGE compared to the state-of-the-arts. Our source code can be found at https://github.com/uJ62JHD/Privacy-Preserving-Social-Network-Embedding.",0
"The rise in popularity of technologies such as the Internet of Things (IoT), mobile devices, and social media has created a vast source of graph data. Graph embedding is a useful method for obtaining low-dimensional feature representations from graph-structured data, which can be applied to various prediction tasks, including node classification and link prediction. However, current graph embedding methods do not consider privacy concerns, leaving users vulnerable to inference attacks where adversaries can extract sensitive information from node representations. To address this issue, we introduce Adversarial Privacy Graph Embedding (APGE), a graph adversarial training framework that incorporates disentangling and purging mechanisms to remove private information from learned node representations. Our approach maintains graph structure and utility attributes while safeguarding users' private data from inference attacks. Experimental results using real-world graph datasets demonstrate that APGE outperforms existing state-of-the-art methods. Our source code is available at https://github.com/uJ62JHD/Privacy-Preserving-Social-Network-Embedding.",1
"Operator-Valued Kernels (OVKs) and associated vector-valued Reproducing Kernel Hilbert Spaces provide an elegant way to extend scalar kernel methods when the output space is a Hilbert space. Although primarily used in finite dimension for problems like multi-task regression, the ability of this framework to deal with infinite dimensional output spaces unlocks many more applications, such as functional regression, structured output prediction, and structured data representation. However, these sophisticated schemes crucially rely on the kernel trick in the output space, so that most of previous works have focused on the square norm loss function, completely neglecting robustness issues that may arise in such surrogate problems. To overcome this limitation, this paper develops a duality approach that allows to solve OVK machines for a wide range of loss functions. The infinite dimensional Lagrange multipliers are handled through a Double Representer Theorem, and algorithms for $\epsilon$-insensitive losses and the Huber loss are thoroughly detailed. Robustness benefits are emphasized by a theoretical stability analysis, as well as empirical improvements on structured data applications.",0
"The use of Operator-Valued Kernels (OVKs) and vector-valued Reproducing Kernel Hilbert Spaces is an elegant way to extend scalar kernel methods when dealing with Hilbert spaces as output spaces. Although it is commonly used in finite dimensions for problems like multi-task regression, this framework has the potential to handle infinite dimensional output spaces, opening up the possibility for functional regression, structured output prediction, and structured data representation. However, previous works have only focused on the square norm loss function, disregarding the possibility of robustness issues that may arise in surrogate problems. To address this, the current paper introduces a duality approach that can solve OVK machines for a wide range of loss functions, including $\epsilon$-insensitive losses and the Huber loss. The Double Representer Theorem is used to handle infinite dimensional Lagrange multipliers, and the paper provides a detailed algorithm for each loss function. The theoretical stability analysis and empirical results demonstrate the benefits of robustness in structured data applications.",1
"As machine learning for images becomes democratized in the Software 2.0 era, one of the serious bottlenecks is securing enough labeled data for training. This problem is especially critical in a manufacturing setting where smart factories rely on machine learning for product quality control by analyzing industrial images. Such images are typically large and may only need to be partially analyzed where only a small portion is problematic (e.g., identifying defects on a surface). Since manual labeling these images is expensive, weak supervision is an attractive alternative where the idea is to generate weak labels that are not perfect, but can be produced at scale. Data programming is a recent paradigm in this category where it uses 0 knowledge in the form of labeling functions and combines them into a generative model. Data programming has been successful in applications based on text or structured data and can also be applied to images usually if one can find a way to convert them into structured data. In this work, we expand the horizon of data programming by directly applying it to images without this conversion, which is a common scenario for industrial applications. We propose Inspector Gadget, an image labeling system that combines crowdsourcing, data augmentation, and data programming to produce weak labels at scale for image classification. We perform experiments on real industrial image datasets and show that Inspector Gadget obtains better performance than other weak-labeling techniques: Snuba, GOGGLES, and self-learning baselines using convolutional neural networks (CNNs) without pre-training.",0
"The Software 2.0 era has made machine learning for images more accessible, but obtaining enough labeled data for training remains a significant bottleneck. This is particularly problematic in manufacturing, where machine learning is crucial for product quality control through the analysis of large industrial images. The cost of manual labeling makes weak supervision an attractive alternative, generating weak labels that are not perfect but can be produced at scale. Data programming is a recent paradigm that combines labeling functions to create a generative model, successfully applied to text and structured data. With images, converting them into structured data is typically necessary, but for industrial applications, this is not always possible. In response, we propose Inspector Gadget, an image labeling system that combines crowdsourcing, data augmentation, and data programming to produce weak labels at scale for image classification. Our experiments on real industrial image datasets demonstrate that Inspector Gadget outperforms other weak-labeling techniques, including Snuba, GOGGLES, and self-learning convolutional neural networks (CNNs) without pre-training.",1
"In this paper, we focus on learning low-dimensional embeddings for nodes in graph-structured data. To achieve this, we propose Caps2NE -- a new unsupervised embedding model leveraging a network of two capsule layers. Caps2NE induces a routing process to aggregate feature vectors of context neighbors of a given target node at the first capsule layer, then feed these features into the second capsule layer to infer a plausible embedding for the target node. Experimental results show that our proposed Caps2NE obtains state-of-the-art performances on benchmark datasets for the node classification task. Our code is available at: \url{https://github.com/daiquocnguyen/Caps2NE}.",0
"The objective of this paper is to teach how to learn low-dimensional embeddings for nodes in graph-structured data. To achieve this, Caps2NE is introduced as a new unsupervised embedding model that uses a network of two capsule layers. Caps2NE applies a routing process to gather feature vectors of context neighbors of a given target node at the first capsule layer, and then these features are sent to the second capsule layer to determine a reasonable embedding for the target node. According to experimental results, Caps2NE is highly effective and achieves state-of-the-art performance on benchmark datasets for the node classification task. Our code can be found at: \url{https://github.com/daiquocnguyen/Caps2NE}.",1
"Deep convolutional neural networks (CNNs) have shown outstanding performance in the task of semantically segmenting images. However, applying the same methods on 3D data still poses challenges due to the heavy memory requirements and the lack of structured data. Here, we propose LatticeNet, a novel approach for 3D semantic segmentation, which takes as input raw point clouds. A PointNet describes the local geometry which we embed into a sparse permutohedral lattice. The lattice allows for fast convolutions while keeping a low memory footprint. Further, we introduce DeformSlice, a novel learned data-dependent interpolation for projecting lattice features back onto the point cloud. We present results of 3D segmentation on various datasets where our method achieves state-of-the-art performance.",0
"Although deep convolutional neural networks (CNNs) have shown exceptional ability in semantically segmenting images, their application in 3D data remains challenging as a result of heavy memory usage and unstructured data. To address this issue, we propose LatticeNet, a new approach for 3D semantic segmentation that employs raw point clouds as input. We utilize PointNet to describe local geometries, which we incorporate into a sparse permutohedral lattice for fast convolutions and minimal memory requirements. Additionally, we introduce DeformSlice, a novel learned data-dependent interpolation method that projects lattice features back onto the point cloud. Our method achieves state-of-the-art performance in 3D segmentation on various datasets.",1
"Deep generative models have made tremendous advances in image and signal representation learning and generation. These models employ the full Euclidean space or a bounded subset as the latent space, whose flat geometry, however, is often too simplistic to meaningfully reflect the manifold structure of the data. In this work, we advocate the use of a multi-chart latent space for better data representation. Inspired by differential geometry, we propose a \textbf{Chart Auto-Encoder (CAE)} and prove a universal approximation theorem on its representation capability. We show that the training data size and the network size scale exponentially in approximation error with an exponent depending on the intrinsic dimension of the data manifold. CAE admits desirable manifold properties that auto-encoders with a flat latent space fail to obey, predominantly proximity of data. We conduct extensive experimentation with synthetic and real-life examples to demonstrate that CAE provides reconstruction with high fidelity, preserves proximity in the latent space, and generates new data remaining near the manifold. These experiments show that CAE is advantageous over existing auto-encoders and variants by preserving the topology of the data manifold as well as its geometry.",0
"Significant progress has been made in the realm of image and signal representation learning and generation with the use of deep generative models. These models often employ a Euclidean space or a subset thereof as the latent space, which fails to accurately reflect the manifold structure of the data due to its simplistic flat geometry. This study proposes the use of a multi-chart latent space, inspired by differential geometry, to improve data representation. The proposed Chart Auto-Encoder (CAE) is capable of representing the data manifold and satisfies desirable manifold properties such as proximity of data, which are not upheld by flat latent space auto-encoders. The study demonstrates the superior performance of CAE through extensive experimentation with synthetic and real-life examples, which highlights its ability to accurately reconstruct data, preserve proximity in the latent space, and generate new data that remains close to the manifold. Compared to other auto-encoders and variants, CAE is advantageous in preserving both the topology and geometry of the data manifold.",1
"The paper introduces two new aggregation functions to encode structural knowledge from tree-structured data. They leverage the Canonical and Tensor-Train decompositions to yield expressive context aggregation while limiting the number of model parameters. Finally, we define two novel neural recursive models for trees leveraging such aggregation functions, and we test them on two tree classification tasks, showing the advantage of proposed models when tree outdegree increases.",0
"In this paper, two innovative aggregation functions are presented for encoding structural knowledge from tree-structured data. These functions utilize the Canonical and Tensor-Train decompositions to enable effective context aggregation, while also minimizing the number of model parameters. Additionally, two innovative neural recursive models for trees are introduced, which utilize these aggregation functions. These models are evaluated on two tree classification tasks, demonstrating their superior performance when the tree outdegree is increased.",1
"Sparse incidence tensors can represent a variety of structured data. For example, we may represent attributed graphs using their node-node, node-edge, or edge-edge incidence matrices. In higher dimensions, incidence tensors can represent simplicial complexes and polytopes. In this paper, we formalize incidence tensors, analyze their structure, and present the family of equivariant networks that operate on them. We show that any incidence tensor decomposes into invariant subsets. This decomposition, in turn, leads to a decomposition of the corresponding equivariant linear maps, for which we prove an efficient pooling-and-broadcasting implementation.",0
"Various forms of structured data can be represented by sparse incidence tensors. For instance, attributed graphs can be represented using their node-node, node-edge, or edge-edge incidence matrices. In higher dimensions, simplicial complexes and polytopes can be represented by incidence tensors. This paper provides a formalization of incidence tensors, an analysis of their structure, and a presentation of the family of equivariant networks that manipulate them. We demonstrate that any incidence tensor can be broken down into invariant subsets. Consequently, the corresponding equivariant linear maps can be decomposed as well, and we offer an efficient pooling-and-broadcasting implementation for this decomposition.",1
"Variational autoencoder (VAE) is a widely used generative model for learning latent representations. Burda et al. in their seminal paper showed that learning capacity of VAE is limited by over-pruning. It is a phenomenon where a significant number of latent variables fail to capture any information about the input data and the corresponding hidden units become inactive. This adversely affects learning diverse and interpretable latent representations. As variational graph autoencoder (VGAE) extends VAE for graph-structured data, it inherits the over-pruning problem. In this paper, we adopt a model based approach and propose epitomic VGAE (EVGAE),a generative variational framework for graph datasets which successfully mitigates the over-pruning problem and also boosts the generative ability of VGAE. We consider EVGAE to consist of multiple sparse VGAE models, called epitomes, that are groups of latent variables sharing the latent space. This approach aids in increasing active units as epitomes compete to learn better representation of the graph data. We verify our claims via experiments on three benchmark datasets. Our experiments show that EVGAE has a better generative ability than VGAE. Moreover, EVGAE outperforms VGAE on link prediction task in citation networks.",0
"The Variational autoencoder (VAE) is a commonly used generative model for acquiring latent representations. However, Burda et al. discovered that the VAE's ability to learn is hindered by over-pruning. This results in many latent variables being unable to capture any information about the input data, resulting in inactive hidden units. This negatively affects the VAE's ability to learn a variety of interpretable latent representations. The over-pruning problem is inherited by the Variational graph autoencoder (VGAE), which is an extension of the VAE for graph-structured data. In this study, we propose the epitomic VGAE (EVGAE), a generative variational framework for graph datasets. This framework successfully mitigates the over-pruning problem and improves the VGAE's generative ability. The EVGAE consists of multiple sparse VGAE models called epitomes, which are groups of latent variables that share the latent space. This approach increases active units as epitomes compete to learn a better representation of the graph data. We confirm our claims through experiments on three benchmark datasets. Our results show that EVGAE has better generative ability than VGAE and outperforms VGAE on the link prediction task in citation networks.",1
"Active learning (AL) on attributed graphs has received increasing attention with the prevalence of graph-structured data. Although AL has been widely studied for alleviating label sparsity issues with the conventional non-related data, how to make it effective over attributed graphs remains an open research question. Existing AL algorithms on graphs attempt to reuse the classic AL query strategies designed for non-related data. However, they suffer from two major limitations. First, different AL query strategies calculated in distinct scoring spaces are often naively combined to determine which nodes to be labelled. Second, the AL query engine and the learning of the classifier are treated as two separating processes, resulting in unsatisfactory performance. In this paper, we propose a SEmi-supervised Adversarial active Learning (SEAL) framework on attributed graphs, which fully leverages the representation power of deep neural networks and devises a novel AL query strategy in an adversarial way. Our framework learns two adversarial components: a graph embedding network that encodes both the unlabelled and labelled nodes into a latent space, expecting to trick the discriminator to regard all nodes as already labelled, and a semi-supervised discriminator network that distinguishes the unlabelled from the existing labelled nodes in the latent space. The divergence score, generated by the discriminator in a unified latent space, serves as the informativeness measure to actively select the most informative node to be labelled by an oracle. The two adversarial components form a closed loop to mutually and simultaneously reinforce each other towards enhancing the active learning performance. Extensive experiments on four real-world networks validate the effectiveness of the SEAL framework with superior performance improvements to state-of-the-art baselines.",0
"Attributed graphs have become increasingly important in data analysis, leading to a growing interest in Active Learning (AL) techniques. However, while AL has been widely used to address label sparsity issues in non-related data, its efficacy in attributed graphs remains an open question. Existing AL algorithms for graphs rely on traditional query strategies, which suffer from two key limitations. First, various strategies from different scoring spaces are combined in a naive way to identify nodes for labeling. Second, the AL query and classifier learning processes are treated as separate, resulting in suboptimal performance. To address these issues, we introduce the SEmi-supervised Adversarial active Learning (SEAL) framework, which leverages deep neural networks to develop an innovative query strategy in an adversarial manner. Our approach involves two adversarial components: a graph embedding network that encodes both labeled and unlabeled nodes into a latent space and a semi-supervised discriminator network that distinguishes unlabeled from labeled nodes in this space. The discriminator generates a divergence score that serves as an informativeness measure for selecting the most informative node to label. Our closed-loop adversarial framework reinforces both components simultaneously, leading to superior performance compared to state-of-the-art baselines. We validate our approach with extensive experiments on four real-world networks.",1
"Graph Convolutional Networks (GCNs) have already demonstrated their powerful ability to model the irregular data, e.g., skeletal data in 0 action recognition, providing an exciting new way to fuse rich structural information for nodes residing in different parts of a graph. In 0 action recognition, current works introduce a dynamic graph generation mechanism to better capture the underlying semantic skeleton connections and thus improves the performance. In this paper, we provide an orthogonal way to explore the underlying connections. Instead of introducing an expensive dynamic graph generation paradigm, we build a more efficient GCN on a Riemann manifold, which we think is a more suitable space to model the graph data, to make the extracted representations fit the embedding matrix. Specifically, we present a novel spatial-temporal GCN (ST-GCN) architecture which is defined via the Poincar\'e geometry such that it is able to better model the latent anatomy of the structure data. To further explore the optimal projection dimension in the Riemann space, we mix different dimensions on the manifold and provide an efficient way to explore the dimension for each ST-GCN layer. With the final resulted architecture, we evaluate our method on two current largest scale 3D datasets, i.e., NTU RGB+D and NTU RGB+D 120. The comparison results show that the model could achieve a superior performance under any given evaluation metrics with only 40\% model size when compared with the previous best GCN method, which proves the effectiveness of our model.",0
"The effectiveness of Graph Convolutional Networks (GCNs) in modeling irregular data, particularly skeletal data in 0 action recognition, has been demonstrated. This presents a promising approach to integrating structural information for nodes in different graph sections. To improve performance in 0 action recognition, current studies use dynamic graph generation mechanisms to capture semantic skeleton connections. This paper proposes an alternative approach that utilizes a more efficient GCN on a Riemann manifold, which is better suited for modeling graph data, without the need for expensive dynamic graph generation. The authors present a novel spatial-temporal GCN architecture called ST-GCN, defined via the Poincaré geometry, which better models the latent anatomy of structural data. To further optimize the projection dimension in the Riemann space, the authors mix different dimensions on the manifold and provide an efficient way to explore each ST-GCN layer's dimension. The proposed model achieves superior performance on two of the largest 3D datasets, NTU RGB+D and NTU RGB+D 120, with only 40% of the model size compared to the previous best GCN method. This demonstrates the effectiveness of the proposed model.",1
"The effective representation, processing, analysis, and visualization of large-scale structured data, especially those related to complex domains such as networks and graphs, are one of the key questions in modern machine learning. Graph signal processing (GSP), a vibrant branch of signal processing models and algorithms that aims at handling data supported on graphs, opens new paths of research to address this challenge. In this article, we review a few important contributions made by GSP concepts and tools, such as graph filters and transforms, to the development of novel machine learning algorithms. In particular, our discussion focuses on the following three aspects: exploiting data structure and relational priors, improving data and computational efficiency, and enhancing model interpretability. Furthermore, we provide new perspectives on future development of GSP techniques that may serve as a bridge between applied mathematics and signal processing on one side, and machine learning and network science on the other. Cross-fertilization across these different disciplines may help unlock the numerous challenges of complex data analysis in the modern age.",0
"Modern machine learning faces a significant challenge in effectively representing, processing, analyzing, and visualizing large-scale structured data, particularly in complex domains like networks and graphs. Graph signal processing (GSP) is a dynamic branch of signal processing models and algorithms that seeks to address this challenge by handling data supported on graphs. This article reviews the critical contributions of GSP concepts and tools, such as graph filters and transforms, to the development of new machine learning algorithms. The discussion focuses on three aspects: leveraging data structure and relational priors, enhancing data and computational efficiency, and improving model interpretability. Additionally, the article presents new perspectives on the future development of GSP techniques, which could serve as a bridge between applied mathematics and signal processing on one side, and machine learning and network science on the other. The cross-fertilization of these different disciplines may help overcome the numerous challenges of complex data analysis in the modern era.",1
"Understanding how certain brain regions relate to a specific neurological disorder has been an important area of neuroimaging research. A promising approach to identify the salient regions is using Graph Neural Networks (GNNs), which can be used to analyze graph structured data, e.g. brain networks constructed by functional magnetic resonance imaging (fMRI). We propose an interpretable GNN framework with a novel salient region selection mechanism to determine neurological brain biomarkers associated with disorders. Specifically, we design novel regularized pooling layers that highlight salient regions of interests (ROIs) so that we can infer which ROIs are important to identify a certain disease based on the node pooling scores calculated by the pooling layers. Our proposed framework, Pooling Regularized-GNN (PR-GNN), encourages reasonable ROI-selection and provides flexibility to preserve either individual- or group-level patterns. We apply the PR-GNN framework on a Biopoint Autism Spectral Disorder (ASD) fMRI dataset. We investigate different choices of the hyperparameters and show that PR-GNN outperforms baseline methods in terms of classification accuracy. The salient ROI detection results show high correspondence with the previous neuroimaging-derived biomarkers for ASD.",0
"Neuroimaging research has focused on identifying the relationship between specific brain regions and neurological disorders. Graph Neural Networks (GNNs) have shown promise in analyzing graph structured data, such as brain networks constructed by functional magnetic resonance imaging (fMRI), to identify salient regions. In this study, we propose an interpretable GNN framework with a novel salient region selection mechanism to determine neurological biomarkers associated with disorders. Our framework, called Pooling Regularized-GNN (PR-GNN), includes regularized pooling layers that highlight regions of interests (ROIs) and provides flexibility to preserve either individual- or group-level patterns. We apply PR-GNN to an fMRI dataset for Autism Spectral Disorder (ASD) and demonstrate its superior performance over baseline methods in terms of classification accuracy. The salient ROI detection results are consistent with previous neuroimaging-derived biomarkers for ASD.",1
"Tensor data with rich structural information becomes increasingly important in process modeling, monitoring, and diagnosis. Here structural information is referred to structural properties such as sparsity, smoothness, low-rank, and piecewise constancy. To reveal useful information from tensor data, we propose to decompose the tensor into the summation of multiple components based on different structural information of them. In this paper, we provide a new definition of structural information in tensor data. Based on it, we propose an additive tensor decomposition (ATD) framework to extract useful information from tensor data. This framework specifies a high dimensional optimization problem to obtain the components with distinct structural information. An alternating direction method of multipliers (ADMM) algorithm is proposed to solve it, which is highly parallelable and thus suitable for the proposed optimization problem. Two simulation examples and a real case study in medical image analysis illustrate the versatility and effectiveness of the ATD framework.",0
"The significance of tensor data that contains valuable structural information is growing in importance for process modeling, monitoring, and diagnosis. The structural properties referred to include sparsity, smoothness, low-rank, and piecewise constancy. To extract useful information from tensor data, our proposed approach involves breaking down the tensor into multiple components based on different structural information. In this research, we redefine structural information in tensor data and introduce the additive tensor decomposition (ATD) framework to extract relevant information. The ATD framework involves solving a high dimensional optimization problem to obtain distinct components with structural information. To solve this problem, we propose an alternating direction method of multipliers (ADMM) algorithm that is highly parallelizable. The effectiveness and versatility of the ATD framework are demonstrated through two simulation examples and a real case study in medical image analysis.",1
"Graph neural networks emerge as a promising modeling method for applications dealing with datasets that are best represented in the graph domain. In specific, developing recommendation systems often require addressing sparse structured data which often lacks the feature richness in either the user and/or item side and requires processing within the correct context for optimal performance. These datasets intuitively can be mapped to and represented as networks or graphs. In this paper, we propose the Hierarchical BiGraph Neural Network (HBGNN), a hierarchical approach of using GNNs as recommendation systems and structuring the user-item features using a bigraph framework. Our experimental results show competitive performance with current recommendation system methods and transferability.",0
"The use of graph neural networks is gaining traction as a viable modeling method for datasets that are best suited for representation in the graph domain. For instance, recommendation systems often encounter sparse structured data that lacks feature richness on either the user or item side. As a result, such datasets require processing within the appropriate context to optimize performance. Graphs or networks are intuitive representations for such datasets. This study proposes the Hierarchical BiGraph Neural Network (HBGNN), a hierarchical approach that utilizes GNNs as recommendation systems and organizes user-item features within a bigraph framework. Our experiments show that HBGNN is competitive with current recommendation system methods and can be transferred to other datasets.",1
"Graph convolutional networks gain remarkable success in semi-supervised learning on graph structured data. The key to graph-based semisupervised learning is capturing the smoothness of labels or features over nodes exerted by graph structure. Previous methods, spectral methods and spatial methods, devote to defining graph convolution as a weighted average over neighboring nodes, and then learn graph convolution kernels to leverage the smoothness to improve the performance of graph-based semi-supervised learning. One open challenge is how to determine appropriate neighborhood that reflects relevant information of smoothness manifested in graph structure. In this paper, we propose GraphHeat, leveraging heat kernel to enhance low-frequency filters and enforce smoothness in the signal variation on the graph. GraphHeat leverages the local structure of target node under heat diffusion to determine its neighboring nodes flexibly, without the constraint of order suffered by previous methods. GraphHeat achieves state-of-the-art results in the task of graph-based semi-supervised classification across three benchmark datasets: Cora, Citeseer and Pubmed.",0
"Graph convolutional networks have achieved great success in semi-supervised learning on graph-structured data by capturing the smoothness of labels or features over nodes exerted by graph structure. In the past, spectral and spatial methods have been used to define graph convolution as a weighted average over neighboring nodes and learn graph convolution kernels to improve the performance of graph-based semi-supervised learning. However, determining an appropriate neighborhood that reflects relevant information of smoothness manifested in the graph structure remains an open challenge. To address this issue, we propose GraphHeat, which leverages the heat kernel to enhance low-frequency filters and enforce smoothness in the signal variation on the graph. GraphHeat uses the local structure of the target node under heat diffusion to determine its neighboring nodes in a flexible manner, without the constraint of order suffered by previous methods. Our experiments show that GraphHeat achieves state-of-the-art results in the task of graph-based semi-supervised classification across three benchmark datasets: Cora, Citeseer and Pubmed.",1
"The objective of active learning (AL) is to train classification models with less number of labeled instances by selecting only the most informative instances for labeling. The AL algorithms designed for other data types such as images and text do not perform well on graph-structured data. Although a few heuristics-based AL algorithms have been proposed for graphs, a principled approach is lacking. In this paper, we propose MetAL, an AL approach that selects unlabeled instances that directly improve the future performance of a classification model. For a semi-supervised learning problem, we formulate the AL task as a bilevel optimization problem. Based on recent work in meta-learning, we use the meta-gradients to approximate the impact of retraining the model with any unlabeled instance on the model performance. Using multiple graph datasets belonging to different domains, we demonstrate that MetAL efficiently outperforms existing state-of-the-art AL algorithms.",0
"Active learning (AL) aims to train classification models using fewer labeled instances by choosing the most informative ones for labeling. However, AL algorithms designed for other data types have not been effective for graph-structured data. Although a few heuristics-based AL algorithms have been proposed for graphs, a principled approach is yet to be established. This paper introduces MetAL, an AL approach that selects unlabeled instances to directly enhance the future performance of a classification model. We formulate the AL task as a bilevel optimization problem for semi-supervised learning problems. Using meta-gradients based on recent work in meta-learning, we approximate the impact of retraining the model with any unlabeled instance on the model performance. We demonstrate that MetAL outperforms existing state-of-the-art AL algorithms efficiently using multiple graph datasets from various domains.",1
"Convolutional neural networks typically consist of many convolutional layers followed by one or more fully connected layers. While convolutional layers map between high-order activation tensors, the fully connected layers operate on flattened activation vectors. Despite empirical success, this approach has notable drawbacks. Flattening followed by fully connected layers discards multilinear structure in the activations and requires many parameters. We address these problems by incorporating tensor algebraic operations that preserve multilinear structure at every layer. First, we introduce Tensor Contraction Layers (TCLs) that reduce the dimensionality of their input while preserving their multilinear structure using tensor contraction. Next, we introduce Tensor Regression Layers (TRLs), which express outputs through a low-rank multilinear mapping from a high-order activation tensor to an output tensor of arbitrary order. We learn the contraction and regression factors end-to-end, and produce accurate nets with fewer parameters. Additionally, our layers regularize networks by imposing low-rank constraints on the activations (TCL) and regression weights (TRL). Experiments on ImageNet show that, applied to VGG and ResNet architectures, TCLs and TRLs reduce the number of parameters compared to fully connected layers by more than 65% while maintaining or increasing accuracy. In addition to the space savings, our approach's ability to leverage topological structure can be crucial for structured data such as MRI. In particular, we demonstrate significant performance improvements over comparable architectures on three tasks associated with the UK Biobank dataset.",0
"Convolutional neural networks usually include multiple convolutional layers followed by one or more fully connected layers. The convolutional layers map between high-order activation tensors, while the fully connected layers operate on flattened activation vectors. However, this approach has some drawbacks, as it discards multilinear structure in the activations and requires many parameters. To overcome these issues, we propose incorporating tensor algebraic operations that preserve multilinear structure at every layer. Specifically, we introduce Tensor Contraction Layers (TCLs) that reduce dimensionality while preserving multilinear structure via tensor contraction, and Tensor Regression Layers (TRLs) that use a low-rank multilinear mapping from a high-order activation tensor to an output tensor of arbitrary order. We learn the contraction and regression factors end-to-end, and our layers impose low-rank constraints on the activations and regression weights, thereby regularizing networks. Experiments on ImageNet demonstrate that our approach reduces the number of parameters compared to fully connected layers by over 65% while maintaining or even increasing accuracy. Our approach's ability to leverage topological structure can also be advantageous for structured data such as MRI, as we demonstrate significant performance improvements over comparable architectures on three tasks associated with the UK Biobank dataset.",1
"Active search is the process of identifying high-value data points in a large and often high-dimensional parameter space that can be expensive to evaluate. Traditional active search techniques like Bayesian optimization trade off exploration and exploitation over consecutive evaluations, and have historically focused on single or small (<5) numbers of examples evaluated per round. As modern data sets grow, so does the need to scale active search to large data sets and batch sizes. In this paper, we present a general hierarchical framework based on bandit algorithms to scale active search to large batch sizes by maximizing information derived from the unique structure of each dataset. Our hierarchical framework, Hierarchical Batch Bandit Search (HBBS), strategically distributes batch selection across a learned embedding space by facilitating wide exploration of different structural elements within a dataset. We focus our application of HBBS on modern biology, where large batch experimentation is often fundamental to the research process, and demonstrate batch design of biological sequences (protein and DNA). We also present a new Gym environment to easily simulate diverse biological sequences and to enable more comprehensive evaluation of active search methods across heterogeneous data sets. The HBBS framework improves upon standard performance, wall-clock, and scalability benchmarks for batch search by using a broad exploration strategy across coarse partitions and fine-grained exploitation within each partition of structured data.",0
"Active search involves identifying valuable data points in a vast and complex parameter space that can be costly to assess. Traditional active search methods, such as Bayesian optimization, have historically focused on a small number of evaluations per round, typically less than five, and balance exploration and exploitation. As data sets continue to expand, the need to scale active search to larger batch sizes becomes more critical. To address this issue, we propose a hierarchical framework based on bandit algorithms, called Hierarchical Batch Bandit Search (HBBS), that leverages the unique structure of each dataset to maximize information. Our approach strategically distributes batch selection across a learned embedding space, enabling broad exploration of structural elements within the dataset. We apply HBBS to modern biology, where large batch experimentation is crucial, and demonstrate its effectiveness in designing biological sequences (protein and DNA). Additionally, we introduce a Gym environment that facilitates diverse biological sequence simulation and evaluation of active search methods across varied data sets. HBBS outperforms standard methods regarding performance, wall-clock time, and scalability by conducting broad exploration across coarse partitions and fine-grained exploitation within each partition of structured data.",1
"Predicting interactions among heterogenous graph structured data has numerous applications such as knowledge graph completion, recommendation systems and drug discovery. Often times, the links to be predicted belong to rare types such as the case in repurposing drugs for novel diseases. This motivates the task of few-shot link prediction. Typically, GCNs are ill-equipped in learning such rare link types since the relation embedding is not learned in an inductive fashion. This paper proposes an inductive RGCN for learning informative relation embeddings even in the few-shot learning regime. The proposed inductive model significantly outperforms the RGCN and state-of-the-art KGE models in few-shot learning tasks. Furthermore, we apply our method on the drug-repurposing knowledge graph (DRKG) for discovering drugs for Covid-19. We pose the drug discovery task as link prediction and learn embeddings for the biological entities that partake in the DRKG. Our initial results corroborate that several drugs used in clinical trials were identified as possible drug candidates. The method in this paper are implemented using the efficient deep graph learning (DGL)",0
"The ability to predict interactions within heterogeneous graph-structured data has numerous practical applications, including knowledge graph completion, drug discovery, and recommendation systems. However, predicting links between rare types of data, such as when repurposing drugs for novel diseases, presents a challenge requiring few-shot link prediction. Unfortunately, GCNs are not well-suited for learning rare link types, as they do not learn relation embeddings in an inductive manner. To address this issue, our paper proposes an inductive RGCN model that can learn informative relation embeddings even in the few-shot learning regime. Our proposed model significantly outperforms both RGCN and state-of-the-art KGE models in few-shot learning tasks. We also apply our method to the drug-repurposing knowledge graph (DRKG) and demonstrate its usefulness for discovering drugs for Covid-19 by posing the drug discovery task as link prediction and learning embeddings for the biological entities within the DRKG. Our initial results show that several drugs used in clinical trials were identified as possible drug candidates. We implement our method using the efficient deep graph learning (DGL) framework.",1
"Applying network science approaches to investigate the functions and anatomy of the 0 brain is prevalent in modern medical imaging analysis. Due to the complex network topology, for an individual brain, mining a discriminative network representation from the multimodal brain networks is non-trivial. The recent success of deep learning techniques on graph-structured data suggests a new way to model the non-linear cross-modality relationship. However, current deep brain network methods either ignore the intrinsic graph topology or require a network basis shared within a group. To address these challenges, we propose a novel end-to-end deep graph representation learning (Deep Multimodal Brain Networks - DMBN) to fuse multimodal brain networks. Specifically, we decipher the cross-modality relationship through a graph encoding and decoding process. The higher-order network mappings from brain structural networks to functional networks are learned in the node domain. The learned network representation is a set of node features that are informative to induce brain saliency maps in a supervised manner. We test our framework in both synthetic and real image data. The experimental results show the superiority of the proposed method over some other state-of-the-art deep brain network models.",0
"The use of network science methods to explore the 0 brain's functions and structure is common in modern medical imaging analysis. However, mining a discriminative network representation from the complex network topology of an individual brain's multimodal brain networks is challenging. Although deep learning techniques have shown promise in modeling the non-linear cross-modality relationship, current deep brain network approaches either disregard the intrinsic graph topology or require a network basis shared within a group. To overcome these obstacles, we introduce an innovative deep graph representation learning approach to fuse multimodal brain networks, known as Deep Multimodal Brain Networks (DMBN). We accomplish this by decoding and encoding the graph, deciphering the cross-modality relationship, and learning higher-order network mappings from brain structural networks to functional networks in the node domain. The learned network representation is a set of node features that can generate brain saliency maps in a supervised way. We evaluate our framework on both synthetic and real image data and demonstrate its superiority over other state-of-the-art deep brain network models.",1
"Despite diverse efforts to mine various modalities of medical data, the conversations between physicians and patients at the time of care remain an untapped source of insights. In this paper, we leverage this data to extract structured information that might assist physicians with post-visit documentation in electronic health records, potentially lightening the clerical burden. In this exploratory study, we describe a new dataset consisting of conversation transcripts, post-visit summaries, corresponding supporting evidence (in the transcript), and structured labels. We focus on the tasks of recognizing relevant diagnoses and abnormalities in the review of organ systems (RoS). One methodological challenge is that the conversations are long (around 1500 words), making it difficult for modern deep-learning models to use them as input. To address this challenge, we extract noteworthy utterances---parts of the conversation likely to be cited as evidence supporting some summary sentence. We find that by first filtering for (predicted) noteworthy utterances, we can significantly boost predictive performance for recognizing both diagnoses and RoS abnormalities.",0
"Although medical data has been widely mined through various methods, the conversations between physicians and patients during care remain an unexplored resource for valuable insights. The aim of this paper is to utilize this data to extract structured information that can aid physicians in the documentation process after visits, potentially reducing their administrative workload. We present an exploratory study that introduces a novel dataset containing conversation transcripts, post-visit summaries, supporting evidence from the transcript, and structured labels. Our focus is on identifying relevant diagnoses and abnormalities in the review of organ systems (RoS). However, the main issue we face is the lengthy nature of the conversations, which can be up to 1500 words. This poses a challenge for modern deep-learning models that require shorter inputs. To overcome this obstacle, we identify noteworthy utterances that are likely to support a summary sentence. By filtering for these utterances, we improve the predictive performance of recognizing both diagnoses and RoS abnormalities.",1
"Graph representation learning nowadays becomes fundamental in analyzing graph-structured data. Inspired by recent success of contrastive methods, in this paper, we propose a novel framework for unsupervised graph representation learning by leveraging a contrastive objective at the node level. Specifically, we generate two graph views by corruption and learn node representations by maximizing the agreement of node representations in these two views. To provide diverse node contexts for the contrastive objective, we propose a hybrid scheme for generating graph views on both structure and attribute levels. Besides, we provide theoretical justification behind our motivation from two perspectives, mutual information and the classical triplet loss. We perform empirical experiments on both transductive and inductive learning tasks using a variety of real-world datasets. Experimental experiments demonstrate that despite its simplicity, our proposed method consistently outperforms existing state-of-the-art methods by large margins. Moreover, our unsupervised method even surpasses its supervised counterparts on transductive tasks, demonstrating its great potential in real-world applications.",0
"The analysis of graph-structured data requires the use of graph representation learning, which has become increasingly important. This paper proposes a new framework for unsupervised graph representation learning that leverages a contrastive objective at the node level, inspired by the success of contrastive methods. The proposed method generates two graph views by corruption, and learns node representations by maximizing the agreement of node representations in these views. A hybrid scheme is proposed for generating graph views on both structure and attribute levels to provide diverse node contexts for the contrastive objective. The motivation behind the proposed method is justified theoretically from two perspectives, mutual information and the classical triplet loss. Empirical experiments on both transductive and inductive learning tasks using real-world datasets demonstrate that the proposed method consistently outperforms existing state-of-the-art methods by significant margins, despite its simplicity. Additionally, the unsupervised method performs better than its supervised counterparts on transductive tasks, demonstrating its potential in real-world applications.",1
"Complex Event Processing (CEP) is an event processing paradigm to perform real-time analytics over streaming data and match high-level event patterns. Presently, CEP is limited to process structured data stream. Video streams are complicated due to their unstructured data model and limit CEP systems to perform matching over them. This work introduces a graph-based structure for continuous evolving video streams, which enables the CEP system to query complex video event patterns. We propose the Video Event Knowledge Graph (VEKG), a graph driven representation of video data. VEKG models video objects as nodes and their relationship interaction as edges over time and space. It creates a semantic knowledge representation of video data derived from the detection of high-level semantic concepts from the video using an ensemble of deep learning models. A CEP-based state optimization - VEKG-Time Aggregated Graph (VEKG-TAG) is proposed over VEKG representation for faster event detection. VEKG-TAG is a spatiotemporal graph aggregation method that provides a summarized view of the VEKG graph over a given time length. We defined a set of nine event pattern rules for two domains (Activity Recognition and Traffic Management), which act as a query and applied over VEKG graphs to discover complex event patterns. To show the efficacy of our approach, we performed extensive experiments over 801 video clips across 10 datasets. The proposed VEKG approach was compared with other state-of-the-art methods and was able to detect complex event patterns over videos with F-Score ranging from 0.44 to 0.90. In the given experiments, the optimized VEKG-TAG was able to reduce 99% and 93% of VEKG nodes and edges, respectively, with 5.19X faster search time, achieving sub-second median latency of 4-20 milliseconds.",0
"The event processing paradigm known as Complex Event Processing (CEP) is used for real-time analytics of streaming data and for matching high-level event patterns. However, CEP has limitations in processing unstructured data streams like video streams. To address this issue, the Video Event Knowledge Graph (VEKG) is introduced, which uses a graph-based structure for continuous evolving video streams enabling the CEP system to query complex video event patterns. VEKG represents video data through a semantic knowledge graph by modeling video objects as nodes and their relationship interactions as edges over time and space. VEKG-TAG, a CEP-based state optimization, is proposed over VEKG representation for faster event detection. It is a spatiotemporal graph aggregation method that summarizes the VEKG graph over a given time length. A set of nine event pattern rules are defined for two domains (Activity Recognition and Traffic Management) that act as a query and applied over VEKG graphs to discover complex event patterns. Extensive experiments were performed over 801 video clips across 10 datasets, showing the efficacy of the proposed VEKG approach in detecting complex event patterns over videos with F-Score ranging from 0.44 to 0.90. The optimized VEKG-TAG was able to reduce 99% and 93% of VEKG nodes and edges, respectively, with 5.19X faster search time, achieving sub-second median latency of 4-20 milliseconds.",1
"We present Geo2DR (Geometric to Distributed Representations), a GPU ready Python library for unsupervised learning on graph-structured data using discrete substructure patterns and neural language models. It contains efficient implementations of popular graph decomposition algorithms and neural language models in PyTorch which can be combined to learn representations of graphs using the distributive hypothesis. Furthermore, Geo2DR comes with general data processing and loading methods to bring substantial speed-up in the training of the neural language models. Through this we provide a modular set of tools and methods to quickly construct systems capable of learning distributed representations of graphs. This is useful for replication of existing methods, modification, or development of completely new methods. This paper serves to present the Geo2DR library and perform a comprehensive comparative analysis of existing methods re-implemented using Geo2DR across widely used graph classification benchmarks. Geo2DR displays a high reproducibility of results in published methods and interoperability with other libraries useful for distributive language modelling.",0
"Introducing Geo2DR (Geometric to Distributed Representations), a Python library optimized for GPU use in unsupervised learning on graph-structured data. It utilizes discrete substructure patterns and neural language models, integrating efficient implementations of popular graph decomposition algorithms and neural language models in PyTorch. By combining these methods, Geo2DR enables the learning of graph representations based on the distributive hypothesis. In addition to the aforementioned features, Geo2DR offers general data processing and loading functions to significantly improve the training speed of neural language models. Our library provides a modular toolkit for constructing systems that can quickly learn distributed representations of graphs, whether they are existing methods or innovative approaches. In this paper, we present the Geo2DR library and conduct a thorough comparative analysis of existing methods re-implemented using Geo2DR, evaluating their performance on widely used graph classification benchmarks. We demonstrate that Geo2DR yields reproducible results similar to those of published methods, and is interoperable with other libraries that are useful for distributive language modelling.",1
"Graph neural networks (GNNs) extends the functionality of traditional neural networks to graph-structured data. Similar to CNNs, an optimized design of graph convolution and pooling is key to success. Borrowing ideas from physics, we propose a path integral based graph neural networks (PAN) for classification and regression tasks on graphs. Specifically, we consider a convolution operation that involves every path linking the message sender and receiver with learnable weights depending on the path length, which corresponds to the maximal entropy random walk. It generalizes the graph Laplacian to a new transition matrix we call maximal entropy transition (MET) matrix derived from a path integral formalism. Importantly, the diagonal entries of the MET matrix are directly related to the subgraph centrality, thus providing a natural and adaptive pooling mechanism. PAN provides a versatile framework that can be tailored for different graph data with varying sizes and structures. We can view most existing GNN architectures as special cases of PAN. Experimental results show that PAN achieves state-of-the-art performance on various graph classification/regression tasks, including a new benchmark dataset from statistical mechanics we propose to boost applications of GNN in physical sciences.",0
"The functionality of traditional neural networks can be extended to graph-structured data through the use of graph neural networks (GNNs). To achieve success, an optimized design of graph convolution and pooling, similar to that of CNNs, is key. We propose a path integral based graph neural network (PAN) for classification and regression tasks on graphs, drawing inspiration from physics. Our approach involves a convolution operation that utilizes every path linking the message sender and receiver, with learnable weights dependent on the path length. This corresponds to the maximal entropy random walk and generalizes the graph Laplacian to a new transition matrix known as the maximal entropy transition (MET) matrix, derived from a path integral formalism. The diagonal entries of the MET matrix are directly related to the subgraph centrality, providing a natural and adaptive pooling mechanism. PAN is a versatile framework that can be customized for different graph data with varying sizes and structures, and most existing GNN architectures can be viewed as special cases of PAN. Our experimental results demonstrate that PAN achieves state-of-the-art performance on various graph classification/regression tasks, including a new benchmark dataset from statistical mechanics that we propose to enhance the application of GNN in physical sciences.",1
"Graph convolutional networks (GCNs) are a powerful deep learning approach for graph-structured data. Recently, GCNs and subsequent variants have shown superior performance in various application areas on real-world datasets. Despite their success, most of the current GCN models are shallow, due to the {\em over-smoothing} problem. In this paper, we study the problem of designing and analyzing deep graph convolutional networks. We propose the GCNII, an extension of the vanilla GCN model with two simple yet effective techniques: {\em Initial residual} and {\em Identity mapping}. We provide theoretical and empirical evidence that the two techniques effectively relieves the problem of over-smoothing. Our experiments show that the deep GCNII model outperforms the state-of-the-art methods on various semi- and full-supervised tasks. Code is available at https://github.com/chennnM/GCNII .",0
"Graph-structured data can be effectively analyzed using Graph Convolutional Networks (GCNs), which are a popular deep learning technique. Real-world datasets have demonstrated the superior performance of GCNs and their variants in different application areas. However, the over-smoothing issue has led to most GCN models being shallow. In this study, we investigate the design and analysis of deep graph convolutional networks. Our proposed model, GCNII, is an extension of the vanilla GCN model that employs two practical and efficient techniques: Initial Residual and Identity Mapping. We provide both theoretical and empirical evidence that these two techniques alleviate the over-smoothing problem effectively. Our experiments demonstrate that the deep GCNII model outperforms existing methods in various semi- and full-supervised tasks. The code for GCNII is available at https://github.com/chennnM/GCNII.",1
"Graph Convolutional Networks (GCNs) have recently become the primary choice for learning from graph-structured data, superseding hash fingerprints in representing chemical compounds. However, GCNs lack the ability to take into account the ordering of node neighbors, even when there is a geometric interpretation of the graph vertices that provides an order based on their spatial positions. To remedy this issue, we propose Spatial Graph Convolutional Network (SGCN) which uses spatial features to efficiently learn from graphs that can be naturally located in space. Our contribution is threefold: we propose a GCN-inspired architecture which (i) leverages node positions, (ii) is a proper generalization of both GCNs and Convolutional Neural Networks (CNNs), (iii) benefits from augmentation which further improves the performance and assures invariance with respect to the desired properties. Empirically, SGCN outperforms state-of-the-art graph-based methods on image classification and chemical tasks.",0
"Recently, Graph Convolutional Networks (GCNs) have become the preferred method for learning from data structured as graphs, replacing hash fingerprints for representing chemical compounds. However, GCNs do not consider the ordering of node neighbors, even when the vertices of the graph have a geometric interpretation that provides a natural order based on their spatial positions. To address this limitation, we introduce the Spatial Graph Convolutional Network (SGCN), which uses spatial features to efficiently learn from graphs that can be naturally located in space. Our contribution is threefold: we propose a GCN-inspired architecture that (i) incorporates node positions, (ii) generalizes GCNs and Convolutional Neural Networks (CNNs), and (iii) benefits from augmentation to improve performance and ensure invariance with respect to desired properties. Empirical results demonstrate that SGCN outperforms state-of-the-art graph-based methods for tasks including image classification and chemical analysis.",1
"Complex applications such as big data analytics involve different forms of coupling relationships that reflect interactions between factors related to technical, business (domain-specific) and environmental (including socio-cultural and economic) aspects. There are diverse forms of couplings embedded in poor-structured and ill-structured data. Such couplings are ubiquitous, implicit and/or explicit, objective and/or subjective, heterogeneous and/or homogeneous, presenting complexities to existing learning systems in statistics, mathematics and computer sciences, such as typical dependency, association and correlation relationships. Modeling and learning such couplings thus is fundamental but challenging. This paper discusses the concept of coupling learning, focusing on the involvement of coupling relationships in learning systems. Coupling learning has great potential for building a deep understanding of the essence of business problems and handling challenges that have not been addressed well by existing learning theories and tools. This argument is verified by several case studies on coupling learning, including handling coupling in recommender systems, incorporating couplings into coupled clustering, coupling document clustering, coupled recommender algorithms and coupled behavior analysis for groups.",0
"Big data analytics applications are complex and involve various types of couplings that reflect interactions between technical, business, and environmental factors. These couplings are found in poorly and ill-structured data, and can be implicit or explicit, subjective or objective, and heterogeneous or homogeneous. These complexities pose challenges for learning systems in statistics, math, and computer science, such as dependency, association, and correlation relationships. This paper explores the concept of coupling learning, which involves studying coupling relationships in learning systems. Coupling learning has the potential to deepen our understanding of business problems and address challenges that existing learning theories and tools cannot solve. This argument is supported by several case studies on coupling learning, including handling couplings in recommender systems, coupled clustering, document clustering, and behavior analysis for groups.",1
"We introduce a family of multilayer graph kernels and establish new links between graph convolutional neural networks and kernel methods. Our approach generalizes convolutional kernel networks to graph-structured data, by representing graphs as a sequence of kernel feature maps, where each node carries information about local graph substructures. On the one hand, the kernel point of view offers an unsupervised, expressive, and easy-to-regularize data representation, which is useful when limited samples are available. On the other hand, our model can also be trained end-to-end on large-scale data, leading to new types of graph convolutional neural networks. We show that our method achieves competitive performance on several graph classification benchmarks, while offering simple model interpretation. Our code is freely available at https://github.com/claying/GCKN.",0
"A new family of multilayer graph kernels is presented, which establishes novel connections between kernel methods and graph convolutional neural networks. The proposed approach extends convolutional kernel networks to graph-structured data by treating the graphs as a sequence of kernel feature maps, where each node carries information about local graph substructures. This enables an expressive, unsupervised, and easily regulable data representation that is particularly useful when dealing with limited samples. Moreover, the model can be trained end-to-end on large-scale data, leading to new types of graph convolutional neural networks. Competitive performance on several graph classification benchmarks is demonstrated, and the model is easy to interpret. Our code is available for free at https://github.com/claying/GCKN.",1
Predicting and discovering drug-drug interactions (DDIs) is an important problem and has been studied extensively both from medical and machine learning point of view. Almost all of the machine learning approaches have focused on text data or textual representation of the structural data of drugs. We present the first work that uses drug structure images as the input and utilizes a Siamese convolutional network architecture to predict DDIs.,0
"The problem of predicting and identifying drug-drug interactions (DDIs) is of significant importance and has been extensively researched from both medical and machine learning perspectives. Prior machine learning methods have primarily concentrated on analyzing text data or representing the structural data of drugs in textual form. However, our research introduces a novel approach by utilizing drug structure images as input and implementing a Siamese convolutional network architecture to forecast DDIs.",1
"Graph neural networks are promising architecture for learning and inference with graph-structured data. Yet difficulties in modelling the ``parts'' and their ``interactions'' still persist in terms of graph classification, where graph-level representations are usually obtained by squeezing the whole graph into a single vector through graph pooling. From complex systems point of view, mixing all the parts of a system together can affect both model interpretability and predictive performance, because properties of a complex system arise largely from the interaction among its components. We analyze the intrinsic difficulty in graph classification under the unified concept of ``resolution dilemmas'' with learning theoretic recovery guarantees, and propose ``SLIM'', an inductive neural network model for Structural Landmarking and Interaction Modelling. It turns out, that by solving the resolution dilemmas, and leveraging explicit interacting relation between component parts of a graph to explain its complexity, SLIM is more interpretable, accurate, and offers new insight in graph representation learning.",0
"Graph neural networks show promise as an architecture for learning and inference with graph-structured data. However, difficulties still exist in modeling the parts of a graph and their interactions, particularly in graph classification where graph-level representations are obtained through graph pooling. This approach can negatively impact model interpretability and predictive performance since complex system properties arise from the interactions between components. To address this issue, we propose an inductive neural network model called ""SLIM"" for Structural Landmarking and Interaction Modelling. We analyze the challenges of graph classification using the concept of ""resolution dilemmas"" and provide learning theoretic recovery guarantees. By resolving these dilemmas and incorporating explicit interaction modeling between components, SLIM offers improved interpretability, accuracy, and insights for graph representation learning.",1
"Graph neural networks (GNNs) have been demonstrated to be powerful in modeling graph-structured data. However, training GNNs usually requires abundant task-specific labeled data, which is often arduously expensive to obtain. One effective way to reduce the labeling effort is to pre-train an expressive GNN model on unlabeled data with self-supervision and then transfer the learned model to downstream tasks with only a few labels. In this paper, we present the GPT-GNN framework to initialize GNNs by generative pre-training. GPT-GNN introduces a self-supervised attributed graph generation task to pre-train a GNN so that it can capture the structural and semantic properties of the graph. We factorize the likelihood of the graph generation into two components: 1) Attribute Generation and 2) Edge Generation. By modeling both components, GPT-GNN captures the inherent dependency between node attributes and graph structure during the generative process. Comprehensive experiments on the billion-scale Open Academic Graph and Amazon recommendation data demonstrate that GPT-GNN significantly outperforms state-of-the-art GNN models without pre-training by up to 9.1% across various downstream tasks.",0
"GNNs have proven to be effective in modeling data with a graph structure. However, training GNNs typically requires a vast amount of labeled data, which can be costly and time-consuming to obtain. To reduce the labeling effort, a solution is to pre-train a powerful GNN model using self-supervision on unlabeled data, and then transfer the learned model to downstream tasks that require only a few labels. Our paper presents the GPT-GNN framework, which initializes GNNs through generative pre-training. GPT-GNN introduces a self-supervised task for generating attributed graphs to pre-train a GNN so that it can effectively capture the graph's structural and semantic properties. We factorize the likelihood of graph generation into Attribute Generation and Edge Generation components. By modeling both components, GPT-GNN captures the intrinsic link between node attributes and graph structure during the generative process. Comprehensive experiments on the Open Academic Graph and Amazon recommendation data indicate that GPT-GNN significantly outperforms state-of-the-art GNN models without pre-training by up to 9.1% in various downstream tasks.",1
"Although graph neural networks (GNNs) have made great progress recently on learning from graph-structured data in practice, their theoretical guarantee on generalizability remains elusive in the literature. In this paper, we provide a theoretically-grounded generalizability analysis of GNNs with one hidden layer for both regression and binary classification problems. Under the assumption that there exists a ground-truth GNN model (with zero generalization error), the objective of GNN learning is to estimate the ground-truth GNN parameters from the training data. To achieve this objective, we propose a learning algorithm that is built on tensor initialization and accelerated gradient descent. We then show that the proposed learning algorithm converges to the ground-truth GNN model for the regression problem, and to a model sufficiently close to the ground-truth for the binary classification problem. Moreover, for both cases, the convergence rate of the proposed learning algorithm is proven to be linear and faster than the vanilla gradient descent algorithm. We further explore the relationship between the sample complexity of GNNs and their underlying graph properties. Lastly, we provide numerical experiments to demonstrate the validity of our analysis and the effectiveness of the proposed learning algorithm for GNNs.",0
"While graph neural networks (GNNs) have made significant advancements in learning from graph-structured data in practical applications, their capacity for generalizability is still lacking in theoretical literature. This paper aims to fill this gap by offering a thorough analysis of the generalizability of GNNs with one hidden layer, addressing both binary classification and regression problems. Our approach assumes the existence of a ground-truth GNN model with zero generalization error, and proposes a learning algorithm that utilizes tensor initialization and accelerated gradient descent to estimate the parameters of this model. We demonstrate that this algorithm converges to the ground-truth GNN model for regression problems and to a model that is sufficiently close to the ground-truth for binary classification problems. Additionally, we prove that the convergence rate of our algorithm is linear and faster than that of the vanilla gradient descent algorithm for both cases. We also investigate the relationship between GNN sample complexity and underlying graph properties, and conduct numerical experiments to validate our analysis and the effectiveness of our proposed learning algorithm.",1
"Graphs are ubiquitous in modelling relational structures. Recent endeavours in machine learning for graph-structured data have led to many architectures and learning algorithms. However, the graph used by these algorithms is often constructed based on inaccurate modelling assumptions and/or noisy data. As a result, it fails to represent the true relationships between nodes. A Bayesian framework which targets posterior inference of the graph by considering it as a random quantity can be beneficial. In this paper, we propose a novel non-parametric graph model for constructing the posterior distribution of graph adjacency matrices. The proposed model is flexible in the sense that it can effectively take into account the output of graph-based learning algorithms that target specific tasks. In addition, model inference scales well to large graphs. We demonstrate the advantages of this model in three different problem settings: node classification, link prediction and recommendation.",0
"The use of graphs for modelling relational structures is widespread. Machine learning has led to the development of many architectures and learning algorithms for graph-structured data. However, these algorithms often use inaccurate modelling assumptions or noisy data to construct the graph, resulting in the failure to truly represent the relationships between nodes. A Bayesian framework that treats the graph as a random quantity and targets posterior inference can provide benefits. This paper proposes a new non-parametric graph model for constructing the posterior distribution of graph adjacency matrices. The model is flexible and can effectively incorporate the output of graph-based learning algorithms that target specific tasks. Additionally, the model inference scales well for large graphs. The advantages of this model are demonstrated in three problem settings: node classification, link prediction, and recommendation.",1
"Graph Neural Networks (GNNs) have achieved a lot of success on graph-structured data. However, it is observed that the performance of graph neural networks does not improve as the number of layers increases. This effect, known as over-smoothing, has been analyzed mostly in linear cases. In this paper, we build upon previous results \cite{oono2019graph} to further analyze the over-smoothing effect in the general graph neural network architecture. We show when the weight matrix satisfies the conditions determined by the spectrum of augmented normalized Laplacian, the Dirichlet energy of embeddings will converge to zero, resulting in the loss of discriminative power. Using Dirichlet energy to measure ""expressiveness"" of embedding is conceptually clean; it leads to simpler proofs than \cite{oono2019graph} and can handle more non-linearities.",0
"Graph Neural Networks (GNNs) have been successful in processing graph-structured data, but their performance does not improve with an increase in the number of layers due to a phenomenon called over-smoothing. This effect has been studied primarily in linear cases, but this paper builds upon previous research to analyze over-smoothing in the general graph neural network architecture. By examining the weight matrix and its relationship with the spectrum of augmented normalized Laplacian, we demonstrate that the Dirichlet energy of embeddings will eventually converge to zero, resulting in a loss of discriminative power. Measuring ""expressiveness"" of embedding through Dirichlet energy leads to simpler proof techniques than previous research and can accommodate more non-linearities.",1
"Graph classification aims to extract accurate information from graph-structured data for classification and is becoming more and more important in graph learning community. Although Graph Neural Networks (GNNs) have been successfully applied to graph classification tasks, most of them overlook the scarcity of labeled graph data in many applications. For example, in bioinformatics, obtaining protein graph labels usually needs laborious experiments. Recently, few-shot learning has been explored to alleviate this problem with only given a few labeled graph samples of test classes. The shared sub-structures between training classes and test classes are essential in few-shot graph classification. Exiting methods assume that the test classes belong to the same set of super-classes clustered from training classes. However, according to our observations, the label spaces of training classes and test classes usually do not overlap in real-world scenario. As a result, the existing methods don't well capture the local structures of unseen test classes. To overcome the limitation, in this paper, we propose a direct method to capture the sub-structures with well initialized meta-learner within a few adaptation steps. More specifically, (1) we propose a novel framework consisting of a graph meta-learner, which uses GNNs based modules for fast adaptation on graph data, and a step controller for the robustness and generalization of meta-learner; (2) we provide quantitative analysis for the framework and give a graph-dependent upper bound of the generalization error based on our framework; (3) the extensive experiments on real-world datasets demonstrate that our framework gets state-of-the-art results on several few-shot graph classification tasks compared to baselines.",0
"The objective of graph classification is to obtain precise information from data structured in graphs to classify, and it is increasingly crucial in the graph learning community. Despite the successful application of Graph Neural Networks (GNNs) in graph classification tasks, most of them tend to ignore the scarcity of labeled graph data in several applications. For instance, in bioinformatics, obtaining protein graph labels requires laborious experiments. Recently, few-shot learning has been explored to mitigate this issue by providing only a few labeled graph samples of test classes. In few-shot graph classification, the shared sub-structures between training classes and test classes play a pivotal role. Currently, existing methods assume that the test classes belong to the same set of super-classes clustered from training classes. However, our observations suggest that the label spaces of training classes and test classes usually do not overlap in real-world scenarios. Consequently, the existing methods fail to capture the local structures of unseen test classes. To address this limitation, we propose a direct method to capture the sub-structures with a well-initialized meta-learner within a few adaptation steps. Specifically, we introduce a novel framework comprising a graph meta-learner that uses GNNs based modules for rapid adaptation on graph data, and a step controller for the robustness and generalization of the meta-learner. We provide quantitative analysis for the framework and give a graph-dependent upper bound of the generalization error based on our framework. Our extensive experiments on real-world datasets demonstrate that our framework outperforms the baselines and achieves state-of-the-art results on several few-shot graph classification tasks.",1
"The success of deep learning has revolutionized many fields of research including areas of computer vision, text and speech processing. Enormous research efforts have led to numerous methods that are capable of efficiently analyzing data, especially in the Euclidean space. However, many problems are posed in non-Euclidean domains modeled as general graphs with complex connection patterns. Increased problem complexity and computational power constraints have limited early approaches to static and small-sized graphs. In recent years, a rising interest in machine learning on graph-structured data has been accompanied by improved methods that overcome the limitations of their predecessors. These methods paved the way for dealing with large-scale and time-dynamic graphs. This work aims to provide an overview of early and modern graph neural network based machine learning methods for node-level prediction tasks. Under the umbrella of taxonomies already established in the literature, we explain the core concepts and provide detailed explanations for convolutional methods that have had strong impact. In addition, we introduce common benchmarks and present selected applications from various areas. Finally, we discuss open problems for further research.",0
"The impact of deep learning has transformed numerous fields of research, including computer vision, text processing, and speech processing. Thanks to extensive research, many methods have been developed that can efficiently analyze data in the Euclidean space. However, numerous problems arise in non-Euclidean domains, modeled as general graphs with complex connection patterns. These problems are more complex and require more computational power, leading to early approaches being limited to static and small-sized graphs. Fortunately, recent years have seen a growing interest in machine learning on graph-structured data, with improved methods that overcome the limitations of their predecessors. These methods have made it possible to deal with large-scale and time-dynamic graphs. This paper provides an overview of early and modern graph neural network based machine learning methods for node-level prediction tasks. We explain the core concepts and provide detailed explanations for convolutional methods that have had a strong impact, using existing taxonomies. We also introduce common benchmarks and present selected applications from various areas. Finally, we discuss open problems for further research.",1
"We consider the problem of whether a given decision model, working with structured data, has individual fairness. Following the work of Dwork, a model is individually biased (or unfair) if there is a pair of valid inputs which are close to each other (according to an appropriate metric) but are treated differently by the model (different class label, or large difference in output), and it is unbiased (or fair) if no such pair exists. Our objective is to construct verifiers for proving individual fairness of a given model, and we do so by considering appropriate relaxations of the problem. We construct verifiers which are sound but not complete for linear classifiers, and kernelized polynomial/radial basis function classifiers. We also report the experimental results of evaluating our proposed algorithms on publicly available datasets.",0
"The issue at hand is whether a decision model utilizing structured data can be considered individually fair. To determine this, we refer to the research of Dwork, who defines an unfair model as one that treats two similar inputs differently, while a fair model does not. Our aim is to create verifiers that can prove individual fairness in a given model by approaching the problem in a more relaxed manner. We have developed verifiers that are reliable but not comprehensive for linear classifiers, as well as kernelized polynomial/radial basis function classifiers. Additionally, we have conducted experiments using publicly accessible datasets to evaluate the effectiveness of our proposed algorithms.",1
"Most machine learning models for structured data encode the structural knowledge of a node by leveraging simple aggregation functions (in neural models, typically a weighted sum) of the information in the node's neighbourhood. Nevertheless, the choice of simple context aggregation functions, such as the sum, can be widely sub-optimal. In this work we introduce a general approach to model aggregation of structural context leveraging a tensor-based formulation. We show how the exponential growth in the size of the parameter space can be controlled through an approximation based on the Tucker tensor decomposition. This approximation allows limiting the parameters space size, decoupling it from its strict relation with the size of the hidden encoding space. By this means, we can effectively regulate the trade-off between expressivity of the encoding, controlled by the hidden size, computational complexity and model generalisation, influenced by parameterisation. Finally, we introduce a new Tensorial Tree-LSTM derived as an instance of our framework and we use it to experimentally assess our working hypotheses on tree classification scenarios.",0
"To represent a node's structural knowledge in machine learning models for structured data, simple aggregation functions like a weighted sum are commonly used. However, this approach may not always be optimal. In this study, we propose a new method for modeling structural context aggregation using a tensor-based formulation. We demonstrate how the Tucker tensor decomposition can be used to control the exponential growth of parameter space, which allows us to regulate the trade-off between encoding expressivity, computational complexity, and model generalization. As an application of this framework, we introduce a Tensorial Tree-LSTM and evaluate it in tree classification scenarios.",1
"Graph deep learning has recently emerged as a powerful ML concept allowing to generalize successful deep neural architectures to non-Euclidean structured data. Such methods have shown promising results on a broad spectrum of applications ranging from social science, biomedicine, and particle physics to computer vision, graphics, and chemistry. One of the limitations of the majority of the current graph neural network architectures is that they are often restricted to the transductive setting and rely on the assumption that the underlying graph is known and fixed. In many settings, such as those arising in medical and healthcare applications, this assumption is not necessarily true since the graph may be noisy, partially- or even completely unknown, and one is thus interested in inferring it from the data. This is especially important in inductive settings when dealing with nodes not present in the graph at training time. Furthermore, sometimes such a graph itself may convey insights that are even more important than the downstream task. In this paper, we introduce Differentiable Graph Module (DGM), a learnable function predicting the edge probability in the graph relevant for the task, that can be combined with convolutional graph neural network layers and trained in an end-to-end fashion. We provide an extensive evaluation of applications from the domains of healthcare (disease prediction), brain imaging (gender and age prediction), computer graphics (3D point cloud segmentation), and computer vision (zero-shot learning). We show that our model provides a significant improvement over baselines both in transductive and inductive settings and achieves state-of-the-art results.",0
"Recently, graph deep learning has become a powerful machine learning concept that allows for the generalization of successful deep neural architectures to non-Euclidean structured data. These methods have demonstrated promising results across a wide range of applications, including social science, biomedicine, particle physics, computer vision, graphics, and chemistry. However, the majority of current graph neural network architectures have limitations, such as being restricted to the transductive setting and assuming that the underlying graph is known and fixed. This can be problematic in various settings, such as medical and healthcare applications, where the graph may be noisy, partially or completely unknown, and where one may be interested in inferring it from the data. Additionally, the graph may convey insights that are more important than the downstream task. Therefore, this paper presents the Differentiable Graph Module (DGM), a learnable function that predicts the edge probability in the graph relevant to the task. It can be combined with convolutional graph neural network layers and trained in an end-to-end fashion. Extensive evaluations are performed across healthcare (disease prediction), brain imaging (gender and age prediction), computer graphics (3D point cloud segmentation), and computer vision (zero-shot learning). The results show a significant improvement over baselines in both transductive and inductive settings, achieving state-of-the-art results.",1
"Applying machine learning algorithms to private data, such as financial or medical data, while preserving their confidentiality, is a difficult task. Homomorphic Encryption (HE) is acknowledged for its ability to allow computation on encrypted data, where both the input and output are encrypted, which therefore enables secure inference on private data. Nonetheless, because of the constraints of HE, such as its inability to evaluate non-polynomial functions or to perform arbitrary matrix multiplication efficiently, only inference of linear models seem usable in practice in the HE paradigm so far.   In this paper, we propose Cryptotree, a framework that enables the use of Random Forests (RF), a very powerful learning procedure compared to linear regression, in the context of HE. To this aim, we first convert a regular RF to a Neural RF, then adapt this to fit the HE scheme CKKS, which allows HE operations on real values. Through SIMD operations, we are able to have quick inference and prediction results better than the original RF on encrypted data.",0
"Performing machine learning on sensitive data, such as financial or medical records, while keeping the data private, poses a challenge. Homomorphic Encryption (HE) is recognized for its capability to permit computation on encrypted data, where both input and output are encrypted, thereby enabling secure inference on private data. However, the limitations of HE, like the inability to evaluate non-polynomial functions or carry out arbitrary matrix multiplication efficiently, restrict the use of only linear models in the HE paradigm. In this study, we present Cryptotree, a framework that allows the use of Random Forests (RF), a potent learning method when compared to linear regression, in the context of HE. To achieve this, we first transform a standard RF to a Neural RF, and then adjust it to fit the HE scheme CKKS, which enables HE operations on real values. With SIMD operations, we can make fast inferences and obtain prediction results superior to those of the original RF on encrypted data.",1
"Graph Attention Network (GAT) and GraphSAGE are neural network architectures that operate on graph-structured data and have been widely studied for link prediction and node classification. One challenge raised by GraphSAGE is how to smartly combine neighbour features based on graph structure. GAT handles this problem through attention, however the challenge with GAT is its scalability over large and dense graphs. In this work, we proposed a new architecture to address these issues that is more efficient and is capable of incorporating different edge type information. It generates node representations by attending to neighbours sampled from weighted multi-step transition probabilities. We conduct experiments on both transductive and inductive settings. Experiments achieved comparable or better results on several graph benchmarks, including the Cora, Citeseer, Pubmed, PPI, Twitter, and YouTube datasets.",0
"Neural network structures, namely Graph Attention Network (GAT) and GraphSAGE, have been extensively studied for node classification and link prediction on graph-structured data. GraphSAGE presents a challenge in terms of effectively merging neighbor features based on graph structure. On the other hand, GAT solves this issue through attention, but its ability to scale over vast and dense graphs remains a challenge. In this study, we propose a novel architecture that is more efficient and can incorporate various edge type information. The proposed architecture generates node representations by paying attention to neighbors selected from weighted multi-step transition probabilities. We conduct experiments on both transductive and inductive settings and achieve comparable or better results on several graph benchmarks, including Twitter, YouTube, Cora, Citeseer, Pubmed, and PPI datasets.",1
"Nowadays, graph-structured data are increasingly used to model complex systems. Meanwhile, detecting anomalies from graph has become a vital research problem of pressing societal concerns. Anomaly detection is an unsupervised learning task of identifying rare data that differ from the majority. As one of the dominant anomaly detection algorithms, One Class Support Vector Machine has been widely used to detect outliers. However, those traditional anomaly detection methods lost their effectiveness in graph data. Since traditional anomaly detection methods are stable, robust and easy to use, it is vitally important to generalize them to graph data. In this work, we propose One Class Graph Neural Network (OCGNN), a one-class classification framework for graph anomaly detection. OCGNN is designed to combine the powerful representation ability of Graph Neural Networks along with the classical one-class objective. Compared with other baselines, OCGNN achieves significant improvements in extensive experiments.",0
"In recent years, complex systems have been modeled using graph-structured data, and detecting anomalies in graphs has become a pressing societal concern. Anomaly detection involves identifying rare data that differ from the majority and is an unsupervised learning task. One Class Support Vector Machine has been a dominant anomaly detection algorithm that is widely used to detect outliers. However, traditional anomaly detection methods are ineffective in graph data. It is important to generalize these methods to graph data because they are stable, robust, and easy to use. This work proposes a one-class classification framework called One Class Graph Neural Network (OCGNN) for graph anomaly detection. OCGNN combines the powerful representation ability of Graph Neural Networks with the classical one-class objective. Extensive experiments demonstrate that OCGNN achieves significant improvements compared to other baselines.",1
"Buying a home is one of the most important buying decisions people have to make in their life. The latest research on real-estate appraisal focuses on incorporating image data in addition to structured data into the modeling process. This research measures the prediction performance of satellite images and structured data by using convolutional neural networks. The resulting CNN model trained performs 7% better in MAE than the advanced baseline of a neural network trained on structured data. Moreover, sliding-window heatmap provides visual interpretability of satellite images, revealing that neighborhood structures are essential in the price estimation.",0
"Purchasing a house is a crucial decision that individuals make at some point in their lives. Recent studies conducted on real estate valuation concentrate on integrating image data with structured data to enhance the modeling process. The research evaluates the accuracy of satellite images and structured data in forecasting by utilizing convolutional neural networks. The CNN model produced performs better by 7% in MAE compared to the advanced baseline of a neural network that uses only structured data. Additionally, the sliding-window heatmap presents a clear representation of satellite images, indicating that the neighboring structures play a crucial role in determining the property price.",1
"Graph Convolutional Networks (GCNs) have been successfully applied to analyze non-grid data, where the classical convolutional neural networks (CNNs) cannot be directly used. One similarity shared by GCNs and CNNs is the requirement of massive amount of labeled data for network training. In addition, GCNs need the adjacency matrix as input to define the relationship between those non-grid data, which leads to all of data including training, validation and test data typically forms only one graph structures data for training. Furthermore, the adjacency matrix is usually pre-defined and stationary, which makes the data augmentation strategies cannot be employed on the constructed graph structures data to augment the amount of training data. To further improve the learning capacity and model performance under the limited training data, in this paper, we propose two types of self-supervised learning strategies to exploit available information from the input graph structure data itself. Our proposed self-supervised learning strategies are examined on two representative GCN models with three public citation network datasets - Citeseer, Cora and Pubmed. The experimental results demonstrate the generalization ability as well as the portability of our proposed strategies, which can significantly improve the performance of GCNs with the power of self-supervised learning in improving feature learning.",0
"Graph Convolutional Networks (GCNs) have proven to be effective in analyzing non-grid data, which cannot be directly processed by classical Convolutional Neural Networks (CNNs). Both GCNs and CNNs require large amounts of labeled data for network training. However, GCNs also require the adjacency matrix as input to establish the relationships between non-grid data, resulting in a single graph structure for training, validation, and test data. Moreover, the adjacency matrix is generally fixed, precluding the use of data augmentation techniques to increase the amount of training data. To overcome these limitations and enhance model performance with limited training data, this paper proposes two types of self-supervised learning strategies that utilize available information from the input graph structure data. The proposed strategies are evaluated on two representative GCN models using three public citation network datasets (Citeseer, Cora, and Pubmed). The experimental results demonstrate the generalization and portability of the proposed strategies, which can significantly improve feature learning and GCN performance through the power of self-supervised learning.",1
"We present Graph Random Neural Features (GRNF), a novel embedding method from graph-structured data to real vectors based on a family of graph neural networks. The embedding naturally deals with graph isomorphism and preserves the metric structure of the graph domain, in probability. In addition to being an explicit embedding method, it also allows us to efficiently and effectively approximate graph metric distances (as well as complete kernel functions); a criterion to select the embedding dimension trading off the approximation accuracy with the computational cost is also provided. GRNF can be used within traditional processing methods or as a training-free input layer of a graph neural network. The theoretical guarantees that accompany GRNF ensure that the considered graph distance is metric, hence allowing to distinguish any pair of non-isomorphic graphs.",0
"Our team has developed a new technique called Graph Random Neural Features (GRNF) that uses a range of graph neural networks to create an embedding method for transforming graph-structured data into real vectors. This method handles graph isomorphism naturally and maintains the metric structure of the graph domain with a high probability. Not only is GRNF an explicit embedding method, but it also enables us to efficiently and effectively estimate graph metric distances, including complete kernel functions. We have also outlined a criterion for choosing the embedding dimension based on a trade-off between approximation accuracy and computational cost. GRNF can be utilized in traditional processing methods or as an input layer of a graph neural network without the need for training. The theoretical guarantees that come with GRNF ensure that the graph distance is metric, which makes it possible to differentiate between any pair of non-isomorphic graphs.",1
"Variational Autoencoders (VAEs) provide a theoretically-backed and popular framework for deep generative models. However, learning a VAE from data poses still unanswered theoretical questions and considerable practical challenges. In this work, we propose an alternative framework for generative modeling that is simpler, easier to train, and deterministic, yet has many of the advantages of VAEs. We observe that sampling a stochastic encoder in a Gaussian VAE can be interpreted as simply injecting noise into the input of a deterministic decoder. We investigate how substituting this kind of stochasticity, with other explicit and implicit regularization schemes, can lead to an equally smooth and meaningful latent space without forcing it to conform to an arbitrarily chosen prior. To retrieve a generative mechanism to sample new data, we introduce an ex-post density estimation step that can be readily applied also to existing VAEs, improving their sample quality. We show, in a rigorous empirical study, that the proposed regularized deterministic autoencoders are able to generate samples that are comparable to, or better than, those of VAEs and more powerful alternatives when applied to images as well as to structured data such as molecules. \footnote{An implementation is available at: \url{https://github.com/ParthaEth/Regularized_autoencoders-RAE-}}",0
"VAEs are a popular and theoretically sound framework for deep generative models. However, learning a VAE from data presents unresolved theoretical concerns and significant practical difficulties. This study offers an alternative framework for generative modeling that is less complex, more trainable, and deterministic, while retaining many of the benefits of VAEs. By interpreting stochastic encoder sampling in Gaussian VAEs as noise injection into deterministic decoders, we investigate how diversifying explicit and implicit regularization schemes can produce a smooth and meaningful latent space without imposing an arbitrary prior. We introduce an ex-post density estimation step to extract a generative mechanism for sampling new data, which can enhance the quality of existing VAE samples. Our empirical study demonstrates that regularized deterministic autoencoders can generate samples that match or surpass those of VAEs and other powerful alternatives for images and structured data, with the implementation available at \url{https://github.com/ParthaEth/Regularized_autoencoders-RAE-}.",1
"Graph classification is an important task on graph-structured data with many real-world applications. The goal of graph classification task is to train a classifier using a set of training graphs. Recently, Graph Neural Networks (GNNs) have greatly advanced the task of graph classification. When building a GNN model for graph classification, the graphs in the training set are usually assumed to be identically distributed. However, in many real-world applications, graphs in the same dataset could have dramatically different structures, which indicates that these graphs are likely non-identically distributed. Therefore, in this paper, we aim to develop graph neural networks for graphs that are not non-identically distributed. Specifically, we propose a general non-IID graph neural network framework, i.e., Non-IID-GNN. Given a graph, Non-IID-GNN can adapt any existing graph neural network model to generate a sample-specific model for this graph. Comprehensive experiments on various graph classification benchmarks demonstrate the effectiveness of the proposed framework. We will release the code of the proposed framework upon the acceptance of the paper.",0
"Graph classification is a crucial task with numerous real-world applications, involving the use of graph-structured data. The objective of this task is to train a classifier with a set of training graphs. Graph Neural Networks (GNNs) have made significant advancements in this area. However, while building a GNN model for graph classification, it is typically assumed that the graphs in the training set are identically distributed. In reality, graphs in the same dataset may have remarkably different structures, indicating that they are likely non-identically distributed. Therefore, this paper introduces a new framework, Non-IID-GNN, for developing graph neural networks that can handle non-identically distributed graphs. Non-IID-GNN is a general framework capable of adapting any existing graph neural network model to generate a sample-specific model for a given graph. The effectiveness of this framework is demonstrated through extensive experiments on various graph classification benchmarks. Upon acceptance of the paper, the code for the proposed framework will be released.",1
"Graph Neural Networks (GNNs) are efficient approaches to process graph-structured data. Modelling long-distance node relations is essential for GNN training and applications. However, conventional GNNs suffer from bad performance in modelling long-distance node relations due to limited-layer information propagation. Existing studies focus on building deep GNN architectures, which face the over-smoothing issue and cannot model node relations in particularly long distance. To address this issue, we propose to model long-distance node relations by simply relying on shallow GNN architectures with two solutions: (1) Implicitly modelling by learning to predict node pair relations (2) Explicitly modelling by adding edges between nodes that potentially have the same label. To combine our two solutions, we propose a model-agnostic training framework named HighwayGraph, which overcomes the challenge of insufficient labeled nodes by sampling node pairs from the training set and adopting the self-training method. Extensive experimental results show that our HighwayGraph achieves consistent and significant improvements over four representative GNNs on three benchmark datasets.",0
"Efficient methods for processing graph-structured data are Graph Neural Networks (GNNs). To train and apply GNNs, it is crucial to model long-distance node relations. However, conventional GNNs have limited-layer information propagation, leading to poor performance in modelling long-distance node relations. Prior studies focus on building deep GNN architectures, which face the over-smoothing issue and cannot model node relations over long distances. To resolve this issue, we suggest two solutions for modelling long-distance node relations: (1) Implicitly modelling by learning to predict node pair relations and (2) Explicitly modelling by adding edges between nodes that potentially have the same label. We propose a model-agnostic training framework called HighwayGraph to combine these solutions. HighwayGraph overcomes the challenge of insufficient labeled nodes by sampling node pairs from the training set and adopting the self-training method. Our extensive experimental results reveal that HighwayGraph outperforms four representative GNNs on three benchmark datasets, achieving consistent and significant improvements.",1
"Learning on graph structured data has drawn increasing interest in recent years. Frameworks like Graph Convolutional Networks (GCNs) have demonstrated their ability to capture structural information and obtain good performance in various tasks. In these frameworks, node aggregation schemes are typically used to capture structural information: a node's feature vector is recursively computed by aggregating features of its neighboring nodes. However, most of aggregation schemes treat all connections in a graph equally, ignoring node feature similarities. In this paper, we re-interpret node aggregation from the perspective of kernel weighting, and present a framework to consider feature similarity in an aggregation scheme. Specifically, we show that normalized adjacency matrix is equivalent to a neighbor-based kernel matrix in a Krein Space. We then propose feature aggregation as the composition of the original neighbor-based kernel and a learnable kernel to encode feature similarities in a feature space. We further show how the proposed method can be extended to Graph Attention Network (GAT). Experimental results demonstrate better performance of our proposed framework in several real-world applications.",0
"Recently, there has been a growing interest in learning on data structured as graphs. Graph Convolutional Networks (GCNs), for instance, have proven capable of capturing structural information and achieving high performance in various tasks. In these networks, node aggregation techniques are commonly used to aggregate features of neighboring nodes and recursively compute a node's feature vector. However, most of these aggregation techniques treat all graph connections equally, disregarding node feature similarities. This paper introduces a new perspective on node aggregation that considers kernel weighting and proposes a framework that accounts for feature similarity in the aggregation scheme. Specifically, the authors demonstrate that the normalized adjacency matrix is equivalent to a neighbor-based kernel matrix in a Krein Space. They then suggest feature aggregation as the combination of the original neighbor-based kernel with a learnable kernel that encodes feature similarities in a feature space. The authors also show how their proposed method can be extended to Graph Attention Network (GAT). The experimental results demonstrate that their proposed framework outperforms existing methods in several real-world applications.",1
"Graph Neural Networks (GNNs) are powerful to learn the representation of graph-structured data. Most of the GNNs use the message-passing scheme, where the embedding of a node is iteratively updated by aggregating the information of its neighbors. To achieve a better expressive capability of node influences, attention mechanism has grown to be popular to assign trainable weights to the nodes in aggregation. Though the attention-based GNNs have achieved remarkable results in various tasks, a clear understanding of their discriminative capacities is missing. In this work, we present a theoretical analysis of the representational properties of the GNN that adopts the attention mechanism as an aggregator. Our analysis determines all cases when those attention-based GNNs can always fail to distinguish certain distinct structures. Those cases appear due to the ignorance of cardinality information in attention-based aggregation. To improve the performance of attention-based GNNs, we propose cardinality preserved attention (CPA) models that can be applied to any kind of attention mechanisms. Our experiments on node and graph classification confirm our theoretical analysis and show the competitive performance of our CPA models.",0
"Graph Neural Networks (GNNs) possess the ability to acquire an understanding of graph-structured data by learning its representation. The majority of GNNs make use of a message-passing system, whereby a node's embedding is progressively updated by consolidating the information of its neighbors. Attention mechanisms have become popular for enhancing node influences by assigning trainable weights to the nodes during aggregation. Despite the significant accomplishments of attention-based GNNs in different tasks, there is a lack of comprehension regarding their ability to distinguish between different structures. This study presents a theoretical evaluation of the representational properties of GNNs that employ attention mechanisms as an aggregator. Our analysis identifies cases where attention-based GNNs may fail to discriminate certain unique structures due to the disregard for cardinality information during aggregation. In order to enhance the performance of attention-based GNNs, we introduce cardinality preserved attention (CPA) models that can be applied to any type of attention mechanism. Our experimentation on node and graph classification validates our theoretical analysis and demonstrates the competitive performance of our CPA models.",1
"Graph Convolutional Networks (GCNs) have been widely used due to their outstanding performance in processing graph-structured data. However, the undirected graphs limit their application scope. In this paper, we extend spectral-based graph convolution to directed graphs by using first- and second-order proximity, which can not only retain the connection properties of the directed graph, but also expand the receptive field of the convolution operation. A new GCN model, called DGCN, is then designed to learn representations on the directed graph, leveraging both the first- and second-order proximity information. We empirically show the fact that GCNs working only with DGCNs can encode more useful information from graph and help achieve better performance when generalized to other models. Moreover, extensive experiments on citation networks and co-purchase datasets demonstrate the superiority of our model against the state-of-the-art methods.",0
"Because of their excellent ability to process data structured as graphs, Graph Convolutional Networks (GCNs) are widely used. However, their use is limited by the fact that they can only process undirected graphs. This paper proposes a solution to this problem by extending spectral-based graph convolution to directed graphs. This is achieved by incorporating first- and second-order proximity, which not only preserves the connection properties of the directed graph, but also expands the receptive field of the convolution operation. A new GCN model named DGCN is developed to learn representations on the directed graph, leveraging both first- and second-order proximity information. Our empirical findings show that by using DGCN, GCNs can encode more useful information from graphs and achieve better performance when generalized to other models. Additionally, we demonstrate the superiority of our model against the state-of-the-art methods through extensive experiments on citation networks and co-purchase datasets.",1
"Representation learning over graph structured data has been mostly studied in static graph settings while efforts for modeling dynamic graphs are still scant. In this paper, we develop a novel hierarchical variational model that introduces additional latent random variables to jointly model the hidden states of a graph recurrent neural network (GRNN) to capture both topology and node attribute changes in dynamic graphs. We argue that the use of high-level latent random variables in this variational GRNN (VGRNN) can better capture potential variability observed in dynamic graphs as well as the uncertainty of node latent representation. With semi-implicit variational inference developed for this new VGRNN architecture (SI-VGRNN), we show that flexible non-Gaussian latent representations can further help dynamic graph analytic tasks. Our experiments with multiple real-world dynamic graph datasets demonstrate that SI-VGRNN and VGRNN consistently outperform the existing baseline and state-of-the-art methods by a significant margin in dynamic link prediction.",0
"The focus of previous studies on representation learning for graph structured data has been primarily on static graph settings, with little attention given to modeling dynamic graphs. This paper proposes a new approach using a hierarchical variational model that includes additional latent random variables to jointly model the hidden states of a graph recurrent neural network (GRNN). This approach can effectively capture changes in both topology and node attributes in dynamic graphs by utilizing high-level latent random variables. The semi-implicit variational inference technique is applied to this new VGRNN architecture (SI-VGRNN), which enables flexible non-Gaussian latent representations to improve dynamic graph analytic tasks. Experimental results using various real-world dynamic graph datasets demonstrate that SI-VGRNN and VGRNN outperform existing baseline and state-of-the-art methods significantly in dynamic link prediction.",1
"Automated anatomical labeling plays a vital role in coronary artery disease diagnosing procedure. The main challenge in this problem is the large individual variability inherited in 0 anatomy. Existing methods usually rely on the position information and the prior knowledge of the topology of the coronary artery tree, which may lead to unsatisfactory performance when the main branches are confusing. Motivated by the wide application of the graph neural network in structured data, in this paper, we propose a conditional partial-residual graph convolutional network (CPR-GCN), which takes both position and CT image into consideration, since CT image contains abundant information such as branch size and spanning direction. Two majority parts, a Partial-Residual GCN and a conditions extractor, are included in CPR-GCN. The conditions extractor is a hybrid model containing the 3D CNN and the LSTM, which can extract 3D spatial image features along the branches. On the technical side, the Partial-Residual GCN takes the position features of the branches, with the 3D spatial image features as conditions, to predict the label for each branches. While on the mathematical side, our approach twists the partial differential equation (PDE) into the graph modeling. A dataset with 511 subjects is collected from the clinic and annotated by two experts with a two-phase annotation process. According to the five-fold cross-validation, our CPR-GCN yields 95.8% meanRecall, 95.4% meanPrecision and 0.955 meanF1, which outperforms state-of-the-art approaches.",0
"The process of diagnosing coronary artery disease relies heavily on automated anatomical labeling. The main challenge in this process is the inherent variability in 0 anatomy, which can cause confusion when identifying the main branches of the coronary artery tree. Existing methods typically use position information and prior knowledge of the topology of the artery tree, but this approach can be unsatisfactory. This paper proposes a new approach called the conditional partial-residual graph convolutional network (CPR-GCN), which takes both position and CT image data into account. The CPR-GCN includes two main components: a Partial-Residual GCN and a conditions extractor. The conditions extractor uses a hybrid model of 3D CNN and LSTM to extract 3D spatial image features along the branches. The Partial-Residual GCN predicts the label for each branch based on position features and 3D spatial image features. Our approach uses graph modeling to twist the partial differential equation (PDE). We collected a dataset of 511 subjects and annotated it with a two-phase annotation process. In five-fold cross-validation, our CPR-GCN achieved better results than existing approaches, with a meanRecall of 95.8%, meanPrecision of 95.4%, and meanF1 of 0.955.",1
"In this paper, we propose a dimensionality reduction method applied to tensor-structured data as a hidden layer (we call it TensorProjection Layer) in a convolutional neural network. Our proposed method transforms input tensors into ones with a smaller dimension by projection. The directions of projection are viewed as training parameters associated with our proposed layer and trained via a supervised learning criterion such as minimization of the cross-entropy loss function. We discuss the gradients of the loss function with respect to the parameters associated with our proposed layer. We also implement simple numerical experiments to evaluate the performance of the TensorProjection Layer.",0
The TensorProjection Layer is introduced as a dimensionality reduction technique for tensor-structured data within a convolutional neural network. This approach involves projecting input tensors onto lower dimensional ones using trainable parameters associated with our proposed layer. We utilize a supervised learning criterion that minimizes the cross-entropy loss function to train these parameters. We examine the gradients of the loss function with respect to our proposed layer's parameters and conduct numerical experiments to assess the TensorProjection Layer's effectiveness.,1
"We present a prior for manifold structured data, such as surfaces of 3D shapes, where deep neural networks are adopted to reconstruct a target shape using gradient descent starting from a random initialization. We show that surfaces generated this way are smooth, with limiting behavior characterized by Gaussian processes, and we mathematically derive such properties for fully-connected as well as convolutional networks. We demonstrate our method in a variety of manifold reconstruction applications, such as point cloud denoising and interpolation, achieving considerably better results against competitive baselines while requiring no training data. We also show that when training data is available, our method allows developing alternate parametrizations of surfaces under the framework of AtlasNet, leading to a compact network architecture and better reconstruction results on standard image to shape reconstruction benchmarks.",0
"Our research focuses on developing a prior for data with manifold structure, such as the surfaces of 3D shapes. We utilize deep neural networks in the form of gradient descent, starting from a random initialization, to reconstruct a target shape. Through our experiments, we demonstrate that the resulting surfaces are smooth and can be characterized by Gaussian processes. We prove the mathematical properties for both fully-connected and convolutional networks. Our method has been tested in manifold reconstruction applications, such as point cloud denoising and interpolation, and has achieved superior results compared to other methods without requiring any training data. Additionally, our approach allows for the development of alternate parametrizations of surfaces under the AtlasNet framework, leading to a more compact network architecture and better reconstruction results on standard image to shape reconstruction benchmarks, when training data is available.",1
"Graph-structured data arise in many scenarios. A fundamental problem is to quantify the similarities of graphs for tasks such as classification. Graph kernels are positive-semidefinite functions that decompose graphs into substructures and compare them. One problem in the effective implementation of this idea is that the substructures are not independent, which leads to high-dimensional feature space. In addition, graph kernels cannot capture the high-order complex interactions between vertices. To mitigate these two problems, we propose a framework called DeepMap to learn deep representations for graph feature maps. The learnt deep representation for a graph is a dense and low-dimensional vector that captures complex high-order interactions in a vertex neighborhood. DeepMap extends Convolutional Neural Networks (CNNs) to arbitrary graphs by aligning vertices across graphs and building the receptive field for each vertex. We empirically validate DeepMap on various graph classification benchmarks and demonstrate that it achieves state-of-the-art performance.",0
"Graph-structured data is prevalent in several scenarios, and a crucial task is to determine the similarities between graphs to perform classification. Graph kernels are functions that are positive-semidefinite and break down graphs into substructures to compare them. However, the problem with graph kernels is that the substructures are not independent, leading to a feature space with high dimensions. Furthermore, graph kernels fall short in capturing high-order complex interactions between vertices. To address these issues, we propose a framework called DeepMap that learns deep representations for graph feature maps. The learned deep representation is a dense and low-dimensional vector that captures complex high-order interactions in a vertex neighborhood. DeepMap extends Convolutional Neural Networks (CNNs) to arbitrary graphs by aligning vertices across graphs and building the receptive field for each vertex. We validate DeepMap using various graph classification benchmarks and demonstrate its state-of-the-art performance through empirical evidence.",1
"Object Cluster Hierarchies is a new variant of Hierarchical Cluster Analysis that gains interest in the field of Machine Learning. Being still at an early stage of development, the lack of tools for systematic analysis of Object Cluster Hierarchies inhibits its further improvement. In this paper we address this issue by proposing a generator of synthetic hierarchical data that can be used for benchmarking Object Cluster Hierarchy methods. The article presents a thorough empirical and theoretical analysis of the generator and provides guidance on how to control its parameters. Conducted experiments show the usefulness of the data generator that is capable of producing a wide range of differently structured data. Further, benchmarking datasets that mirror the most common types of hierarchies are generated and made available to the public, together with the developed generator (http://kio.pwr.edu.pl/?page\_id=396).",0
"Object Cluster Hierarchies is a new form of Hierarchical Cluster Analysis that has piqued interest in the Machine Learning field. However, its potential for improvement is hindered by the lack of systematic analysis tools. To address this issue, we propose a synthetic hierarchical data generator in this paper that can serve as a benchmark for Object Cluster Hierarchy methods. The article includes a comprehensive empirical and theoretical analysis of the generator, as well as instructions on how to adjust its parameters. The experiments demonstrate the usefulness of the data generator, which can produce a diverse range of structures. Additionally, we create benchmarking datasets that reflect the most common types of hierarchies and make them publicly available, along with the developed generator (http://kio.pwr.edu.pl/?page\_id=396).",1
"Vector representations of graphs and relational structures, whether hand-crafted feature vectors or learned representations, enable us to apply standard data analysis and machine learning techniques to the structures. A wide range of methods for generating such embeddings have been studied in the machine learning and knowledge representation literature. However, vector embeddings have received relatively little attention from a theoretical point of view.   Starting with a survey of embedding techniques that have been used in practice, in this paper we propose two theoretical approaches that we see as central for understanding the foundations of vector embeddings. We draw connections between the various approaches and suggest directions for future research.",0
"By utilizing vector representations of graphs and relational structures, either through manual feature vectors or learned representations, we are able to employ conventional data analysis and machine learning methods to these structures. The machine learning and knowledge representation literature has extensively explored various techniques for generating such embeddings. However, vector embeddings have not been thoroughly examined from a theoretical standpoint. This paper initially surveys embedding techniques utilized in practical applications, and then introduces two theoretical approaches that are fundamental to comprehending the fundamentals of vector embeddings. Additionally, we establish correlations between different approaches and suggest potential avenues for further research.",1
"Over the past few years, we have seen fundamental breakthroughs in core problems in machine learning, largely driven by advances in deep neural networks. At the same time, the amount of data collected in a wide array of scientific domains is dramatically increasing in both size and complexity. Taken together, this suggests many exciting opportunities for deep learning applications in scientific settings. But a significant challenge to this is simply knowing where to start. The sheer breadth and diversity of different deep learning techniques makes it difficult to determine what scientific problems might be most amenable to these methods, or which specific combination of methods might offer the most promising first approach. In this survey, we focus on addressing this central issue, providing an overview of many widely used deep learning models, spanning visual, sequential and graph structured data, associated tasks and different training methods, along with techniques to use deep learning with less data and better interpret these complex models --- two central considerations for many scientific use cases. We also include overviews of the full design process, implementation tips, and links to a plethora of tutorials, research summaries and open-sourced deep learning pipelines and pretrained models, developed by the community. We hope that this survey will help accelerate the use of deep learning across different scientific domains.",0
"The field of machine learning has experienced significant advancements in recent years, particularly in deep neural networks, resulting in breakthroughs in core problems. Meanwhile, the amount of data collected in various scientific domains has significantly increased in size and complexity. As a result, there are many exciting opportunities for deep learning applications in scientific settings. However, it is challenging to determine where to begin due to the broad range and diversity of deep learning techniques. It is difficult to identify which scientific problems are best suited for these methods or which particular combination of techniques might provide the most effective initial approach. This survey aims to address this central issue by providing an overview of widely used deep learning models for visual, sequential, and graph structured data, along with associated tasks and various training methods. It also covers techniques for using deep learning with less data and better interpreting complex models, which are two essential considerations for scientific use cases. Additionally, it includes overviews of the full design process, implementation tips, and links to a wide range of tutorials, research summaries, and open-sourced deep learning pipelines and pretrained models developed by the community. The hope is that this survey will accelerate the use of deep learning across different scientific domains.",1
"Technology and the fruition of cultural heritage are becoming increasingly more entwined, especially with the advent of smart audio guides, virtual and augmented reality, and interactive installations. Machine learning and computer vision are important components of this ongoing integration, enabling new interaction modalities between user and museum. Nonetheless, the most frequent way of interacting with paintings and statues still remains taking pictures. Yet images alone can only convey the aesthetics of the artwork, lacking is information which is often required to fully understand and appreciate it. Usually this additional knowledge comes both from the artwork itself (and therefore the image depicting it) and from an external source of knowledge, such as an information sheet. While the former can be inferred by computer vision algorithms, the latter needs more structured data to pair visual content with relevant information. Regardless of its source, this information still must be be effectively transmitted to the user. A popular emerging trend in computer vision is Visual Question Answering (VQA), in which users can interact with a neural network by posing questions in natural language and receiving answers about the visual content. We believe that this will be the evolution of smart audio guides for museum visits and simple image browsing on personal smartphones. This will turn the classic audio guide into a smart personal instructor with which the visitor can interact by asking for explanations focused on specific interests. The advantages are twofold: on the one hand the cognitive burden of the visitor will decrease, limiting the flow of information to what the user actually wants to hear; and on the other hand it proposes the most natural way of interacting with a guide, favoring engagement.",0
"As technology advances, cultural heritage and technology are becoming increasingly intertwined. Smart audio guides, virtual and augmented reality, and interactive installations are just a few examples of this integration. To make this possible, machine learning and computer vision play a crucial role in enabling new ways for users to interact with museums. However, taking pictures is still the most common way to interact with artwork, but this doesn't provide enough information to fully understand and appreciate it. External sources, such as information sheets, are often necessary to provide additional knowledge. Visual Question Answering (VQA) is an emerging trend in computer vision that allows users to interact with a neural network by asking questions in natural language and receiving answers about the visual content. We believe that VQA will replace traditional audio guides and personal smartphones in the future, making them more interactive and engaging for visitors. This will reduce the cognitive burden on visitors and make the experience more enjoyable.",1
"We present a novel framework that can combine multi-domain learning (MDL), data imputation (DI) and multi-task learning (MTL) to improve performance for classification and regression tasks in different domains. The core of our method is an adversarial autoencoder that can: (1) learn to produce domain-invariant embeddings to reduce the difference between domains; (2) learn the data distribution for each domain and correctly perform data imputation on missing data. For MDL, we use the Maximum Mean Discrepancy (MMD) measure to align the domain distributions. For DI, we use an adversarial approach where a generator fill in information for missing data and a discriminator tries to distinguish between real and imputed values. Finally, using the universal feature representation in the embeddings, we train a classifier using MTL that given input from any domain, can predict labels for all domains. We demonstrate the superior performance of our approach compared to other state-of-art methods in three distinct settings, DG-DI in image recognition with unstructured data, MTL-DI in grade estimation with structured data and MDMTL-DI in a selection process using mixed data.",0
"Our new methodology integrates multi-domain learning (MDL), data imputation (DI), and multi-task learning (MTL) to enhance classification and regression performance across diverse domains. Our method employs an adversarial autoencoder that can accomplish two key objectives: (1) produce domain-invariant embeddings that minimize cross-domain divergence, and (2) learn domain-specific data distributions to effectively handle missing data imputation. We use Maximum Mean Discrepancy (MMD) to align domain distributions for MDL, and an adversarial approach for DI, in which a generator fills in missing data and a discriminator distinguishes between real and imputed values. By training a classifier using MTL and a universal feature representation in the embeddings, our method can predict labels for all domains. We demonstrate our approach's superiority to other state-of-the-art methods in diverse settings, including DG-DI in image recognition with unstructured data, MTL-DI in grade estimation with structured data, and MDMTL-DI in a selection process using mixed data.",1
"We introduce AutoGluon-Tabular, an open-source AutoML framework that requires only a single line of Python to train highly accurate machine learning models on an unprocessed tabular dataset such as a CSV file. Unlike existing AutoML frameworks that primarily focus on model/hyperparameter selection, AutoGluon-Tabular succeeds by ensembling multiple models and stacking them in multiple layers. Experiments reveal that our multi-layer combination of many models offers better use of allocated training time than seeking out the best. A second contribution is an extensive evaluation of public and commercial AutoML platforms including TPOT, H2O, AutoWEKA, auto-sklearn, AutoGluon, and Google AutoML Tables. Tests on a suite of 50 classification and regression tasks from Kaggle and the OpenML AutoML Benchmark reveal that AutoGluon is faster, more robust, and much more accurate. We find that AutoGluon often even outperforms the best-in-hindsight combination of all of its competitors. In two popular Kaggle competitions, AutoGluon beat 99% of the participating data scientists after merely 4h of training on the raw data.",0
"Our open-source AutoML framework, AutoGluon-Tabular, can train highly accurate machine learning models on an unprocessed tabular dataset, such as a CSV file, with just one line of Python. Unlike other AutoML frameworks that primarily focus on model/hyperparameter selection, AutoGluon-Tabular excels by ensembling multiple models and stacking them in multiple layers. Our experiments show that our multi-layer combination of many models provides better use of allocated training time than trying to find the best model. Additionally, we extensively evaluated public and commercial AutoML platforms, including TPOT, H2O, AutoWEKA, auto-sklearn, AutoGluon, and Google AutoML Tables. Our tests on a suite of 50 classification and regression tasks from Kaggle and the OpenML AutoML Benchmark demonstrate that AutoGluon is faster, more robust, and significantly more accurate. In fact, AutoGluon often outperforms the best combination of all its competitors. In two popular Kaggle competitions, AutoGluon beat 99% of participating data scientists after only 4 hours of training on raw data.",1
"Computing the similarity between two data points plays a vital role in many machine learning algorithms. Metric learning has the aim of learning a good metric automatically from data. Most existing studies on metric learning for tree-structured data have adopted the approach of learning the tree edit distance. However, the edit distance is not amenable for big data analysis because it incurs high computation cost. In this paper, we propose a new metric learning approach for tree-structured data with pq-grams. The pq-gram distance is a distance for ordered labeled trees, and has much lower computation cost than the tree edit distance. In order to perform metric learning based on pq-grams, we propose a new differentiable parameterized distance, weighted pq-gram distance. We also propose a way to learn the proposed distance based on Large Margin Nearest Neighbors (LMNN), which is a well-studied and practical metric learning scheme. We formulate the metric learning problem as an optimization problem and use the gradient descent technique to perform metric learning. We empirically show that the proposed approach not only achieves competitive results with the state-of-the-art edit distance-based methods in various classification problems, but also solves the classification problems much more rapidly than the edit distance-based methods.",0
"The comparison of two data points is crucial in many machine learning algorithms. Metric learning aims to automatically learn a good metric from data. Previous research on metric learning for tree-structured data has relied on learning the tree edit distance, which is unsuitable for analyzing big data due to its high computation cost. This study proposes a new metric learning approach for tree-structured data using pq-grams, which have a much lower computation cost. To perform metric learning based on pq-grams, a new differentiable parameterized distance called the weighted pq-gram distance is introduced. The proposed distance is learned using Large Margin Nearest Neighbors (LMNN), a widely used metric learning scheme. The metric learning problem is formulated as an optimization problem and solved using gradient descent. The proposed approach achieves competitive results with state-of-the-art edit distance-based methods in various classification problems and solves classification problems faster than the edit distance-based methods according to empirical evidence.",1
"In this paper, we study the problem of learning Graph Convolutional Networks (GCNs) for regression. Current architectures of GCNs are limited to the small receptive field of convolution filters and shared transformation matrix for each node. To address these limitations, we propose Semantic Graph Convolutional Networks (SemGCN), a novel neural network architecture that operates on regression tasks with graph-structured data. SemGCN learns to capture semantic information such as local and global node relationships, which is not explicitly represented in the graph. These semantic relationships can be learned through end-to-end training from the ground truth without additional supervision or hand-crafted rules. We further investigate applying SemGCN to 3D 0 pose regression. Our formulation is intuitive and sufficient since both 2D and 3D 0 poses can be represented as a structured graph encoding the relationships between joints in the skeleton of a 0 body. We carry out comprehensive studies to validate our method. The results prove that SemGCN outperforms state of the art while using 90% fewer parameters.",0
"This paper examines the challenge of using Graph Convolutional Networks (GCNs) for regression and notes that current GCN architectures suffer from limited receptive fields of convolution filters and a shared transformation matrix for each node. To overcome these limitations, the authors propose Semantic Graph Convolutional Networks (SemGCN), a new neural network architecture for regression tasks using graph-structured data. SemGCN captures semantic information such as local and global node relationships, which are not explicitly represented in the graph, and can be learned through end-to-end training without additional supervision. The authors also apply SemGCN to 3D 0 pose regression, where the structured graph represents relationships between joints in a 0 body. The study demonstrates that SemGCN outperforms state-of-the-art methods while using 90% fewer parameters.",1
"In view of the huge success of convolution neural networks (CNN) for image classification and object recognition, there have been attempts to generalize the method to general graph-structured data. One major direction is based on spectral graph theory and graph signal processing. In this paper, we study the problem from a completely different perspective, by introducing parallel flow decomposition of graphs. The essential idea is to decompose a graph into families of non-intersecting one dimensional (1D) paths, after which, we may apply a 1D CNN along each family of paths. We demonstrate that the our method, which we call GraphFlow, is able to transfer CNN architectures to general graphs. To show the effectiveness of our approach, we test our method on the classical MNIST dataset, synthetic datasets on network information propagation and a news article classification dataset.",0
"Due to the remarkable achievements of convolution neural networks (CNN) in recognizing objects and classifying images, endeavors have been made to extend this technique to general graph-structured data, using spectral graph theory and graph signal processing as a core approach. However, in this paper, we present an entirely different perspective by introducing the concept of parallel flow decomposition of graphs. The basic idea of this novel approach is to break down a graph into sets of non-intersecting 1D paths, which can then be subjected to a 1D CNN. Our strategy, which we refer to as GraphFlow, has the ability to transmute CNN architectures to general graphs. To demonstrate the efficacy of our method, we conducted tests on various datasets, including the MNIST dataset, synthetic datasets on network information propagation, and a news article classification dataset.",1
"Recent years have witnessed the emerging success of graph neural networks (GNNs) for modeling structured data. However, most GNNs are designed for homogeneous graphs, in which all nodes and edges belong to the same types, making them infeasible to represent heterogeneous structures. In this paper, we present the Heterogeneous Graph Transformer (HGT) architecture for modeling Web-scale heterogeneous graphs. To model heterogeneity, we design node- and edge-type dependent parameters to characterize the heterogeneous attention over each edge, empowering HGT to maintain dedicated representations for different types of nodes and edges. To handle dynamic heterogeneous graphs, we introduce the relative temporal encoding technique into HGT, which is able to capture the dynamic structural dependency with arbitrary durations. To handle Web-scale graph data, we design the heterogeneous mini-batch graph sampling algorithm---HGSampling---for efficient and scalable training. Extensive experiments on the Open Academic Graph of 179 million nodes and 2 billion edges show that the proposed HGT model consistently outperforms all the state-of-the-art GNN baselines by 9%--21% on various downstream tasks.",0
"Graph neural networks (GNNs) have gained traction in recent years as effective tools for modeling structured data. However, most GNNs are not suitable for modeling heterogeneous graphs, which feature nodes and edges of different types. To address this limitation, we introduce the Heterogeneous Graph Transformer (HGT) architecture, which can model Web-scale heterogeneous graphs. HGT employs node- and edge-type dependent parameters to enable heterogeneous attention over each edge, allowing for dedicated representations of different types of nodes and edges. HGT also incorporates the relative temporal encoding technique to capture dynamic structural dependencies of arbitrary durations, and the heterogeneous mini-batch graph sampling algorithm (HGSampling) for efficient and scalable training of Web-scale graph data. Our experiments on the Open Academic Graph of 179 million nodes and 2 billion edges demonstrate that HGT outperforms state-of-the-art GNN baselines by 9%--21% on various downstream tasks.",1
"Neural networks for structured data like graphs have been studied extensively in recent years. To date, the bulk of research activity has focused mainly on static graphs. However, most real-world networks are dynamic since their topology tends to change over time. Predicting the evolution of dynamic graphs is a task of high significance in the area of graph mining. Despite its practical importance, the task has not been explored in depth so far, mainly due to its challenging nature. In this paper, we propose a model that predicts the evolution of dynamic graphs. Specifically, we use a graph neural network along with a recurrent architecture to capture the temporal evolution patterns of dynamic graphs. Then, we employ a generative model which predicts the topology of the graph at the next time step and constructs a graph instance that corresponds to that topology. We evaluate the proposed model on several artificial datasets following common network evolving dynamics, as well as on real-world datasets. Results demonstrate the effectiveness of the proposed model.",0
"Over the past few years, there has been extensive research on neural networks for structured data such as graphs. However, most of the research has focused on static graphs, despite the fact that real-world networks tend to be dynamic and change over time. Predicting the evolution of dynamic graphs is an important task in graph mining, but it has been challenging and therefore not explored in depth. This paper presents a proposed model that predicts the evolution of dynamic graphs using a graph neural network and a recurrent architecture to capture temporal evolution patterns. A generative model is also employed to predict the topology of the graph at the next time step and construct a corresponding graph instance. The proposed model is evaluated on both artificial and real-world datasets, and the results demonstrate its effectiveness.",1
"Learning generative models for graph-structured data is challenging because graphs are discrete, combinatorial, and the underlying data distribution is invariant to the ordering of nodes. However, most of the existing generative models for graphs are not invariant to the chosen ordering, which might lead to an undesirable bias in the learned distribution. To address this difficulty, we propose a permutation invariant approach to modeling graphs, using the recent framework of score-based generative modeling. In particular, we design a permutation equivariant, multi-channel graph neural network to model the gradient of the data distribution at the input graph (a.k.a., the score function). This permutation equivariant model of gradients implicitly defines a permutation invariant distribution for graphs. We train this graph neural network with score matching and sample from it with annealed Langevin dynamics. In our experiments, we first demonstrate the capacity of this new architecture in learning discrete graph algorithms. For graph generation, we find that our learning approach achieves better or comparable results to existing models on benchmark datasets.",0
"Generating models for graph-structured data is a difficult task due to the discrete and combinatorial nature of graphs, as well as the invariance of the underlying data distribution to node ordering. However, most existing generative models for graphs are not invariant to the chosen ordering, leading to an undesired bias in the learned distribution. To overcome this challenge, we propose a permutation invariant approach using score-based generative modeling. We develop a permutation equivariant, multi-channel graph neural network to model the gradient of the data distribution, which implicitly defines a permutation invariant distribution for graphs. We train the network with score matching and sample from it using annealed Langevin dynamics. Our experiments demonstrate that this architecture can successfully learn discrete graph algorithms and achieve comparable or superior results for graph generation on standard datasets.",1
"Detecting communities on graphs has received significant interest in recent literature. Current state-of-the-art community embedding approach called \textit{ComE} tackles this problem by coupling graph embedding with community detection. Considering the success of hyperbolic representations of graph-structured data in last years, an ongoing challenge is to set up a hyperbolic approach for the community detection problem. The present paper meets this challenge by introducing a Riemannian equivalent of \textit{ComE}. Our proposed approach combines hyperbolic embeddings with Riemannian K-means or Riemannian mixture models to perform community detection. We illustrate the usefulness of this framework through several experiments on real-world social networks and comparisons with \textit{ComE} and recent hyperbolic-based classification approaches.",0
"Recently, there has been considerable interest in identifying communities in graphs. To address this problem, a state-of-the-art approach called \textit{ComE} couples graph embedding with community detection. However, there is a continuing challenge to develop a hyperbolic approach for this problem, given the success of hyperbolic representations of graph-structured data in recent years. This paper presents a Riemannian equivalent of \textit{ComE} that addresses this challenge. Our proposed method combines hyperbolic embeddings with Riemannian K-means or Riemannian mixture models to detect communities. We demonstrate the effectiveness of this framework through experiments on real-world social networks and comparisons with \textit{ComE} and recent hyperbolic-based classification approaches.",1
"Graph-structured data arise ubiquitously in many application domains. A fundamental problem is to quantify their similarities. Graph kernels are often used for this purpose, which decompose graphs into substructures and compare these substructures. However, most of the existing graph kernels do not have the property of scale-adaptivity, i.e., they cannot compare graphs at multiple levels of granularities. Many real-world graphs such as molecules exhibit structure at varying levels of granularities. To tackle this problem, we propose a new graph kernel called Tree++ in this paper. At the heart of Tree++ is a graph kernel called the path-pattern graph kernel. The path-pattern graph kernel first builds a truncated BFS tree rooted at each vertex and then uses paths from the root to every vertex in the truncated BFS tree as features to represent graphs. The path-pattern graph kernel can only capture graph similarity at fine granularities. In order to capture graph similarity at coarse granularities, we incorporate a new concept called super path into it. The super path contains truncated BFS trees rooted at the vertices in a path. Our evaluation on a variety of real-world graphs demonstrates that Tree++ achieves the best classification accuracy compared with previous graph kernels.",0
"Graph-structured data are commonly found in many fields and require a method to measure their similarities. Graph kernels are frequently utilized for this task by breaking down graphs into substructures and comparing them. However, most current graph kernels lack the ability to adjust to different scales, meaning they cannot compare graphs at multiple levels of detail. This is problematic for real-world graphs, like molecules, that have varying levels of structure. To tackle this issue, we introduce a new graph kernel, Tree++, in this paper. Our approach builds on the path-pattern graph kernel, which constructs a truncated breadth-first search tree for each vertex and utilizes paths from the root to each vertex as features to represent graphs. However, the path-pattern graph kernel only captures graph similarity at a fine level of detail. To address this limitation, we introduce the concept of super paths, which contains truncated BFS trees rooted at the vertices in a path, allowing us to capture graph similarity at a coarse level of detail. Our evaluation of Tree++ on a range of real-world graphs reveals that it outperforms previous graph kernels in terms of classification accuracy.",1
"Message-passing neural networks (MPNNs) have been successfully applied to representation learning on graphs in a variety of real-world applications. However, two fundamental weaknesses of MPNNs' aggregators limit their ability to represent graph-structured data: losing the structural information of nodes in neighborhoods and lacking the ability to capture long-range dependencies in disassortative graphs. Few studies have noticed the weaknesses from different perspectives. From the observations on classical neural network and network geometry, we propose a novel geometric aggregation scheme for graph neural networks to overcome the two weaknesses. The behind basic idea is the aggregation on a graph can benefit from a continuous space underlying the graph. The proposed aggregation scheme is permutation-invariant and consists of three modules, node embedding, structural neighborhood, and bi-level aggregation. We also present an implementation of the scheme in graph convolutional networks, termed Geom-GCN (Geometric Graph Convolutional Networks), to perform transductive learning on graphs. Experimental results show the proposed Geom-GCN achieved state-of-the-art performance on a wide range of open datasets of graphs. Code is available at https://github.com/graphdml-uiuc-jlu/geom-gcn.",0
"MPNNs have been utilized with success for representation learning on graphs in diverse real-world applications. Nevertheless, the aggregators of MPNNs possess two essential weaknesses that hinder their ability to represent graph-structured data. These weaknesses are the loss of structural information of nodes in neighborhoods and the inability to capture long-range dependencies in disassortative graphs. Despite this, few studies have considered these weaknesses from different perspectives. Based on observations on classical neural networks and network geometry, we propose a novel geometric aggregation scheme for graph neural networks to overcome these two weaknesses. The fundamental idea behind this scheme is that the aggregation on a graph can benefit from a continuous space that underlies the graph. The proposed aggregation scheme is permutation-invariant and comprises three modules: node embedding, structural neighborhood, and bi-level aggregation. We also introduce an implementation of this scheme in graph convolutional networks, known as Geom-GCN, to perform transductive learning on graphs. Our experimental results demonstrate that Geom-GCN has achieved state-of-the-art performance on a broad range of open datasets of graphs. The code for Geom-GCN is available at https://github.com/graphdml-uiuc-jlu/geom-gcn.",1
"We address the problem of merging graph and feature-space information while learning a metric from structured data. Existing algorithms tackle the problem in an asymmetric way, by either extracting vectorized summaries of the graph structure or adding hard constraints to feature-space algorithms. Following a different path, we define a metric regression scheme where we train metric-constrained linear combinations of dissimilarity matrices. The idea is that the input matrices can be pre-computed dissimilarity measures obtained from any kind of available data (e.g. node attributes or edge structure). As the model inputs are distance measures, we do not need to assume the existence of any underlying feature space. Main challenge is that metric constraints (especially positive-definiteness and sub-additivity), are not automatically respected if, for example, the coefficients of the linear combination are allowed to be negative. Both positive and sub-additive constraints are linear inequalities, but the computational complexity of imposing them scales as O(D3), where D is the size of the input matrices (i.e. the size of the data set). This becomes quickly prohibitive, even when D is relatively small. We propose a new graph-based technique for optimizing under such constraints and show that, in some cases, our approach may reduce the original computational complexity of the optimization process by one order of magnitude. Contrarily to existing methods, our scheme applies to any (possibly non-convex) metric-constrained objective function.",0
"Our focus is on solving the issue of integrating graph and feature-space data when learning a metric from structured data. Current methods approach the problem asymmetrically, by either extracting vectorized summaries of the graph structure or imposing hard constraints on feature-space algorithms. In contrast, we introduce a metric regression scheme that utilizes metric-constrained linear combinations of dissimilarity matrices. This approach allows pre-computed dissimilarity measures obtained from various data sources to be used as input matrices without the need for an underlying feature space assumption. However, imposing metric constraints such as positive-definiteness and sub-additivity can be computationally challenging, especially for large input matrices. We present a new graph-based technique that optimizes under such constraints and can significantly reduce computational complexity. Our approach is applicable to any metric-constrained objective function, including non-convex ones.",1
"While learning models are typically studied for inputs in the form of a fixed dimensional feature vector, real world data is rarely found in this form. In order to meet the basic requirement of traditional learning models, structural data generally have to be converted into fix-length vectors in a handcrafted manner, which is tedious and may even incur information loss. A common form of structured data is what we term ""semantic tree-structures"", corresponding to data where rich semantic information is encoded in a compositional manner, such as those expressed in JavaScript Object Notation (JSON) and eXtensible Markup Language (XML). For tree-structured data, several learning models have been studied to allow for working directly on raw tree-structure data, However such learning models are limited to either a specific tree-topology or a specific tree-structured data format, e.g., synthetic parse trees. In this paper, we propose a novel framework for end-to-end learning on generic semantic tree-structured data of arbitrary topology and heterogeneous data types, such as data expressed in JSON, XML and so on. Motivated by the works in recursive and recurrent neural networks, we develop exemplar neural implementations of our framework for the JSON format. We evaluate our approach on several UCI benchmark datasets, including ablation and data-efficiency studies, and on a toy reinforcement learning task. Experimental results suggest that our framework yields comparable performance to use of standard models with dedicated feature-vectors in general, and even exceeds baseline performance in cases where compositional nature of the data is particularly important.   The source code for a JSON-based implementation of our framework along with experiments can be downloaded at https://github.com/EndingCredits/json2vec.",0
"Typically, learning models are studied using fixed-dimensional feature vectors as input. However, real-world data is not often found in this format. As a result, structural data must be manually converted into fixed-length vectors to meet the requirements of traditional learning models, which can be a tedious process and may lead to information loss. Semantic tree-structures are a common form of structured data that encode rich semantic information in a compositional manner, such as those expressed in JSON and XML. While several learning models have been developed for tree-structured data, they are often limited to specific tree-topologies or formats. In this paper, we propose a novel framework for end-to-end learning on generic semantic tree-structured data of arbitrary topology and heterogeneous data types. We develop neural implementations of our framework for the JSON format, inspired by recursive and recurrent neural networks. Our approach is evaluated on several benchmark datasets and a toy reinforcement learning task, where it achieves comparable or even better performance than standard models with feature-vectors. The code for our JSON-based implementation and experiments can be downloaded at https://github.com/EndingCredits/json2vec.",1
"Paper documents are widely used as an irreplaceable channel of information in many fields, especially in financial industry, fostering a great amount of demand for systems which can convert document images into structured data representations. In this paper, we present a machine learning framework for data ingestion in document images, which processes the images uploaded by users and return fine-grained data in JSON format. Details of model architectures, design strategies, distinctions with existing solutions and lessons learned during development are elaborated. We conduct abundant experiments on both synthetic and real-world data in State Street. The experimental results indicate the effectiveness and efficiency of our methods.",0
"In various industries, particularly in finance, paper documents remain an indispensable source of information. As a result, there is a significant need for systems that can transform document images into structured data representations. This paper introduces a machine learning framework for processing document images submitted by users and returning fine-grained data in JSON format. The model architectures, design strategies, differences from current solutions, and development insights are discussed. The effectiveness and efficiency of our approach are demonstrated through numerous experiments on State Street's synthetic and real-world data.",1
"Machine learning models that can exploit the inherent structure in data have gained prominence. In particular, there is a surge in deep learning solutions for graph-structured data, due to its wide-spread applicability in several fields. Graph attention networks (GAT), a recent addition to the broad class of feature learning models in graphs, utilizes the attention mechanism to efficiently learn continuous vector representations for semi-supervised learning problems. In this paper, we perform a detailed analysis of GAT models, and present interesting insights into their behavior. In particular, we show that the models are vulnerable to heterogeneous rogue nodes and hence propose novel regularization strategies to improve the robustness of GAT models. Using benchmark datasets, we demonstrate performance improvements on semi-supervised learning, using the proposed robust variant of GAT.",0
"The utilization of machine learning models that can capitalize on the inherent structure in data has become increasingly prevalent. Specifically, there has been a surge in the employment of deep learning solutions for graph-structured data due to its broad applicability in various fields. Graph attention networks (GAT), which are a recent addition to the extensive range of feature learning models in graphs, employ the attention mechanism to acquire efficient continuous vector representations for semi-supervised learning problems. This article conducts a thorough analysis of GAT models and provides intriguing insights into their behavior. Notably, it is revealed that the models are susceptible to heterogeneous rogue nodes, leading to the proposal of innovative regularization techniques to enhance the robustness of GAT models. By using benchmark datasets, the study demonstrates that the proposed robust version of GAT can significantly enhance performance in semi-supervised learning.",1
"Despite the phenomenal success of deep neural networks in a broad range of learning tasks, there is a lack of theory to understand the way they work. In particular, Convolutional Neural Networks (CNNs) are known to perform much better than Fully-Connected Networks (FCNs) on spatially structured data: the architectural structure of CNNs benefits from prior knowledge on the features of the data, for instance their translation invariance. The aim of this work is to understand this fact through the lens of dynamics in the loss landscape.   We introduce a method that maps a CNN to its equivalent FCN (denoted as eFCN). Such an embedding enables the comparison of CNN and FCN training dynamics directly in the FCN space. We use this method to test a new training protocol, which consists in training a CNN, embedding it to FCN space at a certain ``relax time'', then resuming the training in FCN space. We observe that for all relax times, the deviation from the CNN subspace is small, and the final performance reached by the eFCN is higher than that reachable by a standard FCN of same architecture. More surprisingly, for some intermediate relax times, the eFCN outperforms the CNN it stemmed, by combining the prior information of the CNN and the expressivity of the FCN in a complementary way. The practical interest of our protocol is limited by the very large size of the highly sparse eFCN. However, it offers interesting insights into the persistence of architectural bias under stochastic gradient dynamics. It shows the existence of some rare basins in the FCN loss landscape associated with very good generalization. These can only be accessed thanks to the CNN prior, which helps navigate the landscape during the early stages of optimization.",0
"Although deep neural networks have achieved remarkable success across a wide range of learning tasks, there is a dearth of theory to explain their functioning. Specifically, Convolutional Neural Networks (CNNs) are known to outperform Fully-Connected Networks (FCNs) when dealing with spatially structured data due to the CNN's architectural structure, which benefits from prior knowledge about the data features, such as translation invariance. This study aims to comprehend this phenomenon by investigating dynamics in the loss landscape. To achieve this, the researchers developed a method that maps a CNN to its equivalent FCN (eFCN), allowing for a direct comparison of CNN and FCN training dynamics in the FCN space. They employed this approach to test a new training protocol which involved training a CNN, embedding it in the FCN space at a specific ""relax time,"" and then resuming training in the FCN space. The results showed that the eFCN's final performance was higher than that of a standard FCN of similar architecture, and for some intermediate relax times, the eFCN outperformed the CNN by combining the CNN's prior knowledge with the FCN's expressivity in a complementary manner. Although the large size of the highly sparse eFCN limits its practical utility, the study provides valuable insights into the persistence of architectural bias under stochastic gradient dynamics, revealing the existence of rare basins in the FCN loss landscape associated with excellent generalization that can only be accessed with the help of the CNN's prior knowledge.",1
"The richness in the content of various information networks such as social networks and communication networks provides the unprecedented potential for learning high-quality expressive representations without external supervision. This paper investigates how to preserve and extract the abundant information from graph-structured data into embedding space in an unsupervised manner. To this end, we propose a novel concept, Graphical Mutual Information (GMI), to measure the correlation between input graphs and high-level hidden representations. GMI generalizes the idea of conventional mutual information computations from vector space to the graph domain where measuring mutual information from two aspects of node features and topological structure is indispensable. GMI exhibits several benefits: First, it is invariant to the isomorphic transformation of input graphs---an inevitable constraint in many existing graph representation learning algorithms; Besides, it can be efficiently estimated and maximized by current mutual information estimation methods such as MINE; Finally, our theoretical analysis confirms its correctness and rationality. With the aid of GMI, we develop an unsupervised learning model trained by maximizing GMI between the input and output of a graph neural encoder. Considerable experiments on transductive as well as inductive node classification and link prediction demonstrate that our method outperforms state-of-the-art unsupervised counterparts, and even sometimes exceeds the performance of supervised ones.",0
"This paper explores the potential for learning high-quality expressive representations without external supervision using the wealth of content available in various information networks such as social and communication networks. The goal is to extract and preserve abundant information from graph-structured data into embedding space in an unsupervised manner. To achieve this, the authors propose a novel concept called Graphical Mutual Information (GMI) that measures the correlation between input graphs and high-level hidden representations. GMI generalizes the idea of conventional mutual information computations from vector space to the graph domain, where measuring mutual information from two aspects of node features and topological structure is essential. GMI offers several benefits, including invariance to the isomorphic transformation of input graphs, efficient estimation and maximization using current mutual information estimation methods such as MINE, and theoretical analysis confirming its correctness and rationality. Using GMI, the authors develop an unsupervised learning model that maximizes GMI between the input and output of a graph neural encoder. The authors conduct extensive experiments on transductive and inductive node classification and link prediction, demonstrating that their method outperforms state-of-the-art unsupervised counterparts and sometimes even surpasses supervised ones.",1
"Hierarchical Sparse Coding (HSC) is a powerful model to efficiently represent multi-dimensional, structured data such as images. The simplest solution to solve this computationally hard problem is to decompose it into independent layer-wise subproblems. However, neuroscientific evidence would suggest inter-connecting these subproblems as in the Predictive Coding (PC) theory, which adds top-down connections between consecutive layers. In this study, a new model called 2-Layers Sparse Predictive Coding (2L-SPC) is introduced to assess the impact of this inter-layer feedback connection. In particular, the 2L-SPC is compared with a Hierarchical Lasso (Hi-La) network made out of a sequence of independent Lasso layers. The 2L-SPC and the 2-layers Hi-La networks are trained on 4 different databases and with different sparsity parameters on each layer. First, we show that the overall prediction error generated by 2L-SPC is lower thanks to the feedback mechanism as it transfers prediction error between layers. Second, we demonstrate that the inference stage of the 2L-SPC is faster to converge than for the Hi-La model. Third, we show that the 2L-SPC also accelerates the learning process. Finally, the qualitative analysis of both models dictionaries, supported by their activation probability, show that the 2L-SPC features are more generic and informative.",0
"The Hierarchical Sparse Coding (HSC) model is adept at efficiently representing structured data like images. To solve the computationally challenging nature of this problem, the model can be divided into independent subproblems for each layer. However, the Predictive Coding (PC) theory suggests inter-connecting these subproblems using top-down connections between consecutive layers. To explore the impact of this feedback connection, a new model called 2-Layers Sparse Predictive Coding (2L-SPC) was introduced and compared with a Hierarchical Lasso (Hi-La) network consisting of independent Lasso layers. The 2L-SPC and the 2-layers Hi-La networks were trained on different databases with varying sparsity parameters on each layer. The study found that the feedback mechanism of 2L-SPC resulted in lower overall prediction error by transferring prediction error between layers. Additionally, the inference stage of 2L-SPC was faster to converge than Hi-La, and 2L-SPC accelerated the learning process. Qualitative analysis of the models' dictionaries, supported by their activation probability, revealed that 2L-SPC features were more generic and informative.",1
