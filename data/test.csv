"Graph neural networks (GNNs) have been widely used to learn vector representation of graph-structured data and achieved better task performance than conventional methods. The foundation of GNNs is the message passing procedure, which propagates the information in a node to its neighbors. Since this procedure proceeds one step per layer, the range of the information propagation among nodes is small in the lower layers, and it expands toward the higher layers. Therefore, a GNN model has to be deep enough to capture global structural information in a graph. On the other hand, it is known that deep GNN models suffer from performance degradation because they lose nodes' local information, which would be essential for good model performance, through many message passing steps. In this study, we propose multi-level attention pooling (MLAP) for graph-level classification tasks, which can adapt to both local and global structural information in a graph. It has an attention pooling layer for each message passing step and computes the final graph representation by unifying the layer-wise graph representations. The MLAP architecture allows models to utilize the structural information of graphs with multiple levels of localities because it preserves layer-wise information before losing them due to oversmoothing. Results of our experiments show that the MLAP architecture improves the graph classification performance compared to the baseline architectures. In addition, analyses on the layer-wise graph representations suggest that aggregating information from multiple levels of localities indeed has the potential to improve the discriminability of learned graph representations.",0
"The use of Graph neural networks (GNNs) has become widespread in the learning of vector representations of graph-structured data, and they have outperformed traditional methods in task performance. The fundamental aspect of GNNs is their message passing procedure, which transfers data among nodes. However, since this procedure progresses one step per layer, the range of the information propagation is limited in the lower layers, expanding gradually towards higher layers. As a result, a GNN model needs to be deep enough to capture the global structural information in a graph. Nevertheless, deep GNN models suffer from performance degradation since they lose the nodes' local information, which is crucial for good model performance, through many message passing steps. In this study, we propose multi-level attention pooling (MLAP) for graph-level classification tasks, which can adjust to both local and global structural information in a graph. It contains an attention pooling layer for each message passing step and computes the final graph representation by merging the layer-wise graph representations. The MLAP architecture enables models to use the structural information of graphs with multiple levels of localities, preserving layer-wise information before losing it due to oversmoothing. Our experiments' results show that the MLAP architecture enhances graph classification performance compared to the baseline architectures. Furthermore, our analyses on the layer-wise graph representations suggest that the aggregation of information from various levels of localities has the potential to improve the discriminability of learned graph representations.",1
"Deep networks and decision forests (such as random forests and gradient boosted trees) are the leading machine learning methods for structured and tabular data, respectively. Many papers have empirically compared large numbers of classifiers on one or two different domains (e.g., on 100 different tabular data settings). However, a careful conceptual and empirical comparison of these two strategies using the most contemporary best practices has yet to be performed. Conceptually, we illustrate that both can be profitably viewed as ""partition and vote"" schemes. Specifically, the representation space that they both learn is a partitioning of feature space into a union of convex polytopes. For inference, each decides on the basis of votes from the activated nodes. This formulation allows for a unified basic understanding of the relationship between these methods. Empirically, we compare these two strategies on hundreds of tabular data settings, as well as several vision and auditory settings. Our focus is on datasets with at most 10,000 samples, which represent a large fraction of scientific and biomedical datasets. In general, we found forests to excel at tabular and structured data (vision and audition) with small sample sizes, whereas deep nets performed better on structured data with larger sample sizes. This suggests that further gains in both scenarios may be realized via further combining aspects of forests and networks. We will continue revising this technical report in the coming months with updated results.",0
"The primary machine learning techniques for structured and tabular data are deep networks and decision forests, respectively, including random forests and gradient boosted trees. Many research studies have examined a large number of classifiers on one or two different domains, such as 100 different tabular data settings. However, a thorough comparison of these two methods using the most modern best practices has yet to be conducted. Both deep networks and decision forests can be profitably viewed as ""partition and vote"" schemes, where the representation space they learn is a partitioning of the feature space into a union of convex polytopes. Empirically, we analyze these two methods on hundreds of tabular data settings and several vision and auditory settings, focusing on datasets with at most 10,000 samples, which represent a significant portion of scientific and biomedical datasets. In general, we found that decision forests excel in tabular and structured data (vision and audition) with small sample sizes, while deep networks perform better on structured data with larger sample sizes. As a result, further advances in both scenarios may be possible by combining aspects of forests and networks. We will continue to refine this technical report in the coming months with updated findings.",1
"Graph convolutional networks (GCNs) are powerful tools for graph-structured data. However, they have been recently shown to be vulnerable to topological attacks. To enhance adversarial robustness, we go beyond spectral graph theory to robust graph theory. By challenging the classical graph Laplacian, we propose a new convolution operator that is provably robust in the spectral domain and is incorporated in the GCN architecture to improve expressivity and interpretability. By extending the original graph to a sequence of graphs, we also propose a robust training paradigm that encourages transferability across graphs that span a range of spatial and spectral characteristics. The proposed approaches are demonstrated in extensive experiments to simultaneously improve performance in both benign and adversarial situations.",0
"GCNs are effective tools for data that is structured in a graph format, yet they have recently been discovered to be susceptible to topological attacks. To enhance their resilience to such attacks, we have moved beyond spectral graph theory and towards robust graph theory. We have developed a new convolution operator that is resilient in the spectral domain and is incorporated into the GCN architecture to increase its expressivity and interpretability. Additionally, we have extended the original graph to a series of graphs, and our robust training method promotes transferability across these graphs, which vary in spatial and spectral characteristics. Our proposed methods have been extensively tested and have been shown to improve performance in benign and adversarial situations alike.",1
"With the increasing popularity of Graph Neural Networks (GNNs) in several sensitive applications like healthcare and medicine, concerns have been raised over the privacy aspects of trained GNNs. More notably, GNNs are vulnerable to privacy attacks, such as membership inference attacks, even if only blackbox access to the trained model is granted. To build defenses, differential privacy has emerged as a mechanism to disguise the sensitive data in training datasets. Following the strategy of Private Aggregation of Teacher Ensembles (PATE), recent methods leverage a large ensemble of teacher models. These teachers are trained on disjoint subsets of private data and are employed to transfer knowledge to a student model, which is then released with privacy guarantees. However, splitting graph data into many disjoint training sets may destroy the structural information and adversely affect accuracy. We propose a new graph-specific scheme of releasing a student GNN, which avoids splitting private training data altogether. The student GNN is trained using public data, partly labeled privately using the teacher GNN models trained exclusively for each query node. We theoretically analyze our approach in the R\`{e}nyi differential privacy framework and provide privacy guarantees. Besides, we show the solid experimental performance of our method compared to several baselines, including the PATE baseline adapted for graph-structured data. Our anonymized code is available.",0
"Graph Neural Networks (GNNs) are increasingly popular in sensitive applications such as healthcare and medicine. However, concerns have been raised about the privacy of trained GNNs, as they are vulnerable to privacy attacks, even with only blackbox access to the model. To address this, differential privacy has emerged as a way to disguise sensitive data in training datasets. Recent methods, such as Private Aggregation of Teacher Ensembles (PATE), use a large ensemble of teacher models trained on disjoint subsets of private data to transfer knowledge to a student model with privacy guarantees. However, splitting graph data into disjoint training sets can destroy structural information and harm accuracy. We propose a new graph-specific scheme that avoids splitting private training data altogether. Instead, we train a student GNN using public data partially labeled privately by teacher GNN models trained exclusively for each query node. We provide privacy guarantees and show strong experimental performance compared to several baselines, including the PATE baseline adapted for graph-structured data. Our anonymized code is available.",1
"Machine learning solutions for pattern classification problems are nowadays widely deployed in society and industry. However, the lack of transparency and accountability of most accurate models often hinders their safe use. Thus, there is a clear need for developing explainable artificial intelligence mechanisms. There exist model-agnostic methods that summarize feature contributions, but their interpretability is limited to predictions made by black-box models. An open challenge is to develop models that have intrinsic interpretability and produce their own explanations, even for classes of models that are traditionally considered black boxes like (recurrent) neural networks. In this paper, we propose a Long-Term Cognitive Network for interpretable pattern classification of structured data. Our method brings its own mechanism for providing explanations by quantifying the relevance of each feature in the decision process. For supporting the interpretability without affecting the performance, the model incorporates more flexibility through a quasi-nonlinear reasoning rule that allows controlling nonlinearity. Besides, we propose a recurrence-aware decision model that evades the issues posed by unique fixed points while introducing a deterministic learning method to compute the tunable parameters. The simulations show that our interpretable model obtains competitive results when compared to the state-of-the-art white and black-box models.",0
"Currently, machine learning solutions are widely used in society and industry for pattern classification problems. However, there is a lack of transparency and accountability in many accurate models, making their use risky. Therefore, there is a need to develop artificial intelligence mechanisms that are explainable. Although model-agnostic methods exist to summarize feature contributions, their interpretability is limited to predictions made by black-box models. An open challenge is to develop models that have intrinsic interpretability and produce their own explanations, even for traditionally considered black-box models such as (recurrent) neural networks. This paper proposes a Long-Term Cognitive Network for interpretable pattern classification of structured data that provides its own explanation mechanism by quantifying the relevance of each feature in the decision process. To support interpretability without affecting performance, the model incorporates more flexibility through a quasi-nonlinear reasoning rule that allows controlling nonlinearity. Additionally, a recurrence-aware decision model is proposed that avoids the issues posed by unique fixed points while introducing a deterministic learning method to compute the tunable parameters. The simulations demonstrate that this interpretable model achieves competitive results compared to state-of-the-art white and black-box models.",1
"Graph neural networks (GNNs) are powerful models for many graph-structured tasks. Existing models often assume that a complete structure of a graph is available during training, however, in practice, graph-structured data is usually formed in a streaming fashion, so that learning a graph continuously is often necessary. In this paper, we aim to bridge GNN to lifelong learning by converting a graph problem to a regular learning problem, so that GNN is able to inherit the lifelong learning techniques developed for convolutional neural networks (CNNs). To this end, we propose a new graph topology based on feature cross-correlation, called the feature graph. It takes features as new nodes and turns nodes into independent graphs. This successfully converts the original problem of node classification to graph classification, in which the increasing nodes are turned into independent training samples. In the experiments, we demonstrate the efficiency and effectiveness of feature graph networks (FGN) by continuously learning a sequence of classical graph datasets. We also show that FGN achieves superior performance in human action recognition with distributed streaming signals for wearable devices.",0
"Many tasks involving graph structures can be accomplished effectively with Graph Neural Networks (GNNs). However, most existing models assume that the complete structure of a graph is available during training, which is often not the case with streaming graph-structured data. Therefore, we propose to connect GNNs to lifelong learning by transforming a graph problem into a regular learning problem. This enables GNNs to take advantage of techniques developed for Convolutional Neural Networks (CNNs) in lifelong learning. We introduce a new graph topology called the feature graph, which leverages feature cross-correlation to create independent graphs from nodes. This transforms the problem of node classification into graph classification, where increasing nodes are treated as independent training samples. Our experiments demonstrate the effectiveness and efficiency of Feature Graph Networks (FGNs) in continuously learning classical graph datasets. Additionally, we demonstrate that FGNs outperform existing methods in human action recognition using distributed streaming signals from wearable devices.",1
"Deep learning models, such as convolutional neural networks, have long been applied to image and multi-media tasks, particularly those with structured data. More recently, there has been more attention to unstructured data that can be represented via graphs. These types of data are often found in health and medicine, social networks, and research data repositories. Graph convolutional neural networks have recently gained attention in the field of deep learning that takes advantage of graph-based data representation with automatic feature extraction via convolutions. Given the popularity of these methods in a wide range of applications, robust uncertainty quantification is vital. This remains a challenge for large models and unstructured datasets. Bayesian inference provides a principled approach to uncertainty quantification of model parameters for deep learning models. Although Bayesian inference has been used extensively elsewhere, its application to deep learning remains limited due to the computational requirements of the Markov Chain Monte Carlo (MCMC) methods. Recent advances in parallel computing and advanced proposal schemes in MCMC sampling methods has opened the path for Bayesian deep learning. In this paper, we present Bayesian graph convolutional neural networks that employ tempered MCMC sampling with Langevin-gradient proposal distribution implemented via parallel computing. Our results show that the proposed method can provide accuracy similar to advanced optimisers while providing uncertainty quantification for key benchmark problems.",0
"Convolutional neural networks have been widely used in image and multimedia tasks, particularly those with structured data. Recently, there has been a growing interest in graph-based representations of unstructured data, which are commonly found in health, social networks, and research data repositories. Graph convolutional neural networks have emerged as a promising technique for deep learning that automatically extracts features from graph-based data. However, robust uncertainty quantification remains a challenge for large models and unstructured datasets. Bayesian inference offers a principled approach to uncertainty quantification, but its practical application to deep learning has been limited due to the computational demands of Markov Chain Monte Carlo methods. Recent advances in parallel computing and MCMC sampling methods have enabled Bayesian deep learning. In this paper, we propose a Bayesian graph convolutional neural network that uses tempered MCMC sampling with Langevin-gradient proposal distribution implemented via parallel computing. Our results demonstrate that this method provides comparable accuracy to advanced optimizers while also providing uncertainty quantification for key benchmark problems.",1
"A Graph Convolutional Network (GCN) stacks several layers and in each layer performs a PROPagation operation (PROP) and a TRANsformation operation (TRAN) for learning node representations over graph-structured data. Though powerful, GCNs tend to suffer performance drop when the model gets deep. Previous works focus on PROPs to study and mitigate this issue, but the role of TRANs is barely investigated. In this work, we study performance degradation of GCNs by experimentally examining how stacking only TRANs or PROPs works. We find that TRANs contribute significantly, or even more than PROPs, to declining performance, and moreover that they tend to amplify node-wise feature variance in GCNs, causing variance inflammation that we identify as a key factor for causing performance drop. Motivated by such observations, we propose a variance-controlling technique termed Node Normalization (NodeNorm), which scales each node's features using its own standard deviation. Experimental results validate the effectiveness of NodeNorm on addressing performance degradation of GCNs. Specifically, it enables deep GCNs to outperform shallow ones in cases where deep models are needed, and to achieve comparable results with shallow ones on 6 benchmark datasets. NodeNorm is a generic plug-in and can well generalize to other GNN architectures. Code is publicly available at https://github.com/miafei/NodeNorm.",0
"The Graph Convolutional Network (GCN) is a multi-layered approach that uses PROPagation (PROP) and TRANsformation (TRAN) operations to learn node representations on graph-structured data. Despite its effectiveness, GCNs experience a decline in performance as the model becomes deeper. Researchers have mainly focused on PROPs to address this issue, neglecting the role of TRANs. This study examines the impact of stacking only TRANs or PROPs on GCN performance and finds that TRANs contribute significantly to performance decline and increase node-wise feature variance, which is a key factor in performance drop. To mitigate this issue, the study proposes a variance-controlling technique called Node Normalization (NodeNorm), which scales each node's features using its own standard deviation. Experimental results demonstrate that NodeNorm enables deep GCNs to outperform shallow ones and achieve comparable results with shallow models on six benchmark datasets. NodeNorm is a general plug-in that can be applied to other GNN architectures, and the code is publicly available at https://github.com/miafei/NodeNorm.",1
"Transformer neural networks have achieved state-of-the-art results for unstructured data such as text and images but their adoption for graph-structured data has been limited. This is partly due to the difficulty of incorporating complex structural information in the basic transformer framework. We propose a simple yet powerful extension to the transformer - residual edge channels. The resultant framework, which we call Edge-augmented Graph Transformer (EGT), can directly accept, process and output structural information as well as node information. It allows us to use global self-attention, the key element of transformers, directly for graphs and comes with the benefit of long-range interaction among nodes. Moreover, the edge channels allow the structural information to evolve from layer to layer, and prediction tasks on edges/links can be performed directly from the output embeddings of these channels. In addition, we introduce a generalized positional encoding scheme for graphs based on Singular Value Decomposition which can improve the performance of EGT. Our framework, which relies on global node feature aggregation, achieves better performance compared to Convolutional/Message-Passing Graph Neural Networks, which rely on local feature aggregation within a neighborhood. We verify the performance of EGT in a supervised learning setting on a wide range of experiments on benchmark datasets. Our findings indicate that convolutional aggregation is not an essential inductive bias for graphs and global self-attention can serve as a flexible and adaptive alternative.",0
"The use of transformer neural networks has resulted in impressive outcomes for unstructured data such as text and images, but they have not been widely adopted for graph-structured data due to the challenge of integrating complex structural information in the basic transformer framework. To address this, we propose a straightforward yet effective modification to the transformer known as residual edge channels, which enables the Edge-augmented Graph Transformer (EGT) to process and output both structural and node information. This approach allows for global self-attention, facilitating long-range interactions between nodes. Furthermore, the edge channels permit structural information to evolve across layers, and edge/link prediction tasks can be accomplished directly from the output embeddings of these channels. Additionally, we present a general positional encoding method for graphs based on Singular Value Decomposition, which improves the performance of EGT. Our global node feature aggregation-based framework outperforms Convolutional/Message-Passing Graph Neural Networks that rely on local feature aggregation in a neighborhood. We validated our framework in a supervised learning setting on various benchmark datasets, demonstrating that global self-attention can serve as a flexible and adaptive alternative to convolutional aggregation as an inductive bias for graphs.",1
"Link prediction is one of the key problems for graph-structured data. With the advancement of graph neural networks, graph autoencoders (GAEs) and variational graph autoencoders (VGAEs) have been proposed to learn graph embeddings in an unsupervised way. It has been shown that these methods are effective for link prediction tasks. However, they do not work well in link predictions when a node whose degree is zero (i.g., isolated node) is involved. We have found that GAEs/VGAEs make embeddings of isolated nodes close to zero regardless of their content features. In this paper, we propose a novel Variational Graph Normalized AutoEncoder (VGNAE) that utilize L2-normalization to derive better embeddings for isolated nodes. We show that our VGNAEs outperform the existing state-of-the-art models for link prediction tasks. The code is available at https://github.com/SeongJinAhn/VGNAE.",0
"Graph-structured data presents a significant challenge in link prediction. Recent advancements in graph neural networks have led to the development of graph autoencoders (GAEs) and variational graph autoencoders (VGAEs) which can learn graph embeddings in an unsupervised manner. These methods have proven to be effective in link prediction tasks. However, they are not effective in predicting links involving nodes with zero degree (i.e., isolated nodes). Our research shows that GAEs/VGAEs produce embeddings for isolated nodes that are close to zero, irrespective of the content features. To address this issue, we introduce a novel approach called Variational Graph Normalized AutoEncoder (VGNAE) that employs L2-normalization to generate better embeddings for isolated nodes. Our experiments demonstrate that VGNAEs outperform existing state-of-the-art models in link prediction tasks. The code for our approach is publicly available at https://github.com/SeongJinAhn/VGNAE.",1
"Data augmentation has been widely used in image data and linguistic data but remains under-explored on graph-structured data. Existing methods focus on augmenting the graph data from a global perspective and largely fall into two genres: structural manipulation and adversarial training with feature noise injection. However, the structural manipulation approach suffers information loss issues while the adversarial training approach may downgrade the feature quality by injecting noise. In this work, we introduce the local augmentation, which enhances node features by its local subgraph structures. Specifically, we model the data argumentation as a feature generation process. Given the central node's feature, our local augmentation approach learns the conditional distribution of its neighbors' features and generates the neighbors' optimal feature to boost the performance of downstream tasks. Based on the local augmentation, we further design a novel framework: LA-GNN, which can apply to any GNN models in a plug-and-play manner. Extensive experiments and analyses show that local augmentation consistently yields performance improvement for various GNN architectures across a diverse set of benchmarks. Code is available at https://github.com/Soughing0823/LAGNN.",0
"While data augmentation has been extensively used in image and linguistic data, its potential for graph-structured data has yet to be fully explored. Current methods for augmenting graph data adopt a global perspective and are primarily focused on structural manipulation and adversarial training with feature noise injection. However, these approaches are not without their flaws, as structural manipulation can result in information loss, and adversarial training may lead to a downgrade of feature quality due to noise injection. This study introduces a new approach called local augmentation, which enhances node features by utilizing its local subgraph structures. Essentially, the data augmentation process is modeled as a feature generation process, where the local augmentation approach learns the conditional distribution of the neighbors' features given the central node's feature. This generates optimal features for the neighbors, boosting the performance of downstream tasks. Based on this approach, a novel framework called LA-GNN is designed, which is compatible with any GNN models in a plug-and-play manner. The findings from extensive experiments and analyses indicate that local augmentation consistently improves the performance of various GNN architectures across a range of benchmarks. The code for this study can be found at https://github.com/Soughing0823/LAGNN.",1
"Graph Neural Network (GNN) research is rapidly growing thanks to the capacity of GNNs in learning distributed representations from graph-structured data. However, centralizing a massive amount of real-world graph data for GNN training is prohibitive due to privacy concerns, regulation restrictions, and commercial competitions. Federated learning (FL), a trending distributed learning paradigm, provides possibilities to solve this challenge while preserving data privacy. Despite recent advances in vision and language domains, there is no suitable platform for the FL of GNNs. To this end, we introduce FedGraphNN, an open FL benchmark system that can facilitate research on federated GNNs. FedGraphNN is built on a unified formulation of graph FL and contains a wide range of datasets from different domains, popular GNN models, and FL algorithms, with secure and efficient system support. Particularly for the datasets, we collect, preprocess, and partition 36 datasets from 7 domains, including both publicly available ones and specifically obtained ones such as hERG and Tencent. Our empirical analysis showcases the utility of our benchmark system, while exposing significant challenges in graph FL: federated GNNs perform worse in most datasets with a non-IID split than centralized GNNs; the GNN model that attains the best result in the centralized setting may not maintain its advantage in the FL setting. These results imply that more research efforts are needed to unravel the mystery behind federated GNNs. Moreover, our system performance analysis demonstrates that the FedGraphNN system is computationally efficient and secure to large-scale graphs datasets. We maintain the source code at https://github.com/FedML-AI/FedGraphNN.",0
"The growth of research on Graph Neural Networks (GNN) is due to their ability to learn distributed representations from graph-structured data. However, privacy concerns, regulation restrictions, and commercial competition make it prohibitive to centralize a large amount of real-world graph data for GNN training. To address this challenge while preserving data privacy, the distributed learning paradigm of Federated Learning (FL) is used. While there have been recent advances in vision and language domains, there is no suitable platform for FL of GNNs. Therefore, FedGraphNN is introduced as an open FL benchmark system that facilitates research on federated GNNs. It is built on a unified formulation of graph FL and contains a range of datasets from different domains, popular GNN models, and FL algorithms, with secure and efficient system support. Empirical analysis shows that federated GNNs perform worse in most datasets with a non-IID split than centralized GNNs, and that more research is needed to understand federated GNNs. The FedGraphNN system is computationally efficient and secure for large-scale graph datasets. The source code is available at https://github.com/FedML-AI/FedGraphNN.",1
"Interpreting deep neural networks from the ordinary differential equations (ODEs) perspective has inspired many efficient and robust network architectures. However, existing ODE based approaches ignore the relationship among data points, which is a critical component in many problems including few-shot learning and semi-supervised learning. In this paper, inspired by the diffusive ODEs, we propose a novel diffusion residual network (Diff-ResNet) to strengthen the interactions among data points. Under the structured data assumption, it is proved that the diffusion mechanism can decrease the distance-diameter ratio that improves the separability of inter-class points and reduces the distance among local intra-class points. This property can be easily adopted by the residual networks for constructing the separable hyperplanes. The synthetic binary classification experiments demonstrate the effectiveness of the proposed diffusion mechanism. Moreover, extensive experiments of few-shot image classification and semi-supervised graph node classification in various datasets validate the advantages of the proposed Diff-ResNet over existing few-shot learning methods.",0
"The ODEs perspective of interpreting deep neural networks has inspired several strong network architectures that are efficient and robust. However, these ODE-based approaches fail to consider the relationship among data points, which is a crucial aspect of many problems, such as semi-supervised and few-shot learning. In this study, we propose a novel network, called Diff-ResNet, that is inspired by diffusive ODEs to improve the interactions among data points. We demonstrate that the diffusion mechanism can enhance the separability of inter-class points and decrease the distance among local intra-class points, which can be effectively used by residual networks to construct separable hyperplanes. Synthetic binary classification experiments show the efficacy of the proposed diffusion mechanism. Furthermore, our extensive experiments on various datasets for few-shot image classification and semi-supervised graph node classification confirm the superiority of Diff-ResNet over existing few-shot learning methods.",1
"The success of machine learning stems from its structured data representation. Similar data have close representation as compressed codes for classification or emerged labels for clustering. We observe that the frequency of the internal representation follows power laws in both supervised and unsupervised learning. The scale-invariant distribution implies that machine learning largely compresses frequent typical data, and at the same time, differentiates many atypical data as outliers. In this study, we derive how the power laws can naturally arise in machine learning. In terms of information theory, the scale-invariant representation corresponds to a maximally uncertain data grouping among possible representations that guarantee pre-specified learning accuracy.",0
"Machine learning owes its success to its structured data representation, which allows for compressed codes to classify similar data and emerged labels for clustering. The frequency of the internal representation follows power laws in both supervised and unsupervised learning, indicating that frequent typical data are compressed while many atypical data are identified as outliers. This study explores how power laws emerge in machine learning and shows that the scale-invariant representation corresponds to an uncertain data grouping that guarantees pre-specified learning accuracy. This implies that machine learning can differentiate between typical and atypical data while compressing frequent data.",1
"Sensitive medical data is often subject to strict usage constraints. In this paper, we trained a generative adversarial network (GAN) on real-world electronic health records (EHR). It was then used to create a data-set of ""fake"" patients through synthetic data generation (SDG) to circumvent usage constraints. This real-world data was tabular, binary, intensive care unit (ICU) patient diagnosis data. The entire data-set was split into separate data silos to mimic real-world scenarios where multiple ICU units across different hospitals may have similarly structured data-sets within their own organisations but do not have access to each other's data-sets. We implemented federated learning (FL) to train separate GANs locally at each organisation, using their unique data silo and then combining the GANs into a single central GAN, without any siloed data ever being exposed. This global, central GAN was then used to generate the synthetic patients data-set. We performed an evaluation of these synthetic patients with statistical measures and through a structured review by a group of medical professionals. It was shown that there was no significant reduction in the quality of the synthetic EHR when we moved between training a single central model and training on separate data silos with individual models before combining them into a central model. This was true for both the statistical evaluation (Root Mean Square Error (RMSE) of 0.0154 for single-source vs. RMSE of 0.0169 for dual-source federated) and also for the medical professionals' evaluation (no quality difference between EHR generated from a single source and EHR generated from multiple sources).",0
"Strict usage constraints often apply to sensitive medical data. In this study, we utilized a generative adversarial network (GAN) to create a data-set of synthetic patients from real-world electronic health records (EHR), using synthetic data generation (SDG) to bypass usage restrictions. The EHR data was tabular and binary, consisting of diagnosis information for intensive care unit (ICU) patients. To simulate real-world scenarios where multiple ICU units may have similar but siloed data-sets, we divided the data into separate silos. We employed federated learning (FL) to train local GANs on each silo's unique data and then combined them into a central GAN without exposing any siloed data. We evaluated the synthetic patients using statistical measures and a review by medical professionals. Our results indicate that there was no significant decrease in synthetic EHR quality between training a single central model and training individual models on separate silos before combining them. Both the statistical evaluation and medical professionals' review showed no quality difference between EHR generated from a single source versus EHR generated from multiple sources.",1
"Message-Passing Neural Networks (MPNNs), the most prominent Graph Neural Network (GNN) framework, celebrate much success in the analysis of graph-structured data. Concurrently, the sparsification of Neural Network models attracts a great amount of academic and industrial interest. In this paper, we conduct a structured study of the effect of sparsification on the trainable part of MPNNs known as the Update step. To this end, we design a series of models to successively sparsify the linear transform in the Update step. Specifically, we propose the ExpanderGNN model with a tuneable sparsification rate and the Activation-Only GNN, which has no linear transform in the Update step. In agreement with a growing trend in the literature, the sparsification paradigm is changed by initialising sparse neural network architectures rather than expensively sparsifying already trained architectures. Our novel benchmark models enable a better understanding of the influence of the Update step on model performance and outperform existing simplified benchmark models such as the Simple Graph Convolution. The ExpanderGNNs, and in some cases the Activation-Only models, achieve performance on par with their vanilla counterparts on several downstream tasks while containing significantly fewer trainable parameters. In experiments with matching parameter numbers, our benchmark models outperform the state-of-the-art GNN models. Our code is publicly available at: https://github.com/ChangminWu/ExpanderGNN.",0
"The analysis of graph-structured data has been successful with Message-Passing Neural Networks (MPNNs), which are the most notable Graph Neural Network (GNN) framework. At the same time, Neural Network models' sparsification is attracting a lot of attention from academia and industry. This study aims to examine the effect of sparsification on the Update step, which is the trainable component of MPNNs. Linear transforms in the Update step are successively sparsified in a series of models, including ExpanderGNN and Activation-Only GNN. Instead of costly sparsification of trained architectures, sparse neural network architectures are initialized to change the sparsification paradigm, reflecting a growing trend in literature. The novel benchmark models are designed to better understand the impact of the Update step on model performance and outperformed existing simplified benchmark models, such as the Simple Graph Convolution. The ExpanderGNNs and, in some cases, Activation-Only models achieved comparable performance to their vanilla counterparts on numerous downstream tasks while having significantly fewer trainable parameters. Benchmark models also surpassed state-of-the-art GNN models with matching parameter numbers. The code is available publicly at https://github.com/ChangminWu/ExpanderGNN.",1
"Graph feature extraction is a fundamental task in graphs analytics. Using feature vectors (graph descriptors) in tandem with data mining algorithms that operate on Euclidean data, one can solve problems such as classification, clustering, and anomaly detection on graph-structured data. This idea has proved fruitful in the past, with spectral-based graph descriptors providing state-of-the-art classification accuracy on benchmark datasets. However, these algorithms do not scale to large graphs since: 1) they require storing the entire graph in memory, and 2) the end-user has no control over the algorithm's runtime. In this paper, we present single-pass streaming algorithms to approximate structural features of graphs (counts of subgraphs of order $k \geq 4$). Operating on edge streams allows us to avoid keeping the entire graph in memory, and controlling the sample size enables us to control the time taken by the algorithm. We demonstrate the efficacy of our descriptors by analyzing the approximation error, classification accuracy, and scalability to massive graphs. Our experiments showcase the effect of the sample size on approximation error and predictive accuracy. The proposed descriptors are applicable on graphs with millions of edges within minutes and outperform the state-of-the-art descriptors in classification accuracy.",0
"In graphs analytics, extracting features from graphs is a crucial task. By using feature vectors called graph descriptors along with data mining algorithms that operate on Euclidean data, problems like clustering, classification, and anomaly detection can be solved for graph-structured data. Spectral-based graph descriptors have shown excellent classification accuracy on benchmark datasets. However, these algorithms are not suitable for large graphs as they require storing the entire graph in memory, and the algorithm's runtime cannot be controlled by the user. This paper presents single-pass streaming algorithms that use edge streams to estimate structural features of graphs, such as counts of subgraphs with order k≥4. This approach enables us to avoid storing the entire graph in memory and control the sample size to manage the algorithm's time. Our proposed descriptors have been tested for scalability, approximation error, and classification accuracy on massive graphs with millions of edges within minutes. Our experiments reveal the impact of the sample size on approximation error and predictive accuracy. The proposed descriptors outperform the state-of-the-art descriptors in classification accuracy.",1
"Networks are ubiquitous in the real world such as social networks and communication networks, and anomaly detection on networks aims at finding nodes whose structural or attributed patterns deviate significantly from the majority of reference nodes. However, most of the traditional anomaly detection methods neglect the relation structure information among data points and therefore cannot effectively generalize to the graph structure data. In this paper, we propose an end-to-end model of Deep Dual Support Vector Data description based Autoencoder (Dual-SVDAE) for anomaly detection on attributed networks, which considers both the structure and attribute for attributed networks. Specifically, Dual-SVDAE consists of a structure autoencoder and an attribute autoencoder to learn the latent representation of the node in the structure space and attribute space respectively. Then, a dual-hypersphere learning mechanism is imposed on them to learn two hyperspheres of normal nodes from the structure and attribute perspectives respectively. Moreover, to achieve joint learning between the structure and attribute of the network, we fuse the structure embedding and attribute embedding as the final input of the feature decoder to generate the node attribute. Finally, abnormal nodes can be detected by measuring the distance of nodes to the learned center of each hypersphere in the latent structure space and attribute space respectively. Extensive experiments on the real-world attributed networks show that Dual-SVDAE consistently outperforms the state-of-the-arts, which demonstrates the effectiveness of the proposed method.",0
"In the real world, networks such as social and communication networks are everywhere. Anomaly detection on networks aims to identify nodes that deviate significantly from the majority of reference nodes in terms of their structural or attributed patterns. However, traditional anomaly detection methods often overlook the relationship structure information among data points, making it difficult to generalize effectively to graph structure data. This paper proposes an end-to-end model for anomaly detection on attributed networks called Deep Dual Support Vector Data description based Autoencoder (Dual-SVDAE). Dual-SVDAE considers both the structure and attribute for attributed networks by using a structure autoencoder and an attribute autoencoder to learn the latent representation of the node in the structure space and attribute space, respectively. A dual-hypersphere learning mechanism is imposed on them to learn two hyperspheres of normal nodes from the structure and attribute perspectives. To achieve joint learning between the structure and attribute of the network, the structure embedding and attribute embedding are fused as the final input of the feature decoder to generate the node attribute. Finally, abnormal nodes can be detected by measuring the distance of nodes to the learned center of each hypersphere in the latent structure space and attribute space. Extensive experiments on real-world attributed networks show that Dual-SVDAE outperforms the state-of-the-art consistently, demonstrating its effectiveness.",1
"Graph neural networks (GNNs) have shown great power in modeling graph structured data. However, similar to other machine learning models, GNNs may make biased predictions w.r.t protected sensitive attributes, e.g., skin color and gender. This is because the training data often contains historical bias towards sensitive attributes. In addition, we empirically show that the discrimination in GNNs can be magnified by graph structures and the message-passing mechanism of GNNs. As a result, the applications of GNNs in high-stake domains such as crime rate prediction would be largely limited. Though extensive studies of fair classification have been conducted on i.i.d data, methods to address the problem of discrimination on non-i.i.d data are rather limited. Generally, learning fair models require abundant sensitive attributes to regularize the model. However, for many graphs such as social networks, users are reluctant to share sensitive attributes. Thus, only limited sensitive attributes are available for fair GNN training in practice. Moreover, directly collecting and applying the sensitive attributes in fair model training may cause privacy issues, because the sensitive information can be leaked in data breach or attacks on the trained model. Therefore, we study a novel and crucial problem of learning fair GNNs with limited and private sensitive attribute information. In an attempt to address these problems, FairGNN is proposed to eliminate the bias of GNNs whilst maintaining high accuracy by leveraging graph structures and limited sensitive information. We further extend FairGNN to NT-FairGNN which can achieve both fairness and privacy on sensitive attributes by using limited and private sensitive attributes. Theoretical analysis and extensive experiments on real-world datasets demonstrate the effectiveness of FairGNN and NT-FairGNN in achieving fair and high-accurate classification.",0
"The ability of Graph neural networks (GNNs) to model graph structured data has been demonstrated to be highly effective. However, like other machine learning models, GNNs may exhibit biased predictions based on protected sensitive attributes such as skin color and gender. This is due to the historical bias towards these attributes in the training data. Furthermore, we have found that the discrimination in GNNs can be exacerbated by the graph structures and message-passing mechanisms. As a result, the use of GNNs in high-stakes domains such as crime rate prediction is limited. Although there have been numerous studies on fair classification of i.i.d data, addressing discrimination on non-i.i.d data is challenging. To create fair models, abundant sensitive attributes are needed to regulate the model, but social network users are often unwilling to share such information. Additionally, directly collecting and applying sensitive attributes in fair model training may lead to privacy issues. Our research addresses this challenge by proposing FairGNN, a model that eliminates the bias of GNNs while maintaining high accuracy by utilizing graph structures and limited sensitive information. We further extended our model to NT-FairGNN, which achieves both fairness and privacy on sensitive attributes by using limited and private sensitive attributes. Our theoretical analysis and experiments on real-world datasets demonstrate the effectiveness of FairGNN and NT-FairGNN in achieving fair and high-accurate classification.",1
"Learning distributions over graph-structured data is a challenging task with many applications in biology and chemistry. In this work we use an energy-based model (EBM) based on multi-channel graph neural networks (GNN) to learn permutation invariant unnormalized density functions on graphs. Unlike standard EBM training methods our approach is to learn the model via minimizing adversarial stein discrepancy. Samples from the model can be obtained via Langevin dynamics based MCMC. We find that this approach achieves competitive results on graph generation compared to benchmark models.",0
"The acquisition of knowledge about distributions over data structured in graphs is a difficult undertaking that is applicable in numerous fields, such as biology and chemistry. Our study employs an energy-based model (EBM) that relies on multi-channel graph neural networks (GNN) to learn unnormalized density functions that are permutation invariant on graphs. Our approach to training the model is dissimilar to conventional EBM training methods as we minimize adversarial Stein discrepancy. By utilizing Langevin dynamics-based Markov Chain Monte Carlo (MCMC), the model can generate samples. Our research reveals that this approach yields comparable outcomes on graph generation when compared to benchmark models.",1
"UMAP is a non-parametric graph-based dimensionality reduction algorithm using applied Riemannian geometry and algebraic topology to find low-dimensional embeddings of structured data. The UMAP algorithm consists of two steps: (1) Compute a graphical representation of a dataset (fuzzy simplicial complex), and (2) Through stochastic gradient descent, optimize a low-dimensional embedding of the graph. Here, we extend the second step of UMAP to a parametric optimization over neural network weights, learning a parametric relationship between data and embedding. We first demonstrate that Parametric UMAP performs comparably to its non-parametric counterpart while conferring the benefit of a learned parametric mapping (e.g. fast online embeddings for new data). We then explore UMAP as a regularization, constraining the latent distribution of autoencoders, parametrically varying global structure preservation, and improving classifier accuracy for semi-supervised learning by capturing structure in unlabeled data. Google Colab walkthrough: https://colab.research.google.com/drive/1WkXVZ5pnMrm17m0YgmtoNjM_XHdnE5Vp?usp=sharing",0
"UMAP is an algorithm that reduces the dimensions of structured data by using non-parametric graph-based techniques and applied Riemannian geometry and algebraic topology. The algorithm comprises of two steps: first, a fuzzy simplicial complex is computed as a graphical representation of the dataset; second, a low-dimensional embedding of the graph is optimized through stochastic gradient descent. In this study, we focus on the second step of UMAP and propose a parametric optimization method using neural network weights to learn a parametric relationship between data and embedding. Our results show that Parametric UMAP performs similarly to non-parametric UMAP but with the added advantage of a learned parametric mapping, which allows for faster online embeddings of new data. Additionally, we explore the use of UMAP as a regularization method to constrain the latent distribution of autoencoders, parametrically vary global structure preservation, and improve classifier accuracy for semi-supervised learning by capturing structure in unlabeled data. A Google Colab walkthrough of our study is available at: https://colab.research.google.com/drive/1WkXVZ5pnMrm17m0YgmtoNjM_XHdnE5Vp?usp=sharing.",1
"Graph Neural Networks (GNNs), which generalize the deep neural networks to graph-structured data, have achieved great success in modeling graphs. However, as an extension of deep learning for graphs, GNNs lack explainability, which largely limits their adoption in scenarios that demand the transparency of models. Though many efforts are taken to improve the explainability of deep learning, they mainly focus on i.i.d data, which cannot be directly applied to explain the predictions of GNNs because GNNs utilize both node features and graph topology to make predictions. There are only very few work on the explainability of GNNs and they focus on post-hoc explanations. Since post-hoc explanations are not directly obtained from the GNNs, they can be biased and misrepresent the true explanations. Therefore, in this paper, we study a novel problem of self-explainable GNNs which can simultaneously give predictions and explanations. We propose a new framework which can find $K$-nearest labeled nodes for each unlabeled node to give explainable node classification, where nearest labeled nodes are found by interpretable similarity module in terms of both node similarity and local structure similarity. Extensive experiments on real-world and synthetic datasets demonstrate the effectiveness of the proposed framework for explainable node classification.",0
"Graph Neural Networks (GNNs) have been successful in modeling graphs but lack transparency, limiting their adoption in scenarios where model explainability is necessary. Existing efforts to improve explainability in deep learning focus on i.i.d data and cannot be directly applied to GNNs. Previous work on GNN explainability has focused on post-hoc explanations, which can be biased and misrepresent true explanations. This paper proposes a novel framework for self-explainable GNNs that can provide predictions and explanations simultaneously. The framework finds the $K$-nearest labeled nodes for each unlabeled node using an interpretable similarity module that accounts for both node and local structure similarity. Extensive experiments on real-world and synthetic datasets demonstrate the effectiveness of the proposed framework for explainable node classification.",1
"Representation learning on static graph-structured data has shown a significant impact on many real-world applications. However, less attention has been paid to the evolving nature of temporal networks, in which the edges are often changing over time. The embeddings of such temporal networks should encode both graph-structured information and the temporally evolving pattern. Existing approaches in learning temporally evolving network representations fail to capture the temporal interdependence. In this paper, we propose Toffee, a novel approach for temporal network representation learning based on tensor decomposition. Our method exploits the tensor-tensor product operator to encode the cross-time information, so that the periodic changes in the evolving networks can be captured. Experimental results demonstrate that Toffee outperforms existing methods on multiple real-world temporal networks in generating effective embeddings for the link prediction tasks.",0
"Although representation learning on static graph-structured data has had a significant impact on various practical applications, there has been less emphasis on the evolving nature of temporal networks, where edges frequently change over time. Embeddings of such temporal networks must encode both the graph-structured information and the temporal pattern. Current approaches for learning such representations fail to capture the temporal interdependence. We present Toffee, a novel method for temporal network representation learning that utilizes tensor decomposition. Our approach utilizes the tensor-tensor product operator to encode cross-time information, thus capturing periodic changes in evolving networks. Experimental results demonstrate that Toffee surpasses existing methods in generating effective embeddings for link prediction tasks on multiple real-world temporal networks.",1
"Graph structured data have enabled several successful applications such as recommendation systems and traffic prediction, given the rich node features and edges information. However, these high-dimensional features and high-order adjacency information are usually heterogeneous and held by different data holders in practice. Given such vertical data partition (e.g., one data holder will only own either the node features or edge information), different data holders have to develop efficient joint training protocols rather than directly transfer data to each other due to privacy concerns. In this paper, we focus on the edge privacy, and consider a training scenario where Bob with node features will first send training node features to Alice who owns the adjacency information. Alice will then train a graph neural network (GNN) with the joint information and release an inference API. During inference, Bob is able to provide test node features and query the API to obtain the predictions for test nodes. Under this setting, we first propose a privacy attack LinkTeller via influence analysis to infer the private edge information held by Alice via designing adversarial queries for Bob. We then empirically show that LinkTeller is able to recover a significant amount of private edges, outperforming existing baselines. To further evaluate the privacy leakage, we adapt an existing algorithm for differentially private graph convolutional network (DP GCN) training and propose a new DP GCN mechanism LapGraph. We show that these DP GCN mechanisms are not always resilient against LinkTeller empirically under mild privacy guarantees ($\varepsilon>5$). Our studies will shed light on future research towards designing more resilient privacy-preserving GCN models; in the meantime, provide an in-depth understanding of the tradeoff between GCN model utility and robustness against potential privacy attacks.",0
"The utilization of graph structured data has resulted in successful applications like traffic prediction and recommendation systems, thanks to the abundant node features and edge information. However, the high-dimensional features and adjacency information are often heterogeneous and owned by different data holders. This vertical data partition poses privacy concerns, meaning that data cannot be directly transferred among data holders. Instead, efficient joint training protocols must be developed. This paper focuses on edge privacy and considers a training scenario where Bob transfers node features to Alice, who possesses adjacency information. Alice trains a graph neural network (GNN) with the joint information and releases an inference API. Bob can then provide test node features and obtain predictions for test nodes via the API. We propose LinkTeller, a privacy attack via influence analysis to infer private edge information held by Alice. Our empirical results show that LinkTeller outperforms existing baselines in recovering private edges. We also propose a new DP GCN mechanism called LapGraph and evaluate its resilience against LinkTeller under mild privacy guarantees. Our findings suggest the need for more resilient privacy-preserving GCN models and provide insight into the tradeoff between utility and robustness against potential privacy attacks.",1
"Spatio-temporal forecasting is of great importance in a wide range of dynamical systems applications from atmospheric science, to recent COVID-19 spread modeling. These applications rely on accurate predictions of spatio-temporal structured data reflecting real-world phenomena. A stunning characteristic is that the dynamical system is not only driven by some physics laws but also impacted by the localized factor in spatial and temporal regions. One of the major challenges is to infer the underlying causes, which generate the perceived data stream and propagate the involved causal dynamics through the distributed observing units. Another challenge is that the success of machine learning based predictive models requires massive annotated data for model training. However, the acquisition of high-quality annotated data is objectively manual and tedious as it needs a considerable amount of human intervention, making it infeasible in fields that require high levels of expertise. To tackle these challenges, we advocate a spatio-temporal physics-coupled neural networks (ST-PCNN) model to learn the underlying physics of the dynamical system and further couple the learned physics to assist the learning of the recurring dynamics. To deal with data-acquisition constraints, an active learning mechanism with Kriging for actively acquiring the most informative data is proposed for ST-PCNN training in a partially observable environment. Our experiments on both synthetic and real-world datasets exhibit that the proposed ST-PCNN with active learning converges to near optimal accuracy with substantially fewer instances.",0
"Accurate predictions of spatio-temporal structured data are crucial for a variety of applications, such as atmospheric science and COVID-19 spread modeling. The dynamical systems involved in these applications are influenced not only by physics laws but also by localized factors in spatial and temporal regions. Identifying the underlying causes of the observed data stream and propagating the causal dynamics through distributed observing units are major challenges. Additionally, machine learning based predictive models require massive amounts of annotated data for training, which is often difficult to acquire manually. To address these challenges, we propose a spatio-temporal physics-coupled neural networks (ST-PCNN) model that learns the underlying physics of the system and couples it with the recurring dynamics. To overcome data-acquisition constraints, we introduce an active learning mechanism with Kriging that actively acquires the most informative data in a partially observable environment. Our experiments on synthetic and real-world datasets demonstrate that the proposed ST-PCNN with active learning achieves near optimal accuracy with substantially fewer instances.",1
"Graph neural networks (GNNs) have been popularly used in analyzing graph-structured data, showing promising results in various applications such as node classification, link prediction and network recommendation. In this paper, we present a new graph attention neural network, namely GIPA, for attributed graph data learning. GIPA consists of three key components: attention, feature propagation and aggregation. Specifically, the attention component introduces a new multi-layer perceptron based multi-head to generate better non-linear feature mapping and representation than conventional implementations such as dot-product. The propagation component considers not only node features but also edge features, which differs from existing GNNs that merely consider node features. The aggregation component uses a residual connection to generate the final embedding. We evaluate the performance of GIPA using the Open Graph Benchmark proteins (ogbn-proteins for short) dataset. The experimental results reveal that GIPA can beat the state-of-the-art models in terms of prediction accuracy, e.g., GIPA achieves an average test ROC-AUC of $0.8700\pm 0.0010$ and outperforms all the previous methods listed in the ogbn-proteins leaderboard.",0
"The use of graph neural networks (GNNs) has become popular in the analysis of data with graph structures. They have shown promising results in various applications such as node classification, link prediction and network recommendation. This paper introduces a new graph attention neural network (GIPA) that is specifically designed for learning attributed graph data. GIPA has three key components: attention, feature propagation and aggregation. The attention component uses a multi-layer perceptron based multi-head to generate better non-linear feature mapping and representation, which is an improvement over dot-product. The propagation component considers both node and edge features, unlike existing GNNs that only consider node features. The aggregation component uses a residual connection to generate the final embedding. The performance of GIPA is evaluated using the Open Graph Benchmark proteins (ogbn-proteins) dataset, and the results show that it outperforms all previous methods listed in the ogbn-proteins leaderboard. GIPA achieves an average test ROC-AUC of $0.8700\pm 0.0010$, indicating a high level of prediction accuracy.",1
"Structured text understanding on Visually Rich Documents (VRDs) is a crucial part of Document Intelligence. Due to the complexity of content and layout in VRDs, structured text understanding has been a challenging task. Most existing studies decoupled this problem into two sub-tasks: entity labeling and entity linking, which require an entire understanding of the context of documents at both token and segment levels. However, little work has been concerned with the solutions that efficiently extract the structured data from different levels. This paper proposes a unified framework named StrucTexT, which is flexible and effective for handling both sub-tasks. Specifically, based on the transformer, we introduce a segment-token aligned encoder to deal with the entity labeling and entity linking tasks at different levels of granularity. Moreover, we design a novel pre-training strategy with three self-supervised tasks to learn a richer representation. StrucTexT uses the existing Masked Visual Language Modeling task and the new Sentence Length Prediction and Paired Boxes Direction tasks to incorporate the multi-modal information across text, image, and layout. We evaluate our method for structured text understanding at segment-level and token-level and show it outperforms the state-of-the-art counterparts with significantly superior performance on the FUNSD, SROIE, and EPHOIE datasets.",0
"Structured text understanding in Visually Rich Documents (VRDs) is a critical aspect of Document Intelligence. However, given the intricate content and layout of VRDs, structured text understanding poses a significant challenge. Previous studies have divided this issue into two sub-tasks, namely entity labeling and entity linking, which require a comprehensive understanding of the document context at the token and segment levels. Unfortunately, there has been little research on how to effectively extract structured data from different levels. To address this gap, this paper presents StrucTexT, a comprehensive framework that efficiently handles both sub-tasks. StrucTexT employs a segment-token aligned encoder based on the transformer architecture to manage entity labeling and entity linking tasks at varying levels of granularity. Additionally, the framework introduces a novel pre-training strategy that incorporates multi-modal information across text, image, and layout using three self-supervised tasks, including Masked Visual Language Modeling, Sentence Length Prediction, and Paired Boxes Direction. The proposed approach outperforms state-of-the-art methods on the FUNSD, SROIE, and EPHOIE datasets for structured text understanding at both the segment and token levels.",1
"One intriguing property of deep neural networks (DNNs) is their inherent vulnerability to backdoor attacks -- a trojan model responds to trigger-embedded inputs in a highly predictable manner while functioning normally otherwise. Despite the plethora of prior work on DNNs for continuous data (e.g., images), the vulnerability of graph neural networks (GNNs) for discrete-structured data (e.g., graphs) is largely unexplored, which is highly concerning given their increasing use in security-sensitive domains. To bridge this gap, we present GTA, the first backdoor attack on GNNs. Compared with prior work, GTA departs in significant ways: graph-oriented -- it defines triggers as specific subgraphs, including both topological structures and descriptive features, entailing a large design spectrum for the adversary; input-tailored -- it dynamically adapts triggers to individual graphs, thereby optimizing both attack effectiveness and evasiveness; downstream model-agnostic -- it can be readily launched without knowledge regarding downstream models or fine-tuning strategies; and attack-extensible -- it can be instantiated for both transductive (e.g., node classification) and inductive (e.g., graph classification) tasks, constituting severe threats for a range of security-critical applications. Through extensive evaluation using benchmark datasets and state-of-the-art models, we demonstrate the effectiveness of GTA. We further provide analytical justification for its effectiveness and discuss potential countermeasures, pointing to several promising research directions.",0
"Deep neural networks (DNNs) have an intriguing property of being naturally vulnerable to backdoor attacks. This means that a trojan model can respond predictably when triggered by a specific input while functioning normally otherwise. While there has been a lot of prior research on DNNs for continuous data, such as images, the vulnerability of graph neural networks (GNNs) for discrete-structured data, like graphs, is largely unexplored. This is a significant concern as GNNs are increasingly used in security-sensitive domains. To address this gap, we propose GTA, the first backdoor attack on GNNs. GTA differs from previous work in several ways. It is graph-oriented, meaning triggers are defined as specific subgraphs, allowing for a large design spectrum for the adversary. It is input-tailored, meaning triggers are dynamically adapted to individual graphs, optimizing both attack effectiveness and evasiveness. It is downstream model-agnostic and can be launched without knowledge of downstream models or fine-tuning strategies. Finally, it is attack-extensible and can be instantiated for both transductive and inductive tasks, making it a severe threat to a range of security-critical applications. We evaluate GTA extensively using benchmark datasets and state-of-the-art models, demonstrating its effectiveness. We also provide analytical justification for its effectiveness and suggest potential countermeasures, highlighting promising research directions.",1
"Deep convolutional neural networks (CNNs) have shown outstanding performance in the task of semantically segmenting images. Applying the same methods on 3D data still poses challenges due to the heavy memory requirements and the lack of structured data. Here, we propose LatticeNet, a novel approach for 3D semantic segmentation, which takes raw point clouds as input. A PointNet describes the local geometry which we embed into a sparse permutohedral lattice. The lattice allows for fast convolutions while keeping a low memory footprint. Further, we introduce DeformSlice, a novel learned data-dependent interpolation for projecting lattice features back onto the point cloud. We present results of 3D segmentation on multiple datasets where our method achieves state-of-the-art performance. We also extend and evaluate our network for instance and dynamic object segmentation.",0
"The use of deep convolutional neural networks (CNNs) has proven to be highly effective in semantically segmenting images. However, applying these methods to 3D data has its own set of challenges. These include the heavy memory requirements and unstructured data. To address this, we propose a new approach called LatticeNet for 3D semantic segmentation. Our method uses raw point clouds as input and incorporates a PointNet to describe local geometry. This information is then embedded into a sparse permutohedral lattice, allowing for fast convolutions with a low memory footprint. Additionally, we introduce DeformSlice, a learned data-dependent interpolation technique that projects lattice features onto the point cloud. Our results show that our approach achieves state-of-the-art performance for 3D segmentation on multiple datasets. We also extend our network to instance and dynamic object segmentation and evaluate its effectiveness.",1
"Combinatorial optimization problems (COPs) on the graph with real-life applications are canonical challenges in Computer Science. The difficulty of finding quality labels for problem instances holds back leveraging supervised learning across combinatorial problems. Reinforcement learning (RL) algorithms have recently been adopted to solve this challenge automatically. The underlying principle of this approach is to deploy a graph neural network (GNN) for encoding both the local information of the nodes and the graph-structured data in order to capture the current state of the environment. Then, it is followed by the actor to learn the problem-specific heuristics on its own and make an informed decision at each state for finally reaching a good solution. Recent studies on this subject mainly focus on a family of combinatorial problems on the graph, such as the travel salesman problem, where the proposed model aims to find an ordering of vertices that optimizes a given objective function. We use the security-aware phone clone allocation in the cloud as a classical quadratic assignment problem (QAP) to investigate whether or not deep RL-based model is generally applicable to solve other classes of such hard problems. Extensive empirical evaluation shows that existing RL-based model may not generalize to QAP.",0
"Canonical challenges in Computer Science are Combinatorial optimization problems (COPs) on real-life application graphs. However, the difficulty of finding quality labels for problem instances hinders the use of supervised learning in combinatorial problems. To address this challenge, Reinforcement learning (RL) algorithms have been adopted, using a graph neural network (GNN) to encode local information and graph-structured data to capture the environment's current state. This enables the actor to learn problem-specific heuristics and make informed decisions for reaching a good solution. Although recent studies have focused on combinatorial problems like the Traveling Salesman Problem, we investigate the applicability of deep RL-based models on other hard problems like the Security-Aware Phone Clone Allocation in the cloud, a Quadratic Assignment Problem (QAP). Unfortunately, extensive empirical evaluation shows that existing RL-based models may not generalize to QAP.",1
"Deep learning's performance has been extensively recognized recently. Graph neural networks (GNNs) are designed to deal with graph-structural data that classical deep learning does not easily manage. Since most GNNs were created using distinct theories, direct comparisons are impossible. Prior research has primarily concentrated on categorizing existing models, with little attention paid to their intrinsic connections. The purpose of this study is to establish a unified framework that integrates GNNs based on spectral graph and approximation theory. The framework incorporates a strong integration between spatial- and spectral-based GNNs while tightly associating approaches that exist within each respective domain.",0
"The remarkable progress of deep learning has been acknowledged widely in recent times. However, classical deep learning encounters difficulty in handling graph-structural data, which is where the Graph Neural Networks (GNNs) come into play. Since GNNs have been developed using different theories, it is not possible to make direct comparisons. Previous research has largely focused on categorizing the existing models, without delving into their inherent connections. Therefore, the objective of this study is to establish a unified framework that integrates GNNs based on spectral graph and approximation theory. This framework aims to combine spatial- and spectral-based GNNs while closely associating the approaches that exist within each respective domain.",1
"Decision forests (Forests), in particular random forests and gradient boosting trees, have demonstrated state-of-the-art accuracy compared to other methods in many supervised learning scenarios. In particular, Forests dominate other methods in tabular data, that is, when the feature space is unstructured, so that the signal is invariant to a permutation of the feature indices. However, in structured data lying on a manifold (such as images, text, and speech) deep networks (Networks), specifically convolutional deep networks (ConvNets), tend to outperform Forests. We conjecture that at least part of the reason for this is that the input to Networks is not simply the feature magnitudes, but also their indices. In contrast, naive Forest implementations fail to explicitly consider feature indices. A recently proposed Forest approach demonstrates that Forests, for each node, implicitly sample a random matrix from some specific distribution. These Forests, like some classes of Networks, learn by partitioning the feature space into convex polytopes corresponding to linear functions. We build on that approach and show that one can choose distributions in a manifold-aware fashion to incorporate feature locality. We demonstrate the empirical performance on data whose features live on three different manifolds: a torus, images, and time-series. Moreover, we demonstrate its strength in multivariate simulated settings and also show superiority in predicting surgical outcome in epilepsy patients and predicting movement direction from raw stereotactic EEG data from non-motor brain regions. In all simulations and real data, Manifold Oblique Random Forest (MORF) algorithm outperforms approaches that ignore feature space structure and challenges the performance of ConvNets. Moreover, MORF runs fast and maintains interpretability and theoretical justification.",0
"Decision forests, specifically random forests and gradient boosting trees, have proven to be highly accurate in comparison to other methods in various supervised learning scenarios. They excel in tabular data where the feature space is unstructured and the signal remains invariant to a permutation of the feature indices. However, deep networks, particularly convolutional deep networks, tend to perform better in structured data lying on a manifold such as images, text, and speech. This may be because deep networks consider not only the feature magnitudes but also their indices, while naive forest implementations fail to explicitly consider feature indices. Recently, a forest approach demonstrated that forests implicitly sample a random matrix from specific distributions and learn by partitioning the feature space into convex polytopes corresponding to linear functions. We build on this approach by incorporating feature locality through manifold-aware distribution selection. We show the empirical performance of the Manifold Oblique Random Forest (MORF) algorithm on data from three different manifolds: the torus, images, and time-series. MORF outperforms approaches that ignore feature space structure and challenges the performance of ConvNets with fast computation, interpretability, and theoretical justification. We also demonstrate the superiority of MORF in predicting surgical outcomes in epilepsy patients and movement direction from raw stereotactic EEG data from non-motor brain regions.",1
"Graph neural networks (GNN) have shown great success in learning from graph-structured data. They are widely used in various applications, such as recommendation, fraud detection, and search. In these domains, the graphs are typically large, containing hundreds of millions of nodes and several billions of edges. To tackle this challenge, we develop DistDGL, a system for training GNNs in a mini-batch fashion on a cluster of machines. DistDGL is based on the Deep Graph Library (DGL), a popular GNN development framework. DistDGL distributes the graph and its associated data (initial features and embeddings) across the machines and uses this distribution to derive a computational decomposition by following an owner-compute rule. DistDGL follows a synchronous training approach and allows ego-networks forming the mini-batches to include non-local nodes. To minimize the overheads associated with distributed computations, DistDGL uses a high-quality and light-weight min-cut graph partitioning algorithm along with multiple balancing constraints. This allows it to reduce communication overheads and statically balance the computations. It further reduces the communication by replicating halo nodes and by using sparse embedding updates. The combination of these design choices allows DistDGL to train high-quality models while achieving high parallel efficiency and memory scalability. We demonstrate our optimizations on both inductive and transductive GNN models. Our results show that DistDGL achieves linear speedup without compromising model accuracy and requires only 13 seconds to complete a training epoch for a graph with 100 million nodes and 3 billion edges on a cluster with 16 machines. DistDGL is now publicly available as part of DGL:https://github.com/dmlc/dgl/tree/master/python/dgl/distributed.",0
"Learning from graph-structured data has been successful with Graph Neural Networks (GNN), which are widely used in various applications such as recommendation, fraud detection, and search. However, the graphs in these domains are typically large, containing hundreds of millions of nodes and several billions of edges, which poses a challenge. To address this challenge, DistDGL was developed, which is a system for training GNNs on a cluster of machines in a mini-batch fashion. DistDGL is based on the Deep Graph Library (DGL) and distributes the graph and its associated data across machines to derive computational decomposition. DistDGL follows a synchronous training approach and uses high-quality, lightweight min-cut graph partitioning algorithms to reduce communication overheads and statically balance computations. DistDGL also minimizes communication overheads by replicating halo nodes and using sparse embedding updates. The design choices of DistDGL allow it to train high-quality models, achieve high parallel efficiency and memory scalability, and demonstrate linear speedup without compromising model accuracy. DistDGL is publicly available as part of DGL: https://github.com/dmlc/dgl/tree/master/python/dgl/distributed.",1
"Data selection methods, such as active learning and core-set selection, are useful tools for improving the data efficiency of deep learning models on large-scale datasets. However, recent deep learning models have moved forward from independent and identically distributed data to graph-structured data, such as social networks, e-commerce user-item graphs, and knowledge graphs. This evolution has led to the emergence of Graph Neural Networks (GNNs) that go beyond the models existing data selection methods are designed for. Therefore, we present Grain, an efficient framework that opens up a new perspective through connecting data selection in GNNs with social influence maximization. By exploiting the common patterns of GNNs, Grain introduces a novel feature propagation concept, a diversified influence maximization objective with novel influence and diversity functions, and a greedy algorithm with an approximation guarantee into a unified framework. Empirical studies on public datasets demonstrate that Grain significantly improves both the performance and efficiency of data selection (including active learning and core-set selection) for GNNs. To the best of our knowledge, this is the first attempt to bridge two largely parallel threads of research, data selection, and social influence maximization, in the setting of GNNs, paving new ways for improving data efficiency.",0
"Methods for selecting data, such as active learning and core-set selection, can enhance the efficiency of deep learning models on vast datasets. However, the advancement of deep learning models has led to the use of graph-structured data such as social networks, e-commerce user-item graphs, and knowledge graphs. This evolution has given rise to Graph Neural Networks (GNNs), which require new data selection methods. To address this issue, we propose Grain, an efficient framework that connects data selection in GNNs with social influence maximization. Grain exploits common GNN patterns and introduces a novel feature propagation concept, diversified influence maximization objective, and greedy algorithm with an approximation guarantee. Empirical studies on public datasets demonstrate that Grain significantly improves both the performance and efficiency of data selection for GNNs, including active learning and core-set selection. This is the first attempt to bridge data selection and social influence maximization research in GNNs, providing new ways to enhance data efficiency.",1
"The geometric structure of an optimization landscape is argued to be fundamentally important to support the success of deep neural network learning. A direct computation of the landscape beyond two layers is hard. Therefore, to capture the global view of the landscape, an interpretable model of the network-parameter (or weight) space must be established. However, the model is lacking so far. Furthermore, it remains unknown what the landscape looks like for deep networks of binary synapses, which plays a key role in robust and energy efficient neuromorphic computation. Here, we propose a statistical mechanics framework by directly building a least structured model of the high-dimensional weight space, considering realistic structured data, stochastic gradient descent training, and the computational depth of neural networks. We also consider whether the number of network parameters outnumbers the number of supplied training data, namely, over- or under-parametrization. Our least structured model reveals that the weight spaces of the under-parametrization and over-parameterization cases belong to the same class, in the sense that these weight spaces are well-connected without any hierarchical clustering structure. In contrast, the shallow-network has a broken weight space, characterized by a discontinuous phase transition, thereby clarifying the benefit of depth in deep learning from the angle of high dimensional geometry. Our effective model also reveals that inside a deep network, there exists a liquid-like central part of the architecture in the sense that the weights in this part behave as randomly as possible, providing algorithmic implications. Our data-driven model thus provides a statistical mechanics insight about why deep learning is unreasonably effective in terms of the high-dimensional weight space, and how deep networks are different from shallow ones.",0
"The success of deep neural network learning relies heavily on the geometric structure of the optimization landscape. However, computing the landscape beyond two layers is difficult, making it necessary to establish an interpretable model of the weight space to capture a global view of the landscape. Unfortunately, such a model is currently lacking. Additionally, the landscape of deep networks with binary synapses, which are crucial for energy-efficient neuromorphic computation, remains unknown. To address these issues, we propose a statistical mechanics framework that considers realistic structured data, stochastic gradient descent training, and the computational depth of neural networks. We also investigate whether over- or under-parametrization affects the weight space. Our model shows that under- and over-parameterization weight spaces are well-connected without hierarchical clustering structure, while shallow networks have a broken weight space characterized by a discontinuous phase transition. This highlights the importance of depth in deep learning from a high-dimensional geometry perspective. Furthermore, our model reveals the existence of a liquid-like central part inside a deep network, where weights behave randomly, providing algorithmic implications. Overall, our data-driven model provides statistical mechanics insights into why deep learning is highly effective in terms of the weight space and how deep networks differ from shallow ones.",1
"Despite the remarkable success of deep learning, optimal convolution operation on point cloud remains indefinite due to its irregular data structure. In this paper, we present Cubic Kernel Convolution (CKConv) that learns to voxelize the features of local points by exploiting both continuous and discrete convolutions. Our continuous convolution uniquely employs a 3D cubic form of kernel weight representation that splits a feature into voxels in embedding space. By consecutively applying discrete 3D convolutions on the voxelized features in a spatial manner, preceding continuous convolution is forced to learn spatial feature mapping, i.e., feature voxelization. In this way, geometric information can be detailed by encoding with subdivided features, and our 3D convolutions on these fixed structured data do not suffer from discretization artifacts thanks to voxelization in embedding space. Furthermore, we propose a spatial attention module, Local Set Attention (LSA), to provide comprehensive structure awareness within the local point set and hence produce representative features. By learning feature voxelization with LSA, CKConv can extract enriched features for effective point cloud analysis. We show that CKConv has great applicability to point cloud processing tasks including object classification, object part segmentation, and scene semantic segmentation with state-of-the-art results.",0
"The irregular data structure of point cloud makes optimal convolution operation challenging despite the success of deep learning. This paper introduces Cubic Kernel Convolution (CKConv), which combines continuous and discrete convolutions to voxelize local point features. The continuous convolution uses a 3D cubic form of kernel weight representation to split features into voxels in embedding space, which are then subjected to spatial discrete 3D convolutions to learn spatial feature mapping. This approach encodes geometric information into subdivided features and avoids discretization artifacts in 3D convolutions. Additionally, a spatial attention module, Local Set Attention (LSA), is proposed to enhance structure awareness and produce representative features. CKConv demonstrates superior performance on object classification, object part segmentation, and scene semantic segmentation tasks.",1
"Graph Neural Networks (GNNs) have exploded onto the machine learning scene in recent years owing to their capability to model and learn from graph-structured data. Such an ability has strong implications in a wide variety of fields whose data is inherently relational, for which conventional neural networks do not perform well. Indeed, as recent reviews can attest, research in the area of GNNs has grown rapidly and has lead to the development of a variety of GNN algorithm variants as well as to the exploration of groundbreaking applications in chemistry, neurology, electronics, or communication networks, among others. At the current stage of research, however, the efficient processing of GNNs is still an open challenge for several reasons. Besides of their novelty, GNNs are hard to compute due to their dependence on the input graph, their combination of dense and very sparse operations, or the need to scale to huge graphs in some applications. In this context, this paper aims to make two main contributions. On the one hand, a review of the field of GNNs is presented from the perspective of computing. This includes a brief tutorial on the GNN fundamentals, an overview of the evolution of the field in the last decade, and a summary of operations carried out in the multiple phases of different GNN algorithm variants. On the other hand, an in-depth analysis of current software and hardware acceleration schemes is provided, from which a hardware-software, graph-aware, and communication-centric vision for GNN accelerators is distilled.",0
"Recent years have seen a surge in the popularity of Graph Neural Networks (GNNs) due to their ability to model and learn from graph-structured data, which is particularly relevant in fields where conventional neural networks struggle. As a result, research in GNNs has grown rapidly, leading to the development of various algorithmic variants and groundbreaking applications in fields such as chemistry, neurology, and communication networks. However, the efficient processing of GNNs remains a challenge due to their dependence on input graphs, a combination of dense and sparse operations, and the need to scale to huge graphs. This paper aims to contribute to the field of GNNs in two ways: first, by reviewing the field from a computing perspective, including a tutorial on GNN fundamentals, an overview of the field's evolution, and a summary of operations in different algorithm variants; second, by analyzing current software and hardware acceleration schemes and proposing a graph-aware, communication-centric, and hardware-software approach for GNN accelerators.",1
"Graph neural networks (GNNs) have achieved remarkable success as a framework for deep learning on graph-structured data. However, GNNs are fundamentally limited by their tree-structured inductive bias: the WL-subtree kernel formulation bounds the representational capacity of GNNs, and polynomial-time GNNs are provably incapable of recognizing triangles in a graph. In this work, we propose to augment the GNN message-passing operations with information defined on ego graphs (i.e., the induced subgraph surrounding each node). We term these approaches Ego-GNNs and show that Ego-GNNs are provably more powerful than standard message-passing GNNs. In particular, we show that Ego-GNNs are capable of recognizing closed triangles, which is essential given the prominence of transitivity in real-world graphs. We also motivate our approach from the perspective of graph signal processing as a form of multiplex graph convolution. Experimental results on node classification using synthetic and real data highlight the achievable performance gains using this approach.",0
"The use of graph neural networks (GNNs) has been highly successful for deep learning on graph-structured data. However, the tree-structured inductive bias of GNNs is a fundamental limitation, as the WL-subtree kernel formulation restricts their representational capacity, and GNNs with polynomial-time are unable to identify triangles in a graph. To address this issue, we suggest enhancing GNN message-passing operations by including information defined on ego graphs (i.e., the subgraph surrounding each node). Our proposed approach, called Ego-GNNs, is demonstrated to be more powerful than standard message-passing GNNs, as it can recognize closed triangles, which is crucial due to transitivity's importance in real-world graphs. We also explain our approach's rationale from the perspective of graph signal processing as a type of multiplex graph convolution. Experimental results using synthetic and real data for node classification highlight the substantial performance gains achievable with our approach.",1
"Graph neural networks (GNNs) is widely used to learn a powerful representation of graph-structured data. Recent work demonstrates that transferring knowledge from self-supervised tasks to downstream tasks could further improve graph representation. However, there is an inherent gap between self-supervised tasks and downstream tasks in terms of optimization objective and training data. Conventional pre-training methods may be not effective enough on knowledge transfer since they do not make any adaptation for downstream tasks. To solve such problems, we propose a new transfer learning paradigm on GNNs which could effectively leverage self-supervised tasks as auxiliary tasks to help the target task. Our methods would adaptively select and combine different auxiliary tasks with the target task in the fine-tuning stage. We design an adaptive auxiliary loss weighting model to learn the weights of auxiliary tasks by quantifying the consistency between auxiliary tasks and the target task. In addition, we learn the weighting model through meta-learning. Our methods can be applied to various transfer learning approaches, it performs well not only in multi-task learning but also in pre-training and fine-tuning. Comprehensive experiments on multiple downstream tasks demonstrate that the proposed methods can effectively combine auxiliary tasks with the target task and significantly improve the performance compared to state-of-the-art methods.",0
"The use of Graph Neural Networks (GNNs) is prevalent in the creation of potent representations for graph-structured data. It has been shown recently that transferring knowledge from self-supervised tasks to downstream tasks could further enhance the graph representation. However, there exists a gap between the optimization objective and training data of self-supervised tasks and downstream tasks. Traditional pre-training methods may not be very effective in knowledge transfer since they do not adapt for downstream tasks. To address these issues, we present a new transfer learning paradigm for GNNs that uses self-supervised tasks as auxiliary tasks to aid in the target task. Our approach selects and combines different auxiliary tasks with the target task in the fine-tuning stage, using an adaptive auxiliary loss weighting model that learns the weights of auxiliary tasks by measuring the consistency between auxiliary tasks and the target task. Furthermore, we learn the weighting model through meta-learning. Our methods are applicable to various transfer learning approaches and perform well in multi-task learning, pre-training, and fine-tuning. We conducted comprehensive experiments on multiple downstream tasks, which showed that our proposed methods can effectively combine auxiliary tasks with the target task and substantially improve performance compared to state-of-the-art methods.",1
"Graph representation learning plays a vital role in processing graph-structured data. However, prior arts on graph representation learning heavily rely on labeling information. To overcome this problem, inspired by the recent success of graph contrastive learning and Siamese networks in visual representation learning, we propose a novel self-supervised approach in this paper to learn node representations by enhancing Siamese self-distillation with multi-scale contrastive learning. Specifically, we first generate two augmented views from the input graph based on local and global perspectives. Then, we employ two objectives called cross-view and cross-network contrastiveness to maximize the agreement between node representations across different views and networks. To demonstrate the effectiveness of our approach, we perform empirical experiments on five real-world datasets. Our method not only achieves new state-of-the-art results but also surpasses some semi-supervised counterparts by large margins. Code is made available at https://github.com/GRAND-Lab/MERIT",0
"The processing of graph-structured data heavily relies on graph representation learning. However, previous methods have been limited by their dependence on labeling information. To address this issue, we have developed a self-supervised approach that combines graph contrastive learning and Siamese networks to learn node representations. Our method involves generating two augmented views of the input graph from local and global perspectives, and then using cross-view and cross-network contrastiveness objectives to maximize agreement between node representations across different views and networks. We have conducted empirical experiments on five real-world datasets and achieved new state-of-the-art results, outperforming some semi-supervised methods by significant margins. The code for our approach is available at https://github.com/GRAND-Lab/MERIT.",1
"Recent achievements in end-to-end deep learning have encouraged the exploration of tasks dealing with highly structured data with unified deep network models. Having such models for compressing audio signals has been challenging since it requires discrete representations that are not easy to train with end-to-end backpropagation. In this paper, we present an end-to-end deep learning approach that combines recurrent neural networks (RNNs) within the training strategy of variational autoencoders (VAEs) with a binary representation of the latent space. We apply a reparametrization trick for the Bernoulli distribution for the discrete representations, which allows smooth backpropagation. In addition, our approach allows the separation of the encoder and decoder, which is necessary for compression tasks. To our best knowledge, this is the first end-to-end learning for a single audio compression model with RNNs, and our model achieves a Signal to Distortion Ratio (SDR) of 20.54.",0
"The recent success of end-to-end deep learning has prompted researchers to explore the use of unified deep network models for tasks involving highly structured data. However, developing such models for compressing audio signals has proven to be challenging due to the need for discrete representations that are difficult to train with end-to-end backpropagation. To address this issue, our paper proposes an end-to-end deep learning approach that combines recurrent neural networks (RNNs) with variational autoencoders (VAEs) and a binary representation of the latent space. We use a reparametrization trick for the Bernoulli distribution to enable smooth backpropagation for the discrete representations. Our approach also allows for the separation of the encoder and decoder, which is essential for compression tasks. Notably, our method represents the first end-to-end learning for a single audio compression model using RNNs. With our proposed model, we achieved a Signal to Distortion Ratio (SDR) of 20.54.",1
"Graph convolutional networks are becoming indispensable for deep learning from graph-structured data. Most of the existing graph convolutional networks share two big shortcomings. First, they are essentially low-pass filters, thus the potentially useful middle and high frequency band of graph signals are ignored. Second, the bandwidth of existing graph convolutional filters is fixed. Parameters of a graph convolutional filter only transform the graph inputs without changing the curvature of a graph convolutional filter function. In reality, we are uncertain about whether we should retain or cut off the frequency at a certain point unless we have expert domain knowledge. In this paper, we propose Automatic Graph Convolutional Networks (AutoGCN) to capture the full spectrum of graph signals and automatically update the bandwidth of graph convolutional filters. While it is based on graph spectral theory, our AutoGCN is also localized in space and has a spatial form. Experimental results show that AutoGCN achieves significant improvement over baseline methods which only work as low-pass filters.",0
"Graph convolutional networks are necessary for deep learning from data with graph structure. However, there are two major drawbacks to most existing graph convolutional networks. Firstly, they are limited to low-pass filters, meaning that they ignore the potentially valuable middle and high frequency ranges of graph signals. Secondly, the bandwidth of current graph convolutional filters remains fixed, which means that the curvature of a graph convolutional filter function does not change, and we cannot determine whether we should retain or cut off the frequency at a certain point without expert domain knowledge. In this article, we present Automatic Graph Convolutional Networks (AutoGCN) to address these issues by automatically adjusting the bandwidth of graph convolutional filters to capture the entire spectrum of graph signals. Although AutoGCN relies on graph spectral theory, it is also spatially localized and has a spatial structure. Experimental findings demonstrate that AutoGCN outperforms baseline methods that only function as low-pass filters.",1
"Multi-relational graph is a ubiquitous and important data structure, allowing flexible representation of multiple types of interactions and relations between entities. Similar to other graph-structured data, link prediction is one of the most important tasks on multi-relational graphs and is often used for knowledge completion. When related graphs coexist, it is of great benefit to build a larger graph via integrating the smaller ones. The integration requires predicting hidden relational connections between entities belonged to different graphs (inter-domain link prediction). However, this poses a real challenge to existing methods that are exclusively designed for link prediction between entities of the same graph only (intra-domain link prediction). In this study, we propose a new approach to tackle the inter-domain link prediction problem by softly aligning the entity distributions between different domains with optimal transport and maximum mean discrepancy regularizers. Experiments on real-world datasets show that optimal transport regularizer is beneficial and considerably improves the performance of baseline methods.",0
"The multi-relational graph is a crucial data structure that offers a flexible representation of various interactions and relationships between entities. One of the primary tasks on multi-relational graphs is link prediction, which is used for knowledge completion. When multiple related graphs are present, it is advantageous to integrate them into a larger graph. However, predicting hidden relational connections between entities that belong to different graphs (inter-domain link prediction) is a major challenge for existing methods that are designed for intra-domain link prediction. To address this issue, we propose a novel approach that utilizes optimal transport and maximum mean discrepancy regularizers to softly align entity distributions between different domains. Our experiments on real-world datasets demonstrate that the optimal transport regularizer improves the performance of baseline methods significantly.",1
"Graph representation learning has attracted a surge of interest recently, whose target at learning discriminant embedding for each node in the graph. Most of these representation methods focus on supervised learning and heavily depend on label information. However, annotating graphs are expensive to obtain in the real world, especially in specialized domains (i.e. biology), as it needs the annotator to have the domain knowledge to label the graph. To approach this problem, self-supervised learning provides a feasible solution for graph representation learning. In this paper, we propose a Multi-Level Graph Contrastive Learning (MLGCL) framework for learning robust representation of graph data by contrasting space views of graphs. Specifically, we introduce a novel contrastive view - topological and feature space views. The original graph is first-order approximation structure and contains uncertainty or error, while the $k$NN graph generated by encoding features preserves high-order proximity. Thus $k$NN graph generated by encoding features not only provide a complementary view, but is more suitable to GNN encoder to extract discriminant representation. Furthermore, we develop a multi-level contrastive mode to preserve the local similarity and semantic similarity of graph-structured data simultaneously. Extensive experiments indicate MLGCL achieves promising results compared with the existing state-of-the-art graph representation learning methods on seven datasets.",0
"Recently, there has been a growing interest in graph representation learning, with the goal of learning discriminative embeddings for each node in the graph. However, most existing representation methods rely heavily on supervised learning and require costly labeling of the graph. This is particularly problematic in specialized domains like biology, where annotators must possess domain knowledge to label the graph. To address this issue, self-supervised learning presents a viable solution for graph representation learning. In this study, we propose a Multi-Level Graph Contrastive Learning (MLGCL) framework that learns robust representations of graph data by contrasting different views of the graph. Specifically, we introduce a novel contrastive view that combines topological and feature space views. While the original graph contains errors and uncertainties, the kNN graph generated by encoding features preserves high-order proximity and provides a complementary view that is more suitable for GNN encoder to extract discriminative representations. Furthermore, we develop a multi-level contrastive mode that preserves both local and semantic similarities of graph-structured data. Our experiments demonstrate that MLGCL outperforms existing state-of-the-art graph representation learning methods on seven datasets.",1
"Various non-trivial spaces are becoming popular for embedding structured data such as graphs, texts, or images. Following spherical and hyperbolic spaces, more general product spaces have been proposed. However, searching for the best configuration of product space is a resource-intensive procedure, which reduces the practical applicability of the idea. We generalize the concept of product space and introduce an overlapping space that does not have the configuration search problem. The main idea is to allow subsets of coordinates to be shared between spaces of different types (Euclidean, hyperbolic, spherical). As a result, parameter optimization automatically learns the optimal configuration. Additionally, overlapping spaces allow for more compact representations since their geometry is more complex. Our experiments confirm that overlapping spaces outperform the competitors in graph embedding tasks. Here, we consider both distortion setup, where the aim is to preserve distances, and ranking setup, where the relative order should be preserved. The proposed method effectively solves the problem and outperforms the competitors in both settings. We also perform an empirical analysis in a realistic information retrieval task, where we compare all spaces by incorporating them into DSSM. In this case, the proposed overlapping space consistently achieves nearly optimal results without any configuration tuning. This allows for reducing training time, which can be significant in large-scale applications.",0
"Structured data such as graphs, texts, and images are increasingly being embedded in non-trivial spaces. While spherical and hyperbolic spaces have been proposed, finding the best configuration for product spaces is time-consuming and limits practicality. To solve this, we propose an overlapping space that allows subsets of coordinates to be shared between different space types, enabling parameter optimization to learn the optimal configuration automatically. Overlapping spaces also offer more compact representations due to their complex geometry. Our experiments show that overlapping spaces outperform competitors in both distortion and ranking setups for graph embedding tasks. We also compare all spaces in a realistic information retrieval task and find that the proposed overlapping space consistently achieves nearly optimal results without any configuration tuning, reducing training time for large-scale applications.",1
"Relational databases are the de facto standard for storing and querying structured data, and extracting insights from structured data requires advanced analytics. Deep neural networks (DNNs) have achieved super-human prediction performance in particular data types, e.g., images. However, existing DNNs may not produce meaningful results when applied to structured data. The reason is that there are correlations and dependencies across combinations of attribute values in a table, and these do not follow simple additive patterns that can be easily mimicked by a DNN. The number of possible such cross features is combinatorial, making them computationally prohibitive to model. Furthermore, the deployment of learning models in real-world applications has also highlighted the need for interpretability, especially for high-stakes applications, which remains another issue of concern to DNNs.   In this paper, we present ARM-Net, an adaptive relation modeling network tailored for structured data, and a lightweight framework ARMOR based on ARM-Net for relational data analytics. The key idea is to model feature interactions with cross features selectively and dynamically, by first transforming the input features into exponential space, and then determining the interaction order and interaction weights adaptively for each cross feature. We propose a novel sparse attention mechanism to dynamically generate the interaction weights given the input tuple, so that we can explicitly model cross features of arbitrary orders with noisy features filtered selectively. Then during model inference, ARM-Net can specify the cross features being used for each prediction for higher accuracy and better interpretability. Our extensive experiments on real-world datasets demonstrate that ARM-Net consistently outperforms existing models and provides more interpretable predictions for data-driven decision making.",0
"Storing and analyzing structured data is commonly done through relational databases, which also require advanced analytics to extract insights. While deep neural networks have achieved impressive results in specific data types, such as images, they may not be suitable for structured data due to complex correlations and dependencies. These dependencies do not follow simple patterns that can be easily replicated by a DNN, making it computationally challenging to model. There is also a need for interpretability, especially for high-stakes applications, which remains a concern for DNNs. This paper introduces ARM-Net, an adaptive relation modeling network, and a lightweight framework called ARMOR that is designed for relational data analytics. ARM-Net models feature interactions selectively and dynamically by transforming input features into exponential space and determining interaction weights adaptively. This allows for explicit modeling of cross-features of any order and selective filtering of noisy features. During model inference, ARM-Net specifies the cross-features used for each prediction, resulting in higher accuracy and better interpretability. Extensive experiments on real-world datasets demonstrate that ARM-Net outperforms existing models and provides more interpretable predictions for data-driven decision making.",1
"We propose a model for hierarchical structured data as an extension to the stochastic temporal convolutional network. The proposed model combines an autoregressive model with a hierarchical variational autoencoder and downsampling to achieve superior computational complexity. We evaluate the proposed model on two different types of sequential data: speech and handwritten text. The results are promising with the proposed model achieving state-of-the-art performance.",0
"Our proposed model, which extends the stochastic temporal convolutional network, introduces a hierarchical structure for data organization. The model integrates an autoregressive model with a hierarchical variational autoencoder and downsampling, resulting in improved computational complexity. We assess the effectiveness of our model on speech and handwritten text data and the outcomes indicate that our approach outperforms existing methods, making it state-of-the-art.",1
"Graph neural networks have recently achieved remarkable success in representing graph-structured data, with rapid progress in both the node embedding and graph pooling methods. Yet, they mostly focus on capturing information from the nodes considering their connectivity, and not much work has been done in representing the edges, which are essential components of a graph. However, for tasks such as graph reconstruction and generation, as well as graph classification tasks for which the edges are important for discrimination, accurately representing edges of a given graph is crucial to the success of the graph representation learning. To this end, we propose a novel edge representation learning framework based on Dual Hypergraph Transformation (DHT), which transforms the edges of a graph into the nodes of a hypergraph. This dual hypergraph construction allows us to apply message passing techniques for node representations to edges. After obtaining edge representations from the hypergraphs, we then cluster or drop edges to obtain holistic graph-level edge representations. We validate our edge representation learning method with hypergraphs on diverse graph datasets for graph representation and generation performance, on which our method largely outperforms existing graph representation learning methods. Moreover, our edge representation learning and pooling method also largely outperforms state-of-the-art graph pooling methods on graph classification, not only because of its accurate edge representation learning, but also due to its lossless compression of the nodes and removal of irrelevant edges for effective message passing.",0
"Recent advancements in graph neural networks have been successful in representing graph-structured data through node embedding and graph pooling methods. However, these methods primarily focus on capturing information from the nodes and overlook the importance of accurately representing edges. In tasks such as graph classification and reconstruction, edge representation is crucial for successful graph representation learning. In response, we introduce a novel edge representation learning framework, Dual Hypergraph Transformation (DHT), which transforms graph edges into hypergraph nodes. This allows for message passing techniques to be applied to edges. Our method outperforms existing graph representation learning methods and state-of-the-art graph pooling methods due to its accurate edge representation learning and lossless compression of nodes.",1
"In the world where big data reigns and there is plenty of hardware prepared to gather a huge amount of non structured data, data acquisition is no longer a problem. Surveillance cameras are ubiquitous and they capture huge numbers of people walking across different scenes. However, extracting value from this data is challenging, specially for tasks that involve human images, such as face recognition and person re-identification. Annotation of this kind of data is a challenging and expensive task. In this work we propose a domain adaptation workflow to allow CNNs that were trained in one domain to be applied to another domain without the need for new annotation of the target data. Our method uses AlignedReID++ as the baseline, trained using a Triplet loss with batch hard. Domain adaptation is done by using pseudo-labels generated using an unsupervised learning strategy. Our results show that domain adaptation techniques really improve the performance of the CNN when applied in the target domain.",0
"Acquiring a vast amount of non-structured data is no longer a problem in a world where big data rules and there is ample hardware available for collecting it. Surveillance cameras are prevalent and capture numerous individuals traversing various areas. However, extracting value from this data is difficult, particularly for tasks involving human images, such as person re-identification and face recognition, as annotating this data is an arduous and costly process. This study proposes a domain adaptation workflow that enables CNNs trained in one domain to be used in another domain without requiring new annotation of the target data. Our approach employs AlignedReID++ as the baseline, trained using a Triplet loss with batch hard, and utilizes pseudo-labels generated using an unsupervised learning strategy for domain adaptation. Our findings demonstrate that domain adaptation techniques can significantly improve CNN performance in the target domain.",1
"In representation learning on the graph-structured data, under heterophily (or low homophily), many popular GNNs may fail to capture long-range dependencies, which leads to their performance degradation. To solve the above-mentioned issue, we propose a graph convolutional networks with structure learning (GCN-SL), and furthermore, the proposed approach can be applied to node classification. The proposed GCN-SL contains two improvements: corresponding to node features and edges, respectively. In the aspect of node features, we propose an efficient-spectral-clustering (ESC) and an ESC with anchors (ESC-ANCH) algorithms to efficiently aggregate feature representations from all similar nodes. In the aspect of edges, we build a re-connected adjacency matrix by using a special data preprocessing technique and similarity learning, and the re-connected adjacency matrix can be optimized directly along with GCN-SL parameters. Considering that the original adjacency matrix may provide misleading information for aggregation in GCN, especially the graphs being with a low level of homophily. The proposed GCN-SL can aggregate feature representations from nearby nodes via re-connected adjacency matrix and is applied to graphs with various levels of homophily. Experimental results on a wide range of benchmark datasets illustrate that the proposed GCN-SL outperforms the stateof-the-art GNN counterparts.",0
"Under conditions of heterophily (or low homophily) in representation learning on graph-structured data, the performance of many popular GNNs may suffer due to their inability to capture long-range dependencies. To address this issue, we propose a graph convolutional network with structure learning (GCN-SL), which can be applied to node classification. Our approach involves two improvements for node features and edges, respectively. For node features, we introduce two efficient-spectral-clustering algorithms (ESC and ESC-ANCH) to aggregate feature representations from similar nodes. For edges, we use a special data preprocessing technique and similarity learning to create a re-connected adjacency matrix that can be optimized alongside GCN-SL parameters. This approach allows for the aggregation of feature representations from nearby nodes and can be applied to graphs with varying levels of homophily. Our experimental results on benchmark datasets demonstrate that GCN-SL outperforms state-of-the-art GNNs.",1
"Self-supervised learning on graph-structured data has drawn recent interest for learning generalizable, transferable and robust representations from unlabeled graphs. Among many, graph contrastive learning (GraphCL) has emerged with promising representation learning performance. Unfortunately, unlike its counterpart on image data, the effectiveness of GraphCL hinges on ad-hoc data augmentations, which have to be manually picked per dataset, by either rules of thumb or trial-and-errors, owing to the diverse nature of graph data. That significantly limits the more general applicability of GraphCL. Aiming to fill in this crucial gap, this paper proposes a unified bi-level optimization framework to automatically, adaptively and dynamically select data augmentations when performing GraphCL on specific graph data. The general framework, dubbed JOint Augmentation Optimization (JOAO), is instantiated as min-max optimization. The selections of augmentations made by JOAO are shown to be in general aligned with previous ""best practices"" observed from handcrafted tuning: yet now being automated, more flexible and versatile. Moreover, we propose a new augmentation-aware projection head mechanism, which will route output features through different projection heads corresponding to different augmentations chosen at each training step. Extensive experiments demonstrate that JOAO performs on par with or sometimes better than the state-of-the-art competitors including GraphCL, on multiple graph datasets of various scales and types, yet without resorting to any laborious dataset-specific tuning on augmentation selection. We release the code at https://github.com/Shen-Lab/GraphCL_Automated.",0
"Recently, there has been growing interest in self-supervised learning on graph-structured data in order to create generalizable, transferable, and robust representations from unlabelled graphs. Graph contrastive learning (GraphCL) has emerged as a promising technique for representation learning. However, unlike its counterpart for image data, GraphCL's effectiveness relies on ad-hoc data augmentations which must be manually selected per dataset through rules of thumb or trial-and-error due to the diverse nature of graph data. This restricts the general applicability of GraphCL. To address this issue, this paper proposes a unified bi-level optimization framework called JOint Augmentation Optimization (JOAO) which automatically, adaptively, and dynamically selects data augmentations for GraphCL on specific graph data. JOAO is instantiated as min-max optimization and the augmentations selected by JOAO are generally aligned with previous best practices observed through manual tuning. Additionally, the paper proposes a new augmentation-aware projection head mechanism which routes output features through different projection heads corresponding to different augmentations chosen at each training step. Extensive experiments demonstrate that JOAO performs on a par with or better than state-of-the-art competitors including GraphCL on multiple graph datasets of varying scales and types, without requiring laborious dataset-specific tuning for augmentation selection. The code is available at https://github.com/Shen-Lab/GraphCL_Automated.",1
"Data augmentation is a key element of deep learning pipelines, as it informs the network during training about transformations of the input data that keep the label unchanged. Manually finding adequate augmentation methods and parameters for a given pipeline is however rapidly cumbersome. In particular, while intuition can guide this decision for images, the design and choice of augmentation policies remains unclear for more complex types of data, such as neuroscience signals. Moreover, label independent strategies might not be suitable for such structured data and class-dependent augmentations might be necessary. This idea has been surprisingly unexplored in the literature, while it is quite intuitive: changing the color of a car image does not change the object class to be predicted, but doing the same to the picture of an orange does. This paper aims to increase the generalization power added through class-wise data augmentation. Yet, as seeking transformations depending on the class largely increases the complexity of the task, using gradient-free optimization techniques as done by most existing automatic approaches becomes intractable for real-world datasets. For this reason we propose to use differentiable data augmentation amenable to gradient-based learning. EEG signals are a perfect example of data for which good augmentation policies are mostly unknown. In this work, we demonstrate the relevance of our approach on the clinically relevant sleep staging classification task, for which we also propose differentiable transformations.",0
"In deep learning pipelines, data augmentation plays a crucial role in training the network by exposing it to various transformations of the input data that do not affect the label. However, manually selecting appropriate augmentation methods and parameters for a given pipeline can become tedious. While intuition can guide the selection process for images, it is unclear how to design and choose augmentation policies for more complex data types, such as neuroscience signals. Furthermore, label independent strategies may not be effective for structured data, and class-dependent augmentations may be necessary. Despite the intuitive nature of this idea, it has not been thoroughly explored in literature. This paper proposes a method to enhance the generalization power of deep learning models by introducing class-wise data augmentation. However, class-dependent transformations add to the complexity of the task, making gradient-free optimization techniques impractical for real-world datasets. In this regard, we propose a differentiable data augmentation method that is compatible with gradient-based learning. EEG signals are an example of data that lack effective augmentation policies. In this study, we demonstrate the importance of our approach on the clinically relevant sleep staging classification task, and we also propose differentiable transformations for this task.",1
"Temporal graph signals are multivariate time series with individual components associated with nodes of a fixed graph structure. Data of this kind arises in many domains including activity of social network users, sensor network readings over time, and time course gene expression within the interaction network of a model organism. Traditional matrix decomposition methods applied to such data fall short of exploiting structural regularities encoded in the underlying graph and also in the temporal patterns of the signal. How can we take into account such structure to obtain a succinct and interpretable representation of temporal graph signals?   We propose a general, dictionary-based framework for temporal graph signal decomposition (TGSD). The key idea is to learn a low-rank, joint encoding of the data via a combination of graph and time dictionaries. We propose a highly scalable decomposition algorithm for both complete and incomplete data, and demonstrate its advantage for matrix decomposition, imputation of missing values, temporal interpolation, clustering, period estimation, and rank estimation in synthetic and real-world data ranging from traffic patterns to social media activity. Our framework achieves 28% reduction in RMSE compared to baselines for temporal interpolation when as many as 75% of the observations are missing. It scales best among baselines taking under 20 seconds on 3.5 million data points and produces the most parsimonious models. To the best of our knowledge, TGSD is the first framework to jointly model graph signals by temporal and graph dictionaries.",0
"Temporal graph signals refer to multivariate time series that are associated with individual components linked to nodes within a fixed graph structure. This type of data is prevalent in various fields, including social network user activity, time course gene expression, and sensor network readings over time. Traditional matrix decomposition methods applied to such data fail to utilize the structural regularities inherent in the underlying graph and temporal patterns of the signal. To address this issue, we propose a general framework for temporal graph signal decomposition (TGSD) that utilizes a combination of graph and time dictionaries to learn a low-rank, joint encoding of the data. Our highly scalable decomposition algorithm is effective for both complete and incomplete data, providing benefits such as matrix decomposition, imputation of missing values, temporal interpolation, clustering, period estimation, and rank estimation in synthetic and real-world data. Our framework outperforms baselines, achieving a 28% reduction in RMSE for temporal interpolation when 75% of observations are missing. It is the first framework to jointly model graph signals using temporal and graph dictionaries.",1
"We introduce the Graph Mixture Density Networks, a new family of machine learning models that can fit multimodal output distributions conditioned on graphs of arbitrary topology. By combining ideas from mixture models and graph representation learning, we address a broader class of challenging conditional density estimation problems that rely on structured data. In this respect, we evaluate our method on a new benchmark application that leverages random graphs for stochastic epidemic simulations. We show a significant improvement in the likelihood of epidemic outcomes when taking into account both multimodality and structure. The empirical analysis is complemented by two real-world regression tasks showing the effectiveness of our approach in modeling the output prediction uncertainty. Graph Mixture Density Networks open appealing research opportunities in the study of structure-dependent phenomena that exhibit non-trivial conditional output distributions.",0
"Introducing the Graph Mixture Density Networks, a novel set of machine learning models capable of fitting multimodal output distributions that are dependent on graphs of any topology. By merging concepts from mixture models and graph representation learning, we tackle a wider range of complex conditional density estimation problems that rely on structured data. We assess our approach on a new benchmark application that employs random graphs for stochastic epidemic simulations, demonstrating a significant improvement in the likelihood of epidemic outcomes by considering both multimodality and structure. Additionally, we conduct two real-world regression tasks that showcase the effectiveness of our strategy in modeling output prediction uncertainty. The Graph Mixture Density Networks present exciting research prospects for investigating structure-dependent phenomena with intricate conditional output distributions.",1
"Hypergraphs are used to model higher-order interactions amongst agents and there exist many practically relevant instances of hypergraph datasets. To enable efficient processing of hypergraph-structured data, several hypergraph neural network platforms have been proposed for learning hypergraph properties and structure, with a special focus on node classification. However, almost all existing methods use heuristic propagation rules and offer suboptimal performance on many datasets. We propose AllSet, a new hypergraph neural network paradigm that represents a highly general framework for (hyper)graph neural networks and for the first time implements hypergraph neural network layers as compositions of two multiset functions that can be efficiently learned for each task and each dataset. Furthermore, AllSet draws on new connections between hypergraph neural networks and recent advances in deep learning of multiset functions. In particular, the proposed architecture utilizes Deep Sets and Set Transformer architectures that allow for significant modeling flexibility and offer high expressive power. To evaluate the performance of AllSet, we conduct the most extensive experiments to date involving ten known benchmarking datasets and three newly curated datasets that represent significant challenges for hypergraph node classification. The results demonstrate that AllSet has the unique ability to consistently either match or outperform all other hypergraph neural networks across the tested datasets. Our implementation and dataset will be released upon acceptance.",0
"Hypergraphs are commonly used to represent complex relationships between agents, and there are many practical examples of hypergraph datasets. However, processing hypergraph-structured data efficiently is a challenge, and existing hypergraph neural network platforms often utilize heuristic propagation rules that result in suboptimal performance on many datasets, particularly in node classification. To address this issue, we propose a new hypergraph neural network paradigm called AllSet, which utilizes a highly general framework that implements hypergraph neural network layers as combinations of two multiset functions. This approach allows for efficient learning on various tasks and datasets, drawing on recent advances in deep learning of multiset functions. Our evaluation involves extensive experiments on ten benchmarking datasets and three newly curated datasets, which demonstrate that AllSet consistently outperforms existing hypergraph neural networks. Our implementation and dataset will be made available upon acceptance.",1
"Deep neural networks, while generalize well, are known to be sensitive to small adversarial perturbations. This phenomenon poses severe security threat and calls for in-depth investigation of the robustness of deep learning models. With the emergence of neural networks for graph structured data, similar investigations are urged to understand their robustness. It has been found that adversarially perturbing the graph structure and/or node features may result in a significant degradation of the model performance. In this work, we show from a different angle that such fragility similarly occurs if the graph contains a few bad-actor nodes, which compromise a trained graph neural network through flipping the connections to any targeted victim. Worse, the bad actors found for one graph model severely compromise other models as well. We call the bad actors ``anchor nodes'' and propose an algorithm, named GUA, to identify them. Thorough empirical investigations suggest an interesting finding that the anchor nodes often belong to the same class; and they also corroborate the intuitive trade-off between the number of anchor nodes and the attack success rate. For the dataset Cora which contains 2708 nodes, as few as six anchor nodes will result in an attack success rate higher than 80\% for GCN and other three models.",0
"Although deep neural networks are known to generalize well, they are also vulnerable to small adversarial perturbations, which can pose a severe security threat. This has led to a need for a thorough investigation of the robustness of deep learning models. Furthermore, with the emergence of neural networks for graph structured data, there is a similar need to understand their robustness. Recent studies have shown that perturbing the graph structure or node features can significantly degrade model performance. However, in this study, we explore a different perspective and demonstrate that a few ""bad-actor"" nodes in the graph can also compromise a trained graph neural network by flipping connections to targeted victims. These nodes, which we call ""anchor nodes,"" severely compromise other models as well. To identify them, we propose an algorithm called GUA. Empirical investigations reveal that anchor nodes often belong to the same class and there is a trade-off between the number of anchor nodes and the attack success rate. For example, in the Cora dataset containing 2708 nodes, as few as six anchor nodes can result in an attack success rate higher than 80\% for GCN and other three models.",1
"Graph Neural Networks (GNNs) have been extensively used for mining graph-structured data with impressive performance. However, traditional GNNs suffer from over-smoothing, non-robustness and over-fitting problems. To solve these weaknesses, we design a novel GNN solution, namely Graph Attention Network with LSTM-based Path Reweighting (PR-GAT). PR-GAT can automatically aggregate multi-hop information, highlight important paths and filter out noises. In addition, we utilize random path sampling in PR-GAT for data augmentation. The augmented data is used for predicting the distribution of corresponding labels. Finally, we demonstrate that PR-GAT can mitigate the issues of over-smoothing, non-robustness and overfitting. We achieve state-of-the-art accuracy on 5 out of 7 datasets and competitive accuracy for other 2 datasets. The average accuracy of 7 datasets have been improved by 0.5\% than the best SOTA from literature.",0
"Graph Neural Networks (GNNs) have been widely employed to extract information from graph-structured data with remarkable outcomes. Nevertheless, conventional GNNs face setbacks such as over-smoothing, non-robustness, and over-fitting. To overcome these limitations, we have devised a new GNN method called Graph Attention Network with LSTM-based Path Reweighting (PR-GAT). PR-GAT has the ability to automatically gather multi-hop data, highlight crucial pathways, and filter out irrelevant information. Additionally, we have implemented random path sampling in PR-GAT to augment the data, which is then used to predict the corresponding label's distribution. Ultimately, we have demonstrated that PR-GAT can alleviate the problems of over-smoothing, non-robustness, and over-fitting. On five out of seven datasets, we have achieved state-of-the-art accuracy, and for the other two datasets, we have obtained competitive accuracy. The average accuracy of all seven datasets has been increased by 0.5% compared to the best existing approach in the literature.",1
"This paper presents a new approach for assembling graph neural networks based on framelet transforms. The latter provides a multi-scale representation for graph-structured data. We decompose an input graph into low-pass and high-pass frequencies coefficients for network training, which then defines a framelet-based graph convolution. The framelet decomposition naturally induces a graph pooling strategy by aggregating the graph feature into low-pass and high-pass spectra, which considers both the feature values and geometry of the graph data and conserves the total information. The graph neural networks with the proposed framelet convolution and pooling achieve state-of-the-art performance in many node and graph prediction tasks. Moreover, we propose shrinkage as a new activation for the framelet convolution, which thresholds high-frequency information at different scales. Compared to ReLU, shrinkage activation improves model performance on denoising and signal compression: noises in both node and structure can be significantly reduced by accurately cutting off the high-pass coefficients from framelet decomposition, and the signal can be compressed to less than half its original size with well-preserved prediction performance.",0
"A novel method for constructing graph neural networks using framelet transforms is introduced in this paper. The approach employs a multi-scale representation of graph-structured data that decomposes an input graph into low-pass and high-pass frequencies coefficients for network training. This, in turn, defines a framelet-based graph convolution. The framelet decomposition enables the aggregation of graph features into low-pass and high-pass spectra, while preserving the total information. The resulting graph neural networks demonstrate superior performance in various node and graph prediction tasks. Additionally, a new activation function called shrinkage is proposed for the framelet convolution, which effectively removes high-frequency information at different scales. Compared to ReLU, the shrinkage activation improves model performance on denoising and signal compression, resulting in significant noise reduction and signal compression without sacrificing prediction accuracy.",1
"The Transformer architecture has become a dominant choice in many domains, such as natural language processing and computer vision. Yet, it has not achieved competitive performance on popular leaderboards of graph-level prediction compared to mainstream GNN variants. Therefore, it remains a mystery how Transformers could perform well for graph representation learning. In this paper, we solve this mystery by presenting Graphormer, which is built upon the standard Transformer architecture, and could attain excellent results on a broad range of graph representation learning tasks, especially on the recent OGB Large-Scale Challenge. Our key insight to utilizing Transformer in the graph is the necessity of effectively encoding the structural information of a graph into the model. To this end, we propose several simple yet effective structural encoding methods to help Graphormer better model graph-structured data. Besides, we mathematically characterize the expressive power of Graphormer and exhibit that with our ways of encoding the structural information of graphs, many popular GNN variants could be covered as the special cases of Graphormer.",0
"Despite its popularity in domains like natural language processing and computer vision, the Transformer architecture has not performed as well as mainstream GNN variants on popular leaderboards of graph-level prediction. This poses a mystery as to how Transformers can excel at graph representation learning. Our paper, Graphormer, solves this mystery by demonstrating how the standard Transformer architecture can attain excellent results on a variety of graph representation learning tasks, particularly the OGB Large-Scale Challenge. Our approach involves effectively encoding the structural information of a graph into the model using several simple yet effective structural encoding methods. We also mathematically characterize Graphormer's expressive power and show that our encoding methods can encompass many popular GNN variants as special cases.",1
"Graphs are versatile tools for representing structured data. As a result, a variety of machine learning methods have been studied for graph data analysis. Although many such learning methods depend on the measurement of differences between input graphs, defining an appropriate distance metric for graphs remains a controversial issue. Hence, we propose a supervised distance metric learning method for the graph classification problem. Our method, named interpretable graph metric learning (IGML), learns discriminative metrics in a subgraph-based feature space, which has a strong graph representation capability. By introducing a sparsity-inducing penalty on the weight of each subgraph, IGML can identify a small number of important subgraphs that can provide insight into the given classification task. Because our formulation has a large number of optimization variables, an efficient algorithm that uses pruning techniques based on safe screening and working set selection methods is also proposed. An important property of IGML is that solution optimality is guaranteed because the problem is formulated as a convex problem and our pruning strategies only discard unnecessary subgraphs. Furthermore, we show that IGML is also applicable to other structured data such as itemset and sequence data, and that it can incorporate vertex-label similarity by using a transportation-based subgraph feature. We empirically evaluate the computational efficiency and classification performance of IGML on several benchmark datasets and provide some illustrative examples of how IGML identifies important subgraphs from a given graph dataset.",0
"Graphs are commonly used to represent organized data and have been the subject of numerous studies in machine learning for data analysis. However, determining a suitable distance metric for graphs remains a contentious issue, particularly as many learning methods rely on measuring the differences between input graphs. To address this problem, we propose a supervised distance metric learning approach known as interpretable graph metric learning (IGML) for graph classification. IGML learns discriminative metrics in a feature space based on subgraphs, which have a strong graph representation capability. By implementing a sparsity-inducing penalty on the weight of each subgraph, IGML can identify a small number of crucial subgraphs that can offer insight into the classification task. To ensure optimality of the solution, we formulate the problem as a convex problem and use pruning strategies based on safe screening and working set selection methods. Furthermore, IGML can be applied to other structured data types such as itemset and sequence data, and can incorporate vertex-label similarity by utilizing a transportation-based subgraph feature. We evaluate the computational efficiency and classification performance of IGML on several benchmark datasets and provide examples of how IGML identifies crucial subgraphs from a given graph dataset.",1
"An outlier is an observation or a data point that is far from rest of the data points in a given dataset or we can be said that an outlier is away from the center of mass of observations. Presence of outliers can skew statistical measures and data distributions which can lead to misleading representation of the underlying data and relationships. It is seen that the removal of outliers from the training dataset before modeling can give better predictions. With the advancement of machine learning, the outlier detection models are also advancing at a good pace. The goal of this work is to highlight and compare some of the existing outlier detection techniques for the data scientists to use that information for outlier algorithm selection while building a machine learning model.",0
"In a given dataset, an outlier refers to an observation or data point that is situated far from the other data points. It can also be described as being far from the center of mass of the observations. The presence of outliers can distort statistical measures and data distributions, leading to inaccurate representations of the data and its relationships. To improve predictions, it is often beneficial to eliminate outliers from the training dataset before modeling. As machine learning continues to advance, so do the outlier detection models. The objective of this study is to compare and highlight some of the existing outlier detection techniques to help data scientists select the appropriate algorithm when building a machine learning model.",1
"The spatio-temporal graph learning is becoming an increasingly important object of graph study. Many application domains involve highly dynamic graphs where temporal information is crucial, e.g. traffic networks and financial transaction graphs. Despite the constant progress made on learning structured data, there is still a lack of effective means to extract dynamic complex features from spatio-temporal structures. Particularly, conventional models such as convolutional networks or recurrent neural networks are incapable of revealing the temporal patterns in short or long terms and exploring the spatial properties in local or global scope from spatio-temporal graphs simultaneously. To tackle this problem, we design a novel multi-scale architecture, Spatio-Temporal U-Net (ST-UNet), for graph-structured time series modeling. In this U-shaped network, a paired sampling operation is proposed in spacetime domain accordingly: the pooling (ST-Pool) coarsens the input graph in spatial from its deterministic partition while abstracts multi-resolution temporal dependencies through dilated recurrent skip connections; based on previous settings in the downsampling, the unpooling (ST-Unpool) restores the original structure of spatio-temporal graphs and resumes regular intervals within graph sequences. Experiments on spatio-temporal prediction tasks demonstrate that our model effectively captures comprehensive features in multiple scales and achieves substantial improvements over mainstream methods on several real-world datasets.",0
"The study of spatio-temporal graphs is becoming increasingly important due to their relevance in various application domains such as traffic networks and financial transaction graphs. However, there is a lack of effective methods to extract dynamic complex features from these structures using conventional models like convolutional networks or recurrent neural networks. These models cannot simultaneously reveal temporal patterns and explore spatial properties in local or global scopes. To address this issue, we present a novel multi-scale architecture, Spatio-Temporal U-Net (ST-UNet), for graph-structured time series modeling. ST-UNet includes a paired sampling operation, ST-Pool and ST-Unpool, which coarsen the input graph in spatial and abstract multi-resolution temporal dependencies through dilated recurrent skip connections. Our model demonstrates improvements over mainstream methods on several real-world datasets, effectively capturing comprehensive features in multiple scales.",1
"The Wasserstein distance provides a notion of dissimilarities between probability measures, which has recent applications in learning of structured data with varying size such as images and text documents. In this work, we analyze the $k$-nearest neighbor classifier ($k$-NN) under the Wasserstein distance and establish the universal consistency on families of distributions. Using previous known results on the consistency of the $k$-NN classifier on infinite dimensional metric spaces, it suffices to show that the families is a countable union of finite dimension sets. As a result, we show that the $k$-NN classifier is universally consistent on spaces of finitely supported measures, the space of Gaussian measures, and the space of measures with finite wavelet densities. In addition, we give a counterexample to show that the universal consistency does not hold on $\mathcal{W}_p((0,1))$.",0
"Recent applications in learning structured data of varying sizes, such as images and text documents, have utilized the Wasserstein distance to measure dissimilarities between probability measures. This study examines the $k$-nearest neighbor classifier ($k$-NN) under the Wasserstein distance and establishes its universal consistency on families of distributions. By relying on previous results regarding the consistency of the $k$-NN classifier on infinite dimensional metric spaces, it is sufficient to demonstrate that the families are a countable union of finite dimension sets. The $k$-NN classifier is shown to be universally consistent on spaces of finitely supported measures, the space of Gaussian measures, and the space of measures with finite wavelet densities. Nevertheless, the study also presents a counterexample that illustrates the lack of universal consistency on $\mathcal{W}_p((0,1))$.",1
"Recent years have witnessed tremendous interest in deep learning on graph-structured data. Due to the high cost of collecting labeled graph-structured data, domain adaptation is important to supervised graph learning tasks with limited samples. However, current graph domain adaptation methods are generally adopted from traditional domain adaptation tasks, and the properties of graph-structured data are not well utilized. For example, the observed social networks on different platforms are controlled not only by the different crowd or communities but also by the domain-specific policies and the background noise. Based on these properties in graph-structured data, we first assume that the graph-structured data generation process is controlled by three independent types of latent variables, i.e., the semantic latent variables, the domain latent variables, and the random latent variables. Based on this assumption, we propose a disentanglement-based unsupervised domain adaptation method for the graph-structured data, which applies variational graph auto-encoders to recover these latent variables and disentangles them via three supervised learning modules. Extensive experimental results on two real-world datasets in the graph classification task reveal that our method not only significantly outperforms the traditional domain adaptation methods and the disentangled-based domain adaptation methods but also outperforms the state-of-the-art graph domain adaptation algorithms.",0
"In recent years, there has been a surge of interest in utilizing deep learning techniques on graph-structured data. However, obtaining labeled graph data can be expensive, making domain adaptation crucial for supervised graph learning tasks with limited samples. Unfortunately, current graph domain adaptation methods are typically borrowed from traditional domain adaptation tasks, and do not fully leverage the unique properties of graph-structured data. For instance, social networks on different platforms are influenced not only by different communities, but also by domain-specific policies and background noise. To address these properties, we propose a method that assumes graph-structured data is generated by three types of latent variables: semantic, domain, and random. Our approach uses variational graph auto-encoders to recover these latent variables and disentangles them through three supervised learning modules, resulting in a disentanglement-based unsupervised domain adaptation method for graph-structured data. Our experiments on two real-world graph classification datasets demonstrate that our method significantly outperforms traditional domain adaptation methods, disentangled-based domain adaptation methods, and state-of-the-art graph domain adaptation algorithms.",1
"While the advent of Graph Neural Networks (GNNs) has greatly improved node and graph representation learning in many applications, the neighborhood aggregation scheme exposes additional vulnerabilities to adversaries seeking to extract node-level information about sensitive attributes. In this paper, we study the problem of protecting sensitive attributes by information obfuscation when learning with graph structured data. We propose a framework to locally filter out pre-determined sensitive attributes via adversarial training with the total variation and the Wasserstein distance. Our method creates a strong defense against inference attacks, while only suffering small loss in task performance. Theoretically, we analyze the effectiveness of our framework against a worst-case adversary, and characterize an inherent trade-off between maximizing predictive accuracy and minimizing information leakage. Experiments across multiple datasets from recommender systems, knowledge graphs and quantum chemistry demonstrate that the proposed approach provides a robust defense across various graph structures and tasks, while producing competitive GNN encoders for downstream tasks.",0
"Graph Neural Networks (GNNs) have significantly enhanced the learning of node and graph representation in numerous applications. However, the neighborhood aggregation scheme makes it easy for adversaries to extract node-level information about sensitive attributes. This study focuses on safeguarding sensitive attributes through information obfuscation when learning with graph structured data. To achieve this, we propose a framework that uses adversarial training with the total variation and the Wasserstein distance to locally filter pre-determined sensitive attributes. Our method provides a robust defense against inference attacks without compromising task performance. We analyze the effectiveness of our framework against a worst-case adversary, highlighting the trade-off between maximizing predictive accuracy and minimizing information leakage. Our experiments across multiple datasets from recommender systems, knowledge graphs, and quantum chemistry demonstrate the effectiveness of our approach in providing a robust defense across various graph structures and tasks while producing competitive GNN encoders for downstream tasks.",1
"Link prediction is an important learning task for graph-structured data. In this paper, we propose a novel topological approach to characterize interactions between two nodes. Our topological feature, based on the extended persistent homology, encodes rich structural information regarding the multi-hop paths connecting nodes. Based on this feature, we propose a graph neural network method that outperforms state-of-the-arts on different benchmarks. As another contribution, we propose a novel algorithm to more efficiently compute the extended persistence diagrams for graphs. This algorithm can be generally applied to accelerate many other topological methods for graph learning tasks.",0
"The identification of connections between nodes is a crucial aspect of learning graph-structured data. This study introduces a new method for characterizing node interactions through a topological approach. The approach employs an extended persistent homology-based feature, which captures intricate structural details about the multi-hop paths that link nodes. A graph neural network technique is then proposed, which uses this feature to outperform existing methods across multiple benchmarks. Additionally, the study presents a novel algorithm to expedite the calculation of extended persistence diagrams for graphs, which can be applied to accelerate various other topological approaches used in graph-based learning tasks.",1
"Deep neural networks have revolutionized many machine learning tasks in power systems, ranging from pattern recognition to signal processing. The data in these tasks is typically represented in Euclidean domains. Nevertheless, there is an increasing number of applications in power systems, where data are collected from non-Euclidean domains and represented as graph-structured data with high dimensional features and interdependency among nodes. The complexity of graph-structured data has brought significant challenges to the existing deep neural networks defined in Euclidean domains. Recently, many publications generalizing deep neural networks for graph-structured data in power systems have emerged. In this paper, a comprehensive overview of graph neural networks (GNNs) in power systems is proposed. Specifically, several classical paradigms of GNNs structures (e.g., graph convolutional networks) are summarized, and key applications in power systems, such as fault scenario application, time series prediction, power flow calculation, and data generation are reviewed in detail. Furthermore, main issues and some research trends about the applications of GNNs in power systems are discussed.",0
"The use of deep neural networks has greatly impacted various machine learning tasks within power systems, including pattern recognition and signal processing. Typically, data in these tasks is represented within Euclidean domains. However, more applications within power systems are using data collected from non-Euclidean domains, which are represented as graph-structured data with high-dimensional features and interdependency among nodes. This complexity has presented significant challenges for existing deep neural networks defined within Euclidean domains. Consequently, there has been a rise in publications discussing the generalization of deep neural networks for graph-structured data within power systems. This paper proposes a comprehensive overview of graph neural networks (GNNs) in power systems, summarizing several classical paradigms of GNN structures (such as graph convolutional networks) and reviewing key applications, including fault scenario application, time series prediction, power flow calculation, and data generation. Additionally, the paper discusses main issues and research trends concerning the use of GNNs in power systems.",1
"HyperGraph Convolutional Neural Networks (HGCNNs) have demonstrated their potential in modeling high-order relations preserved in graph structured data. However, most existing convolution filters are localized and determined by the pre-defined initial hypergraph topology, neglecting to explore implicit and long-ange relations in real-world data. In this paper, we propose the first learning-based method tailored for constructing adaptive hypergraph structure, termed HypERgrAph Laplacian aDaptor (HERALD), which serves as a generic plug-in-play module for improving the representational power of HGCNNs. Specifically, HERALD adaptively optimizes the adjacency relationship between hypernodes and hyperedges in an end-to-end manner and thus the task-aware hypergraph is learned. Furthermore, HERALD employs the self-attention mechanism to capture the non-local paired-nodes relation. Extensive experiments on various popular hypergraph datasets for node classification and graph classification tasks demonstrate that our approach obtains consistent and considerable performance enhancement, proving its effectiveness and generalization ability.",0
"The potential of HyperGraph Convolutional Neural Networks (HGCNNs) in modeling high-order relations in graph structured data has been demonstrated. However, the existing convolution filters are limited by the pre-defined initial hypergraph topology, resulting in a neglect of implicit and long-range relations in real-world data. To address this, we propose a learning-based method, HERALD, that constructs an adaptive hypergraph structure, improving the representational power of HGCNNs. HERALD optimizes the adjacency relationship between hypernodes and hyperedges in an end-to-end manner, resulting in a task-aware hypergraph. Additionally, HERALD uses the self-attention mechanism to capture the non-local paired-nodes relation. Our approach has been tested on popular hypergraph datasets, and the results indicate significant performance enhancement, demonstrating its effectiveness and generalization ability.",1
"Supervised machine learning has several drawbacks that make it difficult to use in many situations. Drawbacks include: heavy reliance on massive training data, limited generalizability and poor expressiveness of high-level semantics. Low-shot Learning attempts to address these drawbacks. Low-shot learning allows the model to obtain good predictive power with very little or no training data, where structured knowledge plays a key role as a high-level semantic representation of human. This article will review the fundamental factors of low-shot learning technologies, with a focus on the operation of structured knowledge under different low-shot conditions. We also introduce other techniques relevant to low-shot learning. Finally, we point out the limitations of low-shot learning, the prospects and gaps of industrial applications, and future research directions.",0
"The use of supervised machine learning can be challenging in various situations due to its drawbacks, including the need for extensive training data, limited generalizability, and poor expressiveness of high-level semantics. To address these issues, Low-shot Learning has emerged as a potential solution. This technique enables models to achieve good predictive power with minimal or no training data, leveraging structured knowledge as a high-level semantic representation of human knowledge. In this article, we will explore the fundamental components of low-shot learning technologies, with an emphasis on the use of structured knowledge in various low-shot scenarios. Additionally, we will introduce other techniques related to low-shot learning, highlight its limitations, opportunities, and gaps in industrial applications, and provide insights into future research directions.",1
"Graph Neural Networks (GNNs) have been widely applied to various fields due to their powerful representations of graph-structured data. Despite the success of GNNs, most existing GNNs are designed to learn node representations on the fixed and homogeneous graphs. The limitations especially become problematic when learning representations on a misspecified graph or a heterogeneous graph that consists of various types of nodes and edges. To address this limitations, we propose Graph Transformer Networks (GTNs) that are capable of generating new graph structures, which preclude noisy connections and include useful connections (e.g., meta-paths) for tasks, while learning effective node representations on the new graphs in an end-to-end fashion. We further propose enhanced version of GTNs, Fast Graph Transformer Networks (FastGTNs), that improve scalability of graph transformations. Compared to GTNs, FastGTNs are 230x faster and use 100x less memory while allowing the identical graph transformations as GTNs. In addition, we extend graph transformations to the semantic proximity of nodes allowing non-local operations beyond meta-paths. Extensive experiments on both homogeneous graphs and heterogeneous graphs show that GTNs and FastGTNs with non-local operations achieve the state-of-the-art performance for node classification tasks. The code is available: https://github.com/seongjunyun/Graph_Transformer_Networks",0
"Due to their ability to provide powerful representations of graph-structured data, Graph Neural Networks (GNNs) have been widely utilized in various fields. However, most existing GNNs are designed for learning node representations on fixed and homogeneous graphs, which can become problematic when attempting to learn representations on misspecified or heterogeneous graphs containing various node and edge types. To overcome these limitations, we introduce Graph Transformer Networks (GTNs), which can generate new graph structures that exclude noisy connections and include useful connections (such as meta-paths) for specific tasks while learning effective node representations on the new graphs in an end-to-end manner. We also propose an enhanced version of GTNs, Fast Graph Transformer Networks (FastGTNs), which improve scalability and are 230 times faster and use 100 times less memory than GTNs while allowing for identical graph transformations. Additionally, we extend graph transformations to the semantic proximity of nodes, allowing for non-local operations beyond meta-paths. Extensive experiments on both homogeneous and heterogeneous graphs demonstrate that GTNs and FastGTNs with non-local operations achieve state-of-the-art performance for node classification tasks. The code for this work is available at https://github.com/seongjunyun/Graph_Transformer_Networks.",1
"Graph neural networks (GNNs) are one of the most popular approaches to using deep learning on graph-structured data, and they have shown state-of-the-art performances on a variety of tasks. However, according to a recent study, a careful choice of pooling functions, which are used for the aggregation or readout operation in GNNs, is crucial for enabling GNNs to extrapolate. Without the ideal combination of pooling functions, which varies across tasks, GNNs completely fail to generalize to out-of-distribution data, while the number of possible combinations grows exponentially with the number of layers. In this paper, we present GNP, a $L^p$ norm-like pooling function that is trainable end-to-end for any given task. Notably, GNP generalizes most of the widely-used pooling functions. We verify experimentally that simply replacing all pooling functions with GNP enables GNNs to extrapolate well on many node-level, graph-level, and set-related tasks; and GNP sometimes performs even better than optimal combinations of existing pooling functions.",0
"The use of deep learning on graph-structured data has become increasingly popular with the implementation of Graph neural networks (GNNs), which have demonstrated exceptional performance on various tasks. However, recent research suggests that the selection of pooling functions, used for the readout or aggregation operation in GNNs, plays a vital role in enabling GNNs to extrapolate. Without the appropriate pooling function combination, which varies with the task at hand and the number of layers, GNNs fail to generalize to out-of-distribution data. In this study, we introduce GNP, a trainable $L^p$ norm-like pooling function that can be applied to any task. GNP is capable of generalizing most commonly used pooling functions and enables GNNs to extrapolate successfully on node-level, graph-level, and set-related tasks. Our experimentation shows that replacing existing pooling functions with GNP improves GNNs' performance and sometimes even surpasses optimal pooling function combinations.",1
"HyperGraph Convolutional Neural Networks (HGCNNs) have demonstrated their potential in modeling high-order relations preserved in graph structured data. However, most existing convolution filters are localized and determined by the pre-defined initial hypergraph topology, neglecting to explore implicit and long-ange relations in real-world data. In this paper, we propose the first learning-based method tailored for constructing adaptive hypergraph structure, termed HypERgrAph Laplacian aDaptor (HERALD), which serves as a generic plug-in-play module for improving the representational power of HGCNNs. Specifically, HERALD adaptively optimizes the adjacency relationship between hypernodes and hyperedges in an end-to-end manner and thus the task-aware hypergraph is learned. Furthermore, HERALD employs the self-attention mechanism to capture the non-local paired-nodes relation. Extensive experiments on various popular hypergraph datasets for node classification and graph classification tasks demonstrate that our approach obtains consistent and considerable performance enhancement, proving its effectiveness and generalization ability.",0
"The potential of HyperGraph Convolutional Neural Networks (HGCNNs) in modeling high-order relations in graph structured data has been demonstrated. However, the existing convolution filters are localized and determined by the pre-defined initial hypergraph topology, failing to explore implicit and long-range relations in real-world data. This paper introduces the HypERgrAph Laplacian aDaptor (HERALD), the first learning-based method designed to construct an adaptive hypergraph structure. HERALD serves as a generic plug-in-play module to enhance the representational power of HGCNNs. By adaptively optimizing the adjacency relationship between hypernodes and hyperedges in an end-to-end manner, HERALD learns the task-aware hypergraph. Additionally, HERALD employs the self-attention mechanism to capture the non-local paired-nodes relation. Extensive experiments performed on various popular hypergraph datasets for node classification and graph classification tasks prove the effectiveness and generalization ability of our approach, as it consistently and considerably enhances performance.",1
"Graph is an usual representation of relational data, which are ubiquitous in manydomains such as molecules, biological and social networks. A popular approach to learningwith graph structured data is to make use of graph kernels, which measure the similaritybetween graphs and are plugged into a kernel machine such as a support vector machine.Weisfeiler-Lehman (WL) based graph kernels, which employ WL labeling scheme to extract subtree patterns and perform node embedding, are demonstrated to achieve great performance while being efficiently computable. However, one of the main drawbacks of ageneral kernel is the decoupling of kernel construction and learning process. For moleculargraphs, usual kernels such as WL subtree, based on substructures of the molecules, consider all available substructures having the same importance, which might not be suitable inpractice. In this paper, we propose a method to learn the weights of subtree patterns in the framework of WWL kernels, the state of the art method for graph classification task [14]. To overcome the computational issue on large scale data sets, we present an efficient learning algorithm and also derive a generalization gap bound to show its convergence. Finally, through experiments on synthetic and real-world data sets, we demonstrate the effectiveness of our proposed method for learning the weights of subtree patterns.",0
"The graph is a common way to display relational data that is found in various fields, such as social networks, molecules, and biology. Graph kernels are a popular method for learning with graph structured data, as they measure the similarity between graphs and can be used with kernel machines like support vector machines. The Weisfeiler-Lehman based graph kernels are effective and computationally efficient, but one drawback is that kernel construction and learning processes are separate. For molecular graphs, the usual kernels consider all substructures equally, which may not be appropriate. This paper introduces a new approach to learning the weights of subtree patterns using the WWL kernel, a state-of-the-art method for graph classification. An efficient learning algorithm is presented to overcome computational challenges, and a generalization gap bound is derived to demonstrate convergence. Experiments on synthetic and real-world data sets show the effectiveness of this proposed method for learning the weights of subtree patterns.",1
"Event forecasting is a challenging, yet important task, as humans seek to constantly plan for the future. Existing automated forecasting studies rely mostly on structured data, such as time-series or event-based knowledge graphs, to help predict future events. In this work, we aim to formulate a task, construct a dataset, and provide benchmarks for developing methods for event forecasting with large volumes of unstructured text data. To simulate the forecasting scenario on temporal news documents, we formulate the problem as a restricted-domain, multiple-choice, question-answering (QA) task. Unlike existing QA tasks, our task limits accessible information, and thus a model has to make a forecasting judgement. To showcase the usefulness of this task formulation, we introduce ForecastQA, a question-answering dataset consisting of 10,392 event forecasting questions, which have been collected and verified via crowdsourcing efforts. We present our experiments on ForecastQA using BERT-based models and find that our best model achieves 60.1% accuracy on the dataset, which still lags behind human performance by about 19%. We hope ForecastQA will support future research efforts in bridging this gap.",0
"Predicting future events is a difficult but essential task for humans who are constantly planning ahead. Currently, automated forecasting studies mainly rely on structured data such as time-series or event-based knowledge graphs to make predictions. This study aims to create a dataset and benchmarks for event forecasting using large amounts of unstructured text data. To simulate forecasting with temporal news documents, the problem is formulated as a restricted-domain, multiple-choice, question-answering (QA) task. Unlike other QA tasks, this task limits the available information to test a model's forecasting skills. A dataset called ForecastQA was created, containing 10,392 event forecasting questions gathered and verified through crowdsourcing efforts. BERT-based models were used to conduct experiments on ForecastQA, and the best model achieved 60.1% accuracy, which is still 19% lower than human performance. This research hopes to support future studies in reducing this performance gap.",1
"Graph Neural Network (GNN) has been demonstrated its effectiveness in dealing with non-Euclidean structural data. Both spatial-based and spectral-based GNNs are relying on adjacency matrix to guide message passing among neighbors during feature aggregation. Recent works have mainly focused on powerful message passing modules, however, in this paper, we show that none of the message passing modules is necessary. Instead, we propose a pure multilayer-perceptron-based framework, Graph-MLP with the supervision signal leveraging graph structure, which is sufficient for learning discriminative node representation. In model-level, Graph-MLP only includes multi-layer perceptrons, activation function, and layer normalization. In the loss level, we design a neighboring contrastive (NContrast) loss to bridge the gap between GNNs and MLPs by utilizing the adjacency information implicitly. This design allows our model to be lighter and more robust when facing large-scale graph data and corrupted adjacency information. Extensive experiments prove that even without adjacency information in testing phase, our framework can still reach comparable and even superior performance against the state-of-the-art models in the graph node classification task.",0
"The effectiveness of Graph Neural Network (GNN) in handling non-Euclidean structural data has been established. Both spatial-based and spectral-based GNNs rely on the adjacency matrix to facilitate message passing among neighbors during feature aggregation. While previous studies have mainly focused on developing powerful message passing modules, our paper demonstrates that these modules are not necessary. Instead, we propose a pure multilayer-perceptron-based framework, Graph-MLP, which utilizes the graph structure as a supervision signal to learn discriminative node representation. The Graph-MLP model comprises only multi-layer perceptrons, activation function, and layer normalization. At the loss level, we introduce a neighboring contrastive (NContrast) loss that leverages the adjacency information implicitly to bridge the gap between GNNs and MLPs. This design results in a lighter and more robust model that can handle large-scale graph data and corrupted adjacency information. Our experiments demonstrate that even without adjacency information in the testing phase, our framework achieves comparable and sometimes superior performance compared to state-of-the-art models in the task of graph node classification.",1
"Various classes of Graph Neural Networks (GNN) have been proposed and shown to be successful in a wide range of applications with graph structured data. In this paper, we propose a theoretical framework able to compare the expressive power of these GNN architectures. The current universality theorems only apply to intractable classes of GNNs. Here, we prove the first approximation guarantees for practical GNNs, paving the way for a better understanding of their generalization. Our theoretical results are proved for invariant GNNs computing a graph embedding (permutation of the nodes of the input graph does not affect the output) and equivariant GNNs computing an embedding of the nodes (permutation of the input permutes the output). We show that Folklore Graph Neural Networks (FGNN), which are tensor based GNNs augmented with matrix multiplication are the most expressive architectures proposed so far for a given tensor order. We illustrate our results on the Quadratic Assignment Problem (a NP-Hard combinatorial problem) by showing that FGNNs are able to learn how to solve the problem, leading to much better average performances than existing algorithms (based on spectral, SDP or other GNNs architectures). On a practical side, we also implement masked tensors to handle batches of graphs of varying sizes.",0
"Graph Neural Networks (GNN) have been proposed in various classes and have proven to be successful in a variety of applications involving graph structured data. This paper presents a theoretical framework that enables comparison of the expressive power of these GNN architectures. Existing universality theorems only apply to classes of GNNs that are intractable. However, our framework provides the first approximation guarantees for practical GNNs, which can lead to a better understanding of their generalization. Our theoretical results are established for two types of GNNs: invariant GNNs that compute a graph embedding, and equivariant GNNs that compute an embedding of the nodes. Furthermore, we demonstrate that Folklore Graph Neural Networks (FGNN) are the most expressive architectures proposed so far for a given tensor order. We validate our results using the Quadratic Assignment Problem, a combinatorial problem that is NP-Hard. We show that FGNNs can learn how to solve this problem, surpassing existing algorithms based on spectral, SDP or other GNN architectures. Additionally, we implement masked tensors to handle batches of graphs of varying sizes.",1
"This paper presents Gem, a model-agnostic approach for providing interpretable explanations for any GNNs on various graph learning tasks. Specifically, we formulate the problem of providing explanations for the decisions of GNNs as a causal learning task. Then we train a causal explanation model equipped with a loss function based on Granger causality. Different from existing explainers for GNNs, Gem explains GNNs on graph-structured data from a causal perspective. It has better generalization ability as it has no requirements on the internal structure of the GNNs or prior knowledge on the graph learning tasks. In addition, Gem, once trained, can be used to explain the target GNN very quickly. Our theoretical analysis shows that several recent explainers fall into a unified framework of additive feature attribution methods. Experimental results on synthetic and real-world datasets show that Gem achieves a relative increase of the explanation accuracy by up to $30\%$ and speeds up the explanation process by up to $110\times$ as compared to its state-of-the-art alternatives.",0
"Gem is a novel approach that offers interpretable explanations for GNNs across various graph learning tasks. The paper formulates the problem of providing explanations for GNN decisions as a causal learning task and trains a causal explanation model that employs a loss function based on Granger causality. Unlike existing explainers for GNNs, Gem explicates GNNs from a causal standpoint, thus possessing superior generalization ability without any requirements on the internal structure of GNNs or prior knowledge on graph learning tasks. Moreover, once trained, Gem can swiftly explain the target GNN. The paper's theoretical analysis reveals that several recent explainers belong to a unified framework of additive feature attribution methods. Experimental results on synthetic and real-world datasets demonstrate that Gem enhances the explanation accuracy by up to $30\%$ and speeds up the explanation process by up to $110\times$ compared to its state-of-the-art alternatives.",1
"Learning the representation of data with hierarchical structures in the hyperbolic space attracts increasing attention in recent years. Due to the constant negative curvature, the hyperbolic space resembles tree metrics and captures the tree-like properties naturally, which enables the hyperbolic embeddings to improve over traditional Euclidean models. However, many real-world hierarchically structured data such as taxonomies and multitree networks have varying local structures and they are not trees, thus they do not ubiquitously match the constant curvature property of the hyperbolic space. To address this limitation of hyperbolic embeddings, we explore the complex hyperbolic space, which has the variable negative curvature, for representation learning. Specifically, we propose to learn the embeddings of hierarchically structured data in the unit ball model of the complex hyperbolic space. The unit ball model based embeddings have a more powerful representation capacity to capture a variety of hierarchical structures. Through experiments on synthetic and real-world data, we show that our approach improves over the hyperbolic embedding models significantly.",0
"In recent years, there has been a growing interest in learning how to represent data with hierarchical structures in the hyperbolic space. This is due to the fact that the hyperbolic space, with its constant negative curvature, is similar to tree metrics and naturally captures tree-like properties. As a result, hyperbolic embeddings have been shown to outperform traditional Euclidean models. However, real-world hierarchically structured data, such as taxonomies and multitree networks, often have varying local structures that do not match the constant curvature property of the hyperbolic space. To overcome this limitation, we propose exploring the complex hyperbolic space, which has variable negative curvature, for representation learning. Specifically, we suggest learning embeddings in the unit ball model of the complex hyperbolic space to better capture a variety of hierarchical structures. Our experiments on both synthetic and real-world data demonstrate that our approach significantly improves upon hyperbolic embedding models.",1
"Imbalanced classification on graphs is ubiquitous yet challenging in many real-world applications, such as fraudulent node detection. Recently, graph neural networks (GNNs) have shown promising performance on many network analysis tasks. However, most existing GNNs have almost exclusively focused on the balanced networks, and would get unappealing performance on the imbalanced networks. To bridge this gap, in this paper, we present a generative adversarial graph network model, called ImGAGN to address the imbalanced classification problem on graphs. It introduces a novel generator for graph structure data, named GraphGenerator, which can simulate both the minority class nodes' attribute distribution and network topological structure distribution by generating a set of synthetic minority nodes such that the number of nodes in different classes can be balanced. Then a graph convolutional network (GCN) discriminator is trained to discriminate between real nodes and fake (i.e., generated) nodes, and also between minority nodes and majority nodes on the synthetic balanced network. To validate the effectiveness of the proposed method, extensive experiments are conducted on four real-world imbalanced network datasets. Experimental results demonstrate that the proposed method ImGAGN outperforms state-of-the-art algorithms for semi-supervised imbalanced node classification task.",0
"The problem of imbalanced classification on graphs is a common issue in real-world scenarios, for example, in detecting fraudulent nodes. Graph neural networks (GNNs) have recently emerged as a promising solution for network analysis tasks. However, most existing GNNs have primarily focused on balanced networks and perform poorly on imbalanced networks. To address this challenge, our paper presents ImGAGN, a generative adversarial graph network model. ImGAGN includes a GraphGenerator that can simulate the minority class nodes' attribute distribution and network topological structure distribution by generating synthetic minority nodes to balance the number of nodes in each class. A graph convolutional network (GCN) discriminator is then trained to distinguish between real and generated nodes and between minority and majority nodes on the synthetic balanced network. We evaluate the proposed method on four real-world imbalanced network datasets and demonstrate that ImGAGN outperforms state-of-the-art algorithms for semi-supervised imbalanced node classification.",1
"Graph Neural Networks (GNNs) are the first choice methods for graph machine learning problems thanks to their ability to learn state-of-the-art level representations from graph-structured data. However, centralizing a massive amount of real-world graph data for GNN training is prohibitive due to user-side privacy concerns, regulation restrictions, and commercial competition. Federated Learning is the de-facto standard for collaborative training of machine learning models over many distributed edge devices without the need for centralization. Nevertheless, training graph neural networks in a federated setting is vaguely defined and brings statistical and systems challenges. This work proposes SpreadGNN, a novel multi-task federated training framework capable of operating in the presence of partial labels and absence of a central server for the first time in the literature. SpreadGNN extends federated multi-task learning to realistic serverless settings for GNNs, and utilizes a novel optimization algorithm with a convergence guarantee, Decentralized Periodic Averaging SGD (DPA-SGD), to solve decentralized multi-task learning problems. We empirically demonstrate the efficacy of our framework on a variety of non-I.I.D. distributed graph-level molecular property prediction datasets with partial labels. Our results show that SpreadGNN outperforms GNN models trained over a central server-dependent federated learning system, even in constrained topologies. The source code is publicly available at https://github.com/FedML-AI/SpreadGNN",0
"GNNs are a popular choice for graph machine learning problems as they can learn high-quality representations from graph-structured data. However, gathering a large amount of real-world graph data for GNN training is difficult due to privacy concerns, regulations, and competition. Federated Learning is the preferred method for collaborative training without centralization, but training GNNs in a federated setting poses challenges. This study introduces SpreadGNN, a novel multi-task federated training framework capable of operating without a central server and with partial labels. SpreadGNN utilizes a novel optimization algorithm, DPA-SGD, to solve decentralized multi-task learning problems. The framework is tested on distributed graph-level molecular property prediction datasets with partial labels and outperforms GNN models trained over a central server-dependent federated learning system, even in constrained topologies. The source code is publicly available on GitHub.",1
"Graph Neural Networks (GNNs) are widely used deep learning models that learn meaningful representations from graph-structured data. Due to the finite nature of the underlying recurrent structure, current GNN methods may struggle to capture long-range dependencies in underlying graphs. To overcome this difficulty, we propose a graph learning framework, called Implicit Graph Neural Networks (IGNN), where predictions are based on the solution of a fixed-point equilibrium equation involving implicitly defined ""state"" vectors. We use the Perron-Frobenius theory to derive sufficient conditions that ensure well-posedness of the framework. Leveraging implicit differentiation, we derive a tractable projected gradient descent method to train the framework. Experiments on a comprehensive range of tasks show that IGNNs consistently capture long-range dependencies and outperform the state-of-the-art GNN models.",0
"Deep learning models known as Graph Neural Networks (GNNs) are commonly utilized for deriving meaningful representations from data structured in graphs. However, these models may struggle with capturing long-range dependencies due to the fixed nature of their recurrent structure. To address this issue, we propose Implicit Graph Neural Networks (IGNN), a graph learning framework that utilizes the solution of a fixed-point equilibrium equation involving implicitly defined ""state"" vectors for predictions. We employ Perron-Frobenius theory to ensure well-posedness of the framework and use implicit differentiation to derive a tractable projected gradient descent method for training. Our experiments demonstrate that IGNNs consistently outperform current GNN models in capturing long-range dependencies across a wide range of tasks.",1
"Graph neural networks (GNNs) have been successfully employed in a myriad of applications involving graph-structured data. Theoretical findings establish that GNNs use nonlinear activation functions to create low-eigenvalue frequency content that can be processed in a stable manner by subsequent graph convolutional filters. However, the exact shape of the frequency content created by nonlinear functions is not known, and thus, it cannot be learned nor controlled. In this work, node-variant graph filters (NVGFs) are shown to be capable of creating frequency content and are thus used in lieu of nonlinear activation functions. This results in a novel GNN architecture that, although linear, is capable of creating frequency content as well. Furthermore, this new frequency content can be either designed or learned from data. In this way, the role of frequency creation is separated from the nonlinear nature of traditional GNNs. Extensive simulations are carried out to differentiate the contributions of frequency creation from those of the nonlinearity.",0
"Graph neural networks (GNNs) have proven to be effective in various applications that involve data structured as graphs. Theoretical studies reveal that GNNs employ nonlinear activation functions to generate low-eigenvalue frequency content that can be processed by subsequent graph convolutional filters in a stable manner. Nevertheless, the shape of the frequency content generated by nonlinear functions is unknown, making it impossible to learn or control it. This study introduces node-variant graph filters (NVGFs) that can create frequency content and therefore replace nonlinear activation functions. This creates a novel GNN architecture that is linear but capable of generating frequency content, which can be either designed or learned from data. This separates the role of frequency creation from the nonlinearity of traditional GNNs. Extensive simulations are conducted to differentiate the contributions of frequency creation and nonlinearity.",1
"Networks are ubiquitous in the real world. Link prediction, as one of the key problems for network-structured data, aims to predict whether there exists a link between two nodes. The traditional approaches are based on the explicit similarity computation between the compact node representation by embedding each node into a low-dimensional space. In order to efficiently handle the intensive similarity computation in link prediction, the hashing technique has been successfully used to produce the node representation in the Hamming space. However, the hashing-based link prediction algorithms face accuracy loss from the randomized hashing techniques or inefficiency from the learning to hash techniques in the embedding process. Currently, the Graph Neural Network (GNN) framework has been widely applied to the graph-related tasks in an end-to-end manner, but it commonly requires substantial computational resources and memory costs due to massive parameter learning, which makes the GNN-based algorithms impractical without the help of a powerful workhorse. In this paper, we propose a simple and effective model called #GNN, which balances the trade-off between accuracy and efficiency. #GNN is able to efficiently acquire node representation in the Hamming space for link prediction by exploiting the randomized hashing technique to implement message passing and capture high-order proximity in the GNN framework. Furthermore, we characterize the discriminative power of #GNN in probability. The extensive experimental results demonstrate that the proposed #GNN algorithm achieves accuracy comparable to the learning-based algorithms and outperforms the randomized algorithm, while running significantly faster than the learning-based algorithms. Also, the proposed algorithm shows excellent scalability on a large-scale network with the limited resources.",0
"Networks are present everywhere in the real world, and one of the major issues faced in dealing with network-structured data is link prediction, which is the task of determining whether a link exists between two nodes. Traditional methods for this involve computing the similarity between nodes by embedding them into a lower-dimensional space. Hashing has been used successfully to handle the intensive similarity computation required in link prediction by producing node representations in the Hamming space. However, hashing-based algorithms face accuracy loss or inefficiency in the embedding process. Graph Neural Networks (GNN) have been widely used for graph-related tasks, but they require significant computational resources and memory costs. This paper proposes a model called #GNN, which balances accuracy and efficiency by using randomized hashing to implement message passing and capture high-order proximity in the GNN framework. The algorithm achieves accuracy comparable to learning-based approaches and outperforms the randomized algorithm while running significantly faster. It also shows excellent scalability on large-scale networks with limited resources.",1
"Estimating the amount of electricity that can be produced by rooftop photovoltaic systems is a time-consuming process that requires on-site measurements, a difficult task to achieve on a large scale. In this paper, we present an approach to estimate the solar potential of rooftops based on their location and architectural characteristics, as well as the amount of solar radiation they receive annually. Our technique uses computer vision to achieve semantic segmentation of roof sections and roof objects on the one hand, and a machine learning model based on structured building features to predict roof pitch on the other hand. We then compute the azimuth and maximum number of solar panels that can be installed on a rooftop with geometric approaches. Finally, we compute precise shading masks and combine them with solar irradiation data that enables us to estimate the yearly solar potential of a rooftop.",0
"The process of determining the electricity production capability of rooftop photovoltaic systems is a time-consuming and challenging task to perform on a large scale due to the need for on-site measurements. To address this issue, we propose a method in this article that uses the location and architectural characteristics of rooftops along with their yearly solar radiation exposure to estimate their solar potential. We utilize computer vision for semantic segmentation of roof sections and roof objects and a machine learning model based on structured building features to predict roof pitch. Geometric approaches help us calculate the azimuth and maximum number of solar panels that a rooftop can accommodate. Finally, we combine shading masks with solar irradiation data to obtain an accurate estimation of the yearly solar potential of a rooftop.",1
"Functional connectivity (FC) between regions of the brain can be assessed by the degree of temporal correlation measured with functional neuroimaging modalities. Based on the fact that these connectivities build a network, graph-based approaches for analyzing the brain connectome have provided insights into the functions of the human brain. The development of graph neural networks (GNNs) capable of learning representation from graph structured data has led to increased interest in learning the graph representation of the brain connectome. Although recent attempts to apply GNN to the FC network have shown promising results, there is still a common limitation that they usually do not incorporate the dynamic characteristics of the FC network which fluctuates over time. In addition, a few studies that have attempted to use dynamic FC as an input for the GNN reported a reduction in performance compared to static FC methods, and did not provide temporal explainability. Here, we propose STAGIN, a method for learning dynamic graph representation of the brain connectome with spatio-temporal attention. Specifically, a temporal sequence of brain graphs is input to the STAGIN to obtain the dynamic graph representation, while novel READOUT functions and the Transformer encoder provide spatial and temporal explainability with attention, respectively. Experiments on the HCP-Rest and the HCP-Task datasets demonstrate exceptional performance of our proposed method. Analysis of the spatio-temporal attention also provide concurrent interpretation with the neuroscientific knowledge, which further validates our method. Code is available at https://github.com/egyptdj/stagin",0
"The degree of temporal correlation between different regions of the brain can be used to assess functional connectivity (FC) using functional neuroimaging techniques. Graph-based approaches have been used to analyze the network of these connectivities and gain insights into the functions of the human brain. Graph neural networks (GNNs) have been developed to learn the representation of the brain connectome, but there is a common limitation that they do not incorporate dynamic characteristics of the FC network. Attempts to use dynamic FC as input for the GNN have reported a reduction in performance and lack of temporal explainability. To address this, we propose STAGIN, a method that uses spatio-temporal attention to learn the dynamic graph representation of the brain connectome. Our method uses a temporal sequence of brain graphs as input and novel READOUT functions and Transformer encoder to provide spatial and temporal explainability with attention. Our experiments on HCP-Rest and HCP-Task datasets demonstrate exceptional performance, and analysis of spatio-temporal attention is concurrent with neuroscientific knowledge. The code for our method is available at https://github.com/egyptdj/stagin.",1
"Kernel methods on discrete domains have shown great promise for many challenging data types, for instance, biological sequence data and molecular structure data. Scalable kernel methods like Support Vector Machines may offer good predictive performances but do not intrinsically provide uncertainty estimates. In contrast, probabilistic kernel methods like Gaussian Processes offer uncertainty estimates in addition to good predictive performance but fall short in terms of scalability. While the scalability of Gaussian processes can be improved using sparse inducing point approximations, the selection of these inducing points remains challenging. We explore different techniques for selecting inducing points on discrete domains, including greedy selection, determinantal point processes, and simulated annealing. We find that simulated annealing, which can select inducing points that are not in the training set, can perform competitively with support vector machines and full Gaussian processes on synthetic data, as well as on challenging real-world DNA sequence data.",0
"Kernel methods have shown promise in handling challenging data types, such as biological sequence and molecular structure data, on discrete domains. While Support Vector Machines offer good predictive performance, they do not provide intrinsic uncertainty estimates. On the other hand, Gaussian Processes offer uncertainty estimates but have limited scalability. However, sparse inducing point approximations can improve their scalability, though selecting these points remains a challenge. To address this, we examine various techniques for selecting inducing points, including greedy selection, determinantal point processes, and simulated annealing. Our findings indicate that simulated annealing, which can select inducing points outside of the training set, can perform competitively with Support Vector Machines and full Gaussian Processes on both synthetic and real-world DNA sequence data.",1
"This paper presents Sparse Tensor Classifier (STC), a supervised classification algorithm for categorical data inspired by the notion of superposition of states in quantum physics. By regarding an observation as a superposition of features, we introduce the concept of wave-particle duality in machine learning and propose a generalized framework that unifies the classical and the quantum probability. We show that STC possesses a wide range of desirable properties not available in most other machine learning methods but it is at the same time exceptionally easy to comprehend and use. Empirical evaluation of STC on structured data and text classification demonstrates that our methodology achieves state-of-the-art performances compared to both standard classifiers and deep learning, at the additional benefit of requiring minimal data pre-processing and hyper-parameter tuning. Moreover, STC provides a native explanation of its predictions both for single instances and for each target label globally.",0
"The article introduces the Sparse Tensor Classifier (STC), a supervised classification algorithm for categorical data. The inspiration for this algorithm comes from the concept of superposition of states in quantum physics. By considering observations as a combination of features, the concept of wave-particle duality is introduced into machine learning. The proposed framework unifies classical and quantum probability, and STC possesses unique properties not found in other machine learning methods. STC is also easy to comprehend and use. Empirical evaluation on structured data and text classification shows that STC outperforms standard classifiers and deep learning, with minimal data pre-processing and hyper-parameter tuning. Additionally, STC provides a native explanation of its predictions at both the single instance and global label levels.",1
"Over the last few years, we have seen increasing data generated from non-Euclidean domains, which are usually represented as graphs with complex relationships, and Graph Neural Networks (GNN) have gained a high interest because of their potential in processing graph-structured data. In particular, there is a strong interest in exploring the possibilities in performing convolution on graphs using an extension of the GNN architecture, generally referred to as Graph Convolutional Neural Networks (GCNN). Convolution on graphs has been achieved mainly in two forms: spectral and spatial convolutions. Due to the higher flexibility in exploring and exploiting the graph structure of data, recently, there is an increasing interest in investigating the possibilities that the spatial approach can offer. The idea of finding a way to adapt the network behaviour to the inputs they process to maximize the total performances has aroused much interest in the neural networks literature over the years. This paper presents a novel method to adapt the behaviour of a GCNN to the input proposing two ways to perform spatial convolution on graphs using input-based filters which are dynamically generated. Our model also investigates the problem of discovering and refining relations among nodes. The experimental assessment confirms the capabilities of the proposed approach, which achieves satisfying results using simple architectures with a low number of filters.",0
"In recent years, there has been a surge in data generated from non-Euclidean domains that are typically represented as graphs with intricate relationships. Graph Neural Networks (GNN) have become increasingly popular due to their potential in processing graph-structured data. A particular area of interest is investigating the possibilities of performing convolution on graphs using Graph Convolutional Neural Networks (GCNN). Spectral and spatial convolutions are two main forms of graph convolution, with the latter gaining more interest recently due to its greater flexibility in exploring and exploiting the graph structure of data. This paper proposes a novel method to adapt the behaviour of a GCNN to the input by introducing two ways to perform spatial convolution on graphs using dynamically generated input-based filters. The model also addresses the problem of discovering and refining relations among nodes. The experimental assessment confirms the effectiveness of the proposed approach, achieving promising results with simple architectures that require only a few filters. The idea of maximizing network performance by adapting its behaviour to the inputs processed is a topic that has garnered much interest in the neural networks literature over the years.",1
"Graph convolutional networks (GCNs) have achieved great success in dealing with data of non-Euclidean structures. Their success directly attributes to fitting graph structures effectively to data such as in social media and knowledge databases. For image processing applications, the use of graph structures and GCNs have not been fully explored. In this paper, we propose a novel encoder-decoder network with added graph convolutions by converting feature maps to vertexes of a pre-generated graph to synthetically construct graph-structured data. By doing this, we inexplicitly apply graph Laplacian regularization to the feature maps, making them more structured. The experiments show that it significantly boosts performance for image restoration tasks, including deblurring and super-resolution. We believe it opens up opportunities for GCN-based approaches in more applications.",0
"The success of graph convolutional networks (GCNs) lies in their ability to effectively fit graph structures to non-Euclidean data, such as in social media and knowledge databases. However, the use of GCNs and graph structures in image processing applications has not yet been fully explored. This paper presents a new encoder-decoder network that incorporates graph convolutions by converting feature maps to vertices on a pre-generated graph, thereby creating graph-structured data. This approach applies graph Laplacian regularization to the feature maps, resulting in greater structure. The experiments demonstrate that this approach significantly enhances image restoration tasks, such as deblurring and super-resolution. This work may provide new opportunities for GCN-based applications in other fields.",1
"Traditional approaches for data anonymization consider relational data and textual data independently. We propose rx-anon, an anonymization approach for heterogeneous semi-structured documents composed of relational and textual attributes. We map sensitive terms extracted from the text to the structured data. This allows us to use concepts like k-anonymity to generate a joined, privacy-preserved version of the heterogeneous data input. We introduce the concept of redundant sensitive information to consistently anonymize the heterogeneous data. To control the influence of anonymization over unstructured textual data versus structured data attributes, we introduce a modified, parameterized Mondrian algorithm. The parameter $\lambda$ allows to give different weight on the relational and textual attributes during the anonymization process. We evaluate our approach with two real-world datasets using a Normalized Certainty Penalty score, adapted to the problem of jointly anonymizing relational and textual data. The results show that our approach is capable of reducing information loss by using the tuning parameter to control the Mondrian partitioning while guaranteeing k-anonymity for relational attributes as well as for sensitive terms. As rx-anon is a framework approach, it can be reused and extended by other anonymization algorithms, privacy models, and textual similarity metrics.",0
"The conventional methods for anonymizing data treat relational data and textual data separately. Our proposed approach, rx-anon, addresses the anonymization of heterogeneous semi-structured documents that contain both relational and textual attributes. We accomplish this by mapping sensitive terms from the text to the structured data. This enables us to employ concepts like k-anonymity to create a combined, privacy-protected version of the mixed data input. We introduce the notion of redundant sensitive information to ensure consistent anonymization of the heterogeneous data. To manage the impact of anonymization on unstructured textual data versus structured data attributes, we present a modified Mondrian algorithm that is parameterized. The $\lambda$ parameter allows for differential weighting of the relational and textual attributes when anonymizing. We assess our approach using two actual datasets and a Normalized Certainty Penalty score, which has been adapted to jointly anonymize relational and textual data. The results indicate that our method can reduce information loss by adjusting the Mondrian partitioning using the tuning parameter while ensuring k-anonymity for relational attributes and sensitive terms. Since rx-anon is a framework approach, it can be reused and expanded by other anonymization algorithms, privacy models, and textual similarity metrics.",1
"We propose an approach to learning with graph-structured data in the problem domain of graph classification. In particular, we present a novel type of readout operation to aggregate node features into a graph-level representation. To this end, we leverage persistent homology computed via a real-valued, learnable, filter function. We establish the theoretical foundation for differentiating through the persistent homology computation. Empirically, we show that this type of readout operation compares favorably to previous techniques, especially when the graph connectivity structure is informative for the learning problem.",0
"Our proposition involves a method for acquiring knowledge from data that is structured in a graph format, specifically in the area of graph classification. Our approach introduces an innovative form of readout operation that can combine node characteristics into an overall representation of the graph. In order to accomplish this, we employ a filter function that is both learnable and real-valued in order to leverage persistent homology. We have established the fundamental basis for being able to differentiate through the computation of persistent homology. Through our experiments, we demonstrate that our type of readout operation performs favorably in comparison to previous techniques, particularly when the graph's connectivity structure imparts significant information for the learning task.",1
"We study the multiple manifold problem, a binary classification task modeled on applications in machine vision, in which a deep fully-connected neural network is trained to separate two low-dimensional submanifolds of the unit sphere. We provide an analysis of the one-dimensional case, proving for a simple manifold configuration that when the network depth $L$ is large relative to certain geometric and statistical properties of the data, the network width $n$ grows as a sufficiently large polynomial in $L$, and the number of i.i.d. samples from the manifolds is polynomial in $L$, randomly-initialized gradient descent rapidly learns to classify the two manifolds perfectly with high probability. Our analysis demonstrates concrete benefits of depth and width in the context of a practically-motivated model problem: the depth acts as a fitting resource, with larger depths corresponding to smoother networks that can more readily separate the class manifolds, and the width acts as a statistical resource, enabling concentration of the randomly-initialized network and its gradients. The argument centers around the neural tangent kernel and its role in the nonasymptotic analysis of training overparameterized neural networks; to this literature, we contribute essentially optimal rates of concentration for the neural tangent kernel of deep fully-connected networks, requiring width $n \gtrsim L\,\mathrm{poly}(d_0)$ to achieve uniform concentration of the initial kernel over a $d_0$-dimensional submanifold of the unit sphere $\mathbb{S}^{n_0-1}$, and a nonasymptotic framework for establishing generalization of networks trained in the NTK regime with structured data. The proof makes heavy use of martingale concentration to optimally treat statistical dependencies across layers of the initial random network. This approach should be of use in establishing similar results for other network architectures.",0
"Our focus is on the multiple manifold problem, which is a binary classification task designed for machine vision applications. The goal is to train a deep fully-connected neural network to distinguish between two low-dimensional submanifolds of the unit sphere. We conducted a one-dimensional analysis and demonstrated that when the network depth $L$ is large compared to certain geometric and statistical properties of the data, the network width $n$ grows as a sufficiently large polynomial in $L$, and the number of i.i.d. samples from the manifolds is polynomial in $L$. With these conditions met, randomly-initialized gradient descent is able to quickly and accurately classify the two manifolds with high probability. Our analysis highlights the advantages of depth and width in this context, with depth providing a fitting resource and width acting as a statistical resource. The neural tangent kernel is central to our argument, and our contribution to the literature includes essentially optimal rates of concentration for the kernel of deep fully-connected networks, requiring width $n \gtrsim L\,\mathrm{poly}(d_0)$ to achieve uniform concentration of the initial kernel over a $d_0$-dimensional submanifold of the unit sphere $\mathbb{S}^{n_0-1}$. Additionally, we establish a nonasymptotic framework for generalizing networks trained in the NTK regime with structured data. Our proof employs martingale concentration to optimally handle statistical dependencies across layers of the initial random network, and this approach could be useful for similar results in other network architectures.",1
"In this paper, we propose a two-stage deep learning framework called VoxelContext-Net for both static and dynamic point cloud compression. Taking advantages of both octree based methods and voxel based schemes, our approach employs the voxel context to compress the octree structured data. Specifically, we first extract the local voxel representation that encodes the spatial neighbouring context information for each node in the constructed octree. Then, in the entropy coding stage, we propose a voxel context based deep entropy model to compress the symbols of non-leaf nodes in a lossless way. Furthermore, for dynamic point cloud compression, we additionally introduce the local voxel representations from the temporal neighbouring point clouds to exploit temporal dependency. More importantly, to alleviate the distortion from the octree construction procedure, we propose a voxel context based 3D coordinate refinement method to produce more accurate reconstructed point cloud at the decoder side, which is applicable to both static and dynamic point cloud compression. The comprehensive experiments on both static and dynamic point cloud benchmark datasets(e.g., ScanNet and Semantic KITTI) clearly demonstrate the effectiveness of our newly proposed method VoxelContext-Net for 3D point cloud geometry compression.",0
"Our paper introduces VoxelContext-Net, a two-stage deep learning framework that compresses both static and dynamic point clouds. We combine octree-based methods and voxel-based schemes to exploit the advantages of each. Our approach uses voxel context to compress octree-structured data. Initially, we extract local voxel representations that encode spatial neighboring context information for each node in the constructed octree. In the entropy coding stage, we propose a voxel context-based deep entropy model to compress the symbols of non-leaf nodes in a lossless way. For dynamic point cloud compression, we introduce local voxel representations from temporal neighboring point clouds to exploit temporal dependency. Additionally, we propose a voxel context-based 3D coordinate refinement method to produce more accurate reconstructed point clouds at the decoder side and alleviate distortion from the octree construction procedure. Experiments on static and dynamic point cloud benchmark datasets support the effectiveness of VoxelContext-Net for 3D point cloud geometry compression.",1
"Graph neural networks (GNNs) have achieved outstanding performance in learning graph-structured data and various tasks. However, many current GNNs suffer from three common problems when facing large-size graphs or using a deeper structure: neighbors explosion, node dependence, and oversmoothing. Such problems attribute to the data structures of the graph itself or the designing of the multi-layers GNNs framework, and can lead to low training efficiency and high space complexity. To deal with these problems, in this paper, we propose a general subgraph-based training framework, namely Ripple Walk Training (RWT), for deep and large graph neural networks. RWT samples subgraphs from the full graph to constitute a mini-batch, and the full GNN is updated based on the mini-batch gradient. We analyze the high-quality subgraphs to train GNNs in a theoretical way. A novel sampling method Ripple Walk Sampler works for sampling these high-quality subgraphs to constitute the mini-batch, which considers both the randomness and connectivity of the graph-structured data. Extensive experiments on different sizes of graphs demonstrate the effectiveness and efficiency of RWT in training various GNNs (GCN & GAT).",0
"Graph neural networks (GNNs) have shown exceptional performance in learning graph-structured data and accomplishing various tasks. However, when dealing with large-sized graphs or deeper structures, many existing GNNs face three common issues: neighbors explosion, node dependence, and oversmoothing. These problems arise from either the graph's data structures or the design of the multi-layer GNNs framework, which can result in low training efficiency and high space complexity. To address these problems, this paper proposes a subgraph-based training framework, called Ripple Walk Training (RWT), for deep and large graph neural networks. RWT selects subgraphs from the full graph to form a mini-batch, and the full GNN is updated based on the mini-batch gradient. The paper analyzes high-quality subgraphs to train GNNs theoretically. A novel sampling method called Ripple Walk Sampler is used to sample these subgraphs, which considers both the randomness and connectivity of the graph-structured data. The effectiveness and efficiency of RWT in training various GNNs (GCN & GAT) are demonstrated through extensive experiments on graphs of different sizes.",1
"Graph neural networks (GNNs) have emerged as the standard method for numerous tasks on graph-structured data such as node classification. However, real-world graphs are often evolving over time and even new classes may arise. We model these challenges as an instance of lifelong learning, in which a learner faces a sequence of tasks and may take over knowledge acquired in past tasks. Such knowledge may be stored explicitly as historic data or implicitly within model parameters. In this work, we systematically analyze the influence of implicit and explicit knowledge. Therefore, we present an incremental training method for lifelong learning on graphs and introduce a new measure based on $k$-neighborhood time differences to address variances in the historic data. We apply our training method to five representative GNN architectures and evaluate them on three new lifelong node classification datasets. Our results show that no more than 50% of the GNN's receptive field is necessary to retain at least 95% accuracy compared to training over the complete history of the graph data. Furthermore, our experiments confirm that implicit knowledge becomes more important when fewer explicit knowledge is available.",0
"Graph neural networks (GNNs) are commonly used for various tasks on graph-structured data, including node classification. However, real-world graphs are often dynamic and may introduce new classes, creating challenges for lifelong learning. In this context, a learner must tackle a sequence of tasks while leveraging the knowledge acquired from the past explicitly or implicitly. In this study, we examine the impact of implicit and explicit knowledge by presenting an incremental training approach for lifelong learning on graphs. We introduce a novel measure based on $k$-neighborhood time differences to address variances in the historic data. We apply our method to five representative GNN architectures and evaluate them on three new lifelong node classification datasets. The results indicate that retaining no more than 50% of the GNN's receptive field is sufficient to achieve at least 95% accuracy, compared to training on the complete history of the graph data. Furthermore, our experiments show that implicit knowledge becomes more critical when explicit knowledge is limited.",1
"\textit{Graph Neural Network} (GNN) is a promising approach for analyzing graph-structured data that tactfully captures their dependency information via node-level message passing. It has achieved state-of-the-art performances in many tasks, such as node classification, graph matching, clustering, and graph generation. As GNNs operate on non-Euclidean data, their irregular data access patterns cause considerable computational costs and overhead on conventional architectures, such as GPU and CPU. Our analysis shows that GNN adopts a hybrid computing model. The \textit{Aggregation} (or \textit{Message Passing}) phase performs vector additions where vectors are fetched with irregular strides. The \textit{Transformation} (or \textit{Node Embedding}) phase can be either dense or sparse-dense matrix multiplication. In this work, We propose \textit{VersaGNN}, an ultra-efficient, systolic-array-based versatile hardware accelerator that unifies dense and sparse matrix multiplication. By applying this single optimized systolic array to both aggregation and transformation phases, we have significantly reduced chip sizes and energy consumption. We then divide the computing engine into blocked systolic arrays to support the \textit{Strassen}'s algorithm for dense matrix multiplication, dramatically scaling down the number of multiplications and enabling high-throughput computation of GNNs. To balance the workload of sparse-dense matrix multiplication, we also introduced a greedy algorithm to combine sparse sub-matrices of compressed format into condensed ones to reduce computational cycles. Compared with current state-of-the-art GNN software frameworks, \textit{VersaGNN} achieves on average 3712$\times$ speedup with 1301.25$\times$ energy reduction on CPU, and 35.4$\times$ speedup with 17.66$\times$ energy reduction on GPU.",0
"The Graph Neural Network (GNN) is a promising approach to analyze data structured as graphs. This method cleverly captures dependency information through node-level message passing and has achieved impressive results in various tasks, including clustering, node classification, graph generation, and graph matching. However, GNNs operate on non-Euclidean data, leading to significant computational costs and overhead on traditional architectures such as GPU and CPU. Our analysis reveals that GNN uses a hybrid computing model, with the Aggregation phase performing vector additions and the Transformation phase involving either dense or sparse-dense matrix multiplication. To address these challenges, we introduce VersaGNN – an ultra-efficient, systolic-array-based hardware accelerator that unifies dense and sparse matrix multiplication. By applying a single optimized systolic array to both phases, we have reduced chip sizes and energy consumption significantly. Furthermore, we have divided the computing engine into blocked systolic arrays to support Strassen's algorithm for dense matrix multiplication, reducing the number of multiplications and enabling high-throughput computation of GNNs. To balance sparse-dense matrix multiplication workload, we have introduced a greedy algorithm to combine sparse sub-matrices of compressed format into condensed ones, reducing computational cycles. Compared to existing GNN software frameworks, VersaGNN achieves average speedups of 3712x with 1301.25x energy reduction on CPU and 35.4x with 17.66x energy reduction on GPU.",1
"Graph neural networks are a popular variant of neural networks that work with graph-structured data. In this work, we consider combining graph neural networks with the energy-based view of Grathwohl et al. (2019) with the aim of obtaining a more robust classifier. We successfully implement this framework by proposing a novel method to ensure generation over features as well as the adjacency matrix and evaluate our method against the standard graph convolutional network (GCN) architecture (Kipf & Welling (2016)). Our approach obtains comparable discriminative performance while improving robustness, opening promising new directions for future research for energy-based graph neural networks.",0
"The focus of this study is on the combination of graph neural networks and the energy-based view introduced by Grathwohl et al. (2019). The goal is to create a more reliable classifier. To achieve this, a new approach is proposed that enables generation over features and the adjacency matrix. The performance of this approach is compared to the standard graph convolutional network (GCN) architecture developed by Kipf and Welling (2016). The findings indicate that our approach achieves similar discriminative performance while enhancing robustness. This study highlights the potential of energy-based graph neural networks and identifies new opportunities for future research.",1
"Network-structured data becomes ubiquitous in daily life and is growing at a rapid pace. It presents great challenges to feature engineering due to the high non-linearity and sparsity of the data. The local and global structure of the real-world networks can be reflected by dynamical transfer behaviors among nodes. This paper proposes a network embedding framework to capture the transfer behaviors on structured networks via deep prediction models. We first design a degree-weight biased random walk model to capture the transfer behaviors on the network. Then a deep network embedding method is introduced to preserve the transfer possibilities among the nodes. A network structure embedding layer is added into conventional deep prediction models, including Long Short-Term Memory Network and Recurrent Neural Network, to utilize the sequence prediction ability. To keep the local network neighborhood, we further perform a Laplacian supervised space optimization on the embedding feature representations. Experimental studies are conducted on various datasets including social networks, citation networks, biomedical network, collaboration network and language network. The results show that the learned representations can be effectively used as features in a variety of tasks, such as clustering, visualization, classification, reconstruction and link prediction, and achieve promising performance compared with state-of-the-arts.",0
"The use of network-structured data has become increasingly common in everyday life and is growing rapidly. However, due to the high non-linearity and sparsity of the data, it poses significant challenges to feature engineering. The transfer behaviors among nodes reflect the local and global structure of real-world networks. To address this, the authors propose a network embedding framework that utilizes deep prediction models to capture transfer behaviors on structured networks. A degree-weight biased random walk model is used to capture the transfer behaviors on the network, and a deep network embedding method is employed to preserve the transfer possibilities among nodes. Additionally, a network structure embedding layer is added to conventional deep prediction models to utilize sequence prediction ability. To maintain the local network neighborhood, the authors perform a Laplacian supervised space optimization on the embedding feature representations. The proposed method is tested on various datasets, and the results demonstrate that the learned representations can be effectively used for clustering, visualization, classification, reconstruction, and link prediction, with comparable performance to state-of-the-art methods.",1
"Efficient video processing is a critical component in many IoMT applications to detect events of interest. Presently, many window optimization techniques have been proposed in event processing with an underlying assumption that the incoming stream has a structured data model. Videos are highly complex due to the lack of any underlying structured data model. Video stream sources such as CCTV cameras and smartphones are resource-constrained edge nodes. At the same time, video content extraction is expensive and requires computationally intensive Deep Neural Network (DNN) models that are primarily deployed at high-end (or cloud) nodes. This paper presents VID-WIN, an adaptive 2-stage allied windowing approach to accelerate video event analytics in an edge-cloud paradigm. VID-WIN runs parallelly across edge and cloud nodes and performs the query and resource-aware optimization for state-based complex event matching. VID-WIN exploits the video content and DNN input knobs to accelerate the video inference process across nodes. The paper proposes a novel content-driven micro-batch resizing, queryaware caching and micro-batch based utility filtering strategy of video frames under resource-constrained edge nodes to improve the overall system throughput, latency, and network usage. Extensive evaluations are performed over five real-world datasets. The experimental results show that VID-WIN video event matching achieves ~2.3X higher throughput with minimal latency and ~99% bandwidth reduction compared to other baselines while maintaining query-level accuracy and resource bounds.",0
"The detection of significant events is crucial in many IoMT applications, and efficient video processing is a vital component in this process. However, due to the lack of a structured data model, video processing is highly complex. Moreover, the resource-constrained edge nodes of video stream sources like CCTV cameras and smartphones make video content extraction challenging. Typically, computationally intensive DNN models are required for this purpose, which are primarily deployed at high-end nodes. To address this challenge, the paper presents VID-WIN, an adaptive 2-stage windowing approach that accelerates video event analytics in an edge-cloud paradigm. VID-WIN performs query and resource-aware optimization for state-based complex event matching and exploits video content and DNN input knobs to improve the video inference process across nodes. The paper proposes a novel content-driven micro-batch resizing, query-aware caching, and micro-batch based utility filtering strategy of video frames under resource-constrained edge nodes to enhance the overall system throughput, latency, and network usage. Extensive evaluations are conducted over five real-world datasets, and the experimental results show that VID-WIN video event matching achieves ~2.3X higher throughput with minimal latency and ~99% bandwidth reduction compared to other baselines while maintaining query-level accuracy and resource bounds.",1
"Graph neural networks (GNNs), an emerging deep learning model class, can extract meaningful representations from highly expressive graph-structured data and are therefore gaining popularity for wider ranges of applications. However, current GNNs suffer from the poor performance of their sparse-dense matrix multiplication (SpMM) operator, even when using powerful GPUs. Our analysis shows that 95% of the inference time could be spent on SpMM when running popular GNN models on NVIDIA's advanced V100 GPU. Such SpMM performance bottleneck hinders GNNs' applicability to large-scale problems or the development of more sophisticated GNN models. To address this inference time bottleneck, we introduce ES-SpMM, a cache-first edge sampling mechanism and codesigned SpMM kernel. ES-SpMM uses edge sampling to downsize the graph to fit into GPU's shared memory. It thus reduces the computation cost and improves SpMM's cache locality. To evaluate ES-SpMM's performance, we integrated it with a popular GNN framework, DGL, and tested it using representative GNN models and datasets. Our results show that ES-SpMM outperforms the highly optimized cuSPARSE SpMM kernel by up to 4.35x with no accuracy loss and by 45.3x with less than a 1% accuracy loss.",0
"Emerging deep learning model class, Graph neural networks (GNNs), are gaining popularity for various applications due to their ability to extract meaningful representations from highly expressive graph-structured data. However, the current GNNs' sparse-dense matrix multiplication (SpMM) operator presents poor performance even when using powerful GPUs, which could consume 95% of inference time. This performance bottleneck limits GNNs' applicability to large-scale problems and hinders the development of more sophisticated GNN models. To address this issue, we propose ES-SpMM, a cache-first edge sampling mechanism and codesigned SpMM kernel that downsizes the graph to fit into GPUs' shared memory. ES-SpMM improves SpMM's cache locality and reduces computation cost. We evaluated ES-SpMM's performance by integrating it with a popular GNN framework, DGL, and testing it with representative GNN models and datasets. Our results show that ES-SpMM outperforms the highly optimized cuSPARSE SpMM kernel by up to 4.35x with no accuracy loss and by 45.3x with less than a 1% accuracy loss.",1
"Tensor networks are a powerful modeling framework developed for computational many-body physics, which have only recently been applied within machine learning. In this work we utilize a uniform matrix product state (u-MPS) model for probabilistic modeling of sequence data. We first show that u-MPS enable sequence-level parallelism, with length-n sequences able to be evaluated in depth O(log n). We then introduce a novel generative algorithm giving trained u-MPS the ability to efficiently sample from a wide variety of conditional distributions, each one defined by a regular expression. Special cases of this algorithm correspond to autoregressive and fill-in-the-blank sampling, but more complex regular expressions permit the generation of richly structured data in a manner that has no direct analogue in neural generative models. Experiments on sequence modeling with synthetic and real text data show u-MPS outperforming a variety of baselines and effectively generalizing their predictions in the presence of limited data.",0
"Tensor networks, which were developed for computational many-body physics, have recently found application in machine learning. In this study, we use a uniform matrix product state (u-MPS) model to probabilistically model sequence data. Our research demonstrates that u-MPS facilitate sequence-level parallelism, with length-n sequences able to be assessed in depth O(log n). Furthermore, we introduce a new generative algorithm that enables trained u-MPS to sample efficiently from a diverse range of conditional distributions, each defined by a regular expression. While some of these distributions correspond to autoregressive and fill-in-the-blank sampling, more intricate regular expressions permit the generation of complexly structured data in a way that is not directly comparable to neural generative models. Synthetic and real text data experiments show that u-MPS outperform various benchmarks and effectively generalize predictions when data is limited.",1
"Bayesian optimization (BO) is a popular paradigm for global optimization of expensive black-box functions, but there are many domains where the function is not completely black-box. The data may have some known structure, e.g. symmetries, and the data generation process can yield useful intermediate or auxiliary information in addition to the value of the optimization objective. However, surrogate models traditionally employed in BO, such as Gaussian Processes (GPs), scale poorly with dataset size and struggle to incorporate known structure or auxiliary information. Instead, we propose performing BO on complex, structured problems by using Bayesian Neural Networks (BNNs), a class of scalable surrogate models that have the representation power and flexibility to handle structured data and exploit auxiliary information. We demonstrate BO on a number of realistic problems in physics and chemistry, including topology optimization of photonic crystal materials using convolutional neural networks, and chemical property optimization of molecules using graph neural networks. On these complex tasks, we show that BNNs often outperform GPs as surrogate models for BO in terms of both sampling efficiency and computational cost.",0
"Although Bayesian optimization (BO) is widely used for optimizing expensive black-box functions, there are domains where the function is not entirely black-box, and there may be known structures in the data, such as symmetries. Furthermore, the data generation process can provide auxiliary information that is beneficial in addition to the optimization objective. However, Gaussian Processes (GPs), which are commonly used as surrogate models in BO, struggle to integrate known structures or auxiliary information and do not scale well with large datasets. To address this issue, we suggest using Bayesian Neural Networks (BNNs), a class of surrogate models that are scalable, flexible, and can handle structured data while exploiting auxiliary information. We illustrate the effectiveness of BNNs on complex optimization tasks in physics and chemistry, including the use of convolutional neural networks for topology optimization of photonic crystal materials and graph neural networks for chemical property optimization of molecules. Our results show that BNNs often outperform GPs in terms of both sampling efficiency and computational cost.",1
"The mean shift (MS) algorithm is a nonparametric method used to cluster sample points and find the local modes of kernel density estimates, using an idea based on iterative gradient ascent. In this paper we develop a mean-shift-inspired algorithm to estimate the modes of regression functions and partition the sample points in the input space. We prove convergence of the sequences generated by the algorithm and derive the non-asymptotic rates of convergence of the estimated local modes for the underlying regression model. We also demonstrate the utility of the algorithm for data-enabled discovery through an application on biomolecular structure data. An extension to subspace constrained mean shift (SCMS) algorithm used to extract ridges of regression functions is briefly discussed.",0
"The nonparametric method known as the mean shift (MS) algorithm utilizes iterative gradient ascent to cluster sample points and locate local modes of kernel density estimates. This paper introduces a mean-shift-based algorithm that estimates regression function modes and partitions sample points in the input space. Our algorithm's convergence is proven, and we derive the non-asymptotic rates of convergence for the estimated local modes of the underlying regression model. Additionally, we highlight the algorithm's usefulness in data-enabled discovery by applying it to biomolecular structure data. We briefly touch on the extension of the subspace constrained mean shift (SCMS) algorithm, which is used to extract ridges of regression functions.",1
"Recently, there has been great success in applying deep neural networks on graph structured data. Most work, however, focuses on either node- or graph-level supervised learning, such as node, link or graph classification or node-level unsupervised learning (e.g. node clustering). Despite its wide range of possible applications, graph-level unsupervised learning has not received much attention yet. This might be mainly attributed to the high representation complexity of graphs, which can be represented by n! equivalent adjacency matrices, where n is the number of nodes. In this work we address this issue by proposing a permutation-invariant variational autoencoder for graph structured data. Our proposed model indirectly learns to match the node ordering of input and output graph, without imposing a particular node ordering or performing expensive graph matching. We demonstrate the effectiveness of our proposed model on various graph reconstruction and generation tasks and evaluate the expressive power of extracted representations for downstream graph-level classification and regression.",0
"Applying deep neural networks on graph structured data has seen significant success lately. However, most of the research has concentrated on supervised learning at either the node or graph level, with little attention paid to graph-level unsupervised learning. The reasoning behind this could be attributed to the complexity of graph representation, which can have n! equivalent adjacency matrices, where n represents the number of nodes. In this study, we propose a permutation-invariant variational autoencoder to address this issue. Our model indirectly learns to match the node ordering of input and output graphs, without imposing a particular node ordering or performing expensive graph matching. We demonstrate the effectiveness of our model on various graph reconstruction and generation tasks and evaluate the expressive power of the extracted representations for downstream graph-level classification and regression.",1
"Graph neural networks (GNNs) have been successfully applied in many structured data domains, with applications ranging from molecular property prediction to the analysis of social networks. Motivated by the broad applicability of GNNs, we propose the family of so-called RankGNNs, a combination of neural Learning to Rank (LtR) methods and GNNs. RankGNNs are trained with a set of pair-wise preferences between graphs, suggesting that one of them is preferred over the other. One practical application of this problem is drug screening, where an expert wants to find the most promising molecules in a large collection of drug candidates. We empirically demonstrate that our proposed pair-wise RankGNN approach either significantly outperforms or at least matches the ranking performance of the naive point-wise baseline approach, in which the LtR problem is solved via GNN-based graph regression.",0
"Numerous fields that involve structured data have successfully employed Graph neural networks (GNNs), such as molecular property prediction and social network analysis. Our motivation to extend the use of GNNs has led us to introduce RankGNNs, which are a fusion of GNNs and neural Learning to Rank (LtR) methods. RankGNNs are trained using pair-wise preferences between graphs, where one graph is preferred over the other. This technique is useful in drug screening, where experts search for the most promising molecules in a vast collection of drug candidates. Our empirical findings demonstrate that our RankGNN approach is superior to or at least equivalent to the naive point-wise baseline approach, which uses GNN-based graph regression to solve the LtR problem.",1
"Graph neural networks (GNN) have been proven to be mature enough for handling graph-structured data on node-level graph representation learning tasks. However, the graph pooling technique for learning expressive graph-level representation is critical yet still challenging. Existing pooling methods either struggle to capture the local substructure or fail to effectively utilize high-order dependency, thus diminishing the expression capability. In this paper we propose HAP, a hierarchical graph-level representation learning framework, which is adaptively sensitive to graph structures, i.e., HAP clusters local substructures incorporating with high-order dependencies. HAP utilizes a novel cross-level attention mechanism MOA to naturally focus more on close neighborhood while effectively capture higher-order dependency that may contain crucial information. It also learns a global graph content GCont that extracts the graph pattern properties to make the pre- and post-coarsening graph content maintain stable, thus providing global guidance in graph coarsening. This novel innovation also facilitates generalization across graphs with the same form of features. Extensive experiments on fourteen datasets show that HAP significantly outperforms twelve popular graph pooling methods on graph classification task with an maximum accuracy improvement of 22.79%, and exceeds the performance of state-of-the-art graph matching and graph similarity learning algorithms by over 3.5% and 16.7%.",0
"The effectiveness of graph neural networks (GNN) in node-level graph representation learning tasks has been well-established. However, the challenge lies in developing graph pooling techniques that can enable the learning of expressive graph-level representation. Existing pooling methods either fail to capture the local substructure or are unable to effectively utilize high-order dependency, thereby limiting their expression capability. This paper proposes a novel framework called HAP, which is a hierarchical graph-level representation learning technique that is sensitive to graph structures. HAP clusters local substructures while incorporating high-order dependencies, utilizing a cross-level attention mechanism MOA that focuses on close neighborhoods and captures higher-order dependency. HAP also learns a global graph content GCont that extracts graph pattern properties to maintain stable pre- and post-coarsening graph content, facilitating generalization across graphs with similar features. Extensive experiments on fourteen datasets demonstrate that HAP outperforms twelve popular graph pooling methods on graph classification tasks, with a maximum accuracy improvement of 22.79%. Additionally, HAP exceeds the performance of state-of-the-art graph matching and graph similarity learning algorithms by over 3.5% and 16.7%, respectively.",1
"In recent years, graph neural networks (GNNs) have been widely adopted in the representation learning of graph-structured data and provided state-of-the-art performance in various applications such as link prediction, node classification, and recommendation. Motivated by recent advances of self-supervision for representation learning in natural language processing and computer vision, self-supervised learning has been recently studied to leverage unlabeled graph-structured data. However, employing self-supervision tasks as auxiliary tasks to assist a primary task has been less explored in the literature on graphs. In this paper, we propose a novel self-supervised auxiliary learning framework to effectively learn graph neural networks. Moreover, this work is the first study showing that a meta-path prediction is beneficial as a self-supervised auxiliary task for heterogeneous graphs. Our method is learning to learn a primary task with various auxiliary tasks to improve generalization performance. The proposed method identifies an effective combination of auxiliary tasks and automatically balances them to improve the primary task. Our methods can be applied to any graph neural network in a plug-in manner without manual labeling or additional data. Also, it can be extended to any other auxiliary tasks. Our experiments demonstrate that the proposed method consistently improves the performance of node classification and link prediction.",0
"Graph neural networks (GNNs) have become increasingly popular in recent years for representation learning of graph-structured data, achieving state-of-the-art results in applications like link prediction, node classification, and recommendation. While self-supervision has been a successful technique for representation learning in natural language processing and computer vision, its use in unlabeled graph-structured data is still underexplored. In this paper, we present a novel self-supervised auxiliary learning framework for effective GNN learning. Our method is the first to demonstrate that meta-path prediction is a useful self-supervised auxiliary task for heterogeneous graphs. We introduce a learning-to-learn approach, utilizing various auxiliary tasks to improve generalization performance and automatically balancing them to enhance the primary task. Our method is plug-in and does not require manual labeling or extra data. Our experiments show consistent improvement in node classification and link prediction performance.",1
"Graph Neural Networks (GNNs) have proved to be an effective representation learning framework for graph-structured data, and have achieved state-of-the-art performance on many practical predictive tasks, such as node classification, link prediction and graph classification. Among the variants of GNNs, Graph Attention Networks (GATs) learn to assign dense attention coefficients over all neighbors of a node for feature aggregation, and improve the performance of many graph learning tasks. However, real-world graphs are often very large and noisy, and GATs are prone to overfitting if not regularized properly. Even worse, the local aggregation mechanism of GATs may fail on disassortative graphs, where nodes within local neighborhood provide more noise than useful information for feature aggregation. In this paper, we propose Sparse Graph Attention Networks (SGATs) that learn sparse attention coefficients under an $L_0$-norm regularization, and the learned sparse attentions are then used for all GNN layers, resulting in an edge-sparsified graph. By doing so, we can identify noisy/task-irrelevant edges, and thus perform feature aggregation on most informative neighbors. Extensive experiments on synthetic and real-world graph learning benchmarks demonstrate the superior performance of SGATs. In particular, SGATs can remove about 50\%-80\% edges from large assortative graphs, while retaining similar classification accuracies. On disassortative graphs, SGATs prune majority of noisy edges and outperform GATs in classification accuracies by significant margins. Furthermore, the removed edges can be interpreted intuitively and quantitatively. To the best of our knowledge, this is the first graph learning algorithm that shows significant redundancies in graphs and edge-sparsified graphs can achieve similar or sometimes higher predictive performances than original graphs.",0
"The effectiveness of Graph Neural Networks (GNNs) in representing graph-structured data and achieving state-of-the-art performance in node classification, link prediction, and graph classification tasks has been established. Graph Attention Networks (GATs), a variant of GNNs, have gained popularity due to their dense attention coefficient assignment for feature aggregation. However, GATs are prone to overfitting and may not perform well on noisy and large graphs. Additionally, they may not work well on disassortative graphs where local neighborhoods provide more noise than useful information. To address these issues, this paper introduces Sparse Graph Attention Networks (SGATs), which use $L_0$-norm regularization to learn sparse attention coefficients. SGATs identify noisy/task-irrelevant edges and perform feature aggregation on the most informative neighbors, resulting in edge-sparsified graphs. The experiments conducted on synthetic and real-world graph learning benchmarks demonstrate the superior performance of SGATs, especially on disassortative graphs. SGATs can remove a significant number of edges from assortative graphs while maintaining similar classification accuracies. The removed edges can be interpreted intuitively and quantitatively. This is the first graph learning algorithm that shows significant redundancies in graphs, and edge-sparsified graphs can achieve similar or better predictive performances than original graphs.",1
"Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics systems, learning molecular fingerprints, predicting protein interface, and classifying diseases demand a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures (like the dependency trees of sentences and the scene graphs of images) is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are neural models that capture the dependence of graphs via message passing between the nodes of graphs. In recent years, variants of GNNs such as graph convolutional network (GCN), graph attention network (GAT), graph recurrent network (GRN) have demonstrated ground-breaking performances on many deep learning tasks. In this survey, we propose a general design pipeline for GNN models and discuss the variants of each component, systematically categorize the applications, and propose four open problems for future research.",0
"There is a high demand for models that can effectively learn from graph data, which contains extensive relation information among its elements. Applications such as modeling physics systems, learning molecular fingerprints, predicting protein interfaces, and classifying diseases require such models. Additionally, in domains where non-structural data like texts and images are used, reasoning on extracted structures, such as dependency trees and scene graphs, is important and also requires graph reasoning models. Graph neural networks (GNNs) are neural models that utilize message passing between nodes to capture graph dependencies. Recently, variants of GNNs, including graph convolutional network (GCN), graph attention network (GAT), and graph recurrent network (GRN), have demonstrated exceptional performances on various deep learning tasks. This survey presents a general design pipeline for GNN models, discusses the different variants of each component, categorizes the applications systematically, and identifies four open problems for future research.",1
"Deep learning has recently demonstrated its promising performance for vision-based parking-slot detection. However, very few existing methods explicitly take into account learning the link information of the marking-points, resulting in complex post-processing and erroneous detection. In this paper, we propose an attentional graph neural network based parking-slot detection method, which refers the marking-points in an around-view image as graph-structured data and utilize graph neural network to aggregate the neighboring information between marking-points. Without any manually designed post-processing, the proposed method is end-to-end trainable. Extensive experiments have been conducted on public benchmark dataset, where the proposed method achieves state-of-the-art accuracy. Code is publicly available at \url{https://github.com/Jiaolong/gcn-parking-slot}.",0
"The effectiveness of vision-based parking-slot detection has been enhanced by recent advancements in deep learning. However, few of the current techniques incorporate the interconnection of marking-points, leading to the need for complex post-processing and incorrect detection. This article presents a parking-slot detection approach based on an attentional graph neural network. The method represents the marking-points in an around-view image as graph-structured data and employs a graph neural network to collect information from neighboring marking-points. The proposed method is end-to-end trainable and does not require any manual post-processing. Benchmark tests have demonstrated that the approach achieves state-of-the-art precision, and the code is openly available at \url{https://github.com/Jiaolong/gcn-parking-slot}.",1
"Generalizable, transferrable, and robust representation learning on graph-structured data remains a challenge for current graph neural networks (GNNs). Unlike what has been developed for convolutional neural networks (CNNs) for image data, self-supervised learning and pre-training are less explored for GNNs. In this paper, we propose a graph contrastive learning (GraphCL) framework for learning unsupervised representations of graph data. We first design four types of graph augmentations to incorporate various priors. We then systematically study the impact of various combinations of graph augmentations on multiple datasets, in four different settings: semi-supervised, unsupervised, and transfer learning as well as adversarial attacks. The results show that, even without tuning augmentation extents nor using sophisticated GNN architectures, our GraphCL framework can produce graph representations of similar or better generalizability, transferrability, and robustness compared to state-of-the-art methods. We also investigate the impact of parameterized graph augmentation extents and patterns, and observe further performance gains in preliminary experiments. Our codes are available at https://github.com/Shen-Lab/GraphCL.",0
"Current graph neural networks (GNNs) face challenges in achieving generalizable, transferrable, and robust representation learning on graph-structured data. Unlike convolutional neural networks (CNNs) for image data, self-supervised learning and pre-training have received less attention for GNNs. In this paper, we introduce a framework called Graph Contrastive Learning (GraphCL) to learn unsupervised representations of graph data. We incorporate four types of graph augmentations to incorporate various priors and systematically study their impact on multiple datasets in four different settings, including semi-supervised, unsupervised, transfer learning, and adversarial attacks. Our results demonstrate that even without tuning augmentation extents nor using sophisticated GNN architectures, our GraphCL framework can produce graph representations with similar or better generalizability, transferrability, and robustness compared to state-of-the-art methods. We also observe further performance gains in preliminary experiments by investigating the impact of parameterized graph augmentation extents and patterns. Our codes are available at https://github.com/Shen-Lab/GraphCL.",1
"Generative modeling of set-structured data, such as point clouds, requires reasoning over local and global structures at various scales. However, adopting multi-scale frameworks for ordinary sequential data to a set-structured data is nontrivial as it should be invariant to the permutation of its elements. In this paper, we propose SetVAE, a hierarchical variational autoencoder for sets. Motivated by recent progress in set encoding, we build SetVAE upon attentive modules that first partition the set and project the partition back to the original cardinality. Exploiting this module, our hierarchical VAE learns latent variables at multiple scales, capturing coarse-to-fine dependency of the set elements while achieving permutation invariance. We evaluate our model on point cloud generation task and achieve competitive performance to the prior arts with substantially smaller model capacity. We qualitatively demonstrate that our model generalizes to unseen set sizes and learns interesting subset relations without supervision. Our implementation is available at https://github.com/jw9730/setvae.",0
"To generate models for set-structured data, such as point clouds, it is necessary to consider both local and global structures at different scales. However, adapting multi-scale frameworks for sequential data to set-structured data is difficult because the model must be invariant to the permutation of elements. This paper introduces SetVAE, a hierarchical variational autoencoder for sets. SetVAE is built using attentive modules that partition the set and project it back to its original cardinality. With this module, our hierarchical VAE learns latent variables at multiple scales, capturing the dependencies of the set elements from coarse to fine, while maintaining permutation invariance. We evaluated our model on point cloud generation tasks and achieved competitive performance compared to previous models with much smaller model capacity. We also demonstrated that our model generalizes to unseen set sizes and learns interesting subset relationships without supervision. Our implementation is available at https://github.com/jw9730/setvae.",1
"Graph Neural Networks (GNNs) draw their strength from explicitly modeling the topological information of structured data. However, existing GNNs suffer from limited capability in capturing the hierarchical graph representation which plays an important role in graph classification. In this paper, we innovatively propose hierarchical graph capsule network (HGCN) that can jointly learn node embeddings and extract graph hierarchies. Specifically, disentangled graph capsules are established by identifying heterogeneous factors underlying each node, such that their instantiation parameters represent different properties of the same entity. To learn the hierarchical representation, HGCN characterizes the part-whole relationship between lower-level capsules (part) and higher-level capsules (whole) by explicitly considering the structure information among the parts. Experimental studies demonstrate the effectiveness of HGCN and the contribution of each component.",0
"The effectiveness of Graph Neural Networks (GNNs) lies in their ability to explicitly model the topological information of structured data. However, their capability in capturing the hierarchical graph representation - a crucial aspect of graph classification - is limited. To address this issue, we propose the hierarchical graph capsule network (HGCN). HGCN is able to learn node embeddings and extract graph hierarchies simultaneously through disentangled graph capsules. These capsules represent different properties of the same entity and are established by identifying heterogeneous factors underlying each node. To learn the hierarchical representation, HGCN considers the structure information among the parts and characterizes the part-whole relationship between lower-level capsules (part) and higher-level capsules (whole). Experimental studies confirm the effectiveness of HGCN and the contribution of each component.",1
"Graph Neural Networks (GNNs) have attracted increasing attention due to its successful applications on various graph-structure data. However, recent studies have shown that adversarial attacks are threatening the functionality of GNNs. Although numerous works have been proposed to defend adversarial attacks from various perspectives, most of them can be robust against the attacks only on specific scenarios. To address this shortage of robust generalization, we propose to defend the adversarial attacks on GNN through applying the Spatio-Temporal sparsification (called ST-Sparse) on the GNN hidden node representation. ST-Sparse is similar to the Dropout regularization in spirit. Through intensive experiment evaluation with GCN as the target GNN model, we identify the benefits of ST-Sparse as follows: (1) ST-Sparse shows the defense performance improvement in most cases, as it can effectively increase the robust accuracy by up to 6\% improvement; (2) ST-Sparse illustrates its robust generalization capability by integrating with the existing defense methods, similar to the integration of Dropout into various deep learning models as a standard regularization technique; (3) ST-Sparse also shows its ordinary generalization capability on clean datasets, in that ST-SparseGCN (the integration of ST-Sparse and the original GCN) even outperform the original GCN, while the other three representative defense methods are inferior to the original GCN.",0
"The successful application of Graph Neural Networks (GNNs) to various graph-structure data has led to increased attention, but recent research has shown that adversarial attacks can threaten their functionality. While several methods have been proposed to defend against attacks, most are only effective in specific scenarios, leaving a gap in robust generalization. To address this issue, we propose using Spatio-Temporal sparsification, known as ST-Sparse, to defend against adversarial attacks by applying it to the GNN hidden node representation. ST-Sparse is similar to Dropout regularization and, through experiments using GCN as the target GNN model, we found that it improves defense performance in most cases, with up to a 6% improvement in robust accuracy. It also demonstrates robust generalization by integrating with existing defense methods and shows ordinary generalization on clean datasets. In fact, the integration of ST-Sparse and the original GCN even outperforms the original GCN, while other defense methods are inferior.",1
"Heart failure (HF) is a major cause of mortality. Accurately monitoring HF progress and adjust therapies are critical for improving patient outcomes. An experienced cardiologist can make accurate HF stage diagnoses based on combination of symptoms, signs, and lab results from the electronic health records (EHR) of a patient, without directly measuring heart function. We examined whether machine learning models, more specifically the XGBoost model, can accurately predict patient stage based on EHR, and we further applied the SHapley Additive exPlanations (SHAP) framework to identify informative features and their interpretations. Our results indicate that based on structured data from EHR, our models could predict patients' ejection fraction (EF) scores with moderate accuracy. SHAP analyses identified informative features and revealed potential clinical subtypes of HF. Our findings provide insights on how to design computing systems to accurately monitor disease progression of HF patients through continuously mining patients' EHR data.",0
"The identification and management of heart failure (HF) is crucial in improving patient outcomes as it is a significant contributor to mortality. Experienced cardiologists can diagnose the stage of HF by analyzing a combination of symptoms, signs, and lab results from the electronic health records (EHR) of a patient, without directly measuring heart function. In this study, we explored the accuracy of machine learning models, specifically the XGBoost model, in predicting patient stage using EHR data. We also utilized the SHapley Additive exPlanations (SHAP) framework to identify informative features and their interpretations. Our results indicate that our models can moderately predict patients' ejection fraction (EF) scores using structured data from EHR. The SHAP analyses revealed informative features and potential clinical subtypes of HF. These findings can aid in designing computing systems to accurately monitor disease progression of HF patients by continuously mining EHR data.",1
"The emergence of Graph Convolutional Network (GCN) has greatly boosted the progress of graph learning. However, two disturbing factors, noise and redundancy in graph data, and lack of interpretation for prediction results, impede further development of GCN. One solution is to recognize a predictive yet compressed subgraph to get rid of the noise and redundancy and obtain the interpretable part of the graph. This setting of subgraph is similar to the information bottleneck (IB) principle, which is less studied on graph-structured data and GCN. Inspired by the IB principle, we propose a novel subgraph information bottleneck (SIB) framework to recognize such subgraphs, named IB-subgraph. However, the intractability of mutual information and the discrete nature of graph data makes the objective of SIB notoriously hard to optimize. To this end, we introduce a bilevel optimization scheme coupled with a mutual information estimator for irregular graphs. Moreover, we propose a continuous relaxation for subgraph selection with a connectivity loss for stabilization. We further theoretically prove the error bound of our estimation scheme for mutual information and the noise-invariant nature of IB-subgraph. Extensive experiments on graph learning and large-scale point cloud tasks demonstrate the superior property of IB-subgraph.",0
"The development of Graph Convolutional Network (GCN) has significantly advanced graph learning, but two obstacles remain: noisy and redundant graph data, and the inability to interpret prediction results. To overcome these challenges, a solution is to identify a compressed subgraph that eliminates noise and redundancy, yielding an interpretable portion of the graph. This subgraph setting resembles the information bottleneck (IB) principle, which has received less attention in the context of graph-structured data and GCN. Drawing inspiration from IB, a subgraph information bottleneck (SIB) framework is proposed to recognize such subgraphs, termed IB-subgraph. However, optimizing SIB poses difficulties due to the intractability of mutual information and the discrete nature of graph data. To address this, a bilevel optimization scheme coupled with a mutual information estimator is introduced for irregular graphs. Additionally, a continuous relaxation for subgraph selection with a connectivity loss is proposed for stabilization. The estimation scheme for mutual information is theoretically proven to have an error bound, and IB-subgraph is shown to be noise-invariant. Extensive experiments on graph learning and large-scale point cloud tasks demonstrate the superior performance of IB-subgraph.",1
"Random forests on the one hand, and neural networks on the other hand, have met great success in the machine learning community for their predictive performance. Combinations of both have been proposed in the literature, notably leading to the so-called deep forests (DF) (Zhou \& Feng,2019). In this paper, our aim is not to benchmark DF performances but to investigate instead their underlying mechanisms. Additionally, DF architecture can be generally simplified into more simple and computationally efficient shallow forest networks. Despite some instability, the latter may outperform standard predictive tree-based methods. We exhibit a theoretical framework in which a shallow tree network is shown to enhance the performance of classical decision trees. In such a setting, we provide tight theoretical lower and upper bounds on its excess risk. These theoretical results show the interest of tree-network architectures for well-structured data provided that the first layer, acting as a data encoder, is rich enough.",0
"The machine learning community has found great success in utilizing random forests and neural networks for their predictive capabilities. A combination of these two methods, known as deep forests (DF), has been proposed in previous literature. However, our focus in this paper is not to compare DF performance, but rather to examine the underlying mechanisms. Furthermore, the DF architecture can be simplified into more efficient shallow forest networks, which may outperform traditional tree-based methods despite some instability. We present a theoretical framework that demonstrates how a shallow tree network can improve the performance of classical decision trees, along with tight theoretical lower and upper bounds on its excess risk. These results highlight the potential benefits of tree-network architectures for well-structured data, provided that the initial layer functions as a sufficiently rich data encoder.",1
"Graph-structured data are ubiquitous. However, graphs encode diverse types of information and thus play different roles in data representation. In this paper, we distinguish the \textit{representational} and the \textit{correlational} roles played by the graphs in node-level prediction tasks, and we investigate how Graph Neural Network (GNN) models can effectively leverage both types of information. Conceptually, the representational information provides guidance for the model to construct better node features; while the correlational information indicates the correlation between node outcomes conditional on node features. Through a simulation study, we find that many popular GNN models are incapable of effectively utilizing the correlational information. By leveraging the idea of the copula, a principled way to describe the dependence among multivariate random variables, we offer a general solution. The proposed Copula Graph Neural Network (CopulaGNN) can take a wide range of GNN models as base models and utilize both representational and correlational information stored in the graphs. Experimental results on two types of regression tasks verify the effectiveness of the proposed method.",0
"The usage of graph-structured data is widespread, but the information they contain can vary, leading to different roles in data representation. Within this article, we explore the two distinct roles that graphs play in node-level prediction tasks: representational and correlational. We also examine how Graph Neural Network (GNN) models can utilize both types of information effectively. The representational information guides the model to construct better node features, while the correlational information indicates the correlation between node outcomes and node features. Many popular GNN models have difficulty utilizing the correlational information, as we discovered through a simulation study. To address this, we propose the Copula Graph Neural Network (CopulaGNN), which can use a variety of GNN models as base models and leverage both the representational and correlational information stored in the graphs. Our experimental results on two types of regression tasks confirm the effectiveness of our proposed method.",1
"Graph convolutional neural networks (GCNNs) have received much attention recently, owing to their capability in handling graph-structured data. Among the existing GCNNs, many methods can be viewed as instances of a neural message passing motif; features of nodes are passed around their neighbors, aggregated and transformed to produce better nodes' representations. Nevertheless, these methods seldom use node transition probabilities, a measure that has been found useful in exploring graphs. Furthermore, when the transition probabilities are used, their transition direction is often improperly considered in the feature aggregation step, resulting in an inefficient weighting scheme. In addition, although a great number of GCNN models with increasing level of complexity have been introduced, the GCNNs often suffer from over-fitting when being trained on small graphs. Another issue of the GCNNs is over-smoothing, which tends to make nodes' representations indistinguishable. This work presents a new method to improve the message passing process based on node transition probabilities by properly considering the transition direction, leading to a better weighting scheme in nodes' features aggregation compared to the existing counterpart. Moreover, we propose a novel regularization method termed DropNode to address the over-fitting and over-smoothing issues simultaneously. DropNode randomly discards part of a graph, thus it creates multiple deformed versions of the graph, leading to data augmentation regularization effect. Additionally, DropNode lessens the connectivity of the graph, mitigating the effect of over-smoothing in deep GCNNs. Extensive experiments on eight benchmark datasets for node and graph classification tasks demonstrate the effectiveness of the proposed methods in comparison with the state of the art.",0
"Recently, Graph Convolutional Neural Networks (GCNNs) have become a popular choice for handling graph-structured data. These networks use a neural message passing motif to pass features between nodes, which are aggregated and transformed to improve the nodes' representations. However, many existing GCNNs do not consider node transition probabilities, which are useful for exploring graphs. Even when transition probabilities are used, the weighting scheme for feature aggregation is often inefficient due to improper consideration of transition direction. Additionally, GCNNs tend to suffer from over-fitting when trained on small graphs, and over-smoothing, which makes nodes' representations indistinguishable. This study introduces a new method that properly considers transition direction to improve the message passing process based on node transition probabilities. A novel regularization method called DropNode is also proposed to address over-fitting and over-smoothing issues simultaneously. DropNode randomly discards part of a graph, creating multiple deformed versions of the graph to improve data augmentation regularization. Furthermore, it lessens graph connectivity to mitigate over-smoothing in deep GCNNs. The proposed methods are evaluated on eight benchmark datasets for node and graph classification tasks, and the results show their effectiveness compared to state-of-the-art approaches.",1
"Graph neural networks (GNNs) have demonstrated strong performance on a wide variety of tasks due to their ability to model non-uniform structured data. Despite their promise, there exists little research exploring methods to make them more efficient at inference time. In this work, we explore the viability of training quantized GNNs, enabling the usage of low precision integer arithmetic during inference. We identify the sources of error that uniquely arise when attempting to quantize GNNs, and propose an architecturally-agnostic method, Degree-Quant, to improve performance over existing quantization-aware training baselines commonly used on other architectures, such as CNNs. We validate our method on six datasets and show, unlike previous attempts, that models generalize to unseen graphs. Models trained with Degree-Quant for INT8 quantization perform as well as FP32 models in most cases; for INT4 models, we obtain up to 26% gains over the baselines. Our work enables up to 4.7x speedups on CPU when using INT8 arithmetic.",0
"Due to their capacity to model non-uniform structured data, graph neural networks (GNNs) have exhibited impressive performance across a diverse range of tasks. However, research on enhancing their efficiency during inference has been limited. This study investigates the possibility of training quantized GNNs by utilizing low precision integer arithmetic during inference. We identify the unique sources of error that arise when quantizing GNNs and propose an architecturally-agnostic technique, Degree-Quant, to improve performance compared to existing quantization-aware training baselines commonly applied to other architectures, such as CNNs. Our approach is validated on six datasets, demonstrating that models trained with Degree-Quant for INT8 quantization perform on par with FP32 models in most scenarios, and INT4 models yield up to 26% improvements over the baselines. Our findings enable up to 4.7x speedups on CPU when using INT8 arithmetic and demonstrate that models generalize to unseen graphs.",1
"In this thesis, we develop various techniques for working with sets in machine learning. Each input or output is not an image or a sequence, but a set: an unordered collection of multiple objects, each object described by a feature vector. Their unordered nature makes them suitable for modeling a wide variety of data, ranging from objects in images to point clouds to graphs. Deep learning has recently shown great success on other types of structured data, so we aim to build the necessary structures for sets into deep neural networks.   The first focus of this thesis is the learning of better set representations (sets as input). Existing approaches have bottlenecks that prevent them from properly modeling relations between objects within the set. To address this issue, we develop a variety of techniques for different scenarios and show that alleviating the bottleneck leads to consistent improvements across many experiments.   The second focus of this thesis is the prediction of sets (sets as output). Current approaches do not take the unordered nature of sets into account properly. We determine that this results in a problem that causes discontinuity issues with many set prediction tasks and prevents them from learning some extremely simple datasets. To avoid this problem, we develop two models that properly take the structure of sets into account. Various experiments show that our set prediction techniques can significantly benefit over existing approaches.",0
"This thesis explores techniques for working with sets in machine learning, where each input or output is an unordered collection of objects described by feature vectors. Sets are suitable for modeling diverse data, such as objects in images, point clouds, and graphs. The goal is to incorporate set structures into deep neural networks to achieve success similar to deep learning on other types of structured data. The first part of the thesis focuses on improving set representations by developing techniques that address bottlenecks that prevent proper modeling of object relations within sets. The second part addresses the issue of predicting sets as output, which is complicated by the unordered nature of sets. To overcome this problem, two models are developed that take set structure into account. The experiments conducted demonstrate that these set prediction techniques offer significant benefits over existing approaches.",1
"An increasing number of machine learning tasks deal with learning representations from set-structured data. Solutions to these problems involve the composition of permutation-equivariant modules (e.g., self-attention, or individual processing via feed-forward neural networks) and permutation-invariant modules (e.g., global average pooling, or pooling by multi-head attention). In this paper, we propose a geometrically-interpretable framework for learning representations from set-structured data, which is rooted in the optimal mass transportation problem. In particular, we treat elements of a set as samples from a probability measure and propose an exact Euclidean embedding for Generalized Sliced Wasserstein (GSW) distances to learn from set-structured data effectively. We evaluate our proposed framework on multiple supervised and unsupervised set learning tasks and demonstrate its superiority over state-of-the-art set representation learning approaches.",0
"The number of machine learning tasks involving set-structured data is increasing, and the solutions require a combination of permutation-equivariant modules (such as self-attention or individual processing using feed-forward neural networks) and permutation-invariant modules (such as global average pooling or pooling using multi-head attention). This paper presents a framework for learning representations from set-structured data that is based on the optimal mass transportation problem and is interpretable geometrically. We view elements of a set as samples from a probability measure and propose an exact Euclidean embedding for Generalized Sliced Wasserstein (GSW) distances to effectively learn from set-structured data. We compare our approach to state-of-the-art set representation learning methods on multiple supervised and unsupervised set learning tasks and demonstrate its superior performance.",1
"Graph neural networks (GNNs) have achieved state-of-the-art performance for node classification on graphs. The vast majority of existing works assume that genuine node labels are always provided for training. However, there has been very little research effort on how to improve the robustness of GNNs in the presence of label noise. Learning with label noise has been primarily studied in the context of image classification, but these techniques cannot be directly applied to graph-structured data, due to two major challenges -- label sparsity and label dependency -- faced by learning on graphs. In this paper, we propose a new framework, UnionNET, for learning with noisy labels on graphs under a semi-supervised setting. Our approach provides a unified solution for robustly training GNNs and performing label correction simultaneously. The key idea is to perform label aggregation to estimate node-level class probability distributions, which are used to guide sample reweighting and label correction. Compared with existing works, UnionNET has two appealing advantages. First, it requires no extra clean supervision, or explicit estimation of the noise transition matrix. Second, a unified learning framework is proposed to robustly train GNNs in an end-to-end manner. Experimental results show that our proposed approach: (1) is effective in improving model robustness against different types and levels of label noise; (2) yields significant improvements over state-of-the-art baselines.",0
"Node classification on graphs has been dominated by Graph neural networks (GNNs) due to their state-of-the-art performance. However, current research primarily assumes that genuine node labels are provided during training, and there has been little emphasis on improving the robustness of GNNs in the presence of label noise. While learning with label noise has been extensively studied in image classification, it cannot be directly applied to graph-structured data due to label sparsity and label dependency challenges. In this study, we propose UnionNET, a new framework for learning with noisy labels on graphs under a semi-supervised setting. Our approach simultaneously performs label correction and robustly trains GNNs by performing label aggregation to estimate node-level class probability distributions. Unlike existing approaches, UnionNET requires no extra clean supervision or explicit estimation of the noise transition matrix. Additionally, our approach provides a unified learning framework to robustly train GNNs end-to-end. Our experimental results demonstrate that UnionNET is effective in improving model robustness against different types and levels of label noise and outperforms state-of-the-art baselines.",1
"While most neural generative models generate outputs in a single pass, the human creative process is usually one of iterative building and refinement. Recent work has proposed models of editing processes, but these mostly focus on editing sequential data and/or only model a single editing pass. In this paper, we present a generic model for incremental editing of structured data (i.e., ""structural edits""). Particularly, we focus on tree-structured data, taking abstract syntax trees of computer programs as our canonical example. Our editor learns to iteratively generate tree edits (e.g., deleting or adding a subtree) and applies them to the partially edited data, thereby the entire editing process can be formulated as consecutive, incremental tree transformations. To show the unique benefits of modeling tree edits directly, we further propose a novel edit encoder for learning to represent edits, as well as an imitation learning method that allows the editor to be more robust. We evaluate our proposed editor on two source code edit datasets, where results show that, with the proposed edit encoder, our editor significantly improves accuracy over previous approaches that generate the edited program directly in one pass. Finally, we demonstrate that training our editor to imitate experts and correct its mistakes dynamically can further improve its performance.",0
"The majority of neural generative models produce outputs in one go, whereas the creative process of humans involves iterative development and refinement. Although some recent research has suggested models for editing processes, these mainly focus on editing sequential data and/or only model a single editing pass. In this study, we introduce a general model for incremental editing of structured data, specifically, ""structural edits."" Our emphasis is on tree-structured data, using abstract syntax trees of computer programs as a prime example. Our editor is trained to produce tree edits iteratively (e.g., adding or deleting a subtree), which are then applied to the partially edited data, resulting in a sequence of consecutive, incremental tree transformations. To highlight the advantages of directly modeling tree edits, we propose a novel edit encoder for learning to represent edits, as well as an imitation learning approach that enhances the editor's robustness. We assess our proposed editor on two source code edit datasets and find that, with the suggested edit encoder, our editor dramatically improves accuracy over prior methods that generate the edited program in one go. Lastly, we demonstrate that training our editor to mimic experts and correct its errors dynamically can further enhance its performance.",1
"Graph Neural Networks (GNNs) are widely used for analyzing graph-structured data. Most GNN methods are highly sensitive to the quality of graph structures and usually require a perfect graph structure for learning informative embeddings. However, the pervasiveness of noise in graphs necessitates learning robust representations for real-world problems. To improve the robustness of GNN models, many studies have been proposed around the central concept of Graph Structure Learning (GSL), which aims to jointly learn an optimized graph structure and corresponding representations. Towards this end, in the presented survey, we broadly review recent progress of GSL methods for learning robust representations. Specifically, we first formulate a general paradigm of GSL, and then review state-of-the-art methods classified by how they model graph structures, followed by applications that incorporate the idea of GSL in other graph tasks. Finally, we point out some issues in current studies and discuss future directions.",0
"Graph Neural Networks (GNNs) are commonly used to analyze data structured in graphs. However, many GNN techniques are highly dependent on the quality of the graph structure and require an ideal graph structure to create meaningful embeddings. This is problematic since real-world graphs frequently contain noise, making it essential to learn robust representations. To address this problem, several studies have focused on Graph Structure Learning (GSL), which aims to learn an optimized graph structure and corresponding representations. In this survey, we provide a comprehensive review of recent advancements in GSL techniques for developing robust representations. We begin by presenting a general paradigm for GSL and then categorize state-of-the-art methods based on how they model graph structures. We also examine applications that integrate the concept of GSL into other graph tasks. Finally, we identify some limitations in current research and suggest future directions.",1
"Graph convolutional networks (GCNs) and their variants have achieved great success in dealing with graph-structured data. However, it is well known that deep GCNs will suffer from over-smoothing problem, where node representations tend to be indistinguishable as we stack up more layers. Although extensive research has confirmed this prevailing understanding, few theoretical analyses have been conducted to study the expressivity and trainability of deep GCNs. In this work, we demonstrate these characterizations by studying the Gaussian Process Kernel (GPK) and Graph Neural Tangent Kernel (GNTK) of an infinitely-wide GCN, corresponding to the analysis on expressivity and trainability, respectively. We first prove the expressivity of infinitely-wide GCNs decaying at an exponential rate by applying the mean-field theory on GPK. Besides, we formulate the asymptotic behaviors of GNTK in the large depth, which enables us to reveal the dropping trainability of wide and deep GCNs at an exponential rate. Additionally, we extend our theoretical framework to analyze residual connection-resemble techniques. We found that these techniques can mildly mitigate exponential decay, but they failed to overcome it fundamentally. Finally, all theoretical results in this work are corroborated experimentally on a variety of graph-structured datasets.",0
"Graph-structured data has been effectively handled by Graph Convolutional Networks (GCNs) and their variations. However, it is widely acknowledged that deep GCNs encounter an over-smoothing issue, leading to node representations becoming indistinguishable as more layers are added. Despite numerous studies confirming this, few theoretical analyses have been conducted to examine the expressivity and trainability of deep GCNs. This study investigates these attributes by analyzing the Gaussian Process Kernel (GPK) and Graph Neural Tangent Kernel (GNTK) of an infinitely-wide GCN. The expressivity of infinitely-wide GCNs is proven using the mean-field theory on GPK to demonstrate a decay at an exponential rate. Furthermore, the asymptotic behaviors of GNTK in the large depth are formulated, revealing the exponential rate at which wide and deep GCNs' trainability drops. Additionally, residual connection-resembling techniques were analyzed using the theoretical framework. It was found that these techniques can mitigate exponential decay slightly, but they are unable to fundamentally overcome it. Finally, the theoretical results were experimentally validated using a range of graph-structured datasets.",1
"Most existing set encoding algorithms operate under the assumption that all the elements of the set are accessible during training and inference. Additionally, it is assumed that there are enough computational resources available for concurrently processing sets of large cardinality. However, both assumptions fail when the cardinality of the set is prohibitively large such that we cannot even load the set into memory. In more extreme cases, the set size could be potentially unlimited, and the elements of the set could be given in a streaming manner, where the model receives subsets of the full set data at irregular intervals. To tackle such practical challenges in large-scale set encoding, we go beyond the usual constraints of invariance and equivariance and introduce a new property termed Mini-Batch Consistency that is required for large scale mini-batch set encoding. We present a scalable and efficient set encoding mechanism that is amenable to mini-batch processing with respect to set elements and capable of updating set representations as more data arrives. The proposed method respects the required symmetries of invariance and equivariance as well as being Mini-Batch Consistent for random partitions of the input set. We perform extensive experiments and show that our method is computationally efficient and results in rich set encoding representations for set-structured data.",0
"Most set encoding algorithms assume that all set elements are available during training and inference and that there are sufficient computational resources for processing large sets. However, when the set's cardinality is too large to fit into memory, or when the set size is potentially unlimited and the elements are provided in a streaming manner, these assumptions fail. To address these challenges, we introduce a new property called Mini-Batch Consistency, which is necessary for large-scale mini-batch set encoding, and we present a scalable and efficient set encoding mechanism that supports mini-batch processing and can update set representations as new data arrives. Our method respects invariance and equivariance symmetries and is Mini-Batch Consistent for random partitions of the input set. Extensive experiments demonstrate that our approach produces computationally efficient and rich set encoding representations for set-structured data.",1
"\textit{Attention} computes the dependency between representations, and it encourages the model to focus on the important selective features. Attention-based models, such as Transformer and graph attention network (GAT), are widely utilized for sequential data and graph-structured data. This paper suggests a new interpretation and generalized structure of the attention in Transformer and GAT. For the attention in Transformer and GAT, we derive that the attention is a product of two parts: 1) the RBF kernel to measure the similarity of two instances and 2) the exponential of $L^{2}$ norm to compute the importance of individual instances. From this decomposition, we generalize the attention in three ways. First, we propose implicit kernel attention with an implicit kernel function instead of manual kernel selection. Second, we generalize $L^{2}$ norm as the $L^{p}$ norm. Third, we extend our attention to structured multi-head attention. Our generalized attention shows better performance on classification, translation, and regression tasks.",0
"The process of \textit{Attention} involves assessing the relationship between representations and directing the model's focus to significant characteristics. Attention-based models, such as Transformer and graph attention network (GAT), are widely utilized for sequential data and graph-structured data. This study offers a new interpretation and a generalized structure of the attention in Transformer and GAT. Our analysis of the attention in Transformer and GAT shows that it comprises two components: 1) the RBF kernel, which measures the similarity between two entities, and 2) the exponential of $L^{2}$ norm, which determines the importance of individual instances. Based on this analysis, we propose three generalizations of the Attention mechanism. Firstly, we suggest an implicit kernel attention approach that uses an implicit kernel function instead of manual kernel selection. Secondly, we generalize the $L^{2}$ norm to the $L^{p}$ norm. Thirdly, we extend our attention to structured multi-head attention. Our generalized attention mechanism shows improved performance on classification, translation, and regression tasks.",1
"As a new approach to train generative models, \emph{generative adversarial networks} (GANs) have achieved considerable success in image generation. This framework has also recently been applied to data with graph structures. We propose labeled-graph generative adversarial networks (LGGAN) to train deep generative models for graph-structured data with node labels. We test the approach on various types of graph datasets, such as collections of citation networks and protein graphs. Experiment results show that our model can generate diverse labeled graphs that match the structural characteristics of the training data and outperforms all alternative approaches in quality and generality. To further evaluate the quality of the generated graphs, we use them on a downstream task of graph classification, and the results show that LGGAN can faithfully capture the important aspects of the graph structure.",0
"Generative adversarial networks (GANs) have been successful in image generation and have now been applied to graph-structured data. Our proposed approach, labeled-graph generative adversarial networks (LGGAN), trains deep generative models for graph-structured data with node labels. We tested LGGAN on various graph datasets, including citation networks and protein graphs, and found that our model outperforms all alternative approaches in both quality and generality. Furthermore, we evaluated the quality of the generated graphs by using them for a downstream task of graph classification, which showed that LGGAN can capture the important aspects of the graph structure.",1
"This paper introduces CloudLSTM, a new branch of recurrent neural models tailored to forecasting over data streams generated by geospatial point-cloud sources. We design a Dynamic Point-cloud Convolution (DConv) operator as the core component of CloudLSTMs, which performs convolution directly over point-clouds and extracts local spatial features from sets of neighboring points that surround different elements of the input. This operator maintains the permutation invariance of sequence-to-sequence learning frameworks, while representing neighboring correlations at each time step -- an important aspect in spatiotemporal predictive learning. The DConv operator resolves the grid-structural data requirements of existing spatiotemporal forecasting models and can be easily plugged into traditional LSTM architectures with sequence-to-sequence learning and attention mechanisms. We apply our proposed architecture to two representative, practical use cases that involve point-cloud streams, i.e., mobile service traffic forecasting and air quality indicator forecasting. Our results, obtained with real-world datasets collected in diverse scenarios for each use case, show that CloudLSTM delivers accurate long-term predictions, outperforming a variety of competitor neural network models.",0
"CloudLSTM is a novel type of recurrent neural model that is specifically designed for predicting data streams generated by geospatial point-cloud sources. The core component of CloudLSTMs is the Dynamic Point-cloud Convolution (DConv) operator, which performs convolution directly over point-clouds and extracts local spatial features from sets of neighboring points. This operator maintains the permutation invariance of sequence-to-sequence learning frameworks and represents neighboring correlations at each time step, which is crucial for spatiotemporal predictive learning. By resolving the grid-structural data requirements of existing spatiotemporal forecasting models, the DConv operator can be easily integrated into traditional LSTM architectures with sequence-to-sequence learning and attention mechanisms. We validated our proposed architecture through two practical use cases: mobile service traffic forecasting and air quality indicator forecasting. Our results, obtained from real-world datasets collected in diverse scenarios, demonstrate that CloudLSTM can deliver highly accurate long-term predictions, outperforming a variety of competitor neural network models.",1
"Our interest is in scientific problems with the following characteristics: (1) Data are naturally represented as graphs; (2) The amount of data available is typically small; and (3) There is significant domain-knowledge, usually expressed in some symbolic form. These kinds of problems have been addressed effectively in the past by Inductive Logic Programming (ILP), by virtue of 2 important characteristics: (a) The use of a representation language that easily captures the relation encoded in graph-structured data, and (b) The inclusion of prior information encoded as domain-specific relations, that can alleviate problems of data scarcity, and construct new relations. Recent advances have seen the emergence of deep neural networks specifically developed for graph-structured data (Graph-based Neural Networks, or GNNs). While GNNs have been shown to be able to handle graph-structured data, less has been done to investigate the inclusion of domain-knowledge. Here we investigate this aspect of GNNs empirically by employing an operation we term ""vertex-enrichment"" and denote the corresponding GNNs as ""VEGNNs"". Using over 70 real-world datasets and substantial amounts of symbolic domain-knowledge, we examine the result of vertex-enrichment across 5 different variants of GNNs. Our results provide support for the following: (a) Inclusion of domain-knowledge by vertex-enrichment can significantly improve the performance of a GNN. That is, the performance VEGNNs is significantly better than GNNs across all GNN variants; (b) The inclusion of domain-specific relations constructed using ILP improves the performance of VEGNNs, across all GNN variants. Taken together, the results provide evidence that it is possible to incorporate symbolic domain knowledge into a GNN, and that ILP can play an important role in providing high-level relationships that are not easily discovered by a GNN.",0
"Our focus is on scientific problems that possess three key attributes: (1) Graphs are the natural representation of data; (2) The amount of available data is typically limited; and (3) There is significant domain-specific knowledge, often expressed symbolically. Inductive Logic Programming (ILP) has previously been effective in solving these types of problems due to two important features: (a) The use of a language that readily captures the relationships present in graph-structured data, and (b) The incorporation of prior knowledge encoded as domain-specific relationships, which can address issues related to data scarcity and facilitate the creation of new relationships. Graph-based Neural Networks (GNNs) have recently emerged as a solution specifically designed for graph-structured data. While GNNs have proven to be effective, little research has been conducted on the integration of domain-specific knowledge. We investigate this aspect of GNNs by introducing a technique called ""vertex-enrichment,"" resulting in ""VEGNNs."" Using more than 70 real-world datasets and a significant amount of symbolic domain knowledge, we evaluate the impact of vertex-enrichment across five different GNN variants. Our findings demonstrate that (a) Including domain-specific knowledge through vertex-enrichment can significantly improve GNN performance, as VEGNNs outperform GNNs across all GNN variants; (b) The addition of domain-specific relationships created through ILP enhances VEGNN performance across all GNN variants. Our results indicate that symbolic domain knowledge can be integrated into GNNs and that ILP can offer valuable high-level relationships that may not be easily discovered by GNNs.",1
"Graph-structured data arise in a variety of real-world context ranging from sensor and transportation to biological and social networks. As a ubiquitous tool to process graph-structured data, spectral graph filters have been used to solve common tasks such as denoising and anomaly detection, as well as design deep learning architectures such as graph neural networks. Despite being an important tool, there is a lack of theoretical understanding of the stability properties of spectral graph filters, which are important for designing robust machine learning models. In this paper, we study filter stability and provide a novel and interpretable upper bound on the change of filter output, where the bound is expressed in terms of the endpoint degrees of the deleted and newly added edges, as well as the spatial proximity of those edges. This upper bound allows us to reason, in terms of structural properties of the graph, when a spectral graph filter will be stable. We further perform extensive experiments to verify intuition that can be gained from the bound.",0
"Graph-structured data is present in various real-world scenarios such as biological and social networks as well as transportation and sensor systems. Spectral graph filters are a widely used tool for processing such data, used for tasks like anomaly detection and denoising. They are also employed in designing deep learning models, like graph neural networks. However, the stability properties of these filters lack theoretical understanding, which is vital to designing robust machine learning models. This paper investigates filter stability and offers a new and easy-to-understand upper bound on the change in filter output. This bound is expressed in terms of the degree of endpoints of added and deleted edges, as well as the proximity of these edges. This upper bound helps us comprehend when a spectral graph filter will be stable based on the structural properties of the graph. We also conduct extensive experiments to verify our findings.",1
"Machine learning on graph-structured data has attracted high research interest due to the emergence of Graph Neural Networks (GNNs). Most of the proposed GNNs are based on the node homophily, i.e neighboring nodes share similar characteristics. However, in many complex networks, nodes that lie to distant parts of the graph share structurally equivalent characteristics and exhibit similar roles (e.g chemical properties of distant atoms in a molecule, type of social network users). A growing literature proposed representations that identify structurally equivalent nodes. However, most of the existing methods require high time and space complexity. In this paper, we propose VNEstruct, a simple approach, based on entropy measures of the neighborhood's topology, for generating low-dimensional structural representations, that is time-efficient and robust to graph perturbations. Empirically, we observe that VNEstruct exhibits robustness on structural role identification tasks. Moreover, VNEstruct can achieve state-of-the-art performance on graph classification, without incorporating the graph structure information in the optimization, in contrast to GNN competitors.",0
"The development of Graph Neural Networks (GNNs) has generated significant research interest in machine learning on graph-structured data. However, most GNNs rely on the concept of node homophily, where neighboring nodes share similar characteristics. This approach does not account for situations where nodes in different areas of the graph possess equivalent features and play similar roles, such as distant atoms in a molecule or social network users. While some methods have been proposed to identify equivalent nodes, they often require a large amount of time and space. This paper presents VNEstruct, a straightforward technique that employs entropy measures of the neighborhood's topology to produce low-dimensional structural representations. VNEstruct is efficient and resilient to graph perturbations, and it has demonstrated robustness in the identification of structural roles. Additionally, VNEstruct has achieved state-of-the-art performance in graph classification, even without incorporating graph structure information into the optimization process, which sets it apart from GNN competitors.",1
"Online real estate platforms have become significant marketplaces facilitating users' search for an apartment or a house. Yet it remains challenging to accurately appraise a property's value. Prior works have primarily studied real estate valuation based on hedonic price models that take structured data into account while accompanying unstructured data is typically ignored. In this study, we investigate to what extent an automated visual analysis of apartment floor plans on online real estate platforms can enhance hedonic rent price appraisal. We propose a tailored two-staged deep learning approach to learn price-relevant designs of floor plans from historical price data. Subsequently, we integrate the floor plan predictions into hedonic rent price models that account for both structural and locational characteristics of an apartment. Our empirical analysis based on a unique dataset of 9174 real estate listings suggests that current hedonic models underutilize the available data. We find that (1) the visual design of floor plans has significant explanatory power regarding rent prices - even after controlling for structural and locational apartment characteristics, and (2) harnessing floor plans results in an up to 10.56% lower out-of-sample prediction error. We further find that floor plans yield a particularly high gain in prediction performance for older and smaller apartments. Altogether, our empirical findings contribute to the existing research body by establishing the link between the visual design of floor plans and real estate prices. Moreover, our approach has important implications for online real estate platforms, which can use our findings to enhance user experience in their real estate listings.",0
"The use of online real estate platforms has become increasingly popular for those searching for apartments or houses. However, accurately assessing the value of a property remains a challenging task. Previous research has focused on real estate valuation based on structured data, while unstructured data has been largely ignored. In this study, we explore the potential benefits of using an automated visual analysis of apartment floor plans to improve hedonic rent price appraisal. To achieve this, we propose a two-stage deep learning approach that learns price-relevant designs from historical data and integrates them into hedonic rent price models. Our analysis of a unique dataset of 9174 real estate listings reveals that the visual design of floor plans has a significant impact on rent prices, even after controlling for structural and locational characteristics of an apartment. Our approach also results in a lower out-of-sample prediction error, with a particularly high gain in prediction performance for older and smaller apartments. These findings have important implications for online real estate platforms, which can use our approach to improve user experience and enhance real estate listings.",1
"Graph neural networks (GNNs) are popular to use for classifying structured data in the context of machine learning. But surprisingly, they are rarely applied to regression problems. In this work, we adopt GNN for a classic but challenging nonlinear regression problem, namely the network localization. Our main findings are in order. First, GNN is potentially the best solution to large-scale network localization in terms of accuracy, robustness and computational time. Second, proper thresholding of the communication range is essential to its superior performance. Simulation results corroborate that the proposed GNN based method outperforms all state-of-the-art benchmarks by far. Such inspiring results are theoretically justified in terms of data aggregation, non-line-of-sight (NLOS) noise removal and low-pass filtering effect, all affected by the threshold for neighbor selection. Code is available at https://github.com/Yanzongzi/GNN-For-localization.",0
"Using graph neural networks (GNNs) is a common practice in machine learning for classifying structured data. However, it is surprising that these networks are not often used for regression problems. We aimed to address this by applying GNNs to a challenging nonlinear regression problem, specifically network localization. Our research has shown that GNNs are potentially the best solution for accurately and robustly localizing large-scale networks while also being computationally efficient. We found that proper thresholding of the communication range is crucial for achieving this superior performance. Our simulation results demonstrate that our GNN-based method outperforms all existing benchmarks by a significant margin. These positive results can be attributed to data aggregation, non-line-of-sight (NLOS) noise removal, and the low-pass filtering effect, all of which are influenced by the threshold for neighbor selection. To access our code, please visit https://github.com/Yanzongzi/GNN-For-localization.",1
"The von Mises-Fisher (vMF) is a well-known density model for directional random variables. The recent surge of the deep embedding methodologies for high-dimensional structured data such as images or texts, aimed at extracting salient directional information, can make the vMF model even more popular. In this article, we will review the vMF model and its mixture, provide detailed recipes of how to train the models, focusing on the maximum likelihood estimators, in Python/PyTorch. In particular, implementation of vMF typically suffers from the notorious numerical issue of the Bessel function evaluation in the density normalizer, especially when the dimensionality is high, and we address the issue using the MPMath library that supports arbitrary precision. For the mixture learning, we provide both minibatch-based large-scale SGD learning, as well as the EM algorithm which is a full batch estimator. For each estimator/methodology, we test our implementation on some synthetic data, while we also demonstrate the use case in a more realistic scenario of image clustering. Our code is publicly available in https://github.com/minyoungkim21/vmf-lib.",0
"The vMF model is a popular density model for directional random variables, which can become even more prevalent due to the recent increase in deep embedding methodologies for high-dimensional structured data. This article aims to review the vMF model and its mixture, with a focus on providing detailed instructions on how to train the models using maximum likelihood estimators in Python/PyTorch. The implementation of the vMF model often encounters numerical issues when evaluating the Bessel function in the density normalizer, particularly in high-dimensional cases. To address this problem, we use the MPMath library, which supports arbitrary precision. We also provide both minibatch-based large-scale stochastic gradient descent (SGD) learning and the EM algorithm as full-batch estimators for mixture learning. We test each estimator/methodology on synthetic data and demonstrate their use in image clustering. Our code is publicly available at https://github.com/minyoungkim21/vmf-lib.",1
"Graph neural networks have shown superior performance in a wide range of applications providing a powerful representation of graph-structured data. Recent works show that the representation can be further improved by auxiliary tasks. However, the auxiliary tasks for heterogeneous graphs, which contain rich semantic information with various types of nodes and edges, have less explored in the literature. In this paper, to learn graph neural networks on heterogeneous graphs we propose a novel self-supervised auxiliary learning method using meta-paths, which are composite relations of multiple edge types. Our proposed method is learning to learn a primary task by predicting meta-paths as auxiliary tasks. This can be viewed as a type of meta-learning. The proposed method can identify an effective combination of auxiliary tasks and automatically balance them to improve the primary task. Our methods can be applied to any graph neural networks in a plug-in manner without manual labeling or additional data. The experiments demonstrate that the proposed method consistently improves the performance of link prediction and node classification on heterogeneous graphs.",0
"Graph neural networks have demonstrated exceptional performance in various applications, providing a potent representation of data that is structured as graphs. Additional studies have revealed that the representation can be enhanced further through auxiliary tasks. However, there has been limited exploration of auxiliary tasks for heterogeneous graphs, which contain diverse types of nodes and edges, as well as rich semantic information. In this study, we propose a novel self-supervised auxiliary learning method that utilizes meta-paths, which are composite relations of multiple edge types, to learn graph neural networks on heterogeneous graphs. Our method involves learning a primary task by predicting meta-paths as auxiliary tasks, which can be considered as a type of meta-learning. The proposed method can automatically identify an effective combination of auxiliary tasks and balance them to optimize the primary task. Our approach can be applied to any graph neural networks without the need for manual labeling or additional data. Our experiments show that our method consistently enhances the performance of link prediction and node classification on heterogeneous graphs.",1
"Graph attention networks (GATs) have been recognized as powerful tools for learning in graph structured data. However, how to enable the attention mechanisms in GATs to smoothly consider both structural and feature information is still very challenging. In this paper, we propose Graph Joint Attention Networks (JATs) to address the aforementioned challenge. Different from previous attention-based graph neural networks (GNNs), JATs adopt novel joint attention mechanisms which can automatically determine the relative significance between node features and structural coefficients learned from graph topology, when computing the attention scores. Therefore, representations concerning more structural properties can be inferred by JATs. Besides, we theoretically analyze the expressive power of JATs and further propose an improved strategy for the joint attention mechanisms that enables JATs to reach the upper bound of expressive power which every message-passing GNN can ultimately achieve, i.e., 1-WL test. JATs can thereby be seen as most powerful message-passing GNNs. The proposed neural architecture has been extensively tested on widely used benchmarking datasets, and has been compared with state-of-the-art GNNs for various downstream predictive tasks. Experimental results show that JATs achieve state-of-the-art performance on all the testing datasets.",0
"Learning in graph structured data can be enhanced through the use of Graph Attention Networks (GATs). However, there is still a challenge in enabling attention mechanisms in GATs to consider both structural and feature information seamlessly. To address this challenge, we present Graph Joint Attention Networks (JATs). Unlike previous attention-based graph neural networks (GNNs), JATs use joint attention mechanisms to automatically determine the relative importance between node features and structural coefficients learned from graph topology when computing attention scores. This allows JATs to infer representations that include more structural properties. Furthermore, we analyze the expressive power of JATs and propose an improved strategy for joint attention mechanisms that enables JATs to reach the upper bound of expressive power achieved by every message-passing GNN, i.e., 1-WL test. Therefore, JATs are considered the most powerful message-passing GNNs. We extensively tested the proposed neural architecture on widely used benchmarking datasets and compared it with state-of-the-art GNNs for various downstream predictive tasks. Experimental results indicate that JATs achieve state-of-the-art performance on all testing datasets.",1
"Convolutional Neural Networks(CNNs) has achieved remarkable performance breakthrough in Euclidean structure data. Recently, aggregation-transformation based Graph Neural networks(GNNs) gradually produce a powerful performance on non-Euclidean data. In this paper, we propose a cross-correlation based graph convolution method allowing to naturally generalize CNNs to non-Euclidean domains and inherit the excellent natures of CNNs, such as local filters, parameter sharing, flexible receptive field, etc. Meanwhile, it leverages dynamically generated convolution kernel and cross-correlation operators to address the shortcomings of prior methods based on aggregation-transformation or their approximations. Our method has achieved or matched popular state-of-the-art results across three established graph benchmarks: the Cora, Citeseer, and Pubmed citation network datasets.",0
"CNNs have been highly successful in analyzing Euclidean structure data, while GNNs based on aggregation-transformation have emerged as powerful tools for non-Euclidean data. This paper introduces a new method for graph convolution that combines the strengths of CNNs with the ability to generalize to non-Euclidean domains. Our approach uses cross-correlation and dynamically generated convolution kernels to overcome limitations of prior aggregation-based methods. We demonstrate the effectiveness of our method on three established graph benchmarks, achieving or matching state-of-the-art results.",1
"The medical field is creating large amount of data that physicians are unable to decipher and use efficiently. Moreover, rule-based expert systems are inefficient in solving complicated medical tasks or for creating insights using big data. Deep learning has emerged as a more accurate and effective technology in a wide range of medical problems such as diagnosis, prediction and intervention. Deep learning is a representation learning method that consists of layers that transform the data non-linearly, thus, revealing hierarchical relationships and structures. In this review we survey deep learning application papers that use structured data, signal and imaging modalities from cardiology. We discuss the advantages and limitations of applying deep learning in cardiology that also apply in medicine in general, while proposing certain directions as the most viable for clinical use.",0
"Physicians are struggling to efficiently use the vast amounts of data being generated in the medical field. Additionally, rule-based expert systems are ineffective in tackling complex medical tasks and analyzing big data. However, deep learning has proven to be a highly accurate and efficient technology for various medical issues, including diagnosis, prediction, and intervention. By transforming data non-linearly through layered processing, deep learning can reveal hierarchical relationships and structures. This paper surveys deep learning applications in cardiology that utilize structured data, signals, and imaging modalities. The advantages and limitations of using deep learning in cardiology and medicine in general are discussed, along with proposed directions for clinical use.",1
"Summarization of long sequences into a concise statement is a core problem in natural language processing, requiring non-trivial understanding of the input. Based on the promising results of graph neural networks on highly structured data, we develop a framework to extend existing sequence encoders with a graph component that can reason about long-distance relationships in weakly structured data such as text. In an extensive evaluation, we show that the resulting hybrid sequence-graph models outperform both pure sequence models as well as pure graph models on a range of summarization tasks.",0
"The main challenge in natural language processing is to summarize lengthy sequences into a brief statement, which demands a comprehensive comprehension of the input. To tackle this issue, we introduce a framework that supplements current sequence encoders with a graph component. This component utilizes the graph neural networks' impressive outcomes on highly organized data to reason about distant relationships in loosely organized data, like text. Through comprehensive testing, we demonstrate that the resulting sequence-graph models surpass pure sequence and pure graph models in various summarization tasks.",1
"Graph-structured data ubiquitously appears in science and engineering. Graph neural networks (GNNs) are designed to exploit the relational inductive bias exhibited in graphs; they have been shown to outperform other forms of neural networks in scenarios where structure information supplements node features. The most common GNN architecture aggregates information from neighborhoods based on message passing. Its generality has made it broadly applicable. In this paper, we focus on a special, yet widely used, type of graphs -- DAGs -- and inject a stronger inductive bias -- partial ordering -- into the neural network design. We propose the \emph{directed acyclic graph neural network}, DAGNN, an architecture that processes information according to the flow defined by the partial order. DAGNN can be considered a framework that entails earlier works as special cases (e.g., models for trees and models updating node representations recurrently), but we identify several crucial components that prior architectures lack. We perform comprehensive experiments, including ablation studies, on representative DAG datasets (i.e., source code, neural architectures, and probabilistic graphical models) and demonstrate the superiority of DAGNN over simpler DAG architectures as well as general graph architectures.",0
"The use of graph-structured data is widespread in science and engineering. Graph neural networks (GNNs) are created to take advantage of the relational inductive bias that is present in graphs. They have been proven to be more effective than other types of neural networks in situations where structure information enhances node features. The most common GNN design aggregates information from nearby nodes based on message passing, which makes it versatile. This paper focuses on directed acyclic graphs (DAGs), which are a widely used type of graph. The neural network design is enhanced with a stronger inductive bias, partial ordering. We propose the DAGNN architecture, which processes information according to the flow defined by the partial order. DAGNN can be viewed as a framework that includes previous works as special cases, such as models for trees and models that recurrently update node representations. However, we identify several essential components that previous architectures lack. We conduct extensive experiments, including ablation studies, on representative DAG datasets, such as source code, neural architectures, and probabilistic graphical models. We demonstrate that DAGNN is superior to simpler DAG architectures and general graph architectures.",1
"A big mystery in deep learning continues to be the ability of methods to generalize when the number of model parameters is larger than the number of training examples. In this work, we take a step towards a better understanding of the underlying phenomena of Deep Autoencoders (AEs), a mainstream deep learning solution for learning compressed, interpretable, and structured data representations. In particular, we interpret how AEs approximate the data manifold by exploiting their continuous piecewise affine structure. Our reformulation of AEs provides new insights into their mapping, reconstruction guarantees, as well as an interpretation of commonly used regularization techniques. We leverage these findings to derive two new regularizations that enable AEs to capture the inherent symmetry in the data. Our regularizations leverage recent advances in the group of transformation learning to enable AEs to better approximate the data manifold without explicitly defining the group underlying the manifold. Under the assumption that the symmetry of the data can be explained by a Lie group, we prove that the regularizations ensure the generalization of the corresponding AEs. A range of experimental evaluations demonstrate that our methods outperform other state-of-the-art regularization techniques.",0
"The challenge of deep learning is still to fathom how models can generalize when there are more parameters than training examples. This study aims to shed light on Deep Autoencoders (AEs), a prevalent deep learning technique used to learn compressed, comprehensible, and organized data representations. Specifically, we examine how AEs approximate the data manifold by utilizing their continuous piecewise affine structure. Our reinterpretation of AEs offers novel insights into their mapping, reconstruction guarantees, and common regularization methods. Our discoveries lead us to develop two new regularization techniques that enable AEs to capture the intrinsic symmetry of the data. These techniques leverage the recent breakthroughs in transformation learning groups to better approximate the data manifold without explicitly defining the group underlying the manifold. We prove that these regularizations ensure the generalization of the corresponding AEs under the assumption that the data symmetry can be explained by a Lie group. Our experiments demonstrate that our approaches outperform other cutting-edge regularization techniques.",1
"Graph Neural Networks (GNNs) have received considerable attention on graph-structured data learning for a wide variety of tasks. The well-designed propagation mechanism which has been demonstrated effective is the most fundamental part of GNNs. Although most of GNNs basically follow a message passing manner, litter effort has been made to discover and analyze their essential relations. In this paper, we establish a surprising connection between different propagation mechanisms with a unified optimization problem, showing that despite the proliferation of various GNNs, in fact, their proposed propagation mechanisms are the optimal solution optimizing a feature fitting function over a wide class of graph kernels with a graph regularization term. Our proposed unified optimization framework, summarizing the commonalities between several of the most representative GNNs, not only provides a macroscopic view on surveying the relations between different GNNs, but also further opens up new opportunities for flexibly designing new GNNs. With the proposed framework, we discover that existing works usually utilize naive graph convolutional kernels for feature fitting function, and we further develop two novel objective functions considering adjustable graph kernels showing low-pass or high-pass filtering capabilities respectively. Moreover, we provide the convergence proofs and expressive power comparisons for the proposed models. Extensive experiments on benchmark datasets clearly show that the proposed GNNs not only outperform the state-of-the-art methods but also have good ability to alleviate over-smoothing, and further verify the feasibility for designing GNNs with our unified optimization framework.",0
"Graph Neural Networks (GNNs) have become a popular tool for learning on graph-structured data for various tasks. The core of GNNs is their well-designed propagation mechanism, which has proven to be effective. Despite most GNNs using a message passing method, little effort has been made to explore and analyze their essential relationships. In this study, we establish a surprising connection between various propagation mechanisms with a unified optimization problem. Our proposed framework summarizes the commonalities between several representative GNNs and provides a macroscopic view on surveying their relationships. Additionally, it opens up new opportunities for designing GNNs flexibly. We discover that existing works typically employ naive graph convolutional kernels for feature fitting function, and we further develop two novel objective functions considering adjustable graph kernels with low-pass or high-pass filtering capabilities. Furthermore, we provide convergence proofs and expressive power comparisons for the proposed models. Extensive experiments on benchmark datasets demonstrate that our proposed GNNs outperform state-of-the-art methods, have the ability to alleviate over-smoothing, and verify the feasibility of designing GNNs with our unified optimization framework.",1
"Kernel methods have been among the most popular techniques in machine learning, where learning tasks are solved using the property of reproducing kernel Hilbert space (RKHS). In this paper, we propose a novel data analysis framework with reproducing kernel Hilbert $C^*$-module (RKHM) and kernel mean embedding (KME) in RKHM. Since RKHM contains richer information than RKHS or vector-valued RKHS (vv RKHS), analysis with RKHM enables us to capture and extract structural properties in multivariate data, functional data and other structured data. We show a branch of theories for RKHM to apply to data analysis, including the representer theorem, and the injectivity and universality of the proposed KME. We also show RKHM generalizes RKHS and vv RKHS. Then, we provide concrete procedures for employing RKHM and the proposed KME to data analysis.",0
"Machine learning has widely adopted kernel methods, which utilize the reproducing kernel Hilbert space (RKHS) property to solve learning tasks. In this study, we introduce a new data analysis framework that employs a reproducing kernel Hilbert C*-module (RKHM) and kernel mean embedding (KME) in RKHM. Our proposed approach offers a more comprehensive analysis of multivariate data, functional data, and structured data than RKHS or vector-valued RKHS (vv RKHS). We present a range of theories for RKHM, including the representer theorem, and the injectivity and universality of KME. Additionally, we demonstrate that RKHM generalizes RKHS and vv RKHS. Finally, we offer a clear set of procedures for utilizing RKHM and KME for data analysis.",1
"Graph representation of structured data can facilitate the extraction of stereoscopic features, and it has demonstrated excellent ability when working with deep learning systems, the so-called Graph Neural Networks (GNNs). Choosing a promising architecture for constructing GNNs can be transferred to a hyperparameter optimisation problem, a very challenging task due to the size of the underlying search space and high computational cost for evaluating candidate GNNs. To address this issue, this research presents a novel genetic algorithm with a hierarchical evaluation strategy (HESGA), which combines the full evaluation of GNNs with a fast evaluation approach. By using full evaluation, a GNN is represented by a set of hyperparameter values and trained on a specified dataset, and root mean square error (RMSE) will be used to measure the quality of the GNN represented by the set of hyperparameter values (for regression problems). While in the proposed fast evaluation process, the training will be interrupted at an early stage, the difference of RMSE values between the starting and interrupted epochs will be used as a fast score, which implies the potential of the GNN being considered. To coordinate both types of evaluations, the proposed hierarchical strategy uses the fast evaluation in a lower level for recommending candidates to a higher level, where the full evaluation will act as a final assessor to maintain a group of elite individuals. To validate the effectiveness of HESGA, we apply it to optimise two types of deep graph neural networks. The experimental results on three benchmark datasets demonstrate its advantages compared to Bayesian hyperparameter optimization.",0
"Structured data can be represented graphically, allowing for the identification of stereoscopic features. Graph Neural Networks (GNNs) have proven to be highly effective in deep learning systems. However, designing a GNN architecture that is both promising and efficient is a difficult task due to the vast search space and computational costs involved. To address this challenge, this study introduces a novel genetic algorithm with a hierarchical evaluation strategy (HESGA). The algorithm combines a full evaluation of GNNs with a fast evaluation approach to efficiently optimize hyperparameters. A GNN is represented by a set of hyperparameter values and trained on a specified dataset, with root mean square error (RMSE) used to measure the quality of the GNN. The proposed fast evaluation process interrupts training early, using the difference in RMSE values between the starting and interrupted epochs as a fast score to assess the potential of the GNN. The hierarchical strategy coordinates both types of evaluations, with the fast evaluation recommending candidates to a higher level where the full evaluation acts as a final assessor to maintain a group of elite individuals. The effectiveness of HESGA is validated through experiments on two types of deep graph neural networks using three benchmark datasets, with results demonstrating its superiority over Bayesian hyperparameter optimization.",1
"Deep generative models, since their inception, have become increasingly more capable of generating novel and perceptually realistic signals (e.g., images and sound waves). With the emergence of deep models for graph structured data, natural interests seek extensions of these generative models for graphs. Successful extensions were seen recently in the case of learning from a collection of graphs (e.g., protein data banks), but the learning from a single graph has been largely under explored. The latter case, however, is important in practice. For example, graphs in financial and healthcare systems contain so much confidential information that their public accessibility is nearly impossible, but open science in these fields can only advance when similar data are available for benchmarking.   In this work, we propose an approach to generating a doppelganger graph that resembles a given one in many graph properties but nonetheless can hardly be used to reverse engineer the original one, in the sense of a near zero edge overlap. The approach is an orchestration of graph representation learning, generative adversarial networks, and graph realization algorithms. Through comparison with several graph generative models (either parameterized by neural networks or not), we demonstrate that our result barely reproduces the given graph but closely matches its properties. We further show that downstream tasks, such as node classification, on the generated graphs reach similar performance to the use of the original ones.",0
"The capabilities of deep generative models have grown significantly over time, enabling them to generate signals that are both novel and realistic in terms of perception, such as images and sound waves. As deep models for graph-structured data have emerged, there has been a natural interest in extending these generative models to graphs. While successful extensions have been made for learning from collections of graphs, such as protein data banks, the learning from a single graph has not been explored as much, despite its practical importance. For instance, graphs in financial and healthcare systems often contain confidential information that cannot be made public, but open science in these fields requires similar data for benchmarking. This work proposes an approach to generate a doppelganger graph that resembles a given graph in many properties, but cannot be used to reverse engineer the original with a near-zero edge overlap. The approach combines graph representation learning, generative adversarial networks, and graph realization algorithms. The results are compared to several graph generative models, both parameterized by neural networks and not, and show that while our approach barely reproduces the given graph, it closely matches its properties. Further testing shows that downstream tasks, such as node classification, on the generated graphs achieve similar performance to the original ones.",1
"A fundamental problem on graph-structured data is that of quantifying similarity between graphs. Graph kernels are an established technique for such tasks; in particular, those based on random walks and return probabilities have proven to be effective in wide-ranging applications, from bioinformatics to social networks to computer vision. However, random walk kernels generally suffer from slowness and tottering, an effect which causes walks to overemphasize local graph topology, undercutting the importance of global structure. To correct for these issues, we recast return probability graph kernels under the more general framework of density of states -- a framework which uses the lens of spectral analysis to uncover graph motifs and properties hidden within the interior of the spectrum -- and use our interpretation to construct scalable, composite density of states based graph kernels which balance local and global information, leading to higher classification accuracies on a host of benchmark datasets.",0
"One of the main challenges in dealing with graphs is determining how similar they are to each other. One way to tackle this issue is by utilizing graph kernels, which have proven to be effective in a variety of fields, from bioinformatics to computer vision, particularly those based on random walks and return probabilities. However, random walk kernels can be slow and unreliable, as they tend to focus too much on local graph topology and overlook global structure. To address these shortcomings, we have reimagined return probability graph kernels using the density of states framework, which leverages spectral analysis to uncover hidden motifs and properties within the graph's interior. By doing so, we have developed composite density of states based graph kernels that strike a balance between local and global information, resulting in more accurate classification outcomes across several benchmark datasets.",1
"Lots of neural network architectures have been proposed to deal with learning tasks on graph-structured data. However, most of these models concentrate on only node features during the learning process. The edge features, which usually play a similarly important role as the nodes, are often ignored or simplified by these models. In this paper, we present edge-featured graph attention networks, namely EGATs, to extend the use of graph neural networks to those tasks learning on graphs with both node and edge features. These models can be regarded as extensions of graph attention networks (GATs). By reforming the model structure and the learning process, the new models can accept node and edge features as inputs, incorporate the edge information into feature representations, and iterate both node and edge features in a parallel but mutual way. The results demonstrate that our work is highly competitive against other node classification approaches, and can be well applied in edge-featured graph learning tasks.",0
"Numerous neural network designs have been proposed for learning tasks on graph-structured data. However, most of these models focus solely on node features during the learning phase, overlooking or simplifying the significance of edge features, which are usually just as crucial as nodes. This paper introduces Edge-featured Graph Attention Networks (EGATs), which expand the use of graph neural networks to tasks that involve learning on graphs with both node and edge features. These models are essentially extensions of Graph Attention Networks (GATs). Through reforming the model structure and learning process, the new models can input node and edge features and integrate edge information into feature representations, while iterating both node and edge features in a parallel but mutual manner. The results indicate that our work is highly competitive compared to other node classification approaches and can be effectively implemented in edge-featured graph learning tasks.",1
"There has been a surge of recent interest in learning representations for graph-structured data. Graph representation learning methods have generally fallen into three main categories, based on the availability of labeled data. The first, network embedding (such as shallow graph embedding or graph auto-encoders), focuses on learning unsupervised representations of relational structure. The second, graph regularized neural networks, leverages graphs to augment neural network losses with a regularization objective for semi-supervised learning. The third, graph neural networks, aims to learn differentiable functions over discrete topologies with arbitrary structure. However, despite the popularity of these areas there has been surprisingly little work on unifying the three paradigms. Here, we aim to bridge the gap between graph neural networks, network embedding and graph regularization models. We propose a comprehensive taxonomy of representation learning methods for graph-structured data, aiming to unify several disparate bodies of work. Specifically, we propose a Graph Encoder Decoder Model (GRAPHEDM), which generalizes popular algorithms for semi-supervised learning on graphs (e.g. GraphSage, Graph Convolutional Networks, Graph Attention Networks), and unsupervised learning of graph representations (e.g. DeepWalk, node2vec, etc) into a single consistent approach. To illustrate the generality of this approach, we fit over thirty existing methods into this framework. We believe that this unifying view both provides a solid foundation for understanding the intuition behind these methods, and enables future research in the area.",0
"Recently, there has been a growing interest in acquiring knowledge about representations for graph-structured data. Generally, methods for learning graph representations can be categorized into three groups, depending on the availability of labeled data. The first group, network embedding, focuses on acquiring unsupervised representations of relational structure, using techniques such as shallow graph embedding or graph auto-encoders. The second group, graph regularized neural networks, uses graphs to augment neural network losses with a regularization objective for semi-supervised learning. The third group, graph neural networks, aims to learn differentiable functions over discrete topologies with arbitrary structure. Despite the popularity of these areas, there has been little work on unifying the three paradigms. This study aims to bridge this gap by proposing a comprehensive taxonomy of representation learning methods for graph-structured data, which unifies several disparate bodies of work. The proposed Graph Encoder Decoder Model (GRAPHEDM) generalizes popular algorithms for semi-supervised and unsupervised learning of graph representations into a single consistent approach. Over thirty existing methods have been fitted into this framework, illustrating the generality of this approach. This unifying view provides a solid foundation for understanding the intuition behind these methods and enables future research in the area.",1
"Standard convolutional neural networks assume a grid structured input is available and exploit discrete convolutions as their fundamental building blocks. This limits their applicability to many real-world applications. In this paper we propose Parametric Continuous Convolution, a new learnable operator that operates over non-grid structured data. The key idea is to exploit parameterized kernel functions that span the full continuous vector space. This generalization allows us to learn over arbitrary data structures as long as their support relationship is computable. Our experiments show significant improvement over the state-of-the-art in point cloud segmentation of indoor and outdoor scenes, and lidar motion estimation of driving scenes.",0
"The conventional convolutional neural networks rely on a grid-structured input and utilize discrete convolutions as their basic components, which limits their usefulness in various real-world scenarios. To overcome this limitation, we propose a novel learnable operator called Parametric Continuous Convolution that can operate on non-grid-structured data. Our approach employs parameterized kernel functions that cover the entire continuous vector space, allowing us to learn from arbitrary data structures as long as their support relationship is computable. Our experiments demonstrate remarkable enhancements in point cloud segmentation of indoor and outdoor scenes, as well as lidar motion estimation of driving scenes, compared to the current state-of-the-art.",1
"Graph convolutional networks have achieved great success on graph-structured data. Many graph convolutional networks can be regarded as low-pass filters for graph signals. In this paper, we propose a new model, BiGCN, which represents a graph neural network as a bi-directional low-pass filter. Specifically, we not only consider the original graph structure information but also the latent correlation between features, thus BiGCN can filter the signals along with both the original graph and a latent feature-connection graph. Our model outperforms previous graph neural networks in the tasks of node classification and link prediction on most of the benchmark datasets, especially when we add noise to the node features.",0
"Great success has been achieved through the use of graph convolutional networks on graph-structured data. These networks can be seen as low-pass filters for graph signals. A novel model, BiGCN, is proposed in this paper, which presents a graph neural network as a bi-directional low-pass filter. Our model takes into account not only the original graph structure information but also the latent correlation between features, allowing BiGCN to filter signals using both the original graph and a latent feature-connection graph. Our model has shown to outperform previous graph neural networks in the tasks of node classification and link prediction on most benchmark datasets, especially when node features contain noise.",1
"Graph convolutional networks (GCN) have recently demonstrated their potential in analyzing non-grid structure data that can be represented as graphs. The core idea is to encode the local topology of a graph, via convolutions, into the feature of a center node. In this paper, we propose a novel GCN model, which we term as Shortest Path Graph Attention Network (SPAGAN). Unlike conventional GCN models that carry out node-based attentions within each layer, the proposed SPAGAN conducts path-based attention that explicitly accounts for the influence of a sequence of nodes yielding the minimum cost, or shortest path, between the center node and its higher-order neighbors. SPAGAN therefore allows for a more informative and intact exploration of the graph structure and further {a} more effective aggregation of information from distant neighbors into the center node, as compared to node-based GCN methods. We test SPAGAN on the downstream classification task on several standard datasets, and achieve performances superior to the state of the art. Code is publicly available at https://github.com/ihollywhy/SPAGAN.",0
"Recently, Graph Convolutional Networks (GCN) have demonstrated their potential to analyze non-grid structure data represented as graphs. The main concept is to encode the graph's local topology through convolutions into a center node's feature. In this paper, we introduce a new GCN model called Shortest Path Graph Attention Network (SPAGAN). Unlike traditional GCN models, which perform node-based attentions within each layer, the proposed SPAGAN uses path-based attention, which explicitly considers the influence of a sequence of nodes yielding the shortest path between the center node and its higher-order neighbors. Thus, SPAGAN allows for a more informative exploration of the graph structure and a more effective aggregation of information from distant neighbors into the center node than node-based GCN methods. We compare SPAGAN's performance on several standard datasets for the downstream classification task with the state of the art and achieve superior results. The code is available at https://github.com/ihollywhy/SPAGAN.",1
"A scalable semi-supervised node classification method on graph-structured data, called GraphHop, is proposed in this work. The graph contains attributes of all nodes but labels of a few nodes. The classical label propagation (LP) method and the emerging graph convolutional network (GCN) are two popular semi-supervised solutions to this problem. The LP method is not effective in modeling node attributes and labels jointly or facing a slow convergence rate on large-scale graphs. GraphHop is proposed to its shortcoming. With proper initial label vector embeddings, each iteration of GraphHop contains two steps: 1) label aggregation and 2) label update. In Step 1, each node aggregates its neighbors' label vectors obtained in the previous iteration. In Step 2, a new label vector is predicted for each node based on the label of the node itself and the aggregated label information obtained in Step 1. This iterative procedure exploits the neighborhood information and enables GraphHop to perform well in an extremely small label rate setting and scale well for very large graphs. Experimental results show that GraphHop outperforms state-of-the-art graph learning methods on a wide range of tasks (e.g., multi-label and multi-class classification on citation networks, social graphs, and commodity consumption graphs) in graphs of various sizes. Our codes are publicly available on GitHub (https://github.com/TianXieUSC/GraphHop).",0
"This paper introduces GraphHop, a method for semi-supervised node classification on graph-structured data that can be scaled. The graph includes attributes of all nodes, but only a few have labels. Classical label propagation (LP) and graph convolutional network (GCN) are two popular semi-supervised methods for this problem, but LP struggles with modelling node attributes and labels simultaneously and has slow convergence on large-scale graphs. GraphHop is proposed as a solution to these issues. The method involves two steps for each iteration: 1) label aggregation, where each node aggregates its neighbours' label vectors from the previous iteration, and 2) label update, where a new label vector is predicted for each node based on its own label and the aggregated label information. This iterative procedure exploits neighbourhood information and allows GraphHop to perform well with a small label rate and on very large graphs. Experimental results demonstrate GraphHop's superior performance compared to other graph learning methods for various tasks and graph sizes. The code for GraphHop is publicly available on GitHub (https://github.com/TianXieUSC/GraphHop).",1
"Graph Neural Networks (graph NNs) are a promising deep learning approach for analyzing graph-structured data. However, it is known that they do not improve (or sometimes worsen) their predictive performance as we pile up many layers and add non-lineality. To tackle this problem, we investigate the expressive power of graph NNs via their asymptotic behaviors as the layer size tends to infinity. Our strategy is to generalize the forward propagation of a Graph Convolutional Network (GCN), which is a popular graph NN variant, as a specific dynamical system. In the case of a GCN, we show that when its weights satisfy the conditions determined by the spectra of the (augmented) normalized Laplacian, its output exponentially approaches the set of signals that carry information of the connected components and node degrees only for distinguishing nodes. Our theory enables us to relate the expressive power of GCNs with the topological information of the underlying graphs inherent in the graph spectra. To demonstrate this, we characterize the asymptotic behavior of GCNs on the Erd\H{o}s -- R\'{e}nyi graph. We show that when the Erd\H{o}s -- R\'{e}nyi graph is sufficiently dense and large, a broad range of GCNs on it suffers from the ""information loss"" in the limit of infinite layers with high probability. Based on the theory, we provide a principled guideline for weight normalization of graph NNs. We experimentally confirm that the proposed weight scaling enhances the predictive performance of GCNs in real data. Code is available at https://github.com/delta2323/gnn-asymptotics.",0
"Graph Neural Networks (graph NNs) are an effective approach to deep learning for analyzing data with a graph structure. However, their predictive performance is not improved, and sometimes even worsened, when we add non-linearity and many layers. To address this issue, we examine the expressive power of graph NNs, specifically their asymptotic behavior as the layer size approaches infinity. Our approach is to generalize the forward propagation of a popular variant of graph NN, the Graph Convolutional Network (GCN), as a specific dynamical system. For GCNs, we demonstrate that their output exponentially approaches a set of signals that carry information about the connected components and node degrees for distinguishing nodes, but only when its weights satisfy certain conditions determined by the spectra of the (augmented) normalized Laplacian. Our theory allows us to relate the expressive power of GCNs to the topological information inherent in the graph spectra of the underlying graphs. We validate our theory by characterizing the asymptotic behavior of GCNs on the Erd\H{o}s -- R\'{e}nyi graph and showing that a broad range of GCNs on it suffer from information loss in the limit of infinite layers with high probability when the graph is sufficiently dense and large. Based on our theory, we provide a principled guideline for weight normalization of graph NNs, which we experimentally confirm enhances the predictive performance of GCNs on real data. Our code is available at https://github.com/delta2323/gnn-asymptotics.",1
"In recent years, ride-hailing services have been increasingly prevalent as they provide huge convenience for passengers. As a fundamental problem, the timely prediction of passenger demands in different regions is vital for effective traffic flow control and route planning. As both spatial and temporal patterns are indispensable passenger demand prediction, relevant research has evolved from pure time series to graph-structured data for modeling historical passenger demand data, where a snapshot graph is constructed for each time slot by connecting region nodes via different relational edges (e.g., origin-destination relationship, geographical distance, etc.). Consequently, the spatiotemporal passenger demand records naturally carry dynamic patterns in the constructed graphs, where the edges also encode important information about the directions and volume (i.e., weights) of passenger demands between two connected regions. However, existing graph-based solutions fail to simultaneously consider those three crucial aspects of dynamic, directed, and weighted (DDW) graphs, leading to limited expressiveness when learning graph representations for passenger demand prediction. Therefore, we propose a novel spatiotemporal graph attention network, namely Gallat (Graph prediction with all attention) as a solution. In Gallat, by comprehensively incorporating those three intrinsic properties of DDW graphs, we build three attention layers to fully capture the spatiotemporal dependencies among different regions across all historical time slots. Moreover, the model employs a subtask to conduct pretraining so that it can obtain accurate results more quickly. We evaluate the proposed model on real-world datasets, and our experimental results demonstrate that Gallat outperforms the state-of-the-art approaches.",0
"Over the years, ride-hailing services have become increasingly popular due to the convenience they offer passengers. A crucial issue in the effective management of traffic flow and route planning is the timely prediction of passenger demand in various regions. As both spatial and temporal patterns are crucial to predict passenger demand, prior research has progressed from pure time series to graph-structured data models of historical passenger demand data. These models construct a snapshot graph for each time slot by linking region nodes via different relational edges, such as origin-destination relationships and geographical distance. As a result, the spatiotemporal passenger demand records contain dynamic patterns in the constructed graphs, where the edges encode essential information about the directions and volume of passenger demands between two connected regions. However, existing graph-based solutions fail to simultaneously consider the dynamic, directed, and weighted (DDW) aspects of graphs, leading to limited expressiveness when learning graph representations for passenger demand prediction. Therefore, we propose Gallat, a novel spatiotemporal graph attention network that comprehensively incorporates the intrinsic properties of DDW graphs. Gallat builds three attention layers to capture spatiotemporal dependencies among different regions across all historical time slots. Additionally, Gallat employs a subtask for pretraining to obtain faster and more accurate results. We evaluate Gallat on real-world datasets and demonstrate that it outperforms state-of-the-art approaches.",1
"The development of Graph Neural Networks (GNNs) has led to great progress in machine learning on graph-structured data. These networks operate via diffusing information across the graph nodes while capturing the structure of the graph. Recently there has also seen tremendous progress in quantum computing techniques. In this work, we explore applications of multi-particle quantum walks on diffusing information across graphs. Our model is based on learning the operators that govern the dynamics of quantum random walkers on graphs. We demonstrate the effectiveness of our method on classification and regression tasks.",0
"Great advancements in machine learning on graph-structured data have been achieved with the emergence of Graph Neural Networks (GNNs), which propagate information throughout graph nodes and capture the graph structure. Furthermore, the field of quantum computing has recently made significant strides. In this study, we investigate the potential of multi-particle quantum walks for information diffusion on graphs. Our approach involves acquiring knowledge of the operators that control the dynamics of quantum random walkers on graphs. We show the efficacy of our technique in accomplishing classification and regression tasks.",1
"Graph Neural Networks (GNNs) are the subject of intense focus by the machine learning community for problems involving relational reasoning. GNNs can be broadly divided into spatial and spectral approaches. Spatial approaches use a form of learned message-passing, in which interactions among vertices are computed locally, and information propagates over longer distances on the graph with greater numbers of message-passing steps. Spectral approaches use eigendecompositions of the graph Laplacian to produce a generalization of spatial convolutions to graph structured data which access information over short and long time scales simultaneously. Here we introduce the Spectral Graph Network, which applies message passing to both the spatial and spectral domains. Our model projects vertices of the spatial graph onto the Laplacian eigenvectors, which are each represented as vertices in a fully connected ""spectral graph"", and then applies learned message passing to them. We apply this model to various benchmark tasks including a graph-based variant of MNIST classification, molecular property prediction on MoleculeNet and QM9, and shortest path problems on random graphs. Our results show that the Spectral GN promotes efficient training, reaching high performance with fewer training iterations despite having more parameters. The model also provides robustness to edge dropout and outperforms baselines for the classification tasks. We also explore how these performance benefits depend on properties of the dataset.",0
"The machine learning community is heavily focused on Graph Neural Networks (GNNs) for relational reasoning problems, which can be categorized as spatial or spectral approaches. Spatial approaches use learned message-passing to compute interactions among vertices locally, while spectral approaches use eigendecompositions of the graph Laplacian to access information over short and long time scales simultaneously. The Spectral Graph Network combines both approaches by projecting the spatial graph's vertices onto Laplacian eigenvectors in a fully connected ""spectral graph"" and applying learned message passing. We apply this model to various benchmark tasks and find that it promotes efficient training, reaching high performance with fewer training iterations despite having more parameters. The model also provides robustness to edge dropout and outperforms baselines for classification tasks. We explore how these performance benefits depend on dataset properties.",1
"Graph Neural Networks (GNNs) have been shown to be effective models for different predictive tasks on graph-structured data. Recent work on their expressive power has focused on isomorphism tasks and countable feature spaces. We extend this theoretical framework to include continuous features - which occur regularly in real-world input domains and within the hidden layers of GNNs - and we demonstrate the requirement for multiple aggregation functions in this context. Accordingly, we propose Principal Neighbourhood Aggregation (PNA), a novel architecture combining multiple aggregators with degree-scalers (which generalize the sum aggregator). Finally, we compare the capacity of different models to capture and exploit the graph structure via a novel benchmark containing multiple tasks taken from classical graph theory, alongside existing benchmarks from real-world domains, all of which demonstrate the strength of our model. With this work, we hope to steer some of the GNN research towards new aggregation methods which we believe are essential in the search for powerful and robust models.",0
"Different predictive tasks on graph-structured data have been effectively modeled by Graph Neural Networks (GNNs). Recent studies concerning their expressive power have mainly focused on countable feature spaces and isomorphism tasks. However, real-world input domains and GNNs' hidden layers frequently involve continuous features, which are not incorporated into the theoretical framework. We show the significance of multiple aggregation functions in this context and propose the Principal Neighbourhood Aggregation (PNA), a novel architecture that combines several aggregators with degree-scalers. Furthermore, we introduce a new benchmark that includes various tasks from classical graph theory and previous benchmarks from real-world domains to compare the capacity of different models in capturing and exploiting the graph structure. Our model proves to be robust and powerful, emphasizing the importance of new aggregation methods in GNN research.",1
"When compared to unimodal systems, multimodal biometric systems have several advantages, including lower error rate, higher accuracy, and larger population coverage. However, multimodal systems have an increased demand for integrity and privacy because they must store multiple biometric traits associated with each user. In this paper, we present a deep learning framework for feature-level fusion that generates a secure multimodal template from each user's face and iris biometrics. We integrate a deep hashing (binarization) technique into the fusion architecture to generate a robust binary multimodal shared latent representation. Further, we employ a hybrid secure architecture by combining cancelable biometrics with secure sketch techniques and integrate it with a deep hashing framework, which makes it computationally prohibitive to forge a combination of multiple biometrics that pass the authentication. The efficacy of the proposed approach is shown using a multimodal database of face and iris and it is observed that the matching performance is improved due to the fusion of multiple biometrics. Furthermore, the proposed approach also provides cancelability and unlinkability of the templates along with improved privacy of the biometric data. Additionally, we also test the proposed hashing function for an image retrieval application using a benchmark dataset. The main goal of this paper is to develop a method for integrating multimodal fusion, deep hashing, and biometric security, with an emphasis on structural data from modalities like face and iris. The proposed approach is in no way a general biometric security framework that can be applied to all biometric modalities, as further research is needed to extend the proposed framework to other unconstrained biometric modalities.",0
"Multimodal biometric systems offer several advantages over unimodal systems, such as increased accuracy, lower error rates, and wider population coverage. However, these systems require enhanced privacy and integrity measures due to the need to store multiple biometric traits per user. This study presents a deep learning feature-level fusion framework that generates secure multimodal templates from face and iris biometrics. The framework integrates deep hashing techniques to produce a binary multimodal shared latent representation, along with a hybrid architecture that combines cancelable biometrics and secure sketch techniques to prevent authentication forgery. The proposed approach is tested using a database of face and iris biometrics, demonstrating improved matching performance and better privacy protection. Additionally, the proposed hashing function is tested for image retrieval applications using a benchmark dataset. The study primarily focuses on integrating multimodal fusion, deep hashing, and biometric security for face and iris modalities, and further research is needed to extend the framework to other biometric modalities.",1
"In this paper, we propose a novel hierarchical representation via message propagation (HRMP) method for robust model fitting, which simultaneously takes advantages of both the consensus analysis and the preference analysis to estimate the parameters of multiple model instances from data corrupted by outliers, for robust model fitting. Instead of analyzing the information of each data point or each model hypothesis independently, we formulate the consensus information and the preference information as a hierarchical representation to alleviate the sensitivity to gross outliers. Specifically, we firstly construct a hierarchical representation, which consists of a model hypothesis layer and a data point layer. The model hypothesis layer is used to remove insignificant model hypotheses and the data point layer is used to remove gross outliers. Then, based on the hierarchical representation, we propose an effective hierarchical message propagation (HMP) algorithm and an improved affinity propagation (IAP) algorithm to prune insignificant vertices and cluster the remaining data points, respectively. The proposed HRMP can not only accurately estimate the number and parameters of multiple model instances, but also handle multi-structural data contaminated with a large number of outliers. Experimental results on both synthetic data and real images show that the proposed HRMP significantly outperforms several state-of-the-art model fitting methods in terms of fitting accuracy and speed.",0
"Our paper presents a new method called hierarchical representation via message propagation (HRMP) for robust model fitting. HRMP combines consensus and preference analyses to estimate parameters in the presence of outliers. Instead of analyzing data points and model hypotheses independently, we create a hierarchical representation that captures consensus and preference information to reduce sensitivity to gross outliers. This representation has two layers, one for model hypotheses and one for data points. We then use an effective hierarchical message propagation (HMP) algorithm and an improved affinity propagation (IAP) algorithm to prune insignificant vertices and cluster remaining data points, respectively. HRMP can accurately estimate parameters for multiple model instances and handle multi-structural data with many outliers. Our experiments with synthetic and real data demonstrate that HRMP outperforms state-of-the-art model fitting methods in terms of accuracy and speed.",1
"Semi-supervised node classification on graph-structured data has many applications such as fraud detection, fake account and review detection, user's private attribute inference in social networks, and community detection. Various methods such as pairwise Markov Random Fields (pMRF) and graph neural networks were developed for semi-supervised node classification. pMRF is more efficient than graph neural networks. However, existing pMRF-based methods are less accurate than graph neural networks, due to a key limitation that they assume a heuristics-based constant edge potential for all edges. In this work, we aim to address the key limitation of existing pMRF-based methods. In particular, we propose to learn edge potentials for pMRF. Our evaluation results on various types of graph datasets show that our optimized pMRF-based method consistently outperforms existing graph neural networks in terms of both accuracy and efficiency. Our results highlight that previous work may have underestimated the power of pMRF for semi-supervised node classification.",0
"There are numerous practical applications for semi-supervised node classification on graph-structured data, including detecting fraud, fake accounts, and reviews, inferring private attributes of users on social networks, and identifying communities. Various techniques have been developed for semi-supervised node classification, such as pairwise Markov Random Fields (pMRF) and graph neural networks. While pMRF is more efficient than graph neural networks, existing pMRF-based methods are less accurate due to their reliance on a heuristics-based constant edge potential for all edges. This study aims to overcome this limitation by proposing a method to learn edge potentials for pMRF. Our evaluation on different types of graph datasets shows that our optimized pMRF-based method outperforms existing graph neural networks in both accuracy and efficiency. Our findings suggest that pMRF is more powerful than previous research has suggested for semi-supervised node classification.",1
"Mesh is a powerful data structure for 3D shapes. Representation learning for 3D meshes is important in many computer vision and graphics applications. The recent success of convolutional neural networks (CNNs) for structured data (e.g., images) suggests the value of adapting insight from CNN for 3D shapes. However, 3D shape data are irregular since each node's neighbors are unordered. Various graph neural networks for 3D shapes have been developed with isotropic filters or predefined local coordinate systems to overcome the node inconsistency on graphs. However, isotropic filters or predefined local coordinate systems limit the representation power. In this paper, we propose a local structure-aware anisotropic convolutional operation (LSA-Conv) that learns adaptive weighting matrices for each node according to the local neighboring structure and performs shared anisotropic filters. In fact, the learnable weighting matrix is similar to the attention matrix in the random synthesizer -- a new Transformer model for natural language processing (NLP). Comprehensive experiments demonstrate that our model produces significant improvement in 3D shape reconstruction compared to state-of-the-art methods.",0
"The utilization of Mesh as a data structure for 3D shapes holds great potential. The study of representation learning for 3D meshes is highly crucial in various computer vision and graphics applications. The prosperity of convolutional neural networks (CNNs) for structured data, such as images, implies that it is valuable to adapt insights from CNNs for 3D shapes. Nonetheless, 3D shape information is irregular due to the unordered neighbors of each node. Graph neural networks for 3D shapes have been developed using isotropic filters or predefined local coordinate systems to address the node inconsistency on graphs. However, such filters or systems limit the representation power. In this study, we propose a local structure-aware anisotropic convolutional operation (LSA-Conv) that learns adaptive weighting matrices for each node based on their local neighboring structure and executes shared anisotropic filters. The learnable weighting matrix is similar to the attention matrix used in the random synthesizer, a new Transformer model for natural language processing (NLP). We conducted comprehensive experiments to demonstrate that our model significantly improves 3D shape reconstruction compared to current state-of-the-art methods.",1
"Spectral graph convolutional networks are generalizations of standard convolutional networks for graph-structured data using the Laplacian operator. A common misconception is the instability of spectral filters, i.e. the impossibility to transfer spectral filters between graphs of variable size and topology. This misbelief has limited the development of spectral networks for multi-graph tasks in favor of spatial graph networks. However, recent works have proved the stability of spectral filters under graph perturbation. Our work complements and emphasizes further the high quality of spectral transferability by benchmarking spectral graph networks on tasks involving graphs of different size and connectivity. Numerical experiments exhibit favorable performance on graph regression, graph classification, and node classification problems on two graph benchmarks. The implementation of our experiments is available on GitHub for reproducibility.",0
"Spectral graph convolutional networks utilize the Laplacian operator to extend standard convolutional networks for graph-structured data. It is commonly believed that spectral filters are unstable and cannot be transferred between graphs with varying size and topology. This belief has hindered the development of spectral networks for multi-graph tasks, leading to a preference for spatial graph networks. Recent research has demonstrated that spectral filters are, in fact, stable under graph perturbation. Our work builds upon this research by testing the transferability of spectral graph networks on tasks involving graphs of differing size and connectivity. Our experiments show that spectral graph networks perform favorably on graph regression, graph classification, and node classification problems across two graph benchmarks. We have shared the implementation of our experiments on GitHub for reproducibility purposes.",1
"Named Entity Recognition has been extensively investigated in many fields. However, the application of sensitive entity detection for production systems in financial institutions has not been well explored due to the lack of publicly available, labeled datasets. In this paper, we use internal and synthetic datasets to evaluate various methods of detecting NPI (Nonpublic Personally Identifiable) information commonly found within financial institutions, in both unstructured and structured data formats. Character-level neural network models including CNN, LSTM, BiLSTM-CRF, and CNN-CRF are investigated on two prediction tasks: (i) entity detection on multiple data formats, and (ii) column-wise entity prediction on tabular datasets. We compare these models with other standard approaches on both real and synthetic data, with respect to F1-score, precision, recall, and throughput. The real datasets include internal structured data and public email data with manually tagged labels. Our experimental results show that the CNN model is simple yet effective with respect to accuracy and throughput and thus, is the most suitable candidate model to be deployed in the production environment(s). Finally, we provide several lessons learned on data limitations, data labelling and the intrinsic overlap of data entities.",0
"The recognition of Named Entities has been widely researched across various domains. However, there is a lack of research on the detection of sensitive entities in financial institutions due to the unavailability of publicly labeled datasets. This study utilizes both internal and synthetic datasets to assess several approaches for detecting NPI information in structured and unstructured data formats. The research examines character-level neural network models, including BiLSTM-CRF, CNN-CRF, CNN, and LSTM, for two prediction tasks: (i) entity detection across multiple data formats, and (ii) column-wise entity prediction on tabular datasets. The models are compared with standard approaches on real and synthetic data, based on F1-score, precision, recall, and throughput. The real datasets include internal structured data and public email data with manually tagged labels. The results indicate that the CNN model is the most appropriate for deployment in production environments due to its simplicity and effectiveness in terms of accuracy and throughput. Additionally, the research highlights the limitations of the data, data labeling, and the overlap of data entities.",1
"Graph representation learning has many real-world applications, from super-resolution imaging, 3D computer vision to drug repurposing, protein classification, social networks analysis. An adequate representation of graph data is vital to the learning performance of a statistical or machine learning model for graph-structured data. In this paper, we propose a novel multiscale representation system for graph data, called decimated framelets, which form a localized tight frame on the graph. The decimated framelet system allows storage of the graph data representation on a coarse-grained chain and processes the graph data at multi scales where at each scale, the data is stored at a subgraph. Based on this, we then establish decimated G-framelet transforms for the decomposition and reconstruction of the graph data at multi resolutions via a constructive data-driven filter bank. The graph framelets are built on a chain-based orthonormal basis that supports fast graph Fourier transforms. From this, we give a fast algorithm for the decimated G-framelet transforms, or FGT, that has linear computational complexity O(N) for a graph of size N. The theory of decimated framelets and FGT is verified with numerical examples for random graphs. The effectiveness is demonstrated by real-world applications, including multiresolution analysis for traffic network, and graph neural networks for graph classification tasks.",0
"There are numerous practical uses for graph representation learning, such as in super-resolution imaging, 3D computer vision, drug repurposing, protein classification, and social network analysis. A proper representation of graph data is crucial for the learning performance of statistical or machine learning models that deal with graph-structured data. This paper introduces a new multiscale representation system for graph data called decimated framelets, which create a tight frame localized on the graph. The decimated framelet system enables the storage of graph data representation on a coarse-grained chain and processes the data at multiple scales where each scale stores the data in a subgraph. We then establish decimated G-framelet transforms through a data-driven filter bank for the decomposition and reconstruction of graph data at multiple resolutions. The graph framelets are based on a chain-based orthonormal basis that supports fast graph Fourier transforms, and we present a fast algorithm for the decimated G-framelet transforms, or FGT, which has linear computational complexity for a graph of any size. We verify the theory of decimated framelets and FGT through numerical examples for random graphs and demonstrate their effectiveness through real-world applications, such as multiresolution analysis for traffic networks and graph neural networks for graph classification tasks.",1
"Most of the existing coastal flood Forecast and Early-Warning Systems do not model the flood, but instead, rely on the prediction of hydrodynamic conditions at the coast and on expert judgment. Recent scientific contributions are now capable to precisely model flood events, even in situations where wave overtopping plays a significant role. Such models are nevertheless costly-to-evaluate and surrogate ones need to be exploited for substantial computational savings. For the latter models, the hydro-meteorological forcing conditions (inputs) or flood events (outputs) are conveniently parametrised into scalar representations. However, they neglect the fact that inputs are actually functions (more precisely, time series), and that floods spatially propagate inland. Here, we introduce a multi-output Gaussian process model accounting for both criteria. On various examples, we test its versatility for both learning spatial maps and inferring unobserved ones. We demonstrate that efficient implementations are obtained by considering tensor-structured data and/or sparse-variational approximations. Finally, the proposed framework is applied on a coastal application aiming at predicting flood events. We conclude that accurate predictions are obtained in the order of minutes rather than the couples of days required by dedicated hydrodynamic simulators.",0
"Many coastal flood Forecast and Early-Warning Systems do not use flood modeling and instead rely on expert judgment and hydrodynamic conditions to make predictions. However, recent scientific advancements have allowed for precise flood modeling, even in situations where wave overtopping is a factor. While these models are expensive to evaluate, surrogate models can be used to save on computations. These surrogate models use scalar representations of inputs and outputs, but fail to account for the fact that inputs are time series and floods propagate spatially. In this study, a multi-output Gaussian process model is introduced that addresses both of these issues. This model is tested on various examples and is shown to be efficient when using tensor-structured data and sparse-variational approximations. Finally, the model is applied to predict coastal flood events, and it is found that accurate predictions can be made in minutes rather than days.",1
"Class imbalance is a challenging issue in practical classification problems for deep learning models as well as for traditional models. Traditionally successful countermeasures such as synthetic over-sampling have had limited success with complex, structured data handled by deep learning models. In this work, we propose to use a Generative Adversarial Network (GAN) equipped with a generator network G, a discriminator network D and a classifier network C to remove the class-imbalance in visual data sets. The generator network is initialized with auto-encoder to make it stable. The discriminator D ensures that G adheres to class distribution of imbalanced class. In conventional methods, where Generator G competes with discriminator D in a min-max game, we propose to further add an additional classifier network to the original network. Now, the generator network tries to compete in a min-max game with Discriminator as well as the new classifier that we have introduced. An additional condition is enforced on generator network G to produce points in the convex hull of desired imbalanced class. Further the contention of adversarial game with classifier C, pushes conditional distribution learned by G towards the periphery of the respective class, compensating the problem of class imbalance. Experimental evidence shows that this initialization results in stable training of the network. We achieve state of the art performance on extreme visual classification task on the FashionMNIST, MNIST, SVHN, ExDark, MVTec Anomaly Detection dataset, Chest X-Ray dataset and others.",0
"In both deep learning and traditional models, class imbalance presents a difficult issue in practical classification problems. While synthetic over-sampling has been previously successful in addressing this issue, it has proven ineffective with complex, structured data that is handled by deep learning models. To alleviate this problem, we propose the use of a Generative Adversarial Network (GAN) that is equipped with a generator network (G), a discriminator network (D), and a classifier network (C) to remove class imbalance in visual data sets. The generator network is initialized with an auto-encoder to ensure its stability, while the discriminator ensures that G adheres to the class distribution of the imbalanced class. We propose adding an additional classifier network to the original network, which results in the generator network competing in a min-max game with both the discriminator and this new classifier. To produce points in the convex hull of the desired imbalanced class, an additional condition is enforced on the generator network. The adversarial game with classifier C pushes the conditional distribution learned by G towards the periphery of the respective class, which compensates for the problem of class imbalance. Our experimental evidence shows that this initialization results in stable training of the network and achieves state-of-the-art performance on extreme visual classification tasks on various datasets.",1
"Deep Neural Network (DNN) models have vulnerabilities related to security concerns, with attackers usually employing complex hacking techniques to expose their structures. Data poisoning-enabled perturbation attacks are complex adversarial ones that inject false data into models. They negatively impact the learning process, with no benefit to deeper networks, as they degrade a model's accuracy and convergence rates. In this paper, we propose an attack-agnostic-based defense method for mitigating their influence. In it, a Defensive Feature Layer (DFL) is integrated with a well-known DNN architecture which assists in neutralizing the effects of illegitimate perturbation samples in the feature space. To boost the robustness and trustworthiness of this method for correctly classifying attacked input samples, we regularize the hidden space of a trained model with a discriminative loss function called Polarized Contrastive Loss (PCL). It improves discrimination among samples in different classes and maintains the resemblance of those in the same class. Also, we integrate a DFL and PCL in a compact model for defending against data poisoning attacks. This method is trained and tested using the CIFAR-10 and MNIST datasets with data poisoning-enabled perturbation attacks, with the experimental results revealing its excellent performance compared with those of recent peer techniques.",0
"Security concerns related to Deep Neural Network (DNN) models have led to the development of complex hacking techniques by attackers to expose their vulnerabilities. Adversarial attacks, such as data poisoning-enabled perturbation attacks, inject false data into models which negatively affect the learning process and degrade a model's accuracy and convergence rates. In this paper, we propose an attack-agnostic-based defense method that integrates a Defensive Feature Layer (DFL) with a well-known DNN architecture to neutralize the effects of illegitimate perturbation samples in the feature space. To improve the robustness and trustworthiness of the method, we regularize the hidden space of a trained model with a discriminative loss function called Polarized Contrastive Loss (PCL). This method is trained and tested on CIFAR-10 and MNIST datasets, and the experimental results demonstrate its superior performance compared to recent peer techniques. Additionally, we integrate DFL and PCL in a compact model for defending against data poisoning attacks.",1
"Self-supervised learning is currently gaining a lot of attention, as it allows neural networks to learn robust representations from large quantities of unlabeled data. Additionally, multi-task learning can further improve representation learning by training networks simultaneously on related tasks, leading to significant performance improvements. In this paper, we propose three novel self-supervised auxiliary tasks to train graph-based neural network models in a multi-task fashion. Since Graph Convolutional Networks are among the most promising approaches for capturing relationships among structured data points, we use them as a building block to achieve competitive results on standard semi-supervised graph classification tasks.",0
"The present focus is on self-supervised learning, which empowers neural networks to acquire strong representations from vast amounts of unlabeled data. Furthermore, multi-task learning can enhance this representation learning by training networks concurrently on linked tasks, resulting in noteworthy performance advancements. Our paper suggests three new self-supervised auxiliary tasks for training graph-based neural network models in a multi-task manner. We use Graph Convolutional Networks as a foundation since they are one of the most encouraging techniques for capturing structured data point relationships. This approach helps us achieve competitive outcomes on standard semi-supervised graph classification tasks.",1
"Understanding the reasons for the success of deep neural networks trained using stochastic gradient-based methods is a key open problem for the nascent theory of deep learning. The types of data where these networks are most successful, such as images or sequences of speech, are characterised by intricate correlations. Yet, most theoretical work on neural networks does not explicitly model training data, or assumes that elements of each data sample are drawn independently from some factorised probability distribution. These approaches are thus by construction blind to the correlation structure of real-world data sets and their impact on learning in neural networks. Here, we introduce a generative model for structured data sets that we call the hidden manifold model (HMM). The idea is to construct high-dimensional inputs that lie on a lower-dimensional manifold, with labels that depend only on their position within this manifold, akin to a single layer decoder or generator in a generative adversarial network. We demonstrate that learning of the hidden manifold model is amenable to an analytical treatment by proving a ""Gaussian Equivalence Property"" (GEP), and we use the GEP to show how the dynamics of two-layer neural networks trained using one-pass stochastic gradient descent is captured by a set of integro-differential equations that track the performance of the network at all times. This permits us to analyse in detail how a neural network learns functions of increasing complexity during training, how its performance depends on its size and how it is impacted by parameters such as the learning rate or the dimension of the hidden manifold.",0
"The success of deep neural networks trained through stochastic gradient-based methods is a major issue that needs to be addressed by the emerging theory of deep learning. These networks perform best on data sets that have complex correlations, such as images or speech sequences. However, the majority of theoretical work on neural networks does not account for the correlation structure of real-world data sets, assuming that each element of the sample is drawn independently from a factorised probability distribution. In this study, we introduce the hidden manifold model (HMM) as a generative model for structured data sets. The HMM constructs high-dimensional inputs that exist on a lower-dimensional manifold, with labels dependent on their position within this manifold. We prove a ""Gaussian Equivalence Property"" (GEP) that permits us to analyse the dynamics of two-layer neural networks trained using one-pass stochastic gradient descent. This analysis allows us to examine in detail how the neural network learns functions of increasing complexity during training, how its performance relies on its size, and how it is affected by parameters such as the learning rate or the dimension of the hidden manifold.",1
"Many modern data analyses benefit from explicitly modeling dependence structure in data -- such as measurements across time or space, ordered words in a sentence, or genes in a genome. A gold standard evaluation technique is structured cross-validation (CV), which leaves out some data subset (such as data within a time interval or data in a geographic region) in each fold. But CV here can be prohibitively slow due to the need to re-run already-expensive learning algorithms many times. Previous work has shown approximate cross-validation (ACV) methods provide a fast and provably accurate alternative in the setting of empirical risk minimization. But this existing ACV work is restricted to simpler models by the assumptions that (i) data across CV folds are independent and (ii) an exact initial model fit is available. In structured data analyses, both these assumptions are often untrue. In the present work, we address (i) by extending ACV to CV schemes with dependence structure between the folds. To address (ii), we verify -- both theoretically and empirically -- that ACV quality deteriorates smoothly with noise in the initial fit. We demonstrate the accuracy and computational benefits of our proposed methods on a diverse set of real-world applications.",0
"Explicitly modeling dependence structure in data can be advantageous for many modern data analyses, including measurements across time or space, ordered words in a sentence, or genes in a genome. Structured cross-validation (CV) is a widely accepted evaluation technique that omits some data subset, such as data within a time interval or data in a geographic region, in each fold. However, CV can be time-consuming due to the need to re-run already-expensive learning algorithms many times. Previous research has demonstrated that approximate cross-validation (ACV) methods are a fast and reliable alternative for empirical risk minimization. However, the existing ACV work is limited to simpler models due to two assumptions: that data across CV folds are independent, and that an exact initial model fit is available. In structured data analyses, these assumptions are often incorrect. In this work, we address both assumptions by extending ACV to CV schemes with dependence structure between the folds and verifying, both theoretically and empirically, that ACV quality deteriorates smoothly with noise in the initial fit. We illustrate the accuracy and computational benefits of our proposed methods on a variety of real-world applications.",1
"We consider the problem of learning a manifold from a teacher's demonstration. Extending existing approaches of learning from randomly sampled data points, we consider contexts where data may be chosen by a teacher. We analyze learning from teachers who can provide structured data such as individual examples (isolated data points) and demonstrations (sequences of points). Our analysis shows that for the purpose of teaching the topology of a manifold, demonstrations can yield remarkable decreases in the amount of data points required in comparison to teaching with randomly sampled points. We also discuss the implications of our analysis for learning in humans and machines.",0
"The focus of our study is on acquiring knowledge of a manifold using a teacher's guidance. Instead of relying on the traditional method of learning from randomly selected data points, we explore scenarios where the teacher can choose the data. Our investigation includes examining the benefits of learning from structured data, including isolated data points and point sequences that the teacher provides. We have found that demonstrations can significantly reduce the number of data points needed to teach the topology of a manifold compared to randomly selected points. Additionally, we discuss the implications of our findings for both human and machine learning.",1
"Graph-structured data exist in numerous applications in real life. As a state-of-the-art graph neural network, the graph convolutional network (GCN) plays an important role in processing graph-structured data. However, a recent study reported that GCNs are also vulnerable to adversarial attacks, which means that GCN models may suffer malicious attacks with unnoticeable modifications of the data. Among all the adversarial attacks on GCNs, there is a special kind of attack method called the universal adversarial attack, which generates a perturbation that can be applied to any sample and causes GCN models to output incorrect results. Although universal adversarial attacks in computer vision have been extensively researched, there are few research works on universal adversarial attacks on graph structured data. In this paper, we propose a targeted universal adversarial attack against GCNs. Our method employs a few nodes as the attack nodes. The attack capability of the attack nodes is enhanced through a small number of fake nodes connected to them. During an attack, any victim node will be misclassified by the GCN as the attack node class as long as it is linked to them. The experiments on three popular datasets show that the average attack success rate of the proposed attack on any victim node in the graph reaches 83% when using only 3 attack nodes and 6 fake nodes. We hope that our work will make the community aware of the threat of this type of attack and raise the attention given to its future defense.",0
"Graph-structured data is prevalent in various real-life applications, and the graph convolutional network (GCN) is a state-of-the-art neural network that plays a significant role in processing such data. However, recent research has revealed that GCNs are susceptible to adversarial attacks, which can modify data in an unnoticeable manner and cause GCN models to malfunction. One such attack is the universal adversarial attack, which produces a perturbation capable of affecting any sample and causing GCN models to produce incorrect results. Despite the extensive research on universal adversarial attacks in computer vision, very little work has been done on these attacks in graph-structured data. In this paper, we present a targeted universal adversarial attack against GCNs that leverages a few attack nodes and a small number of fake nodes to enhance the attack's capability. During an attack, any victim node linked to the attack nodes will be misclassified as the attack node class. Our experiments on three popular datasets indicate that our proposed attack achieves an average success rate of 83% using only 3 attack nodes and 6 fake nodes. By highlighting the dangers of this type of attack, we hope to raise awareness and promote future research in this area's defense.",1
"Graph Neural Networks (GNNs) are a popular approach for predicting graph structured data. As GNNs tightly entangle the input graph into the neural network structure, common explainable AI approaches are not applicable. To a large extent, GNNs have remained black-boxes for the user so far. In this paper, we show that GNNs can in fact be naturally explained using higher-order expansions, i.e. by identifying groups of edges that jointly contribute to the prediction. Practically, we find that such explanations can be extracted using a nested attribution scheme, where existing techniques such as layer-wise relevance propagation (LRP) can be applied at each step. The output is a collection of walks into the input graph that are relevant for the prediction. Our novel explanation method, which we denote by GNN-LRP, is applicable to a broad range of graph neural networks and lets us extract practically relevant insights on sentiment analysis of text data, structure-property relationships in quantum chemistry, and image classification.",0
"GNNs are a well-known method for predicting data that is structured in the form of a graph. However, traditional approaches to explainable AI cannot be effectively applied to GNNs because the input graph is tightly intertwined with the neural network structure. This has resulted in GNNs being considered black-boxes for users. This paper presents a novel method called GNN-LRP, which uses higher-order expansions to explain GNNs. By identifying groups of edges that contribute to the prediction, GNN-LRP allows for the extraction of relevant insights through a nested attribution scheme. This method can be applied to a variety of graph neural networks and has been shown to provide valuable insights in areas such as sentiment analysis, quantum chemistry, and image classification.",1
"Trust and credibility in machine learning models is bolstered by the ability of a model to explain itsdecisions. While explainability of deep learning models is a well-known challenge, a further chal-lenge is clarity of the explanation itself, which must be interpreted by downstream users. Layer-wiseRelevance Propagation (LRP), an established explainability technique developed for deep models incomputer vision, provides intuitive human-readable heat maps of input images. We present the novelapplication of LRP for the first time with structured datasets using a deep neural network (1D-CNN),for Credit Card Fraud detection and Telecom Customer Churn prediction datasets. We show how LRPis more effective than traditional explainability concepts of Local Interpretable Model-agnostic Ex-planations (LIME) and Shapley Additive Explanations (SHAP) for explainability. This effectivenessis both local to a sample level and holistic over the whole testing set. We also discuss the significantcomputational time advantage of LRP (1-2s) over LIME (22s) and SHAP (108s), and thus its poten-tial for real time application scenarios. In addition, our validation of LRP has highlighted features forenhancing model performance, thus opening up a new area of research of using XAI as an approachfor feature subset selection",0
"The trustworthiness and reliability of machine learning models are enhanced when they can provide explanations for their decisions. Although deep learning models have a known challenge in terms of explainability, another challenge is ensuring that the explanation itself is clear enough for downstream users to understand. To address this, an established explainability technique called Layer-wise Relevance Propagation (LRP) has been developed for deep models in computer vision, which provides human-readable heat maps of input images. In this study, we introduce the novel application of LRP for the first time with structured datasets using a deep neural network (1D-CNN) for Credit Card Fraud detection and Telecom Customer Churn prediction datasets. We demonstrate that LRP is more effective than traditional explainability concepts such as Local Interpretable Model-agnostic Explanations (LIME) and Shapley Additive Explanations (SHAP) for explainability, both locally and holistically. Furthermore, LRP has a significant computational time advantage (1-2s) over LIME (22s) and SHAP (108s), making it a potential option for real-time application scenarios. Additionally, our validation of LRP has identified features that can enhance model performance, opening up a new area of research using XAI as an approach for feature subset selection.",1
"Deep Learning architectures, albeit successful in most computer vision tasks, were designed for data with an underlying Euclidean structure, which is not usually fulfilled since pre-processed data may lie on a non-linear space. In this paper, we propose a geometry aware deep learning approach for skeleton-based action recognition. Skeleton sequences are first modeled as trajectories on Kendall's shape space and then mapped to the linear tangent space. The resulting structured data are then fed to a deep learning architecture, which includes a layer that optimizes over rigid and non rigid transformations of the 3D skeletons, followed by a CNN-LSTM network. The assessment on two large scale skeleton datasets, namely NTU-RGB+D and NTU-RGB+D 120, has proven that proposed approach outperforms existing geometric deep learning methods and is competitive with respect to recently published approaches.",0
"While Deep Learning architectures have proven effective in most computer vision tasks, they were originally designed to work with data having an underlying Euclidean structure. However, pre-processed data often exists in a non-linear space, making these architectures less effective. To address this issue, we present a geometry-aware deep learning approach for skeleton-based action recognition. Our approach first models skeleton sequences as trajectories on Kendall's shape space and then maps them to the linear tangent space. The resulting structured data is then fed to a deep learning architecture, which includes a layer that optimizes over both rigid and non-rigid transformations of the 3D skeletons, followed by a CNN-LSTM network. Our approach was assessed on two large-scale skeleton datasets, namely NTU-RGB+D and NTU-RGB+D 120, and was found to outperform existing geometric deep learning methods and to be competitive with recently published approaches.",1
"Time series are all around in real-world applications. However, unexpected accidents for example broken sensors or missing of the signals will cause missing values in time series, making the data hard to be utilized. It then does harm to the downstream applications such as traditional classification or regression, sequential data integration and forecasting tasks, thus raising the demand for data imputation. Currently, time series data imputation is a well-studied problem with different categories of methods. However, these works rarely take the temporal relations among the observations and treat the time series as normal structured data, losing the information from the time data. In recent, deep learning models have raised great attention. Time series methods based on deep learning have made progress with the usage of models like RNN, since it captures time information from data. In this paper, we mainly focus on time series imputation technique with deep learning methods, which recently made progress in this field. We will review and discuss their model architectures, their pros and cons as well as their effects to show the development of the time series imputation methods.",0
"In various real-world applications, time series are commonly used. Nevertheless, unforeseen events like malfunctioning sensors or missing signals can lead to missing values in time series, which can make the data difficult to use. This can negatively affect downstream applications such as sequential data integration, traditional classification or regression, and forecasting tasks, thus leading to a demand for data imputation. Although there are various categories of methods available for time series data imputation, these methods often do not consider the temporal relations between observations. Instead, they treat the time series as normal structured data, thereby losing valuable time data information. Recently, deep learning models such as RNN have been gaining attention as they capture time information from data. This paper concentrates on imputation techniques for time series using deep learning methods, which have made significant progress in this field. The paper reviews and discusses the model architectures, pros and cons, and effects of deep learning methods to demonstrate the development of time series imputation methods.",1
"Graph neural networks (GNNs) are a popular class of parametric model for learning over graph-structured data. Recent work has argued that GNNs primarily use the graph for feature smoothing, and have shown competitive results on benchmark tasks by simply operating on graph-smoothed node features, rather than using end-to-end learned feature hierarchies that are challenging to scale to large graphs. In this work, we ask whether these results can be extended to heterogeneous graphs, which encode multiple types of relationship between different entities. We propose Neighbor Averaging over Relation Subgraphs (NARS), which trains a classifier on neighbor-averaged features for randomly-sampled subgraphs of the ""metagraph"" of relations. We describe optimizations to allow these sets of node features to be computed in a memory-efficient way, both at training and inference time. NARS achieves a new state of the art accuracy on several benchmark datasets, outperforming more expensive GNN-based methods",0
"Parametric models called Graph Neural Networks (GNNs) are widely used for learning over graph-structured data. Recent research has suggested that GNNs mainly use the graph for feature smoothing, and can achieve competitive results on benchmark tasks by operating on graph-smoothed node features, instead of using complex learned feature hierarchies that are difficult to scale for large graphs. This study aims to determine if these findings extend to heterogeneous graphs that encode multiple types of relationships between different entities. To accomplish this, the researchers propose Neighbor Averaging over Relation Subgraphs (NARS), which trains a classifier on neighbor-averaged features for randomly-sampled subgraphs of the ""metagraph"" of relations. The researchers describe optimizations to compute these sets of node features in a memory-efficient way during both training and inference time. NARS surpasses more expensive GNN-based methods and achieves a new state of the art accuracy on various benchmark datasets.",1
"A model involving Gaussian processes (GPs) is introduced to simultaneously handle multi-task learning, clustering, and prediction for multiple functional data. This procedure acts as a model-based clustering method for functional data as well as a learning step for subsequent predictions for new tasks. The model is instantiated as a mixture of multi-task GPs with common mean processes. A variational EM algorithm is derived for dealing with the optimisation of the hyper-parameters along with the hyper-posteriors' estimation of latent variables and processes. We establish explicit formulas for integrating the mean processes and the latent clustering variables within a predictive distribution, accounting for uncertainty on both aspects. This distribution is defined as a mixture of cluster-specific GP predictions, which enhances the performances when dealing with group-structured data. The model handles irregular grid of observations and offers different hypotheses on the covariance structure for sharing additional information across tasks. The performances on both clustering and prediction tasks are assessed through various simulated scenarios and real datasets. The overall algorithm, called MagmaClust, is publicly available as an R package.",0
"The article presents a new approach that utilizes Gaussian processes (GPs) to address multi-task learning, clustering, and prediction for multiple functional data at the same time. This methodology serves as a model-based clustering method for functional data and a learning phase for subsequent predictions for new tasks. The model is composed of a mixture of multi-task GPs with common mean processes. A variational EM algorithm is used to optimize the hyper-parameters and estimate the hyper-posteriors of latent variables and processes. The study also provides explicit formulas to integrate mean processes and latent clustering variables within a predictive distribution, accounting for uncertainty on both aspects. This distribution is defined as a mixture of cluster-specific GP predictions, which improves performance when dealing with group-structured data. The model can handle irregular grids of observations and offers different hypotheses on the covariance structure to share additional information across tasks. The effectiveness of the model is evaluated through various simulated scenarios and real datasets. The publicly available R package, named MagmaClust, implements the overall algorithm.",1
"Graph neural networks~(GNNs) apply deep learning techniques to graph-structured data and have achieved promising performance in graph representation learning. However, existing GNNs rely heavily on enough labels or well-designed negative samples. To address these issues, we propose a new self-supervised graph representation method: deep graph bootstrapping~(DGB). DGB consists of two neural networks: online and target networks, and the input of them are different augmented views of the initial graph. The online network is trained to predict the target network while the target network is updated with a slow-moving average of the online network, which means the online and target networks can learn from each other. As a result, the proposed DGB can learn graph representation without negative examples in an unsupervised manner. In addition, we summarize three kinds of augmentation methods for graph-structured data and apply them to the DGB. Experiments on the benchmark datasets show the DGB performs better than the current state-of-the-art methods and how the augmentation methods affect the performances.",0
"The application of deep learning techniques to graph-structured data, known as Graph Neural Networks (GNNs), has proven to be successful in graph representation learning. However, current GNNs heavily rely on appropriate labeling and negative samples, posing issues for their effectiveness. To overcome these limitations, we propose a novel self-supervised graph representation method, dubbed Deep Graph Bootstrapping (DGB), which employs two neural networks: the online and target networks, both taking different augmented views of the initial graph as input. The online network predicts the target network, while the target network updates with a slow-moving average of the online network, enabling mutual learning. As a result, the DGB method can learn graph representation without negative examples in an unsupervised manner. Additionally, we summarize three graph-structured data augmentation methods and apply them to the DGB, achieving better performance than current state-of-the-art methods, as shown through experiments on benchmark datasets. We also analyze the effects of the augmentation methods on performance.",1
"Discrete structures play an important role in applications like program language modeling and software engineering. Current approaches to predicting complex structures typically consider autoregressive models for their tractability, with some sacrifice in flexibility. Energy-based models (EBMs) on the other hand offer a more flexible and thus more powerful approach to modeling such distributions, but require partition function estimation. In this paper we propose ALOE, a new algorithm for learning conditional and unconditional EBMs for discrete structured data, where parameter gradients are estimated using a learned sampler that mimics local search. We show that the energy function and sampler can be trained efficiently via a new variational form of power iteration, achieving a better trade-off between flexibility and tractability. Experimentally, we show that learning local search leads to significant improvements in challenging application domains. Most notably, we present an energy model guided fuzzer for software testing that achieves comparable performance to well engineered fuzzing engines like libfuzzer.",0
"Discrete structures are significant in various applications such as program language modeling and software engineering. To predict complex structures, autoregressive models are commonly used due to their ease of use, however, they are less flexible. In contrast, energy-based models (EBMs) provide a more robust approach to modeling such distributions, but require partition function estimation. This paper introduces ALOE, a novel algorithm that utilizes a learned sampler to estimate parameter gradients for conditional and unconditional EBMs for discrete structured data. The proposed method uses a variational form of power iteration to efficiently train the energy function and sampler, achieving a better balance between flexibility and tractability. The experimental results demonstrate that learning local search leads to significant improvements in challenging application domains. Particularly, the paper presents an energy model guided fuzzer for software testing that is comparable to well-engineered fuzzing engines such as libfuzzer.",1
"Graph-structured data are an integral part of many application domains, including chemoinformatics, computational biology, neuroimaging, and social network analysis. Over the last two decades, numerous graph kernels, i.e. kernel functions between graphs, have been proposed to solve the problem of assessing the similarity between graphs, thereby making it possible to perform predictions in both classification and regression settings. This manuscript provides a review of existing graph kernels, their applications, software plus data resources, and an empirical comparison of state-of-the-art graph kernels.",0
"Many areas of application, such as chemoinformatics, computational biology, neuroimaging, and social network analysis, rely on data structured in graphs. In the past twenty years, a variety of graph kernels, which are kernel functions that compare graphs to assess their similarity, have been introduced to address the challenge of predicting outcomes in both classification and regression settings. This article presents an overview of different graph kernels, their uses, software and data resources, and a comparative analysis of the latest graph kernels.",1
"In recent years, developing a speech understanding system that classifies a waveform to structured data, such as intents and slots, without first transcribing the speech to text has emerged as an interesting research problem. This work proposes such as system with an additional constraint of designing a system that has a small enough footprint to run on small micro-controllers and embedded systems with minimal latency. Given a streaming input speech signal, the proposed system can process it segment-by-segment without the need to have the entire stream at the moment of processing. The proposed system is evaluated on the publicly available Fluent Speech Commands dataset. Experiments show that the proposed system yields state-of-the-art performance with the advantage of low latency and a much smaller model when compared to other published works on the same task.",0
"Recently, there has been interest in developing a speech understanding system that can categorize a waveform into structured data, like intents and slots, without the need for transcription. This research proposes such a system, with the added constraint of being small enough to function on micro-controllers and embedded systems, and with minimal latency. The system processes streaming input speech signal in segments, without requiring the entire stream at once. The proposed system was tested on the Fluent Speech Commands dataset and demonstrated state-of-the-art performance, with the added benefits of low latency and a smaller model than other published works on the same task.",1
"Optimal Transport is a theory that allows to define geometrical notions of distance between probability distributions and to find correspondences, relationships, between sets of points. Many machine learning applications are derived from this theory, at the frontier between mathematics and optimization. This thesis proposes to study the complex scenario in which the different data belong to incomparable spaces. In particular we address the following questions: how to define and apply Optimal Transport between graphs, between structured data? How can it be adapted when the data are varied and not embedded in the same metric space? This thesis proposes a set of Optimal Transport tools for these different cases. An important part is notably devoted to the study of the Gromov-Wasserstein distance whose properties allow to define interesting transport problems on incomparable spaces. More broadly, we analyze the mathematical properties of the various proposed tools, we establish algorithmic solutions to compute them and we study their applicability in numerous machine learning scenarii which cover, in particular, classification, simplification, partitioning of structured data, as well as heterogeneous domain adaptation.",0
"The theory of Optimal Transport enables the definition of geometric distance between probability distributions and the identification of relationships between sets of points, leading to numerous machine learning applications. This thesis focuses on the challenging scenario where data belongs to disparate spaces. It examines how Optimal Transport can be applied to graphs and structured data, and adapted to varying data that are not embedded in the same metric space. The thesis presents a range of Optimal Transport tools for these different cases, with a significant focus on the Gromov-Wasserstein distance and its transport problems on incomparable spaces. The mathematical properties of the tools are analyzed, algorithmic solutions for computation are established, and their applicability in various machine learning scenarios - such as classification, simplification, partitioning of structured data, and heterogeneous domain adaptation - are studied.",1
"Modern machine learning applications should be able to address the intrinsic challenges arising over inference on massive real-world datasets, including scalability and robustness to outliers. Despite the multiple benefits of Bayesian methods (such as uncertainty-aware predictions, incorporation of experts knowledge, and hierarchical modeling), the quality of classic Bayesian inference depends critically on whether observations conform with the assumed data generating model, which is impossible to guarantee in practice. In this work, we propose a variational inference method that, in a principled way, can simultaneously scale to large datasets, and robustify the inferred posterior with respect to the existence of outliers in the observed data. Reformulating Bayes theorem via the $\beta$-divergence, we posit a robustified pseudo-Bayesian posterior as the target of inference. Moreover, relying on the recent formulations of Riemannian coresets for scalable Bayesian inference, we propose a sparse variational approximation of the robustified posterior and an efficient stochastic black-box algorithm to construct it. Overall our method allows releasing cleansed data summaries that can be applied broadly in scenarios including structured data corruption. We illustrate the applicability of our approach in diverse simulated and real datasets, and various statistical models, including Gaussian mean inference, logistic and neural linear regression, demonstrating its superiority to existing Bayesian summarization methods in the presence of outliers.",0
"To effectively handle real-world datasets in machine learning, it is crucial to address issues such as scalability and outlier robustness. Although Bayesian methods have advantages such as incorporating expert knowledge and uncertainty-aware predictions, their quality depends heavily on the accuracy of the assumed data generating model, which is not always feasible in practice. This study proposes a principled variational inference approach that can scale to large datasets and enhance the inferred posterior's robustness against outliers. The method reformulates Bayes theorem through the $\beta$-divergence and introduces a robustified pseudo-Bayesian posterior. Using Riemannian coresets for scalable Bayesian inference, the approach proposes a sparse variational approximation of the robustified posterior and an efficient stochastic black-box algorithm to construct it. The method allows for the release of cleansed data summaries applicable to various scenarios, including structured data corruption. The approach is tested on simulated and real datasets with different statistical models, including Gaussian mean inference, logistic and neural linear regression, demonstrating its superiority over existing Bayesian summarization methods in dealing with outliers.",1
"The ability to detect and count certain substructures in graphs is important for solving many tasks on graph-structured data, especially in the contexts of computational chemistry and biology as well as social network analysis. Inspired by this, we propose to study the expressive power of graph neural networks (GNNs) via their ability to count attributed graph substructures, extending recent works that examine their power in graph isomorphism testing and function approximation. We distinguish between two types of substructure counting: induced-subgraph-count and subgraph-count, and establish both positive and negative answers for popular GNN architectures. Specifically, we prove that Message Passing Neural Networks (MPNNs), 2-Weisfeiler-Lehman (2-WL) and 2-Invariant Graph Networks (2-IGNs) cannot perform induced-subgraph-count of substructures consisting of 3 or more nodes, while they can perform subgraph-count of star-shaped substructures. As an intermediary step, we prove that 2-WL and 2-IGNs are equivalent in distinguishing non-isomorphic graphs, partly answering an open problem raised in Maron et al. (2019). We also prove positive results for k-WL and k-IGNs as well as negative results for k-WL with a finite number of iterations. We then conduct experiments that support the theoretical results for MPNNs and 2-IGNs. Moreover, motivated by substructure counting and inspired by Murphy et al. (2019), we propose the Local Relational Pooling model and demonstrate that it is not only effective for substructure counting but also able to achieve competitive performance on molecular prediction tasks.",0
"The ability to identify and tally specific elements within graphs is crucial for resolving various graph-based problems, particularly in the fields of computational chemistry, biology, and social network analysis. To investigate the potential of graph neural networks (GNNs) in this area, we examine their capacity for counting attributed graph substructures, building on prior research that explored their effectiveness in graph isomorphism testing and function approximation. We define two categories of substructure counting - induced-subgraph-count and subgraph-count - and present both positive and negative outcomes for popular GNN architectures. Our findings reveal that Message Passing Neural Networks (MPNNs), 2-Weisfeiler-Lehman (2-WL), and 2-Invariant Graph Networks (2-IGNs) are unable to perform induced-subgraph-count on substructures comprising three or more nodes, but can successfully execute subgraph-count on star-shaped substructures. Additionally, we prove that 2-WL and 2-IGNs are equivalent in distinguishing non-isomorphic graphs, addressing an open question raised in Maron et al. (2019). We also establish positive results for k-WL and k-IGNs, as well as negative outcomes for k-WL with a finite number of iterations. Our experimental study corroborates the theoretical results for MPNNs and 2-IGNs. Furthermore, inspired by substructure counting and work by Murphy et al. (2019), we propose the Local Relational Pooling model, which not only performs well in substructure counting but also achieves competitive performance in molecular prediction tasks.",1
"We propose a flexible framework for clustering hypergraph-structured data based on recently proposed random walks utilizing edge-dependent vertex weights. When incorporating edge-dependent vertex weights (EDVW), a weight is associated with each vertex-hyperedge pair, yielding a weighted incidence matrix of the hypergraph. Such weightings have been utilized in term-document representations of text data sets. We explain how random walks with EDVW serve to construct different hypergraph Laplacian matrices, and then develop a suite of clustering methods that use these incidence matrices and Laplacians for hypergraph clustering. Using several data sets from real-life applications, we compare the performance of these clustering algorithms experimentally against a variety of existing hypergraph clustering methods. We show that the proposed methods produce higher-quality clusters and conclude by highlighting avenues for future work.",0
"Our proposal presents a versatile structure for clustering hypergraph-structured data, which is based on newly introduced random walks that utilize vertex weights dependent on the edges. By incorporating these edge-dependent vertex weights (EDVW), each vertex-hyperedge pair is assigned a weight, leading to a weighted incidence matrix of the hypergraph. This type of weighting has been utilized in term-document representations of text data sets. We illustrate how random walks with EDVW are utilized to construct different hypergraph Laplacian matrices and then proceed to develop a set of clustering techniques that use these incidence matrices and Laplacians for hypergraph clustering. To test the efficacy of these clustering algorithms, we utilize various data sets from real-life applications and compare their performance experimentally against a range of existing hypergraph clustering methods. Our findings show that the proposed methods yield higher-quality clusters. We conclude by highlighting potential areas for future research.",1
"The recent introduction of Graph Neural Networks (GNNs) and their growing popularity in the past few years has enabled the application of deep learning algorithms to non-Euclidean, graph-structured data. GNNs have achieved state-of-the-art results across an impressive array of graph-based machine learning problems. Nevertheless, despite their rapid pace of development, much of the work on GNNs has focused on graph classification and embedding techniques, largely ignoring regression tasks over graph data. In this paper, we develop a Graph Mixture Density Network (GraphMDN), which combines graph neural networks with mixture density network (MDN) outputs. By combining these techniques, GraphMDNs have the advantage of naturally being able to incorporate graph structured information into a neural architecture, as well as the ability to model multi-modal regression targets. As such, GraphMDNs are designed to excel on regression tasks wherein the data are graph structured, and target statistics are better represented by mixtures of densities rather than singular values (so-called ``inverse problems""). To demonstrate this, we extend an existing GNN architecture known as Semantic GCN (SemGCN) to a GraphMDN structure, and show results from the Human3.6M pose estimation task. The extended model consistently outperforms both GCN and MDN architectures on their own, with a comparable number of parameters.",0
"Graph Neural Networks (GNNs) have gained popularity in recent years, allowing deep learning algorithms to be applied to non-Euclidean, graph-structured data. They have been successful in a range of graph-based machine learning problems, particularly in graph classification and embedding techniques. However, regression tasks over graph data have been largely ignored. In this study, we introduce a Graph Mixture Density Network (GraphMDN) that combines GNNs with Mixture Density Network (MDN) outputs. GraphMDNs excel in regression tasks with graph-structured data and multi-modal regression targets, where target statistics are better represented by mixtures of densities instead of singular values. We demonstrate the effectiveness of GraphMDNs by extending an existing GNN architecture, Semantic GCN (SemGCN), to a GraphMDN structure, and showing results from the Human3.6M pose estimation task. The extended model consistently outperforms both GCN and MDN architectures with a comparable number of parameters.",1
"Graph Neural Networks achieve remarkable results on problems with structured data but come as black-box predictors. Transferring existing explanation techniques, such as occlusion, fails as even removing a single node or edge can lead to drastic changes in the graph. The resulting graphs can differ from all training examples, causing model confusion and wrong explanations. Thus, we argue that explicability must use graphs compliant with the distribution underlying the training data. We coin this property Distribution Compliant Explanation (DCE) and present a novel Contrastive GNN Explanation (CoGE) technique following this paradigm. An experimental study supports the efficacy of CoGE.",0
"While Graph Neural Networks perform well on structured data, they act as opaque predictors. Traditional explanation techniques like occlusion fail because removing even a single node or edge can result in significant changes in the graph. Such changes can cause confusion in the model and lead to incorrect explanations. Therefore, we propose that graphs used for explanations must be in line with the distribution that underlies the training data. We call this characteristic Distribution Compliant Explanation (DCE), and we introduce a new method called Contrastive GNN Explanation (CoGE) that adheres to this principle. Our experimental study shows that CoGE is effective.",1
"Graph, as an important data representation, is ubiquitous in many real world applications ranging from social network analysis to biology. How to correctly and effectively learn and extract information from graph is essential for a large number of machine learning tasks. Graph embedding is a way to transform and encode the data structure in high dimensional and non-Euclidean feature space to a low dimensional and structural space, which is easily exploited by other machine learning algorithms. We have witnessed a huge surge of such embedding methods, from statistical approaches to recent deep learning methods such as the graph convolutional networks (GCN). Deep learning approaches usually outperform the traditional methods in most graph learning benchmarks by building an end-to-end learning framework to optimize the loss function directly. However, most of the existing GCN methods can only perform convolution operations with node features, while ignoring the handy information in edge features, such as relations in knowledge graphs. To address this problem, we present CensNet, Convolution with Edge-Node Switching graph neural network, for learning tasks in graph-structured data with both node and edge features. CensNet is a general graph embedding framework, which embeds both nodes and edges to a latent feature space. By using line graph of the original undirected graph, the role of nodes and edges are switched, and two novel graph convolution operations are proposed for feature propagation. Experimental results on real-world academic citation networks and quantum chemistry graphs show that our approach achieves or matches the state-of-the-art performance in four graph learning tasks, including semi-supervised node classification, multi-task graph classification, graph regression, and link prediction.",0
"Graphs are widely used in various fields, including social network analysis and biology, as a crucial means of data representation. To effectively extract information from graphs, it is essential to learn how to do so correctly. Graph embedding is a method that transforms and encodes the data structure of graphs into a lower-dimensional and structural space, making it easier for machine learning algorithms to utilize. While traditional statistical approaches have been used in the past, there has been a recent surge in deep learning methods, such as the graph convolutional networks (GCN), which have outperformed traditional methods in most graph learning benchmarks. However, most GCN methods only consider node features, ignoring the valuable information in edge features, such as knowledge graph relations. To address this issue, we introduce CensNet, a Convolution with Edge-Node Switching graph neural network that can handle both node and edge features. CensNet is a general graph embedding framework that embeds both nodes and edges into a latent feature space. By using the line graph of the original undirected graph, the roles of nodes and edges are swapped, and two novel graph convolution operations are proposed for feature propagation. Our experimental results demonstrate that CensNet achieves state-of-the-art or comparable performance in various graph learning tasks, including semi-supervised node classification, multi-task graph classification, graph regression, and link prediction, on real-world academic citation networks and quantum chemistry graphs.",1
"Representation learning of graph-structured data is challenging because both graph structure and node features carry important information. Graph Neural Networks (GNNs) provide an expressive way to fuse information from network structure and node features. However, GNNs are prone to adversarial attacks. Here we introduce Graph Information Bottleneck (GIB), an information-theoretic principle that optimally balances expressiveness and robustness of the learned representation of graph-structured data. Inheriting from the general Information Bottleneck (IB), GIB aims to learn the minimal sufficient representation for a given task by maximizing the mutual information between the representation and the target, and simultaneously constraining the mutual information between the representation and the input data. Different from the general IB, GIB regularizes the structural as well as the feature information. We design two sampling algorithms for structural regularization and instantiate the GIB principle with two new models: GIB-Cat and GIB-Bern, and demonstrate the benefits by evaluating the resilience to adversarial attacks. We show that our proposed models are more robust than state-of-the-art graph defense models. GIB-based models empirically achieve up to 31% improvement with adversarial perturbation of the graph structure as well as node features.",0
"Learning representations of graph-structured data poses a challenge due to the significant information carried by both the graph structure and node features. Graph Neural Networks (GNNs) offer an effective approach to combine information from network structure and node features, but they are susceptible to adversarial attacks. To address this issue, we introduce the Graph Information Bottleneck (GIB) principle, which balances the expressiveness and robustness of the learned representation of graph-structured data. GIB is based on the Information Bottleneck (IB) principle, which seeks to learn the minimal sufficient representation for a given task by maximizing the mutual information between the representation and the target while limiting the mutual information between the representation and the input data. Unlike IB, GIB takes into account both the structural and feature information and incorporates two sampling algorithms for structural regularization. We propose two new models, GIB-Cat and GIB-Bern, and show that they are more robust to adversarial attacks than existing graph defense models, achieving up to 31% improvement with adversarial perturbation of both graph structure and node features.",1
"Graph neural networks (GNNs) have been widely used to analyze the graph-structured data in various application domains, e.g., social networks, molecular biology, and anomaly detection. With great power, the GNN models, usually as valuable Intellectual Properties of their owners, also become attractive targets of the attacker. Recent studies show that machine learning models are facing a severe threat called Model Extraction Attacks, where a well-trained private model owned by a service provider can be stolen by the attacker pretending as a client. Unfortunately, existing works focus on the models trained on the Euclidean space, e.g., images and texts, while how to extract a GNN model that contains a graph structure and node features is yet to be explored. In this paper, we explore and develop model extraction attacks against GNN models. Given only black-box access to a target GNN model, the attacker aims to reconstruct a duplicated one via several nodes he obtained (called attacker nodes). We first systematically formalise the threat modeling in the context of GNN model extraction and classify the adversarial threats into seven categories by considering different background knowledge of the attacker, e.g., attributes and/or neighbor connectives of the attacker nodes. Then we present the detailed methods which utilize the accessible knowledge in each threat to implement the attacks. By evaluating over three real-world datasets, our attacks are shown to extract duplicated models effectively, i.e., more than 89% inputs in the target domain have the same output predictions as the victim model.",0
"Graph neural networks (GNNs) are widely used in various domains such as social networks, molecular biology, and anomaly detection to analyze graph-structured data. However, as valuable intellectual properties, GNN models have become attractive targets for attackers. Model Extraction Attacks are a severe threat to machine learning models, where a well-trained private model owned by a service provider can be stolen by an attacker posing as a client. Unfortunately, existing works only focus on models trained on Euclidean space, such as images and texts, but not those containing a graph structure and node features. In this paper, we explore and develop model extraction attacks against GNN models using black-box access to a target GNN model. The attacker aims to reconstruct a duplicated model via several nodes obtained, called attacker nodes. We formalize the threat modeling and classify the adversarial threats into seven categories by considering different background knowledge of the attacker. Then we present detailed methods that utilize accessible knowledge to implement the attacks. Our attacks are shown to effectively extract duplicated models, with more than 89% of inputs in the target domain having the same output predictions as the victim model, as demonstrated on three real-world datasets.",1
"Graph embeddings are a ubiquitous tool for machine learning tasks, such as node classification and link prediction, on graph-structured data. However, computing the embeddings for large-scale graphs is prohibitively inefficient even if we are interested only in a small subset of relevant vertices. To address this, we present an efficient graph coarsening approach, based on Schur complements, for computing the embedding of the relevant vertices. We prove that these embeddings are preserved exactly by the Schur complement graph that is obtained via Gaussian elimination on the non-relevant vertices. As computing Schur complements is expensive, we give a nearly-linear time algorithm that generates a coarsened graph on the relevant vertices that provably matches the Schur complement in expectation in each iteration. Our experiments involving prediction tasks on graphs demonstrate that computing embeddings on the coarsened graph, rather than the entire graph, leads to significant time savings without sacrificing accuracy.",0
"In machine learning, graph embeddings are commonly used for tasks like node classification and link prediction on data with a graph structure. However, computing embeddings for large graphs is inefficient, even for a small subset of relevant vertices. We propose a solution by introducing an efficient graph coarsening approach that utilizes Schur complements to compute embeddings of relevant vertices. We demonstrate that the embeddings are precisely retained by the Schur complement graph generated through Gaussian elimination on non-relevant vertices. Since computing Schur complements is costly, we present a nearly-linear time algorithm that generates a coarsened graph that matches the Schur complement's expectation in every iteration. Our experiments reveal that computing embeddings on the coarsened graph instead of the entire graph significantly saves time without compromising accuracy in prediction tasks on graphs.",1
"Graph convolution is a fundamental building block for many deep neural networks on graph-structured data. In this paper, we introduce a simple, yet very effective graph convolutional network with skip connections for semi-supervised anomaly detection. The proposed multi-layer network architecture is theoretically motivated by the concept of implicit fairing in geometry processing, and comprises a graph convolution module for aggregating information from immediate node neighbors and a skip connection module for combining layer-wise neighborhood representations. In addition to capturing information from distant graph nodes through skip connections between the network's layers, our approach exploits both the graph structure and node features for learning discriminative node representations. The effectiveness of our model is demonstrated through extensive experiments on five benchmark datasets, achieving better or comparable anomaly detection results against strong baseline methods.",0
"Many deep neural networks on graph-structured data rely on graph convolution as a fundamental building block. This paper presents a graph convolutional network with skip connections that is both simple and highly effective for semi-supervised anomaly detection. The multi-layer network architecture is based on the concept of implicit fairing in geometry processing and consists of a graph convolution module that gathers information from immediate node neighbors, and a skip connection module that combines layer-wise neighborhood representations. Our approach utilizes both graph structure and node features to learn discriminative node representations, capturing information from distant graph nodes through skip connections between network layers. Experiments on five benchmark datasets show that our model achieves better or comparable anomaly detection results compared to strong baseline methods.",1
"This paper presents and characterizes an Open Application Repository for Federated Learning (OARF), a benchmark suite for federated machine learning systems. Previously available benchmarks for federated learning have focused mainly on synthetic datasets and use a very limited number of applications. OARF includes different data partitioning methods (horizontal, vertical and hybrid) as well as emerging applications in image, text and structured data, which represent different scenarios in federated learning. Our characterization shows that the benchmark suite is diverse in data size, distribution, feature distribution and learning task complexity. We have developed reference implementations, and evaluated the important aspects of federated learning, including model accuracy, communication cost, differential privacy, secure multiparty computation and vertical federated learning.",0
"In this paper, an Open Application Repository for Federated Learning (OARF) is introduced and examined as a benchmark suite for federated machine learning systems. Current benchmarks for federated learning have been limited to synthetic datasets and a small number of applications. OARF offers various data partitioning techniques (horizontal, vertical and hybrid) and includes new applications in image, text and structured data, providing diverse scenarios for federated learning. The benchmark suite displays diversity in data size, distribution, feature distribution and learning task complexity. Additionally, reference implementations were developed and important aspects of federated learning were evaluated, such as model accuracy, communication cost, differential privacy, secure multiparty computation and vertical federated learning.",1
"Graph neural network models have been extensively used to learn node representations for graph structured data in an end-to-end setting. These models often rely on localized first order approximations of spectral graph convolutions and hence are unable to capture higher-order relational information between nodes. Probabilistic Graphical Models form another class of models that provide rich flexibility in incorporating such relational information but are limited by inefficient approximate inference algorithms at higher order. In this paper, we propose to combine these approaches to learn better node and graph representations. First, we derive an efficient approximate sum-product loopy belief propagation inference algorithm for higher-order PGMs. We then embed the message passing updates into a neural network to provide the inductive bias of the inference algorithm in end-to-end learning. This gives us a model that is flexible enough to accommodate domain knowledge while maintaining the computational advantage. We further propose methods for constructing higher-order factors that are conditioned on node and edge features and share parameters wherever necessary. Our experimental evaluation shows that our model indeed captures higher-order information, substantially outperforming state-of-the-art $k$-order graph neural networks in molecular datasets.",0
"The use of graph neural network models to learn node representations for graph structured data has been widespread in an end-to-end setting. However, these models have limitations as they rely on localized first order approximations of spectral graph convolutions and cannot capture higher-order relational information between nodes. On the other hand, Probabilistic Graphical Models offer a more flexible approach to incorporate such relational information, but they are hindered by inefficient approximate inference algorithms at higher order. This paper proposes a combination of these approaches to improve node and graph representations. The proposed model derives an efficient approximate sum-product loopy belief propagation inference algorithm for higher-order PGMs, which is embedded into a neural network to provide inductive bias for end-to-end learning. This model accommodates domain knowledge while maintaining computational advantage. Additionally, higher-order factors are constructed based on node and edge features and share parameters where needed. Experimental results show that this model captures higher-order information, outperforming state-of-the-art $k$-order graph neural networks in molecular datasets.",1
"Fairness in machine learning is crucial when individuals are subject to automated decisions made by models in high-stake domains. Organizations that employ these models may also need to satisfy regulations that promote responsible and ethical A.I. While fairness metrics relying on comparing model error rates across subpopulations have been widely investigated for the detection and mitigation of bias, fairness in terms of the equalized ability to achieve recourse for different protected attribute groups has been relatively unexplored. We present a novel formulation for training neural networks that considers the distance of data points to the decision boundary such that the new objective: (1) reduces the average distance to the decision boundary between two groups for individuals subject to a negative outcome in each group, i.e. the network is more fair with respect to the ability to obtain recourse, and (2) increases the average distance of data points to the boundary to promote adversarial robustness. We demonstrate that training with this loss yields more fair and robust neural networks with similar accuracies to models trained without it. Moreover, we qualitatively motivate and empirically show that reducing recourse disparity across groups also improves fairness measures that rely on error rates. To the best of our knowledge, this is the first time that recourse capabilities across groups are considered to train fairer neural networks, and a relation between error rates based fairness and recourse based fairness is investigated.",0
"The importance of fairness in machine learning cannot be overstated, especially in high-stakes domains where automated decisions made by models can have a significant impact on individuals. Responsible and ethical A.I. regulations may also need to be followed by organizations employing these models. While fairness metrics that rely on comparing model error rates across subpopulations have been extensively researched for detecting and mitigating bias, there has been relatively little exploration of fairness in terms of the equalized ability to achieve recourse for different protected attribute groups. In this study, we introduce a new approach to training neural networks that considers the distance of data points to the decision boundary. The new objective aims to reduce the average distance to the decision boundary between two groups for individuals subject to a negative outcome in each group, thus making the network more fair with respect to the ability to obtain recourse. Additionally, it aims to increase the average distance of data points to the boundary to promote adversarial robustness. Our results demonstrate that training with this loss yields more fair and robust neural networks with similar accuracies to models trained without it. Furthermore, we show that reducing recourse disparity across groups also improves fairness measures that rely on error rates. This study is the first to consider recourse capabilities across groups to train fairer neural networks, and to investigate the relationship between error rates based fairness and recourse based fairness.",1
"Exploiting the rapid advances in probabilistic inference, in particular variational Bayes and variational autoencoders (VAEs), for anomaly detection (AD) tasks remains an open research question. Previous works argued that training VAE models only with inliers is insufficient and the framework should be significantly modified in order to discriminate the anomalous instances. In this work, we exploit the deep conditional variational autoencoder (CVAE) and we define an original loss function together with a metric that targets hierarchically structured data AD. Our motivating application is a real world problem: monitoring the trigger system which is a basic component of many particle physics experiments at the CERN Large Hadron Collider (LHC). In the experiments we show the superior performance of this method for classical machine learning (ML) benchmarks and for our application.",0
"The use of probabilistic inference, specifically variational Bayes and variational autoencoders (VAEs), in anomaly detection (AD) tasks is still a topic of research. Previous studies have pointed out that training VAE models with only inliers is not sufficient and that significant modifications must be made to detect anomalous instances. In this study, we utilize the deep conditional variational autoencoder (CVAE) and introduce an original loss function and metric designed for AD of hierarchically structured data. Our research is motivated by the need to monitor the trigger system, a critical component in many particle physics experiments at the CERN Large Hadron Collider (LHC). Our experiments show that this method outperforms classical machine learning (ML) benchmarks and is effective for our application.",1
"Recently, graph neural networks have attracted great attention and achieved prominent performance in various research fields. Most of those algorithms have assumed pairwise relationships of objects of interest. However, in many real applications, the relationships between objects are in higher-order, beyond a pairwise formulation. To efficiently learn deep embeddings on the high-order graph-structured data, we introduce two end-to-end trainable operators to the family of graph neural networks, i.e., hypergraph convolution and hypergraph attention. Whilst hypergraph convolution defines the basic formulation of performing convolution on a hypergraph, hypergraph attention further enhances the capacity of representation learning by leveraging an attention module. With the two operators, a graph neural network is readily extended to a more flexible model and applied to diverse applications where non-pairwise relationships are observed. Extensive experimental results with semi-supervised node classification demonstrate the effectiveness of hypergraph convolution and hypergraph attention.",0
"Graph neural networks have gained significant attention and demonstrated exceptional performance in various research domains. However, most of these algorithms assume pairwise relationships between objects of interest. In real-life scenarios, however, the relationships between objects go beyond pairwise formulations, making it challenging to learn deep embeddings on high-order graph-structured data. To address this challenge, we introduce two end-to-end trainable operators, hypergraph convolution and hypergraph attention, to the family of graph neural networks. Hypergraph convolution performs convolution on a hypergraph, while hypergraph attention enhances representation learning by using an attention module. These operators enable a graph neural network to be extended into a more adaptable model and applied to diverse applications that involve non-pairwise relationships. Our experimental results with semi-supervised node classification demonstrate the effectiveness of hypergraph convolution and hypergraph attention.",1
"Graphs are the most ubiquitous form of structured data representation used in machine learning. They model, however, only pairwise relations between nodes and are not designed for encoding the higher-order relations found in many real-world datasets. To model such complex relations, hypergraphs have proven to be a natural representation. Learning the node representations in a hypergraph is more complex than in a graph as it involves information propagation at two levels: within every hyperedge and across the hyperedges. Most current approaches first transform a hypergraph structure to a graph for use in existing geometric deep learning algorithms. This transformation leads to information loss, and sub-optimal exploitation of the hypergraph's expressive power. We present HyperSAGE, a novel hypergraph learning framework that uses a two-level neural message passing strategy to accurately and efficiently propagate information through hypergraphs. The flexible design of HyperSAGE facilitates different ways of aggregating neighborhood information. Unlike the majority of related work which is transductive, our approach, inspired by the popular GraphSAGE method, is inductive. Thus, it can also be used on previously unseen nodes, facilitating deployment in problems such as evolving or partially observed hypergraphs. Through extensive experimentation, we show that HyperSAGE outperforms state-of-the-art hypergraph learning methods on representative benchmark datasets. We also demonstrate that the higher expressive power of HyperSAGE makes it more stable in learning node representations as compared to the alternatives.",0
"Machine learning commonly uses graphs as a structured data representation. However, graphs only model pairwise relations between nodes and are insufficient for encoding higher-order relations in real-world datasets. Hypergraphs are a more natural representation for modeling complex relations. Learning node representations in hypergraphs is more complicated than in graphs since it involves information propagation within and across hyperedges. Many approaches transform hypergraph structures into graphs for use in existing geometric deep learning algorithms, but this results in information loss and sub-optimal use of the hypergraph's expressive power. We present HyperSAGE, a novel hypergraph learning framework that accurately and efficiently propagates information through hypergraphs using a two-level neural message passing strategy. HyperSAGE's flexible design allows for various ways of aggregating neighborhood information. Unlike transductive methods, HyperSAGE is inductive, meaning it can be used on previously unseen nodes, making it useful for problems with evolving or partially observed hypergraphs. Through experimentation, we demonstrate that HyperSAGE outperforms state-of-the-art hypergraph learning methods on benchmark datasets and is more stable in learning node representations.",1
"Graph neural networks have become an important tool for modeling structured data. In many real-world systems, intricate hidden information may exist, e.g., heterogeneity in nodes/edges, static node/edge attributes, and spatiotemporal node/edge features. However, most existing methods only take part of the information into consideration. In this paper, we present the Co-evolved Meta Graph Neural Network (CoMGNN), which applies meta graph attention to heterogeneous graphs with co-evolution of node and edge states. We further propose a spatiotemporal adaption of CoMGNN (ST-CoMGNN) for modeling spatiotemporal patterns on nodes and edges. We conduct experiments on two large-scale real-world datasets. Experimental results show that our models significantly outperform the state-of-the-art methods, demonstrating the effectiveness of encoding diverse information from different aspects.",0
"Structured data can contain complex hidden information such as heterogeneous nodes/edges, static attributes, and spatiotemporal features. However, current methods only consider some of this information. Graph neural networks have become crucial for modeling structured data, and in this study, we introduce the Co-evolved Meta Graph Neural Network (CoMGNN) that utilizes meta graph attention to tackle heterogeneous graphs. We also propose the spatiotemporal adaptation of CoMGNN (ST-CoMGNN) for modeling spatiotemporal patterns on nodes and edges. Our experiments on large-scale real-world datasets reveal that our models outperform existing methods, highlighting the importance of including diverse information from various perspectives.",1
"Understanding customer behavior is fundamental for many use-cases in industry, especially in accelerated growth areas such as fin-tech and e-commerce. Structured data are often expensive, time-consuming and inadequate to analyze and study complex customer behaviors. In this paper, we propose a multi-graph embedding approach for creating a non-linear representation of customers in order to have a better knowledge of their characteristics without having any prior information about their financial status or their interests. By applying the current method we are able to predict users' future behavior with a reasonably high accuracy only by having the information of their friendship network. Potential applications include recommendation systems and credit risk forecasting.",0
"In industries like fin-tech and e-commerce, comprehending customer behavior is crucial for several purposes. However, analyzing complex customer behaviors using structured data can be costly, time-consuming, and insufficient. To overcome these challenges, we suggest a multi-graph embedding technique that generates a non-linear portrayal of customers, even without prior knowledge of their financial standing or interests. Using this approach, we can forecast future behavior of users with a considerable level of precision by solely examining their friendship network. This method can be employed in various applications, including credit risk prediction and recommendation systems.",1
"Graph Neural Networks (GNNs) have risen to prominence in learning representations for graph structured data. A single GNN layer typically consists of a feature transformation and a feature aggregation operation. The former normally uses feed-forward networks to transform features, while the latter aggregates the transformed features over the graph. Numerous recent works have proposed GNN models with different designs in the aggregation operation. In this work, we establish mathematically that the aggregation processes in a group of representative GNN models including GCN, GAT, PPNP, and APPNP can be regarded as (approximately) solving a graph denoising problem with a smoothness assumption. Such a unified view across GNNs not only provides a new perspective to understand a variety of aggregation operations but also enables us to develop a unified graph neural network framework UGNN. To demonstrate its promising potential, we instantiate a novel GNN model, ADA-UGNN, derived from UGNN, to handle graphs with adaptive smoothness across nodes. Comprehensive experiments show the effectiveness of ADA-UGNN.",0
"Learning representations for graph structured data has been revolutionized by Graph Neural Networks (GNNs). Typically, a GNN layer consists of a feature transformation and aggregation operation. While feed-forward networks are used for feature transformation, the transformed features are usually aggregated over the graph. Recent works have proposed GNN models with various designs in the aggregation operation. This study establishes mathematically that the aggregation processes in representative GNN models, including GCN, GAT, PPNP, and APPNP, approximately solve a graph denoising problem with a smoothness assumption. This unified view across GNNs not only facilitates understanding of different aggregation operations but also enables the development of a unified graph neural network framework UGNN. To demonstrate the potential of UGNN, a novel GNN model called ADA-UGNN is instantiated to handle graphs with adaptive smoothness across nodes. Comprehensive experiments confirm the effectiveness of ADA-UGNN.",1
"The Large Scale Visual Recognition Challenge based on the well-known Imagenet dataset catalyzed an intense flurry of progress in computer vision. Benchmark tasks have propelled other sub-fields of machine learning forward at an equally impressive pace, but in healthcare it has primarily been image processing tasks, such as in dermatology and radiology, that have experienced similar benchmark-driven progress. In the present study, we performed a comprehensive review of benchmarks in medical machine learning for structured data, identifying one based on the Medical Information Mart for Intensive Care (MIMIC-III) that allows the first direct comparison of predictive performance and thus the evaluation of progress on four clinical prediction tasks: mortality, length of stay, phenotyping, and patient decompensation. We find that little meaningful progress has been made over a 3 year period on these tasks, despite significant community engagement. Through our meta-analysis, we find that the performance of deep recurrent models is only superior to logistic regression on certain tasks. We conclude with a synthesis of these results, possible explanations, and a list of desirable qualities for future benchmarks in medical machine learning.",0
"The Imagenet dataset's Large Scale Visual Recognition Challenge spurred significant advancements in computer vision. Other machine learning sub-fields have also benefited from benchmark tasks, but medical machine learning progress has primarily been limited to image processing tasks in areas such as dermatology and radiology. This study reviews medical machine learning benchmarks for structured data and identifies one based on MIMIC-III that enables the direct comparison of performance on four clinical prediction tasks. The study finds no significant progress over a 3-year period despite community engagement, and deep recurrent models perform only marginally better than logistic regression on certain tasks. The study concludes with a summary of the results, potential explanations, and a list of desirable qualities for future medical machine learning benchmarks.",1
"Graph Neural Networks (GNNs), a generalization of neural networks to graph-structured data, are often implemented using message passes between entities of a graph. While GNNs are effective for node classification, link prediction and graph classification, they are vulnerable to adversarial attacks, i.e., a small perturbation to the structure can lead to a non-trivial performance degradation. In this work, we propose Uncertainty Matching GNN (UM-GNN), that is aimed at improving the robustness of GNN models, particularly against poisoning attacks to the graph structure, by leveraging epistemic uncertainties from the message passing framework. More specifically, we propose to build a surrogate predictor that does not directly access the graph structure, but systematically extracts reliable knowledge from a standard GNN through a novel uncertainty-matching strategy. Interestingly, this uncoupling makes UM-GNN immune to evasion attacks by design, and achieves significantly improved robustness against poisoning attacks. Using empirical studies with standard benchmarks and a suite of global and target attacks, we demonstrate the effectiveness of UM-GNN, when compared to existing baselines including the state-of-the-art robust GCN.",0
"Graph Neural Networks (GNNs) are neural networks designed for graph-structured data, and often rely on message passing between entities of the graph. Although GNNs are effective for tasks like node classification, link prediction, and graph classification, they are vulnerable to adversarial attacks. Even small changes to the graph structure can result in significant performance degradation. To address this issue, we propose the Uncertainty Matching GNN (UM-GNN) which leverages epistemic uncertainties from the message passing framework to improve the robustness of GNN models against poisoning attacks. Our approach involves building a surrogate predictor that extracts reliable knowledge from a standard GNN through a novel uncertainty-matching strategy. This uncoupling makes UM-GNN immune to evasion attacks and achieves significantly improved robustness against poisoning attacks. We demonstrate the effectiveness of UM-GNN through empirical studies with standard benchmarks and a suite of global and target attacks, and compare it to existing baselines, including the state-of-the-art robust GCN.",1
"Graph structured data has wide applicability in various domains such as physics, chemistry, biology, computer vision, and social networks, to name a few. Recently, graph neural networks (GNN) were shown to be successful in effectively representing graph structured data because of their good performance and generalization ability. GNN is a deep learning based method that learns a node representation by combining specific nodes and the structural/topological information of a graph. However, like other deep models, explaining the effectiveness of GNN models is a challenging task because of the complex nonlinear transformations made over the iterations. In this paper, we propose GraphLIME, a local interpretable model explanation for graphs using the Hilbert-Schmidt Independence Criterion (HSIC) Lasso, which is a nonlinear feature selection method. GraphLIME is a generic GNN-model explanation framework that learns a nonlinear interpretable model locally in the subgraph of the node being explained. More specifically, to explain a node, we generate a nonlinear interpretable model from its $N$-hop neighborhood and then compute the K most representative features as the explanations of its prediction using HSIC Lasso. Through experiments on two real-world datasets, the explanations of GraphLIME are found to be of extraordinary degree and more descriptive in comparison to the existing explanation methods.",0
"Graph structured data has vast applicability in diverse fields, including physics, chemistry, biology, computer vision, and social networks. Graph neural networks (GNN) have emerged as a successful method for effectively representing graph structured data due to their high performance and ability to generalize. GNN is a deep learning technique that combines specific nodes and the structural/topological information of a graph to learn a node representation. However, as with other deep models, explaining the effectiveness of GNN models is challenging due to the complex nonlinear transformations made over the iterations. This paper introduces GraphLIME, a local interpretable model explanation for graphs that uses the Hilbert-Schmidt Independence Criterion (HSIC) Lasso, which is a nonlinear feature selection method. GraphLIME is a generic GNN-model explanation framework that learns a nonlinear interpretable model locally in the subgraph of the node being explained. To explain a node, a nonlinear interpretable model is generated from its $N$-hop neighborhood, and the K most representative features are computed as the explanations of its prediction using HSIC Lasso. Experiments on two real-world datasets demonstrate that GraphLIME provides highly descriptive and exceptional degree explanations compared to existing explanation methods.",1
"Graph Neural Networks (GNNs) have attracted considerable attention and have emerged as a new promising paradigm to process graph-structured data. GNNs are usually stacked to multiple layers and the node representations in each layer are computed through propagating and aggregating the neighboring node features with respect to the graph. By stacking to multiple layers, GNNs are able to capture the long-range dependencies among the data on the graph and thus bring performance improvements. To train a GNN with multiple layers effectively, some normalization techniques (e.g., node-wise normalization, batch-wise normalization) are necessary. However, the normalization techniques for GNNs are highly task-relevant and different application tasks prefer to different normalization techniques, which is hard to know in advance. To tackle this deficiency, in this paper, we propose to learn graph normalization by optimizing a weighted combination of normalization techniques at four different levels, including node-wise normalization, adjacency-wise normalization, graph-wise normalization, and batch-wise normalization, in which the adjacency-wise normalization and the graph-wise normalization are newly proposed in this paper to take into account the local structure and the global structure on the graph, respectively. By learning the optimal weights, we are able to automatically select a single best or a best combination of multiple normalizations for a specific task. We conduct extensive experiments on benchmark datasets for different tasks, including node classification, link prediction, graph classification and graph regression, and confirm that the learned graph normalization leads to competitive results and that the learned weights suggest the appropriate normalization techniques for the specific task. Source code is released here https://github.com/cyh1112/GraphNormalization.",0
"Graph Neural Networks (GNNs) have gained significant attention as a promising approach for processing graph-structured data. Typically, GNNs consist of multiple layers that compute node representations by propagating and aggregating neighboring node features based on the graph. By stacking multiple layers, GNNs can capture long-range dependencies and improve performance. However, to train GNNs with multiple layers effectively, normalization techniques such as node-wise or batch-wise normalization are necessary. Unfortunately, identifying the appropriate normalization technique for a particular task is challenging. To address this issue, we propose learning graph normalization by optimizing a weighted combination of four normalization techniques, including node-wise, adjacency-wise, graph-wise, and batch-wise normalization. We introduce adjacency-wise and graph-wise normalization to account for local and global structures on the graph. By learning optimal weights, we can automatically select the best normalization technique or combination of techniques for a specific task. We test our approach on various benchmark datasets for node classification, link prediction, graph classification, and graph regression and demonstrate that our learned graph normalization yields competitive results and recommends appropriate normalization techniques for each task. We provide the source code at https://github.com/cyh1112/GraphNormalization.",1
"While existing predictive frameworks are able to handle Euclidean structured data (i.e, brain images), they might fail to generalize to geometric non-Euclidean data such as brain networks. Besides, these are rooted the sample selection step in using Euclidean or learned similarity measure between vectorized training and testing brain networks. Such sample connectomic representation might include irrelevant and redundant features that could mislead the training sample selection step. Undoubtedly, this fails to exploit and preserve the topology of the brain connectome. To overcome this major drawback, we propose Residual Embedding Similarity-Based Network selection (RESNets) for predicting brain network evolution trajectory from a single timepoint. RESNets first learns a compact geometric embedding of each training and testing sample using adversarial connectome embedding network. This nicely reduces the high-dimensionality of brain networks while preserving their topological properties via graph convolutional networks. Next, to compute the similarity between subjects, we introduce the concept of a connectional brain template (CBT), a fixed network reference, where we further represent each training and testing network as a deviation from the reference CBT in the embedding space. As such, we select the most similar training subjects to the testing subject at baseline by comparing their learned residual embeddings with respect to the pre-defined CBT. Once the best training samples are selected at baseline, we simply average their corresponding brain networks at follow-up timepoints to predict the evolution trajectory of the testing network. Our experiments on both healthy and disordered brain networks demonstrate the success of our proposed method in comparison to RESNets ablated versions and traditional approaches.",0
"Current predictive frameworks are effective for processing Euclidean structured data, such as brain images, but may not perform well when dealing with non-Euclidean geometric data, such as brain networks. This is due to the use of Euclidean or learned similarity measures between vectorized training and testing brain networks during the sample selection step. Such connectomic representations may contain irrelevant and redundant features, which can lead to inaccurate results. To address this issue, we introduce the Residual Embedding Similarity-Based Network selection (RESNets) method for predicting brain network evolution trajectory from a single timepoint. RESNets utilize an adversarial connectome embedding network to learn a compact geometric embedding of each training and testing sample, reducing the high-dimensionality of brain networks while preserving their topological properties using graph convolutional networks. To compute the similarity between subjects, we introduce the concept of a connectional brain template (CBT), a fixed network reference. Each training and testing network is represented as a deviation from the CBT in the embedding space. By comparing their learned residual embeddings with respect to the pre-defined CBT, we select the most similar training subjects to the testing subject at baseline. Once the best training samples are selected, we average their corresponding brain networks at follow-up timepoints to predict the evolution trajectory of the testing network. Our experiments demonstrate the effectiveness of RESNets compared to traditional approaches.",1
"Graph neural networks (GNNs) have achieved high performance in analyzing graph-structured data and have been widely deployed in safety-critical areas, such as finance and autonomous driving. However, only a few works have explored GNNs' robustness to adversarial attacks, and their designs are usually limited by the scale of input datasets (i.e., focusing on small graphs with only thousands of nodes). In this work, we propose, SAG, the first scalable adversarial attack method with Alternating Direction Method of Multipliers (ADMM). We first decouple the large-scale graph into several smaller graph partitions and cast the original problem into several subproblems. Then, we propose to solve these subproblems using projected gradient descent on both the graph topology and the node features that lead to considerably lower memory consumption compared to the conventional attack methods. Rigorous experiments further demonstrate that SAG can significantly reduce the computation and memory overhead compared with the state-of-the-art approach, making SAG applicable towards graphs with large size of nodes and edges.",0
"Graph neural networks (GNNs) have demonstrated exceptional performance in the analysis of graph-structured data, and are widely used in safety-critical fields like finance and autonomous driving. However, researchers have only explored a few GNNs' resilience to adversarial attacks, and their designs have been limited by small input datasets of a few thousand nodes. In this study, we introduce SAG, the first scalable adversarial attack method using Alternating Direction Method of Multipliers (ADMM). We split the large-scale graph into smaller partitions and solve the original problem through multiple subproblems. We apply projected gradient descent to the graph topology and node features to achieve significantly lower memory consumption than conventional attack methods. Comprehensive experiments demonstrate that SAG reduces computation and memory overhead compared to the state-of-the-art approach, making it suitable for larger graphs with many nodes and edges.",1
"Despite substantial progress in signal source separation, results for richly structured data continue to contain perceptible artifacts. In contrast, recent deep generative models can produce authentic samples in a variety of domains that are indistinguishable from samples of the data distribution. This paper introduces a Bayesian approach to source separation that uses generative models as priors over the components of a mixture of sources, and noise-annealed Langevin dynamics to sample from the posterior distribution of sources given a mixture. This decouples the source separation problem from generative modeling, enabling us to directly use cutting-edge generative models as priors. The method achieves state-of-the-art performance for MNIST digit separation. We introduce new methodology for evaluating separation quality on richer datasets, providing quantitative evaluation of separation results on CIFAR-10. We also provide qualitative results on LSUN.",0
"Despite notable advancements in signal source separation, artifacts are still noticeable in the outcomes for data with complex structures. Conversely, recent deep generative models are capable of producing authentic samples in various fields that are virtually identical to data distribution samples. This research presents a Bayesian strategy for source separation that employs generative models as priors for a mixture of sources, and noise-annealed Langevin dynamics to sample from the posterior distribution of sources given a mixture. This approach separates the source separation issue from generative modeling, allowing us to directly use state-of-the-art generative models as priors. For MNIST digit separation, this technique achieves top-notch performance. We introduce novel methodology for assessing separation quality on more complex datasets, providing quantitative evaluation of separation results on CIFAR-10. We also offer qualitative results on LSUN.",1
"Axis-aligned decision forests have long been the leading class of machine learning algorithms for modeling tabular data. In many applications of machine learning such as learning-to-rank, decision forests deliver remarkable performance. They also possess other coveted characteristics such as interpretability. Despite their widespread use and rich history, decision forests to date fail to consume raw structured data such as text, or learn effective representations for them, a factor behind the success of deep neural networks in recent years. While there exist methods that construct smoothed decision forests to achieve representation learning, the resulting models are decision forests in name only: They are no longer axis-aligned, use stochastic decisions, or are not interpretable. Furthermore, none of the existing methods are appropriate for problems that require a Transfer Learning treatment. In this work, we present a novel but intuitive proposal to achieve representation learning for decision forests without imposing new restrictions or necessitating structural changes. Our model is simply a decision forest, possibly trained using any forest learning algorithm, atop a deep neural network. By approximating the gradients of the decision forest through input perturbation, a purely analytical procedure, the decision forest directs the neural network to learn or fine-tune representations. Our framework has the advantage that it is applicable to any arbitrary decision forest and that it allows the use of arbitrary deep neural networks for representation learning. We demonstrate the feasibility and effectiveness of our proposal through experiments on synthetic and benchmark classification datasets.",0
"For a long time, axis-aligned decision forests have been the top machine learning algorithm for modeling tabular data. They excel in applications such as learning-to-rank and are also prized for their interpretability. However, they have not been successful in processing unprocessed structured data like text and learning efficient representations for them - an area where deep neural networks have been successful. Although some methods exist, they do not produce true decision forests as they lose their axis-alignment, use stochastic decisions, or are not interpretable. Additionally, none of these methods work for Transfer Learning problems. The authors propose a straightforward solution that achieves representation learning for decision forests without imposing new restrictions or requiring structural changes. Their approach involves a decision forest on top of a deep neural network. The decision forest directs the neural network to learn or fine-tune representations by approximating the gradients of the decision forest through input perturbation, which is an analytical process. The authors demonstrate the effectiveness of their model through experiments on synthetic and benchmark classification datasets, and their framework is applicable to any arbitrary decision forest and allows the use of any deep neural network for representation learning.",1
"Urban ride-hailing demand prediction is a crucial but challenging task for intelligent transportation system construction. Predictable ride-hailing demand can facilitate more reasonable vehicle scheduling and online car-hailing platform dispatch. Conventional deep learning methods with no external structured data can be accomplished via hybrid models of CNNs and RNNs by meshing plentiful pixel-level labeled data, but spatial data sparsity and limited learning capabilities on temporal long-term dependencies are still two striking bottlenecks. To address these limitations, we propose a new virtual graph modeling method to focus on significant demand regions and a novel Deep Multi-View Spatiotemporal Virtual Graph Neural Network (DMVST-VGNN) to strengthen learning capabilities of spatial dynamics and temporal long-term dependencies. Specifically, DMVST-VGNN integrates the structures of 1D Convolutional Neural Network, Multi Graph Attention Neural Network and Transformer layer, which correspond to short-term temporal dynamics view, spatial dynamics view and long-term temporal dynamics view respectively. In this paper, experiments are conducted on two large-scale New York City datasets in fine-grained prediction scenes. And the experimental results demonstrate effectiveness and superiority of DMVST-VGNN framework in significant citywide ride-hailing demand prediction.",0
"The prediction of demand for urban ride-hailing is a challenging task that is crucial for the development of intelligent transportation systems. Accurate prediction can help with better scheduling of vehicles and dispatching on online car-hailing platforms. Although conventional deep learning methods can use CNNs and RNNs to analyze labeled data, there are still two main obstacles: limited ability to learn from long-term temporal dependencies and sparse spatial data. To address these issues, a new method called virtual graph modeling has been proposed, along with a Deep Multi-View Spatiotemporal Virtual Graph Neural Network (DMVST-VGNN). This network integrates different types of neural networks to analyze short-term temporal dynamics, spatial dynamics, and long-term temporal dynamics. The effectiveness of this framework was demonstrated through experiments on two large-scale datasets in New York City, with promising results for significant citywide ride-hailing demand prediction.",1
"Representation learning over graph structure data has been widely studied due to its wide application prospects. However, previous methods mainly focus on static graphs while many real-world graphs evolve over time. Modeling such evolution is important for predicting properties of unseen networks. To resolve this challenge, we propose SGRNN, a novel neural architecture that applies stochastic latent variables to simultaneously capture the evolution in node attributes and topology. Specifically, deterministic states are separated from stochastic states in the iterative process to suppress mutual interference. With semi-implicit variational inference integrated to SGRNN, a non-Gaussian variational distribution is proposed to help further improve the performance. In addition, to alleviate KL-vanishing problem in SGRNN, a simple and interpretable structure is proposed based on the lower bound of KL-divergence. Extensive experiments on real-world datasets demonstrate the effectiveness of the proposed model. Code is available at https://github.com/StochasticGRNN/SGRNN.",0
"The study of representation learning for graph structure data has been widely explored due to its vast potential applications. Nevertheless, previous approaches have mainly focused on static graphs, while many real-world graphs undergo continuous changes over time. It is crucial to model this evolution in order to be able to predict the properties of unseen networks. To overcome this challenge, we present SGRNN, a novel neural architecture that incorporates stochastic latent variables to simultaneously capture the evolution in both node attributes and topology. The deterministic states are separated from the stochastic states during the iterative process to avoid mutual interference. To further improve the performance, we propose a non-Gaussian variational distribution using semi-implicit variational inference. Additionally, we suggest a simple and comprehensible structure based on the lower bound of KL-divergence to address the KL-vanishing problem in SGRNN. The proposed model's effectiveness is demonstrated through extensive experiments on real-world datasets. The code is available at https://github.com/StochasticGRNN/SGRNN.",1
"Recently, the surge in popularity of Internet of Things (IoT), mobile devices, social media, etc. has opened up a large source for graph data. Graph embedding has been proved extremely useful to learn low-dimensional feature representations from graph structured data. These feature representations can be used for a variety of prediction tasks from node classification to link prediction. However, existing graph embedding methods do not consider users' privacy to prevent inference attacks. That is, adversaries can infer users' sensitive information by analyzing node representations learned from graph embedding algorithms. In this paper, we propose Adversarial Privacy Graph Embedding (APGE), a graph adversarial training framework that integrates the disentangling and purging mechanisms to remove users' private information from learned node representations. The proposed method preserves the structural information and utility attributes of a graph while concealing users' private attributes from inference attacks. Extensive experiments on real-world graph datasets demonstrate the superior performance of APGE compared to the state-of-the-arts. Our source code can be found at https://github.com/uJ62JHD/Privacy-Preserving-Social-Network-Embedding.",0
"The growing popularity of Internet of Things (IoT), mobile devices, and social media has resulted in a vast amount of graph data. Graph embedding is a valuable tool for acquiring low-dimensional feature representations from graph structured data, which can be employed for various prediction tasks. Nevertheless, current graph embedding techniques do not take into account user privacy, leaving them vulnerable to inference attacks. These attacks can reveal sensitive information by analyzing node representations from graph embedding algorithms. To address this, we present Adversarial Privacy Graph Embedding (APGE), a graph adversarial training framework that uses disentangling and purging mechanisms to remove users' private information from node representations. Our method preserves the graph's structural information and utility attributes while protecting users' private attributes from inference attacks. We conducted extensive experiments on real-world graph datasets and demonstrated that APGE outperforms state-of-the-art methods. The source code for APGE can be found at https://github.com/uJ62JHD/Privacy-Preserving-Social-Network-Embedding.",1
"Operator-Valued Kernels (OVKs) and associated vector-valued Reproducing Kernel Hilbert Spaces provide an elegant way to extend scalar kernel methods when the output space is a Hilbert space. Although primarily used in finite dimension for problems like multi-task regression, the ability of this framework to deal with infinite dimensional output spaces unlocks many more applications, such as functional regression, structured output prediction, and structured data representation. However, these sophisticated schemes crucially rely on the kernel trick in the output space, so that most of previous works have focused on the square norm loss function, completely neglecting robustness issues that may arise in such surrogate problems. To overcome this limitation, this paper develops a duality approach that allows to solve OVK machines for a wide range of loss functions. The infinite dimensional Lagrange multipliers are handled through a Double Representer Theorem, and algorithms for $\epsilon$-insensitive losses and the Huber loss are thoroughly detailed. Robustness benefits are emphasized by a theoretical stability analysis, as well as empirical improvements on structured data applications.",0
"The use of Operator-Valued Kernels (OVKs) and vector-valued Reproducing Kernel Hilbert Spaces provides an elegant solution for extending scalar kernel methods in situations where the output space is a Hilbert space. While these methods have primarily been used in finite dimensions for tasks like multi-task regression, their ability to handle infinite dimensional output spaces unlocks a wide range of applications, including structured data representation, functional regression, and structured output prediction. However, previous works have focused solely on the square norm loss function, ignoring potential robustness issues that may arise when using surrogate problems. To address this limitation, this paper proposes a duality approach that enables the use of OVK machines with a variety of loss functions. The approach handles infinite dimensional Lagrange multipliers through a Double Representer Theorem, and the paper also offers detailed algorithms for $\epsilon$-insensitive losses and the Huber loss. Theoretical stability analysis and empirical results on structured data applications highlight the benefits of this approach.",1
"As machine learning for images becomes democratized in the Software 2.0 era, one of the serious bottlenecks is securing enough labeled data for training. This problem is especially critical in a manufacturing setting where smart factories rely on machine learning for product quality control by analyzing industrial images. Such images are typically large and may only need to be partially analyzed where only a small portion is problematic (e.g., identifying defects on a surface). Since manual labeling these images is expensive, weak supervision is an attractive alternative where the idea is to generate weak labels that are not perfect, but can be produced at scale. Data programming is a recent paradigm in this category where it uses human knowledge in the form of labeling functions and combines them into a generative model. Data programming has been successful in applications based on text or structured data and can also be applied to images usually if one can find a way to convert them into structured data. In this work, we expand the horizon of data programming by directly applying it to images without this conversion, which is a common scenario for industrial applications. We propose Inspector Gadget, an image labeling system that combines crowdsourcing, data augmentation, and data programming to produce weak labels at scale for image classification. We perform experiments on real industrial image datasets and show that Inspector Gadget obtains better performance than other weak-labeling techniques: Snuba, GOGGLES, and self-learning baselines using convolutional neural networks (CNNs) without pre-training.",0
"In the era of Software 2.0, democratized machine learning for images faces a significant challenge of obtaining enough labeled data for training. This issue is especially crucial in a manufacturing environment where machine learning is necessary for product quality control through the analysis of industrial images. These images are often large and require partial analysis, such as identifying defects on a surface. Manual labeling is expensive, making weak supervision an attractive alternative. Data programming is a recent approach that utilizes human knowledge in the form of labeling functions and combines them into a generative model. While successful in text and structured data applications, data programming can also be applied to images, provided they can be converted into structured data. This study proposes Inspector Gadget, a system that combines crowdsourcing, data augmentation, and data programming to produce weak labels at scale for image classification in industrial applications. The study shows that Inspector Gadget outperforms other weak-labeling techniques, including Snuba, GOGGLES, and self-learning baselines that use CNNs without pre-training.",1
"In this paper, we focus on learning low-dimensional embeddings for nodes in graph-structured data. To achieve this, we propose Caps2NE -- a new unsupervised embedding model leveraging a network of two capsule layers. Caps2NE induces a routing process to aggregate feature vectors of context neighbors of a given target node at the first capsule layer, then feed these features into the second capsule layer to infer a plausible embedding for the target node. Experimental results show that our proposed Caps2NE obtains state-of-the-art performances on benchmark datasets for the node classification task. Our code is available at: \url{https://github.com/daiquocnguyen/Caps2NE}.",0
"The focus of this paper is on acquiring low-dimensional embeddings for nodes within graph-structured data. To accomplish this task, we introduce Caps2NE, a novel unsupervised embedding model that employs a two-layer capsule network. Caps2NE initiates a routing process that amalgamates feature vectors of context neighbors of a specific target node on the first capsule layer, then utilizes these features to infer a plausible embedding for the target node on the second capsule layer. Our experimental results demonstrate that Caps2NE achieves superior performances on benchmark datasets for the node classification task. Interested parties may access our code at \url{https://github.com/daiquocnguyen/Caps2NE}.",1
"Deep convolutional neural networks (CNNs) have shown outstanding performance in the task of semantically segmenting images. However, applying the same methods on 3D data still poses challenges due to the heavy memory requirements and the lack of structured data. Here, we propose LatticeNet, a novel approach for 3D semantic segmentation, which takes as input raw point clouds. A PointNet describes the local geometry which we embed into a sparse permutohedral lattice. The lattice allows for fast convolutions while keeping a low memory footprint. Further, we introduce DeformSlice, a novel learned data-dependent interpolation for projecting lattice features back onto the point cloud. We present results of 3D segmentation on various datasets where our method achieves state-of-the-art performance.",0
"While deep convolutional neural networks (CNNs) have demonstrated exceptional performance in semantically segmenting images, applying the same techniques to 3D data presents challenges due to extensive memory requirements and a lack of structured data. In response, we propose a new approach called LatticeNet, which processes raw point clouds for 3D semantic segmentation. We utilize a PointNet to describe local geometry, which we embed into a sparse permutohedral lattice, allowing for quick convolutions while maintaining a small memory footprint. In addition, we introduce DeformSlice, a learned data-dependent interpolation for projecting lattice features back onto the point cloud. We present results of 3D segmentation on various datasets where our method achieves state-of-the-art performance.",1
"Deep generative models have made tremendous advances in image and signal representation learning and generation. These models employ the full Euclidean space or a bounded subset as the latent space, whose flat geometry, however, is often too simplistic to meaningfully reflect the manifold structure of the data. In this work, we advocate the use of a multi-chart latent space for better data representation. Inspired by differential geometry, we propose a \textbf{Chart Auto-Encoder (CAE)} and prove a universal approximation theorem on its representation capability. We show that the training data size and the network size scale exponentially in approximation error with an exponent depending on the intrinsic dimension of the data manifold. CAE admits desirable manifold properties that auto-encoders with a flat latent space fail to obey, predominantly proximity of data. We conduct extensive experimentation with synthetic and real-life examples to demonstrate that CAE provides reconstruction with high fidelity, preserves proximity in the latent space, and generates new data remaining near the manifold. These experiments show that CAE is advantageous over existing auto-encoders and variants by preserving the topology of the data manifold as well as its geometry.",0
"Significant advancements have been made in image and signal representation learning and generation through the use of deep generative models. These models utilize either the entire Euclidean space or a limited subset as the latent space. However, the flat geometry of these spaces is often too simplistic to accurately reflect the manifold structure of the data. To address this issue, a multi-chart latent space is proposed in this study for improved data representation. The Chart Auto-Encoder (CAE) is proposed, which is inspired by differential geometry, and a universal approximation theorem is established on its representation capability. The training data size and network size exhibit exponential scaling in approximation error, with the exponent being dependent on the intrinsic dimension of the data manifold. CAE possesses desirable manifold properties, such as proximity of data, that flat latent space auto-encoders lack. Synthetic and real-life examples are used in extensive experimentation to demonstrate that CAE provides high fidelity reconstruction, preserves proximity in the latent space, and generates new data that remains near the manifold. These experiments indicate that CAE is superior to existing auto-encoders and their variants by preserving both the topology and geometry of the data manifold.",1
"The paper introduces two new aggregation functions to encode structural knowledge from tree-structured data. They leverage the Canonical and Tensor-Train decompositions to yield expressive context aggregation while limiting the number of model parameters. Finally, we define two novel neural recursive models for trees leveraging such aggregation functions, and we test them on two tree classification tasks, showing the advantage of proposed models when tree outdegree increases.",0
"In this paper, we present two fresh aggregation functions that encode structural knowledge of tree-structured data. These functions use Canonical and Tensor-Train decompositions to offer rich context aggregation while keeping the number of model parameters in check. Additionally, we introduce two unique neural recursive models for trees that utilize these aggregation functions. We evaluate these models on two tree classification tasks and demonstrate their superiority when the tree outdegree is high.",1
"Sparse incidence tensors can represent a variety of structured data. For example, we may represent attributed graphs using their node-node, node-edge, or edge-edge incidence matrices. In higher dimensions, incidence tensors can represent simplicial complexes and polytopes. In this paper, we formalize incidence tensors, analyze their structure, and present the family of equivariant networks that operate on them. We show that any incidence tensor decomposes into invariant subsets. This decomposition, in turn, leads to a decomposition of the corresponding equivariant linear maps, for which we prove an efficient pooling-and-broadcasting implementation.",0
"Incidence tensors have versatile applications in representing structured data, including attributed graphs that can be represented through node-node, node-edge, or edge-edge incidence matrices. Additionally, in higher dimensions, simplicial complexes and polytopes can be represented through incidence tensors. This paper formalizes the concept of incidence tensors, explores their structure, and introduces a family of equivariant networks that operate on them. Our analysis reveals that any incidence tensor can be broken down into invariant subsets, leading to a corresponding decomposition of equivariant linear maps, which we demonstrate through an efficient pooling-and-broadcasting implementation.",1
"Variational autoencoder (VAE) is a widely used generative model for learning latent representations. Burda et al. in their seminal paper showed that learning capacity of VAE is limited by over-pruning. It is a phenomenon where a significant number of latent variables fail to capture any information about the input data and the corresponding hidden units become inactive. This adversely affects learning diverse and interpretable latent representations. As variational graph autoencoder (VGAE) extends VAE for graph-structured data, it inherits the over-pruning problem. In this paper, we adopt a model based approach and propose epitomic VGAE (EVGAE),a generative variational framework for graph datasets which successfully mitigates the over-pruning problem and also boosts the generative ability of VGAE. We consider EVGAE to consist of multiple sparse VGAE models, called epitomes, that are groups of latent variables sharing the latent space. This approach aids in increasing active units as epitomes compete to learn better representation of the graph data. We verify our claims via experiments on three benchmark datasets. Our experiments show that EVGAE has a better generative ability than VGAE. Moreover, EVGAE outperforms VGAE on link prediction task in citation networks.",0
"The widespread use of the Variational Autoencoder (VAE) as a generative model for acquiring latent representations has been limited by over-pruning, as demonstrated by Burda et al. This phenomenon arises when a significant number of latent variables fail to capture any information about the input data, causing the corresponding hidden units to become inactive. This has a negative impact on the generation of diverse and interpretable latent representations. Variational Graph Autoencoder (VGAE), an extension of VAE for graph-structured data, inherits this over-pruning issue. To address this, we propose Epitomic VGAE (EVGAE), a generative variational framework for graph datasets that effectively mitigates over-pruning and enhances VGAE's generative capacity. EVGAE is composed of several sparse VGAE models (called epitomes) that share the latent space. This approach promotes the activation of units, as epitomes compete to acquire a better representation of the graph data. We demonstrate our findings through experiments on three benchmark datasets, which reveal that EVGAE has superior generative capacity compared to VGAE. Additionally, EVGAE surpasses VGAE in the link prediction task for citation networks.",1
"Active learning (AL) on attributed graphs has received increasing attention with the prevalence of graph-structured data. Although AL has been widely studied for alleviating label sparsity issues with the conventional non-related data, how to make it effective over attributed graphs remains an open research question. Existing AL algorithms on graphs attempt to reuse the classic AL query strategies designed for non-related data. However, they suffer from two major limitations. First, different AL query strategies calculated in distinct scoring spaces are often naively combined to determine which nodes to be labelled. Second, the AL query engine and the learning of the classifier are treated as two separating processes, resulting in unsatisfactory performance. In this paper, we propose a SEmi-supervised Adversarial active Learning (SEAL) framework on attributed graphs, which fully leverages the representation power of deep neural networks and devises a novel AL query strategy in an adversarial way. Our framework learns two adversarial components: a graph embedding network that encodes both the unlabelled and labelled nodes into a latent space, expecting to trick the discriminator to regard all nodes as already labelled, and a semi-supervised discriminator network that distinguishes the unlabelled from the existing labelled nodes in the latent space. The divergence score, generated by the discriminator in a unified latent space, serves as the informativeness measure to actively select the most informative node to be labelled by an oracle. The two adversarial components form a closed loop to mutually and simultaneously reinforce each other towards enhancing the active learning performance. Extensive experiments on four real-world networks validate the effectiveness of the SEAL framework with superior performance improvements to state-of-the-art baselines.",0
"Attributed graphs have become increasingly prevalent, leading to an increase in interest regarding active learning (AL) on this type of data. However, while AL has been widely studied for non-related data to alleviate label sparsity issues, making it effective on attributed graphs remains an open research question. Existing AL algorithms for graphs attempt to reuse non-related data query strategies, but face two major limitations. Firstly, different query strategies are often combined naively, and secondly, the separation of the AL query engine and classifier learning results in suboptimal performance. To address these issues, we propose the SEmi-supervised Adversarial active Learning (SEAL) framework, which utilizes deep neural networks and an adversarial AL query strategy. Our framework includes a graph embedding network and a semi-supervised discriminator network, which work together to provide an informativeness measure for selecting nodes to label. The adversarial components reinforce each other to enhance the active learning performance. Our experiments on four real-world networks demonstrate that the SEAL framework outperforms state-of-the-art baselines.",1
"Graph Convolutional Networks (GCNs) have already demonstrated their powerful ability to model the irregular data, e.g., skeletal data in human action recognition, providing an exciting new way to fuse rich structural information for nodes residing in different parts of a graph. In human action recognition, current works introduce a dynamic graph generation mechanism to better capture the underlying semantic skeleton connections and thus improves the performance. In this paper, we provide an orthogonal way to explore the underlying connections. Instead of introducing an expensive dynamic graph generation paradigm, we build a more efficient GCN on a Riemann manifold, which we think is a more suitable space to model the graph data, to make the extracted representations fit the embedding matrix. Specifically, we present a novel spatial-temporal GCN (ST-GCN) architecture which is defined via the Poincar\'e geometry such that it is able to better model the latent anatomy of the structure data. To further explore the optimal projection dimension in the Riemann space, we mix different dimensions on the manifold and provide an efficient way to explore the dimension for each ST-GCN layer. With the final resulted architecture, we evaluate our method on two current largest scale 3D datasets, i.e., NTU RGB+D and NTU RGB+D 120. The comparison results show that the model could achieve a superior performance under any given evaluation metrics with only 40\% model size when compared with the previous best GCN method, which proves the effectiveness of our model.",0
"The powerful ability of Graph Convolutional Networks (GCNs) to model irregular data, such as skeletal data in human action recognition, has already been demonstrated. This provides an exciting new way to incorporate rich structural information for nodes residing in different parts of a graph. In current human action recognition works, a dynamic graph generation mechanism is introduced to better capture the underlying semantic skeleton connections and improve performance. In this paper, an alternative approach is proposed which involves building a more efficient GCN on a Riemann manifold, which is believed to be a more suitable space to model the graph data, and to make the extracted representations fit the embedding matrix. The proposed spatial-temporal GCN (ST-GCN) architecture is defined via the Poincar\'e geometry to better model the latent anatomy of the structure data. To further explore the optimal projection dimension in the Riemann space, different dimensions on the manifold are mixed, providing an efficient way to explore the dimension for each ST-GCN layer. The proposed architecture is evaluated on two of the largest scale 3D datasets, NTU RGB+D and NTU RGB+D 120, and the results demonstrate that the model achieves superior performance under any given evaluation metric with only 40\% model size when compared with the previous best GCN method.",1
"The effective representation, processing, analysis, and visualization of large-scale structured data, especially those related to complex domains such as networks and graphs, are one of the key questions in modern machine learning. Graph signal processing (GSP), a vibrant branch of signal processing models and algorithms that aims at handling data supported on graphs, opens new paths of research to address this challenge. In this article, we review a few important contributions made by GSP concepts and tools, such as graph filters and transforms, to the development of novel machine learning algorithms. In particular, our discussion focuses on the following three aspects: exploiting data structure and relational priors, improving data and computational efficiency, and enhancing model interpretability. Furthermore, we provide new perspectives on future development of GSP techniques that may serve as a bridge between applied mathematics and signal processing on one side, and machine learning and network science on the other. Cross-fertilization across these different disciplines may help unlock the numerous challenges of complex data analysis in the modern age.",0
"Modern machine learning faces the challenge of effectively representing, processing, analyzing, and visualizing large-scale structured data, particularly in complex domains like networks and graphs. Graph signal processing (GSP) is a thriving branch of signal processing models and algorithms that deals with data supported on graphs, and presents promising avenues for research in this area. This article reviews the significant contributions of GSP concepts and tools, such as graph filters and transforms, to the development of novel machine learning algorithms. The focus is on exploiting data structure and relational priors, improving data and computational efficiency, and enhancing model interpretability. Moreover, the article presents new perspectives on the future development of GSP techniques that can bridge applied mathematics and signal processing with machine learning and network science. By fostering cross-fertilization across these various disciplines, the challenges of complex data analysis in the modern era can be overcome.",1
"Understanding how certain brain regions relate to a specific neurological disorder has been an important area of neuroimaging research. A promising approach to identify the salient regions is using Graph Neural Networks (GNNs), which can be used to analyze graph structured data, e.g. brain networks constructed by functional magnetic resonance imaging (fMRI). We propose an interpretable GNN framework with a novel salient region selection mechanism to determine neurological brain biomarkers associated with disorders. Specifically, we design novel regularized pooling layers that highlight salient regions of interests (ROIs) so that we can infer which ROIs are important to identify a certain disease based on the node pooling scores calculated by the pooling layers. Our proposed framework, Pooling Regularized-GNN (PR-GNN), encourages reasonable ROI-selection and provides flexibility to preserve either individual- or group-level patterns. We apply the PR-GNN framework on a Biopoint Autism Spectral Disorder (ASD) fMRI dataset. We investigate different choices of the hyperparameters and show that PR-GNN outperforms baseline methods in terms of classification accuracy. The salient ROI detection results show high correspondence with the previous neuroimaging-derived biomarkers for ASD.",0
"The identification of brain regions associated with specific neurological disorders is a crucial area of neuroimaging research. Graph Neural Networks (GNNs) are a promising method for analyzing graph structured data, such as brain networks constructed through functional magnetic resonance imaging (fMRI), to determine the most important regions. To this end, we have developed an interpretable GNN framework with a unique salient region selection mechanism that identifies neurological brain biomarkers associated with disorders. Our framework employs novel regularized pooling layers to highlight salient regions of interest (ROIs) and infer which ROIs are crucial in identifying a particular disease based on node pooling scores. Our Pooling Regularized-GNN (PR-GNN) framework encourages reasonable ROI-selection and preserves either individual- or group-level patterns. We have applied the PR-GNN framework to a Biopoint Autism Spectral Disorder (ASD) fMRI dataset, and our results demonstrate that PR-GNN outperforms baseline methods in terms of classification accuracy. The salient ROI detection results correspond highly with previous neuroimaging-derived biomarkers for ASD.",1
"Tensor data with rich structural information becomes increasingly important in process modeling, monitoring, and diagnosis. Here structural information is referred to structural properties such as sparsity, smoothness, low-rank, and piecewise constancy. To reveal useful information from tensor data, we propose to decompose the tensor into the summation of multiple components based on different structural information of them. In this paper, we provide a new definition of structural information in tensor data. Based on it, we propose an additive tensor decomposition (ATD) framework to extract useful information from tensor data. This framework specifies a high dimensional optimization problem to obtain the components with distinct structural information. An alternating direction method of multipliers (ADMM) algorithm is proposed to solve it, which is highly parallelable and thus suitable for the proposed optimization problem. Two simulation examples and a real case study in medical image analysis illustrate the versatility and effectiveness of the ATD framework.",0
"The importance of tensor data with rich structural information is growing in process modeling, monitoring, and diagnosis. This structural information encompasses properties such as sparsity, smoothness, low-rank, and piecewise constancy. To uncover valuable insights from tensor data, we suggest decomposing it into multiple components based on their different structural information. This paper defines structural information in tensor data and proposes an additive tensor decomposition (ATD) framework to extract useful information. The framework outlines a high-dimensional optimization problem to obtain components with distinct structural information. An ADMM algorithm is proposed to solve this problem, which is highly parallelizable and suitable for the optimization problem. The ATD framework's versatility and effectiveness are demonstrated through two simulation examples and a real case study in medical image analysis.",1
"Graph neural networks emerge as a promising modeling method for applications dealing with datasets that are best represented in the graph domain. In specific, developing recommendation systems often require addressing sparse structured data which often lacks the feature richness in either the user and/or item side and requires processing within the correct context for optimal performance. These datasets intuitively can be mapped to and represented as networks or graphs. In this paper, we propose the Hierarchical BiGraph Neural Network (HBGNN), a hierarchical approach of using GNNs as recommendation systems and structuring the user-item features using a bigraph framework. Our experimental results show competitive performance with current recommendation system methods and transferability.",0
"Graph neural networks are a promising modeling technique for datasets that are best suited for the graph domain. In particular, recommendation systems often require addressing sparse structured data that lacks feature richness in either the user or item side and benefits from processing within the correct context for optimal performance. These datasets can be intuitively mapped and represented as networks or graphs. Our paper suggests the Hierarchical BiGraph Neural Network (HBGNN) as a hierarchical approach to using GNNs as recommendation systems and structuring the user-item features using a bigraph framework. Our experiments reveal competitive performance with current recommendation system methods and transferability.",1
"Graph convolutional networks gain remarkable success in semi-supervised learning on graph structured data. The key to graph-based semisupervised learning is capturing the smoothness of labels or features over nodes exerted by graph structure. Previous methods, spectral methods and spatial methods, devote to defining graph convolution as a weighted average over neighboring nodes, and then learn graph convolution kernels to leverage the smoothness to improve the performance of graph-based semi-supervised learning. One open challenge is how to determine appropriate neighborhood that reflects relevant information of smoothness manifested in graph structure. In this paper, we propose GraphHeat, leveraging heat kernel to enhance low-frequency filters and enforce smoothness in the signal variation on the graph. GraphHeat leverages the local structure of target node under heat diffusion to determine its neighboring nodes flexibly, without the constraint of order suffered by previous methods. GraphHeat achieves state-of-the-art results in the task of graph-based semi-supervised classification across three benchmark datasets: Cora, Citeseer and Pubmed.",0
"Graph convolutional networks have achieved notable success in semi-supervised learning on graph-structured data by capturing the smoothness of labels or features over nodes through the graph structure. The two previous methods, spectral and spatial, define graph convolution as a weighted average over neighboring nodes and learn graph convolution kernels to improve performance. However, determining the appropriate neighborhood that reflects relevant information of smoothness manifested in graph structure remains an open challenge. In this study, we introduce GraphHeat, which utilizes the heat kernel to enhance low-frequency filters and enforce smoothness in signal variation on the graph. Unlike previous methods, GraphHeat leverages the local structure of the target node under heat diffusion to determine its neighboring nodes flexibly. Our method outperforms state-of-the-art results in graph-based semi-supervised classification on three benchmark datasets: Cora, Citeseer, and Pubmed.",1
"The objective of active learning (AL) is to train classification models with less number of labeled instances by selecting only the most informative instances for labeling. The AL algorithms designed for other data types such as images and text do not perform well on graph-structured data. Although a few heuristics-based AL algorithms have been proposed for graphs, a principled approach is lacking. In this paper, we propose MetAL, an AL approach that selects unlabeled instances that directly improve the future performance of a classification model. For a semi-supervised learning problem, we formulate the AL task as a bilevel optimization problem. Based on recent work in meta-learning, we use the meta-gradients to approximate the impact of retraining the model with any unlabeled instance on the model performance. Using multiple graph datasets belonging to different domains, we demonstrate that MetAL efficiently outperforms existing state-of-the-art AL algorithms.",0
"Active learning (AL) aims to train classification models with fewer labeled instances by choosing only the most informative ones for labeling. However, AL algorithms designed for other data types, such as images and text, do not work well on graph-structured data. Although a few heuristic-based AL algorithms have been proposed for graphs, there is a lack of a principled approach. This paper introduces MetAL, an AL approach that selects unlabeled instances that directly enhance the future performance of a classification model. To solve the AL task for a semi-supervised learning problem, we formulate it as a bilevel optimization problem. Using recent work in meta-learning, we approximate the impact of retraining the model with any unlabeled instance on the model performance with meta-gradients. We demonstrate the efficiency of MetAL on multiple graph datasets from different domains, outperforming existing state-of-the-art AL algorithms.",1
"Convolutional neural networks typically consist of many convolutional layers followed by one or more fully connected layers. While convolutional layers map between high-order activation tensors, the fully connected layers operate on flattened activation vectors. Despite empirical success, this approach has notable drawbacks. Flattening followed by fully connected layers discards multilinear structure in the activations and requires many parameters. We address these problems by incorporating tensor algebraic operations that preserve multilinear structure at every layer. First, we introduce Tensor Contraction Layers (TCLs) that reduce the dimensionality of their input while preserving their multilinear structure using tensor contraction. Next, we introduce Tensor Regression Layers (TRLs), which express outputs through a low-rank multilinear mapping from a high-order activation tensor to an output tensor of arbitrary order. We learn the contraction and regression factors end-to-end, and produce accurate nets with fewer parameters. Additionally, our layers regularize networks by imposing low-rank constraints on the activations (TCL) and regression weights (TRL). Experiments on ImageNet show that, applied to VGG and ResNet architectures, TCLs and TRLs reduce the number of parameters compared to fully connected layers by more than 65% while maintaining or increasing accuracy. In addition to the space savings, our approach's ability to leverage topological structure can be crucial for structured data such as MRI. In particular, we demonstrate significant performance improvements over comparable architectures on three tasks associated with the UK Biobank dataset.",0
"Convolutional neural networks are typically composed of multiple convolutional layers, followed by one or more fully connected layers. These fully connected layers operate on flattened activation vectors, which discards multilinear structure and requires numerous parameters. To address these issues, we have introduced Tensor Contraction Layers (TCLs) and Tensor Regression Layers (TRLs) that preserve multilinear structure at every layer and reduce the number of parameters. We learn the contraction and regression factors end-to-end, which also imposes low-rank constraints on the activations and regression weights. Our approach reduces the number of parameters by over 65% while maintaining or increasing accuracy on VGG and ResNet architectures. It also improves performance on three tasks associated with the UK Biobank dataset. This approach's ability to leverage topological structure can be especially beneficial for structured data such as MRI.",1
"Active search is the process of identifying high-value data points in a large and often high-dimensional parameter space that can be expensive to evaluate. Traditional active search techniques like Bayesian optimization trade off exploration and exploitation over consecutive evaluations, and have historically focused on single or small (<5) numbers of examples evaluated per round. As modern data sets grow, so does the need to scale active search to large data sets and batch sizes. In this paper, we present a general hierarchical framework based on bandit algorithms to scale active search to large batch sizes by maximizing information derived from the unique structure of each dataset. Our hierarchical framework, Hierarchical Batch Bandit Search (HBBS), strategically distributes batch selection across a learned embedding space by facilitating wide exploration of different structural elements within a dataset. We focus our application of HBBS on modern biology, where large batch experimentation is often fundamental to the research process, and demonstrate batch design of biological sequences (protein and DNA). We also present a new Gym environment to easily simulate diverse biological sequences and to enable more comprehensive evaluation of active search methods across heterogeneous data sets. The HBBS framework improves upon standard performance, wall-clock, and scalability benchmarks for batch search by using a broad exploration strategy across coarse partitions and fine-grained exploitation within each partition of structured data.",0
"The process of active search involves identifying valuable data points in a parameter space that may be costly to evaluate due to its size and complexity. Traditional active search methods like Bayesian optimization have historically focused on evaluating a small number of examples per round, making them less suitable for large and growing datasets. To address this issue, we present a hierarchical framework called Hierarchical Batch Bandit Search (HBBS) that uses bandit algorithms to scale active search to large batch sizes. By distributing batch selection across a learned embedding space, HBBS can explore different structural elements within a dataset while maintaining fine-grained exploitation. We apply HBBS to modern biology, where it is crucial to perform large batch experimentation, and demonstrate its effectiveness in designing biological sequences. Additionally, we introduce a Gym environment that allows for the simulation and evaluation of active search methods across diverse biological datasets. HBBS outperforms standard methods in terms of performance, scalability, and wall-clock benchmarks by using a broad exploration strategy across coarse partitions and fine-grained exploitation within each partition of structured data.",1
"Predicting interactions among heterogenous graph structured data has numerous applications such as knowledge graph completion, recommendation systems and drug discovery. Often times, the links to be predicted belong to rare types such as the case in repurposing drugs for novel diseases. This motivates the task of few-shot link prediction. Typically, GCNs are ill-equipped in learning such rare link types since the relation embedding is not learned in an inductive fashion. This paper proposes an inductive RGCN for learning informative relation embeddings even in the few-shot learning regime. The proposed inductive model significantly outperforms the RGCN and state-of-the-art KGE models in few-shot learning tasks. Furthermore, we apply our method on the drug-repurposing knowledge graph (DRKG) for discovering drugs for Covid-19. We pose the drug discovery task as link prediction and learn embeddings for the biological entities that partake in the DRKG. Our initial results corroborate that several drugs used in clinical trials were identified as possible drug candidates. The method in this paper are implemented using the efficient deep graph learning (DGL)",0
"There are various applications for predicting interactions among graph structured data that have diverse characteristics, including knowledge graph completion, recommendation systems, and drug discovery. In some cases, the links that need to be predicted are rare, such as when repurposing drugs for new diseases, which leads to the need for few-shot link prediction. However, GCNs are not well-suited for learning rare link types since the relation embedding is not learned in an inductive manner. To address this issue, this paper proposes an inductive RGCN that can learn informative relation embeddings even in the few-shot learning scenario. The proposed inductive model significantly outperforms both the RGCN and state-of-the-art KGE models in few-shot learning tasks. Additionally, the authors apply their method to the drug-repurposing knowledge graph (DRKG) to discover drugs for Covid-19 by posing the drug discovery task as link prediction and learning embeddings for the biological entities involved in the DRKG. The initial results show that several drugs used in clinical trials were identified as potential drug candidates. The method employed in this study is implemented using the efficient deep graph learning (DGL) framework.",1
"Applying network science approaches to investigate the functions and anatomy of the human brain is prevalent in modern medical imaging analysis. Due to the complex network topology, for an individual brain, mining a discriminative network representation from the multimodal brain networks is non-trivial. The recent success of deep learning techniques on graph-structured data suggests a new way to model the non-linear cross-modality relationship. However, current deep brain network methods either ignore the intrinsic graph topology or require a network basis shared within a group. To address these challenges, we propose a novel end-to-end deep graph representation learning (Deep Multimodal Brain Networks - DMBN) to fuse multimodal brain networks. Specifically, we decipher the cross-modality relationship through a graph encoding and decoding process. The higher-order network mappings from brain structural networks to functional networks are learned in the node domain. The learned network representation is a set of node features that are informative to induce brain saliency maps in a supervised manner. We test our framework in both synthetic and real image data. The experimental results show the superiority of the proposed method over some other state-of-the-art deep brain network models.",0
"The use of network science techniques to explore the structure and function of the human brain is prevalent in current medical imaging analysis. However, extracting a discriminative network representation from the complex topology of an individual brain's multimodal networks is a challenging task. Recent advances in deep learning on graph-structured data offer a promising solution to model the non-linear cross-modality relationship. Nevertheless, current deep brain network methods fail to consider the intrinsic graph topology or require a shared network basis within a group. To tackle these issues, we introduce a novel end-to-end deep graph representation learning approach called Deep Multimodal Brain Networks (DMBN) to combine multimodal brain networks. Our method utilizes a graph encoding and decoding process to decipher the cross-modality relationship and learns higher-order network mappings from structural to functional brain networks in the node domain. The resulting network representation is a set of informative node features that can induce brain saliency maps in a supervised manner. We evaluate our framework on synthetic and real image data and demonstrate the superiority of our method over some other state-of-the-art deep brain network models.",1
"Despite diverse efforts to mine various modalities of medical data, the conversations between physicians and patients at the time of care remain an untapped source of insights. In this paper, we leverage this data to extract structured information that might assist physicians with post-visit documentation in electronic health records, potentially lightening the clerical burden. In this exploratory study, we describe a new dataset consisting of conversation transcripts, post-visit summaries, corresponding supporting evidence (in the transcript), and structured labels. We focus on the tasks of recognizing relevant diagnoses and abnormalities in the review of organ systems (RoS). One methodological challenge is that the conversations are long (around 1500 words), making it difficult for modern deep-learning models to use them as input. To address this challenge, we extract noteworthy utterances---parts of the conversation likely to be cited as evidence supporting some summary sentence. We find that by first filtering for (predicted) noteworthy utterances, we can significantly boost predictive performance for recognizing both diagnoses and RoS abnormalities.",0
"Although there have been numerous efforts to extract medical data from various sources, the conversations between doctors and patients during care have yet to be utilized. This paper aims to capitalize on this untapped resource by extracting structured information from these conversations that can aid physicians in completing electronic health records and decrease administrative workload. The study focuses on recognizing relevant diagnoses and abnormalities in the review of organ systems (RoS) but faces the challenge of lengthy conversations, which make it difficult for modern deep-learning models to use them as input. To overcome this hurdle, noteworthy utterances - parts of the conversation that are likely to be cited as evidence supporting some summary sentence - are extracted. By filtering for these utterances, predictive performance for recognizing diagnoses and RoS abnormalities is significantly improved.",1
"Graph representation learning nowadays becomes fundamental in analyzing graph-structured data. Inspired by recent success of contrastive methods, in this paper, we propose a novel framework for unsupervised graph representation learning by leveraging a contrastive objective at the node level. Specifically, we generate two graph views by corruption and learn node representations by maximizing the agreement of node representations in these two views. To provide diverse node contexts for the contrastive objective, we propose a hybrid scheme for generating graph views on both structure and attribute levels. Besides, we provide theoretical justification behind our motivation from two perspectives, mutual information and the classical triplet loss. We perform empirical experiments on both transductive and inductive learning tasks using a variety of real-world datasets. Experimental experiments demonstrate that despite its simplicity, our proposed method consistently outperforms existing state-of-the-art methods by large margins. Moreover, our unsupervised method even surpasses its supervised counterparts on transductive tasks, demonstrating its great potential in real-world applications.",0
"In today's world, the analysis of graph-structured data relies heavily on graph representation learning. This paper introduces a new framework for unsupervised graph representation learning that is inspired by the success of contrastive methods. The proposed method leverages a contrastive objective at the node level by generating two graph views through corruption and learning node representations by maximizing the agreement of node representations in these two views. To provide diverse node contexts, a hybrid scheme is proposed for generating graph views on both structure and attribute levels. The motivation behind the proposed method is supported by theoretical justification from two perspectives, mutual information and the classical triplet loss. Empirical experiments are conducted on various real-world datasets for both transductive and inductive learning tasks, and the results show that the proposed method consistently outperforms existing state-of-the-art methods by a significant margin, including its supervised counterparts on transductive tasks. These findings demonstrate the potential of the unsupervised method in real-world applications despite its simplicity.",1
"Complex Event Processing (CEP) is an event processing paradigm to perform real-time analytics over streaming data and match high-level event patterns. Presently, CEP is limited to process structured data stream. Video streams are complicated due to their unstructured data model and limit CEP systems to perform matching over them. This work introduces a graph-based structure for continuous evolving video streams, which enables the CEP system to query complex video event patterns. We propose the Video Event Knowledge Graph (VEKG), a graph driven representation of video data. VEKG models video objects as nodes and their relationship interaction as edges over time and space. It creates a semantic knowledge representation of video data derived from the detection of high-level semantic concepts from the video using an ensemble of deep learning models. A CEP-based state optimization - VEKG-Time Aggregated Graph (VEKG-TAG) is proposed over VEKG representation for faster event detection. VEKG-TAG is a spatiotemporal graph aggregation method that provides a summarized view of the VEKG graph over a given time length. We defined a set of nine event pattern rules for two domains (Activity Recognition and Traffic Management), which act as a query and applied over VEKG graphs to discover complex event patterns. To show the efficacy of our approach, we performed extensive experiments over 801 video clips across 10 datasets. The proposed VEKG approach was compared with other state-of-the-art methods and was able to detect complex event patterns over videos with F-Score ranging from 0.44 to 0.90. In the given experiments, the optimized VEKG-TAG was able to reduce 99% and 93% of VEKG nodes and edges, respectively, with 5.19X faster search time, achieving sub-second median latency of 4-20 milliseconds.",0
"The process of Complex Event Processing (CEP) involves analyzing real-time streaming data and identifying high-level event patterns. However, CEP systems are currently limited to structured data streams and are unable to match patterns in unstructured video streams. To address this issue, we propose the Video Event Knowledge Graph (VEKG), a graph-based structure that models video data as nodes and their interactions over time and space as edges. VEKG creates a semantic knowledge representation of video data using deep learning models and enables faster event detection through a spatiotemporal graph aggregation method called VEKG-Time Aggregated Graph (VEKG-TAG). We defined nine event pattern rules for Activity Recognition and Traffic Management domains, which act as queries over VEKG graphs to discover complex event patterns. Our experiments show that VEKG outperforms other state-of-the-art methods with F-Score ranging from 0.44 to 0.90. VEKG-TAG reduces 99% and 93% of VEKG nodes and edges, respectively, with 5.19X faster search time, achieving sub-second median latency of 4-20 milliseconds.",1
"We present Geo2DR (Geometric to Distributed Representations), a GPU ready Python library for unsupervised learning on graph-structured data using discrete substructure patterns and neural language models. It contains efficient implementations of popular graph decomposition algorithms and neural language models in PyTorch which can be combined to learn representations of graphs using the distributive hypothesis. Furthermore, Geo2DR comes with general data processing and loading methods to bring substantial speed-up in the training of the neural language models. Through this we provide a modular set of tools and methods to quickly construct systems capable of learning distributed representations of graphs. This is useful for replication of existing methods, modification, or development of completely new methods. This paper serves to present the Geo2DR library and perform a comprehensive comparative analysis of existing methods re-implemented using Geo2DR across widely used graph classification benchmarks. Geo2DR displays a high reproducibility of results in published methods and interoperability with other libraries useful for distributive language modelling.",0
"Introducing Geo2DR (Geometric to Distributed Representations), a Python library designed for unsupervised learning on graph-structured data. This library is GPU ready and utilizes both discrete substructure patterns and neural language models to efficiently implement popular graph decomposition algorithms in PyTorch. By combining these algorithms, Geo2DR can learn representations of graphs using the distributive hypothesis. Additionally, the library includes general data processing and loading methods that significantly speed up the training of neural language models. Overall, Geo2DR provides a modular set of tools and methods for quickly constructing systems capable of learning distributed representations of graphs. This paper presents the Geo2DR library and conducts a comprehensive comparative analysis of existing methods using Geo2DR across widely used graph classification benchmarks. Results show that Geo2DR can reproduce published methods with high accuracy and can also be used in conjunction with other libraries for distributive language modelling.",1
"Graph neural networks (GNNs) extends the functionality of traditional neural networks to graph-structured data. Similar to CNNs, an optimized design of graph convolution and pooling is key to success. Borrowing ideas from physics, we propose a path integral based graph neural networks (PAN) for classification and regression tasks on graphs. Specifically, we consider a convolution operation that involves every path linking the message sender and receiver with learnable weights depending on the path length, which corresponds to the maximal entropy random walk. It generalizes the graph Laplacian to a new transition matrix we call maximal entropy transition (MET) matrix derived from a path integral formalism. Importantly, the diagonal entries of the MET matrix are directly related to the subgraph centrality, thus providing a natural and adaptive pooling mechanism. PAN provides a versatile framework that can be tailored for different graph data with varying sizes and structures. We can view most existing GNN architectures as special cases of PAN. Experimental results show that PAN achieves state-of-the-art performance on various graph classification/regression tasks, including a new benchmark dataset from statistical mechanics we propose to boost applications of GNN in physical sciences.",0
"The functionality of traditional neural networks can be extended to graph-structured data with Graph Neural Networks (GNNs). Success with GNNs, like with Convolutional Neural Networks (CNNs), requires an optimized design of graph convolution and pooling. Drawing inspiration from physics, we introduce a path integral based Graph Neural Network (PAN) for classification and regression tasks on graphs. PAN utilizes a convolution operation that incorporates every path linking the message sender and receiver, with learnable weights dependent on the path length. This convolution generalizes the graph Laplacian to a new transition matrix, referred to as the Maximal Entropy Transition (MET) matrix, derived from a path integral formalism. The diagonal entries of the MET matrix are directly related to the subgraph centrality, providing a natural and adaptive pooling mechanism. PAN is a versatile framework that can be customized for various graph data sizes and structures. Existing GNN architectures can be viewed as special cases of PAN. Experimental results demonstrate that PAN achieves state-of-the-art performance on various graph classification/regression tasks, including a new benchmark dataset from statistical mechanics that we propose to promote the application of GNNs in physical sciences.",1
"Graph convolutional networks (GCNs) are a powerful deep learning approach for graph-structured data. Recently, GCNs and subsequent variants have shown superior performance in various application areas on real-world datasets. Despite their success, most of the current GCN models are shallow, due to the {\em over-smoothing} problem. In this paper, we study the problem of designing and analyzing deep graph convolutional networks. We propose the GCNII, an extension of the vanilla GCN model with two simple yet effective techniques: {\em Initial residual} and {\em Identity mapping}. We provide theoretical and empirical evidence that the two techniques effectively relieves the problem of over-smoothing. Our experiments show that the deep GCNII model outperforms the state-of-the-art methods on various semi- and full-supervised tasks. Code is available at https://github.com/chennnM/GCNII .",0
"The utilization of Graph Convolutional Networks (GCNs) has become increasingly popular in the realm of deep learning and has shown remarkable results in various real-world datasets. However, most existing GCN models are not deep due to the issue of excessive smoothing. In this article, we explore the concept of creating and evaluating deep graph convolutional networks and propose GCNII as an extension of the standard GCN model. GCNII introduces two uncomplicated yet effective techniques, Initial Residual and Identity Mapping, to alleviate the over-smoothing problem. Through theoretical and empirical analysis, we demonstrate that the two techniques are successful in addressing the issue of over-smoothing. Our research proves that the deep GCNII model surpasses previous state-of-the-art methods in both semi- and fully-supervised tasks. The code for GCNII is available at https://github.com/chennnM/GCNII.",1
"Graph Convolutional Networks (GCNs) have recently become the primary choice for learning from graph-structured data, superseding hash fingerprints in representing chemical compounds. However, GCNs lack the ability to take into account the ordering of node neighbors, even when there is a geometric interpretation of the graph vertices that provides an order based on their spatial positions. To remedy this issue, we propose Spatial Graph Convolutional Network (SGCN) which uses spatial features to efficiently learn from graphs that can be naturally located in space. Our contribution is threefold: we propose a GCN-inspired architecture which (i) leverages node positions, (ii) is a proper generalization of both GCNs and Convolutional Neural Networks (CNNs), (iii) benefits from augmentation which further improves the performance and assures invariance with respect to the desired properties. Empirically, SGCN outperforms state-of-the-art graph-based methods on image classification and chemical tasks.",0
"Recently, Graph Convolutional Networks (GCNs) have become the preferred method for learning from data structured in graphs, replacing hash fingerprints as a way to represent chemical compounds. However, GCNs do not consider the order of node neighbors, even when there is a spatial interpretation of the graph vertices that provides a natural order based on their location. To address this, we propose the Spatial Graph Convolutional Network (SGCN), which uses spatial features to learn efficiently from graphs that can be located in space. Our contribution is threefold: we introduce a GCN-inspired architecture that (i) takes advantage of node positions, (ii) generalizes both GCNs and Convolutional Neural Networks (CNNs), and (iii) benefits from augmentation to improve performance and ensure invariance with respect to desired properties. Empirically, SGCN outperforms state-of-the-art graph-based methods for image classification and chemical tasks.",1
"Complex applications such as big data analytics involve different forms of coupling relationships that reflect interactions between factors related to technical, business (domain-specific) and environmental (including socio-cultural and economic) aspects. There are diverse forms of couplings embedded in poor-structured and ill-structured data. Such couplings are ubiquitous, implicit and/or explicit, objective and/or subjective, heterogeneous and/or homogeneous, presenting complexities to existing learning systems in statistics, mathematics and computer sciences, such as typical dependency, association and correlation relationships. Modeling and learning such couplings thus is fundamental but challenging. This paper discusses the concept of coupling learning, focusing on the involvement of coupling relationships in learning systems. Coupling learning has great potential for building a deep understanding of the essence of business problems and handling challenges that have not been addressed well by existing learning theories and tools. This argument is verified by several case studies on coupling learning, including handling coupling in recommender systems, incorporating couplings into coupled clustering, coupling document clustering, coupled recommender algorithms and coupled behavior analysis for groups.",0
"Big data analytics involves various types of couplings that represent interactions between technical, business, and environmental factors, including socio-cultural and economic aspects. Poorly structured and ill-structured data often contain diverse forms of couplings that are ubiquitous, implicit or explicit, objective or subjective, and heterogeneous or homogeneous, which make it challenging for existing learning systems in statistics, mathematics, and computer sciences to model and learn such couplings. This paper explores the concept of coupling learning and its potential to provide a deep understanding of business problems and address challenges that existing learning theories and tools have not tackled well. The article presents several case studies on coupling learning, including recommender systems, coupled clustering, document clustering, coupled recommender algorithms, and coupled behavior analysis for groups.",1
"We introduce a family of multilayer graph kernels and establish new links between graph convolutional neural networks and kernel methods. Our approach generalizes convolutional kernel networks to graph-structured data, by representing graphs as a sequence of kernel feature maps, where each node carries information about local graph substructures. On the one hand, the kernel point of view offers an unsupervised, expressive, and easy-to-regularize data representation, which is useful when limited samples are available. On the other hand, our model can also be trained end-to-end on large-scale data, leading to new types of graph convolutional neural networks. We show that our method achieves competitive performance on several graph classification benchmarks, while offering simple model interpretation. Our code is freely available at https://github.com/claying/GCKN.",0
"Our research introduces a range of multilayer graph kernels and establishes novel connections between kernel methods and graph convolutional neural networks. Our methodology extends convolutional kernel networks to accommodate graph-structured data by representing graphs as a sequence of kernel feature maps, with each node providing information regarding local graph substructures. This approach offers an unsupervised, expressive, and easy-to-regularize data representation, which is particularly valuable when working with restricted samples. Additionally, our model can be trained end-to-end on large-scale data, resulting in novel types of graph convolutional neural networks. We demonstrate that our methodology achieves comparable results on various graph classification benchmarks, while providing straightforward model interpretation. Our code is accessible for free at https://github.com/claying/GCKN.",1
"Predicting and discovering drug-drug interactions (DDIs) is an important problem and has been studied extensively both from medical and machine learning point of view. Almost all of the machine learning approaches have focused on text data or textual representation of the structural data of drugs. We present the first work that uses drug structure images as the input and utilizes a Siamese convolutional network architecture to predict DDIs.",0
"The identification and forecast of drug-drug interactions (DDIs) is a significant issue that has been extensively researched in both medical and machine learning fields. Majority of machine learning techniques have concentrated on the textual data or textual portrayal of drug structural data. Our study, however, is the first to utilize drug structure images as input and applies a Siamese convolutional network architecture to predict DDIs.",1
"Graph neural networks are promising architecture for learning and inference with graph-structured data. Yet difficulties in modelling the ``parts'' and their ``interactions'' still persist in terms of graph classification, where graph-level representations are usually obtained by squeezing the whole graph into a single vector through graph pooling. From complex systems point of view, mixing all the parts of a system together can affect both model interpretability and predictive performance, because properties of a complex system arise largely from the interaction among its components. We analyze the intrinsic difficulty in graph classification under the unified concept of ``resolution dilemmas'' with learning theoretic recovery guarantees, and propose ``SLIM'', an inductive neural network model for Structural Landmarking and Interaction Modelling. It turns out, that by solving the resolution dilemmas, and leveraging explicit interacting relation between component parts of a graph to explain its complexity, SLIM is more interpretable, accurate, and offers new insight in graph representation learning.",0
"Learning and inferring with graph-structured data can be effectively achieved using graph neural networks. However, challenges still exist in accurately modeling the individual ""parts"" and their interactions in order to classify graphs. Commonly, graph-level representations are obtained through graph pooling, which involves squeezing the entire graph into a single vector. This approach can negatively impact interpretability and predictive performance, as the properties of complex systems are largely derived from the interactions among their components. We propose an inductive neural network model called ""SLIM"", which addresses the inherent difficulty of graph classification through the concept of ""resolution dilemmas"" and provides learning theoretic recovery guarantees. By leveraging explicit interaction between component parts of a graph, SLIM is able to explain its complexity and provide more accurate and interpretable results.",1
"Graph neural networks (GNNs) have been demonstrated to be powerful in modeling graph-structured data. However, training GNNs usually requires abundant task-specific labeled data, which is often arduously expensive to obtain. One effective way to reduce the labeling effort is to pre-train an expressive GNN model on unlabeled data with self-supervision and then transfer the learned model to downstream tasks with only a few labels. In this paper, we present the GPT-GNN framework to initialize GNNs by generative pre-training. GPT-GNN introduces a self-supervised attributed graph generation task to pre-train a GNN so that it can capture the structural and semantic properties of the graph. We factorize the likelihood of the graph generation into two components: 1) Attribute Generation and 2) Edge Generation. By modeling both components, GPT-GNN captures the inherent dependency between node attributes and graph structure during the generative process. Comprehensive experiments on the billion-scale Open Academic Graph and Amazon recommendation data demonstrate that GPT-GNN significantly outperforms state-of-the-art GNN models without pre-training by up to 9.1% across various downstream tasks.",0
"Graph neural networks (GNNs) are effective in modeling data with graph structures, but they often require a lot of labeled data which can be very expensive to obtain. To reduce labeling efforts, a method is to pre-train a GNN model on unlabeled data using self-supervision, and then transfer the model to downstream tasks with only a few labeled data. This paper presents the GPT-GNN framework, which uses generative pre-training to initialize GNNs. GPT-GNN uses a self-supervised attributed graph generation task to pre-train a GNN model that can capture the graph's structural and semantic properties. The likelihood of the graph generation is factorized into two components: Attribute Generation and Edge Generation. By modeling both components, GPT-GNN captures the relationship between node attributes and graph structure during the generative process. Comprehensive experiments on the billion-scale Open Academic Graph and Amazon recommendation data show that GPT-GNN significantly outperforms state-of-the-art GNN models without pre-training by up to 9.1% in various downstream tasks.",1
"Although graph neural networks (GNNs) have made great progress recently on learning from graph-structured data in practice, their theoretical guarantee on generalizability remains elusive in the literature. In this paper, we provide a theoretically-grounded generalizability analysis of GNNs with one hidden layer for both regression and binary classification problems. Under the assumption that there exists a ground-truth GNN model (with zero generalization error), the objective of GNN learning is to estimate the ground-truth GNN parameters from the training data. To achieve this objective, we propose a learning algorithm that is built on tensor initialization and accelerated gradient descent. We then show that the proposed learning algorithm converges to the ground-truth GNN model for the regression problem, and to a model sufficiently close to the ground-truth for the binary classification problem. Moreover, for both cases, the convergence rate of the proposed learning algorithm is proven to be linear and faster than the vanilla gradient descent algorithm. We further explore the relationship between the sample complexity of GNNs and their underlying graph properties. Lastly, we provide numerical experiments to demonstrate the validity of our analysis and the effectiveness of the proposed learning algorithm for GNNs.",0
"Despite the recent progress of graph neural networks (GNNs) in learning from graph-structured data, there is a lack of theoretical guarantee on their ability to generalize. In this study, we aim to provide a generalizability analysis of GNNs with one hidden layer for regression and binary classification tasks. Our approach assumes the existence of a ground-truth GNN model with zero generalization error and proposes a learning algorithm based on tensor initialization and accelerated gradient descent to estimate the ground-truth GNN parameters from training data. We demonstrate that the proposed algorithm converges to the ground-truth GNN model for regression and a model close to the ground-truth for binary classification, with a faster convergence rate than the vanilla gradient descent algorithm. We also investigate the sample complexity of GNNs in relation to their underlying graph properties and present numerical experiments to validate our analysis and the effectiveness of our learning algorithm for GNNs.",1
"Graphs are ubiquitous in modelling relational structures. Recent endeavours in machine learning for graph-structured data have led to many architectures and learning algorithms. However, the graph used by these algorithms is often constructed based on inaccurate modelling assumptions and/or noisy data. As a result, it fails to represent the true relationships between nodes. A Bayesian framework which targets posterior inference of the graph by considering it as a random quantity can be beneficial. In this paper, we propose a novel non-parametric graph model for constructing the posterior distribution of graph adjacency matrices. The proposed model is flexible in the sense that it can effectively take into account the output of graph-based learning algorithms that target specific tasks. In addition, model inference scales well to large graphs. We demonstrate the advantages of this model in three different problem settings: node classification, link prediction and recommendation.",0
"The use of graphs to model relational structures is widespread. The development of machine learning for graph-structured data has resulted in numerous architectures and learning algorithms. However, these algorithms often rely on inaccurate assumptions and/or noisy data, causing them to fail in accurately representing the relationships between nodes. A Bayesian framework that treats the graph as a random quantity and targets posterior inference can prove beneficial. This paper proposes a new non-parametric graph model for constructing the posterior distribution of graph adjacency matrices. The model is adaptable, as it can effectively incorporate the output of graph-based learning algorithms that focus on specific tasks. Furthermore, model inference is suitable for large graphs. The advantages of this model are demonstrated in three problem settings: node classification, link prediction, and recommendation.",1
"Graph Neural Networks (GNNs) have achieved a lot of success on graph-structured data. However, it is observed that the performance of graph neural networks does not improve as the number of layers increases. This effect, known as over-smoothing, has been analyzed mostly in linear cases. In this paper, we build upon previous results \cite{oono2019graph} to further analyze the over-smoothing effect in the general graph neural network architecture. We show when the weight matrix satisfies the conditions determined by the spectrum of augmented normalized Laplacian, the Dirichlet energy of embeddings will converge to zero, resulting in the loss of discriminative power. Using Dirichlet energy to measure ""expressiveness"" of embedding is conceptually clean; it leads to simpler proofs than \cite{oono2019graph} and can handle more non-linearities.",0
"Graph Neural Networks (GNNs) have been successful in dealing with graph-structured data. However, it has been observed that the performance of GNNs does not improve with an increase in the number of layers, a phenomenon known as over-smoothing. Although this effect has been mostly analyzed in linear cases, this paper builds on previous research \cite{oono2019graph} to further investigate over-smoothing in the general GNN architecture. By satisfying the conditions determined by the spectrum of an augmented normalized Laplacian weight matrix, we demonstrate that the Dirichlet energy of embeddings converges to zero, leading to a loss of discriminative power. The use of Dirichlet energy to measure the ""expressiveness"" of embedding simplifies the proofs and can handle more non-linearities than previous research \cite{oono2019graph}.",1
"Graph classification aims to extract accurate information from graph-structured data for classification and is becoming more and more important in graph learning community. Although Graph Neural Networks (GNNs) have been successfully applied to graph classification tasks, most of them overlook the scarcity of labeled graph data in many applications. For example, in bioinformatics, obtaining protein graph labels usually needs laborious experiments. Recently, few-shot learning has been explored to alleviate this problem with only given a few labeled graph samples of test classes. The shared sub-structures between training classes and test classes are essential in few-shot graph classification. Exiting methods assume that the test classes belong to the same set of super-classes clustered from training classes. However, according to our observations, the label spaces of training classes and test classes usually do not overlap in real-world scenario. As a result, the existing methods don't well capture the local structures of unseen test classes. To overcome the limitation, in this paper, we propose a direct method to capture the sub-structures with well initialized meta-learner within a few adaptation steps. More specifically, (1) we propose a novel framework consisting of a graph meta-learner, which uses GNNs based modules for fast adaptation on graph data, and a step controller for the robustness and generalization of meta-learner; (2) we provide quantitative analysis for the framework and give a graph-dependent upper bound of the generalization error based on our framework; (3) the extensive experiments on real-world datasets demonstrate that our framework gets state-of-the-art results on several few-shot graph classification tasks compared to baselines.",0
"The objective of graph classification is to accurately extract information from data structured as graphs for classification purposes. This field is increasingly significant in the graph learning community. Although Graph Neural Networks (GNNs) have achieved success in graph classification tasks, they tend to ignore the shortage of labeled graph data in many applications. For instance, in bioinformatics, obtaining protein graph labels requires extensive experimental work. Recently, few-shot learning has emerged as a solution to this problem, using only a few labeled graph samples of test classes. In this approach, shared sub-structures between training and test classes play a crucial role in few-shot graph classification. Existing methods assume that the test classes belong to the same set of super-classes clustered from training classes. However, in real-world scenarios, the label spaces of training and test classes usually do not overlap. Consequently, the existing methods fail to capture the local structures of unseen test classes. To overcome this limitation, this paper proposes a direct method to capture sub-structures using a well-initialized meta-learner within a few adaptation steps. The proposed approach consists of a novel framework comprising a graph meta-learner that leverages GNN-based modules for fast adaptation on graph data, and a step controller to enhance the meta-learner's robustness and generalization. Additionally, the paper provides quantitative analysis for the framework and presents a graph-dependent upper bound of the generalization error based on their approach. The extensive experiments conducted on real-world datasets demonstrate that the proposed framework outperforms baselines and achieves state-of-the-art results on several few-shot graph classification tasks.",1
"The success of deep learning has revolutionized many fields of research including areas of computer vision, text and speech processing. Enormous research efforts have led to numerous methods that are capable of efficiently analyzing data, especially in the Euclidean space. However, many problems are posed in non-Euclidean domains modeled as general graphs with complex connection patterns. Increased problem complexity and computational power constraints have limited early approaches to static and small-sized graphs. In recent years, a rising interest in machine learning on graph-structured data has been accompanied by improved methods that overcome the limitations of their predecessors. These methods paved the way for dealing with large-scale and time-dynamic graphs. This work aims to provide an overview of early and modern graph neural network based machine learning methods for node-level prediction tasks. Under the umbrella of taxonomies already established in the literature, we explain the core concepts and provide detailed explanations for convolutional methods that have had strong impact. In addition, we introduce common benchmarks and present selected applications from various areas. Finally, we discuss open problems for further research.",0
"The success of deep learning has brought about a revolution in various research fields, such as computer vision, text, and speech processing. Many research efforts have been dedicated to developing efficient data analysis methods, particularly in the Euclidean space. However, non-Euclidean domains, which are modeled as complex graphs with intricate connections, pose many challenges. Early approaches were limited by computational power constraints and static or small-sized graphs. Recently, there has been an increased interest in machine learning on graph-structured data, which has led to improved methods capable of dealing with large-scale and time-dynamic graphs. This article provides an overview of graph neural network-based machine learning methods for node-level prediction tasks, covering both early and modern methods. We explain the core concepts, provide detailed explanations for convolutional methods, and introduce common benchmarks and selected applications from various areas. Lastly, we discuss open problems for further research.",1
"We consider the problem of whether a given decision model, working with structured data, has individual fairness. Following the work of Dwork, a model is individually biased (or unfair) if there is a pair of valid inputs which are close to each other (according to an appropriate metric) but are treated differently by the model (different class label, or large difference in output), and it is unbiased (or fair) if no such pair exists. Our objective is to construct verifiers for proving individual fairness of a given model, and we do so by considering appropriate relaxations of the problem. We construct verifiers which are sound but not complete for linear classifiers, and kernelized polynomial/radial basis function classifiers. We also report the experimental results of evaluating our proposed algorithms on publicly available datasets.",0
"The problem at hand is determining whether a decision model that operates on structured data is individually fair. According to Dwork's research, a model is considered biased or unfair if two valid inputs that are similar to each other (based on a relevant metric) are treated differently by the model, resulting in varied class labels or significant output differences. Conversely, if no such pairs exist, the model is considered unbiased or fair. Our goal is to create verifiers that can demonstrate the individual fairness of a given model by approaching the problem with appropriate modifications. We have successfully developed sound verifiers for linear classifiers, as well as kernelized polynomial/radial basis function classifiers, that are not complete. Additionally, we have conducted experiments on publicly available datasets to evaluate the effectiveness of our proposed algorithms.",1
"Most machine learning models for structured data encode the structural knowledge of a node by leveraging simple aggregation functions (in neural models, typically a weighted sum) of the information in the node's neighbourhood. Nevertheless, the choice of simple context aggregation functions, such as the sum, can be widely sub-optimal. In this work we introduce a general approach to model aggregation of structural context leveraging a tensor-based formulation. We show how the exponential growth in the size of the parameter space can be controlled through an approximation based on the Tucker tensor decomposition. This approximation allows limiting the parameters space size, decoupling it from its strict relation with the size of the hidden encoding space. By this means, we can effectively regulate the trade-off between expressivity of the encoding, controlled by the hidden size, computational complexity and model generalisation, influenced by parameterisation. Finally, we introduce a new Tensorial Tree-LSTM derived as an instance of our framework and we use it to experimentally assess our working hypotheses on tree classification scenarios.",0
"Most machine learning models that deal with structured data utilize basic aggregation functions (such as a weighted sum in neural models) to encode a node's structural knowledge based on the information in its surrounding area. However, using simple context aggregation functions like the sum can often be less than optimal. In this study, we present a universal method for aggregating structural context using a tensor-based formulation. We demonstrate how the exponential increase in the parameter space can be managed by approximating it with the Tucker tensor decomposition. This approximation limits the size of the parameter space, freeing it from its strict relationship with the hidden encoding space size. Thus, we can regulate the trade-off between the encoding's expressivity (controlled by the hidden size), computational complexity, and model generalisation (influenced by parameterisation). Lastly, we introduce a new Tensorial Tree-LSTM derived from our framework and use it to test our hypotheses on tree classification scenarios.",1
"Graph deep learning has recently emerged as a powerful ML concept allowing to generalize successful deep neural architectures to non-Euclidean structured data. Such methods have shown promising results on a broad spectrum of applications ranging from social science, biomedicine, and particle physics to computer vision, graphics, and chemistry. One of the limitations of the majority of the current graph neural network architectures is that they are often restricted to the transductive setting and rely on the assumption that the underlying graph is known and fixed. In many settings, such as those arising in medical and healthcare applications, this assumption is not necessarily true since the graph may be noisy, partially- or even completely unknown, and one is thus interested in inferring it from the data. This is especially important in inductive settings when dealing with nodes not present in the graph at training time. Furthermore, sometimes such a graph itself may convey insights that are even more important than the downstream task. In this paper, we introduce Differentiable Graph Module (DGM), a learnable function predicting the edge probability in the graph relevant for the task, that can be combined with convolutional graph neural network layers and trained in an end-to-end fashion. We provide an extensive evaluation of applications from the domains of healthcare (disease prediction), brain imaging (gender and age prediction), computer graphics (3D point cloud segmentation), and computer vision (zero-shot learning). We show that our model provides a significant improvement over baselines both in transductive and inductive settings and achieves state-of-the-art results.",0
"Recently, graph deep learning has emerged as a powerful machine learning concept that can generalize successful deep neural architectures to non-Euclidean structured data. This method has shown promising results in a wide range of applications, including social science, biomedicine, particle physics, computer vision, graphics, and chemistry. However, most current graph neural network architectures have limitations, as they are usually restricted to the transductive setting and depend on the assumption that the underlying graph is known and fixed. In many cases, such as in medical and healthcare applications, this assumption may not be true, and the graph may be noisy, partially, or even completely unknown. Therefore, it is essential to infer the graph from the data, especially in inductive settings when dealing with nodes not present in the graph at training time. Furthermore, the graph itself may convey insights that are even more important than the downstream task. In this paper, we introduce the Differentiable Graph Module (DGM), a learnable function for predicting the edge probability in the graph relevant to the task. It can be combined with convolutional graph neural network layers and trained in an end-to-end fashion. Our evaluation includes applications from healthcare (disease prediction), brain imaging (gender and age prediction), computer graphics (3D point cloud segmentation), and computer vision (zero-shot learning). The results show that our model provides a significant improvement over baselines in both transductive and inductive settings, achieving state-of-the-art results.",1
"Applying machine learning algorithms to private data, such as financial or medical data, while preserving their confidentiality, is a difficult task. Homomorphic Encryption (HE) is acknowledged for its ability to allow computation on encrypted data, where both the input and output are encrypted, which therefore enables secure inference on private data. Nonetheless, because of the constraints of HE, such as its inability to evaluate non-polynomial functions or to perform arbitrary matrix multiplication efficiently, only inference of linear models seem usable in practice in the HE paradigm so far.   In this paper, we propose Cryptotree, a framework that enables the use of Random Forests (RF), a very powerful learning procedure compared to linear regression, in the context of HE. To this aim, we first convert a regular RF to a Neural RF, then adapt this to fit the HE scheme CKKS, which allows HE operations on real values. Through SIMD operations, we are able to have quick inference and prediction results better than the original RF on encrypted data.",0
"The task of applying machine learning algorithms to sensitive data, such as financial or medical data, while maintaining confidentiality, is challenging. Homomorphic Encryption (HE) is recognized for its ability to conduct computations on encrypted data, where both the input and output are encrypted, enabling secure inference on private data. However, HE has limitations, such as its inability to evaluate non-polynomial functions or perform arbitrary matrix multiplication efficiently, limiting its practical use to linear models. This paper introduces Cryptotree, a framework that enables the use of Random Forests (RF), a powerful learning procedure compared to linear regression, within HE. We accomplish this by converting a regular RF to a Neural RF and adapting it to fit the HE scheme CKKS, which enables HE operations on real values. Using SIMD operations, we achieve quick inference and improve prediction results compared to the original RF on encrypted data.",1
"Graph Attention Network (GAT) and GraphSAGE are neural network architectures that operate on graph-structured data and have been widely studied for link prediction and node classification. One challenge raised by GraphSAGE is how to smartly combine neighbour features based on graph structure. GAT handles this problem through attention, however the challenge with GAT is its scalability over large and dense graphs. In this work, we proposed a new architecture to address these issues that is more efficient and is capable of incorporating different edge type information. It generates node representations by attending to neighbours sampled from weighted multi-step transition probabilities. We conduct experiments on both transductive and inductive settings. Experiments achieved comparable or better results on several graph benchmarks, including the Cora, Citeseer, Pubmed, PPI, Twitter, and YouTube datasets.",0
"Neural networks like Graph Attention Network (GAT) and GraphSAGE have been extensively researched for their ability to operate on graph-structured data and perform functions like link prediction and node classification. One of the challenges faced by GraphSAGE is how to effectively combine neighbour features based on graph structure. GAT solves this issue by using attention, but it struggles with scaling over large and dense graphs. This research proposes a more efficient architecture that can handle different edge types and generate node representations by attending to neighbours sampled from weighted multi-step transition probabilities. Experiments conducted on transductive and inductive settings show that this architecture achieves comparable or better results on various graph benchmarks, including the Cora, Citeseer, Pubmed, PPI, Twitter, and YouTube datasets.",1
"Nowadays, graph-structured data are increasingly used to model complex systems. Meanwhile, detecting anomalies from graph has become a vital research problem of pressing societal concerns. Anomaly detection is an unsupervised learning task of identifying rare data that differ from the majority. As one of the dominant anomaly detection algorithms, One Class Support Vector Machine has been widely used to detect outliers. However, those traditional anomaly detection methods lost their effectiveness in graph data. Since traditional anomaly detection methods are stable, robust and easy to use, it is vitally important to generalize them to graph data. In this work, we propose One Class Graph Neural Network (OCGNN), a one-class classification framework for graph anomaly detection. OCGNN is designed to combine the powerful representation ability of Graph Neural Networks along with the classical one-class objective. Compared with other baselines, OCGNN achieves significant improvements in extensive experiments.",0
"Graph-structured data are becoming increasingly popular for modeling intricate systems, and the identification of anomalies in graphs has become a crucial research problem. Anomaly detection involves the unsupervised learning task of recognizing rare data that deviates from the norm. One Class Support Vector Machine is a widely used algorithm for detecting outliers, but it has proven ineffective when applied to graph data. Given the stability, robustness, and ease of use of traditional anomaly detection methods, it is important to extend them to graph data. This paper introduces One Class Graph Neural Network (OCGNN), a one-class classification framework for graph anomaly detection that combines the powerful representation capabilities of Graph Neural Networks with the classical one-class objective. OCGNN outperforms other baselines in extensive experiments.",1
"Buying a home is one of the most important buying decisions people have to make in their life. The latest research on real-estate appraisal focuses on incorporating image data in addition to structured data into the modeling process. This research measures the prediction performance of satellite images and structured data by using convolutional neural networks. The resulting CNN model trained performs 7% better in MAE than the advanced baseline of a neural network trained on structured data. Moreover, sliding-window heatmap provides visual interpretability of satellite images, revealing that neighborhood structures are essential in the price estimation.",0
"The choice to purchase a home is a significant decision people make. Recently, real estate appraisal research has concentrated on including image data with structured data in modeling. Convolutional neural networks are used to assess the forecast accuracy of satellite images and structured data. The CNN model trained produced a 7% improvement in MAE compared to the advanced neural network trained solely on structured data. Additionally, a sliding-window heatmap offers visual insight into satellite images, demonstrating that neighborhood structures are crucial in price estimation.",1
"Graph Convolutional Networks (GCNs) have been successfully applied to analyze non-grid data, where the classical convolutional neural networks (CNNs) cannot be directly used. One similarity shared by GCNs and CNNs is the requirement of massive amount of labeled data for network training. In addition, GCNs need the adjacency matrix as input to define the relationship between those non-grid data, which leads to all of data including training, validation and test data typically forms only one graph structures data for training. Furthermore, the adjacency matrix is usually pre-defined and stationary, which makes the data augmentation strategies cannot be employed on the constructed graph structures data to augment the amount of training data. To further improve the learning capacity and model performance under the limited training data, in this paper, we propose two types of self-supervised learning strategies to exploit available information from the input graph structure data itself. Our proposed self-supervised learning strategies are examined on two representative GCN models with three public citation network datasets - Citeseer, Cora and Pubmed. The experimental results demonstrate the generalization ability as well as the portability of our proposed strategies, which can significantly improve the performance of GCNs with the power of self-supervised learning in improving feature learning.",0
"Graph Convolutional Networks (GCNs) have proven to be effective in analyzing non-grid data, which cannot be directly processed by classical Convolutional Neural Networks (CNNs). Both GCNs and CNNs require large amounts of labeled data for network training. GCNs need the adjacency matrix to define the relationship between non-grid data, resulting in a single graph structure comprising all data for training, validation, and testing. However, the pre-defined and stationary adjacency matrix makes it impossible to apply data augmentation strategies to augment the amount of training data. To address this limitation and enhance model performance with limited training data, this paper proposes two self-supervised learning strategies to utilize information from the input graph structure data. The proposed strategies are evaluated on two typical GCN models using three public citation network datasets, Citeseer, Cora, and Pubmed, with experimental results demonstrating their potential to improve feature learning and GCN performance.",1
"We present Graph Random Neural Features (GRNF), a novel embedding method from graph-structured data to real vectors based on a family of graph neural networks. The embedding naturally deals with graph isomorphism and preserves the metric structure of the graph domain, in probability. In addition to being an explicit embedding method, it also allows us to efficiently and effectively approximate graph metric distances (as well as complete kernel functions); a criterion to select the embedding dimension trading off the approximation accuracy with the computational cost is also provided. GRNF can be used within traditional processing methods or as a training-free input layer of a graph neural network. The theoretical guarantees that accompany GRNF ensure that the considered graph distance is metric, hence allowing to distinguish any pair of non-isomorphic graphs.",0
"Introducing Graph Random Neural Features (GRNF), a fresh approach to converting graph-structured data into real vectors using a range of graph neural networks. This method naturally handles graph isomorphism and maintains the metric structure of the graph domain with a high degree of probability. In addition to its explicit embedding capabilities, GRNF efficiently and effectively approximates graph metric distances, including complete kernel functions. It also provides a dimension selection criterion that balances approximation accuracy with computational cost. GRNF is suitable for use in conventional processing techniques or as a training-free input layer in a graph neural network. The theoretical guarantees associated with GRNF ensure that the graph distance under consideration is metric, making it possible to differentiate between any pair of non-isomorphic graphs.",1
"Variational Autoencoders (VAEs) provide a theoretically-backed and popular framework for deep generative models. However, learning a VAE from data poses still unanswered theoretical questions and considerable practical challenges. In this work, we propose an alternative framework for generative modeling that is simpler, easier to train, and deterministic, yet has many of the advantages of VAEs. We observe that sampling a stochastic encoder in a Gaussian VAE can be interpreted as simply injecting noise into the input of a deterministic decoder. We investigate how substituting this kind of stochasticity, with other explicit and implicit regularization schemes, can lead to an equally smooth and meaningful latent space without forcing it to conform to an arbitrarily chosen prior. To retrieve a generative mechanism to sample new data, we introduce an ex-post density estimation step that can be readily applied also to existing VAEs, improving their sample quality. We show, in a rigorous empirical study, that the proposed regularized deterministic autoencoders are able to generate samples that are comparable to, or better than, those of VAEs and more powerful alternatives when applied to images as well as to structured data such as molecules. \footnote{An implementation is available at: \url{https://github.com/ParthaEth/Regularized_autoencoders-RAE-}}",0
"VAEs have gained popularity as a deep generative model framework, despite the theoretical and practical difficulties associated with their implementation. In this study, we present a simpler and deterministic alternative for generative modeling that retains many of the benefits of VAEs. By replacing the stochasticity of a Gaussian VAE with other explicit and implicit regularization methods, we can achieve a smooth and meaningful latent space without relying on an arbitrary prior. Furthermore, we introduce an ex-post density estimation step to enhance the sample quality of both our proposed model and existing VAEs. Our empirical study demonstrates that our regularized deterministic autoencoders can generate high-quality samples for both images and structured data, surpassing VAEs and other models. To access our implementation, visit: \url{https://github.com/ParthaEth/Regularized_autoencoders-RAE-}.",1
"Graph classification is an important task on graph-structured data with many real-world applications. The goal of graph classification task is to train a classifier using a set of training graphs. Recently, Graph Neural Networks (GNNs) have greatly advanced the task of graph classification. When building a GNN model for graph classification, the graphs in the training set are usually assumed to be identically distributed. However, in many real-world applications, graphs in the same dataset could have dramatically different structures, which indicates that these graphs are likely non-identically distributed. Therefore, in this paper, we aim to develop graph neural networks for graphs that are not non-identically distributed. Specifically, we propose a general non-IID graph neural network framework, i.e., Non-IID-GNN. Given a graph, Non-IID-GNN can adapt any existing graph neural network model to generate a sample-specific model for this graph. Comprehensive experiments on various graph classification benchmarks demonstrate the effectiveness of the proposed framework. We will release the code of the proposed framework upon the acceptance of the paper.",0
"The classification of graphs is a crucial task with many practical applications. It involves training a classifier using a set of training graphs, and Graph Neural Networks (GNNs) have greatly improved this task. However, when creating a GNN model for graph classification, it is typically assumed that the graphs in the training set are identically distributed. This assumption is not realistic in many real-world applications since the graphs could have substantially different structures. Therefore, this paper aims to develop a non-IID graph neural network framework, known as Non-IID-GNN, that can handle non-identically distributed graphs. This framework allows any existing graph neural network model to adapt to generate a sample-specific model for each graph. The proposed framework is effective, as demonstrated by extensive experiments on various graph classification benchmarks. The code for this framework will be made available upon paper acceptance.",1
"Graph Neural Networks (GNNs) are efficient approaches to process graph-structured data. Modelling long-distance node relations is essential for GNN training and applications. However, conventional GNNs suffer from bad performance in modelling long-distance node relations due to limited-layer information propagation. Existing studies focus on building deep GNN architectures, which face the over-smoothing issue and cannot model node relations in particularly long distance. To address this issue, we propose to model long-distance node relations by simply relying on shallow GNN architectures with two solutions: (1) Implicitly modelling by learning to predict node pair relations (2) Explicitly modelling by adding edges between nodes that potentially have the same label. To combine our two solutions, we propose a model-agnostic training framework named HighwayGraph, which overcomes the challenge of insufficient labeled nodes by sampling node pairs from the training set and adopting the self-training method. Extensive experimental results show that our HighwayGraph achieves consistent and significant improvements over four representative GNNs on three benchmark datasets.",0
"GNNs are effective in handling data in graph structure. For successful GNN training and application, it is crucial to model node relations over long distances. However, traditional GNNs struggle with this task due to limited-layer information propagation. Existing studies focus on building deep GNN architectures, but this approach results in over-smoothing and cannot model node relations over long distances effectively. To tackle this issue, we propose two solutions for modelling long-distance node relations using shallow GNN architectures: (1) Implicit modelling by learning to predict node pair relations, and (2) Explicit modelling by adding edges between nodes that may have the same label. We combine these solutions through our model-agnostic training framework named HighwayGraph, which addresses the issue of insufficient labeled nodes by sampling node pairs from the training set and adopting the self-training method. Comprehensive experimental results demonstrate that our HighwayGraph outperforms four representative GNNs on three benchmark datasets with consistent and significant improvements.",1
"Learning on graph structured data has drawn increasing interest in recent years. Frameworks like Graph Convolutional Networks (GCNs) have demonstrated their ability to capture structural information and obtain good performance in various tasks. In these frameworks, node aggregation schemes are typically used to capture structural information: a node's feature vector is recursively computed by aggregating features of its neighboring nodes. However, most of aggregation schemes treat all connections in a graph equally, ignoring node feature similarities. In this paper, we re-interpret node aggregation from the perspective of kernel weighting, and present a framework to consider feature similarity in an aggregation scheme. Specifically, we show that normalized adjacency matrix is equivalent to a neighbor-based kernel matrix in a Krein Space. We then propose feature aggregation as the composition of the original neighbor-based kernel and a learnable kernel to encode feature similarities in a feature space. We further show how the proposed method can be extended to Graph Attention Network (GAT). Experimental results demonstrate better performance of our proposed framework in several real-world applications.",0
"In recent years, there has been a growing interest in learning on data structured as graphs. Graph Convolutional Networks (GCNs) have proven to be effective in capturing structural information and achieving good results in various tasks. These frameworks typically use node aggregation schemes to capture structural information, whereby a node's feature vector is recursively computed by aggregating features of its neighboring nodes. However, most existing aggregation schemes treat all connections in a graph equally, without considering node feature similarities. This paper proposes a new framework for node aggregation that takes into account feature similarity by re-interpreting it from the perspective of kernel weighting. The proposed method involves using a neighbor-based kernel matrix that is equivalent to the normalized adjacency matrix in a Krein Space. A learnable kernel is then added to encode feature similarities in a feature space, resulting in a feature aggregation composition. The proposed framework can also be extended to Graph Attention Network (GAT). Experimental results show that our proposed method outperforms existing methods in several real-world applications.",1
"Graph Neural Networks (GNNs) are powerful to learn the representation of graph-structured data. Most of the GNNs use the message-passing scheme, where the embedding of a node is iteratively updated by aggregating the information of its neighbors. To achieve a better expressive capability of node influences, attention mechanism has grown to be popular to assign trainable weights to the nodes in aggregation. Though the attention-based GNNs have achieved remarkable results in various tasks, a clear understanding of their discriminative capacities is missing. In this work, we present a theoretical analysis of the representational properties of the GNN that adopts the attention mechanism as an aggregator. Our analysis determines all cases when those attention-based GNNs can always fail to distinguish certain distinct structures. Those cases appear due to the ignorance of cardinality information in attention-based aggregation. To improve the performance of attention-based GNNs, we propose cardinality preserved attention (CPA) models that can be applied to any kind of attention mechanisms. Our experiments on node and graph classification confirm our theoretical analysis and show the competitive performance of our CPA models.",0
"Graph Neural Networks (GNNs) have the ability to effectively learn and represent graph-structured data. The majority of GNNs use a message-passing approach to iteratively update node embeddings by aggregating information from neighbors. To enhance the expressive power of node influences, attention mechanisms have become a popular method for assigning trainable weights to nodes during aggregation. Despite the impressive results achieved by attention-based GNNs in various tasks, their discriminative capacities are not yet fully understood. This study presents a theoretical analysis of attention-based GNNs' representational properties and identifies scenarios where they may fail to distinguish different structures due to the lack of cardinality information in attention-based aggregation. To address this issue, we propose cardinality preserved attention (CPA) models that can be applied to any type of attention mechanism. Our experiments on node and graph classification support our theoretical analysis and demonstrate the competitive performance of our CPA models.",1
"Graph Convolutional Networks (GCNs) have been widely used due to their outstanding performance in processing graph-structured data. However, the undirected graphs limit their application scope. In this paper, we extend spectral-based graph convolution to directed graphs by using first- and second-order proximity, which can not only retain the connection properties of the directed graph, but also expand the receptive field of the convolution operation. A new GCN model, called DGCN, is then designed to learn representations on the directed graph, leveraging both the first- and second-order proximity information. We empirically show the fact that GCNs working only with DGCNs can encode more useful information from graph and help achieve better performance when generalized to other models. Moreover, extensive experiments on citation networks and co-purchase datasets demonstrate the superiority of our model against the state-of-the-art methods.",0
"The exceptional performance of Graph Convolutional Networks (GCNs) in processing graph-structured data has made them widely used. However, their application scope is limited due to the restriction of undirected graphs. This paper proposes a method to extend spectral-based graph convolution to directed graphs by utilizing first- and second-order proximity. This enables the retention of connection properties of the directed graph and expand the receptive field of the convolution operation. The proposed DGCN model leverages both first- and second-order proximity information to learn representations on the directed graph. We demonstrate through empirical analysis that GCNs working with DGCNs can encode more useful information from graphs and achieve better performance when generalized to other models. Additionally, extensive experiments on citation networks and co-purchase datasets prove the superiority of our model over state-of-the-art methods.",1
"Representation learning over graph structured data has been mostly studied in static graph settings while efforts for modeling dynamic graphs are still scant. In this paper, we develop a novel hierarchical variational model that introduces additional latent random variables to jointly model the hidden states of a graph recurrent neural network (GRNN) to capture both topology and node attribute changes in dynamic graphs. We argue that the use of high-level latent random variables in this variational GRNN (VGRNN) can better capture potential variability observed in dynamic graphs as well as the uncertainty of node latent representation. With semi-implicit variational inference developed for this new VGRNN architecture (SI-VGRNN), we show that flexible non-Gaussian latent representations can further help dynamic graph analytic tasks. Our experiments with multiple real-world dynamic graph datasets demonstrate that SI-VGRNN and VGRNN consistently outperform the existing baseline and state-of-the-art methods by a significant margin in dynamic link prediction.",0
"The study of representation learning for graph structured data has mainly focused on static graph settings, and there has been little research on modeling dynamic graphs. This paper presents a new hierarchical variational model that uses additional latent random variables to jointly model the hidden states of a graph recurrent neural network (GRNN). This approach captures both topology and node attribute changes in dynamic graphs. The authors argue that the use of high-level latent random variables in this variational GRNN (VGRNN) is better suited to capture potential variability observed in dynamic graphs and the uncertainty of node latent representation. The authors also introduce a semi-implicit variational inference technique for the new VGRNN architecture (SI-VGRNN) that utilizes flexible non-Gaussian latent representations to enhance dynamic graph analytic tasks. The experiments conducted on multiple real-world dynamic graph datasets demonstrate that SI-VGRNN and VGRNN consistently outperform existing baseline and state-of-the-art methods by a significant margin in dynamic link prediction.",1
"Automated anatomical labeling plays a vital role in coronary artery disease diagnosing procedure. The main challenge in this problem is the large individual variability inherited in human anatomy. Existing methods usually rely on the position information and the prior knowledge of the topology of the coronary artery tree, which may lead to unsatisfactory performance when the main branches are confusing. Motivated by the wide application of the graph neural network in structured data, in this paper, we propose a conditional partial-residual graph convolutional network (CPR-GCN), which takes both position and CT image into consideration, since CT image contains abundant information such as branch size and spanning direction. Two majority parts, a Partial-Residual GCN and a conditions extractor, are included in CPR-GCN. The conditions extractor is a hybrid model containing the 3D CNN and the LSTM, which can extract 3D spatial image features along the branches. On the technical side, the Partial-Residual GCN takes the position features of the branches, with the 3D spatial image features as conditions, to predict the label for each branches. While on the mathematical side, our approach twists the partial differential equation (PDE) into the graph modeling. A dataset with 511 subjects is collected from the clinic and annotated by two experts with a two-phase annotation process. According to the five-fold cross-validation, our CPR-GCN yields 95.8% meanRecall, 95.4% meanPrecision and 0.955 meanF1, which outperforms state-of-the-art approaches.",0
"The labeling of anatomical structures is crucial in diagnosing coronary artery disease, but the variability in human anatomy poses a challenge. Current methods rely on position information and prior knowledge of the coronary artery tree, which can be problematic when the main branches are difficult to identify. To improve performance, we propose a new method called CPR-GCN that combines position and CT image data to extract branch size and spanning direction information. Our approach includes a Partial-Residual GCN and a conditions extractor, which uses a 3D CNN and LSTM to extract spatial image features. The Partial-Residual GCN predicts labels for each branch using position features and image conditions, and we use a graph modeling approach that twists the partial differential equation (PDE). Our dataset includes 511 subjects, and our CPR-GCN approach outperforms state-of-the-art methods with a meanRecall of 95.8%, meanPrecision of 95.4%, and meanF1 of 0.955, as determined by five-fold cross-validation.",1
"In this paper, we propose a dimensionality reduction method applied to tensor-structured data as a hidden layer (we call it TensorProjection Layer) in a convolutional neural network. Our proposed method transforms input tensors into ones with a smaller dimension by projection. The directions of projection are viewed as training parameters associated with our proposed layer and trained via a supervised learning criterion such as minimization of the cross-entropy loss function. We discuss the gradients of the loss function with respect to the parameters associated with our proposed layer. We also implement simple numerical experiments to evaluate the performance of the TensorProjection Layer.",0
"The TensorProjection Layer is introduced in this paper as a dimensionality reduction technique for tensor-structured data within a convolutional neural network. Input tensors are projected to smaller dimensions using our proposed method, where the directions of projection are considered as training parameters and optimized through cross-entropy loss function. Our paper also examines the gradients of the loss function in relation to the TensorProjection Layer parameters and presents numerical experiments to assess its effectiveness.",1
"We present a prior for manifold structured data, such as surfaces of 3D shapes, where deep neural networks are adopted to reconstruct a target shape using gradient descent starting from a random initialization. We show that surfaces generated this way are smooth, with limiting behavior characterized by Gaussian processes, and we mathematically derive such properties for fully-connected as well as convolutional networks. We demonstrate our method in a variety of manifold reconstruction applications, such as point cloud denoising and interpolation, achieving considerably better results against competitive baselines while requiring no training data. We also show that when training data is available, our method allows developing alternate parametrizations of surfaces under the framework of AtlasNet, leading to a compact network architecture and better reconstruction results on standard image to shape reconstruction benchmarks.",0
"Our approach involves using deep neural networks to reconstruct 3D shape surfaces in manifold structured data, such as those of shapes. The reconstruction is achieved through gradient descent, starting from a random initialization. Our research shows that the resulting surfaces are smooth, with Gaussian processes characterizing the limiting behavior. We have mathematically derived these properties for both convolutional and fully-connected networks. Our method is demonstrated in manifold reconstruction applications, such as point cloud denoising and interpolation, and we have achieved superior results compared to other baselines without requiring any training data. Moreover, when training data is available, our approach allows for the development of alternative parametrizations of surfaces using the AtlasNet framework. This leads to a more compact network architecture and better reconstruction outcomes on standard image to shape reconstruction benchmarks.",1
"Graph-structured data arise in many scenarios. A fundamental problem is to quantify the similarities of graphs for tasks such as classification. Graph kernels are positive-semidefinite functions that decompose graphs into substructures and compare them. One problem in the effective implementation of this idea is that the substructures are not independent, which leads to high-dimensional feature space. In addition, graph kernels cannot capture the high-order complex interactions between vertices. To mitigate these two problems, we propose a framework called DeepMap to learn deep representations for graph feature maps. The learnt deep representation for a graph is a dense and low-dimensional vector that captures complex high-order interactions in a vertex neighborhood. DeepMap extends Convolutional Neural Networks (CNNs) to arbitrary graphs by aligning vertices across graphs and building the receptive field for each vertex. We empirically validate DeepMap on various graph classification benchmarks and demonstrate that it achieves state-of-the-art performance.",0
"In various situations, data organized in a graph structure can be encountered. A crucial issue is to measure the similarities between graphs for tasks like classification. To do so, graph kernels are utilized, which are functions that decompose graphs into substructures and compare them. However, a challenge in the effective implementation of this approach is that substructures are not independent, causing a high-dimensional feature space. Additionally, graph kernels cannot account for complex high-order interactions between vertices. To address these two challenges, we propose DeepMap, a framework that generates deep representations for graph feature maps. The learned deep representation is a low-dimensional vector that captures complex high-order interactions in a vertex neighborhood. DeepMap expands Convolutional Neural Networks (CNNs) to any graph by aligning vertices across graphs and constructing a receptive field for each vertex. We validate the effectiveness of DeepMap on different graph classification benchmarks and show that it outperforms other methods.",1
"Object Cluster Hierarchies is a new variant of Hierarchical Cluster Analysis that gains interest in the field of Machine Learning. Being still at an early stage of development, the lack of tools for systematic analysis of Object Cluster Hierarchies inhibits its further improvement. In this paper we address this issue by proposing a generator of synthetic hierarchical data that can be used for benchmarking Object Cluster Hierarchy methods. The article presents a thorough empirical and theoretical analysis of the generator and provides guidance on how to control its parameters. Conducted experiments show the usefulness of the data generator that is capable of producing a wide range of differently structured data. Further, benchmarking datasets that mirror the most common types of hierarchies are generated and made available to the public, together with the developed generator (http://kio.pwr.edu.pl/?page\_id=396).",0
"A new form of Hierarchical Cluster Analysis, known as Object Cluster Hierarchies, is gaining attention in the Machine Learning field. However, due to being in its early stages of development, the lack of tools for systematic analysis of Object Cluster Hierarchies is hindering its progress. To address this issue, the authors propose a generator of synthetic hierarchical data that can be utilized for benchmarking Object Cluster Hierarchy techniques. The paper offers a comprehensive analysis of the generator and instructs on how to regulate its parameters. The experiments conducted demonstrate the usefulness of the data generator, which can produce diverse structured data. Additionally, benchmarking datasets that represent the most common types of hierarchies have been created and made available to the public, along with the developed generator (http://kio.pwr.edu.pl/?page\_id=396).",1
"Vector representations of graphs and relational structures, whether hand-crafted feature vectors or learned representations, enable us to apply standard data analysis and machine learning techniques to the structures. A wide range of methods for generating such embeddings have been studied in the machine learning and knowledge representation literature. However, vector embeddings have received relatively little attention from a theoretical point of view.   Starting with a survey of embedding techniques that have been used in practice, in this paper we propose two theoretical approaches that we see as central for understanding the foundations of vector embeddings. We draw connections between the various approaches and suggest directions for future research.",0
"Vector embeddings, which can be either manually created or learned, provide a means of implementing standard data analysis and machine learning methods on graphs and relational structures. The machine learning and knowledge representation fields have explored various methods for developing such embeddings, but the theoretical aspects of vector embeddings have not been extensively examined. This paper surveys practical embedding techniques and introduces two theoretical approaches that are essential for comprehending the basis of vector embeddings. Furthermore, we establish links between the different techniques and propose future research avenues.",1
"Over the past few years, we have seen fundamental breakthroughs in core problems in machine learning, largely driven by advances in deep neural networks. At the same time, the amount of data collected in a wide array of scientific domains is dramatically increasing in both size and complexity. Taken together, this suggests many exciting opportunities for deep learning applications in scientific settings. But a significant challenge to this is simply knowing where to start. The sheer breadth and diversity of different deep learning techniques makes it difficult to determine what scientific problems might be most amenable to these methods, or which specific combination of methods might offer the most promising first approach. In this survey, we focus on addressing this central issue, providing an overview of many widely used deep learning models, spanning visual, sequential and graph structured data, associated tasks and different training methods, along with techniques to use deep learning with less data and better interpret these complex models --- two central considerations for many scientific use cases. We also include overviews of the full design process, implementation tips, and links to a plethora of tutorials, research summaries and open-sourced deep learning pipelines and pretrained models, developed by the community. We hope that this survey will help accelerate the use of deep learning across different scientific domains.",0
"In recent years, deep neural networks have led to significant advancements in machine learning, particularly in solving core problems. Additionally, there has been a significant increase in the amount of data being collected across various scientific fields, which presents exciting opportunities for deep learning applications. However, the diversity and vast number of deep learning techniques available make it a challenge to determine the most suitable method for specific scientific problems. This survey aims to address this issue by providing an overview of popular deep learning models, training methods, and techniques for interpreting complex models, which are relevant to scientific use cases. Additionally, we offer implementation tips, links to tutorials, research summaries, and open-sourced deep learning pipelines and pretrained models developed by the community. Our hope is that this survey will accelerate the adoption of deep learning in various scientific domains.",1
"Technology and the fruition of cultural heritage are becoming increasingly more entwined, especially with the advent of smart audio guides, virtual and augmented reality, and interactive installations. Machine learning and computer vision are important components of this ongoing integration, enabling new interaction modalities between user and museum. Nonetheless, the most frequent way of interacting with paintings and statues still remains taking pictures. Yet images alone can only convey the aesthetics of the artwork, lacking is information which is often required to fully understand and appreciate it. Usually this additional knowledge comes both from the artwork itself (and therefore the image depicting it) and from an external source of knowledge, such as an information sheet. While the former can be inferred by computer vision algorithms, the latter needs more structured data to pair visual content with relevant information. Regardless of its source, this information still must be be effectively transmitted to the user. A popular emerging trend in computer vision is Visual Question Answering (VQA), in which users can interact with a neural network by posing questions in natural language and receiving answers about the visual content. We believe that this will be the evolution of smart audio guides for museum visits and simple image browsing on personal smartphones. This will turn the classic audio guide into a smart personal instructor with which the visitor can interact by asking for explanations focused on specific interests. The advantages are twofold: on the one hand the cognitive burden of the visitor will decrease, limiting the flow of information to what the user actually wants to hear; and on the other hand it proposes the most natural way of interacting with a guide, favoring engagement.",0
"The integration of technology and cultural heritage is becoming increasingly intertwined, especially with the development of smart audio guides, virtual and augmented reality, and interactive installations. This integration is heavily reliant on machine learning and computer vision, which enable new ways of interacting with museums. However, taking pictures remains the most common method of interacting with paintings and statues, although this method only conveys the aesthetics of the artwork. Additional knowledge is required to fully appreciate and understand the artwork, which is often obtained from an external source of information. While computer vision algorithms can infer some information, structured data is required to provide users with relevant information. Visual Question Answering (VQA) is an emerging trend in computer vision that allows users to interact with a neural network by asking natural language questions about visual content. We believe that VQA will be the future of smart audio guides, enabling visitors to interact with a smart personal instructor to ask for explanations focused on specific interests. This approach reduces cognitive burden and promotes engagement, creating a more natural way of interacting with a guide.",1
"We present a novel framework that can combine multi-domain learning (MDL), data imputation (DI) and multi-task learning (MTL) to improve performance for classification and regression tasks in different domains. The core of our method is an adversarial autoencoder that can: (1) learn to produce domain-invariant embeddings to reduce the difference between domains; (2) learn the data distribution for each domain and correctly perform data imputation on missing data. For MDL, we use the Maximum Mean Discrepancy (MMD) measure to align the domain distributions. For DI, we use an adversarial approach where a generator fill in information for missing data and a discriminator tries to distinguish between real and imputed values. Finally, using the universal feature representation in the embeddings, we train a classifier using MTL that given input from any domain, can predict labels for all domains. We demonstrate the superior performance of our approach compared to other state-of-art methods in three distinct settings, DG-DI in image recognition with unstructured data, MTL-DI in grade estimation with structured data and MDMTL-DI in a selection process using mixed data.",0
"Our study introduces a unique framework that merges multi-domain learning (MDL), data imputation (DI), and multi-task learning (MTL) to enhance classification and regression tasks in various domains. The crux of our approach is an adversarial autoencoder that can attain two objectives: (1) create domain-invariant embeddings to decrease the gap between domains; (2) understand the data distribution for each domain and accurately complete missing data. To align the domain distributions for MDL, we apply the Maximum Mean Discrepancy (MMD) measure. For DI, we use an adversarial technique where a generator fills in missing data, and a discriminator distinguishes between real and imputed values. Finally, using the universal feature representation in the embeddings, we employ MTL to train a classifier that can predict labels for all domains, given input from any domain. We prove the superiority of our method over other state-of-the-art techniques in three specific settings: DG-DI in image recognition with unstructured data, MTL-DI in grade estimation with structured data, and MDMTL-DI in a selection process using mixed data.",1
"We introduce AutoGluon-Tabular, an open-source AutoML framework that requires only a single line of Python to train highly accurate machine learning models on an unprocessed tabular dataset such as a CSV file. Unlike existing AutoML frameworks that primarily focus on model/hyperparameter selection, AutoGluon-Tabular succeeds by ensembling multiple models and stacking them in multiple layers. Experiments reveal that our multi-layer combination of many models offers better use of allocated training time than seeking out the best. A second contribution is an extensive evaluation of public and commercial AutoML platforms including TPOT, H2O, AutoWEKA, auto-sklearn, AutoGluon, and Google AutoML Tables. Tests on a suite of 50 classification and regression tasks from Kaggle and the OpenML AutoML Benchmark reveal that AutoGluon is faster, more robust, and much more accurate. We find that AutoGluon often even outperforms the best-in-hindsight combination of all of its competitors. In two popular Kaggle competitions, AutoGluon beat 99% of the participating data scientists after merely 4h of training on the raw data.",0
"AutoGluon-Tabular is an AutoML tool that can train highly accurate machine learning models on an unprocessed tabular dataset with just one line of Python. Unlike other AutoML frameworks that concentrate on model/hyperparameter selection, AutoGluon-Tabular employs an ensemble of multiple models stacked in many layers, which offers better utilization of training time. Furthermore, AutoGluon's effectiveness was evaluated against other public and commercial AutoML platforms, including TPOT, H2O, AutoWEKA, auto-sklearn, and Google AutoML Tables. AutoGluon outperformed its competitors in terms of speed, robustness, and accuracy, even surpassing the best-in-hindsight combination of all its competitors. In two popular Kaggle competitions, AutoGluon outperformed 99% of the participating data scientists after only 4 hours of training on raw data.",1
"Computing the similarity between two data points plays a vital role in many machine learning algorithms. Metric learning has the aim of learning a good metric automatically from data. Most existing studies on metric learning for tree-structured data have adopted the approach of learning the tree edit distance. However, the edit distance is not amenable for big data analysis because it incurs high computation cost. In this paper, we propose a new metric learning approach for tree-structured data with pq-grams. The pq-gram distance is a distance for ordered labeled trees, and has much lower computation cost than the tree edit distance. In order to perform metric learning based on pq-grams, we propose a new differentiable parameterized distance, weighted pq-gram distance. We also propose a way to learn the proposed distance based on Large Margin Nearest Neighbors (LMNN), which is a well-studied and practical metric learning scheme. We formulate the metric learning problem as an optimization problem and use the gradient descent technique to perform metric learning. We empirically show that the proposed approach not only achieves competitive results with the state-of-the-art edit distance-based methods in various classification problems, but also solves the classification problems much more rapidly than the edit distance-based methods.",0
"The computation of similarity between two data points is essential in several machine learning algorithms. Metric learning aims to automatically learn a good metric from data. Existing studies on metric learning for tree-structured data have mainly focused on learning the tree edit distance, which is not suitable for analyzing big data due to high computation costs. This paper introduces a new metric learning approach for tree-structured data using pq-grams, which have a lower computation cost than the tree edit distance. To achieve this, we propose a differentiable parameterized distance called the weighted pq-gram distance and a method to learn it based on Large Margin Nearest Neighbors (LMNN). We formulate the metric learning problem as an optimization problem and use gradient descent to perform metric learning. Empirical results show that our approach achieves competitive results with state-of-the-art edit distance-based methods in various classification problems while being much faster.",1
"In this paper, we study the problem of learning Graph Convolutional Networks (GCNs) for regression. Current architectures of GCNs are limited to the small receptive field of convolution filters and shared transformation matrix for each node. To address these limitations, we propose Semantic Graph Convolutional Networks (SemGCN), a novel neural network architecture that operates on regression tasks with graph-structured data. SemGCN learns to capture semantic information such as local and global node relationships, which is not explicitly represented in the graph. These semantic relationships can be learned through end-to-end training from the ground truth without additional supervision or hand-crafted rules. We further investigate applying SemGCN to 3D human pose regression. Our formulation is intuitive and sufficient since both 2D and 3D human poses can be represented as a structured graph encoding the relationships between joints in the skeleton of a human body. We carry out comprehensive studies to validate our method. The results prove that SemGCN outperforms state of the art while using 90% fewer parameters.",0
"The objective of this article is to examine the problem of acquiring knowledge about Graph Convolutional Networks (GCNs) for regression. The current designs of GCNs are constrained by the small receptive field of convolution filters and shared transformation matrix for each node. To overcome these constraints, we propose a new neural network architecture called Semantic Graph Convolutional Networks (SemGCN) that can be used for regression tasks with graph-structured data. SemGCN learns to comprehend semantic information such as local and global node relationships that are not explicitly represented in the graph. These semantic relationships can be learned through end-to-end training from the ground truth without additional supervision or hand-crafted rules. We also explore the possibility of using SemGCN for 3D human pose regression. Our formulation is simple and sufficient since both 2D and 3D human poses can be represented as a structured graph encoding the relationships between joints in the skeleton of a human body. We conduct extensive research to validate our approach. The results indicate that SemGCN outperforms state of the art while utilizing 90% fewer parameters.",1
"In view of the huge success of convolution neural networks (CNN) for image classification and object recognition, there have been attempts to generalize the method to general graph-structured data. One major direction is based on spectral graph theory and graph signal processing. In this paper, we study the problem from a completely different perspective, by introducing parallel flow decomposition of graphs. The essential idea is to decompose a graph into families of non-intersecting one dimensional (1D) paths, after which, we may apply a 1D CNN along each family of paths. We demonstrate that the our method, which we call GraphFlow, is able to transfer CNN architectures to general graphs. To show the effectiveness of our approach, we test our method on the classical MNIST dataset, synthetic datasets on network information propagation and a news article classification dataset.",0
"Due to the remarkable achievement of convolution neural networks (CNN) in image recognition and classification tasks, researchers have been exploring ways to extend this method to graphs with varied structures. One of the approaches is based on spectral graph theory and graph signal processing. However, this paper takes a different approach by proposing a parallel flow decomposition of graphs. The idea is to divide a graph into groups of one-dimensional (1D) paths that do not intersect, and then apply a 1D CNN to each set of paths. This method, named GraphFlow, can successfully transfer CNN architectures to general graphs. To demonstrate the effectiveness of this approach, we tested it on various datasets, including the classic MNIST dataset, synthetic datasets on network information propagation, and a news article classification dataset.",1
"Recent years have witnessed the emerging success of graph neural networks (GNNs) for modeling structured data. However, most GNNs are designed for homogeneous graphs, in which all nodes and edges belong to the same types, making them infeasible to represent heterogeneous structures. In this paper, we present the Heterogeneous Graph Transformer (HGT) architecture for modeling Web-scale heterogeneous graphs. To model heterogeneity, we design node- and edge-type dependent parameters to characterize the heterogeneous attention over each edge, empowering HGT to maintain dedicated representations for different types of nodes and edges. To handle dynamic heterogeneous graphs, we introduce the relative temporal encoding technique into HGT, which is able to capture the dynamic structural dependency with arbitrary durations. To handle Web-scale graph data, we design the heterogeneous mini-batch graph sampling algorithm---HGSampling---for efficient and scalable training. Extensive experiments on the Open Academic Graph of 179 million nodes and 2 billion edges show that the proposed HGT model consistently outperforms all the state-of-the-art GNN baselines by 9%--21% on various downstream tasks.",0
"Graph neural networks (GNNs) have gained traction in recent years for their ability to model structured data. However, most GNNs are designed for homogeneous graphs, where all nodes and edges are of the same type, making them unsuitable for representing heterogeneous structures. In this study, we introduce the Heterogeneous Graph Transformer (HGT) architecture to model Web-scale heterogeneous graphs. To account for heterogeneity, we incorporate node- and edge-type specific parameters to capture the heterogeneous attention over each edge, enabling HGT to maintain distinct representations for different types of nodes and edges. To handle dynamic heterogeneous graphs, we introduce the relative temporal encoding technique, which captures the dynamic structural dependency with arbitrary durations. To train on Web-scale graph data, we propose the heterogeneous mini-batch graph sampling algorithm, HGSampling, for efficient and scalable training. Our experiments on the Open Academic Graph of 179 million nodes and 2 billion edges demonstrate that HGT outperforms all state-of-the-art GNN baselines by 9%-21% on various downstream tasks.",1
"Neural networks for structured data like graphs have been studied extensively in recent years. To date, the bulk of research activity has focused mainly on static graphs. However, most real-world networks are dynamic since their topology tends to change over time. Predicting the evolution of dynamic graphs is a task of high significance in the area of graph mining. Despite its practical importance, the task has not been explored in depth so far, mainly due to its challenging nature. In this paper, we propose a model that predicts the evolution of dynamic graphs. Specifically, we use a graph neural network along with a recurrent architecture to capture the temporal evolution patterns of dynamic graphs. Then, we employ a generative model which predicts the topology of the graph at the next time step and constructs a graph instance that corresponds to that topology. We evaluate the proposed model on several artificial datasets following common network evolving dynamics, as well as on real-world datasets. Results demonstrate the effectiveness of the proposed model.",0
"Recent years have seen extensive research on neural networks for structured data such as graphs. However, the bulk of this research has focused on static graphs, while most real-world networks are dynamic and undergo topology changes over time. Predicting the evolution of dynamic graphs is a crucial task in graph mining, but it has not been explored in detail due to its challenging nature. In this study, we propose a model that uses a graph neural network with a recurrent architecture to capture the temporal evolution patterns of dynamic graphs. Additionally, we employ a generative model to predict the topology of the graph at the next time step and construct a corresponding graph instance. We evaluate the proposed model on both artificial and real-world datasets, demonstrating its effectiveness.",1
"Learning generative models for graph-structured data is challenging because graphs are discrete, combinatorial, and the underlying data distribution is invariant to the ordering of nodes. However, most of the existing generative models for graphs are not invariant to the chosen ordering, which might lead to an undesirable bias in the learned distribution. To address this difficulty, we propose a permutation invariant approach to modeling graphs, using the recent framework of score-based generative modeling. In particular, we design a permutation equivariant, multi-channel graph neural network to model the gradient of the data distribution at the input graph (a.k.a., the score function). This permutation equivariant model of gradients implicitly defines a permutation invariant distribution for graphs. We train this graph neural network with score matching and sample from it with annealed Langevin dynamics. In our experiments, we first demonstrate the capacity of this new architecture in learning discrete graph algorithms. For graph generation, we find that our learning approach achieves better or comparable results to existing models on benchmark datasets.",0
"The task of learning generative models for graph-structured data is difficult because graphs are discrete, combinatorial, and the underlying data distribution is not affected by the nodes' ordering. However, most existing generative models for graphs are not invariant to the selected ordering, potentially resulting in biased learned distribution. To overcome this obstacle, we suggest a permutation invariant approach to graph modeling using the score-based generative modeling framework. We create a permutation equivariant, multi-channel graph neural network to model the gradient of the data distribution at the input graph, which implicitly defines a permutation invariant distribution for graphs. We train this network with score matching and sample from it with annealed Langevin dynamics. Our experiments show that this architecture can learn discrete graph algorithms effectively and achieve comparable or superior results to existing models on benchmark datasets for graph generation.",1
"Detecting communities on graphs has received significant interest in recent literature. Current state-of-the-art community embedding approach called \textit{ComE} tackles this problem by coupling graph embedding with community detection. Considering the success of hyperbolic representations of graph-structured data in last years, an ongoing challenge is to set up a hyperbolic approach for the community detection problem. The present paper meets this challenge by introducing a Riemannian equivalent of \textit{ComE}. Our proposed approach combines hyperbolic embeddings with Riemannian K-means or Riemannian mixture models to perform community detection. We illustrate the usefulness of this framework through several experiments on real-world social networks and comparisons with \textit{ComE} and recent hyperbolic-based classification approaches.",0
"Recently, there has been significant interest in detecting communities on graphs. One popular technique, known as \textit{ComE}, combines graph embedding with community detection to achieve state-of-the-art results. However, there is still a challenge in applying hyperbolic representations to this problem. To address this challenge, this paper presents a Riemannian equivalent of \textit{ComE}. Our approach uses hyperbolic embeddings and Riemannian K-means or Riemannian mixture models to detect communities. We demonstrate the effectiveness of our approach through experiments on real-world social networks and comparisons with \textit{ComE} and recent hyperbolic-based classification methods.",1
"Graph-structured data arise ubiquitously in many application domains. A fundamental problem is to quantify their similarities. Graph kernels are often used for this purpose, which decompose graphs into substructures and compare these substructures. However, most of the existing graph kernels do not have the property of scale-adaptivity, i.e., they cannot compare graphs at multiple levels of granularities. Many real-world graphs such as molecules exhibit structure at varying levels of granularities. To tackle this problem, we propose a new graph kernel called Tree++ in this paper. At the heart of Tree++ is a graph kernel called the path-pattern graph kernel. The path-pattern graph kernel first builds a truncated BFS tree rooted at each vertex and then uses paths from the root to every vertex in the truncated BFS tree as features to represent graphs. The path-pattern graph kernel can only capture graph similarity at fine granularities. In order to capture graph similarity at coarse granularities, we incorporate a new concept called super path into it. The super path contains truncated BFS trees rooted at the vertices in a path. Our evaluation on a variety of real-world graphs demonstrates that Tree++ achieves the best classification accuracy compared with previous graph kernels.",0
"In various domains of application, data in graph structure are commonly encountered, and a crucial issue is measuring their similarities. Graph kernels are typically employed for this purpose, dividing graphs into substructures for comparison. Nonetheless, most graph kernels currently in use lack scale-adaptivity, meaning they cannot compare graphs at different levels of granularity. Real-world graphs, such as those of molecules, display structure at varying levels of granularity. In light of this, we present a novel graph kernel called Tree++, which includes the path-pattern graph kernel at its core. The path-pattern graph kernel first constructs a truncated BFS tree for each vertex, and then employs paths from the root to each vertex in the truncated BFS tree as features to represent the graphs. However, it can only capture similarity at fine granularities. Therefore, we introduce the concept of a super path, which contains truncated BFS trees rooted at vertices in a path, to capture similarity at coarse granularities. Our experiments on a range of real-world graphs demonstrate that Tree++ attains the highest classification accuracy compared to previous graph kernels.",1
"Message-passing neural networks (MPNNs) have been successfully applied to representation learning on graphs in a variety of real-world applications. However, two fundamental weaknesses of MPNNs' aggregators limit their ability to represent graph-structured data: losing the structural information of nodes in neighborhoods and lacking the ability to capture long-range dependencies in disassortative graphs. Few studies have noticed the weaknesses from different perspectives. From the observations on classical neural network and network geometry, we propose a novel geometric aggregation scheme for graph neural networks to overcome the two weaknesses. The behind basic idea is the aggregation on a graph can benefit from a continuous space underlying the graph. The proposed aggregation scheme is permutation-invariant and consists of three modules, node embedding, structural neighborhood, and bi-level aggregation. We also present an implementation of the scheme in graph convolutional networks, termed Geom-GCN (Geometric Graph Convolutional Networks), to perform transductive learning on graphs. Experimental results show the proposed Geom-GCN achieved state-of-the-art performance on a wide range of open datasets of graphs. Code is available at https://github.com/graphdml-uiuc-jlu/geom-gcn.",0
"MPNNs have been successfully applied to learn representations of graphs in various real-world applications. However, the aggregators used in MPNNs have two fundamental weaknesses that limit their ability to represent graph-structured data. Firstly, they lose the structural information of nodes in neighborhoods, and secondly, they lack the ability to capture long-range dependencies in disassortative graphs. These weaknesses have not been thoroughly addressed in previous studies. Our proposed solution is a novel geometric aggregation scheme that leverages the continuous space underlying the graph. This permutation-invariant aggregation scheme consists of three modules: node embedding, structural neighborhood, and bi-level aggregation. We have implemented this scheme in graph convolutional networks, called Geom-GCN, for transductive learning on graphs. Our experimental results demonstrate that Geom-GCN achieves state-of-the-art performance on various open datasets of graphs. The code for Geom-GCN is available at https://github.com/graphdml-uiuc-jlu/geom-gcn.",1
"We address the problem of merging graph and feature-space information while learning a metric from structured data. Existing algorithms tackle the problem in an asymmetric way, by either extracting vectorized summaries of the graph structure or adding hard constraints to feature-space algorithms. Following a different path, we define a metric regression scheme where we train metric-constrained linear combinations of dissimilarity matrices. The idea is that the input matrices can be pre-computed dissimilarity measures obtained from any kind of available data (e.g. node attributes or edge structure). As the model inputs are distance measures, we do not need to assume the existence of any underlying feature space. Main challenge is that metric constraints (especially positive-definiteness and sub-additivity), are not automatically respected if, for example, the coefficients of the linear combination are allowed to be negative. Both positive and sub-additive constraints are linear inequalities, but the computational complexity of imposing them scales as O(D3), where D is the size of the input matrices (i.e. the size of the data set). This becomes quickly prohibitive, even when D is relatively small. We propose a new graph-based technique for optimizing under such constraints and show that, in some cases, our approach may reduce the original computational complexity of the optimization process by one order of magnitude. Contrarily to existing methods, our scheme applies to any (possibly non-convex) metric-constrained objective function.",0
"Our focus is on combining graph and feature-space information during the learning of a metric from structured data. Current approaches handle this problem in an imbalanced manner, either by producing vectorized summaries of the graph structure or by incorporating rigid constraints into feature-space algorithms. We have taken a different approach by creating a metric regression scheme that trains metric-constrained linear combinations of dissimilarity matrices. These matrices can be pre-computed dissimilarity measures obtained from available data, such as node attributes or edge structure. Since our model inputs are distance measures, we do not need to assume the existence of any underlying feature space. However, we face the challenge of ensuring metric constraints, such as positive-definiteness and sub-additivity, are maintained even if the coefficients of the linear combination are negative. These constraints are linear inequalities, but imposing them has a computational complexity that scales as O(D3), where D is the size of the input matrices. This becomes problematic even when D is relatively small. Our proposed solution is a new graph-based technique that optimizes under such constraints and, in some cases, reduces the original computational complexity of the optimization process by one order of magnitude. Unlike existing methods, our scheme applies to any metric-constrained objective function, even non-convex ones.",1
"While learning models are typically studied for inputs in the form of a fixed dimensional feature vector, real world data is rarely found in this form. In order to meet the basic requirement of traditional learning models, structural data generally have to be converted into fix-length vectors in a handcrafted manner, which is tedious and may even incur information loss. A common form of structured data is what we term ""semantic tree-structures"", corresponding to data where rich semantic information is encoded in a compositional manner, such as those expressed in JavaScript Object Notation (JSON) and eXtensible Markup Language (XML). For tree-structured data, several learning models have been studied to allow for working directly on raw tree-structure data, However such learning models are limited to either a specific tree-topology or a specific tree-structured data format, e.g., synthetic parse trees. In this paper, we propose a novel framework for end-to-end learning on generic semantic tree-structured data of arbitrary topology and heterogeneous data types, such as data expressed in JSON, XML and so on. Motivated by the works in recursive and recurrent neural networks, we develop exemplar neural implementations of our framework for the JSON format. We evaluate our approach on several UCI benchmark datasets, including ablation and data-efficiency studies, and on a toy reinforcement learning task. Experimental results suggest that our framework yields comparable performance to use of standard models with dedicated feature-vectors in general, and even exceeds baseline performance in cases where compositional nature of the data is particularly important.   The source code for a JSON-based implementation of our framework along with experiments can be downloaded at https://github.com/EndingCredits/json2vec.",0
"Typically, learning models are examined with inputs in the form of a fixed-dimensional feature vector, but real-world data is often not in this format. To meet the requirements of traditional learning models, structural data must be transformed into fixed-length vectors in a manual way, which can be tedious and may result in loss of information. ""Semantic tree-structures"" are a common form of structured data that encode rich semantic information in a compositional manner, such as those expressed in JSON and XML. Several learning models have been developed for tree-structured data to work directly on raw data, but these models are generally limited to a specific tree-topology or a specific tree-structured data format, such as synthetic parse trees. In this paper, we propose a novel framework for end-to-end learning on generic semantic tree-structured data of arbitrary topology and heterogeneous data types, including data expressed in JSON, XML, and others. We create neural implementations of our framework for the JSON format, inspired by the works in recursive and recurrent neural networks. Our approach is evaluated on several UCI benchmark datasets, including ablation and data-efficiency studies, and on a toy reinforcement learning task. The experimental results show that our framework performs comparably to standard models with dedicated feature-vectors in general, and even outperforms baseline performance in cases where the compositional nature of the data is particularly important. The source code for a JSON-based implementation of our framework along with experiments is available for download at https://github.com/EndingCredits/json2vec.",1
"Paper documents are widely used as an irreplaceable channel of information in many fields, especially in financial industry, fostering a great amount of demand for systems which can convert document images into structured data representations. In this paper, we present a machine learning framework for data ingestion in document images, which processes the images uploaded by users and return fine-grained data in JSON format. Details of model architectures, design strategies, distinctions with existing solutions and lessons learned during development are elaborated. We conduct abundant experiments on both synthetic and real-world data in State Street. The experimental results indicate the effectiveness and efficiency of our methods.",0
"The use of paper documents remains prevalent in various fields, notably the financial industry, where they serve as a crucial source of information. As a result, there is a high demand for systems that can convert document images into structured data representations. This paper introduces a machine learning framework that can process uploaded document images and produce detailed data in JSON format. We discuss the model architectures, design strategies, and differences from existing solutions, as well as the lessons we learned during development. Furthermore, we conducted numerous experiments on synthetic and real-world data in State Street, and our findings demonstrate the effectiveness and efficiency of our approach.",1
"Machine learning models that can exploit the inherent structure in data have gained prominence. In particular, there is a surge in deep learning solutions for graph-structured data, due to its wide-spread applicability in several fields. Graph attention networks (GAT), a recent addition to the broad class of feature learning models in graphs, utilizes the attention mechanism to efficiently learn continuous vector representations for semi-supervised learning problems. In this paper, we perform a detailed analysis of GAT models, and present interesting insights into their behavior. In particular, we show that the models are vulnerable to heterogeneous rogue nodes and hence propose novel regularization strategies to improve the robustness of GAT models. Using benchmark datasets, we demonstrate performance improvements on semi-supervised learning, using the proposed robust variant of GAT.",0
"The use of machine learning models that can take advantage of the natural structure in data is becoming more widespread. Recently, there has been a rise in the use of deep learning methods for graph-based data due to its wide range of applications. Graph attention networks (GAT) are a type of feature learning model that uses the attention mechanism to effectively learn continuous vector representations for semi-supervised learning problems. This study examines GAT models in detail and provides valuable insights into their behavior. Specifically, the study reveals that GAT models are susceptible to heterogeneous rogue nodes and proposes new regularization techniques to enhance their robustness. The study demonstrates that the proposed robust version of GAT outperforms standard GAT models in semi-supervised learning tasks on benchmark datasets.",1
"Despite the phenomenal success of deep neural networks in a broad range of learning tasks, there is a lack of theory to understand the way they work. In particular, Convolutional Neural Networks (CNNs) are known to perform much better than Fully-Connected Networks (FCNs) on spatially structured data: the architectural structure of CNNs benefits from prior knowledge on the features of the data, for instance their translation invariance. The aim of this work is to understand this fact through the lens of dynamics in the loss landscape.   We introduce a method that maps a CNN to its equivalent FCN (denoted as eFCN). Such an embedding enables the comparison of CNN and FCN training dynamics directly in the FCN space. We use this method to test a new training protocol, which consists in training a CNN, embedding it to FCN space at a certain ``relax time'', then resuming the training in FCN space. We observe that for all relax times, the deviation from the CNN subspace is small, and the final performance reached by the eFCN is higher than that reachable by a standard FCN of same architecture. More surprisingly, for some intermediate relax times, the eFCN outperforms the CNN it stemmed, by combining the prior information of the CNN and the expressivity of the FCN in a complementary way. The practical interest of our protocol is limited by the very large size of the highly sparse eFCN. However, it offers interesting insights into the persistence of architectural bias under stochastic gradient dynamics. It shows the existence of some rare basins in the FCN loss landscape associated with very good generalization. These can only be accessed thanks to the CNN prior, which helps navigate the landscape during the early stages of optimization.",0
"Although deep neural networks have achieved remarkable success in various learning tasks, their functioning lacks theoretical understanding. Convolutional Neural Networks (CNNs) have been found to outperform Fully-Connected Networks (FCNs) on spatially structured data due to the CNN architecture's ability to utilize prior knowledge about features such as translation invariance. This study aims to comprehend this phenomenon through a loss landscape dynamics perspective. The authors introduce a method that maps a CNN to its equivalent FCN, enabling the direct comparison of CNN and FCN training dynamics in the FCN space. They test a new training protocol involving training a CNN, embedding it to FCN space at a certain ""relax time,"" and then resuming training in the FCN space. The authors observe that the eFCN's final performance is higher than that of a standard FCN of the same architecture. They also find that for some intermediate relax times, the eFCN outperforms the CNN by combining its prior information with the FCN's expressivity in a complementary way. The study offers insights into the persistence of architectural bias under stochastic gradient dynamics and indicates that rare basins in the FCN loss landscape associated with excellent generalization can only be accessed thanks to the CNN prior.",1
"The richness in the content of various information networks such as social networks and communication networks provides the unprecedented potential for learning high-quality expressive representations without external supervision. This paper investigates how to preserve and extract the abundant information from graph-structured data into embedding space in an unsupervised manner. To this end, we propose a novel concept, Graphical Mutual Information (GMI), to measure the correlation between input graphs and high-level hidden representations. GMI generalizes the idea of conventional mutual information computations from vector space to the graph domain where measuring mutual information from two aspects of node features and topological structure is indispensable. GMI exhibits several benefits: First, it is invariant to the isomorphic transformation of input graphs---an inevitable constraint in many existing graph representation learning algorithms; Besides, it can be efficiently estimated and maximized by current mutual information estimation methods such as MINE; Finally, our theoretical analysis confirms its correctness and rationality. With the aid of GMI, we develop an unsupervised learning model trained by maximizing GMI between the input and output of a graph neural encoder. Considerable experiments on transductive as well as inductive node classification and link prediction demonstrate that our method outperforms state-of-the-art unsupervised counterparts, and even sometimes exceeds the performance of supervised ones.",0
"This study explores the potential of social and communication networks as a source of information for learning expressive representations without external supervision. The goal is to extract and preserve the abundant information from graph-structured data into embedding space in an unsupervised manner. To achieve this, a novel concept called Graphical Mutual Information (GMI) is proposed to measure the correlation between input graphs and high-level hidden representations. GMI measures mutual information from two aspects of node features and topological structure in the graph domain. This approach is invariant to the isomorphic transformation of input graphs, can be efficiently estimated and maximized by the current mutual information estimation methods such as MINE, and has been validated with theoretical analysis. An unsupervised learning model is developed using GMI to maximize the correlation between input and output of a graph neural encoder. The experiments show that this method outperforms state-of-the-art unsupervised counterparts and sometimes even exceeds the performance of supervised ones in transductive and inductive node classification and link prediction.",1
"Hierarchical Sparse Coding (HSC) is a powerful model to efficiently represent multi-dimensional, structured data such as images. The simplest solution to solve this computationally hard problem is to decompose it into independent layer-wise subproblems. However, neuroscientific evidence would suggest inter-connecting these subproblems as in the Predictive Coding (PC) theory, which adds top-down connections between consecutive layers. In this study, a new model called 2-Layers Sparse Predictive Coding (2L-SPC) is introduced to assess the impact of this inter-layer feedback connection. In particular, the 2L-SPC is compared with a Hierarchical Lasso (Hi-La) network made out of a sequence of independent Lasso layers. The 2L-SPC and the 2-layers Hi-La networks are trained on 4 different databases and with different sparsity parameters on each layer. First, we show that the overall prediction error generated by 2L-SPC is lower thanks to the feedback mechanism as it transfers prediction error between layers. Second, we demonstrate that the inference stage of the 2L-SPC is faster to converge than for the Hi-La model. Third, we show that the 2L-SPC also accelerates the learning process. Finally, the qualitative analysis of both models dictionaries, supported by their activation probability, show that the 2L-SPC features are more generic and informative.",0
"The Hierarchical Sparse Coding (HSC) model is effective in efficiently representing structured, multi-dimensional data like images. To solve this complex problem, it is common to break it down into subproblems that are independent and layer-specific. However, the Predictive Coding (PC) theory suggests that inter-connecting these subproblems with top-down connections between consecutive layers is more beneficial. This study introduces a new model called 2-Layers Sparse Predictive Coding (2L-SPC) to test the impact of this inter-layer feedback connection, which is compared to a Hierarchical Lasso (Hi-La) network consisting of independent Lasso layers. The two models are trained on 4 different databases and with different sparsity parameters for each layer. The results show that the feedback mechanism in 2L-SPC reduces the overall prediction error by transferring it between layers, and the inference stage is faster to converge than in the Hi-La model. Furthermore, 2L-SPC accelerates the learning process and generates more generic and informative features, as supported by the qualitative analysis of both models' dictionaries and their activation probability.",1
"Graph Neural Networks (GNN) have been shown to work effectively for modeling graph structured data to solve tasks such as node classification, link prediction and graph classification. There has been some recent progress in defining the notion of pooling in graphs whereby the model tries to generate a graph level representation by downsampling and summarizing the information present in the nodes. Existing pooling methods either fail to effectively capture the graph substructure or do not easily scale to large graphs. In this work, we propose ASAP (Adaptive Structure Aware Pooling), a sparse and differentiable pooling method that addresses the limitations of previous graph pooling architectures. ASAP utilizes a novel self-attention network along with a modified GNN formulation to capture the importance of each node in a given graph. It also learns a sparse soft cluster assignment for nodes at each layer to effectively pool the subgraphs to form the pooled graph. Through extensive experiments on multiple datasets and theoretical analysis, we motivate our choice of the components used in ASAP. Our experimental results show that combining existing GNN architectures with ASAP leads to state-of-the-art results on multiple graph classification benchmarks. ASAP has an average improvement of 4%, compared to current sparse hierarchical state-of-the-art method.",0
"Graph Neural Networks (GNN) have proven to be an effective tool for modeling graph structured data to tackle various tasks, such as node classification, graph classification, and link prediction. Recent advancements have been made in the notion of pooling in graphs, where the model attempts to create a graph level representation by downsampling and summarizing the information present in the nodes. However, existing pooling methods have limitations, as they are unable to effectively capture the graph substructure or scale up to large graphs. To address these limitations, we propose ASAP (Adaptive Structure Aware Pooling), which is a sparse and differentiable pooling method. ASAP leverages a self-attention network and modified GNN formulation to capture the importance of each node in a given graph. It also learns a sparse soft cluster assignment for nodes at each layer to effectively pool the subgraphs and form the pooled graph. We conducted extensive experiments on multiple datasets and performed theoretical analysis to justify our choice of components used in ASAP. Results demonstrate that combining existing GNN architectures with ASAP leads to state-of-the-art results on multiple graph classification benchmarks, with an average improvement of 4% compared to current sparse hierarchical state-of-the-art methods.",1
"Classifiers built with neural networks handle large-scale high dimensional data, such as facial images from computer vision, extremely well while traditional statistical methods often fail miserably. In this paper, we attempt to understand this empirical success in high dimensional classification by deriving the convergence rates of excess risk. In particular, a teacher-student framework is proposed that assumes the Bayes classifier to be expressed as ReLU neural networks. In this setup, we obtain a sharp rate of convergence, i.e., $\tilde{O}_d(n^{-2/3})$, for classifiers trained using either 0-1 loss or hinge loss. This rate can be further improved to $\tilde{O}_d(n^{-1})$ when the data distribution is separable. Here, $n$ denotes the sample size. An interesting observation is that the data dimension only contributes to the $\log(n)$ term in the above rates. This may provide one theoretical explanation for the empirical successes of deep neural networks in high dimensional classification, particularly for structured data.",0
"Neural networks are effective at handling large-scale, high dimensional data like facial images from computer vision, while traditional statistical methods often struggle. This paper aims to understand why neural networks are successful in high dimensional classification by examining the convergence rates of excess risk. The authors propose a teacher-student framework which assumes the Bayes classifier can be expressed as ReLU neural networks. The resulting classifiers have a sharp convergence rate of $\tilde{O}_d(n^{-2/3})$ when trained using either 0-1 loss or hinge loss, and this rate improves to $\tilde{O}_d(n^{-1})$ when the data distribution is separable. The sample size is denoted by $n$, and the data dimension only contributes to the $\log(n)$ term in the rates. These findings may explain why deep neural networks perform well in high dimensional classification, especially for structured data.",1
"Graph Convolutional Networks (GCNs) have recently been shown to be quite successful in modeling graph-structured data. However, the primary focus has been on handling simple undirected graphs. Multi-relational graphs are a more general and prevalent form of graphs where each edge has a label and direction associated with it. Most of the existing approaches to handle such graphs suffer from over-parameterization and are restricted to learning representations of nodes only. In this paper, we propose CompGCN, a novel Graph Convolutional framework which jointly embeds both nodes and relations in a relational graph. CompGCN leverages a variety of entity-relation composition operations from Knowledge Graph Embedding techniques and scales with the number of relations. It also generalizes several of the existing multi-relational GCN methods. We evaluate our proposed method on multiple tasks such as node classification, link prediction, and graph classification, and achieve demonstrably superior results. We make the source code of CompGCN available to foster reproducible research.",0
"Recent research has demonstrated the effectiveness of Graph Convolutional Networks (GCNs) in modeling graph-structured data. However, the focus has mainly been on handling simple undirected graphs, while multi-relational graphs are a more common form of graphs that contain labeled edges and directions. Most current approaches to handling such graphs suffer from over-parameterization and only allow for learning representations of nodes. This paper introduces CompGCN, a novel Graph Convolutional framework that simultaneously embeds nodes and relations in a relational graph. By using a variety of entity-relation composition operations from Knowledge Graph Embedding techniques, CompGCN scales with the number of relations and generalizes existing multi-relational GCN methods. The proposed method is evaluated on various tasks, including node classification, link prediction, and graph classification, and outperforms other methods. The source code of CompGCN is publicly available to promote reproducible research.",1
"We propose a novel spectral convolutional neural network (CNN) model on graph structured data, namely Distributed Feedback-Looped Networks (DFNets). This model is incorporated with a robust class of spectral graph filters, called feedback-looped filters, to provide better localization on vertices, while still attaining fast convergence and linear memory requirements. Theoretically, feedback-looped filters can guarantee convergence w.r.t. a specified error bound, and be applied universally to any graph without knowing its structure. Furthermore, the propagation rule of this model can diversify features from the preceding layers to produce strong gradient flows. We have evaluated our model using two benchmark tasks: semi-supervised document classification on citation networks and semi-supervised entity classification on a knowledge graph. The experimental results show that our model considerably outperforms the state-of-the-art methods in both benchmark tasks over all datasets.",0
"DFNets is a new type of spectral convolutional neural network (CNN) model designed specifically for graph structured data. The model incorporates feedback-looped filters, a robust class of spectral graph filters, which allow for better localization on vertices while still maintaining fast convergence and linear memory requirements. Feedback-looped filters can be applied to any graph without knowledge of its structure and can ensure convergence w.r.t. a specified error bound. The propagation rule for the model diversifies features from preceding layers to produce strong gradient flows. The model was evaluated using two benchmark tasks: semi-supervised document classification on citation networks and semi-supervised entity classification on a knowledge graph. Results showed that DFNets outperformed state-of-the-art methods on all datasets for both benchmark tasks.",1
"We present Spectral Inference Networks, a framework for learning eigenfunctions of linear operators by stochastic optimization. Spectral Inference Networks generalize Slow Feature Analysis to generic symmetric operators, and are closely related to Variational Monte Carlo methods from computational physics. As such, they can be a powerful tool for unsupervised representation learning from video or graph-structured data. We cast training Spectral Inference Networks as a bilevel optimization problem, which allows for online learning of multiple eigenfunctions. We show results of training Spectral Inference Networks on problems in quantum mechanics and feature learning for videos on synthetic datasets. Our results demonstrate that Spectral Inference Networks accurately recover eigenfunctions of linear operators and can discover interpretable representations from video in a fully unsupervised manner.",0
"Spectral Inference Networks offer a method for learning eigenfunctions of linear operators using stochastic optimization. This framework extends Slow Feature Analysis to various symmetric operators and has a close relationship with Variational Monte Carlo methods in computational physics. These networks can be a potent tool for unsupervised representation learning from video or graph-structured data. The training of Spectral Inference Networks is framed as a bilevel optimization problem, which allows for the online learning of multiple eigenfunctions. We present the results of training these networks on synthetic datasets for problems in quantum mechanics and feature learning for videos. Our findings demonstrate that Spectral Inference Networks can recover eigenfunctions of linear operators accurately and discover interpretable representations from video in a fully unsupervised manner.",1
"Kernel classifiers and regressors designed for structured data, such as sequences, trees and graphs, have significantly advanced a number of interdisciplinary areas such as computational biology and drug design. Typically, kernels are designed beforehand for a data type which either exploit statistics of the structures or make use of probabilistic generative models, and then a discriminative classifier is learned based on the kernels via convex optimization. However, such an elegant two-stage approach also limited kernel methods from scaling up to millions of data points, and exploiting discriminative information to learn feature representations.   We propose, structure2vec, an effective and scalable approach for structured data representation based on the idea of embedding latent variable models into feature spaces, and learning such feature spaces using discriminative information. Interestingly, structure2vec extracts features by performing a sequence of function mappings in a way similar to graphical model inference procedures, such as mean field and belief propagation. In applications involving millions of data points, we showed that structure2vec runs 2 times faster, produces models which are $10,000$ times smaller, while at the same time achieving the state-of-the-art predictive performance.",0
"Advanced kernel classifiers and regressors have been developed for structured data, including sequences, trees, and graphs, resulting in significant advancements in fields such as computational biology and drug design. These kernels are typically designed beforehand for a specific data type, utilizing either statistical structures or probabilistic generative models, before a discriminative classifier is learned through convex optimization. However, this two-stage approach has limited kernel methods in terms of scalability and feature representation. To address this, we propose structure2vec, an approach that embeds latent variable models into feature spaces and learns these spaces using discriminative information. This approach utilizes function mappings similar to graphical model inference procedures, resulting in faster and smaller models with state-of-the-art predictive performance, making it ideal for applications with millions of data points.",1
"Semantic segmentation with deep learning has achieved great progress in classifying the pixels in the image. However, the local location information is usually ignored in the high-level feature extraction by the deep learning, which is important for image semantic segmentation. To avoid this problem, we propose a graph model initialized by a fully convolutional network (FCN) named Graph-FCN for image semantic segmentation. Firstly, the image grid data is extended to graph structure data by a convolutional network, which transforms the semantic segmentation problem into a graph node classification problem. Then we apply graph convolutional network to solve this graph node classification problem. As far as we know, it is the first time that we apply the graph convolutional network in image semantic segmentation. Our method achieves competitive performance in mean intersection over union (mIOU) on the VOC dataset(about 1.34% improvement), compared to the original FCN model.",0
"Significant advancements have been made in classifying image pixels through deep learning with semantic segmentation. However, the deep learning process often overlooks the crucial local location information required for image semantic segmentation. To address this issue, we present a graph model called Graph-FCN for image semantic segmentation, which utilizes a fully convolutional network (FCN) to initialize the graph. Initially, the image grid data is transformed into graph node structure data through a convolutional network that converts the semantic segmentation problem into a graph node classification issue. We then apply the graph convolutional network to solve this problem. This is the first instance of graph convolutional network being used for image semantic segmentation. Our approach has achieved competitive performance on the VOC dataset, with a mean intersection over union (mIOU) improvement of approximately 1.34% compared to the original FCN model.",1
"Graph Neural Networks (GNNs), which generalize deep neural networks to graph-structured data, have drawn considerable attention and achieved state-of-the-art performance in numerous graph related tasks. However, existing GNN models mainly focus on designing graph convolution operations. The graph pooling (or downsampling) operations, that play an important role in learning hierarchical representations, are usually overlooked. In this paper, we propose a novel graph pooling operator, called Hierarchical Graph Pooling with Structure Learning (HGP-SL), which can be integrated into various graph neural network architectures. HGP-SL incorporates graph pooling and structure learning into a unified module to generate hierarchical representations of graphs. More specifically, the graph pooling operation adaptively selects a subset of nodes to form an induced subgraph for the subsequent layers. To preserve the integrity of graph's topological information, we further introduce a structure learning mechanism to learn a refined graph structure for the pooled graph at each layer. By combining HGP-SL operator with graph neural networks, we perform graph level representation learning with focus on graph classification task. Experimental results on six widely used benchmarks demonstrate the effectiveness of our proposed model.",0
"Considerable attention has been drawn to Graph Neural Networks (GNNs), which have achieved state-of-the-art performance in numerous graph-related tasks by generalizing deep neural networks to graph-structured data. However, current GNN models concentrate primarily on designing graph convolution operations, neglecting the significance of graph pooling (or downsampling) operations in learning hierarchical representations. We present a novel graph pooling operator, the Hierarchical Graph Pooling with Structure Learning (HGP-SL), which can be integrated into various graph neural network architectures. HGP-SL combines graph pooling and structure learning into a single module to generate hierarchical representations of graphs. Specifically, the graph pooling operation selects a subset of nodes to create an induced subgraph for subsequent layers. To ensure the preservation of the graph's topological information, we introduce a structure learning mechanism to refine the graph structure for the pooled graph at each layer. By combining the HGP-SL operator with graph neural networks, we perform graph-level representation learning, with a focus on graph classification tasks. Our proposed model is shown to be effective through experimental results on six widely-used benchmarks.",1
"Graph neural networks (GNNs) are an emerging model for learning graph embeddings and making predictions on graph structured data. However, robustness of graph neural networks is not yet well-understood. In this work, we focus on node structural identity predictions, where a representative GNN model is able to achieve near-perfect accuracy. We also show that the same GNN model is not robust to addition of structural noise, through a controlled dataset and set of experiments. Finally, we show that under the right conditions, graph-augmented training is capable of significantly improving robustness to structural noise.",0
"The use of graph neural networks (GNNs) is becoming increasingly popular for learning graph embeddings and making predictions on data structured in a graph format. However, the degree of robustness of GNNs is currently unknown. This study specifically concentrates on predicting node structural identity, for which a typical GNN model can attain almost flawless accuracy. However, through a series of experiments conducted on a controlled dataset, it is demonstrated that the same GNN model does not remain resilient when exposed to structural noise. Ultimately, it is shown that graph-augmented training can considerably enhance the GNN model's ability to withstand structural noise, provided that certain conditions are met.",1
"In the last decade, many diverse advances have occurred in the field of information extraction from data. Information extraction in its simplest form takes place in computing environments, where structured data can be extracted through a series of queries. The continuous expansion of quantities of data have therefore provided an opportunity for knowledge extraction (KE) from a textual document (TD). A typical problem of this kind is the extraction of common characteristics and knowledge from a group of TDs, with the possibility to group such similar TDs in a process known as clustering. In this paper we present a technique for such KE among a group of TDs related to the common characteristics and meaning of their content. Our technique is based on the Spearman's Rank Correlation Coefficient (SRCC), for which the conducted experiments have proven to be comprehensive measure to achieve a high-quality KE.",0
"Over the past decade, the field of information extraction from data has witnessed numerous advancements. This process typically occurs in computing environments, where structured data can be obtained through a series of queries. With the increasing amount of data available, there is now an opportunity for knowledge extraction (KE) from textual documents (TDs). One common challenge is to extract shared characteristics and knowledge from a group of TDs, which can be accomplished through clustering. This paper introduces a technique for KE among a group of TDs that share common characteristics and meaning in their content. Our approach relies on the Spearman's Rank Correlation Coefficient (SRCC), which has been proven through experiments to be a comprehensive measure for achieving high-quality KE.",1
"The (variational) graph auto-encoder and its variants have been popularly used for representation learning on graph-structured data. While the encoder is often a powerful graph convolutional network, the decoder reconstructs the graph structure by only considering two nodes at a time, thus ignoring possible interactions among edges. On the other hand, structured prediction, which considers the whole graph simultaneously, is computationally expensive. In this paper, we utilize the well-known triadic closure property which is exhibited in many real-world networks. We propose the triad decoder, which considers and predicts the three edges involved in a local triad together. The triad decoder can be readily used in any graph-based auto-encoder. In particular, we incorporate this to the (variational) graph auto-encoder. Experiments on link prediction, node clustering and graph generation show that the use of triads leads to more accurate prediction, clustering and better preservation of the graph characteristics.",0
"Representation learning on graph-structured data has been widely adopted using the (variational) graph auto-encoder and its variations. The encoder, which is often a potent graph convolutional network, is paired with a decoder that reconstructs the graph structure by only considering two nodes at a time, neglecting the possible interactions among edges. While structured prediction considers the entire graph simultaneously, it is computationally intensive. In this study, we exploit the triadic closure property, commonly found in many real-world networks, to introduce the triad decoder. This decoder predicts and considers the three edges involved in a local triad together, and it can be easily integrated into any graph-based auto-encoder. Specifically, we incorporate it into the (variational) graph auto-encoder. Our experiments demonstrate that using triads leads to more precise predictions, improved clustering, and better preservation of graph characteristics for link prediction, node clustering, and graph generation.",1
"Global pooling, such as max- or sum-pooling, is one of the key ingredients in deep neural networks used for processing images, texts, graphs and other types of structured data. Based on the recent DeepSets architecture proposed by Zaheer et al. (NIPS 2017), we introduce a Set Aggregation Network (SAN) as an alternative global pooling layer. In contrast to typical pooling operators, SAN allows to embed a given set of features to a vector representation of arbitrary size. We show that by adjusting the size of embedding, SAN is capable of preserving the whole information from the input. In experiments, we demonstrate that replacing global pooling layer by SAN leads to the improvement of classification accuracy. Moreover, it is less prone to overfitting and can be used as a regularizer.",0
"Deep neural networks rely on global pooling, such as max- or sum-pooling, to process various types of structured data, including images, texts, and graphs. In this study, we propose an alternative global pooling layer called Set Aggregation Network (SAN) based on the recent DeepSets architecture. Unlike traditional pooling operators, SAN can convert a set of features into a vector representation of any size. Our experiments show that adjusting the size of the embedding allows SAN to retain all the information from the input. Replacing the global pooling layer with SAN improves the accuracy of classification and reduces overfitting. Moreover, SAN can be used as a regularizer.",1
"The current paper is a study in Recurrent Neural Networks (RNN), motivated by the lack of examples simple enough so that they can be thoroughly understood theoretically, but complex enough to be realistic. We constructed an example of structured data, motivated by problems from image-to-text conversion (OCR), which requires long-term memory to decode. Our data is a simple writing system, encoding characters 'X' and 'O' as their upper halves, which is possible due to symmetry of the two characters. The characters can be connected, as in some languages using cursive, such as Arabic (abjad). The string 'XOOXXO' may be encoded as '${\vee}{\wedge}\kern-1.5pt{\wedge}{\vee}\kern-1.5pt{\vee}{\wedge}$'. It follows that we may need to know arbitrarily long past to decode a current character, thus requiring long-term memory. Subsequently we constructed an RNN capable of decoding sequences encoded in this manner. Rather than by training, we constructed our RNN ""by inspection"", i.e. we guessed its weights. This involved a sequence of steps. We wrote a conventional program which decodes the sequences as the example above. Subsequently, we interpreted the program as a neural network (the only example of this kind known to us). Finally, we generalized this neural network to discover a new RNN architecture whose instance is our handcrafted RNN. It turns out to be a 3 layer network, where the middle layer is capable of performing simple logical inferences; thus the name ""deductron"". It is demonstrated that it is possible to train our network by simulated annealing. Also, known variants of stochastic gradient descent (SGD) methods are shown to work.",0
"The lack of simple yet realistic examples of Recurrent Neural Networks (RNN) motivated us to conduct a study on the subject. For this purpose, we developed a structured data example using a writing system that encodes characters 'X' and 'O' as their upper halves. The system is symmetrical, allowing the characters to be connected, much like some cursive languages such as Arabic (abjad). To decode a character, long-term memory is required, as we may need to know the past arbitrarily. Our RNN was constructed through guesswork instead of training. We first created a program that decodes sequences, which we then interpreted as a neural network. We then generalized this neural network, resulting in a new RNN architecture called ""deductron."" This 3-layer network's middle layer can perform logical inferences. We demonstrated that our network can be trained using simulated annealing and variants of stochastic gradient descent (SGD) methods.",1
"Adverse drug-drug interactions (DDIs) remain a leading cause of morbidity and mortality. Identifying potential DDIs during the drug design process is critical for patients and society. Although several computational models have been proposed for DDI prediction, there are still limitations: (1) specialized design of drug representation for DDI predictions is lacking; (2) predictions are based on limited labelled data and do not generalize well to unseen drugs or DDIs; and (3) models are characterized by a large number of parameters, thus are hard to interpret. In this work, we develop a ChemicAl SubstrucTurE Representation (CASTER) framework that predicts DDIs given chemical structures of drugs.CASTER aims to mitigate these limitations via (1) a sequential pattern mining module rooted in the DDI mechanism to efficiently characterize functional sub-structures of drugs; (2) an auto-encoding module that leverages both labelled and unlabelled chemical structure data to improve predictive accuracy and generalizability; and (3) a dictionary learning module that explains the prediction via a small set of coefficients which measure the relevance of each input sub-structures to the DDI outcome. We evaluated CASTER on two real-world DDI datasets and showed that it performed better than state-of-the-art baselines and provided interpretable predictions.",0
"The occurrence of adverse drug-drug interactions (DDIs) is a significant contributor to morbidity and mortality. Therefore, identifying potential DDIs during drug design is crucial for patients and society. Despite several proposed computational models for DDI prediction, there are limitations. Firstly, there is a lack of specialized drug representation design for DDI predictions. Secondly, the predictions heavily rely on limited labelled data, which makes it difficult to generalize to unseen drugs or DDIs. Lastly, the models contain numerous parameters, making them challenging to interpret. To address these limitations, we present the ChemicAl SubstrucTurE Representation (CASTER) framework, which predicts DDIs using the chemical structures of drugs. CASTER aims to overcome these drawbacks by incorporating a sequential pattern mining module rooted in the DDI mechanism to efficiently characterize functional drug sub-structures, an auto-encoding module that utilizes both labelled and unlabelled chemical structure data to enhance predictive accuracy and generalizability, and a dictionary learning module that explains the prediction through a small set of coefficients measuring the relevance of each input sub-structure to the DDI outcome. We evaluated CASTER on two real-world DDI datasets and demonstrated its superior performance compared to state-of-the-art baselines while also providing interpretable predictions.",1
"Healthcare data continues to flourish yet a relatively small portion, mostly structured, is being utilized effectively for predicting clinical outcomes. The rich subjective information available in unstructured clinical notes can possibly facilitate higher discrimination but tends to be under-utilized in mortality prediction. This work attempts to assess the gain in performance when multiple notes that have been minimally preprocessed are used as an input for prediction. A hierarchical architecture consisting of both convolutional and recurrent layers is used to concurrently model the different notes compiled in an individual hospital stay. This approach is evaluated on predicting in-hospital mortality on the MIMIC-III dataset. On comparison to approaches utilizing structured data, it achieved higher metrics despite requiring less cleaning and preprocessing. This demonstrates the potential of unstructured data in enhancing mortality prediction and signifies the need to incorporate more raw unstructured data into current clinical prediction methods.",0
"Although there is a wealth of healthcare data available, only a small portion of it - mainly structured information - is being effectively utilized to predict clinical outcomes. Unstructured clinical notes contain valuable subjective information that could improve mortality prediction but are often overlooked. In this study, we aim to evaluate the potential benefits of incorporating multiple minimally preprocessed unstructured notes into mortality prediction. To achieve this, we employ a hierarchical architecture with both convolutional and recurrent layers to model the different notes from a single hospital stay. We use the MIMIC-III dataset to predict in-hospital mortality and compare our approach to those using structured data. Despite requiring less cleaning and preprocessing, our method outperforms structured data approaches. Our findings highlight the significance of unstructured data in enhancing mortality prediction and emphasize the need to incorporate raw unstructured data into existing clinical prediction methods.",1
"The scattering transform is a multilayered wavelet-based deep learning architecture that acts as a model of convolutional neural networks. Recently, several works have introduced generalizations of the scattering transform for non-Euclidean settings such as graphs. Our work builds upon these constructions by introducing windowed and non-windowed graph scattering transforms based upon a very general class of asymmetric wavelets. We show that these asymmetric graph scattering transforms have many of the same theoretical guarantees as their symmetric counterparts. This work helps bridge the gap between scattering and other graph neural networks by introducing a large family of networks with provable stability and invariance guarantees. This lays the groundwork for future deep learning architectures for graph-structured data that have learned filters and also provably have desirable theoretical properties.",0
"The scattering transform is a deep learning model that uses wavelets and convolutional neural networks. Recently, researchers have extended the scattering transform to non-Euclidean settings like graphs. Our research builds on these extensions by creating both windowed and non-windowed graph scattering transforms using a broad class of asymmetric wavelets. We demonstrate that these asymmetric transforms have similar theoretical properties to their symmetric counterparts and can provide stability and invariance guarantees. By introducing this family of networks, we are closing the gap between scattering and other graph neural networks, providing a foundation for future deep learning architectures for graph-structured data that have learned filters and desirable theoretical properties.",1
"GAN is a deep-learning based generative approach to generate contents such as images, languages and speeches. Recently, studies have shown that GAN can also be applied to generative adversarial attack examples to fool the machine-learning models. In comparison with the previous non-learning adversarial example attack approaches, the GAN-based adversarial attack example approach can generate the adversarial samples quickly using the GAN architecture every time facing a new sample after training, but meanwhile needs to perturb the attack samples in great quantities, which results in the unpractical application in reality. To address this issue, we propose a new approach, named Few-Feature-Attack-GAN (FFA-GAN). FFA-GAN has a significant time-consuming advantage than the non-learning adversarial samples approaches and a better non-zero-features performance than the GANbased adversarial sample approaches. FFA-GAN can automatically generate the attack samples in the black-box attack through the GAN architecture instead of the evolutional algorithms or the other non-learning approaches. Besides, we introduce the mask mechanism into the generator network of the GAN architecture to optimize the constraint issue, which can also be regarded as the sparsity problem of the important features. During the training, the different weights of losses of the generator are set in the different training phases to ensure the divergence of the two above mentioned parallel networks of the generator. Experiments are made respectively on the structured data sets KDD-Cup 1999 and CIC-IDS 2017, in which the dimensions of the data are relatively low, and also on the unstructured data sets MNIST and CIFAR-10 with the data of the relatively high dimensions. The results of the experiments demonstrate the effectiveness and the robustness of our proposed approach.",0
"GAN is a form of generative deep-learning used to create various forms of content, including images, languages, and speeches. It has been discovered that GAN can also be used to generate adversarial attack examples to deceive machine learning models. However, the GAN-based approach requires large quantities of attack samples to be perturbed, making it impractical for real-world applications. To overcome this issue, we introduce a new approach called Few-Feature-Attack-GAN (FFA-GAN), which is faster than non-learning adversarial sample approaches and has better performance than GAN-based adversarial sample approaches. FFA-GAN uses the GAN architecture to automatically generate attack samples in black-box attacks, and a mask mechanism is introduced to optimize the constraint issue. During training, different weights of losses are assigned to the generator's parallel networks. We test the approach on various structured and unstructured data sets, and the results demonstrate its effectiveness and robustness.",1
"Human action recognition from skeleton data, fueled by the Graph Convolutional Network (GCN), has attracted lots of attention, due to its powerful capability of modeling non-Euclidean structure data. However, many existing GCN methods provide a pre-defined graph and fix it through the entire network, which can loss implicit joint correlations. Besides, the mainstream spectral GCN is approximated by one-order hop, thus higher-order connections are not well involved. Therefore, huge efforts are required to explore a better GCN architecture. To address these problems, we turn to Neural Architecture Search (NAS) and propose the first automatically designed GCN for skeleton-based action recognition. Specifically, we enrich the search space by providing multiple dynamic graph modules after fully exploring the spatial-temporal correlations between nodes. Besides, we introduce multiple-hop modules and expect to break the limitation of representational capacity caused by one-order approximation. Moreover, a sampling- and memory-efficient evolution strategy is proposed to search an optimal architecture for this task. The resulted architecture proves the effectiveness of the higher-order approximation and the dynamic graph modeling mechanism with temporal interactions, which is barely discussed before. To evaluate the performance of the searched model, we conduct extensive experiments on two very large scaled datasets and the results show that our model gets the state-of-the-art results.",0
"The Graph Convolutional Network (GCN) has gained significant attention in the field of human action recognition from skeleton data due to its ability to model non-Euclidean structure data. However, existing GCN methods use a pre-defined graph throughout the network, leading to the loss of implicit joint correlations. Additionally, the mainstream spectral GCN only approximates one-order hop, resulting in the exclusion of higher-order connections. Therefore, we propose a new GCN architecture using Neural Architecture Search (NAS), which automatically designs a dynamic graph incorporating spatial-temporal correlations between nodes. Multiple-hop modules are introduced to increase representational capacity, and an efficient evolution strategy is used to search for the optimal architecture. The resulting architecture proves the effectiveness of higher-order approximation and dynamic graph modeling with temporal interactions, which has not been discussed before. Extensive experiments show that our model achieves state-of-the-art results on two large-scale datasets.",1
"Machine learning on graph structured data has attracted much research interest due to its ubiquity in real world data. However, how to efficiently represent graph data in a general way is still an open problem. Traditional methods use handcraft graph features in a tabular form but suffer from the defects of domain expertise requirement and information loss. Graph representation learning overcomes these defects by automatically learning the continuous representations from graph structures, but they require abundant training labels, which are often hard to fulfill for graph-level prediction problems. In this work, we demonstrate that, if available, the domain expertise used for designing handcraft graph features can improve the graph-level representation learning when training labels are scarce. Specifically, we proposed a multi-task knowledge distillation method. By incorporating network-theory-based graph metrics as auxiliary tasks, we show on both synthetic and real datasets that the proposed multi-task learning method can improve the prediction performance of the original learning task, especially when the training data size is small.",0
"The ubiquity of graph structured data in real-world scenarios has sparked much interest in machine learning. However, representing graph data efficiently and comprehensively remains an unresolved issue. Traditional techniques utilize manual graph features in a tabular format, which are limited by their dependence on domain expertise and information loss. Graph representation learning addresses these shortcomings by automatically generating continuous representations from graph structures but necessitates a vast number of training labels, which are frequently difficult to obtain for graph-level prediction challenges. Our study demonstrates that incorporating domain expertise used in designing handcrafted graph features can enhance graph-level representation learning, particularly when training labels are limited. We propose a multi-task knowledge distillation approach that utilizes network-theory-based graph metrics as auxiliary tasks. We show that this multi-task learning method can enhance prediction performance on synthetic and actual data sets, particularly when training data is scarce.",1
"Deep generative priors offer powerful models for complex-structured data, such as images, audio, and text. Using these priors in inverse problems typically requires estimating the input and/or hidden signals in a multi-layer deep neural network from observation of its output. While these approaches have been successful in practice, rigorous performance analysis is complicated by the non-convex nature of the underlying optimization problems. This paper presents a novel algorithm, Multi-Layer Vector Approximate Message Passing (ML-VAMP), for inference in multi-layer stochastic neural networks. ML-VAMP can be configured to compute maximum a priori (MAP) or approximate minimum mean-squared error (MMSE) estimates for these networks. We show that the performance of ML-VAMP can be exactly predicted in a certain high-dimensional random limit. Furthermore, under certain conditions, ML-VAMP yields estimates that achieve the minimum (i.e., Bayes-optimal) MSE as predicted by the replica method. In this way, ML-VAMP provides a computationally efficient method for multi-layer inference with an exact performance characterization and testable conditions for optimality in the large-system limit.",0
"Deep generative priors are effective models for intricate data structures like images, audio, and text. However, implementing these priors in inverse problems involves estimating input and/or hidden signals in a multi-layer deep neural network based on its output. Although this approach has shown practical success, analyzing its performance is challenging due to the non-convex nature of the optimization problems involved. This study introduces a new algorithm called Multi-Layer Vector Approximate Message Passing (ML-VAMP) for inference in multi-layer stochastic neural networks. ML-VAMP can compute maximum a priori (MAP) or approximate minimum mean-squared error (MMSE) estimates for these networks. The performance of ML-VAMP can be precisely predicted in a particular high-dimensional random limit. Under specific conditions, ML-VAMP provides estimates that achieve the minimum (i.e., Bayes-optimal) MSE as predicted by the replica method. Thus, ML-VAMP is a computationally efficient method for multi-layer inference with an exact performance characterization and testable conditions for optimality in the large-system limit.",1
"Some recent studies have suggested using GANs for numeric data generation such as to generate data for completing the imbalanced numeric data. Considering the significant difference between the dimensions of the numeric data and images, as well as the strong correlations between features of numeric data, the conventional GANs normally face an overfitting problem, consequently leads to an ill-conditioning problem in generating numeric and structured data. This paper studies the constrained network structures between generator G and discriminator D in WGAN, designs several structures including isomorphic, mirror and self-symmetric structures. We evaluates the performances of the constrained WGANs in data augmentations, taking the non-constrained GANs and WGANs as the baselines. Experiments prove the constrained structures have been improved in 17/20 groups of experiments. In twenty experiments on four UCI Machine Learning Repository datasets, Australian Credit Approval data, German Credit data, Pima Indians Diabetes data and SPECT heart data facing five conventional classifiers. Especially, Isomorphic WGAN is the best in 15/20 experiments. Finally, we theoretically proves that the effectiveness of constrained structures by the directed graphic model (DGM) analysis.",0
"Recent studies propose the use of GANs for generating numeric data, particularly for imbalanced numeric data completion. However, conventional GANs face overfitting and ill-conditioning issues when dealing with numeric data due to differences in dimensions and strong feature correlations. To address this, this paper explores constrained network structures between generator G and discriminator D in WGAN, including isomorphic, mirror, and self-symmetric structures. The performance of constrained WGANs is evaluated in data augmentations against non-constrained GANs and WGANs. Results show improvement in 17/20 experiments and Isomorphic WGAN performs best in 15/20 experiments on four UCI Machine Learning Repository datasets, Australian Credit Approval data, German Credit data, Pima Indians Diabetes data, and SPECT heart data. The effectiveness of constrained structures is theoretically proven through directed graphic model (DGM) analysis.",1
"We introduce an improved unsupervised clustering protocol specially suited for large-scale structured data. The protocol follows three steps: a dimensionality reduction of the data, a density estimation over the low dimensional representation of the data, and a final segmentation of the density landscape. For the dimensionality reduction step we introduce a parallelized implementation of the well-known t-Stochastic Neighbouring Embedding (t-SNE) algorithm that significantly alleviates some inherent limitations, while improving its suitability for large datasets. We also introduce a new adaptive Kernel Density Estimation particularly coupled with the t-SNE framework in order to get accurate density estimates out of the embedded data, and a variant of the rainfalling watershed algorithm to identify clusters within the density landscape. The whole mapping protocol is wrapped in the bigMap R package, together with visualization and analysis tools to ease the qualitative and quantitative assessment of the clustering.",0
"Our team has developed an enhanced clustering method that is specifically designed for analyzing large-scale structured data without any supervision. This method is implemented in three stages: first, the data is reduced in dimensionality; next, density is estimated on the resulting low-dimensional representation; and finally, the density landscape is segmented. To achieve the dimensionality reduction, we have created a parallelized version of the t-Stochastic Neighbouring Embedding (t-SNE) algorithm, which overcomes certain limitations and is better suited for larger datasets. Additionally, we have devised a new adaptive Kernel Density Estimation technique that works in conjunction with the t-SNE framework to obtain precise density estimates from the embedded data. Finally, we have developed a variation of the rainfalling watershed algorithm to detect clusters within the density landscape. The entire mapping protocol is available in the bigMap R package, which also includes tools for visualization and analysis that facilitate both qualitative and quantitative evaluation of the clustering.",1
"We propose and analyze a method for semi-supervised learning from partially-labeled network-structured data. Our approach is based on a graph signal recovery interpretation under a clustering hypothesis that labels of data points belonging to the same well-connected subset (cluster) are similar valued. This lends naturally to learning the labels by total variation (TV) minimization, which we solve by applying a recently proposed primal-dual method for non-smooth convex optimization. The resulting algorithm allows for a highly scalable implementation using message passing over the underlying empirical graph, which renders the algorithm suitable for big data applications. By applying tools of compressed sensing, we derive a sufficient condition on the underlying network structure such that TV minimization recovers clusters in the empirical graph of the data. In particular, we show that the proposed primal-dual method amounts to maximizing network flows over the empirical graph of the dataset. Moreover, the learning accuracy of the proposed algorithm is linked to the set of network flows between data points having known labels. The effectiveness and scalability of our approach is verified by numerical experiments.",0
"Our method for semi-supervised learning from partially-labeled network-structured data involves utilizing a graph signal recovery interpretation. We assume that data points belonging to the same well-connected subset (cluster) have similar valued labels. By minimizing total variation (TV) using a primal-dual method for non-smooth convex optimization, we can effectively learn these labels. This method is highly scalable and suitable for big data applications, as it can be implemented using message passing over the empirical graph. We have derived a sufficient condition on the network structure using compressed sensing, which allows us to recover clusters in the empirical graph of the data. Our proposed primal-dual method maximizes network flows over the empirical graph of the dataset, and the accuracy of our learning algorithm is linked to the set of network flows between data points with known labels. Our approach has been verified through numerical experiments.",1
"We consider a family of problems that are concerned about making predictions for the majority of unlabeled, graph-structured data samples based on a small proportion of labeled samples. Relational information among the data samples, often encoded in the graph/network structure, is shown to be helpful for these semi-supervised learning tasks. However, conventional graph-based regularization methods and recent graph neural networks do not fully leverage the interrelations between the features, the graph, and the labels. In this work, we propose a flexible generative framework for graph-based semi-supervised learning, which approaches the joint distribution of the node features, labels, and the graph structure. Borrowing insights from random graph models in network science literature, this joint distribution can be instantiated using various distribution families. For the inference of missing labels, we exploit recent advances of scalable variational inference techniques to approximate the Bayesian posterior. We conduct thorough experiments on benchmark datasets for graph-based semi-supervised learning. Results show that the proposed methods outperform the state-of-the-art models in most settings.",0
"A group of problems is being investigated that involve predicting the majority of unlabeled data samples that are structured in a graph, using a small number of labeled samples. The graph structure often contains relational information that can be utilized in semi-supervised learning tasks. However, current methods, including conventional graph-based regularization and modern graph neural networks, do not fully utilize the relationships between features, graph, and labels. This study proposes a flexible generative framework for graph-based semi-supervised learning that approaches the joint distribution of node features, labels, and graph structure. This joint distribution can be created using various distribution families, using insights from random graph models in network science literature. For missing label inference, recent scalable variational inference techniques are used to approximate the Bayesian posterior. The proposed methods were tested on benchmark datasets for graph-based semi-supervised learning and showed superior performance to state-of-the-art models in most settings.",1
"In recent years there has been a rapid increase in classification methods on graph structured data. Both in graph kernels and graph neural networks, one of the implicit assumptions of successful state-of-the-art models was that incorporating graph isomorphism features into the architecture leads to better empirical performance. However, as we discover in this work, commonly used data sets for graph classification have repeating instances which cause the problem of isomorphism bias, i.e. artificially increasing the accuracy of the models by memorizing target information from the training set. This prevents fair competition of the algorithms and raises a question of the validity of the obtained results. We analyze 54 data sets, previously extensively used for graph-related tasks, on the existence of isomorphism bias, give a set of recommendations to machine learning practitioners to properly set up their models, and open source new data sets for the future experiments.",0
"The classification of graph structured data has seen a rapid surge in recent years. The integration of graph isomorphism features into the architecture of both graph kernels and graph neural networks has been a key factor in the success of state-of-the-art models. However, our work reveals that popular data sets used for graph classification contain repeating instances, resulting in isomorphism bias. This bias artificially inflates the model's accuracy by memorizing target information from the training set, hindering fair competition between algorithms and questioning the reliability of the results. We examined 54 widely used data sets for graph-related tasks and offer recommendations to machine learning practitioners to ensure proper model setup. Additionally, we have made new data sets available for future experiments.",1
"Graph Neural Networks (GNNs) have become a topic of intense research recently due to their powerful capability in high-dimensional classification and regression tasks for graph-structured data. However, as GNNs typically define the graph convolution by the orthonormal basis for the graph Laplacian, they suffer from high computational cost when the graph size is large. This paper introduces Haar basis which is a sparse and localized orthonormal system for a coarse-grained chain on graph. The graph convolution under Haar basis, called Haar convolution, can be defined accordingly for GNNs. The sparsity and locality of the Haar basis allow Fast Haar Transforms (FHTs) on graph, by which a fast evaluation of Haar convolution between graph data and filters can be achieved. We conduct experiments on GNNs equipped with Haar convolution, which demonstrates state-of-the-art results on graph-based regression and node classification tasks.",0
"The article discusses the growing interest in Graph Neural Networks (GNNs) due to their exceptional ability to handle high-dimensional classification and regression tasks for graph-structured data. However, GNNs often use the orthonormal basis for the graph Laplacian to define the graph convolution, which can lead to high computational costs for larger graphs. To address this issue, the paper introduces the Haar basis as a sparse and localized orthonormal system for a coarse-grained chain on a graph. The Haar convolution can be defined for GNNs using this basis, enabling Fast Haar Transforms (FHTs) on graph and a fast evaluation of Haar convolution between graph data and filters. Experiments have shown that GNNs equipped with Haar convolution can achieve state-of-the-art results on graph-based regression and node classification tasks.",1
"Graph structured data are abundant in the real world. Among different graph types, directed acyclic graphs (DAGs) are of particular interest to machine learning researchers, as many machine learning models are realized as computations on DAGs, including neural networks and Bayesian networks. In this paper, we study deep generative models for DAGs, and propose a novel DAG variational autoencoder (D-VAE). To encode DAGs into the latent space, we leverage graph neural networks. We propose an asynchronous message passing scheme that allows encoding the computations on DAGs, rather than using existing simultaneous message passing schemes to encode local graph structures. We demonstrate the effectiveness of our proposed DVAE through two tasks: neural architecture search and Bayesian network structure learning. Experiments show that our model not only generates novel and valid DAGs, but also produces a smooth latent space that facilitates searching for DAGs with better performance through Bayesian optimization.",0
"In real life, there is a lot of data that is structured in the form of graphs. Out of the various types of graphs, directed acyclic graphs (DAGs) are particularly important to machine learning researchers because many machine learning models, such as neural networks and Bayesian networks, are based on computations on DAGs. Our research aims to explore deep generative models for DAGs and introduce a new DAG variational autoencoder (D-VAE). To encode DAGs into the latent space, we use graph neural networks and propose an asynchronous message passing scheme that allows encoding computations on DAGs. This is different from existing simultaneous message passing schemes that encode local graph structures. Our model is effective in two tasks: neural architecture search and Bayesian network structure learning. Through experiments, we show that our model generates valid and novel DAGs and produces a smooth latent space that simplifies the search for DAGs with better performance using Bayesian optimization.",1
"Learning from graph-structured data is an important task in machine learning and artificial intelligence, for which Graph Neural Networks (GNNs) have shown great promise. Motivated by recent advances in geometric representation learning, we propose a novel GNN architecture for learning representations on Riemannian manifolds with differentiable exponential and logarithmic maps. We develop a scalable algorithm for modeling the structural properties of graphs, comparing Euclidean and hyperbolic geometry. In our experiments, we show that hyperbolic GNNs can lead to substantial improvements on various benchmark datasets.",0
"The task of learning from data structured as graphs is crucial in machine learning and artificial intelligence. Graph Neural Networks (GNNs) are a promising approach for this task. Drawing inspiration from recent developments in geometric representation learning, we introduce a new GNN architecture that can learn representations on Riemannian manifolds using differentiable exponential and logarithmic maps. Our algorithm can model the structural properties of graphs in a scalable manner, and we compare the effectiveness of Euclidean and hyperbolic geometry. Our experimental results demonstrate that hyperbolic GNNs can achieve significant enhancements on various benchmark datasets.",1
"Development of metrics for structural data-generating mechanisms is fundamental in machine learning and the related fields. In this paper, we give a general framework to construct metrics on random nonlinear dynamical systems, defined with the Perron-Frobenius operators in vector-valued reproducing kernel Hilbert spaces (vvRKHSs). We employ vvRKHSs to design mathematically manageable metrics and also to introduce operator-valued kernels, which enables us to handle randomness in systems. Our metric provides an extension of the existing metrics for deterministic systems, and gives a specification of the kernel maximal mean discrepancy of random processes. Moreover, by considering the time-wise independence of random processes, we clarify a connection between our metric and the independence criteria with kernels such as Hilbert-Schmidt independence criteria. We empirically illustrate our metric with synthetic data, and evaluate it in the context of the independence test for random processes. We also evaluate the performance with real time seris datas via clusering tasks.",0
"The creation of metrics for mechanisms that generate structural data is crucial in the field of machine learning and related areas. This paper presents a general framework for producing metrics on random nonlinear dynamical systems using Perron-Frobenius operators in vector-valued reproducing kernel Hilbert spaces (vvRKHSs). We utilize vvRKHSs to construct mathematically feasible metrics and introduce operator-valued kernels to handle systems that involve randomness. Our metric extends existing metrics for deterministic systems and specifies the kernel maximal mean discrepancy of random processes. Furthermore, we demonstrate the connection between our metric and independence criteria with kernels like the Hilbert-Schmidt independence criteria by considering the time-wise independence of random processes. We showcase our metric's effectiveness with synthetic data and evaluate it in the context of independence testing for random processes. Additionally, we assess the performance of our metric with real-time series data through clustering tasks.",1
"Graph Neural Networks (GNNs) for prediction tasks like node classification or edge prediction have received increasing attention in recent machine learning from graphically structured data. However, a large quantity of labeled graphs is difficult to obtain, which significantly limits the true success of GNNs. Although active learning has been widely studied for addressing label-sparse issues with other data types like text, images, etc., how to make it effective over graphs is an open question for research. In this paper, we present an investigation on active learning with GNNs for node classification tasks. Specifically, we propose a new method, which uses node feature propagation followed by K-Medoids clustering of the nodes for instance selection in active learning. With a theoretical bound analysis we justify the design choice of our approach. In our experiments on four benchmark datasets, the proposed method outperforms other representative baseline methods consistently and significantly.",0
"Recently, Graph Neural Networks (GNNs) have gained popularity for prediction tasks involving graphically structured data, such as node classification or edge prediction. However, obtaining a large quantity of labeled graphs is challenging, which greatly limits the success of GNNs. Although active learning has been studied for label-sparse data types like text and images, how to make it effective for graphs remains an open research question. This paper investigates active learning with GNNs for node classification tasks and proposes a new method that uses node feature propagation and K-Medoids clustering for instance selection. The design choice of our approach is justified by a theoretical bound analysis. In experiments on four benchmark datasets, our proposed method consistently and significantly outperforms other representative baseline methods.",1
"Convolutional neural networks are nowadays witnessing a major success in different pattern recognition problems. These learning models were basically designed to handle vectorial data such as images but their extension to non-vectorial and semi-structured data (namely graphs with variable sizes, topology, etc.) remains a major challenge, though a few interesting solutions are currently emerging. In this paper, we introduce MLGCN; a novel spectral Multi-Laplacian Graph Convolutional Network. The main contribution of this method resides in a new design principle that learns graph-laplacians as convex combinations of other elementary laplacians each one dedicated to a particular topology of the input graphs. We also introduce a novel pooling operator, on graphs, that proceeds in two steps: context-dependent node expansion is achieved, followed by a global average pooling; the strength of this two-step process resides in its ability to preserve the discrimination power of nodes while achieving permutation invariance. Experiments conducted on SBU and UCF-101 datasets, show the validity of our method for the challenging task of action recognition.",0
"Nowadays, convolutional neural networks have achieved significant success in various pattern recognition problems, primarily designed for vectorial data like images. However, extending them to non-vectorial and semi-structured data, such as graphs with varying sizes and topology, remains a significant challenge, albeit with a few emerging solutions. This paper introduces the MLGCN, a novel spectral Multi-Laplacian Graph Convolutional Network that learns graph-laplacians by using a new design principle to create convex combinations of other elementary laplacians, each dedicated to a specific topology of the input graphs. Additionally, a new pooling operator is introduced on graphs, proceeding in two steps: first, context-dependent node expansion is achieved, followed by a global average pooling. This two-step process preserves the discrimination power of nodes while achieving permutation invariance. Experiments conducted on SBU and UCF-101 datasets confirm the validity of our method for action recognition, a challenging task.",1
"Data samples collected for training machine learning models are typically assumed to be independent and identically distributed (iid). Recent research has demonstrated that this assumption can be problematic as it simplifies the manifold of structured data. This has motivated different research areas such as data poisoning, model improvement, and explanation of machine learning models. In this work, we study the influence of a sample on determining the intrinsic topological features of its underlying manifold. We propose the Shapley Homology framework, which provides a quantitative metric for the influence of a sample of the homology of a simplicial complex. By interpreting the influence as a probability measure, we further define an entropy which reflects the complexity of the data manifold. Our empirical studies show that when using the 0-dimensional homology, on neighboring graphs, samples with higher influence scores have more impact on the accuracy of neural networks for determining the graph connectivity and on several regular grammars whose higher entropy values imply more difficulty in being learned.",0
"The commonly accepted assumption for machine learning model training data is that they are independent and identically distributed (iid). However, recent studies have shown that this assumption oversimplifies the structured data manifold, leading to potential problems such as data poisoning, model improvement, and machine learning model explanations. This study focuses on the impact of individual samples on determining the intrinsic topological features of the underlying manifold. To measure this influence, we introduce the Shapley Homology framework, which provides a quantitative metric for a sample's influence on the homology of a simplicial complex. By interpreting the influence as a probability measure, we define an entropy that reflects the complexity of the data manifold. Our empirical findings demonstrate that higher influence scores of samples on the 0-dimensional homology have a greater impact on the accuracy of neural networks for determining graph connectivity and on learning more challenging regular grammars with higher entropy values.",1
"Efforts are underway to study ways via which the power of deep neural networks can be extended to non-standard data types such as structured data (e.g., graphs) or manifold-valued data (e.g., unit vectors or special matrices). Often, sizable empirical improvements are possible when the geometry of such data spaces are incorporated into the design of the model, architecture, and the algorithms. Motivated by neuroimaging applications, we study formulations where the data are {\em sequential manifold-valued measurements}. This case is common in brain imaging, where the samples correspond to symmetric positive definite matrices or orientation distribution functions. Instead of a recurrent model which poses computational/technical issues, and inspired by recent results showing the viability of dilated convolutional models for sequence prediction, we develop a dilated convolutional neural network architecture for this task. On the technical side, we show how the modules needed in our network can be derived while explicitly taking the Riemannian manifold structure into account. We show how the operations needed can leverage known results for calculating the weighted Fr\'{e}chet Mean (wFM). Finally, we present scientific results for group difference analysis in Alzheimer's disease (AD) where the groups are derived using AD pathology load: here the model finds several brain fiber bundles that are related to AD even when the subjects are all still cognitively healthy.",0
"Current efforts are being made to explore methods that can expand the capabilities of deep neural networks to handle non-standard data types, such as structured data (e.g., graphs) or manifold-valued data (e.g., unit vectors or special matrices). Significant improvements can often be achieved by incorporating the geometry of these data spaces into the model's design, architecture, and algorithms. In this study, we focus on sequential manifold-valued measurements, which are common in neuroimaging applications, especially in brain imaging, where the samples correspond to symmetric positive definite matrices or orientation distribution functions. Instead of utilizing a recurrent model, which may pose computational and technical issues, we develop a dilated convolutional neural network architecture for this task, inspired by recent results demonstrating the effectiveness of dilated convolutional models for sequence prediction. We demonstrate how the necessary network modules can be derived by explicitly considering the Riemannian manifold structure and leveraging known results for calculating the weighted Fr\'{e}chet Mean (wFM). Our scientific findings show group differences in Alzheimer's disease (AD), where the groups are derived using AD pathology load. The model identifies several brain fiber bundles related to AD, even in subjects who are still cognitively healthy.",1
"In this paper, we propose Continuous Graph Flow, a generative continuous flow based method that aims to model complex distributions of graph-structured data. Once learned, the model can be applied to an arbitrary graph, defining a probability density over the random variables represented by the graph. It is formulated as an ordinary differential equation system with shared and reusable functions that operate over the graphs. This leads to a new type of neural graph message passing scheme that performs continuous message passing over time. This class of models offers several advantages: a flexible representation that can generalize to variable data dimensions; ability to model dependencies in complex data distributions; reversible and memory-efficient; and exact and efficient computation of the likelihood of the data. We demonstrate the effectiveness of our model on a diverse set of generation tasks across different domains: graph generation, image puzzle generation, and layout generation from scene graphs. Our proposed model achieves significantly better performance compared to state-of-the-art models.",0
"The paper presents a novel approach called Continuous Graph Flow, which is a generative method that utilizes continuous flow to model intricate distributions of data structured as graphs. The model can be applied to any graph and defines a probability density over the represented random variables. The method is developed as a system of ordinary differential equations that uses shared and reusable functions to operate over the graphs. This leads to a distinctive neural graph message passing scheme that performs continuous message passing over time. The model has several benefits, including a versatile representation that can scale to variable data dimensions, the ability to model dependencies in complex data distributions, reversibility and memory efficiency, and efficient computation of data likelihood. The effectiveness of the model is demonstrated through various generation tasks in different domains, such as graph generation, image puzzle generation, and layout generation from scene graphs. The proposed model achieves significantly better performance than existing state-of-the-art models.",1
"Processing an input signal that contains arbitrary structures, e.g., superpixels and point clouds, remains a big challenge in computer vision. Linear diffusion, an effective model for image processing, has been recently integrated with deep learning algorithms. In this paper, we propose to learn pairwise relations among data points in a global fashion to improve semantic segmentation with arbitrarily-structured data, through spatial generalized propagation networks (SGPN). The network propagates information on a group of graphs, which represent the arbitrarily-structured data, through a learned, linear diffusion process. The module is flexible to be embedded and jointly trained with many types of networks, e.g., CNNs. We experiment with semantic segmentation networks, where we use our propagation module to jointly train on different data -- images, superpixels and point clouds. We show that SGPN consistently improves the performance of both pixel and point cloud segmentation, compared to networks that do not contain this module. Our method suggests an effective way to model the global pairwise relations for arbitrarily-structured data.",0
"The task of processing input signals containing structures such as superpixels and point clouds is a significant challenge in computer vision. Recently, deep learning algorithms have integrated linear diffusion, a model effective for image processing. In this study, we propose utilizing spatial generalized propagation networks (SGPN) to learn the pairwise relations among data points in a global fashion to improve semantic segmentation with arbitrarily-structured data. The SGPN network propagates information through a learned, linear diffusion process on a group of graphs representing the data's structure. This module can be embedded and jointly trained with various networks, including CNNs. We conducted experiments with semantic segmentation networks, using our propagation module to jointly train on different data types, such as images, superpixels, and point clouds. Our results show that SGPN consistently improves the performance of both pixel and point cloud segmentation compared to networks without this module. Our study offers an effective way to model global pairwise relations for arbitrarily-structured data.",1
"Metadata are general characteristics of the data in a well-curated and condensed format, and have been proven to be useful for decision making, knowledge discovery, and also heterogeneous data organization of biobank. Among all data types in the biobank, pathology is the key component of the biobank and also serves as the gold standard of diagnosis. To maximize the utility of biobank and allow the rapid progress of biomedical science, it is essential to organize the data with well-populated pathology metadata. However, manual annotation of such information is tedious and time-consuming. In the study, we develop a multimodal multitask learning framework to predict four major slide-level metadata of pathology images. The framework learns generalizable representations across tissue slides, pathology reports, and case-level structured data. We demonstrate improved performance across all four tasks with the proposed method compared to a single modal single task baseline on two test sets, one external test set from a distinct data source (TCGA) and one internal held-out test set (TTH). In the test sets, the performance improvements on the averaged area under receiver operating characteristic curve across the four tasks are 16.48% and 9.05% on TCGA and TTH, respectively. Such pathology metadata prediction system may be adopted to mitigate the effort of expert annotation and ultimately accelerate the data-driven research by better utilization of the pathology biobank.",0
"Metadata refers to concise and well-curated information about data, which can aid in decision-making, knowledge discovery, and organizing heterogeneous data in biobanks. Pathology is a crucial component of biobanks, serving as the gold standard for diagnosis. To optimize the use of biobank and expedite biomedical research, it is necessary to organize data with well-populated pathology metadata. However, manual annotation of such information can be tedious and time-consuming. Our study presents a multi-modal multitask learning framework that predicts four major slide-level metadata of pathology images. The framework learns generalizable representations across tissue slides, pathology reports, and case-level structured data, leading to improved performance across all four tasks compared to a single modal single task baseline. We demonstrate the effectiveness of our approach on two test sets, one external from a distinct data source and one internal held-out test set. The proposed pathology metadata prediction system has the potential to reduce the burden of expert annotation and facilitate data-driven research by better utilizing pathology biobanks.",1
"Machine learning and deep learning have gained popularity and achieved immense success in Drug discovery in recent decades. Historically, machine learning and deep learning models were trained on either structural data or chemical properties by separated model. In this study, we proposed an architecture training simultaneously both type of data in order to improve the overall performance. Given the molecular structure in the form of SMILES notation and their label, we generated the SMILES-based feature matrix and molecular descriptors. These data were trained on a deep learning model which was also integrated with the Attention mechanism to facilitate training and interpreting. Experiments showed that our model could raise the performance of prediction comparing to the reference. With the maximum MCC 0.58 and AUC 90% by cross-validation on EGFR inhibitors dataset, our architecture was outperforming the referring model. We also successfully integrated Attention mechanism into our model, which helped to interpret the contribution of chemical structures on bioactivity.",0
"In recent years, Drug discovery has seen a surge in popularity and success with the implementation of machine learning and deep learning. Previously, these models were trained separately on either structural data or chemical properties. However, our study proposes an architecture that trains on both types of data simultaneously, resulting in improved overall performance. By using SMILES notation and molecular descriptors to generate a SMILES-based feature matrix, we were able to train a deep learning model. This model was integrated with the Attention mechanism to aid in training and interpretation. Our experiments demonstrated that our model outperformed the reference, achieving a maximum MCC of 0.58 and AUC of 90% through cross-validation on the EGFR inhibitors dataset. Additionally, we successfully integrated the Attention mechanism into our model, which helped to interpret the contribution of chemical structures on bioactivity.",1
"Graph Convolutional Neural Networks (GCNNs) are generalizations of CNNs to graph-structured data, in which convolution is guided by the graph topology. In many cases where graphs are unavailable, existing methods manually construct graphs or learn task-driven adaptive graphs. In this paper, we propose Graph Learning Neural Networks (GLNNs), which exploit the optimization of graphs (the adjacency matrix in particular) from both data and tasks. Leveraging on spectral graph theory, we propose the objective of graph learning from a sparsity constraint, properties of a valid adjacency matrix as well as a graph Laplacian regularizer via maximum a posteriori estimation. The optimization objective is then integrated into the loss function of the GCNN, which adapts the graph topology to not only labels of a specific task but also the input data. Experimental results show that our proposed GLNN outperforms state-of-the-art approaches over widely adopted social network datasets and citation network datasets for semi-supervised classification.",0
"The Graph Convolutional Neural Networks (GCNNs) extend the CNNs to deal with data in graph format, where the convolution follows the graph structure. In scenarios where graphs are not readily available, current techniques include creating graphs manually or creating task-specific adaptive graphs. This study introduces the Graph Learning Neural Networks (GLNNs) that utilize the optimization of graphs, specifically the adjacency matrix, from both data and tasks. By utilizing spectral graph theory, the GLNNs use a sparsity constraint, valid adjacency matrix properties, and a graph Laplacian regularizer via maximum a posteriori estimation to propose an objective for graph learning. This objective is then combined with the GCNN's loss function to adapt the graph topology to the input data and the specific task's labels. The experimental results demonstrate that the proposed GLNN outperforms the current state-of-the-art approaches on popular social network and citation network datasets for semi-supervised classification.",1
"Graph neural networks (GNN) has been successfully applied to operate on the graph-structured data. Given a specific scenario, rich human expertise and tremendous laborious trials are usually required to identify a suitable GNN architecture. It is because the performance of a GNN architecture is significantly affected by the choice of graph convolution components, such as aggregate function and hidden dimension. Neural architecture search (NAS) has shown its potential in discovering effective deep architectures for learning tasks in image and language modeling. However, existing NAS algorithms cannot be directly applied to the GNN search problem. First, the search space of GNN is different from the ones in existing NAS work. Second, the representation learning capacity of GNN architecture changes obviously with slight architecture modifications. It affects the search efficiency of traditional search methods. Third, widely used techniques in NAS such as parameter sharing might become unstable in GNN.   To bridge the gap, we propose the automated graph neural networks (AGNN) framework, which aims to find an optimal GNN architecture within a predefined search space. A reinforcement learning based controller is designed to greedily validate architectures via small steps. AGNN has a novel parameter sharing strategy that enables homogeneous architectures to share parameters, based on a carefully-designed homogeneity definition. Experiments on real-world benchmark datasets demonstrate that the GNN architecture identified by AGNN achieves the best performance, comparing with existing handcrafted models and tradistional search methods.",0
"The successful application of Graph neural networks (GNN) on graph-structured data requires expert knowledge and extensive experimentation to identify a suitable architecture due to the significant impact of graph convolution components, such as aggregate function and hidden dimension. While Neural architecture search (NAS) has been effective in discovering deep architectures for image and language modeling tasks, it cannot be directly applied to the GNN search problem as the search space and representation learning capacity of GNN architecture are different and slight modifications can affect search efficiency. Additionally, parameter sharing techniques may become unstable in GNN. To address these challenges, we propose the automated graph neural networks (AGNN) framework that uses a reinforcement learning based controller to find the optimal GNN architecture within a predefined search space. AGNN also incorporates a novel parameter sharing strategy that allows homogeneous architectures to share parameters based on a carefully-defined homogeneity definition. Our experiments on real-world benchmark datasets demonstrate that AGNN outperforms existing handcrafted models and traditional search methods in identifying the best GNN architecture.",1
The paper discusses a pooling mechanism to induce subsampling in graph structured data and introduces it as a component of a graph convolutional neural network. The pooling mechanism builds on the Non-Negative Matrix Factorization (NMF) of a matrix representing node adjacency and node similarity as adaptively obtained through the vertices embedding learned by the model. Such mechanism is applied to obtain an incrementally coarser graph where nodes are adaptively pooled into communities based on the outcomes of the non-negative factorization. The empirical analysis on graph classification benchmarks shows how such coarsening process yields significant improvements in the predictive performance of the model with respect to its non-pooled counterpart.,0
"In this paper, a method for inducing subsampling in graph structured data is discussed. The method involves using a pooling mechanism, which is introduced as a component of a graph convolutional neural network. The pooling mechanism is based on the Non-Negative Matrix Factorization (NMF) of a matrix that represents node adjacency and node similarity. This matrix is adaptively obtained through the vertices embedding learned by the model. The pooling mechanism is used to create a coarser graph by adaptively pooling nodes into communities based on the outcomes of the non-negative factorization. The empirical analysis conducted on graph classification benchmarks demonstrates that this coarsening process significantly improves the predictive performance of the model compared to its non-pooled counterpart.",1
"Learning graph-structured data with graph neural networks (GNNs) has been recently emerging as an important field because of its wide applicability in bioinformatics, chemoinformatics, social network analysis and data mining. Recent GNN algorithms are based on neural message passing, which enables GNNs to integrate local structures and node features recursively. However, past GNN algorithms based on 1-hop neighborhood neural message passing are exposed to a risk of loss of information on local structures and relationships. In this paper, we propose Neighborhood Edge AggregatoR (NEAR), a novel framework that aggregates relations between the nodes in the neighborhood via edges. NEAR, which can be orthogonally combined with previous GNN algorithms, gives integrated information that describes which nodes in the neighborhood are connected. Therefore, GNNs combined with NEAR reflect each node's local structure beyond the nodes themselves. Experimental results on multiple graph classification tasks show that our algorithm achieves state-of-the-art results.",0
"The field of graph neural networks (GNNs) has gained significance due to its versatility in various domains, such as bioinformatics, chemoinformatics, social network analysis, and data mining. Neural message passing is the foundation of recent GNN algorithms, which allows them to incorporate local structures and node features recursively. However, GNN algorithms based on 1-hop neighborhood neural message passing have a potential risk of losing information on local structures and relationships. This article introduces a new framework called Neighborhood Edge AggregatoR (NEAR), which aggregates relations between nodes in the neighborhood through edges. NEAR can be combined with previous GNN algorithms to provide comprehensive information on the connectivity of nodes in the neighborhood. This approach enables GNNs combined with NEAR to capture the local structure of each node beyond themselves. Our experiments on multiple graph classification tasks show that our algorithm outperforms existing methods in terms of accuracy.",1
"Existing deep learning models may encounter great challenges in handling graph structured data. In this paper, we introduce a new deep learning model for graph data specifically, namely the deep loopy neural network. Significantly different from the previous deep models, inside the deep loopy neural network, there exist a large number of loops created by the extensive connections among nodes in the input graph data, which makes model learning an infeasible task. To resolve such a problem, in this paper, we will introduce a new learning algorithm for the deep loopy neural network specifically. Instead of learning the model variables based on the original model, in the proposed learning algorithm, errors will be back-propagated through the edges in a group of extracted spanning trees. Extensive numerical experiments have been done on several real-world graph datasets, and the experimental results demonstrate the effectiveness of both the proposed model and the learning algorithm in handling graph data.",0
"Graph structured data can pose significant challenges for existing deep learning models. To address this issue, we propose a new deep learning model called the deep loopy neural network, specifically designed for graph data. Unlike previous deep models, the deep loopy neural network contains numerous loops created by the extensive connections among nodes in the input graph data, making model learning a challenging task. To overcome this problem, we introduce a new learning algorithm for the deep loopy neural network. In this algorithm, errors are back-propagated through the edges in a group of extracted spanning trees, instead of learning the model variables based on the original model. We conducted extensive numerical experiments on several real-world graph datasets, and the results demonstrate the effectiveness of both the proposed model and the learning algorithm in handling graph data.",1
"Non-linear kernel methods can be approximated by fast linear ones using suitable explicit feature maps allowing their application to large scale problems. We investigate how convolution kernels for structured data are composed from base kernels and construct corresponding feature maps. On this basis we propose exact and approximative feature maps for widely used graph kernels based on the kernel trick. We analyze for which kernels and graph properties computation by explicit feature maps is feasible and actually more efficient. In particular, we derive approximative, explicit feature maps for state-of-the-art kernels supporting real-valued attributes including the GraphHopper and graph invariant kernels. In extensive experiments we show that our approaches often achieve a classification accuracy close to the exact methods based on the kernel trick, but require only a fraction of their running time. Moreover, we propose and analyze algorithms for computing random walk, shortest-path and subgraph matching kernels by explicit and implicit feature maps. Our theoretical results are confirmed experimentally by observing a phase transition when comparing running time with respect to label diversity, walk lengths and subgraph size, respectively.",0
"By utilizing appropriate explicit feature maps, non-linear kernel approaches can be approximated by faster linear ones, which enables their use in large-scale problems. Our research focuses on the construction of feature maps for convolution kernels for structured data, which are composed of base kernels. We suggest exact and approximative feature maps for commonly used graph kernels based on the kernel trick, and analyze the feasibility and efficiency of explicit feature maps for various kernels and graph properties. We also derive approximative, explicit feature maps for state-of-the-art kernels that support real-valued attributes, such as the GraphHopper and graph invariant kernels. Through extensive experimentation, we demonstrate that our approaches frequently achieve a classification accuracy similar to the kernel trick's exact methods, but with only a fraction of the running time. Additionally, we present and evaluate algorithms for computing random walk, shortest-path, and subgraph matching kernels using both explicit and implicit feature maps. Our theoretical findings are supported by experimental results that show a phase transition in running time relative to label diversity, walk lengths, and subgraph size.",1
"Many machine learning models can be attacked with adversarial examples, i.e. inputs close to correctly classified examples that are classified incorrectly. However, most research on adversarial attacks to date is limited to vectorial data, in particular image data. In this contribution, we extend the field by introducing adversarial edit attacks for tree-structured data with potential applications in medicine and automated program analysis. Our approach solely relies on the tree edit distance and a logarithmic number of black-box queries to the attacked classifier without any need for gradient information. We evaluate our approach on two programming and two biomedical data sets and show that many established tree classifiers, like tree-kernel-SVMs and recursive neural networks, can be attacked effectively.",0
"Adversarial examples, which are inputs that are similar to correctly classified examples but lead to incorrect classification, can be used to attack many machine learning models. However, most research on this topic has only focused on vectorial data, specifically image data. This paper introduces adversarial edit attacks for tree-structured data, which could have potential applications in medicine and automated program analysis. Our approach relies solely on the tree edit distance and a limited number of black-box queries to the attacked classifier. No gradient information is required. We evaluated our approach on two programming and two biomedical data sets and found that many established tree classifiers can be effectively attacked, including tree-kernel-SVMs and recursive neural networks.",1
"Recent works reveal that network embedding techniques enable many machine learning models to handle diverse downstream tasks on graph structured data. However, as previous methods usually focus on learning embeddings for a single network, they can not learn representations transferable on multiple networks. Hence, it is important to design a network embedding algorithm that supports downstream model transferring on different networks, known as domain adaptation. In this paper, we propose a novel Domain Adaptive Network Embedding framework, which applies graph convolutional network to learn transferable embeddings. In DANE, nodes from multiple networks are encoded to vectors via a shared set of learnable parameters so that the vectors share an aligned embedding space. The distribution of embeddings on different networks are further aligned by adversarial learning regularization. In addition, DANE's advantage in learning transferable network embedding can be guaranteed theoretically. Extensive experiments reflect that the proposed framework outperforms other state-of-the-art network embedding baselines in cross-network domain adaptation tasks.",0
"New research shows that network embedding techniques can help many machine learning models handle various downstream tasks involving graph structured data. However, previous methods have primarily focused on learning embeddings for a single network, which means they cannot learn representations that can be transferred across multiple networks. As a result, it is important to create a network embedding algorithm that supports downstream model transferring on different networks, which is known as domain adaptation. This paper proposes a novel Domain Adaptive Network Embedding framework that utilizes graph convolutional network to learn transferable embeddings. DANE encodes nodes from multiple networks to vectors using a shared set of learnable parameters, allowing the vectors to share an aligned embedding space. The distribution of embeddings on different networks is further aligned through adversarial learning regularization. Additionally, DANE's ability to learn transferable network embedding can be theoretically guaranteed. Extensive experiments demonstrate that the proposed framework outperforms other state-of-the-art network embedding baselines in cross-network domain adaptation tasks.",1
Kernels for structured data are commonly obtained by decomposing objects into their parts and adding up the similarities between all pairs of parts measured by a base kernel. Assignment kernels are based on an optimal bijection between the parts and have proven to be an effective alternative to the established convolution kernels. We explore how the base kernel can be learned as part of the classification problem. We build on the theory of valid assignment kernels derived from hierarchies defined on the parts. We show that the weights of this hierarchy can be optimized via multiple kernel learning. We apply this result to learn vertex similarities for the Weisfeiler-Lehman optimal assignment kernel for graph classification. We present first experimental results which demonstrate the feasibility and effectiveness of the approach.,0
"One way to create kernels for structured data is by breaking objects down into their component parts and calculating the similarities between all pairs of parts using a base kernel. Another option is to use assignment kernels, which rely on a perfect bijection between the parts and have been shown to be a viable alternative to traditional convolution kernels. In this study, we investigate the possibility of learning the base kernel as part of the classification process. To achieve this, we draw on the concept of valid assignment kernels, which are based on hierarchies that describe the parts. We demonstrate that the weights of this hierarchy can be optimized through multiple kernel learning, and we apply this approach to the development of vertex similarities for the Weisfeiler-Lehman optimal assignment kernel used in graph classification. Our preliminary experimental results indicate that this method is both feasible and effective.",1
"The recent proliferation of publicly available graph-structured data has sparked an interest in machine learning algorithms for graph data. Since most traditional machine learning algorithms assume data to be tabular, embedding algorithms for mapping graph data to real-valued vector spaces has become an active area of research. Existing graph embedding approaches are based purely on structural information and ignore any semantic information from the underlying domain. In this paper, we demonstrate that semantic information can play a useful role in computing graph embeddings. Specifically, we present a framework for devising embedding strategies aware of domain-specific interpretations of graph nodes and edges, and use knowledge of downstream machine learning tasks to identify relevant graph substructures. Using two real-life domains, we show that our framework yields embeddings that are simple to implement and yet achieve equal or greater accuracy in machine learning tasks compared to domain independent approaches.",0
"The availability of graph-structured data to the public has led to an interest in machine learning algorithms for such data. Traditional machine learning algorithms assume tabular data, which has led to the development of embedding algorithms that map graph data to real-valued vector spaces. However, current graph embedding approaches only consider structural information and disregard any semantic information from the underlying domain. This paper aims to demonstrate that semantic information can be valuable in computing graph embeddings. The authors present a framework that considers domain-specific interpretations of graph nodes and edges and uses knowledge of downstream machine learning tasks to identify relevant graph substructures. The framework achieves equal or greater accuracy in machine learning tasks compared to domain independent approaches and is simple to implement, as demonstrated in two real-life domains.",1
"In this paper we propose Structuring AutoEncoders (SAE). SAEs are neural networks which learn a low dimensional representation of data which are additionally enriched with a desired structure in this low dimensional space. While traditional Autoencoders have proven to structure data naturally they fail to discover semantic structure that is hard to recognize in the raw data. The SAE solves the problem by enhancing a traditional Autoencoder using weak supervision to form a structured latent space. In the experiments we demonstrate, that the structured latent space allows for a much more efficient data representation for further tasks such as classification for sparsely labeled data, an efficient choice of data to label, and morphing between classes. To demonstrate the general applicability of our method, we show experiments on the benchmark image datasets MNIST, Fashion-MNIST, DeepFashion2 and on a dataset of 3D human shapes.",0
"The Structuring AutoEncoders (SAE) are introduced in this paper. These neural networks learn a low dimensional representation of data, enriched with a desired structure in this space. Although traditional Autoencoders can structure data naturally, they may not recognize semantic structure in the raw data. The SAE overcomes this issue by using weak supervision to enhance a traditional Autoencoder, forming a structured latent space. Through experiments, we demonstrate that the structured latent space efficiently represents data for further tasks, including classification for sparsely labeled data, selecting data to label, and morphing between classes. We apply our method to various datasets, including MNIST, Fashion-MNIST, DeepFashion2, and a dataset of 3D human shapes, demonstrating its general applicability.",1
"Architectures for sparse hierarchical representation learning have recently been proposed for graph-structured data, but so far assume the absence of edge features in the graph. We close this gap and propose a method to pool graphs with edge features, inspired by the hierarchical nature of chemistry. In particular, we introduce two types of pooling layers compatible with an edge-feature graph-convolutional architecture and investigate their performance for molecules relevant to drug discovery on a set of two classification and two regression benchmark datasets of MoleculeNet. We find that our models significantly outperform previous benchmarks on three of the datasets and reach state-of-the-art results on the fourth benchmark, with pooling improving performance for three out of four tasks, keeping performance stable on the fourth task, and generally speeding up the training process.",0
"Recently, there have been proposals for architectures that enable sparse hierarchical representation learning for graph-structured data. However, these proposals assume that the graph has no edge features. To address this limitation, we introduce a method for pooling graphs with edge features that is inspired by the hierarchical structure of chemistry. We present two types of pooling layers that are compatible with an edge-feature graph-convolutional architecture. We evaluate the performance of our approach on a set of two classification and two regression benchmark datasets of MoleculeNet, which are relevant to drug discovery. Our models outperform previous benchmarks on three of the datasets and achieve state-of-the-art results on the fourth benchmark. We also find that pooling improves performance for three out of four tasks, while keeping performance stable on the fourth task, and speeds up the training process.",1
"Neural networks have many successful applications, while much less theoretical understanding has been gained. Towards bridging this gap, we study the problem of learning a two-layer overparameterized ReLU neural network for multi-class classification via stochastic gradient descent (SGD) from random initialization. In the overparameterized setting, when the data comes from mixtures of well-separated distributions, we prove that SGD learns a network with a small generalization error, albeit the network has enough capacity to fit arbitrary labels. Furthermore, the analysis provides interesting insights into several aspects of learning neural networks and can be verified based on empirical studies on synthetic data and on the MNIST dataset.",0
"Despite the many successful applications of neural networks, there is still a lack of theoretical understanding. To address this issue, we have conducted a study on the problem of learning a two-layer overparameterized ReLU neural network for multi-class classification using stochastic gradient descent (SGD) from random initialization. In the overparameterized scenario, if the data comes from mixtures of well-separated distributions, we have proven that SGD can learn a network with a small generalization error, even though the network can fit arbitrary labels due to its ample capacity. Our analysis yields valuable insights into various aspects of neural network learning, which can be verified through experiments on synthetic data and the MNIST dataset.",1
"Many common sequential data sources, such as source code and natural language, have a natural tree-structured representation. These trees can be generated by fitting a sequence to a grammar, yielding a hierarchical ordering of the tokens in the sequence. This structure encodes a high degree of syntactic information, making it ideal for problems such as grammar correction. However, little work has been done to develop neural networks that can operate on and exploit tree-structured data. In this paper we present the Tree-Transformer \textemdash{} a novel neural network architecture designed to translate between arbitrary input and output trees. We applied this architecture to correction tasks in both the source code and natural language domains. On source code, our model achieved an improvement of $25\%$ $\text{F}0.5$ over the best sequential method. On natural language, we achieved comparable results to the most complex state of the art systems, obtaining a $10\%$ improvement in recall on the CoNLL 2014 benchmark and the highest to date $\text{F}0.5$ score on the AESW benchmark of $50.43$.",0
"There are many data sources that follow a tree structure, including natural language and source code. This structure is created by fitting a sequence to a grammar, resulting in a hierarchical ordering of tokens in the sequence. This tree structure contains a lot of syntactic information, which makes it ideal for tasks such as grammar correction. However, there has been little research done on developing neural networks that can work with and make use of tree-structured data. This paper introduces the Tree-Transformer, a new neural network architecture that can translate between input and output trees of any type. The Tree-Transformer was tested on correction tasks in natural language and source code domains and achieved significant improvements over existing methods. For source code, the model improved F0.5 by 25%, while for natural language, it achieved results comparable to state-of-the-art systems, with a 10% improvement in recall on the CoNLL 2014 benchmark and a record-breaking F0.5 score of 50.43 on the AESW benchmark.",1
"Graph structured data provide two-fold information: graph structures and node attributes. Numerous graph-based algorithms rely on both information to achieve success in supervised tasks, such as node classification and link prediction. However, node attributes could be missing or incomplete, which significantly deteriorates the performance. The task of node attribute generation aims to generate attributes for those nodes whose attributes are completely unobserved. This task benefits many real-world problems like profiling, node classification and graph data augmentation. To tackle this task, we propose a deep adversarial learning based method to generate node attributes; called node attribute neural generator (NANG). NANG learns a unifying latent representation which is shared by both node attributes and graph structures and can be translated to different modalities. We thus use this latent representation as a bridge to convert information from one modality to another. We further introduce practical applications to quantify the performance of node attribute generation. Extensive experiments are conducted on four real-world datasets and the empirical results show that node attributes generated by the proposed method are high-qualitative and beneficial to other applications. The datasets and codes are available online.",0
"Graph structured data contains information on graph structures and node attributes, which are both crucial for successful graph-based algorithms in supervised tasks like node classification and link prediction. However, missing or incomplete node attributes can significantly impact performance. To address this, the task of node attribute generation aims to create attributes for unobserved nodes, which benefits real-world problems like profiling and graph data augmentation. Our proposed solution is NANG, a deep adversarial learning method that generates node attributes by learning a shared latent representation for both node attributes and graph structures. This representation acts as a bridge to convert information between modalities. We also introduce practical applications to measure node attribute generation performance and conduct extensive experiments on four real-world datasets, with results showing that NANG-generated attributes are high-quality and useful for other applications. The datasets and codes are available online.",1
"Graph Convolutional Networks (GCNs) are a class of general models that can learn from graph structured data. Despite being general, GCNs are admittedly inferior to convolutional neural networks (CNNs) when applied to vision tasks, mainly due to the lack of domain knowledge that is hardcoded into CNNs, such as spatially oriented translation invariant filters. However, a great advantage of GCNs is the ability to work on irregular inputs, such as superpixels of images. This could significantly reduce the computational cost of image reasoning tasks. Another key advantage inherent to GCNs is the natural ability to model multirelational data. Building upon these two promising properties, in this work, we show best practices for designing GCNs for image classification; in some cases even outperforming CNNs on the MNIST, CIFAR-10 and PASCAL image datasets.",0
"GCNs are a type of versatile models that can be trained on data structured as graphs. While they are less effective than CNNs in vision-related tasks, due to the absence of domain-specific knowledge that is hardcoded into CNNs, such as spatially-oriented translation invariant filters, GCNs have the advantage of being able to handle irregular inputs such as superpixels of images, which could significantly reduce the computational cost of image reasoning tasks. Furthermore, GCNs are naturally suited for modeling multi-relational data. This study explores the implementation of GCNs for image classification, demonstrating that in some cases, they outperform CNNs on image datasets such as MNIST, CIFAR-10, and PASCAL.",1
"Finding the biomarkers associated with ASD is helpful for understanding the underlying roots of the disorder and can lead to earlier diagnosis and more targeted treatment. A promising approach to identify biomarkers is using Graph Neural Networks (GNNs), which can be used to analyze graph structured data, i.e. brain networks constructed by fMRI. One way to interpret important features is through looking at how the classification probability changes if the features are occluded or replaced. The major limitation of this approach is that replacing values may change the distribution of the data and lead to serious errors. Therefore, we develop a 2-stage pipeline to eliminate the need to replace features for reliable biomarker interpretation. Specifically, we propose an inductive GNN to embed the graphs containing different properties of task-fMRI for identifying ASD and then discover the brain regions/sub-graphs used as evidence for the GNN classifier. We first show GNN can achieve high accuracy in identifying ASD. Next, we calculate the feature importance scores using GNN and compare the interpretation ability with Random Forest. Finally, we run with different atlases and parameters, proving the robustness of the proposed method. The detected biomarkers reveal their association with social behaviors. We also show the potential of discovering new informative biomarkers. Our pipeline can be generalized to other graph feature importance interpretation problems.",0
"To gain insight into the causes of ASD and improve diagnosis and treatment, it is valuable to identify biomarkers linked to the disorder. A promising method for identifying biomarkers is through Graph Neural Networks (GNNs), which can analyze brain networks constructed by fMRI. To interpret significant features, one can observe how classification probability changes when features are occluded or replaced. However, this approach has limitations as replacing values can lead to errors. Therefore, we developed a two-stage pipeline that does not require replacing features for reliable biomarker interpretation. Firstly, we propose an inductive GNN to embed graphs with different properties of task-fMRI to identify ASD and discover brain regions/sub-graphs as evidence for the GNN classifier. Secondly, we calculate feature importance scores using GNN and compare the interpretation ability with Random Forest. The method's robustness is tested with different atlases and parameters. The identified biomarkers are associated with social behaviors, and the pipeline has the potential to discover new informative biomarkers. Our method is applicable to other graph feature importance interpretation problems.",1
"Co-Clustering, the problem of simultaneously identifying clusters across multiple aspects of a data set, is a natural generalization of clustering to higher-order structured data. Recent convex formulations of bi-clustering and tensor co-clustering, which shrink estimated centroids together using a convex fusion penalty, allow for global optimality guarantees and precise theoretical analysis, but their computational properties have been less well studied. In this note, we present three efficient operator-splitting methods for the convex co-clustering problem: a standard two-block ADMM, a Generalized ADMM which avoids an expensive tensor Sylvester equation in the primal update, and a three-block ADMM based on the operator splitting scheme of Davis and Yin. Theoretical complexity analysis suggests, and experimental evidence confirms, that the Generalized ADMM is far more efficient for large problems.",0
"Co-Clustering refers to the identification of clusters across various aspects of a data set and is a natural extension of clustering for structured data of higher order. Convex formulations of bi-clustering and tensor co-clustering have been developed recently. These formulations allow for global optimality guarantees and precise theoretical analysis but their computational properties have not been extensively studied. This note presents three operator-splitting methods for convex co-clustering: a standard two-block ADMM, a Generalized ADMM that avoids an expensive tensor Sylvester equation in the primal update, and a three-block ADMM based on the operator splitting scheme of Davis and Yin. Theoretical complexity analysis suggests that the Generalized ADMM is more efficient for large problems, which is supported by experimental evidence.",1
"We present a versatile formulation of the convolution operation that we term a ""mapped convolution."" The standard convolution operation implicitly samples the pixel grid and computes a weighted sum. Our mapped convolution decouples these two components, freeing the operation from the confines of the image grid and allowing the kernel to process any type of structured data. As a test case, we demonstrate its use by applying it to dense inference on spherical data. We perform an in-depth study of existing spherical image convolution methods and propose an improved sampling method for equirectangular images. Then, we discuss the impact of data discretization when deriving a sampling function, highlighting drawbacks of the cube map representation for spherical data. Finally, we illustrate how mapped convolutions enable us to convolve directly on a mesh by projecting the spherical image onto a geodesic grid and training on the textured mesh. This method exceeds the state of the art for spherical depth estimation by nearly 17%. Our findings suggest that mapped convolutions can be instrumental in expanding the application scope of convolutional neural networks.",0
"Our formulation of the convolution operation, called ""mapped convolution,"" is a flexible alternative to the standard method. Unlike traditional convolution, which samples the pixel grid and calculates a weighted sum, our approach separates these two components. This frees the operation from the restrictions of the image grid, making it possible for the kernel to process any type of structured data. We demonstrate this by using the method for dense inference on spherical data. Our study examines the existing methods for spherical image convolution and proposes an improved sampling technique for equirectangular images. We also discuss the drawbacks of using a cube map representation for spherical data and the impact of data discretization when deriving a sampling function. Finally, we show how mapped convolutions enable direct mesh convolution by projecting the spherical image onto a geodesic grid and training on the textured mesh. This method outperforms the state of the art for spherical depth estimation by almost 17%. Our results indicate that mapped convolutions can significantly expand the application scope of convolutional neural networks.",1
"Neural machine translation models are used to automatically generate a document from given source code since this can be regarded as a machine translation task. Source code summarization is one of the components for automatic document generation, which generates a summary in natural language from given source code. This suggests that techniques used in neural machine translation, such as Long Short-Term Memory (LSTM), can be used for source code summarization. However, there is a considerable difference between source code and natural language: Source code is essentially {\em structured}, having loops and conditional branching, etc. Therefore, there is some obstacle to apply known machine translation models to source code.   Abstract syntax trees (ASTs) capture these structural properties and play an important role in recent machine learning studies on source code. Tree-LSTM is proposed as a generalization of LSTMs for tree-structured data. However, there is a critical issue when applying it to ASTs: It cannot handle a tree that contains nodes having an arbitrary number of children and their order simultaneously, which ASTs generally have such nodes. To address this issue, we propose an extension of Tree-LSTM, which we call \emph{Multi-way Tree-LSTM} and apply it for source code summarization. As a result of computational experiments, our proposal achieved better results when compared with several state-of-the-art techniques.",0
"Neural machine translation models are utilized for automatic document creation from a given source code, which can be viewed as a machine translation undertaking. One of the aspects of automatic document generation is source code summarization, which produces a natural language summary from the given source code. This implies that methods employed in neural machine translation, such as Long Short-Term Memory (LSTM), can be utilized for source code summarization. Nevertheless, the significant difference between source code and natural language is that source code is fundamentally structured, consisting of loops and conditional branching, among other things. Therefore, applying established machine translation models to source code is challenging. Recent machine learning studies on source code have emphasized the importance of Abstract Syntax Trees (ASTs), which capture these structural properties. Tree-LSTM is a generalization of LSTMs for tree-structured data but has a significant drawback when used on ASTs. It cannot handle a tree containing nodes with an arbitrary number of children and their order simultaneously, a characteristic commonly found in ASTs. Therefore, to address this issue, we propose an extension of Tree-LSTM called Multi-way Tree-LSTM, which we apply for source code summarization. Our proposal outperforms several state-of-the-art techniques, as demonstrated by experimental results.",1
"Extracting key information from documents, such as receipts or invoices, and preserving the interested texts to structured data is crucial in the document-intensive streamline processes of office automation in areas that includes but not limited to accounting, financial, and taxation areas. To avoid designing expert rules for each specific type of document, some published works attempt to tackle the problem by learning a model to explore the semantic context in text sequences based on the Named Entity Recognition (NER) method in the NLP field. In this paper, we propose to harness the effective information from both semantic meaning and spatial distribution of texts in documents. Specifically, our proposed model, Convolutional Universal Text Information Extractor (CUTIE), applies convolutional neural networks on gridded texts where texts are embedded as features with semantical connotations. We further explore the effect of employing different structures of convolutional neural network and propose a fast and portable structure. We demonstrate the effectiveness of the proposed method on a dataset with up to $4,484$ labelled receipts, without any pre-training or post-processing, achieving state of the art performance that is much better than the NER based methods in terms of either speed and accuracy. Experimental results also demonstrate that the proposed CUTIE model being able to achieve good performance with a much smaller amount of training data.",0
"It is essential to extract important information from documents like receipts and invoices and convert them into structured data to streamline office automation processes in various fields such as accounting, finance, and taxation. Designing expert rules for each document type can be avoided by using the Named Entity Recognition (NER) method in Natural Language Processing (NLP) to learn a model that explores the semantic context in text sequences. In this study, we propose a model called Convolutional Universal Text Information Extractor (CUTIE) that utilizes both semantic meaning and spatial distribution of texts in documents. CUTIE uses convolutional neural networks on gridded texts where texts are embedded as features with semantical connotations. We also explore the effect of employing different convolutional neural network structures and propose a fast and portable structure. We demonstrate the effectiveness of the proposed method on a dataset with up to $4,484$ labelled receipts, achieving state-of-the-art performance without any pre-training or post-processing. Our experimental results show that CUTIE can achieve good performance with a much smaller amount of training data in terms of speed and accuracy compared to NER-based methods.",1
"Compared with shallow domain adaptation, recent progress in deep domain adaptation has shown that it can achieve higher predictive performance and stronger capacity to tackle structural data (e.g., image and sequential data). The underlying idea of deep domain adaptation is to bridge the gap between source and target domains in a joint space so that a supervised classifier trained on labeled source data can be nicely transferred to the target domain. This idea is certainly intuitive and powerful, however, limited theoretical understandings have been developed to support its underpinning principle. In this paper, we have provided a rigorous framework to explain why it is possible to close the gap of the target and source domains in the joint space. More specifically, we first study the loss incurred when performing transfer learning from the source to the target domain. This provides a theory that explains and generalizes existing work in deep domain adaptation which was mainly empirical. This enables us to further explain why closing the gap in the joint space can directly minimize the loss incurred for transfer learning between the two domains. To our knowledge, this offers the first theoretical result that characterizes a direct bound on the joint space and the gain of transfer learning via deep domain adaptation",0
"Recent advancements in deep domain adaptation have shown that it can achieve higher predictive performance and stronger capacity in handling structural data, such as image and sequential data, compared to shallow domain adaptation. The basic concept of deep domain adaptation is to establish a link between the source and target domains in a joint space, allowing a supervised classifier trained on labeled source data to be smoothly transferred to the target domain. Although this idea is intuitive and powerful, there is limited theoretical understanding to support its principle. This paper presents a rigorous framework that explains the possibility of closing the gap between the target and source domains in the joint space. We studied the loss incurred during transfer learning from the source to the target domain, providing a theory that generalizes existing empirical work on deep domain adaptation. This framework explains why closing the gap in the joint space can minimize the loss incurred in transfer learning between the two domains. This work offers the first theoretical result characterizing a direct bound on the joint space and the gain of transfer learning via deep domain adaptation.",1
"Advanced methods of applying deep learning to structured data such as graphs have been proposed in recent years. In particular, studies have focused on generalizing convolutional neural networks to graph data, which includes redefining the convolution and the downsampling (pooling) operations for graphs. The method of generalizing the convolution operation to graphs has been proven to improve performance and is widely used. However, the method of applying downsampling to graphs is still difficult to perform and has room for improvement. In this paper, we propose a graph pooling method based on self-attention. Self-attention using graph convolution allows our pooling method to consider both node features and graph topology. To ensure a fair comparison, the same training procedures and model architectures were used for the existing pooling methods and our method. The experimental results demonstrate that our method achieves superior graph classification performance on the benchmark datasets using a reasonable number of parameters.",0
"Recent years have seen the development of advanced techniques for implementing deep learning on structured data like graphs. These techniques center on adapting convolutional neural networks to graph data, necessitating the redefinition of convolution and downsampling (pooling) operations. While the convolution operation has been successfully generalized to graphs and is widely used, downsampling remains challenging and in need of improvement. This paper proposes a self-attention-based graph pooling method that considers both node features and graph topology, achieving superior performance compared to existing methods on benchmark datasets with a reasonable number of parameters. The same training procedures and model architectures were used for all pooling methods to ensure a fair comparison.",1
"For many structured learning tasks, the data annotation process is complex and costly. Existing annotation schemes usually aim at acquiring completely annotated structures, under the common perception that partial structures are of low quality and could hurt the learning process. This paper questions this common perception, motivated by the fact that structures consist of interdependent sets of variables. Thus, given a fixed budget, partly annotating each structure may provide the same level of supervision, while allowing for more structures to be annotated. We provide an information theoretic formulation for this perspective and use it, in the context of three diverse structured learning tasks, to show that learning from partial structures can sometimes outperform learning from complete ones. Our findings may provide important insights into structured data annotation schemes and could support progress in learning protocols for structured tasks.",0
"The process of data annotation can be complicated and expensive for numerous structured learning assignments. The current annotation methods are designed to obtain completely annotated structures, assuming that partially annotated structures are of poor quality and may harm the learning process. However, this paper challenges this belief by highlighting that structures are made up of interdependent variable sets. Consequently, allocating a specific budget to partially annotate each structure could provide the same level of supervision, while enabling more structures to be annotated. This article presents an information theoretic approach to support this perspective and demonstrates, through three different structured learning tasks, that learning from partial structures can, at times, outperform learning from complete structures. These findings are significant for developing structured data annotation methods and improving learning protocols for structured tasks.",1
"Machine-learning driven safety-critical autonomous systems, such as self-driving cars, must be able to detect situations where its trained model is not able to make a trustworthy prediction. Often viewed as a black-box, it is non-obvious to determine when a model will make a safe decision and when it will make an erroneous, perhaps life-threatening one. Prior work on novelty detection deal with highly structured data and do not translate well to dynamic, real-world situations. This paper proposes a multi-step framework for the detection of novel scenarios in vision-based autonomous systems by leveraging information learned by the trained prediction model and a new image similarity metric. We demonstrate the efficacy of this method through experiments on a real-world driving dataset as well as on our in-house indoor racing environment.",0
"For safety-critical autonomous systems like self-driving cars that rely on machine learning, it is crucial to identify situations where their trained model cannot make reliable predictions. These systems are often considered black boxes, making it challenging to determine when they will make safe decisions or potentially dangerous ones. Previous research on novelty detection has focused on structured data and has not been effective in real-world settings. To address this issue, this study presents a multi-step framework that utilizes a new image similarity metric and the information acquired by the prediction model to detect novel scenarios in vision-based autonomous systems. The effectiveness of this approach is demonstrated through experiments on a real-world driving dataset and an in-house indoor racing environment.",1
"Graph data widely exist in many high-impact applications. Inspired by the success of deep learning in grid-structured data, graph neural network models have been proposed to learn powerful node-level or graph-level representation. However, most of the existing graph neural networks suffer from the following limitations: (1) there is limited analysis regarding the graph convolution properties, such as seed-oriented, degree-aware and order-free; (2) the node's degree-specific graph structure is not explicitly expressed in graph convolution for distinguishing structure-aware node neighborhoods; (3) the theoretical explanation regarding the graph-level pooling schemes is unclear.   To address these problems, we propose a generic degree-specific graph neural network named DEMO-Net motivated by Weisfeiler-Lehman graph isomorphism test that recursively identifies 1-hop neighborhood structures. In order to explicitly capture the graph topology integrated with node attributes, we argue that graph convolution should have three properties: seed-oriented, degree-aware, order-free. To this end, we propose multi-task graph convolution where each task represents node representation learning for nodes with a specific degree value, thus leading to preserving the degree-specific graph structure. In particular, we design two multi-task learning methods: degree-specific weight and hashing functions for graph convolution. In addition, we propose a novel graph-level pooling/readout scheme for learning graph representation provably lying in a degree-specific Hilbert kernel space. The experimental results on several node and graph classification benchmark data sets demonstrate the effectiveness and efficiency of our proposed DEMO-Net over state-of-the-art graph neural network models.",0
"Graph data is prevalent in many high-impact applications, and graph neural network models have been proposed as a way to learn powerful node-level or graph-level representation, inspired by the success of deep learning in grid-structured data. However, existing graph neural networks face several limitations. Firstly, there is limited analysis of graph convolution properties, such as seed-oriented, degree-aware and order-free. Secondly, the node's degree-specific graph structure is not explicitly expressed in graph convolution, which makes it difficult to distinguish structure-aware node neighborhoods. Lastly, the theoretical explanation regarding the graph-level pooling schemes is unclear. In order to address these limitations, we propose a generic degree-specific graph neural network called DEMO-Net. This network is motivated by the Weisfeiler-Lehman graph isomorphism test, which recursively identifies 1-hop neighborhood structures. To explicitly capture the graph topology integrated with node attributes, we argue that graph convolution should have three properties: seed-oriented, degree-aware, and order-free. We propose multi-task graph convolution, where each task represents node representation learning for nodes with a specific degree value, leading to the preservation of the degree-specific graph structure. We design two multi-task learning methods: degree-specific weight and hashing functions for graph convolution. Additionally, we propose a novel graph-level pooling/readout scheme for learning graph representation in a degree-specific Hilbert kernel space. Our proposed DEMO-Net demonstrates effectiveness and efficiency over state-of-the-art graph neural network models, as shown in experimental results on several node and graph classification benchmark data sets.",1
"Recently, graph neural networks (GNNs) have proved to be suitable in tasks on unstructured data. Particularly in tasks as community detection, node classification, and link prediction. However, most GNN models still operate with static relationships. We propose the Graph Learning Network (GLN), a simple yet effective process to learn node embeddings and structure prediction functions. Our model uses graph convolutions to propose expected node features, and predict the best structure based on them. We repeat these steps recursively to enhance the prediction and the embeddings.",0
"Graph neural networks (GNNs) have demonstrated their applicability in handling unstructured data, particularly in tasks such as community detection, node classification, and link prediction. Despite this, many GNN models still rely on static relationships. To address this issue, we present the Graph Learning Network (GLN), a straightforward yet powerful approach for learning node embeddings and structure prediction functions. Our model harnesses graph convolutions to suggest anticipated node features and determine the optimal structure based on these features. We iteratively carry out these steps to improve the prediction and embeddings.",1
"We present RL-VAE, a graph-to-graph variational autoencoder that uses reinforcement learning to decode molecular graphs from latent embeddings. Methods have been described previously for graph-to-graph autoencoding, but these approaches require sophisticated decoders that increase the complexity of training and evaluation (such as requiring parallel encoders and decoders or non-trivial graph matching). Here, we repurpose a simple graph generator to enable efficient decoding and generation of molecular graphs.",0
"RL-VAE is a variational autoencoder that converts graph-to-graph and employs reinforcement learning to decode molecular graphs from latent embeddings. Although techniques for graph-to-graph autoencoding have been previously discussed, they necessitate complex decoders, which make training and evaluation more challenging by requiring parallel encoders and decoders or non-trivial graph matching. Instead, we utilize a basic graph generator to facilitate the efficient decoding and generation of molecular graphs.",1
"Graph Neural Networks (GNNs) have proven to be successful in many classification tasks, outperforming previous state-of-the-art methods in terms of accuracy. However, accuracy alone is not enough for high-stakes decision making. Decision makers want to know the likelihood that a specific GNN prediction is correct. For this purpose, obtaining calibrated models is essential. In this work, we perform an empirical evaluation of the calibration of state-of-the-art GNNs on multiple datasets. Our experiments show that GNNs can be calibrated in some datasets but also badly miscalibrated in others, and that state-of-the-art calibration methods are helpful but do not fix the problem.",0
"The effectiveness of Graph Neural Networks (GNNs) has been extensively demonstrated in various classification tasks, surpassing previous state-of-the-art techniques in terms of accuracy. However, accuracy alone is inadequate for decision making involving high stakes. Decision makers require an understanding of the probability that a specific GNN prediction is accurate. Therefore, obtaining calibrated models is crucial. This study conducts an empirical assessment of the calibration of current GNNs on multiple datasets. Results reveal that GNNs are aptly calibrated in some datasets but severely miscalibrated in others. Furthermore, state-of-the-art calibration techniques offer assistance but do not resolve the issue.",1
"This paper extends the proof of density of neural networks in the space of continuous (or even measurable) functions on Euclidean spaces to functions on compact sets of probability measures. By doing so the work parallels a more then a decade old results on mean-map embedding of probability measures in reproducing kernel Hilbert spaces. The work has wide practical consequences for multi-instance learning, where it theoretically justifies some recently proposed constructions. The result is then extended to Cartesian products, yielding universal approximation theorem for tree-structured domains, which naturally occur in data-exchange formats like JSON, XML, YAML, AVRO, and ProtoBuffer. This has important practical implications, as it enables to automatically create an architecture of neural networks for processing structured data (AutoML paradigms), as demonstrated by an accompanied library for JSON format.",0
"This article expands upon the proof of neural network density in the realm of continuous (or measurable) functions on Euclidean spaces to encompass functions on compact sets of probability measures. This parallels work done over a decade ago on mean-map embedding of probability measures in reproducing kernel Hilbert spaces. The results have significant practical applications for multi-instance learning by justifying recently proposed constructions. Furthermore, the theorem is extended to Cartesian products, resulting in a universal approximation theorem for tree-structured domains found in data-exchange formats like JSON, XML, YAML, AVRO, and ProtoBuffer. This has important real-world implications, as it enables the automatic creation of neural network architectures for processing structured data (AutoML paradigms). An accompanying library for JSON format demonstrates this concept.",1
"Most of the successful deep neural network architectures are structured, often consisting of elements like convolutional neural networks and gated recurrent neural networks. Recently, graph neural networks have been successfully applied to graph structured data such as point cloud and molecular data. These networks often only consider pairwise dependencies, as they operate on a graph structure. We generalize the graph neural network into a factor graph neural network (FGNN) in order to capture higher order dependencies. We show that FGNN is able to represent Max-Product Belief Propagation, an approximate inference algorithm on probabilistic graphical models; hence it is able to do well when Max-Product does well. Promising results on both synthetic and real datasets demonstrate the effectiveness of the proposed model.",0
"Many successful deep neural network architectures consist of structured elements like convolutional neural networks and gated recurrent neural networks. Recently, graph neural networks have been applied to graph structured data, such as point cloud and molecular data, but these networks only consider pairwise dependencies due to operating on a graph structure. To capture higher order dependencies, we propose a factor graph neural network (FGNN). Our model can represent Max-Product Belief Propagation and performs well when Max-Product is effective. Promising results on synthetic and real datasets demonstrate the effectiveness of the proposed FGNN.",1
"Recently, a method [7] was proposed to generate contrastive explanations for differentiable models such as deep neural networks, where one has complete access to the model. In this work, we propose a method, Model Agnostic Contrastive Explanations Method (MACEM), to generate contrastive explanations for \emph{any} classification model where one is able to \emph{only} query the class probabilities for a desired input. This allows us to generate contrastive explanations for not only neural networks, but models such as random forests, boosted trees and even arbitrary ensembles that are still amongst the state-of-the-art when learning on structured data [13]. Moreover, to obtain meaningful explanations we propose a principled approach to handle real and categorical features leading to novel formulations for computing pertinent positives and negatives that form the essence of a contrastive explanation. A detailed treatment of the different data types of this nature was not performed in the previous work, which assumed all features to be positive real valued with zero being indicative of the least interesting value. We part with this strong implicit assumption and generalize these methods so as to be applicable across a much wider range of problem settings. We quantitatively and qualitatively validate our approach over 5 public datasets covering diverse domains.",0
"A recent publication introduced a technique to create contrastive explanations for differentiable models like deep neural networks, which require full access to the model. We propose a new method called Model Agnostic Contrastive Explanations Method (MACEM) that allows for generating contrastive explanations for \emph{any} classification model with only access to the class probabilities of a given input. This means that our method can be used on models like random forests, boosted trees, and other state-of-the-art ensembles that learn on structured data. We also present a principled approach for handling real and categorical features to compute pertinent positives and negatives, which are the basis of a contrastive explanation. Unlike previous methods, we do not assume that all features are positive real valued, and we validate our approach on five public datasets from diverse domains both quantitatively and qualitatively.",1
"Bottom-Up Hidden Tree Markov Model is a highly expressive model for tree-structured data. Unfortunately, it cannot be used in practice due to the intractable size of its state-transition matrix. We propose a new approximation which lies on the Tucker factorisation of tensors. The probabilistic interpretation of such approximation allows us to define a new probabilistic model for tree-structured data. Hence, we define the new approximated model and we derive its learning algorithm. Then, we empirically assess the effective power of the new model evaluating it on two different tasks. In both cases, our model outperforms the other approximated model known in the literature.",0
"The Bottom-Up Hidden Tree Markov Model is an expressive model that is not practical to use due to its state-transition matrix being too large. We suggest a new approximation method based on the Tucker factorisation of tensors, which has a probabilistic interpretation that allows us to create a new probabilistic model for tree-structured data. We define the new approximated model and its learning algorithm and evaluate its effectiveness on two tasks. Our model outperforms the other approximated model known in the literature in both cases.",1
"Much of the recent work on learning molecular representations has been based on Graph Convolution Networks (GCN). These models rely on local aggregation operations and can therefore miss higher-order graph properties. To remedy this, we propose Path-Augmented Graph Transformer Networks (PAGTN) that are explicitly built on longer-range dependencies in graph-structured data. Specifically, we use path features in molecular graphs to create global attention layers. We compare our PAGTN model against the GCN model and show that our model consistently outperforms GCNs on molecular property prediction datasets including quantum chemistry (QM7, QM8, QM9), physical chemistry (ESOL, Lipophilictiy) and biochemistry (BACE, BBBP).",0
"Recent studies on molecular representation learning have predominantly utilized Graph Convolution Networks (GCN). However, these models may overlook higher-order graph characteristics due to their dependence on local aggregation operations. To address this limitation, we present Path-Augmented Graph Transformer Networks (PAGTN), which explicitly incorporate longer-range dependencies in graph-structured data. Our approach involves utilizing path features in molecular graphs to construct global attention layers. Through comparisons with the GCN model, we demonstrate that our PAGTN model consistently outperforms GCNs in predicting molecular properties across various datasets in quantum chemistry (QM7, QM8, QM9), physical chemistry (ESOL, Lipophilicity), and biochemistry (BACE, BBBP).",1
"Graph neural networks (GNNs) have achieved lots of success on graph-structured data. In the light of this, there has been increasing interest in studying their representation power. One line of work focuses on the universal approximation of permutation-invariant functions by certain classes of GNNs, and another demonstrates the limitation of GNNs via graph isomorphism tests.   Our work connects these two perspectives and proves their equivalence. We further develop a framework of the representation power of GNNs with the language of sigma-algebra, which incorporates both viewpoints. Using this framework, we compare the expressive power of different classes of GNNs as well as other methods on graphs. In particular, we prove that order-2 Graph G-invariant networks fail to distinguish non-isomorphic regular graphs with the same degree. We then extend them to a new architecture, Ring-GNNs, which succeeds on distinguishing these graphs and provides improvements on real-world social network datasets.",0
"Graph neural networks (GNNs) have been highly successful in analyzing graph-structured data, leading to increased interest in exploring their representation abilities. Some researchers have investigated the capacity of certain classes of GNNs to approximate permutation-invariant functions universally, while others have used graph isomorphism tests to demonstrate the limitations of GNNs. Our research links these two viewpoints and demonstrates their equivalence through a sigma-algebra framework. We utilize this framework to compare the expressive power of various GNN classes and other graph analysis methods. Our findings reveal that order-2 Graph G-invariant networks are unable to differentiate between non-isomorphic regular graphs with similar degrees. However, we introduce a new architecture, Ring-GNNs, that overcomes this limitation and delivers significant improvements in analyzing real-world social network datasets.",1
"We propose GraphNVP, the first invertible, normalizing flow-based molecular graph generation model. We decompose the generation of a graph into two steps: generation of (i) an adjacency tensor and (ii) node attributes. This decomposition yields the exact likelihood maximization on graph-structured data, combined with two novel reversible flows. We empirically demonstrate that our model efficiently generates valid molecular graphs with almost no duplicated molecules. In addition, we observe that the learned latent space can be used to generate molecules with desired chemical properties.",0
"Our novel model, GraphNVP, offers the first normalizing flow-based molecular graph generation to be invertible. To generate a graph, we divide the process into two steps: generating an adjacency tensor and generating node attributes. This approach enables us to obtain an exact likelihood maximization on graph-structured data, utilizing two unique reversible flows. Through our experiments, we demonstrate that our model generates valid molecular graphs with minimal duplication efficiently. Moreover, we observe that the latent space learned by the model can be employed to generate molecules with specific chemical properties.",1
"Many machine learning tasks such as multiple instance learning, 3D shape recognition, and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the order of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces the computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating the state-of-the-art performance compared to recent methods for set-structured data.",0
"Several machine learning tasks, including 3D shape recognition, few-shot image classification, and multiple instance learning, operate on sets of instances. As the order of set elements does not affect the solutions to such problems, the models used to solve them must be permutation invariant. Our study introduces the Set Transformer, an attention-based neural network module created for the express purpose of modeling interactions among input set elements. The module comprises an encoder and decoder, both of which employ attention mechanisms. We leverage an attention scheme inspired by sparse Gaussian process literature's inducing point methods to reduce computational complexity and self-attention computation time from quadratic to linear in the number of set elements. Our theoretical analysis of the model demonstrates its attractiveness, and we evaluate it on various tasks, demonstrating its state-of-the-art performance compared to recent set-structured data methods.",1
"Graph neural networks have become one of the most important techniques to solve machine learning problems on graph-structured data. Recent work on vertex classification proposed deep and distributed learning models to achieve high performance and scalability. However, we find that the feature vectors of benchmark datasets are already quite informative for the classification task, and the graph structure only provides a means to denoise the data. In this paper, we develop a theoretical framework based on graph signal processing for analyzing graph neural networks. Our results indicate that graph neural networks only perform low-pass filtering on feature vectors and do not have the non-linear manifold learning property. We further investigate their resilience to feature noise and propose some insights on GCN-based graph neural network design.",0
"The use of graph neural networks is crucial in solving machine learning problems that involve graph-structured data. Researchers have proposed deep and distributed learning models for vertex classification, which have resulted in high performance and scalability. However, it has been discovered that the feature vectors of benchmark datasets are already informative enough for the classification task, and the graph structure only serves to reduce noise. This paper introduces a theoretical framework based on graph signal processing to analyze graph neural networks. The results show that graph neural networks only perform low-pass filtering on feature vectors and lack the non-linear manifold learning property. Moreover, the paper examines their resistance to feature noise and provides insights on the design of GCN-based graph neural networks.",1
"Auto-encoders have emerged as a successful framework for unsupervised learning. However, conventional auto-encoders are incapable of utilizing explicit relations in structured data. To take advantage of relations in graph-structured data, several graph auto-encoders have recently been proposed, but they neglect to reconstruct either the graph structure or node attributes. In this paper, we present the graph attention auto-encoder (GATE), a neural network architecture for unsupervised representation learning on graph-structured data. Our architecture is able to reconstruct graph-structured inputs, including both node attributes and the graph structure, through stacked encoder/decoder layers equipped with self-attention mechanisms. In the encoder, by considering node attributes as initial node representations, each layer generates new representations of nodes by attending over their neighbors' representations. In the decoder, we attempt to reverse the encoding process to reconstruct node attributes. Moreover, node representations are regularized to reconstruct the graph structure. Our proposed architecture does not need to know the graph structure upfront, and thus it can be applied to inductive learning. Our experiments demonstrate competitive performance on several node classification benchmark datasets for transductive and inductive tasks, even exceeding the performance of supervised learning baselines in most cases.",0
"Auto-encoders have been successful in unsupervised learning, but they cannot utilize explicit relations in structured data. Several graph auto-encoders have been proposed to take advantage of relations in graph-structured data, but they fail to reconstruct the graph structure or node attributes. This paper introduces the graph attention auto-encoder (GATE), a neural network architecture that can reconstruct graph-structured inputs, including node attributes and the graph structure, through stacked encoder/decoder layers equipped with self-attention mechanisms. The encoder generates new representations of nodes by attending over their neighbors' representations, while the decoder attempts to reverse the encoding process to reconstruct node attributes. Regularization is used to reconstruct the graph structure. GATE does not require prior knowledge of the graph structure, making it suitable for inductive learning. Experimental results demonstrate competitive performance on several node classification benchmark datasets for transductive and inductive tasks, often outperforming supervised learning baselines.",1
"We present batch virtual adversarial training (BVAT), a novel regularization method for graph convolutional networks (GCNs). BVAT addresses the shortcoming of GCNs that do not consider the smoothness of the model's output distribution against local perturbations around the input. We propose two algorithms, sample-based BVAT and optimization-based BVAT, which are suitable to promote the smoothness of the model for graph-structured data by either finding virtual adversarial perturbations for a subset of nodes far from each other or generating virtual adversarial perturbations for all nodes with an optimization process. Extensive experiments on three citation network datasets Cora, Citeseer and Pubmed and a knowledge graph dataset Nell validate the effectiveness of the proposed method, which establishes state-of-the-art results in the semi-supervised node classification tasks.",0
"The article introduces a new regularization technique called batch virtual adversarial training (BVAT) for graph convolutional networks (GCNs). The method aims to overcome GCN's limitation of not considering the smoothness of the model's output distribution against local perturbations around the input. The paper presents two algorithms, sample-based BVAT and optimization-based BVAT, both suitable for promoting the smoothness of the model for graph-structured data. The former finds virtual adversarial perturbations for a subset of nodes far from each other, while the latter generates virtual adversarial perturbations for all nodes using an optimization process. The proposed method is evaluated on four datasets, and the results show that it outperforms existing techniques in semi-supervised node classification tasks.",1
"The construction of a meaningful graph topology plays a crucial role in the effective representation, processing, analysis and visualization of structured data. When a natural choice of the graph is not readily available from the data sets, it is thus desirable to infer or learn a graph topology from the data. In this tutorial overview, we survey solutions to the problem of graph learning, including classical viewpoints from statistics and physics, and more recent approaches that adopt a graph signal processing (GSP) perspective. We further emphasize the conceptual similarities and differences between classical and GSP-based graph inference methods, and highlight the potential advantage of the latter in a number of theoretical and practical scenarios. We conclude with several open issues and challenges that are keys to the design of future signal processing and machine learning algorithms for learning graphs from data.",0
"The effective representation, processing, analysis, and visualization of structured data rely heavily on constructing a meaningful graph topology. If the data sets do not have a natural graph topology, inferring or learning one from the data is desirable. This tutorial overview examines solutions to the problem of graph learning, including classical viewpoints from statistics and physics, and more recent approaches using a graph signal processing (GSP) perspective. We highlight the conceptual similarities and differences between classical and GSP-based graph inference methods and discuss the potential advantages of the latter in theoretical and practical scenarios. Finally, we identify open issues and challenges that will be critical to designing future signal processing and machine learning algorithms for learning graphs from data.",1
"Graph neural networks, which generalize deep neural network models to graph structured data, have attracted increasing attention in recent years. They usually learn node representations by transforming, propagating and aggregating node features and have been proven to improve the performance of many graph related tasks such as node classification and link prediction. To apply graph neural networks for the graph classification task, approaches to generate the \textit{graph representation} from node representations are demanded. A common way is to globally combine the node representations. However, rich structural information is overlooked. Thus a hierarchical pooling procedure is desired to preserve the graph structure during the graph representation learning. There are some recent works on hierarchically learning graph representation analogous to the pooling step in conventional convolutional neural (CNN) networks. However, the local structural information is still largely neglected during the pooling process. In this paper, we introduce a pooling operator $\pooling$ based on graph Fourier transform, which can utilize the node features and local structures during the pooling process. We then design pooling layers based on the pooling operator, which are further combined with traditional GCN convolutional layers to form a graph neural network framework $\m$ for graph classification. Theoretical analysis is provided to understand $\pooling$ from both local and global perspectives. Experimental results of the graph classification task on $6$ commonly used benchmarks demonstrate the effectiveness of the proposed framework.",0
"Recently, there has been a growing interest in graph neural networks, which extend deep neural network models to work with graph structured data. These networks typically learn node representations by transforming, propagating, and aggregating node features, and have been shown to improve performance in various graph-related tasks, such as node classification and link prediction. To use graph neural networks for graph classification, there is a need to develop methods for generating a graph representation from node representations. While combining node representations globally is a common approach, it overlooks valuable structural information. Therefore, a hierarchical pooling procedure is required to preserve the graph structure during representation learning. Some recent works have used a hierarchical approach to learn graph representation, similar to the pooling step in conventional convolutional neural networks. However, these methods neglect local structural information during the pooling process. In this paper, we propose a pooling operator $\pooling$ based on graph Fourier transform, which can utilize both node features and local structures during pooling. We then design pooling layers based on this operator, which are combined with traditional GCN convolutional layers to create a graph neural network framework $\m$ for graph classification. The effectiveness of our approach is demonstrated through experimental results on six commonly used benchmarks for graph classification, and we provide theoretical analysis to understand $\pooling$ from both local and global perspectives.",1
"Benchmark data sets are an indispensable ingredient of the evaluation of graph-based machine learning methods. We release a new data set, compiled from International Planning Competitions (IPC), for benchmarking graph classification, regression, and related tasks. Apart from the graph construction (based on AI planning problems) that is interesting in its own right, the data set possesses distinctly different characteristics from popularly used benchmarks. The data set, named IPC, consists of two self-contained versions, grounded and lifted, both including graphs of large and skewedly distributed sizes, posing substantial challenges for the computation of graph models such as graph kernels and graph neural networks. The graphs in this data set are directed and the lifted version is acyclic, offering the opportunity of benchmarking specialized models for directed (acyclic) structures. Moreover, the graph generator and the labeling are computer programmed; thus, the data set may be extended easily if a larger scale is desired. The data set is accessible from \url{https://github.com/IBM/IPC-graph-data}.",0
"To assess graph-based machine learning methods, benchmark data sets are essential. We have introduced a new data set for benchmarking graph classification, regression, and related tasks, compiled from International Planning Competitions (IPC). This data set is distinct from commonly used benchmarks due to its unique graph construction (based on AI planning problems) and characteristics. The IPC data set comprises two self-contained versions, grounded and lifted, both containing large and unevenly distributed graphs that pose significant challenges for computing graph models such as graph kernels and graph neural networks. The directed graphs in this data set also provide an opportunity to benchmark specialized models for directed (acyclic) structures. Additionally, the graph generator and labeling are computer programmed, allowing for easy expansion if necessary. The IPC data set is available at \url{https://github.com/IBM/IPC-graph-data}.",1
"Performing machine learning on structured data is complicated by the fact that such data does not have vectorial form. Therefore, multiple approaches have emerged to construct vectorial representations of structured data, from kernel and distance approaches to recurrent, recursive, and convolutional neural networks. Recent years have seen heightened attention in this demanding field of research and several new approaches have emerged, such as metric learning on structured data, graph convolutional neural networks, and recurrent decoder networks for structured data. In this contribution, we provide an high-level overview of the state-of-the-art in representation learning and embeddings for structured data across a wide range of machine learning fields.",0
"The complexity of performing machine learning on structured data arises from the fact that such data lacks vectorial form. Consequently, various techniques have surfaced to create vectorial representations of structured data, ranging from kernel and distance methods to recurrent, recursive, and convolutional neural networks. Over the years, this field of research has garnered increased attention, leading to the emergence of innovative approaches like metric learning on structured data, graph convolutional neural networks, and recurrent decoder networks for structured data. This article presents a comprehensive summary of the latest advances in representation learning and embeddings for structured data across diverse machine learning domains.",1
"Obtaining continuous representations of structural data such as directed acyclic graphs (DAGs) has gained attention in machine learning and artificial intelligence. However, embedding complex DAGs in which both ancestors and descendants of nodes are exponentially increasing is difficult. Tackling in this problem, we develop Disk Embeddings, which is a framework for embedding DAGs into quasi-metric spaces. Existing state-of-the-art methods, Order Embeddings and Hyperbolic Entailment Cones, are instances of Disk Embedding in Euclidean space and spheres respectively. Furthermore, we propose a novel method Hyperbolic Disk Embeddings to handle exponential growth of relations. The results of our experiments show that our Disk Embedding models outperform existing methods especially in complex DAGs other than trees.",0
"The representation of structural data, such as directed acyclic graphs (DAGs), has become a focus in the fields of machine learning and artificial intelligence. However, embedding complex DAGs with exponentially increasing ancestors and descendants poses a challenge. To address this issue, we introduce Disk Embeddings, a framework for embedding DAGs into quasi-metric spaces. Existing state-of-the-art methods, Order Embeddings and Hyperbolic Entailment Cones, are instances of Disk Embedding in Euclidean space and spheres, respectively. Additionally, we propose a new approach, Hyperbolic Disk Embeddings, to handle the exponential growth of relations. Our experimental results demonstrate that our Disk Embedding models outperform existing methods, particularly in complex DAGs that are not trees.",1
"Stochastic blockmodels (SBM) and their variants, $e.g.$, mixed-membership and overlapping stochastic blockmodels, are latent variable based generative models for graphs. They have proven to be successful for various tasks, such as discovering the community structure and link prediction on graph-structured data. Recently, graph neural networks, $e.g.$, graph convolutional networks, have also emerged as a promising approach to learn powerful representations (embeddings) for the nodes in the graph, by exploiting graph properties such as locality and invariance. In this work, we unify these two directions by developing a \emph{sparse} variational autoencoder for graphs, that retains the interpretability of SBMs, while also enjoying the excellent predictive performance of graph neural nets. Moreover, our framework is accompanied by a fast recognition model that enables fast inference of the node embeddings (which are of independent interest for inference in SBM and its variants). Although we develop this framework for a particular type of SBM, namely the \emph{overlapping} stochastic blockmodel, the proposed framework can be adapted readily for other types of SBMs. Experimental results on several benchmarks demonstrate encouraging results on link prediction while learning an interpretable latent structure that can be used for community discovery.",0
"Generative models for graphs, such as stochastic blockmodels (SBM) and their variations, have been successful in tasks such as discovering community structure and link prediction. Meanwhile, graph neural networks, like graph convolutional networks, have emerged as a promising method for learning powerful node representations. This study presents a sparse variational autoencoder for graphs that unifies these two approaches, combining the interpretability of SBMs with the excellent predictive performance of graph neural nets. The framework also includes a fast recognition model for quick inference of node embeddings, which are useful for SBM and its variations. Though developed for overlapping SBM, the framework can be adapted easily for other SBMs. Experimental results show promising outcomes on link prediction while learning an interpretable latent structure for community discovery.",1
"This work considers the problem of computing distances between structured objects such as undirected graphs, seen as probability distributions in a specific metric space. We consider a new transportation distance (i.e. that minimizes a total cost of transporting probability masses) that unveils the geometric nature of the structured objects space. Unlike Wasserstein or Gromov-Wasserstein metrics that focus solely and respectively on features (by considering a metric in the feature space) or structure (by seeing structure as a metric space), our new distance exploits jointly both information, and is consequently called Fused Gromov-Wasserstein (FGW). After discussing its properties and computational aspects, we show results on a graph classification task, where our method outperforms both graph kernels and deep graph convolutional networks. Exploiting further on the metric properties of FGW, interesting geometric objects such as Fr\'echet means or barycenters of graphs are illustrated and discussed in a clustering context.",0
"The focus of this work is on calculating distances between structured objects, specifically undirected graphs that are viewed as probability distributions in a particular metric space. A new transportation distance is introduced, which minimizes the cost of transporting probability masses and reveals the geometric nature of the space. This distance, called Fused Gromov-Wasserstein (FGW), combines both features and structure information, unlike the Wasserstein or Gromov-Wasserstein metrics that only consider one or the other. The FGW distance is analyzed for its properties and computational aspects, and it is shown to outperform graph kernels and deep graph convolutional networks in a graph classification task. Additionally, the metric properties of FGW are used to illustrate and discuss interesting geometric objects such as Fr\'echet means and barycenters of graphs in a clustering context.",1
"This paper addresses the challenging problem of retrieval and matching of graph structured objects, and makes two key contributions. First, we demonstrate how Graph Neural Networks (GNN), which have emerged as an effective model for various supervised prediction problems defined on structured data, can be trained to produce embedding of graphs in vector spaces that enables efficient similarity reasoning. Second, we propose a novel Graph Matching Network model that, given a pair of graphs as input, computes a similarity score between them by jointly reasoning on the pair through a new cross-graph attention-based matching mechanism. We demonstrate the effectiveness of our models on different domains including the challenging problem of control-flow-graph based function similarity search that plays an important role in the detection of vulnerabilities in software systems. The experimental analysis demonstrates that our models are not only able to exploit structure in the context of similarity learning but they can also outperform domain-specific baseline systems that have been carefully hand-engineered for these problems.",0
"In this article, we tackle the difficult task of retrieving and matching graph-based objects and present two significant contributions. Firstly, we illustrate how Graph Neural Networks (GNN) can be trained to generate graph embeddings in vector spaces that allow for efficient similarity reasoning. GNN has proven effective in various supervised prediction problems that involve structured data. Secondly, we introduce a new Graph Matching Network model that calculates a similarity score between two graphs using a cross-graph attention-based matching mechanism. Our model can jointly reason on a pair of graphs, enabling it to be effective in different domains, such as control-flow-graph based function similarity search. Our experimental analysis demonstrates that our models are not only able to utilize structure when learning similarity but also outperform domain-specific baseline systems that were carefully hand-engineered for these problems.",1
"Graph classification receives a great deal of attention from the non-Euclidean machine learning community. Recent advances in graph coarsening have enabled the training of deeper networks and produced new state-of-the-art results in many benchmark tasks. We examine how these architectures train and find that performance is highly-sensitive to initialisation and depends strongly on jumping-knowledge structures. We then show that, despite the great complexity of these models, competitive performance is achieved by the simplest of models -- structure-blind MLP, single-layer GCN and fixed-weight GCN -- and propose these be included as baselines in future.",0
"The non-Euclidean machine learning community is highly interested in graph classification. With the recent development in graph coarsening, the training of deeper networks has become possible, leading to new state-of-the-art results in various benchmark tasks. Our study focuses on the training process of these architectures and discovers that initialisation greatly affects their performance and relies heavily on knowledge transfer structures. Moreover, we demonstrate that despite the complexity of these models, the simplest ones such as structure-blind MLP, single-layer GCN, and fixed-weight GCN can achieve competitive performance. Therefore, we recommend these models to be included as baseline models in future studies.",1
"Interpretable machine learning has become a strong competitor for traditional black-box models. However, the possible loss of the predictive performance for gaining interpretability is often inevitable, putting practitioners in a dilemma of choosing between high accuracy (black-box models) and interpretability (interpretable models). In this work, we propose a novel framework for building a Hybrid Predictive Model (HPM) that integrates an interpretable model with any black-box model to combine their strengths. The interpretable model substitutes the black-box model on a subset of data where the black-box is overkill or nearly overkill, gaining transparency at no or low cost of the predictive accuracy. We design a principled objective function that considers predictive accuracy, model interpretability, and model transparency (defined as the percentage of data processed by the interpretable substitute.) Under this framework, we propose two hybrid models, one substituting with association rules and the other with linear models, and we design customized training algorithms for both models. We test the hybrid models on structured data and text data where interpretable models collaborate with various state-of-the-art black-box models. Results show that hybrid models obtain an efficient trade-off between transparency and predictive performance, characterized by our proposed efficient frontiers.",0
"The rise of interpretable machine learning has become a formidable opponent to traditional black-box models. Yet, practitioners are often faced with a dilemma of choosing between high accuracy with black-box models or interpretability with interpretable models, where the latter may result in a loss of predictive performance. To address this issue, we propose a new framework for constructing a Hybrid Predictive Model (HPM) that combines the strengths of both models. Our approach involves substituting the black-box model with an interpretable model on a subset of data where the black-box becomes overly complex or nearly so, resulting in increased transparency at little or no cost to predictive accuracy. We develop a principled objective function that considers model interpretability, transparency, and predictive accuracy. We present two hybrid models, one using association rules and the other using linear models, and we design customized training algorithms for both. We evaluate our approach on structured and text data, where interpretable models work collaboratively with state-of-the-art black-box models, and show that our hybrid models achieve an optimal balance between transparency and predictive performance, as characterized by our proposed efficient frontiers.",1
"Recent years have witnessed some exciting developments in the domain of generating images from scene-based text descriptions. These approaches have primarily focused on generating images from a static text description and are limited to generating images in a single pass. They are unable to generate an image interactively based on an incrementally additive text description (something that is more intuitive and similar to the way we describe an image). We propose a method to generate an image incrementally based on a sequence of graphs of scene descriptions (scene-graphs). We propose a recurrent network architecture that preserves the image content generated in previous steps and modifies the cumulative image as per the newly provided scene information. Our model utilizes Graph Convolutional Networks (GCN) to cater to variable-sized scene graphs along with Generative Adversarial image translation networks to generate realistic multi-object images without needing any intermediate supervision during training. We experiment with Coco-Stuff dataset which has multi-object images along with annotations describing the visual scene and show that our model significantly outperforms other approaches on the same dataset in generating visually consistent images for incrementally growing scene graphs.",0
"Exciting advancements have been made in the realm of generating images from textual descriptions of scenes. However, current methods are limited to generating images from a static text description and cannot incrementally add to the image based on a developing text description. Our proposed method involves using a sequence of scene-graphs to generate an image incrementally. We utilize a recurrent network that maintains previously generated image content while modifying it based on newly provided scene information. Our model incorporates Graph Convolutional Networks to accommodate varying scene-graph sizes and Generative Adversarial image translation networks for realistic multi-object images without intermediate supervision during training. We conducted experiments using the Coco-Stuff dataset and demonstrated our model's superiority over other approaches in generating visually consistent images for growing scene-graphs.",1
"3D object recognition accuracy can be improved by learning the multi-scale spatial features from 3D spatial geometric representations of objects such as point clouds, 3D models, surfaces, and RGB-D data. Current deep learning approaches learn such features either using structured data representations (voxel grids and octrees) or from unstructured representations (graphs and point clouds). Learning features from such structured representations is limited by the restriction on resolution and tree depth while unstructured representations creates a challenge due to non-uniformity among data samples. In this paper, we propose an end-to-end multi-level learning approach on a multi-level voxel grid to overcome these drawbacks. To demonstrate the utility of the proposed multi-level learning, we use a multi-level voxel representation of 3D objects to perform object recognition. The multi-level voxel representation consists of a coarse voxel grid that contains volumetric information of the 3D object. In addition, each voxel in the coarse grid that contains a portion of the object boundary is subdivided into multiple fine-level voxel grids. The performance of our multi-level learning algorithm for object recognition is comparable to dense voxel representations while using significantly lower memory.",0
"Improved accuracy in 3D object recognition can result from learning multi-scale spatial features derived from 3D spatial geometric representations like point clouds, 3D models, surfaces, and RGB-D data. Currently, deep learning methods acquire such features either from structured data representations like voxel grids and octrees or unstructured representations like graphs and point clouds. Structured representations have limited resolution and tree depth, while unstructured representations pose a challenge due to non-uniformity among data samples. Our proposed solution is an end-to-end multi-level learning approach on a multi-level voxel grid that overcomes these limitations. We use the multi-level voxel representation of 3D objects to perform object recognition, consisting of a coarse voxel grid with volumetric information and fine-level voxel grids for each voxel containing a portion of the object boundary. Our multi-level learning algorithm for object recognition performs comparably to dense voxel representations while using less memory.",1
"Graph kernels have attracted a lot of attention during the last decade, and have evolved into a rapidly developing branch of learning on structured data. During the past 20 years, the considerable research activity that occurred in the field resulted in the development of dozens of graph kernels, each focusing on specific structural properties of graphs. Graph kernels have proven successful in a wide range of domains, ranging from social networks to bioinformatics. The goal of this survey is to provide a unifying view of the literature on graph kernels. In particular, we present a comprehensive overview of a wide range of graph kernels. Furthermore, we perform an experimental evaluation of several of those kernels on publicly available datasets, and provide a comparative study. Finally, we discuss key applications of graph kernels, and outline some challenges that remain to be addressed.",0
"Over the last decade, graph kernels have gained significant attention and become a rapidly growing area of structured data learning. The field has seen extensive research in the past 20 years, resulting in the development of numerous graph kernels, each concentrating on specific graph structural properties. Graph kernels have proven to be successful in various domains, including bioinformatics and social networks. This survey aims to offer a comprehensive and unified overview of the literature on graph kernels, showcasing a wide range of them. Additionally, we conduct an experimental assessment of multiple kernels using publicly available datasets and provide a comparative study. Finally, we explore essential applications of graph kernels and highlight some of the challenges that require addressing.",1
"Convolution operations designed for graph-structured data usually utilize the graph Laplacian, which can be seen as message passing between the adjacent neighbors through a generic random walk. In this paper, we propose PAN, a new graph convolution framework that involves every path linking the message sender and receiver with learnable weights depending on the path length, which corresponds to the maximal entropy random walk. PAN generalizes the graph Laplacian to a new transition matrix we call \emph{maximal entropy transition} (MET) matrix derived from a path integral formalism. Most previous graph convolutional network architectures can be adapted to our framework, and many variations and derivatives based on the path integral idea can be developed. Experimental results show that the path integral based graph neural networks have great learnability and fast convergence rate, and achieve state-of-the-art performance on benchmark tasks.",0
"The convolution techniques commonly used for graph-structured data rely on the graph Laplacian, which facilitates message passing between neighboring nodes using a random walk approach. This paper presents a novel graph convolution framework called PAN, which incorporates all the paths connecting the message sender and receiver with trainable weights based on the path length. This approach corresponds to the maximal entropy random walk and extends the graph Laplacian to a new transition matrix referred to as the maximal entropy transition (MET) matrix. The MET matrix is derived from a path integral formalism and can be applied to various graph convolutional network architectures. Furthermore, the path integral concept can be modified to create different variations and derivatives. Experimental results demonstrate that the path integral-based graph neural networks have high learnability, fast convergence, and outperform existing benchmark tasks.",1
"The variational autoencoder (VAE) framework remains a popular option for training unsupervised generative models, especially for discrete data where generative adversarial networks (GANs) require workaround to create gradient for the generator. In our work modeling US postal addresses, we show that our discrete VAE with tree recursive architecture demonstrates limited capability of capturing field correlations within structured data, even after overcoming the challenge of posterior collapse with scheduled sampling and tuning of the KL-divergence weight $\beta$. Worse, VAE seems to have difficulty mapping its generated samples to the latent space, as their VAE loss lags behind or even increases during the training process. Motivated by this observation, we show that augmenting training data with generated variants (augmented training) and training a VAE with multiple values of $\beta$ simultaneously (multiscale VAE) both improve the generation quality of VAE. Despite their differences in motivation and emphasis, we show that augmented training and multiscale VAE are actually connected and have similar effects on the model.",0
"The VAE framework is a popular choice for training unsupervised generative models, particularly for discrete data where GANs require workarounds to create a generator gradient. Our research on US postal addresses using a discrete VAE with a tree recursive architecture shows that it has limited capability in capturing field correlations within structured data, despite overcoming posterior collapse challenges through scheduled sampling and tuning of the KL-divergence weight $\beta$. Furthermore, VAE struggles to map its generated samples to the latent space, with its loss lagging behind or even increasing during training. Based on this finding, we demonstrate that augmented training, which involves training the VAE with generated variants, and multiscale VAE, which involves training the VAE with multiple values of $\beta$ simultaneously, both improve the quality of VAE's generation. Despite their diverse motivations and focuses, we establish that augmented training and multiscale VAE are interconnected and display similar impacts on the model.",1
"We present GraphTSNE, a novel visualization technique for graph-structured data based on t-SNE. The growing interest in graph-structured data increases the importance of gaining human insight into such datasets by means of visualization. Among the most popular visualization techniques, classical t-SNE is not suitable on such datasets because it has no mechanism to make use of information from the graph structure. On the other hand, visualization techniques which operate on graphs, such as Laplacian Eigenmaps and tsNET, have no mechanism to make use of information from node features. Our proposed method GraphTSNE produces visualizations which account for both graph structure and node features. It is based on scalable and unsupervised training of a graph convolutional network on a modified t-SNE loss. By assembling a suite of evaluation metrics, we demonstrate that our method produces desirable visualizations on three benchmark datasets.",0
"GraphTSNE is a new visualization technique that enables human insight into graph-structured data. With the increasing interest in such datasets, visualization techniques are becoming more important. However, classical t-SNE is not suitable for graph-structured data since it does not use information from the graph structure. Visualization techniques that work on graphs, such as Laplacian Eigenmaps and tsNET, do not use information from node features. To address this issue, we propose GraphTSNE, which considers both graph structure and node features. Our method uses a graph convolutional network that is trained without supervision on a modified t-SNE loss. We demonstrate that our method produces desirable visualizations on three benchmark datasets by using a suite of evaluation metrics.",1
"In many applications where collecting data is expensive, for example neuroscience or medical imaging, the sample size is typically small compared to the feature dimension. It is challenging in this setting to train expressive, non-linear models without overfitting. These datasets call for intelligent regularization that exploits known structure, such as correlations between the features arising from the measurement device. However, existing structured regularizers need specially crafted solvers, which are difficult to apply to complex models. We propose a new regularizer specifically designed to leverage structure in the data in a way that can be applied efficiently to complex models. Our approach relies on feature grouping, using a fast clustering algorithm inside a stochastic gradient descent loop: given a family of feature groupings that capture feature covariations, we randomly select these groups at each iteration. We show that this approach amounts to enforcing a denoising regularizer on the solution. The method is easy to implement in many model architectures, such as fully connected neural networks, and has a linear computational cost. We apply this regularizer to a real-world fMRI dataset and the Olivetti Faces datasets. Experiments on both datasets demonstrate that the proposed approach produces models that generalize better than those trained with conventional regularizers, and also improves convergence speed.",0
"When collecting data in expensive applications like neuroscience or medical imaging, the sample size is often small compared to the feature dimension. This poses a challenge when training expressive, non-linear models without overfitting. To address this issue, intelligent regularization is needed to exploit known structure, such as correlations between features from the measurement device. However, existing structured regularizers require specially crafted solvers that are difficult to apply to complex models. To overcome this, we introduce a new regularizer designed to efficiently leverage structure in the data. Our approach involves feature grouping using a fast clustering algorithm within a stochastic gradient descent loop. We randomly select feature groups at each iteration, which enforces a denoising regularizer on the solution. This method is easy to implement in various model architectures and has a linear computational cost. We apply this regularizer to real-world fMRI and Olivetti Faces datasets, which demonstrate that our approach produces models that generalize better and improve convergence speed compared to conventional regularizers.",1
"With the prevalence of accessible depth sensors, dynamic human body skeletons have attracted much attention as a robust modality for action recognition. Previous methods model skeletons based on RNN or CNN, which has limited expressive power for irregular skeleton joints. While graph convolutional networks (GCN) have been proposed to address irregular graph-structured data, the fundamental graph construction remains challenging. In this paper, we represent skeletons naturally on graphs, and propose a graph regression based GCN (GR-GCN) for skeleton-based action recognition, aiming to capture the spatio-temporal variation in the data. As the graph representation is crucial to graph convolution, we first propose graph regression to statistically learn the underlying graph from multiple observations. In particular, we provide spatio-temporal modeling of skeletons and pose an optimization problem on the graph structure over consecutive frames, which enforces the sparsity of the underlying graph for efficient representation. The optimized graph not only connects each joint to its neighboring joints in the same frame strongly or weakly, but also links with relevant joints in the previous and subsequent frames. We then feed the optimized graph into the GCN along with the coordinates of the skeleton sequence for feature learning, where we deploy high-order and fast Chebyshev approximation of spectral graph convolution. Further, we provide analysis of the variation characterization by the Chebyshev approximation. Experimental results validate the effectiveness of the proposed graph regression and show that the proposed GR-GCN achieves the state-of-the-art performance on the widely used NTU RGB+D, UT-Kinect and SYSU 3D datasets.",0
"Dynamic human body skeletons have gained significant attention as a reliable modality for action recognition with the increasing accessibility of depth sensors. Past methods have utilized RNN or CNN to model skeletons, which have limited expressive power for the irregular skeleton joints. While graph convolutional networks (GCN) have been proposed to address the irregular graph-structured data, the fundamental graph construction remains challenging. This paper proposes a graph regression based GCN (GR-GCN) for skeleton-based action recognition, aiming to capture the spatio-temporal variation in the data by representing skeletons naturally on graphs. To statistically learn the underlying graph from multiple observations, we propose graph regression, which enforces the sparsity of the underlying graph for efficient representation by posing an optimization problem on the graph structure over consecutive frames. The optimized graph links each joint to its neighboring joints in the same frame strongly or weakly and links with relevant joints in the previous and subsequent frames. We then feed the optimized graph into the GCN along with the coordinates of the skeleton sequence for feature learning, where we deploy high-order and fast Chebyshev approximation of spectral graph convolution. Experimental results validate the effectiveness of the proposed graph regression and show that the proposed GR-GCN achieves the state-of-the-art performance on the widely used NTU RGB+D, UT-Kinect, and SYSU 3D datasets.",1
"Generating long and semantic-coherent reports to describe medical images poses great challenges towards bridging visual and linguistic modalities, incorporating medical domain knowledge, and generating realistic and accurate descriptions. We propose a novel Knowledge-driven Encode, Retrieve, Paraphrase (KERP) approach which reconciles traditional knowledge- and retrieval-based methods with modern learning-based methods for accurate and robust medical report generation. Specifically, KERP decomposes medical report generation into explicit medical abnormality graph learning and subsequent natural language modeling. KERP first employs an Encode module that transforms visual features into a structured abnormality graph by incorporating prior medical knowledge; then a Retrieve module that retrieves text templates based on the detected abnormalities; and lastly, a Paraphrase module that rewrites the templates according to specific cases. The core of KERP is a proposed generic implementation unit---Graph Transformer (GTR) that dynamically transforms high-level semantics between graph-structured data of multiple domains such as knowledge graphs, images and sequences. Experiments show that the proposed approach generates structured and robust reports supported with accurate abnormality description and explainable attentive regions, achieving the state-of-the-art results on two medical report benchmarks, with the best medical abnormality and disease classification accuracy and improved human evaluation performance.",0
"Generating descriptive reports for medical images presents significant challenges in integrating visual and linguistic modalities, incorporating medical expertise, and producing accurate and realistic descriptions. Our solution is the KERP approach, which combines traditional knowledge- and retrieval-based methods with modern learning-based methods to generate precise and reliable medical reports. KERP breaks down medical report generation into two stages: explicit medical abnormality graph learning and natural language modeling. Using an Encode module, KERP transforms visual features into a structured abnormality graph that incorporates medical knowledge. Next, a Retrieve module selects text templates based on the identified abnormalities, and a Paraphrase module adjusts the templates for specific cases. Our proposed Graph Transformer (GTR) is a flexible implementation unit that dynamically transforms high-level semantics between graph-structured data of multiple domains. Experiments demonstrate that KERP produces structured and robust reports with accurate abnormality descriptions and explainable attentive regions, achieving state-of-the-art results on two medical report benchmarks with improved human evaluation performance and the best medical abnormality and disease classification accuracy.",1
"We provide a new approach to training neural models to exhibit transparency in a well-defined, functional manner. Our approach naturally operates over structured data and tailors the predictor, functionally, towards a chosen family of (local) witnesses. The estimation problem is setup as a co-operative game between an unrestricted predictor such as a neural network, and a set of witnesses chosen from the desired transparent family. The goal of the witnesses is to highlight, locally, how well the predictor conforms to the chosen family of functions, while the predictor is trained to minimize the highlighted discrepancy. We emphasize that the predictor remains globally powerful as it is only encouraged to agree locally with locally adapted witnesses. We analyze the effect of the proposed approach, provide example formulations in the context of deep graph and sequence models, and empirically illustrate the idea in chemical property prediction, temporal modeling, and molecule representation learning.",0
"Our innovative method trains neural models to display transparency in a clearly defined and functional manner. Our method is designed to operate naturally over structured data and customize the predictor functionally according to a selected family of (local) witnesses. The estimation problem is formulated as a cooperative game between the predictor, which is unrestricted and often a neural network, and a set of witnesses chosen from the desired transparent family. The witnesses aim to highlight, locally, how closely the predictor adheres to the chosen family of functions, while the predictor is trained to minimize the highlighted differences. It is worth noting that the predictor remains powerful globally, as it is only required to agree with locally adapted witnesses. We examine the impact of this approach, provide examples in the context of deep graph and sequence models, and demonstrate the concept empirically in chemical property prediction, temporal modeling, and molecule representation learning.",1
"Finding an optimal parameter of a black-box function is important for searching stable material structures and finding optimal neural network structures, and Bayesian optimization algorithms are widely used for the purpose. However, most of existing Bayesian optimization algorithms can only handle vector data and cannot handle complex structured data. In this paper, we propose the topological Bayesian optimization, which can efficiently find an optimal solution from structured data using \emph{topological information}. More specifically, in order to apply Bayesian optimization to structured data, we extract useful topological information from a structure and measure the proper similarity between structures. To this end, we utilize persistent homology, which is a topological data analysis method that was recently applied in machine learning. Moreover, we propose the Bayesian optimization algorithm that can handle multiple types of topological information by using a linear combination of kernels for persistence diagrams. Through experiments, we show that topological information extracted by persistent homology contributes to a more efficient search for optimal structures compared to the random search baseline and the graph Bayesian optimization algorithm.",0
"The pursuit of an optimal parameter for a black-box function holds great significance in the search for stable material structures and optimal neural network structures. Bayesian optimization algorithms have been widely employed for this purpose. However, the majority of current Bayesian optimization algorithms are only capable of handling vector data, and cannot handle complex structured data. In this article, we propose the implementation of topological Bayesian optimization, which utilizes topological information to efficiently find optimal solutions from structured data. Specifically, we extract useful topological information from a structure and measure the appropriate similarity between structures to apply Bayesian optimization to structured data. Persistent homology, a topological data analysis method that has recently been used in machine learning, is utilized for this purpose. Additionally, we suggest a Bayesian optimization algorithm that can handle multiple types of topological information by using a linear combination of kernels for persistence diagrams. Our experiments demonstrate that the topological information extracted by persistent homology contributes to a more efficient search for optimal structures compared to the random search baseline and the graph Bayesian optimization algorithm.",1
"We present a novel and hierarchical approach for supervised classification of signals spanning over a fixed graph, reflecting shared properties of the dataset. To this end, we introduce a Convolutional Cluster Pooling layer exploiting a multi-scale clustering in order to highlight, at different resolutions, locally connected regions on the input graph. Our proposal generalises well-established neural models such as Convolutional Neural Networks (CNNs) on irregular and complex domains, by means of the exploitation of the weight sharing property in a graph-oriented architecture. In this work, such property is based on the centrality of each vertex within its soft-assigned cluster. Extensive experiments on NTU RGB+D, CIFAR-10 and 20NEWS demonstrate the effectiveness of the proposed technique in capturing both local and global patterns in graph-structured data out of different domains.",0
"Our approach for supervised signal classification on a fixed graph involves a hierarchical method that reflects shared properties of the data. We introduce a Convolutional Cluster Pooling layer that utilizes multi-scale clustering to identify locally connected regions on the input graph at different resolutions. This approach can be applied to irregular and complex domains, and is based on the weight sharing property in a graph-oriented architecture. We use the centrality of each vertex within its soft-assigned cluster to achieve this. Our experiments on various datasets, including NTU RGB+D, CIFAR-10, and 20NEWS, show that this technique is effective in capturing both local and global patterns in graph-structured data from different domains.",1
"A graph is a powerful concept for representation of relations between pairs of entities. Data with underlying graph structure can be found across many disciplines and there is a natural desire for understanding such data better. Deep learning (DL) has achieved significant breakthroughs in a variety of machine learning tasks in recent years, especially where data is structured on a grid, such as in text, speech, or image understanding. However, surprisingly little has been done to explore the applicability of DL on arbitrary graph-structured data directly.   The goal of this thesis is to investigate architectures for DL on graphs and study how to transfer, adapt or generalize concepts that work well on sequential and image data to this domain. We concentrate on two important primitives: embedding graphs or their nodes into a continuous vector space representation (encoding) and, conversely, generating graphs from such vectors back (decoding). To that end, we make the following contributions.   First, we introduce Edge-Conditioned Convolutions (ECC), a convolution-like operation on graphs performed in the spatial domain where filters are dynamically generated based on edge attributes. The method is used to encode graphs with arbitrary and varying structure.   Second, we propose SuperPoint Graph, an intermediate point cloud representation with rich edge attributes encoding the contextual relationship between object parts. Based on this representation, ECC is employed to segment large-scale point clouds without major sacrifice in fine details.   Third, we present GraphVAE, a graph generator allowing us to decode graphs with variable but upper-bounded number of nodes making use of approximate graph matching for aligning the predictions of an autoencoder with its inputs. The method is applied to the task of molecule generation.",0
"Graphs are a useful tool for representing relationships between entities. Graph-structured data is present in many fields, and there is a desire to better understand it. Although deep learning has made significant progress in machine learning tasks involving grid-structured data, such as text and images, there has been little exploration of its potential for arbitrary graph-structured data. This thesis aims to investigate architectures for deep learning on graphs and explore how concepts that work well on sequential and image data can be adapted to this domain. Specifically, the thesis focuses on two key components: encoding graphs or their nodes into a continuous vector space representation and generating graphs from these vectors. The thesis makes several contributions, including introducing Edge-Conditioned Convolutions, proposing SuperPoint Graph as an intermediate point cloud representation, and presenting GraphVAE, a graph generator for molecule generation.",1
"Recently, Graph Convolutional Networks (GCNs) have been widely studied for graph-structured data representation and learning. However, in many real applications, data are coming with multiple graphs, and it is non-trivial to adapt GCNs to deal with data representation with multiple graph structures. One main challenge for multi-graph representation is how to exploit both structure information of each individual graph and correlation information across multiple graphs simultaneously. In this paper, we propose a novel Multiple Graph Adversarial Learning (MGAL) framework for multi-graph representation and learning. MGAL aims to learn an optimal structure-invariant and consistent representation for multiple graphs in a common subspace via a novel adversarial learning framework, which thus incorporates both structure information of intra-graph and correlation information of inter-graphs simultaneously. Based on MGAL, we then provide a unified network for semi-supervised learning task. Promising experimental results demonstrate the effectiveness of MGAL model.",0
"Graph Convolutional Networks (GCNs) have been widely researched for representing and learning graph-structured data. However, in real-world applications, data often come with multiple graphs, and adapting GCNs to deal with multiple graph structures poses a significant challenge. One of the main obstacles is how to utilize both individual graph structure information and correlation information across multiple graphs. This paper proposes a new framework for multi-graph representation and learning called Multiple Graph Adversarial Learning (MGAL). The MGAL framework aims to learn an optimal representation for multiple graphs in a common subspace that is both structure-invariant and consistent. This is achieved through a novel adversarial learning framework that incorporates both intra-graph structure information and inter-graph correlation information. Using MGAL, a unified network is provided for semi-supervised learning task. Experimental results show that the MGAL model is effective.",1
"Graph matching is an important and persistent problem in computer vision and pattern recognition for finding node-to-node correspondence between graph-structured data. However, as widely used, graph matching that incorporates pairwise constraints can be formulated as a quadratic assignment problem (QAP), which is NP-complete and results in intrinsic computational difficulties. In this paper, we present a functional representation for graph matching (FRGM) that aims to provide more geometric insights on the problem and reduce the space and time complexities of corresponding algorithms. To achieve these goals, we represent a graph endowed with edge attributes by a linear function space equipped with a functional such as inner product or metric, that has an explicit geometric meaning. Consequently, the correspondence between graphs can be represented as a linear representation map of that functional. Specifically, we reformulate the linear functional representation map as a new parameterization for Euclidean graph matching, which is associative with geometric parameters for graphs under rigid or nonrigid deformations. This allows us to estimate the correspondence and geometric deformations simultaneously. The use of the representation of edge attributes rather than the affinity matrix enables us to reduce the space complexity by two orders of magnitudes. Furthermore, we propose an efficient optimization strategy with low time complexity to optimize the objective function. The experimental results on both synthetic and real-world datasets demonstrate that the proposed FRGM can achieve state-of-the-art performance.",0
"The identification of corresponding nodes in graph-structured data is a persistent and significant challenge in pattern recognition and computer vision, commonly referred to as graph matching. However, incorporating pairwise constraints in graph matching leads to a quadratic assignment problem (QAP), which is NP-complete and poses computational difficulties. To address these issues, our paper introduces a functional representation for graph matching (FRGM), which offers geometric insights and reduces the complexities of corresponding algorithms. Specifically, we represent graphs with edge attributes using a linear function space equipped with a functional, such as inner product or metric, that has explicit geometric meaning. Thus, graph correspondence can be expressed as a linear representation map of that functional. We reformulate this linear functional representation map as a new parameterization for Euclidean graph matching, which is associative with geometric parameters for graphs under rigid or nonrigid deformations. This enables us to simultaneously estimate correspondences and geometric deformations. By using the representation of edge attributes instead of the affinity matrix, we reduce space complexity by two orders of magnitude. Additionally, we propose an efficient optimization strategy with low time complexity to optimize the objective function. Our experimental results on both synthetic and real-world datasets demonstrate that the FRGM approach achieves state-of-the-art performance.",1
"Semi-supervised learning on graph structured data has received significant attention with the recent introduction of Graph Convolution Networks (GCN). While traditional methods have focused on optimizing a loss augmented with Laplacian regularization framework, GCNs perform an implicit Laplacian type regularization to capture local graph structure. In this work, we propose Lovasz Convolutional Network (LCNs) which are capable of incorporating global graph properties. LCNs achieve this by utilizing Lovasz's orthonormal embeddings of the nodes. We analyse local and global properties of graphs and demonstrate settings where LCNs tend to work better than GCNs. We validate the proposed method on standard random graph models such as stochastic block models (SBM) and certain community structure based graphs where LCNs outperform GCNs and learn more intuitive embeddings. We also perform extensive binary and multi-class classification experiments on real world datasets to demonstrate LCN's effectiveness. In addition to simple graphs, we also demonstrate the use of LCNs on hyper-graphs by identifying settings where they are expected to work better than GCNs.",0
"The recent introduction of Graph Convolution Networks (GCN) has led to increased interest in semi-supervised learning on graph structured data. Traditional methods focused on optimizing a loss augmented with Laplacian regularization framework, while GCNs perform implicit Laplacian type regularization to capture local graph structure. Our proposed Lovasz Convolutional Network (LCN) incorporates global graph properties by utilizing Lovasz's orthonormal embeddings of the nodes. We analyze the local and global properties of graphs and show that LCNs tend to work better than GCNs in certain settings. We validate our method on standard random graph models and demonstrate that LCNs outperform GCNs in learning more intuitive embeddings. We also demonstrate the effectiveness of LCNs through extensive binary and multi-class classification experiments on real-world datasets. Additionally, we show that LCNs can be used on hyper-graphs and identify settings where they are expected to work better than GCNs.",1
"We apply network Lasso to semi-supervised regression problems involving network structured data. This approach lends quite naturally to highly scalable learning algorithms in the form of message passing over an empirical graph which represents the network structure of the data. By using a simple non-parametric regression model, which is motivated by a clustering hypothesis, we provide an analysis of the estimation error incurred by network Lasso. This analysis reveals conditions on the the network structure and the available training data which guarantee network Lasso to be accurate. Remarkably, the accuracy of network Lasso is related to the existence of sufficiently large network flows over the empirical graph. Thus, our analysis reveals a connection between network Lasso and maximum flow problems.",0
"Semi-supervised regression problems with network structured data can be addressed using network Lasso. This method is suitable for scalable learning algorithms that can pass messages over an empirical graph representing the data's network structure. A non-parametric regression model based on clustering hypothesis is used, and an analysis of the estimation error incurred by network Lasso is provided. The study identifies specific conditions for the network structure and available training data that guarantee the accuracy of network Lasso. Interestingly, the accuracy is linked to the existence of sufficiently large network flows over the empirical graph, indicating a relationship between network Lasso and maximum flow problems.",1
"We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs---both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning.",0
"Our article introduces Deep Graph Infomax (DGI), a general method for unsupervised learning of node representations in graph-structured data. DGI maximizes mutual information between patch representations and high-level summaries of graphs, both of which are constructed using established graph convolutional network architectures. The patch representations capture subgraphs that center around nodes of interest and can be used for downstream node-wise learning tasks. Unlike other unsupervised GCN-learning approaches, DGI does not rely on random walk objectives and is applicable to both transductive and inductive learning setups. Our experiments demonstrate competitive performance on various node classification benchmarks, sometimes even surpassing that of supervised learning.",1
"The training phases of Deep neural network~(DNN) consumes enormous processing time and energy. Compression techniques utilizing the sparsity of DNNs can effectively accelerate the inference phase of DNNs. However, it can be hardly used in the training phase because the training phase involves dense matrix-multiplication using General Purpose Computation on Graphics Processors (GPGPU), which endorse regular and structural data layout. In this paper, we propose the Approximate Random Dropout that replaces the conventional random dropout of neurons and synapses with a regular and predefined patterns to eliminate the unnecessary computation and data access. To compensate the potential performance loss we develop a SGD-based Search Algorithm to produce the distribution of dropout patterns. We prove our approach is statistically equivalent to the previous dropout method. Experiments results on MLP and LSTM using well-known benchmarks show that the proposed Approximate Random Dropout can reduce the training time by $20\%$-$77\%$ ($19\%$-$60\%$) when dropout rate is $0.3$-$0.7$ on MLP (LSTM) with marginal accuracy drop.",0
"The process of training Deep Neural Networks (DNNs) is time and energy-intensive, and efforts have been made to speed up the inference phase of DNNs using compression techniques that take advantage of the sparsity of DNNs. However, these techniques are not effective during the training phase as it involves dense matrix multiplication that requires regular and structured data layouts using General Purpose Computation on Graphics Processors (GPGPU). This paper suggests a new method called Approximate Random Dropout, which replaces the conventional random dropout of neurons and synapses with predefined patterns to eliminate unnecessary computation and data access. To compensate for the potential decrease in performance, a SGD-based Search Algorithm has been developed to produce the distribution of dropout patterns. It has been proven that this approach is statistically equivalent to the previous dropout method. The results of experiments on MLP and LSTM using well-known benchmarks show that the proposed Approximate Random Dropout can reduce training time by 20%-77% (19%-60%) when the dropout rate is 0.3-0.7 on MLP (LSTM) with marginal accuracy drop.",1
"We propose Graphical Generative Adversarial Networks (Graphical-GAN) to model structured data. Graphical-GAN conjoins the power of Bayesian networks on compactly representing the dependency structures among random variables and that of generative adversarial networks on learning expressive dependency functions. We introduce a structured recognition model to infer the posterior distribution of latent variables given observations. We generalize the Expectation Propagation (EP) algorithm to learn the generative model and recognition model jointly. Finally, we present two important instances of Graphical-GAN, i.e. Gaussian Mixture GAN (GMGAN) and State Space GAN (SSGAN), which can successfully learn the discrete and temporal structures on visual datasets, respectively.",0
"Our proposal is to use Graphical Generative Adversarial Networks (Graphical-GAN) for modeling structured data. The Graphical-GAN combines the ability of Bayesian networks to represent the dependency structures among random variables in a concise manner and the power of generative adversarial networks to learn expressive dependency functions. To determine the posterior distribution of latent variables based on observations, we introduce a structured recognition model. We have generalized the Expectation Propagation (EP) algorithm to enable the joint learning of the generative model and recognition model. Lastly, we present two crucial examples of Graphical-GAN, namely Gaussian Mixture GAN (GMGAN) and State Space GAN (SSGAN). These two structures can effectively learn visual datasets' discrete and temporal structures, respectively.",1
"Tensor network decomposition, originated from quantum physics to model entangled many-particle quantum systems, turns out to be a promising mathematical technique to efficiently represent and process big data in parsimonious manner. In this study, we show that tensor networks can systematically partition structured data, e.g. color images, for distributed storage and communication in privacy-preserving manner. Leveraging the sea of big data and metadata privacy, empirical results show that neighbouring subtensors with implicit information stored in tensor network formats cannot be identified for data reconstruction. This technique complements the existing encryption and randomization techniques which store explicit data representation at one place and highly susceptible to adversarial attacks such as side-channel attacks and de-anonymization. Furthermore, we propose a theory for adversarial examples that mislead convolutional neural networks to misclassification using subspace analysis based on singular value decomposition (SVD). The theory is extended to analyze higher-order tensors using tensor-train SVD (TT-SVD); it helps to explain the level of susceptibility of different datasets to adversarial attacks, the structural similarity of different adversarial attacks including global and localized attacks, and the efficacy of different adversarial defenses based on input transformation. An efficient and adaptive algorithm based on robust TT-SVD is then developed to detect strong and static adversarial attacks.",0
"Tensor network decomposition, which was originally developed in quantum physics to model entangled many-particle quantum systems, has emerged as a promising mathematical technique for efficiently processing large data sets in a parsimonious manner. Our research demonstrates that tensor networks can be used to partition structured data, such as color images, to enable distributed storage and communication while preserving privacy. By leveraging the vast sea of big data and metadata privacy, we have shown that neighboring subtensors with implicit information stored in tensor network formats cannot be identified for data reconstruction. This approach complements existing encryption and randomization techniques, which store explicit data representation in a single location and are highly vulnerable to adversarial attacks. Additionally, we propose a theory for adversarial examples that can mislead convolutional neural networks to misclassify using subspace analysis based on singular value decomposition (SVD). This theory is extended to analyze higher-order tensors using tensor-train SVD (TT-SVD), which helps to explain the level of susceptibility of different datasets to adversarial attacks, the structural similarity of different adversarial attacks, including global and localized attacks, and the efficacy of various adversarial defenses based on input transformation. Finally, we developed an efficient and adaptive algorithm based on robust TT-SVD to detect strong and static adversarial attacks.",1
"Structural health monitoring is a condition-based field of study utilised to monitor infrastructure, via sensing systems. It is therefore used in the field of aerospace engineering to assist in monitoring the health of aerospace structures. A difficulty however is that in structural health monitoring the data input is usually from sensor arrays, which results in data which are highly redundant and correlated, an area in which traditional two-way matrix approaches have had difficulty in deconstructing and interpreting. Newer methods involving tensor analysis allow us to analyse this multi-way structural data in a coherent manner. In our approach, we demonstrate the usefulness of tensor-based learning coupled with for damage detection, on a novel $N$-DoF Lagrangian aeroservoelastic model.",0
"Structural health monitoring involves using sensing systems to monitor infrastructure for condition-based purposes. This field is particularly useful in aerospace engineering for monitoring the health of aerospace structures. However, a challenge in structural health monitoring is dealing with highly redundant and correlated data from sensor arrays, which traditional matrix approaches struggle to interpret. Tensor analysis provides newer methods for coherent analysis of this multi-way structural data. In our approach, we showcase the effectiveness of using tensor-based learning with damage detection on a unique $N$-DoF Lagrangian aeroservoelastic model.",1
"Graph-based methods are known to be successful in many machine learning and pattern classification tasks. These methods consider semi-structured data as graphs where nodes correspond to primitives (parts, interest points, segments, etc.) and edges characterize the relationships between these primitives. However, these non-vectorial graph data cannot be straightforwardly plugged into off-the-shelf machine learning algorithms without a preliminary step of -- explicit/implicit -- graph vectorization and embedding. This embedding process should be resilient to intra-class graph variations while being highly discriminant. In this paper, we propose a novel high-order stochastic graphlet embedding (SGE) that maps graphs into vector spaces. Our main contribution includes a new stochastic search procedure that efficiently parses a given graph and extracts/samples unlimitedly high-order graphlets. We consider these graphlets, with increasing orders, to model local primitives as well as their increasingly complex interactions. In order to build our graph representation, we measure the distribution of these graphlets into a given graph, using particular hash functions that efficiently assign sampled graphlets into isomorphic sets with a very low probability of collision. When combined with maximum margin classifiers, these graphlet-based representations have positive impact on the performance of pattern comparison and recognition as corroborated through extensive experiments using standard benchmark databases.",0
"Many machine learning and pattern classification tasks have been successful using graph-based methods. These methods view semi-structured data as graphs, where nodes represent primitives such as segments, interest points, and parts, and edges indicate the relationships between these primitives. However, these non-vectorial graph data cannot be directly used with off-the-shelf machine learning algorithms without first converting them into vector form through graph vectorization and embedding. This embedding process must be highly discriminant and resilient to intra-class graph variations. To address this, we propose a novel high-order stochastic graphlet embedding (SGE) that maps graphs into vector spaces. Our approach includes a new stochastic search procedure that efficiently extracts/samples unlimitedly high-order graphlets from a given graph, using hash functions that assign sampled graphlets into isomorphic sets with low collision probability. We use these graphlets, with increasing orders, to model local primitives and their increasingly complex interactions. Our graph representation, when combined with maximum margin classifiers, has a positive impact on pattern recognition and comparison performance, as evidenced by our extensive experiments on standard benchmark databases.",1
"Recently, techniques for applying convolutional neural networks to graph-structured data have emerged. Graph convolutional neural networks (GCNNs) have been used to address node and graph classification and matrix completion. Although the performance has been impressive, the current implementations have limited capability to incorporate uncertainty in the graph structure. Almost all GCNNs process a graph as though it is a ground-truth depiction of the relationship between nodes, but often the graphs employed in applications are themselves derived from noisy data or modelling assumptions. Spurious edges may be included; other edges may be missing between nodes that have very strong relationships. In this paper we adopt a Bayesian approach, viewing the observed graph as a realization from a parametric family of random graphs. We then target inference of the joint posterior of the random graph parameters and the node (or graph) labels. We present the Bayesian GCNN framework and develop an iterative learning procedure for the case of assortative mixed-membership stochastic block models. We present the results of experiments that demonstrate that the Bayesian formulation can provide better performance when there are very few labels available during the training process.",0
"New techniques have recently emerged that apply convolutional neural networks to data structured in graphs. These Graph Convolutional Neural Networks (GCNNs) have been successful in classifying nodes and graphs, as well as completing matrices. However, current implementations have limited ability to incorporate uncertainty in the graph structure. Most GCNNs treat the graph as a ground-truth representation of node relationships, despite the fact that graphs used in applications are often derived from noisy data or modeling assumptions. Such graphs may contain erroneous edges or lack edges between strongly related nodes. In this paper, we use a Bayesian approach where we view the observed graph as a realization from a parametric family of random graphs. We aim to infer the joint posterior of the random graph parameters and the node or graph labels. We introduce the Bayesian GCNN framework and an iterative learning procedure for assortative mixed-membership stochastic block models. Our experiments demonstrate that the Bayesian formulation can provide better performance when there are few labels available during training.",1
"Spectral Graph Convolutional Networks (GCNs) are a generalization of convolutional networks to learning on graph-structured data. Applications of spectral GCNs have been successful, but limited to a few problems where the graph is fixed, such as shape correspondence and node classification. In this work, we address this limitation by revisiting a particular family of spectral graph networks, Chebyshev GCNs, showing its efficacy in solving graph classification tasks with a variable graph structure and size. Chebyshev GCNs restrict graphs to have at most one edge between any pair of nodes. To this end, we propose a novel multigraph network that learns from multi-relational graphs. We model learned edges with abstract meaning and experiment with different ways to fuse the representations extracted from annotated and learned edges, achieving competitive results on a variety of chemical classification benchmarks.",0
"Convolutional networks adapted to graph-structured data, known as Spectral Graph Convolutional Networks (GCNs), have been successful in solving a limited number of problems where the graph structure is fixed. These problems include shape correspondence and node classification. However, this work aims to overcome this limitation by focusing on Chebyshev GCNs, a family of spectral graph networks. Chebyshev GCNs restrict graphs to have only one edge between any two nodes. We propose a novel multigraph network that learns from multi-relational graphs, which model learned edges with abstract meaning. We experiment with various ways to fuse the representations extracted from annotated and learned edges, which leads to competitive results on different chemical classification benchmarks.",1
"Link prediction is a key problem for network-structured data. Link prediction heuristics use some score functions, such as common neighbors and Katz index, to measure the likelihood of links. They have obtained wide practical uses due to their simplicity, interpretability, and for some of them, scalability. However, every heuristic has a strong assumption on when two nodes are likely to link, which limits their effectiveness on networks where these assumptions fail. In this regard, a more reasonable way should be learning a suitable heuristic from a given network instead of using predefined ones. By extracting a local subgraph around each target link, we aim to learn a function mapping the subgraph patterns to link existence, thus automatically learning a `heuristic' that suits the current network. In this paper, we study this heuristic learning paradigm for link prediction. First, we develop a novel $\gamma$-decaying heuristic theory. The theory unifies a wide range of heuristics in a single framework, and proves that all these heuristics can be well approximated from local subgraphs. Our results show that local subgraphs reserve rich information related to link existence. Second, based on the $\gamma$-decaying theory, we propose a new algorithm to learn heuristics from local subgraphs using a graph neural network (GNN). Its experimental results show unprecedented performance, working consistently well on a wide range of problems.",0
"Link prediction is a critical issue for network-based data, and link prediction heuristics are commonly employed to measure the probability of links using score functions such as common neighbors and Katz index. These heuristics are popular due to their simplicity, interpretability, and scalability, but their effectiveness is limited when their assumptions fail. Therefore, it is more reasonable to learn a heuristic suitable for a given network instead of using predefined ones. Our approach involves extracting a local subgraph around each target link and learning a function that maps subgraph patterns to link existence. In this paper, we explore this heuristic learning paradigm for link prediction. We introduce a novel $\gamma$-decaying heuristic theory that unifies a wide range of heuristics in a single framework and proves that local subgraphs contain rich information related to link existence. Using this theory, we propose a new algorithm to learn heuristics from local subgraphs using a graph neural network (GNN), which exhibits unprecedented performance across a broad range of problems.",1
"Generative concept representations have three major advantages over discriminative ones: they can represent uncertainty, they support integration of learning and reasoning, and they are good for unsupervised and semi-supervised learning. We discuss probabilistic and generative deep learning, which generative concept representations are based on, and the use of variational autoencoders and generative adversarial networks for learning generative concept representations, particularly for concepts whose data are sequences, structured data or graphs.",0
"The superiority of generative concept representations over discriminative ones lies in their ability to handle uncertainty, facilitate learning and reasoning integration, and offer excellent results for unsupervised and semi-supervised learning. We delve into the subject of generative deep learning, the foundation for generative concept representations, and explore the utility of variational autoencoders and generative adversarial networks for learning such representations. Specifically, we focus on the effectiveness of these methods in learning generative concept representations for sequential, structured, and graphical data.",1
"Convolution Neural Network (CNN) has gained tremendous success in computer vision tasks with its outstanding ability to capture the local latent features. Recently, there has been an increasing interest in extending convolution operations to the non-Euclidean geometry. Although various types of convolution operations have been proposed for graphs or manifolds, their connections with traditional convolution over grid-structured data are not well-understood. In this paper, we show that depthwise separable convolution can be successfully generalized for the unification of both graph-based and grid-based convolution methods. Based on this insight we propose a novel Depthwise Separable Graph Convolution (DSGC) approach which is compatible with the tradition convolution network and subsumes existing convolution methods as special cases. It is equipped with the combined strengths in model expressiveness, compatibility (relatively small number of parameters), modularity and computational efficiency in training. Extensive experiments show the outstanding performance of DSGC in comparison with strong baselines on multi-domain benchmark datasets.",0
"The Convolution Neural Network (CNN) has been remarkably successful in the field of computer vision due to its exceptional capability to capture local latent features. There has been a growing interest in expanding convolution operations beyond the Euclidean geometry. Despite the availability of various convolution operations for graphs or manifolds, their connection to the traditional convolution over grid-structured data remains unclear. In this study, we demonstrate that depthwise separable convolution can be generalized to unify graph-based and grid-based convolution approaches. As a result, we propose the Depthwise Separable Graph Convolution (DSGC) method, which is compatible with traditional convolution networks and incorporates existing convolution methods as special cases. It possesses the advantages of model expressiveness, modularity, computational efficiency, and compatibility (with a relatively small number of parameters) in training. Extensive experiments have confirmed the superior performance of DSGC over robust benchmark datasets in multiple domains.",1
"Graphs are essential representations of many real-world data such as social networks. Recent years have witnessed the increasing efforts made to extend the neural network models to graph-structured data. These methods, which are usually known as the graph neural networks, have been applied to advance many graphs related tasks such as reasoning dynamics of the physical system, graph classification, and node classification. Most of the existing graph neural network models have been designed for static graphs, while many real-world graphs are inherently dynamic. For example, social networks are naturally evolving as new users joining and new relations being created. Current graph neural network models cannot utilize the dynamic information in dynamic graphs. However, the dynamic information has been proven to enhance the performance of many graph analytic tasks such as community detection and link prediction. Hence, it is necessary to design dedicated graph neural networks for dynamic graphs. In this paper, we propose DGNN, a new {\bf D}ynamic {\bf G}raph {\bf N}eural {\bf N}etwork model, which can model the dynamic information as the graph evolving. In particular, the proposed framework can keep updating node information by capturing the sequential information of edges (interactions), the time intervals between edges and information propagation coherently. Experimental results on various dynamic graphs demonstrate the effectiveness of the proposed framework.",0
"Graphs are crucial for representing a wide range of real-world data, including social networks. In recent years, there has been a growing interest in extending neural network models to graph-structured data, commonly referred to as graph neural networks. These networks have been applied to various graph-related tasks, such as graph classification, node classification, and reasoning dynamics of physical systems. However, most existing graph neural network models are designed for static graphs, whereas many real-world graphs are inherently dynamic, such as social networks that continually evolve with the addition of new users and relationships. Unfortunately, current graph neural network models cannot leverage the dynamic information present in dynamic graphs, despite evidence that such information can enhance the performance of graph analytic tasks. Therefore, there is a need to develop specialized graph neural networks for dynamic graphs. In this study, we introduce DGNN, a new Dynamic Graph Neural Network model that can model the dynamic information as the graph evolves. Specifically, our proposed framework updates node information by capturing the sequential information of edges or interactions, time intervals between edges, and information propagation coherently. Our experimental results on various dynamic graphs demonstrate the effectiveness of the proposed framework.",1
"With the expeditious advancement of information technologies, health-related data presented unprecedented potentials for medical and health discoveries but at the same time significant challenges for machine learning techniques both in terms of size and complexity. Those challenges include: the structured data with various storage formats and value types caused by heterogeneous data sources; the uncertainty widely existing in every aspect of medical diagnosis and treatments; the high dimensionality of the feature space; the longitudinal medical records data with irregular intervals between adjacent observations; the richness of relations existing among objects with similar genetic factors, location or socio-demographic background. This thesis aims to develop advanced Statistical Relational Learning approaches in order to effectively exploit such health-related data and facilitate the discoveries in medical research. It presents the work on cost-sensitive statistical relational learning for mining structured imbalanced data, the first continuous-time probabilistic logic model for predicting sequential events from longitudinal structured data as well as hybrid probabilistic relational models for learning from heterogeneous structured data. It also demonstrates the outstanding performance of these proposed models as well as other state of the art machine learning models when applied to medical research problems and other real-world large-scale systems, reveals the great potential of statistical relational learning for exploring the structured health-related data to facilitate medical research.",0
"Due to the rapid development of information technologies, health-related data offers immense potential for medical and health discoveries. However, this also presents significant challenges for machine learning techniques due to the size and complexity of the data. These challenges include structured data with varying storage formats and value types resulting from heterogeneous data sources, uncertainty in medical diagnosis and treatments, high dimensionality of the feature space, longitudinal medical records data with irregular intervals between adjacent observations, and rich relations among objects with similar genetic factors, location, or socio-demographic background. This thesis aims to address these challenges by developing advanced Statistical Relational Learning approaches to effectively utilize health-related data and facilitate medical research. The proposed approaches include cost-sensitive statistical relational learning for mining structured imbalanced data, the first continuous-time probabilistic logic model for predicting sequential events from longitudinal structured data, and hybrid probabilistic relational models for learning from heterogeneous structured data. Experimental results demonstrate the efficacy of these models and other state-of-the-art machine learning models in solving medical research problems and real-world large-scale systems, highlighting the potential of statistical relational learning for exploring structured health-related data to facilitate medical research.",1
"The development of a metric for structural data is a long-term problem in pattern recognition and machine learning. In this paper, we develop a general metric for comparing nonlinear dynamical systems that is defined with Perron-Frobenius operators in reproducing kernel Hilbert spaces. Our metric includes the existing fundamental metrics for dynamical systems, which are basically defined with principal angles between some appropriately-chosen subspaces, as its special cases. We also describe the estimation of our metric from finite data. We empirically illustrate our metric with an example of rotation dynamics in a unit disk in a complex plane, and evaluate the performance with real-world time-series data.",0
"Pattern recognition and machine learning have long struggled with creating a metric for structural data. This paper presents a solution to this problem with the development of a general metric for comparing nonlinear dynamical systems. Our metric utilizes Perron-Frobenius operators in reproducing kernel Hilbert spaces and encompasses fundamental metrics for dynamical systems as special cases. We also explain how to estimate our metric from finite data and provide an empirical example of rotation dynamics in a unit disk in a complex plane. Finally, we evaluate the performance of our metric with real-world time-series data.",1
"The rise of graph-structured data such as social networks, regulatory networks, citation graphs, and functional brain networks, in combination with resounding success of deep learning in various applications, has brought the interest in generalizing deep learning models to non-Euclidean domains. In this paper, we introduce a new spectral domain convolutional architecture for deep learning on graphs. The core ingredient of our model is a new class of parametric rational complex functions (Cayley polynomials) allowing to efficiently compute spectral filters on graphs that specialize on frequency bands of interest. Our model generates rich spectral filters that are localized in space, scales linearly with the size of the input data for sparsely-connected graphs, and can handle different constructions of Laplacian operators. Extensive experimental results show the superior performance of our approach, in comparison to other spectral domain convolutional architectures, on spectral image classification, community detection, vertex classification and matrix completion tasks.",0
"The popularity of graph-structured data, such as social networks, citation graphs, regulatory networks, and functional brain networks, has led to a growing interest in extending deep learning models to non-Euclidean domains. This interest is fueled by the success of deep learning in various applications. In this article, we present a new spectral domain convolutional architecture for deep learning on graphs. Our model is based on a class of parametric rational complex functions (Cayley polynomials) that can efficiently compute spectral filters on graphs, which specialize in frequency bands of interest. Our model produces spatially localized and rich spectral filters that scale linearly with the size of the input data for sparsely-connected graphs and can handle different constructions of Laplacian operators. The results of our extensive experiments demonstrate the superior performance of our approach in comparison to other spectral domain convolutional architectures for spectral image classification, community detection, vertex classification, and matrix completion tasks.",1
"Using predictive models to identify patterns that can act as biomarkers for different neuropathoglogical conditions is becoming highly prevalent. In this paper, we consider the problem of Autism Spectrum Disorder (ASD) classification where previous work has shown that it can be beneficial to incorporate a wide variety of meta features, such as socio-cultural traits, into predictive modeling. A graph-based approach naturally suits these scenarios, where a contextual graph captures traits that characterize a population, while the specific brain activity patterns are utilized as a multivariate signal at the nodes. Graph neural networks have shown improvements in inferencing with graph-structured data. Though the underlying graph strongly dictates the overall performance, there exists no systematic way of choosing an appropriate graph in practice, thus making predictive models non-robust. To address this, we propose a bootstrapped version of graph convolutional neural networks (G-CNNs) that utilizes an ensemble of weakly trained G-CNNs, and reduce the sensitivity of models on the choice of graph construction. We demonstrate its effectiveness on the challenging Autism Brain Imaging Data Exchange (ABIDE) dataset and show that our approach improves upon recently proposed graph-based neural networks. We also show that our method remains more robust to noisy graphs.",0
"The use of predictive models to detect biomarkers for various neuropathological conditions is becoming increasingly common. This paper examines the issue of classifying Autism Spectrum Disorder (ASD) using predictive modeling, which has been shown to benefit from the incorporation of a range of meta features, including socio-cultural traits. In these scenarios, a graph-based approach is well-suited, with a contextual graph capturing population traits and specific brain activity patterns used as multivariate signals at the nodes. While graph neural networks have shown improvements in inference with graph-structured data, the lack of a systematic method for selecting an appropriate graph in practice makes predictive models non-robust. To address this, we propose a bootstrapped version of graph convolutional neural networks (G-CNNs) that utilizes an ensemble of weakly trained G-CNNs, reducing model sensitivity to graph construction. Our approach is demonstrated on the challenging Autism Brain Imaging Data Exchange (ABIDE) dataset, showing improvements over recently proposed graph-based neural networks and greater robustness to noisy graphs.",1
"Algebraic topology methods have recently played an important role for statistical analysis with complicated geometric structured data such as shapes, linked twist maps, and material data. Among them, \textit{persistent homology} is a well-known tool to extract robust topological features, and outputs as \textit{persistence diagrams} (PDs). However, PDs are point multi-sets which can not be used in machine learning algorithms for vector data. To deal with it, an emerged approach is to use kernel methods, and an appropriate geometry for PDs is an important factor to measure the similarity of PDs. A popular geometry for PDs is the \textit{Wasserstein metric}. However, Wasserstein distance is not \textit{negative definite}. Thus, it is limited to build positive definite kernels upon the Wasserstein distance \textit{without approximation}. In this work, we rely upon the alternative \textit{Fisher information geometry} to propose a positive definite kernel for PDs \textit{without approximation}, namely the Persistence Fisher (PF) kernel. Then, we analyze eigensystem of the integral operator induced by the proposed kernel for kernel machines. Based on that, we derive generalization error bounds via covering numbers and Rademacher averages for kernel machines with the PF kernel. Additionally, we show some nice properties such as stability and infinite divisibility for the proposed kernel. Furthermore, we also propose a linear time complexity over the number of points in PDs for an approximation of our proposed kernel with a bounded error. Throughout experiments with many different tasks on various benchmark datasets, we illustrate that the PF kernel compares favorably with other baseline kernels for PDs.",0
"Recently, algebraic topology methods have become important for statistical analysis of geometrically complex data, including shapes, linked twist maps, and material data. Persistent homology is a well-known tool used to extract robust topological features, producing point multi-set persistence diagrams (PDs). However, PDs cannot be used in machine learning algorithms for vector data. To address this, kernel methods have emerged, and an appropriate geometry for PDs is crucial for measuring similarity. The Wasserstein metric is a popular choice, but its lack of negative definiteness limits its ability to build positive definite kernels without approximation. This work proposes the Persistence Fisher (PF) kernel, which is based on Fisher information geometry and is positive definite without approximation. Eigensystem analysis of the integral operator induced by the PF kernel is used to derive generalization error bounds via covering numbers and Rademacher averages for kernel machines. The PF kernel exhibits properties such as stability and infinite divisibility and has a linear time complexity over the number of points in PDs for an approximation with a bounded error. Experiments show that the PF kernel performs favorably compared to other baseline kernels for PDs on various benchmark datasets.",1
"Neural networks have been shown to be an effective tool for learning algorithms over graph-structured data. However, graph representation techniques---that convert graphs to real-valued vectors for use with neural networks---are still in their infancy. Recent works have proposed several approaches (e.g., graph convolutional networks), but these methods have difficulty scaling and generalizing to graphs with different sizes and shapes. We present Graph2Seq, a new technique that represents vertices of graphs as infinite time-series. By not limiting the representation to a fixed dimension, Graph2Seq scales naturally to graphs of arbitrary sizes and shapes. Graph2Seq is also reversible, allowing full recovery of the graph structure from the sequences. By analyzing a formal computational model for graph representation, we show that an unbounded sequence is necessary for scalability. Our experimental results with Graph2Seq show strong generalization and new state-of-the-art performance on a variety of graph combinatorial optimization problems.",0
"Learning algorithms for graph-structured data have been effectively facilitated by neural networks. However, the techniques that convert graphs into real-valued vectors for neural network usage are still developing. Although graph convolutional networks have been proposed, they face challenges in scaling and generalizing to graphs of different sizes and shapes. In this study, we introduce Graph2Seq, a new approach that represents graph vertices as infinite time-series. The representation is not limited to a fixed dimension, which enables Graph2Seq to scale naturally to graphs of any size and shape. Additionally, Graph2Seq is reversible, allowing complete retrieval of the graph structure from the sequences. By analyzing a formal computational model for graph representation, we demonstrate that an unbounded sequence is necessary for scalability. Our experimental results using Graph2Seq exhibit strong generalization and new state-of-the-art performance in various graph combinatorial optimization problems.",1
"We present a novel graph diffusion-embedding networks (GDEN) for graph structured data. GDEN is motivated by our closed-form formulation on regularized feature diffusion on graph. GDEN integrates both regularized feature diffusion and low-dimensional embedding simultaneously in a unified network model. Moreover, based on GDEN, we can naturally deal with structured data with multiple graph structures. Experiments on semi-supervised learning tasks on several benchmark datasets demonstrate the better performance of the proposed GDEN when comparing with the traditional GCN models.",0
"Our novel approach, the graph diffusion-embedding network (GDEN), is designed for graph structured data. It is based on our closed-form formulation of regularized feature diffusion on a graph. GDEN combines regularized feature diffusion and low-dimensional embedding in a single network model. Additionally, GDEN can handle structured data with multiple graph structures. Experimental results on several benchmark datasets show that GDEN outperforms traditional GCN models in semi-supervised learning tasks.",1
"Recently, graph convolutional network (GCN) has been widely used for semi-supervised classification and deep feature representation on graph-structured data. However, existing GCN generally fails to consider the local invariance constraint in learning and representation process. That is, if two data points Xi and Xj are close in the intrinsic geometry of the data distribution, then their labels/representations should also be close to each other. This is known as local invariance assumption which plays an essential role in the development of various kinds of traditional algorithms, such as dimensionality reduction and semi-supervised learning, in machine learning area. To overcome this limitation, we introduce a graph Laplacian GCN (gLGCN) approach for graph data representation and semi-supervised classification. The proposed gLGCN model is capable of encoding both graph structure and node features together while maintains the local invariance constraint naturally for robust data representation and semi-supervised classification. Experiments show the benefit of the benefits the proposed gLGCN network.",0
"The use of graph convolutional network (GCN) has become increasingly popular for graph-structured data in semi-supervised classification and deep feature representation. However, current GCN methods have not taken into account the local invariance constraint during the learning and representation process. This constraint ensures that if two data points, Xi and Xj, are close in the data distribution's intrinsic geometry, their labels or representations should also be close. The local invariance assumption is crucial in traditional algorithms such as semi-supervised learning and dimensionality reduction in machine learning. To address this limitation, we propose a graph Laplacian GCN (gLGCN) approach for robust data representation and semi-supervised classification. Our proposed gLGCN model can encode both graph structure and node features while naturally maintaining the local invariance constraint. Our experiments show the benefits of our proposed gLGCN network.",1
"Deep learning systems extensively use convolution operations to process input data. Though convolution is clearly defined for structured data such as 2D images or 3D volumes, this is not true for other data types such as sparse point clouds. Previous techniques have developed approximations to convolutions for restricted conditions. Unfortunately, their applicability is limited and cannot be used for general point clouds. We propose an efficient and effective method to learn convolutions for non-uniformly sampled point clouds, as they are obtained with modern acquisition techniques. Learning is enabled by four key novelties: first, representing the convolution kernel itself as a multilayer perceptron; second, phrasing convolution as a Monte Carlo integration problem, third, using this notion to combine information from multiple samplings at different levels; and fourth using Poisson disk sampling as a scalable means of hierarchical point cloud learning. The key idea across all these contributions is to guarantee adequate consideration of the underlying non-uniform sample distribution function from a Monte Carlo perspective. To make the proposed concepts applicable to real-world tasks, we furthermore propose an efficient implementation which significantly reduces the GPU memory required during the training process. By employing our method in hierarchical network architectures we can outperform most of the state-of-the-art networks on established point cloud segmentation, classification and normal estimation benchmarks. Furthermore, in contrast to most existing approaches, we also demonstrate the robustness of our method with respect to sampling variations, even when training with uniformly sampled data only. To support the direct application of these concepts, we provide a ready-to-use TensorFlow implementation of these layers at https://github.com/viscom-ulm/MCCNN",0
"Convolution operations are widely utilized in deep learning systems to process input data, but they are specifically defined for structured data such as 2D images or 3D volumes, not for sparse point clouds. Although previous techniques have developed approximations to convolutions for certain conditions, their applicability is limited and cannot be used for general point clouds. Our proposed method offers an efficient and effective way to learn convolutions for non-uniformly sampled point clouds acquired with modern acquisition techniques. We achieve this by representing the convolution kernel as a multilayer perceptron, phrasing convolution as a Monte Carlo integration problem, using this notion to combine information from multiple samplings at different levels, and using Poisson disk sampling as a scalable means of hierarchical point cloud learning. The key goal is to ensure adequate consideration of the underlying non-uniform sample distribution function from a Monte Carlo perspective. To make the method applicable to real-world tasks, we propose an efficient implementation that significantly reduces GPU memory during the training process. By incorporating this method in hierarchical network architectures, we have shown better performance than most state-of-the-art networks on established point cloud segmentation, classification, and normal estimation benchmarks. Additionally, we have demonstrated the robustness of our method with respect to sampling variations, even when training with uniformly sampled data only. We provide a readily available TensorFlow implementation of these layers to facilitate the direct application of these concepts. (https://github.com/viscom-ulm/MCCNN)",1
"Tasks involving the analysis of geometric (graph- and manifold-structured) data have recently gained prominence in the machine learning community, giving birth to a rapidly developing field of geometric deep learning. In this work, we leverage graph neural networks to improve signal detection in the IceCube neutrino observatory. The IceCube detector array is modeled as a graph, where vertices are sensors and edges are a learned function of the sensors' spatial coordinates. As only a subset of IceCube's sensors is active during a given observation, we note the adaptive nature of our GNN, wherein computation is restricted to the input signal support. We demonstrate the effectiveness of our GNN architecture on a task classifying IceCube events, where it outperforms both a traditional physics-based method as well as classical 3D convolution neural networks.",0
"The analysis of geometric data, particularly those with structured graphs and manifolds, has become increasingly important in the field of machine learning. This has led to the emergence of geometric deep learning as a rapidly growing area of study. Our research focuses on utilizing graph neural networks to enhance signal detection in the IceCube neutrino observatory. The IceCube detector array is represented as a graph, with sensors as vertices and edges determined by their spatial coordinates. Since a subset of sensors are active during observation, our GNN is adaptive and computation is limited to the input signal support. We demonstrate the superior performance of our GNN architecture over traditional physics-based methods and classical 3D convolution neural networks in classifying IceCube events.",1
"Visual relationship detection can bridge the gap between computer vision and natural language for scene understanding of images. Different from pure object recognition tasks, the relation triplets of subject-predicate-object lie on an extreme diversity space, such as \textit{person-behind-person} and \textit{car-behind-building}, while suffering from the problem of combinatorial explosion. In this paper, we propose a context-dependent diffusion network (CDDN) framework to deal with visual relationship detection. To capture the interactions of different object instances, two types of graphs, word semantic graph and visual scene graph, are constructed to encode global context interdependency. The semantic graph is built through language priors to model semantic correlations across objects, whilst the visual scene graph defines the connections of scene objects so as to utilize the surrounding scene information. For the graph-structured data, we design a diffusion network to adaptively aggregate information from contexts, which can effectively learn latent representations of visual relationships and well cater to visual relationship detection in view of its isomorphic invariance to graphs. Experiments on two widely-used datasets demonstrate that our proposed method is more effective and achieves the state-of-the-art performance.",0
"The detection of visual relationships has the potential to bridge the gap between computer vision and natural language, allowing for a better understanding of scene images. Unlike object recognition tasks, visual relationships are diverse and complex, posing a challenge due to the problem of combinatorial explosion. In this study, we propose a context-dependent diffusion network (CDDN) framework to address this issue. To capture the interactions between object instances, we construct two types of graphs: a word semantic graph and a visual scene graph. The semantic graph utilizes language priors to model semantic correlations between objects, while the visual scene graph defines connections between scene objects to utilize surrounding scene information. Using a diffusion network to adaptively aggregate information from contexts, we can effectively learn latent representations of visual relationships and cater to visual relationship detection. Our experiments on two widely-used datasets demonstrate that our proposed method outperforms existing approaches and achieves state-of-the-art performance.",1
"Machine Learning on graph-structured data is an important and omnipresent task for a vast variety of applications including anomaly detection and dynamic network analysis. In this paper, a deep generative model is introduced to capture continuous probability densities corresponding to the nodes of an arbitrary graph. In contrast to all learning formulations in the area of discriminative pattern recognition, we propose a scalable generative optimization/algorithm theoretically proved to capture distributions at the nodes of a graph. Our model is able to generate samples from the probability densities learned at each node. This probabilistic data generation model, i.e. convolutional graph auto-encoder (CGAE), is devised based on the localized first-order approximation of spectral graph convolutions, deep learning, and the variational Bayesian inference. We apply our CGAE to a new problem, the spatio-temporal probabilistic solar irradiance prediction. Multiple solar radiation measurement sites in a wide area in northern states of the US are modeled as an undirected graph. Using our proposed model, the distribution of future irradiance given historical radiation observations is estimated for every site/node. Numerical results on the National Solar Radiation Database show state-of-the-art performance for probabilistic radiation prediction on geographically distributed irradiance data in terms of reliability, sharpness, and continuous ranked probability score.",0
"The use of Machine Learning in analyzing data structured in graphs is crucial for various applications, such as anomaly detection and dynamic network analysis. This study presents a deep generative model that captures continuous probability densities associated with the nodes of any graph. Unlike other learning forms in discriminative pattern recognition, this research proposes a scalable generative optimization algorithm that can theoretically capture distributions at the nodes of a graph. Our model generates samples from the probability densities learned at each node, known as the convolutional graph auto-encoder (CGAE), which uses the localized first-order approximation of spectral graph convolutions, deep learning, and the variational Bayesian inference. We applied the CGAE to solve the spatio-temporal probabilistic solar irradiance prediction problem by modeling multiple solar radiation measurement sites in a wide area in northern states of the US as an undirected graph. Our proposed model estimated the distribution of future irradiance based on historical radiation observations for every site/node. The results showed state-of-the-art performance for probabilistic radiation prediction on geographically distributed irradiance data in terms of reliability, sharpness, and continuous ranked probability score in the National Solar Radiation Database.",1
"Graph-structured data arise in wide applications, such as computer vision, bioinformatics, and social networks. Quantifying similarities among graphs is a fundamental problem. In this paper, we develop a framework for computing graph kernels, based on return probabilities of random walks. The advantages of our proposed kernels are that they can effectively exploit various node attributes, while being scalable to large datasets. We conduct extensive graph classification experiments to evaluate our graph kernels. The experimental results show that our graph kernels significantly outperform existing state-of-the-art approaches in both accuracy and computational efficiency.",0
"Graphs are prevalent in various fields like computer vision, bioinformatics, and social networks. Measuring the likeness between graphs is a crucial issue. In this study, we introduce a model for calculating graph kernels using the return probabilities of random walks. Our suggested kernels have the benefit of utilizing diverse node attributes and can handle vast amounts of data. We run extensive experiments on graph classification to assess our graph kernels. The outcomes of the experiments demonstrate that our graph kernels surpass existing state-of-the-art techniques in both accuracy and efficiency.",1
"Recursive neural networks have widely been used by researchers to handle applications with recursively or hierarchically structured data. However, embedded control flow deep learning frameworks such as TensorFlow, Theano, Caffe2, and MXNet fail to efficiently represent and execute such neural networks, due to lack of support for recursion. In this paper, we add recursion to the programming model of existing frameworks by complementing their design with recursive execution of dataflow graphs as well as additional APIs for recursive definitions. Unlike iterative implementations, which can only understand the topological index of each node in recursive data structures, our recursive implementation is able to exploit the recursive relationships between nodes for efficient execution based on parallel computation. We present an implementation on TensorFlow and evaluation results with various recursive neural network models, showing that our recursive implementation not only conveys the recursive nature of recursive neural networks better than other implementations, but also uses given resources more effectively to reduce training and inference time.",0
"Researchers have utilized recursive neural networks extensively to handle data with hierarchical or recursive structures. However, existing deep learning frameworks such as TensorFlow, Theano, Caffe2, and MXNet do not adequately represent and execute such neural networks because they lack support for recursion. This paper proposes a solution by incorporating recursion into the programming model of these frameworks, using recursive execution of dataflow graphs and additional APIs for recursive definitions. Unlike iterative implementations, our recursive approach can take advantage of the recursive relationships between nodes for efficient parallel computation. Our implementation on TensorFlow, along with evaluations using various recursive neural network models, demonstrates that our approach performs better than other implementations by more effectively utilizing resources to reduce training and inference time while also conveying the recursive nature of these networks.",1
"Feature selection, as a data preprocessing strategy, has been proven to be effective and efficient in preparing data (especially high-dimensional data) for various data mining and machine learning problems. The objectives of feature selection include: building simpler and more comprehensible models, improving data mining performance, and preparing clean, understandable data. The recent proliferation of big data has presented some substantial challenges and opportunities to feature selection. In this survey, we provide a comprehensive and structured overview of recent advances in feature selection research. Motivated by current challenges and opportunities in the era of big data, we revisit feature selection research from a data perspective and review representative feature selection algorithms for conventional data, structured data, heterogeneous data and streaming data. Methodologically, to emphasize the differences and similarities of most existing feature selection algorithms for conventional data, we categorize them into four main groups: similarity based, information theoretical based, sparse learning based and statistical based methods. To facilitate and promote the research in this community, we also present an open-source feature selection repository that consists of most of the popular feature selection algorithms (\url{http://featureselection.asu.edu/}). Also, we use it as an example to show how to evaluate feature selection algorithms. At the end of the survey, we present a discussion about some open problems and challenges that require more attention in future research.",0
"The process of feature selection has been found to be a successful and efficient method of preparing data, particularly for high-dimensional data, for various machine learning and data mining problems. Its goals include creating simpler and more understandable models, enhancing data mining performance, and producing clear and comprehensible data. With the rise of big data, feature selection has presented both opportunities and challenges. This survey provides a well-structured and comprehensive overview of recent advancements in feature selection research, taking into account the current challenges and opportunities in the era of big data. The survey categorizes existing feature selection algorithms for conventional data into four main groups - similarity-based, information-theoretical-based, sparse learning-based, and statistical-based methods - to emphasize their differences and similarities. To encourage research in this field, an open-source feature selection repository comprising popular algorithms has been presented, along with an example of how to evaluate these algorithms. Finally, the survey concludes with an analysis of open problems and challenges that require attention in future research.",1
"Important advances have been made using convolutional neural network (CNN) approaches to solve complicated problems in areas that rely on grid structured data such as image processing and object classification. Recently, research on graph convolutional neural networks (GCNN) has increased dramatically as researchers try to replicate the success of CNN for graph structured data. Unfortunately, traditional CNN methods are not readily transferable to GCNN, given the irregularity and geometric complexity of graphs. The emerging field of GCNN is further complicated by research papers that differ greatly in their scope, detail, and level of academic sophistication needed by the reader.   The present paper provides a review of some basic properties of GCNN. As a guide to the interested reader, recent examples of GCNN research are then grouped according to techniques that attempt to uncover the underlying topology of the graph model and those that seek to generalize traditional CNN methods on graph data to improve prediction of class membership. Discrete Signal Processing on Graphs (DSPg) is used as a theoretical framework to better understand some of the performance gains and limitations of these recent GCNN approaches. A brief discussion of Topology Adaptive Graph Convolutional Networks (TAGCN) is presented as an approach motivated by DSPg and future research directions using this approach are briefly discussed.",0
"Convolutional neural networks (CNN) have made significant progress in solving complex problems related to grid structured data, such as image processing and object classification. However, the irregularity and geometric complexity of graphs make it difficult to transfer traditional CNN methods to graph convolutional neural networks (GCNN). As a result, research on GCNN has increased dramatically, but the varying scope and academic level of research papers make it challenging for readers to understand the field. This paper provides a review of basic GCNN properties and recent research examples grouped according to techniques that uncover the graph model's underlying topology or generalize traditional CNN methods. The paper uses Discrete Signal Processing on Graphs (DSPg) as a theoretical framework to better understand the performance gains and limitations of recent GCNN approaches. The paper also briefly discusses Topology Adaptive Graph Convolutional Networks (TAGCN) as a DSPg-motivated approach and future research directions.",1
"Early detection of preventable diseases is important for better disease management, improved inter-ventions, and more efficient health-care resource allocation. Various machine learning approacheshave been developed to utilize information in Electronic Health Record (EHR) for this task. Majorityof previous attempts, however, focus on structured fields and lose the vast amount of information inthe unstructured notes. In this work we propose a general multi-task framework for disease onsetprediction that combines both free-text medical notes and structured information. We compareperformance of different deep learning architectures including CNN, LSTM and hierarchical models.In contrast to traditional text-based prediction models, our approach does not require disease specificfeature engineering, and can handle negations and numerical values that exist in the text. Ourresults on a cohort of about 1 million patients show that models using text outperform modelsusing just structured data, and that models capable of using numerical values and negations in thetext, in addition to the raw text, further improve performance. Additionally, we compare differentvisualization methods for medical professionals to interpret model predictions.",0
"The timely identification of preventable illnesses is crucial to enhance the management of diseases, improve interventions, and allocate healthcare resources more efficiently. To achieve this goal, several machine learning techniques have been developed to leverage the data stored in Electronic Health Records (EHR). Unfortunately, most of these techniques have focused on structured fields, overlooking the wealth of unstructured information contained in clinical notes. In this study, we propose a novel multi-task framework for predicting disease onset that integrates both structured and free-text data. We evaluate the performance of various deep learning architectures, such as CNN, LSTM, and hierarchical models. Unlike traditional text-based models, our approach does not require disease-specific feature engineering and can handle negations and numerical values present in the text. Our results on a cohort of approximately one million patients demonstrate that models incorporating clinical notes outperform models based solely on structured data. Moreover, models capable of utilizing numerical values and negations in addition to the raw text further enhance performance. Additionally, we compare different visualization methods to help medical professionals interpret model predictions.",1
"We apply the network Lasso to solve binary classification and clustering problems for network-structured data. To this end, we generalize ordinary logistic regression to non-Euclidean data with an intrinsic network structure. The resulting ""logistic network Lasso"" amounts to solving a non-smooth convex regularized empirical risk minimization. The risk is measured using the logistic loss incurred over a small set of labeled nodes. For the regularization, we propose to use the total variation of the classifier requiring it to conform to the underlying network structure. A scalable implementation of the learning method is obtained using an inexact variant of the alternating direction methods of multipliers which results in a scalable learning algorithm",0
"Our approach involves utilizing the network Lasso to address binary classification and clustering problems with network-structured data. We accomplish this by extending regular logistic regression to non-Euclidean data with a network structure. This creates the ""logistic network Lasso,"" which involves solving a non-smooth convex regularized empirical risk minimization. We measure the risk by calculating the logistic loss incurred over a small set of labeled nodes. For regularization, we suggest utilizing the total variation of the classifier to ensure it adheres to the underlying network structure. To create a scalable learning algorithm, we utilize an inexact version of the alternating direction methods of multipliers.",1
"We study instancewise feature importance scoring as a method for model interpretation. Any such method yields, for each predicted instance, a vector of importance scores associated with the feature vector. Methods based on the Shapley score have been proposed as a fair way of computing feature attributions of this kind, but incur an exponential complexity in the number of features. This combinatorial explosion arises from the definition of the Shapley value and prevents these methods from being scalable to large data sets and complex models. We focus on settings in which the data have a graph structure, and the contribution of features to the target variable is well-approximated by a graph-structured factorization. In such settings, we develop two algorithms with linear complexity for instancewise feature importance scoring. We establish the relationship of our methods to the Shapley value and another closely related concept known as the Myerson value from cooperative game theory. We demonstrate on both language and image data that our algorithms compare favorably with other methods for model interpretation.",0
"Our research delves into instancewise feature importance scoring as a technique for interpreting models. This method generates a vector of importance scores for each predicted instance based on its feature vector. Although Shapley score-based methods have been proposed for computing feature attributions in a fair manner, they suffer from exponential complexity in the number of features, which makes them unsuitable for large datasets and complex models. Our focus is on data with a graph structure, where feature contribution to the target variable can be well-approximated by a graph-structured factorization. In such scenarios, we have developed two algorithms that have linear complexity for instancewise feature importance scoring. We have also established the relationship between our techniques and the Shapley value and the Myerson value from cooperative game theory. Our algorithms have been tested on both language and image data, and they have demonstrated favorable results when compared to other methods for model interpretation.",1
"The latent feature relational model (LFRM) is a generative model for graph-structured data to learn a binary vector representation for each node in the graph. The binary vector denotes the node's membership in one or more communities. At its core, the LFRM miller2009nonparametric is an overlapping stochastic blockmodel, which defines the link probability between any pair of nodes as a bilinear function of their community membership vectors. Moreover, using a nonparametric Bayesian prior (Indian Buffet Process) enables learning the number of communities automatically from the data. However, despite its appealing properties, inference in LFRM remains a challenge and is typically done via MCMC methods. This can be slow and may take a long time to converge. In this work, we develop a small-variance asymptotics based framework for the non-parametric Bayesian LFRM. This leads to an objective function that retains the nonparametric Bayesian flavor of LFRM, while enabling us to design deterministic inference algorithms for this model, that are easy to implement (using generic or specialized optimization routines) and are fast in practice. Our results on several benchmark datasets demonstrate that our algorithm is competitive to methods such as MCMC, while being much faster.",0
"The latent feature relational model (LFRM) is a model used to learn a binary vector representation for each node in a graph that indicates the node's membership in one or more communities. It is an overlapping stochastic blockmodel that defines the probability of links between nodes as a bilinear function of their community membership vectors. By using a nonparametric Bayesian prior, it can automatically learn the number of communities from the data. However, inferring the model is a challenge and is typically done with MCMC methods, which can be slow and take a long time to converge. We introduce a framework based on small-variance asymptotics for non-parametric Bayesian LFRM that enables us to design deterministic inference algorithms for the model that are easy to implement using optimization routines and are fast in practice. Our algorithm is competitive with MCMC methods, but much faster, as demonstrated on several benchmark datasets.",1
"Numerous pattern recognition applications can be formed as learning from graph-structured data, including social network, protein-interaction network, the world wide web data, knowledge graph, etc. While convolutional neural network (CNN) facilitates great advances in gridded image/video understanding tasks, very limited attention has been devoted to transform these successful network structures (including Inception net, Residual net, Dense net, etc.) to establish convolutional networks on graph, due to its irregularity and complexity geometric topologies (unordered vertices, unfixed number of adjacent edges/vertices). In this paper, we aim to give a comprehensive analysis of when work matters by transforming different classical network structures to graph CNN, particularly in the basic graph recognition problem. Specifically, we firstly review the general graph CNN methods, especially in its spectral filtering operation on the irregular graph data. We then introduce the basic structures of ResNet, Inception and DenseNet into graph CNN and construct these network structures on graph, named as G_ResNet, G_Inception, G_DenseNet. In particular, it seeks to help graph CNNs by shedding light on how these classical network structures work and providing guidelines for choosing appropriate graph network frameworks. Finally, we comprehensively evaluate the performance of these different network structures on several public graph datasets (including social networks and bioinformatic datasets), and demonstrate how different network structures work on graph CNN in the graph recognition task.",0
"Learning from graph-structured data has numerous pattern recognition applications, such as social networks, protein-interaction networks, the world wide web data, and knowledge graphs. Although convolutional neural networks (CNN) have been successful in gridded image/video understanding tasks, they have not been widely used for graph networks due to their irregular and complex geometric topologies. In this study, we aim to analyze the transformation of classical network structures to graph CNN, particularly in basic graph recognition problems. We review general graph CNN methods and introduce ResNet, Inception, and DenseNet structures into graph CNN, named G_ResNet, G_Inception, and G_DenseNet. We evaluate the performance of these network structures on public graph datasets and provide guidelines for choosing appropriate graph network frameworks. Our study sheds light on how these classical network structures work on graph CNN and demonstrates their effectiveness in graph recognition tasks.",1
"Recent advances in graph convolutional networks have significantly improved the performance of chemical predictions, raising a new research question: ""how do we explain the predictions of graph convolutional networks?"" A possible approach to answer this question is to visualize evidence substructures responsible for the predictions. For chemical property prediction tasks, the sample size of the training data is often small and/or a label imbalance problem occurs, where a few samples belong to a single class and the majority of samples belong to the other classes. This can lead to uncertainty related to the learned parameters of the machine learning model. To address this uncertainty, we propose BayesGrad, utilizing the Bayesian predictive distribution, to define the importance of each node in an input graph, which is computed efficiently using the dropout technique. We demonstrate that BayesGrad successfully visualizes the substructures responsible for the label prediction in the artificial experiment, even when the sample size is small. Furthermore, we use a real dataset to evaluate the effectiveness of the visualization. The basic idea of BayesGrad is not limited to graph-structured data and can be applied to other data types.",0
"The performance of chemical predictions has been significantly enhanced by recent developments in graph convolutional networks, which has led to a new research inquiry: ""how can we clarify the predictions of graph convolutional networks?"" One possible way to answer this question is to visualize the evidence substructures that are accountable for the predictions. In chemical property prediction tasks, the training data sample size is frequently small and/or a label imbalance problem arises, where a few samples belong to one category and the majority belong to other categories. This can result in uncertainty regarding the learned parameters of the machine learning model. To tackle this uncertainty, BayesGrad is proposed, which utilizes the Bayesian predictive distribution to define the importance of each node in an input graph. This is done efficiently using the dropout technique. BayesGrad has been shown to effectively visualize the substructures that are responsible for label prediction in an artificial experiment, even when the sample size is small. Additionally, a real dataset was used to evaluate the effectiveness of the visualization. The fundamental idea behind BayesGrad is not restricted to graph-structured data and can be used for other types of data as well.",1
"Fashion preference is a fuzzy concept that depends on customer taste, prevailing norms in fashion product/style, henceforth used interchangeably, and a customer's perception of utility or fashionability, yet fashion e-retail relies on algorithmically generated search and recommendation systems that process structured data and images to best match customer preference. Retailers study tastes solely as a function of what sold vs what did not, and take it to represent customer preference. Such explicit modeling, however, belies the underlying user preference, which is a complicated interplay of preference and commercials such as brand, price point, promotions, other sale events, and competitor push/marketing. It is hard to infer a notion of utility or even customer preference by looking at sales data.   In search and recommendation systems for fashion e-retail, customer preference is implicitly derived by user-user similarity or item-item similarity. In this work, we aim to derive a metric that separates the buying preferences of users from the commercials of the merchandise (price, promotions, etc). We extend our earlier work on explicit signals to gauge sellability or preference with implicit signals from user behaviour.",0
"The idea of fashion preference is complex, as it is influenced by a variety of factors such as personal taste, fashion trends, and perceived usefulness or stylishness. Despite this, online fashion retailers rely on algorithmic systems that use structured data and images to match customer preferences. Retailers often equate sales with customer preference, but this oversimplifies the true nature of user preference. This is because a customer's preference is affected by various commercial factors, including brand, price, promotions, and competitor marketing. Sales data alone cannot accurately determine a customer's preference or utility. Instead, fashion e-retail systems use implicit signals derived from user-user or item-item similarity to gauge customer preference. This study aims to create a metric that distinguishes between a user's buying preferences and commercial factors. It builds on previous work that used explicit signals to measure preference and sellability and incorporates implicit signals from user behavior.",1
"Knowledge graphs are a versatile framework to encode richly structured data relationships, but it can be challenging to combine these graphs with unstructured data. Methods for retrofitting pre-trained entity representations to the structure of a knowledge graph typically assume that entities are embedded in a connected space and that relations imply similarity. However, useful knowledge graphs often contain diverse entities and relations (with potentially disjoint underlying corpora) which do not accord with these assumptions. To overcome these limitations, we present Functional Retrofitting, a framework that generalizes current retrofitting methods by explicitly modeling pairwise relations. Our framework can directly incorporate a variety of pairwise penalty functions previously developed for knowledge graph completion. Further, it allows users to encode, learn, and extract information about relation semantics. We present both linear and neural instantiations of the framework. Functional Retrofitting significantly outperforms existing retrofitting methods on complex knowledge graphs and loses no accuracy on simpler graphs (in which relations do imply similarity). Finally, we demonstrate the utility of the framework by predicting new drug--disease treatment pairs in a large, complex health knowledge graph.",0
"Although knowledge graphs are a useful tool for encoding complex data relationships, it can be difficult to integrate them with unstructured data. Existing methods for retrofitting pre-trained entity representations to knowledge graphs often assume that entities are connected and that relations imply similarity, which may not hold true for diverse knowledge graphs with disjoint underlying corpora. To address this issue, we propose the Functional Retrofitting framework, which models pairwise relations and allows for a variety of pairwise penalty functions. This framework also enables users to encode, learn, and extract information about relation semantics through both linear and neural instantiations. Our experimental results show that Functional Retrofitting performs better on complex knowledge graphs and maintains accuracy on simpler graphs. Additionally, we demonstrate the framework's effectiveness by predicting new drug-disease treatment pairs in a large health knowledge graph.",1
"High dimensional structured data enriched model describes groups of observations by shared and per-group individual parameters, each with its own structure such as sparsity or group sparsity. In this paper, we consider the general form of data enrichment where data comes in a fixed but arbitrary number of groups G. Any convex function, e.g., norms, can characterize the structure of both shared and individual parameters. We propose an estimator for high dimensional data enriched model and provide conditions under which it consistently estimates both shared and individual parameters. We also delineate sample complexity of the estimator and present high probability non-asymptotic bound on estimation error of all parameters. Interestingly the sample complexity of our estimator translates to conditions on both per-group sample sizes and the total number of samples. We propose an iterative estimation algorithm with linear convergence rate and supplement our theoretical analysis with synthetic and real experimental results. Particularly, we show the predictive power of data-enriched model along with its interpretable results in anticancer drug sensitivity analysis.",0
"The high dimensional structured data enriched model characterizes groups of observations based on shared and individual parameters, each having its own structure like sparsity or group sparsity. This paper examines the general form of data enrichment, where data is divided into a fixed number of arbitrary groups G. Any convex function, such as norms, can describe the structure of shared and individual parameters. We propose an estimator for this model, which consistently estimates both shared and individual parameters under certain conditions. The estimator's sample complexity is discussed, and we present a high probability non-asymptotic bound on the estimation error of all parameters. Importantly, our estimator's sample complexity depends on both per-group sample sizes and the total number of samples. We introduce an iterative estimation algorithm that has a linear convergence rate and support our theoretical analysis with synthetic and real experimental results. Specifically, we demonstrate the data-enriched model's predictive power and interpretable results in anticancer drug sensitivity analysis.",1
"Network biology has been successfully used to help reveal complex mechanisms of disease, especially cancer. On the other hand, network biology requires in-depth knowledge to construct disease-specific networks, but our current knowledge is very limited even with the recent advances in human cancer biology. Deep learning has shown a great potential to address the difficult situation like this. However, deep learning technologies conventionally use grid-like structured data, thus application of deep learning technologies to the classification of human disease subtypes is yet to be explored. Recently, graph based deep learning techniques have emerged, which becomes an opportunity to leverage analyses in network biology. In this paper, we proposed a hybrid model, which integrates two key components 1) graph convolution neural network (graph CNN) and 2) relation network (RN). We utilize graph CNN as a component to learn expression patterns of cooperative gene community, and RN as a component to learn associations between learned patterns. The proposed model is applied to the PAM50 breast cancer subtype classification task, the standard breast cancer subtype classification of clinical utility. In experiments of both subtype classification and patient survival analysis, our proposed method achieved significantly better performances than existing methods. We believe that this work is an important starting point to realize the upcoming personalized medicine.",0
"The use of network biology has been successful in uncovering complex disease mechanisms, particularly in cancer research. However, constructing disease-specific networks requires extensive knowledge, which is currently limited despite recent advancements in human cancer biology. Deep learning has the potential to address this issue, but traditional deep learning methods rely on grid-like structured data and have not been explored for the classification of human disease subtypes. Graph-based deep learning techniques have emerged as an opportunity to enhance network biology analyses. In this study, we proposed a hybrid model that combines a graph convolution neural network and a relation network to learn expression patterns of cooperative gene communities and associations between them. We applied this model to the PAM50 breast cancer subtype classification task, achieving significantly better performance than existing methods in both subtype classification and patient survival analysis. We believe that this work is a crucial starting point for the development of personalized medicine.",1
"The task of representing entire graphs has seen a surge of prominent results, mainly due to learning convolutional neural networks (CNNs) on graph-structured data. While CNNs demonstrate state-of-the-art performance in graph classification task, such methods are supervised and therefore steer away from the original problem of network representation in task-agnostic manner. Here, we coherently propose an approach for embedding entire graphs and show that our feature representations with SVM classifier increase classification accuracy of CNN algorithms and traditional graph kernels. For this we describe a recently discovered graph object, anonymous walk, on which we design task-independent algorithms for learning graph representations in explicit and distributed way. Overall, our work represents a new scalable unsupervised learning of state-of-the-art representations of entire graphs.",0
"Learning how to represent entire graphs has experienced a surge in impressive outcomes, primarily because of the employment of convolutional neural networks (CNNs) for analyzing data with a graph structure. Although CNNs have demonstrated exceptional performance in graph classification tasks, these approaches are supervised and deviate from the initial objective of representing networks in a task-agnostic manner. In this paper, we propose a coherent method for embedding entire graphs, and we demonstrate that our feature representations, coupled with an SVM classifier, enhance the classification accuracy of traditional graph kernels and CNN algorithms. We describe a newly discovered graph object, anonymous walk, and utilize it to design algorithms that enable learning task-independent graph representations in an explicit and distributed manner. Overall, our study presents a new method for scalable unsupervised learning of state-of-the-art graph representations.",1
"Deep learning on graph structures has shown exciting results in various applications. However, few attentions have been paid to the robustness of such models, in contrast to numerous research work for image or text adversarial attack and defense. In this paper, we focus on the adversarial attacks that fool the model by modifying the combinatorial structure of data. We first propose a reinforcement learning based attack method that learns the generalizable attack policy, while only requiring prediction labels from the target classifier. Also, variants of genetic algorithms and gradient methods are presented in the scenario where prediction confidence or gradients are available. We use both synthetic and real-world data to show that, a family of Graph Neural Network models are vulnerable to these attacks, in both graph-level and node-level classification tasks. We also show such attacks can be used to diagnose the learned classifiers.",0
"Various applications have yielded exciting results from deep learning on graph structures. However, unlike the numerous works on adversarial attack and defense for images or text, little attention has been paid to the robustness of such models. This paper, therefore, focuses on adversarial attacks that aim to deceive the model by modifying the data's combinatorial structure. To achieve this, we propose a reinforcement learning-based attack method that learns a generalizable attack policy, requiring only prediction labels from the target classifier. Additionally, we present variants of genetic algorithms and gradient methods when prediction confidence or gradients are available. We demonstrate that a family of Graph Neural Network models is vulnerable to these attacks in both graph-level and node-level classification tasks, using synthetic and real-world data. Furthermore, we show that these attacks can be utilized to diagnose learned classifiers.",1
"In recent years, there has been a surge of interest in developing deep learning methods for non-Euclidean structured data such as graphs. In this paper, we propose Dual-Primal Graph CNN, a graph convolutional architecture that alternates convolution-like operations on the graph and its dual. Our approach allows to learn both vertex- and edge features and generalizes the previous graph attention (GAT) model. We provide extensive experimental validation showing state-of-the-art results on a variety of tasks tested on established graph benchmarks, including CORA and Citeseer citation networks as well as MovieLens, Flixter, Douban and Yahoo Music graph-guided recommender systems.",0
"Recently, there has been a rising interest in developing deep learning techniques for non-Euclidean structured data, particularly graphs. This article proposes a Graph Convolutional Neural Network (GCNN) architecture, called Dual-Primal Graph CNN, which performs convolution-like operations on both the graph and its dual. Our method enables the learning of both vertex and edge features and surpasses the previous Graph Attention (GAT) model. We conducted extensive experiments and obtained state-of-the-art results on various tasks evaluated on established graph benchmarks, including CORA and Citeseer citation networks, as well as MovieLens, Flixter, Douban, and Yahoo Music graph-guided recommender systems.",1
"Hidden tree Markov models allow learning distributions for tree structured data while being interpretable as nondeterministic automata. We provide a concise summary of the main approaches in literature, focusing in particular on the causality assumptions introduced by the choice of a specific tree visit direction. We will then sketch a novel non-parametric generalization of the bottom-up hidden tree Markov model with its interpretation as a nondeterministic tree automaton with infinite states.",0
"The use of hidden tree Markov models enables the learning of distributions for data structured in trees, while also being understandable as nondeterministic automata. Our summary of the existing literature on this topic concentrates on the causal assumptions made by selecting a certain direction for visiting the tree. Additionally, we introduce a new bottom-up hidden tree Markov model that is non-parametric and can be interpreted as a nondeterministic tree automaton with an infinite number of states.",1
"Deep generative models for graph-structured data offer a new angle on the problem of chemical synthesis: by optimizing differentiable models that directly generate molecular graphs, it is possible to side-step expensive search procedures in the discrete and vast space of chemical structures. We introduce MolGAN, an implicit, likelihood-free generative model for small molecular graphs that circumvents the need for expensive graph matching procedures or node ordering heuristics of previous likelihood-based methods. Our method adapts generative adversarial networks (GANs) to operate directly on graph-structured data. We combine our approach with a reinforcement learning objective to encourage the generation of molecules with specific desired chemical properties. In experiments on the QM9 chemical database, we demonstrate that our model is capable of generating close to 100% valid compounds. MolGAN compares favorably both to recent proposals that use string-based (SMILES) representations of molecules and to a likelihood-based method that directly generates graphs, albeit being susceptible to mode collapse.",0
"New approaches to chemical synthesis can be achieved by using deep generative models for graph-structured data. By optimizing differentiable models that generate molecular graphs, it is possible to avoid expensive search procedures in the vast space of chemical structures. MolGAN is an implicit, likelihood-free generative model for small molecular graphs that eliminates the need for expensive graph matching procedures or node ordering heuristics of previous likelihood-based methods. Our method utilizes generative adversarial networks (GANs) to operate directly on graph-structured data and is combined with a reinforcement learning objective to encourage the generation of molecules with desired chemical properties. In experiments on the QM9 chemical database, our model generates close to 100% valid compounds and performs better than recent proposals that use string-based (SMILES) representations of molecules and a likelihood-based method that directly generates graphs, which is susceptible to mode collapse.",1
"This paper investigates the computational complexity of sparse label propagation which has been proposed recently for processing network structured data. Sparse label propagation amounts to a convex optimization problem and might be considered as an extension of basis pursuit from sparse vectors to network structured datasets. Using a standard first-order oracle model, we characterize the number of iterations for sparse label propagation to achieve a prescribed accuracy. In particular, we derive an upper bound on the number of iterations required to achieve a certain accuracy and show that this upper bound is sharp for datasets having a chain structure (e.g., time series).",0
"The focus of this research is on the computational complexity of sparse label propagation, a technique that has been put forward for handling network structured data. Sparse label propagation involves solving a convex optimization problem and can be seen as an expansion of basis pursuit to encompass datasets with network structure. Through the use of a standard first-order oracle model, the study examines the number of iterations needed for sparse label propagation to reach a desired level of accuracy. The researchers establish an upper limit on the number of iterations required to achieve the desired accuracy and demonstrate that this upper bound is precise for datasets that possess a chain structure, such as time series.",1
"Geometric model fitting is a fundamental research topic in computer vision and it aims to fit and segment multiple-structure data. In this paper, we propose a novel superpixel-guided two-view geometric model fitting method (called SDF), which can obtain reliable and consistent results for real images. Specifically, SDF includes three main parts: a deterministic sampling algorithm, a model hypothesis updating strategy and a novel model selection algorithm. The proposed deterministic sampling algorithm generates a set of initial model hypotheses according to the prior information of superpixels. Then the proposed updating strategy further improves the quality of model hypotheses. After that, by analyzing the properties of the updated model hypotheses, the proposed model selection algorithm extends the conventional ""fit-and-remove"" framework to estimate model instances in multiple-structure data. The three parts are tightly coupled to boost the performance of SDF in both speed and accuracy, and SDF has the deterministic nature. Experimental results show that the proposed SDF has significant advantages over several state-of-the-art fitting methods when it is applied to real images with single-structure and multiple-structure data.",0
"The fitting of geometric models is a crucial focus of study in computer vision as it involves the fitting and segmentation of data with multiple structures. To achieve reliable and consistent results for real images, we present a new method called SDF, which is a superpixel-guided two-view geometric model fitting technique. SDF comprises three key components: a deterministic sampling algorithm, a strategy for updating model hypotheses, and a novel algorithm for selecting models. The deterministic sampling algorithm generates initial model hypotheses based on the prior information of superpixels. The updating strategy enhances the quality of model hypotheses, while the model selection algorithm extends the conventional ""fit-and-remove"" framework to estimate model instances in data with multiple structures by analyzing the properties of updated model hypotheses. The three components work together to enhance the performance of SDF in terms of both speed and accuracy, with SDF being deterministic in nature. Experimental results indicate that SDF outperforms several state-of-the-art fitting methods when applied to real images containing single and multiple-structure data.",1
"Structural data from Electronic Health Records as complementary information to imaging data for disease prediction. We incorporate novel weighting layer into the Graph Convolutional Networks, which weights every element of structural data by exploring its relation to the underlying disease. We demonstrate the superiority of our developed technique in terms of computational speed and obtained encouraging results where our method outperforms the state-of-the-art methods when applied to two publicly available datasets ABIDE and Chest X-ray in terms of relative performance for the accuracy of prediction by 5.31 % and 8.15 % and for the area under the ROC curve by 4.96 % and 10.36 % respectively. Additionally, the model is lightweight, fast and easily trainable.",0
"The use of Electronic Health Records (EHR) structural data as a complement to imaging data for disease prediction is explored in this study. A novel weighting layer has been incorporated into Graph Convolutional Networks which assigns weights to each element of the structural data based on its relationship with the underlying disease. The developed technique has demonstrated superior computational speed and encouraging results, outperforming existing methods in terms of relative performance for accuracy prediction and area under the ROC curve when applied to two publicly available datasets, ABIDE and Chest X-ray, by 5.31% and 8.15%, and 4.96% and 10.36%, respectively. Additionally, the model is lightweight, fast, and easily trainable.",1
"Graph-structured data such as social networks, functional brain networks, gene regulatory networks, communications networks have brought the interest in generalizing deep learning techniques to graph domains. In this paper, we are interested to design neural networks for graphs with variable length in order to solve learning problems such as vertex classification, graph classification, graph regression, and graph generative tasks. Most existing works have focused on recurrent neural networks (RNNs) to learn meaningful representations of graphs, and more recently new convolutional neural networks (ConvNets) have been introduced. In this work, we want to compare rigorously these two fundamental families of architectures to solve graph learning tasks. We review existing graph RNN and ConvNet architectures, and propose natural extension of LSTM and ConvNet to graphs with arbitrary size. Then, we design a set of analytically controlled experiments on two basic graph problems, i.e. subgraph matching and graph clustering, to test the different architectures. Numerical results show that the proposed graph ConvNets are 3-17% more accurate and 1.5-4x faster than graph RNNs. Graph ConvNets are also 36% more accurate than variational (non-learning) techniques. Finally, the most effective graph ConvNet architecture uses gated edges and residuality. Residuality plays an essential role to learn multi-layer architectures as they provide a 10% gain of performance.",0
"The interest in generalizing deep learning techniques to graph domains has increased due to the prevalence of graph-structured data in social networks, functional brain networks, gene regulatory networks, and communications networks. In this study, we aim to design neural networks for graphs with variable length to solve learning problems like vertex classification, graph classification, graph regression, and graph generative tasks. Previous works have focused on recurrent neural networks (RNNs) to learn meaningful representations of graphs, and newer convolutional neural networks (ConvNets) have been introduced. We want to compare these two types of architectures rigorously to solve graph learning tasks. Our research reviews existing graph RNN and ConvNet architectures and proposes an extension of LSTM and ConvNet to graphs with arbitrary size. We then conduct a set of controlled experiments on two basic graph problems, subgraph matching and graph clustering, to test the different architectures. The numerical results show that graph ConvNets are 3-17% more accurate and 1.5-4x faster than graph RNNs. Additionally, graph ConvNets are 36% more accurate than variational (non-learning) techniques. Finally, we found that the most effective graph ConvNet architecture incorporates gated edges and residuality. Residuality plays a crucial role in learning multi-layer architectures, providing a 10% gain in performance.",1
"We propose a new class of metrics on sets, vectors, and functions that can be used in various stages of data mining, including exploratory data analysis, learning, and result interpretation. These new distance functions unify and generalize some of the popular metrics, such as the Jaccard and bag distances on sets, Manhattan distance on vector spaces, and Marczewski-Steinhaus distance on integrable functions. We prove that the new metrics are complete and show useful relationships with $f$-divergences for probability distributions. To further extend our approach to structured objects such as concept hierarchies and ontologies, we introduce information-theoretic metrics on directed acyclic graphs drawn according to a fixed probability distribution. We conduct empirical investigation to demonstrate intuitive interpretation of the new metrics and their effectiveness on real-valued, high-dimensional, and structured data. Extensive comparative evaluation demonstrates that the new metrics outperformed multiple similarity and dissimilarity functions traditionally used in data mining, including the Minkowski family, the fractional $L^p$ family, two $f$-divergences, cosine distance, and two correlation coefficients. Finally, we argue that the new class of metrics is particularly appropriate for rapid processing of high-dimensional and structured data in distance-based learning.",0
"A novel set of metrics for sets, vectors, and functions are proposed in this study for use in various stages of data mining, including exploratory data analysis, learning, and result interpretation. These new distance functions unify and generalize some of the well-known metrics, such as the Jaccard and bag distances on sets, Manhattan distance on vector spaces, and Marczewski-Steinhaus distance on integrable functions. The completeness of the new metrics is proven, and useful relationships with $f$-divergences for probability distributions are demonstrated. In addition, information-theoretic metrics on directed acyclic graphs drawn according to a fixed probability distribution are introduced to extend the approach to structured objects such as concept hierarchies and ontologies. Empirical investigations are conducted to demonstrate the intuitive interpretation of the new metrics and their effectiveness on real-valued, high-dimensional, and structured data. Extensive comparative evaluations show that the new metrics outperform traditional similarity and dissimilarity functions used in data mining, including the Minkowski family, the fractional $L^p$ family, two $f$-divergences, cosine distance, and two correlation coefficients. Finally, it is argued that the new class of metrics is particularly suitable for rapid processing of high-dimensional and structured data in distance-based learning.",1
"Convolutional neural networks (CNNs) have massively impacted visual recognition in 2D images, and are now ubiquitous in state-of-the-art approaches. CNNs do not easily extend, however, to data that are not represented by regular grids, such as 3D shape meshes or other graph-structured data, to which traditional local convolution operators do not directly apply. To address this problem, we propose a novel graph-convolution operator to establish correspondences between filter weights and graph neighborhoods with arbitrary connectivity. The key novelty of our approach is that these correspondences are dynamically computed from features learned by the network, rather than relying on predefined static coordinates over the graph as in previous work. We obtain excellent experimental results that significantly improve over previous state-of-the-art shape correspondence results. This shows that our approach can learn effective shape representations from raw input coordinates, without relying on shape descriptors.",0
"The impact of convolutional neural networks (CNNs) on the recognition of 2D images is widespread and considered the most advanced approach. However, CNNs are not easily applicable to non-grid-like data, such as 3D shape meshes or graph-structured data, that do not directly support traditional local convolution operators. To overcome this challenge, we introduce a novel graph-convolution operator that establishes correspondences between filter weights and graph neighborhoods with arbitrary connectivity. What sets our approach apart is that these correspondences are computed dynamically from features learned by the network, instead of relying on predefined static coordinates over the graph as in previous research. By doing so, we achieve significantly improved experimental results for shape correspondence, showing the effectiveness of our approach in learning shape representations from raw input coordinates without relying on shape descriptors.",1
Protein-ligand scoring is an important step in a structure-based drug design pipeline. Selecting a correct binding pose and predicting the binding affinity of a protein-ligand complex enables effective virtual screening. Machine learning techniques can make use of the increasing amounts of structural data that are becoming publicly available. Convolutional neural network (CNN) scoring functions in particular have shown promise in pose selection and affinity prediction for protein-ligand complexes. Neural networks are known for being difficult to interpret. Understanding the decisions of a particular network can help tune parameters and training data to maximize performance. Visualization of neural networks helps decompose complex scoring functions into pictures that are more easily parsed by humans. Here we present three methods for visualizing how individual protein-ligand complexes are interpreted by 3D convolutional neural networks. We also present a visualization of the convolutional filters and their weights. We describe how the intuition provided by these visualizations aids in network design.,0
"The scoring of protein-ligand interactions is a crucial aspect of the drug design process, as it allows for the identification of optimal binding poses and the prediction of binding affinity. This, in turn, facilitates effective virtual screening. With the increasing availability of structural data, machine learning techniques, such as convolutional neural networks (CNNs), have shown promise in these areas. However, understanding the inner workings of neural networks can be challenging, making parameter tuning and training difficult. To aid in this process, we present three visualization methods for interpreting how individual protein-ligand complexes are scored by 3D CNNs, as well as a visualization of the convolutional filters and their weights. These visualizations help to deconstruct complex scoring functions and facilitate network design.",1
"Superior performance and ease of implementation have fostered the adoption of Convolutional Neural Networks (CNNs) for a wide array of inference and reconstruction tasks. CNNs implement three basic blocks: convolution, pooling and pointwise nonlinearity. Since the two first operations are well-defined only on regular-structured data such as audio or images, application of CNNs to contemporary datasets where the information is defined in irregular domains is challenging. This paper investigates CNNs architectures to operate on signals whose support can be modeled using a graph. Architectures that replace the regular convolution with a so-called linear shift-invariant graph filter have been recently proposed. This paper goes one step further and, under the framework of multiple-input multiple-output (MIMO) graph filters, imposes additional structure on the adopted graph filters, to obtain three new (more parsimonious) architectures. The proposed architectures result in a lower number of model parameters, reducing the computational complexity, facilitating the training, and mitigating the risk of overfitting. Simulations show that the proposed simpler architectures achieve similar performance as more complex models.",0
"Adoption of Convolutional Neural Networks (CNNs) has been fostered due to their superior performance and ease of implementation in various inference and reconstruction tasks. These networks utilize three fundamental blocks, namely convolution, pooling, and pointwise nonlinearity. However, CNNs are challenging to apply in contemporary datasets where data is irregularly structured. This paper examines CNN architectures for operating on signals represented by graphs. Recent architectures that replace regular convolution with linear shift-invariant graph filters have been proposed. This paper takes a step further by using multiple-input multiple-output (MIMO) graph filters to impose additional structure on the graph filters, resulting in three new, more parsimonious architectures. The proposed architectures have fewer model parameters, lower computational complexity, easier training, and reduced overfitting risk. Simulations indicate that the proposed simpler architectures perform similarly to more complex models.",1
"Graph convolutional networks (GCNs) are powerful deep neural networks for graph-structured data. However, GCN computes the representation of a node recursively from its neighbors, making the receptive field size grow exponentially with the number of layers. Previous attempts on reducing the receptive field size by subsampling neighbors do not have a convergence guarantee, and their receptive field size per node is still in the order of hundreds. In this paper, we develop control variate based algorithms which allow sampling an arbitrarily small neighbor size. Furthermore, we prove new theoretical guarantee for our algorithms to converge to a local optimum of GCN. Empirical results show that our algorithms enjoy a similar convergence with the exact algorithm using only two neighbors per node. The runtime of our algorithms on a large Reddit dataset is only one seventh of previous neighbor sampling algorithms.",0
"The utilization of graph convolutional networks (GCNs) is highly effective in handling graph-structured data. However, the method used by GCN to recursively compute node representations from its neighbors results in the exponential growth of the receptive field size with the number of layers. Previous efforts to decrease the receptive field size by subsampling neighbors did not have guaranteed convergence and still maintained a receptive field size per node of hundreds. This paper introduces new control variate based algorithms that allow for the sampling of a neighbor size that can be arbitrarily small. Additionally, we offer a proof of the algorithm's ability to converge to a local optimum of GCN. Empirical results demonstrate that our algorithms have a comparable convergence rate to the exact algorithm, utilizing only two neighbors per node. Our algorithm's runtime on a large Reddit dataset is only one seventh of that of previous neighbor sampling algorithms.",1
"Graph-based methods are known to be successful in many machine learning and pattern classification tasks. These methods consider semi-structured data as graphs where nodes correspond to primitives (parts, interest points, segments, etc.) and edges characterize the relationships between these primitives. However, these non-vectorial graph data cannot be straightforwardly plugged into off-the-shelf machine learning algorithms without a preliminary step of -- explicit/implicit -- graph vectorization and embedding. This embedding process should be resilient to intra-class graph variations while being highly discriminant. In this paper, we propose a novel high-order stochastic graphlet embedding (SGE) that maps graphs into vector spaces. Our main contribution includes a new stochastic search procedure that efficiently parses a given graph and extracts/samples unlimitedly high-order graphlets. We consider these graphlets, with increasing orders, to model local primitives as well as their increasingly complex interactions. In order to build our graph representation, we measure the distribution of these graphlets into a given graph, using particular hash functions that efficiently assign sampled graphlets into isomorphic sets with a very low probability of collision. When combined with maximum margin classifiers, these graphlet-based representations have positive impact on the performance of pattern comparison and recognition as corroborated through extensive experiments using standard benchmark databases.",0
"Many machine learning and pattern classification tasks have been successful with graph-based methods. These approaches view semi-structured data as graphs, where the nodes represent primitives and the edges reflect the relationships between these primitives. However, these non-vectorial graph data require graph vectorization and embedding before plugging into off-the-shelf machine learning algorithms. The embedding process must be both highly discriminant and resilient to intra-class graph variations. This paper introduces a novel high-order stochastic graphlet embedding (SGE) that maps graphs into vector spaces. The proposed method includes a new stochastic search procedure that parses a graph and extracts/samples unlimitedly high-order graphlets to model local primitives and their increasingly complex interactions. The graphlets' distribution into a given graph is measured using specific hash functions that assign sampled graphlets into isomorphic sets with low collision probability. Combining these graphlet-based representations with maximum margin classifiers positively impacts pattern comparison and recognition performance, as demonstrated through extensive experiments using standard benchmark databases.",1
"We propose an extension of Convolutional Neural Networks (CNNs) to graph-structured data, including strided convolutions and data augmentation on graphs.   Our method matches the accuracy of state-of-the-art CNNs when applied on images, without any prior about their 2D regular structure.   On fMRI data, we obtain a significant gain in accuracy compared with existing graph-based alternatives.",0
"Our suggestion is to enhance Convolutional Neural Networks (CNNs) to be compatible with graph-structured data. This will include integrating strided convolutions as well as applying data augmentation on graphs. By using this approach, we achieve comparable accuracy to state-of-the-art CNNs on images, without any prior knowledge of their 2D regular structure. Additionally, on fMRI data, our method outperforms existing graph-based alternatives by a significant margin.",1
"Graph Convolutional Networks (GCNs) have shown significant improvements in semi-supervised learning on graph-structured data. Concurrently, unsupervised learning of graph embeddings has benefited from the information contained in random walks. In this paper, we propose a model: Network of GCNs (N-GCN), which marries these two lines of work. At its core, N-GCN trains multiple instances of GCNs over node pairs discovered at different distances in random walks, and learns a combination of the instance outputs which optimizes the classification objective. Our experiments show that our proposed N-GCN model improves state-of-the-art baselines on all of the challenging node classification tasks we consider: Cora, Citeseer, Pubmed, and PPI. In addition, our proposed method has other desirable properties, including generalization to recently proposed semi-supervised learning methods such as GraphSAGE, allowing us to propose N-SAGE, and resilience to adversarial input perturbations.",0
"Significant progress has been made in semi-supervised learning on graph-structured data through the use of Graph Convolutional Networks (GCNs). Meanwhile, unsupervised learning of graph embeddings has also benefited from the information obtained through random walks. This paper presents a new model called Network of GCNs (N-GCN) that combines these two approaches. N-GCN trains several instances of GCNs on node pairs found at different distances in random walks and optimizes the classification objective by combining the outputs of these instances. Our experiments demonstrate that N-GCN outperforms existing methods on challenging node classification tasks, including Cora, Citeseer, Pubmed, and PPI. Furthermore, our proposed approach has additional desirable features, such as the ability to generalize to newly proposed semi-supervised learning techniques like GraphSAGE, which allows us to introduce N-SAGE, and robustness against adversarial input perturbations.",1
"Deep generative models have been enjoying success in modeling continuous data. However it remains challenging to capture the representations for discrete structures with formal grammars and semantics, e.g., computer programs and molecular structures. How to generate both syntactically and semantically correct data still remains largely an open problem. Inspired by the theory of compiler where the syntax and semantics check is done via syntax-directed translation (SDT), we propose a novel syntax-directed variational autoencoder (SD-VAE) by introducing stochastic lazy attributes. This approach converts the offline SDT check into on-the-fly generated guidance for constraining the decoder. Comparing to the state-of-the-art methods, our approach enforces constraints on the output space so that the output will be not only syntactically valid, but also semantically reasonable. We evaluate the proposed model with applications in programming language and molecules, including reconstruction and program/molecule optimization. The results demonstrate the effectiveness in incorporating syntactic and semantic constraints in discrete generative models, which is significantly better than current state-of-the-art approaches.",0
"Although deep generative models have been successful in modeling continuous data, it remains difficult to accurately represent discrete structures such as computer programs and molecular structures, which require formal grammars and semantics. Generating data that is both syntactically and semantically correct is a challenge that has yet to be fully solved. Taking inspiration from the theory of compilers, which use syntax-directed translation to check syntax and semantics, we propose a new approach called the syntax-directed variational autoencoder (SD-VAE). This method introduces stochastic lazy attributes to convert offline SDT checks into on-the-fly generated guidance for the decoder. Compared to current state-of-the-art approaches, our approach enforces constraints on the output space, resulting in output that is both syntactically valid and semantically reasonable. We demonstrate the effectiveness of our model in programming language and molecule applications, including reconstruction and optimization. Our results show that incorporating syntactic and semantic constraints into discrete generative models is significantly better than current state-of-the-art methods.",1
"Convolutional Neural Network (CNN)-based machine learning systems have made breakthroughs in feature extraction and image recognition tasks in two dimensions (2D). Although there is significant ongoing work to apply CNN technology to domains involving complex 3D data, the success of such efforts has been constrained, in part, by limitations in data representation techniques. Most current approaches rely upon low-resolution 3D models, strategic limitation of scope in the 3D space, or the application of lossy projection techniques to allow for the use of 2D CNNs. To address this issue, we present a mapping algorithm that converts 3D structures to 2D and 1D data grids by mapping a traversal of a 3D space-filling curve to the traversal of corresponding 2D and 1D curves. We explore the performance of 2D and 1D CNNs trained on data encoded with our method versus comparable volumetric CNNs operating upon raw 3D data from a popular benchmarking dataset. Our experiments demonstrate that both 2D and 1D representations of 3D data generated via our method preserve a significant proportion of the 3D data's features in forms learnable by CNNs. Furthermore, we demonstrate that our method of encoding 3D data into lower-dimensional representations allows for decreased CNN training time cost, increased original 3D model rendering resolutions, and supports increased numbers of data channels when compared to purely volumetric approaches. This demonstration is accomplished in the context of a structural biology classification task wherein we train 3D, 2D, and 1D CNNs on examples of two homologous branches within the Ras protein family. The essential contribution of this paper is the introduction of a dimensionality-reduction method that may ease the application of powerful deep learning tools to domains characterized by complex structural data.",0
"Convolutional Neural Network (CNN) technology has successfully been used to extract features and recognize images in two dimensions (2D). However, applying CNN to domains involving complex 3D data has been challenging due to limited data representation techniques. Current approaches rely on low-resolution 3D models, limiting the scope in the 3D space, or using lossy projection techniques to allow for the use of 2D CNNs. To overcome these limitations, a mapping algorithm has been developed that converts 3D structures to 2D and 1D data grids. This algorithm maps a traversal of a 3D space-filling curve to the corresponding 2D and 1D curves. Experiments show that both 2D and 1D representations of 3D data preserve a significant proportion of the 3D data's features in forms learnable by CNNs. This method also allows for decreased CNN training time cost, increased original 3D model rendering resolutions, and supports increased numbers of data channels. The paper presents a structural biology classification task where 3D, 2D, and 1D CNNs are trained on examples of two homologous branches within the Ras protein family. The dimensionality-reduction method introduced in this paper may ease the application of deep learning tools to domains characterized by complex structural data.",1
"'Big' high-dimensional data are commonly analyzed in low-dimensions, after performing a dimensionality-reduction step that inherently distorts the data structure. For the same purpose, clustering methods are also often used. These methods also introduce a bias, either by starting from the assumption of a particular geometric form of the clusters, or by using iterative schemes to enhance cluster contours, with uncontrollable consequences. The goal of data analysis should, however, be to encode and detect structural data features at all scales and densities simultaneously, without assuming a parametric form of data point distances, or modifying them. We propose a novel approach that directly encodes data point neighborhood similarities as a sparse graph. Our non-iterative framework permits a transparent interpretation of data, without altering the original data dimension and metric. Several natural and synthetic data applications demonstrate the efficacy of our novel approach.",0
"Large sets of high-dimensional data are frequently analyzed in reduced dimensions, which alters the data structure. Clustering methods are also commonly used for this purpose, but they introduce biases by starting with assumptions about the geometric shape of the clusters or by using iterative processes to refine cluster boundaries, which can have unpredictable outcomes. The ultimate goal of data analysis should be to identify structural features of the data at all scales and densities without making assumptions about the parametric form of the distances between data points or modifying them. Our proposed method directly encodes neighborhood similarities between data points as a sparse graph, without altering the original dimensions or metric. Our non-iterative framework provides a clear interpretation of the data. We demonstrate the effectiveness of our approach using both real and synthetic data.",1
"Spectral graph convolutional neural networks (CNNs) require approximation to the convolution to alleviate the computational complexity, resulting in performance loss. This paper proposes the topology adaptive graph convolutional network (TAGCN), a novel graph convolutional network defined in the vertex domain. We provide a systematic way to design a set of fixed-size learnable filters to perform convolutions on graphs. The topologies of these filters are adaptive to the topology of the graph when they scan the graph to perform convolution. The TAGCN not only inherits the properties of convolutions in CNN for grid-structured data, but it is also consistent with convolution as defined in graph signal processing. Since no approximation to the convolution is needed, TAGCN exhibits better performance than existing spectral CNNs on a number of data sets and is also computationally simpler than other recent methods.",0
"To reduce the computational complexity of spectral graph convolutional neural networks (CNNs), an approximation to the convolution is necessary, but this often results in a loss of performance. In order to address this issue, this paper introduces the topology adaptive graph convolutional network (TAGCN), which is a new type of graph convolutional network that operates in the vertex domain. The TAGCN uses a set of fixed-size learnable filters that are systematically designed to perform convolutions on graphs. These filters adapt to the topology of the graph as they scan it to perform the convolution, which makes TAGCN consistent with convolution as defined in graph signal processing. Unlike existing spectral CNNs, TAGCN does not require any approximation to the convolution, resulting in better performance and simpler computation.",1
"We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).",0
"Introducing novel neural network architectures called graph attention networks (GATs), which operate on graph-structured data, we aim to overcome the limitations of prior methods that rely on graph convolutions or their approximations. Our GATs use masked self-attentional layers to stack nodes that can attend over their neighborhoods' features, allowing us to specify different weights to different nodes in a neighborhood without any matrix operations or prior knowledge of the graph structure. This approach addresses key challenges of spectral-based graph neural networks and makes our model applicable to inductive and transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks, including the Cora, Citeseer, and Pubmed citation network datasets, as well as a protein-protein interaction dataset with unseen test graphs during training.",1
"In this paper, we propose a simple and effective {geometric} model fitting method to fit and segment multi-structure data even in the presence of severe outliers. We cast the task of geometric model fitting as a representative mode-seeking problem on hypergraphs. Specifically, a hypergraph is firstly constructed, where the vertices represent model hypotheses and the hyperedges denote data points. The hypergraph involves higher-order similarities (instead of pairwise similarities used on a simple graph), and it can characterize complex relationships between model hypotheses and data points. {In addition, we develop a hypergraph reduction technique to remove ""insignificant"" vertices while retaining as many ""significant"" vertices as possible in the hypergraph}. Based on the {simplified hypergraph, we then propose a novel mode-seeking algorithm to search for representative modes within reasonable time. Finally, the} proposed mode-seeking algorithm detects modes according to two key elements, i.e., the weighting scores of vertices and the similarity analysis between vertices. Overall, the proposed fitting method is able to efficiently and effectively estimate the number and the parameters of model instances in the data simultaneously. Experimental results demonstrate that the proposed method achieves significant superiority over {several} state-of-the-art model fitting methods on both synthetic data and real images.",0
"Our paper introduces a model fitting method that is both straightforward and powerful enough to segment multi-structure data despite the presence of substantial outliers. We approach the task of geometric model fitting as a mode-seeking problem on hypergraphs, where each vertex represents a model hypothesis and each hyperedge represents a data point. Unlike simple graphs that use pairwise similarities, our hypergraph relies on higher-order similarities to capture intricate relationships between model hypotheses and data points. To further refine the hypergraph, we have developed a technique to eliminate insignificant vertices while preserving as many significant vertices as possible. With this simplified hypergraph, we have created a novel mode-seeking algorithm that can detect representative modes in a reasonable amount of time, using weighting scores and similarity analysis between vertices as key criteria. Our method can simultaneously estimate the number and parameters of model instances in the data, as demonstrated through experiments that show its superiority over several state-of-the-art model fitting methods on both synthetic data and real images.",1
"Learning representations on Grassmann manifolds is popular in quite a few visual recognition tasks. In order to enable deep learning on Grassmann manifolds, this paper proposes a deep network architecture by generalizing the Euclidean network paradigm to Grassmann manifolds. In particular, we design full rank mapping layers to transform input Grassmannian data to more desirable ones, exploit re-orthonormalization layers to normalize the resulting matrices, study projection pooling layers to reduce the model complexity in the Grassmannian context, and devise projection mapping layers to respect Grassmannian geometry and meanwhile achieve Euclidean forms for regular output layers. To train the Grassmann networks, we exploit a stochastic gradient descent setting on manifolds of the connection weights, and study a matrix generalization of backpropagation to update the structured data. The evaluations on three visual recognition tasks show that our Grassmann networks have clear advantages over existing Grassmann learning methods, and achieve results comparable with state-of-the-art approaches.",0
"The use of Grassmann manifolds for learning representations has gained popularity in several visual recognition tasks. To facilitate deep learning on Grassmann manifolds, this research introduces a deep network architecture that extends the Euclidean network paradigm to these manifolds. The proposed architecture employs full rank mapping layers to transform input Grassmannian data into more desirable forms, re-orthonormalization layers to normalize resulting matrices, projection pooling layers to reduce model complexity, and projection mapping layers to respect Grassmannian geometry while achieving Euclidean forms for regular output layers. To train these networks, a stochastic gradient descent setting is used on manifolds of the connection weights, and a matrix generalization of backpropagation is employed to update structured data. The evaluations on three visual recognition tasks demonstrate the clear advantages of our Grassmann networks over existing Grassmann learning methods, achieving results comparable with state-of-the-art approaches.",1
"Molecular activity prediction is critical in drug design. Machine learning techniques such as kernel methods and random forests have been successful for this task. These models require fixed-size feature vectors as input while the molecules are variable in size and structure. As a result, fixed-size fingerprint representation is poor in handling substructures for large molecules. In addition, molecular activity tests, or a so-called BioAssays, are relatively small in the number of tested molecules due to its complexity. Here we approach the problem through deep neural networks as they are flexible in modeling structured data such as grids, sequences and graphs. We train multiple BioAssays using a multi-task learning framework, which combines information from multiple sources to improve the performance of prediction, especially on small datasets. We propose Graph Memory Network (GraphMem), a memory-augmented neural network to model the graph structure in molecules. GraphMem consists of a recurrent controller coupled with an external memory whose cells dynamically interact and change through a multi-hop reasoning process. Applied to the molecules, the dynamic interactions enable an iterative refinement of the representation of molecular graphs with multiple bond types. GraphMem is capable of jointly training on multiple datasets by using a specific-task query fed to the controller as an input. We demonstrate the effectiveness of the proposed model for separately and jointly training on more than 100K measurements, spanning across 9 BioAssay activity tests.",0
"Prediction of molecular activity is crucial in drug design, and successful machine learning techniques such as kernel methods and random forests have been used for this purpose. However, these methods require fixed-size feature vectors as input, which is problematic as molecules vary in size and structure. Fixed-size fingerprint representations are particularly poor at handling substructures for large molecules, and molecular activity tests (BioAssays) are typically small in scale due to complexity. To overcome these challenges, we propose using deep neural networks, which can model structured data such as grids, sequences, and graphs, and apply a multi-task learning framework to combine information from multiple sources and improve prediction performance, especially on small datasets. Our approach involves using a Graph Memory Network (GraphMem), a memory-augmented neural network that models the graph structure in molecules. GraphMem includes a recurrent controller coupled with an external memory that interacts dynamically and changes through a multi-hop reasoning process. This enables an iterative refinement of the representation of molecular graphs with multiple bond types. GraphMem can be trained on multiple datasets using a specific-task query fed to the controller as input. We demonstrate the effectiveness of our proposed model by separately and jointly training on over 100K measurements across nine BioAssay activity tests.",1
"Videos are a rich source of high-dimensional structured data, with a wide range of interacting components at varying levels of granularity. In order to improve understanding of unconstrained internet videos, it is important to consider the role of labels at separate levels of abstraction. In this paper, we consider the use of the Bidirectional Inference Neural Network (BINN) for performing graph-based inference in label space for the task of video classification. We take advantage of the inherent hierarchy between labels at increasing granularity. The BINN is evaluated on the first and second release of the YouTube-8M large scale multilabel video dataset. Our results demonstrate the effectiveness of BINN, achieving significant improvements against baseline models.",0
"Structured data of high dimensionality can be found in videos, which consist of a variety of interconnected elements at varying levels of detail. To better comprehend internet videos without restrictions, it is crucial to acknowledge the significance of labels on different levels of abstraction. In this research, we explore the Bidirectional Inference Neural Network (BINN) as a means of utilizing graph-based inference in the labeling space for video classification. We leverage the natural hierarchy between labels at different levels of detail. The efficacy of BINN is assessed using the YouTube-8M large-scale multilabel video dataset's first and second releases. Our findings indicate that BINN is highly effective, surpassing baseline models by a significant margin.",1
"Deep learning has the potential to revolutionize quantum chemistry as it is ideally suited to learn representations for structured data and speed up the exploration of chemical space. While convolutional neural networks have proven to be the first choice for images, audio and video data, the atoms in molecules are not restricted to a grid. Instead, their precise locations contain essential physical information, that would get lost if discretized. Thus, we propose to use continuous-filter convolutional layers to be able to model local correlations without requiring the data to lie on a grid. We apply those layers in SchNet: a novel deep learning architecture modeling quantum interactions in molecules. We obtain a joint model for the total energy and interatomic forces that follows fundamental quantum-chemical principles. This includes rotationally invariant energy predictions and a smooth, differentiable potential energy surface. Our architecture achieves state-of-the-art performance for benchmarks of equilibrium molecules and molecular dynamics trajectories. Finally, we introduce a more challenging benchmark with chemical and structural variations that suggests the path for further work.",0
"The use of deep learning in quantum chemistry could transform the field by facilitating the learning of representations for structured data and speeding up the exploration of chemical space. However, while convolutional neural networks are commonly used for images, audio, and video, atoms in molecules do not conform to a grid, and discretization could cause the loss of critical physical information. To address this issue, we propose using continuous-filter convolutional layers to model local correlations without imposing the grid requirement. Our approach is implemented in SchNet, a novel deep learning framework that models quantum interactions in molecules. Our model achieves a joint prediction for the total energy and interatomic forces that abides by fundamental quantum-chemical principles, including energy predictions that are rotationally invariant and a potential energy surface that is smooth and differentiable. The architecture achieves state-of-the-art results for benchmarking equilibrium molecules and molecular dynamics trajectories, and we present a more challenging benchmark for further research.",1
"The paper introduces the Hidden Tree Markov Network (HTN), a neuro-probabilistic hybrid fusing the representation power of generative models for trees with the incremental and discriminative learning capabilities of neural networks. We put forward a modular architecture in which multiple generative models of limited complexity are trained to learn structural feature detectors whose outputs are then combined and integrated by neural layers at a later stage. In this respect, the model is both deep, thanks to the unfolding of the generative models on the input structures, as well as wide, given the potentially large number of generative modules that can be trained in parallel. Experimental results show that the proposed approach can outperform state-of-the-art syntactic kernels as well as generative kernels built on the same probabilistic model as the HTN.",0
"The Hidden Tree Markov Network (HTN) is a hybrid model that combines the strengths of generative models for trees with the learning abilities of neural networks. The paper presents a modular design, where multiple generative models are trained to identify structural features, and the neural layers integrate their outputs. The model is both deep and wide, as the generative models can be trained in parallel on input structures. The experiments show that HTN outperforms state-of-the-art syntactic and generative kernels based on the same probabilistic model as HTN.",1
"A fundamental challenge in developing high-impact machine learning technologies is balancing the need to model rich, structured domains with the ability to scale to big data. Many important problem areas are both richly structured and large scale, from social and biological networks, to knowledge graphs and the Web, to images, video, and natural language. In this paper, we introduce two new formalisms for modeling structured data, and show that they can both capture rich structure and scale to big data. The first, hinge-loss Markov random fields (HL-MRFs), is a new kind of probabilistic graphical model that generalizes different approaches to convex inference. We unite three approaches from the randomized algorithms, probabilistic graphical models, and fuzzy logic communities, showing that all three lead to the same inference objective. We then define HL-MRFs by generalizing this unified objective. The second new formalism, probabilistic soft logic (PSL), is a probabilistic programming language that makes HL-MRFs easy to define using a syntax based on first-order logic. We introduce an algorithm for inferring most-probable variable assignments (MAP inference) that is much more scalable than general-purpose convex optimization methods, because it uses message passing to take advantage of sparse dependency structures. We then show how to learn the parameters of HL-MRFs. The learned HL-MRFs are as accurate as analogous discrete models, but much more scalable. Together, these algorithms enable HL-MRFs and PSL to model rich, structured data at scales not previously possible.",0
"Developing machine learning technologies that have a substantial impact can be challenging because of the need to balance the ability to model intricate, organized domains with the capacity to handle large datasets. Many crucial problem areas are both vast and structured, such as biological and social networks, the Web, knowledge graphs, images, natural language, and more. The article introduces two novel formalisms for modeling structured data, and demonstrates that they can model rich structures while being scalable to big data. The first formalism is hinge-loss Markov random fields (HL-MRFs), which is a probabilistic graphical model that extends different approaches to convex inference. The article combines three methods from the randomized algorithms, probabilistic graphical models, and fuzzy logic communities, revealing that all three lead to the same inference objective. HL-MRFs are then defined by generalizing this unified objective. The second formalism introduced is probabilistic soft logic (PSL), which is a probabilistic programming language that simplifies the definition of HL-MRFs using a syntax based on first-order logic. The article presents an algorithm for inferring the most-probable variable assignments (MAP inference), which is much more scalable than general-purpose convex optimization approaches because it uses message passing to take advantage of sparse dependency structures. The article also outlines how to learn the parameters of HL-MRFs. The resulting HL-MRFs are just as precise as analogous discrete models but much more scalable. Together, these algorithms allow HL-MRFs and PSL to model rich, structured data at scales that were not previously feasible.",1
"We consider matrix completion for recommender systems from the point of view of link prediction on graphs. Interaction data such as movie ratings can be represented by a bipartite user-item graph with labeled edges denoting observed ratings. Building on recent progress in deep learning on graph-structured data, we propose a graph auto-encoder framework based on differentiable message passing on the bipartite interaction graph. Our model shows competitive performance on standard collaborative filtering benchmarks. In settings where complimentary feature information or structured data such as a social network is available, our framework outperforms recent state-of-the-art methods.",0
"The perspective of link prediction on graphs is adopted when considering matrix completion for recommender systems. A user-item graph with labeled edges indicating observed ratings can represent interaction data such as movie ratings. By leveraging advancements in deep learning on graph-structured data, we suggest a graph auto-encoder framework that relies on differentiable message passing on the bipartite interaction graph. Our model delivers satisfactory results on common collaborative filtering benchmarks. When supplementary feature information or structured data, such as a social network, is obtainable, our framework surpasses recent state-of-the-art techniques.",1
"In this paper, we presented a novel convolutional neural network framework for graph modeling, with the introduction of two new modules specially designed for graph-structured data: the $k$-th order convolution operator and the adaptive filtering module. Importantly, our framework of High-order and Adaptive Graph Convolutional Network (HA-GCN) is a general-purposed architecture that fits various applications on both node and graph centrics, as well as graph generative models. We conducted extensive experiments on demonstrating the advantages of our framework. Particularly, our HA-GCN outperforms the state-of-the-art models on node classification and molecule property prediction tasks. It also generates 32% more real molecules on the molecule generation task, both of which will significantly benefit real-world applications such as material design and drug screening.",0
"This paper introduces a new framework for graph modeling using a convolutional neural network. The framework, called High-order and Adaptive Graph Convolutional Network (HA-GCN), includes two new modules designed specifically for graph-structured data: the $k$-th order convolution operator and the adaptive filtering module. The HA-GCN framework is versatile and can be used for a range of applications, including node and graph-centric tasks and graph generative models. Extensive experiments were conducted to demonstrate the effectiveness of the framework, which showed that HA-GCN outperforms existing models in node classification and molecule property prediction tasks. In addition, HA-GCN generates 32% more real molecules for molecule generation tasks, providing significant benefits for real-world applications such as material design and drug screening.",1
"We propose a family of near-metrics based on local graph diffusion to capture similarity for a wide class of data sets. These quasi-metametrics, as their names suggest, dispense with one or two standard axioms of metric spaces, specifically distinguishability and symmetry, so that similarity between data points of arbitrary type and form could be measured broadly and effectively. The proposed near-metric family includes the forward k-step diffusion and its reverse, typically on the graph consisting of data objects and their features. By construction, this family of near-metrics is particularly appropriate for categorical data, continuous data, and vector representations of images and text extracted via deep learning approaches. We conduct extensive experiments to evaluate the performance of this family of similarity measures and compare and contrast with traditional measures of similarity used for each specific application and with the ground truth when available. We show that for structured data including categorical and continuous data, the near-metrics corresponding to normalized forward k-step diffusion (k small) work as one of the best performing similarity measures; for vector representations of text and images including those extracted from deep learning, the near-metrics derived from normalized and reverse k-step graph diffusion (k very small) exhibit outstanding ability to distinguish data points from different classes.",0
"Our proposal is to use local graph diffusion to create a family of near-metrics that can effectively measure similarity for a wide range of data sets. These quasi-metametrics are called as such because they do not conform to all of the standard axioms of metric spaces, specifically distinguishability and symmetry. This allows for a broad and effective measurement of similarity between data points of varying types and forms. The family of near-metrics includes the forward k-step diffusion and its reverse, typically on a graph made up of data objects and their features. This family of near-metrics is particularly well-suited for categorical data, continuous data, and vector representations of images and text obtained through deep learning methods. We conducted extensive experiments to evaluate the performance of this family of similarity measures and compared them to traditional measures of similarity used for each specific application, as well as the ground truth when available. Our results show that for structured data, including categorical and continuous data, the near-metrics corresponding to normalized forward k-step diffusion (k small) are among the best performing similarity measures. For vector representations of text and images, including those obtained through deep learning, the near-metrics derived from normalized and reverse k-step graph diffusion (k very small) have exceptional ability to distinguish between data points from different classes.",1
"A recently proposed learning algorithm for massive network-structured data sets (big data over networks) is the network Lasso (nLasso), which extends the well- known Lasso estimator from sparse models to network-structured datasets. Efficient implementations of the nLasso have been presented using modern convex optimization methods. In this paper, we provide sufficient conditions on the network structure and available label information such that nLasso accurately learns a vector-valued graph signal (representing label information) from the information provided by the labels of a few data points.",0
"The network Lasso (nLasso) is a newly proposed learning algorithm that can handle massive network-structured data sets, i.e., big data over networks. It is an extension of the well-known Lasso estimator, specifically designed for sparse models. The nLasso has efficient implementations using modern convex optimization methods. This paper establishes sufficient conditions for the network structure and available label information, which allows nLasso to accurately learn a vector-valued graph signal that represents label information, using the labels of only a few data points.",1
"We propose Embedding Propagation (EP), an unsupervised learning framework for graph-structured data. EP learns vector representations of graphs by passing two types of messages between neighboring nodes. Forward messages consist of label representations such as representations of words and other attributes associated with the nodes. Backward messages consist of gradients that result from aggregating the label representations and applying a reconstruction loss. Node representations are finally computed from the representation of their labels. With significantly fewer parameters and hyperparameters an instance of EP is competitive with and often outperforms state of the art unsupervised and semi-supervised learning methods on a range of benchmark data sets.",0
"An unsupervised learning framework for graph-structured data called Embedding Propagation (EP) is being proposed. EP involves learning vector representations of graphs through the exchange of two types of messages between neighboring nodes - forward messages which include label representations and backward messages which include gradients from aggregated label representations and a reconstruction loss. Node representations are then computed from label representations. EP is competitive with and often surpasses state of the art unsupervised and semi-supervised learning methods on various benchmark data sets, despite having fewer parameters and hyperparameters.",1
"Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures.",0
"The occurrence of data structured as graphs is common in various fields such as chemistry, natural language understanding, social networks, and knowledge bases. Our research focuses on studying methods for learning features from graph-structured inputs. Initially, we build on previous research on Graph Neural Networks by Scarselli et al. (2009), but we modify it by using gated recurrent units and modern optimization techniques and expand it to generate output sequences. The outcome is a versatile and widely applicable class of neural network models that possess beneficial inductive biases compared to purely sequence-based models like LSTMs, particularly when addressing graph-structured issues. We showcase its potential by performing simple AI (bAbI) and graph algorithm learning tasks and demonstrate its achievement of state-of-the-art performance on a program verification issue, which entails matching subgraphs to abstract data structures.",1
"Sales forecast is an essential task in E-commerce and has a crucial impact on making informed business decisions. It can help us to manage the workforce, cash flow and resources such as optimizing the supply chain of manufacturers etc. Sales forecast is a challenging problem in that sales is affected by many factors including promotion activities, price changes, and user preferences etc. Traditional sales forecast techniques mainly rely on historical sales data to predict future sales and their accuracies are limited. Some more recent learning-based methods capture more information in the model to improve the forecast accuracy. However, these methods require case-by-case manual feature engineering for specific commercial scenarios, which is usually a difficult, time-consuming task and requires expert knowledge. To overcome the limitations of existing methods, we propose a novel approach in this paper to learn effective features automatically from the structured data using the Convolutional Neural Network (CNN). When fed with raw log data, our approach can automatically extract effective features from that and then forecast sales using those extracted features. We test our method on a large real-world dataset from CaiNiao.com and the experimental results validate the effectiveness of our method.",0
"Making accurate sales forecasts is a critical task in E-commerce as it significantly impacts informed business decisions. It allows for effective management of various aspects such as workforce, cash flow, and supply chain optimization. However, predicting sales is a challenging problem as it is influenced by factors like promotional activities, pricing changes, and customer preferences. Traditional sales forecasting techniques rely on historical data, but their accuracy is limited. Some recent methods improve accuracy by incorporating more information into the model, but they require manual feature engineering for specific scenarios, which is a difficult and time-consuming task. In this paper, we propose a novel approach that utilizes Convolutional Neural Networks (CNN) to automatically extract effective features from structured data to forecast sales. Our approach performs well on a large real-world dataset from CaiNiao.com, validating its effectiveness in overcoming existing limitations.",1
"A number of problems can be formulated as prediction on graph-structured data. In this work, we generalize the convolution operator from regular grids to arbitrary graphs while avoiding the spectral domain, which allows us to handle graphs of varying size and connectivity. To move beyond a simple diffusion, filter weights are conditioned on the specific edge labels in the neighborhood of a vertex. Together with the proper choice of graph coarsening, we explore constructing deep neural networks for graph classification. In particular, we demonstrate the generality of our formulation in point cloud classification, where we set the new state of the art, and on a graph classification dataset, where we outperform other deep learning approaches. The source code is available at https://github.com/mys007/ecc",0
"Prediction on data structured as graphs can present several challenges. In this study, we have developed a method for extending the convolution operator to graphs with different sizes and connections, without relying on the spectral domain. Our approach involves conditioning filter weights on edge labels within a vertex's surrounding neighborhood, allowing for more than simple diffusion. We have also explored the use of graph coarsening to construct deep neural networks for graph classification. Our method has been successful in point cloud classification, achieving a new state-of-the-art, and in a graph classification dataset, outperforming other deep learning methods. Access to the source code can be found at https://github.com/mys007/ecc.",1
"We are in the era of data analytics and data science which is on full bloom. There is abundance of all kinds of data for example biometrics based data, satellite images data, chip-seq data, social network data, sensor based data etc. from a variety of sources. This data abundance is the result of the fact that storage cost is getting cheaper day by day, so people as well as almost all business or scientific organizations are storing more and more data. Most of the real data is multi-dimensional, non-uniform, and big in size, such that it requires a unique pre-processing before analyzing it. In order to make data useful for any kind of analysis, pre-processing is a very important step. This paper presents a unique and novel pre-processing method for multi-dimensional and non-uniform data with the aim of making it uniform and reduced in size without losing much of its value. We have chosen biometric signature data to demonstrate the proposed method as it qualifies for the attributes of being multi-dimensional, non-uniform and big in size. Biometric signature data does not only captures the structural characteristics of a signature but also its behavioral characteristics that are captured using a dynamic signature capture device. These features like pen pressure, pen tilt angle, time taken to sign a document when collected in real-time turn out to be of varying dimensions. This feature data set along with the structural data needs to be pre-processed in order to use it to train a machine learning based model for signature verification purposes. We demonstrate the success of the proposed method over other methods using experimental results for biometric signature data but the same can be implemented for any other data with similar properties from a different domain.",0
"The current era is marked by the flourishing of data analytics and data science. There is an abundance of various types of data available, such as biometrics-based data, satellite images data, chip-seq data, social network data, and sensor-based data, sourced from different places. This abundance is due to the decreasing cost of storage, which has led to individuals, businesses, and scientific organizations storing more and more data. Most of the available data is non-uniform, multi-dimensional, and large-sized, hence requiring unique pre-processing before it can be analyzed. Pre-processing is a crucial step in making data useful for any form of analysis. This paper proposes a novel pre-processing method for multi-dimensional, non-uniform data, with the objective of making it uniform and smaller in size without losing its value. Biometric signature data is chosen as a demonstration of the proposed method due to its multi-dimensional, non-uniform, and large-sized attributes. This type of data captures both the structural and behavioral characteristics of a signature, such as pen pressure, pen tilt angle, and signature duration, which are of varying dimensions when collected in real-time. The proposed method is successful in pre-processing the biometric signature data, as evidenced by the experimental results, and can be applied to other data types with similar characteristics.",1
"With the recent rise in the amount of structured data available, there has been considerable interest in methods for machine learning with graphs. Many of these approaches have been kernel methods, which focus on measuring the similarity between graphs. These generally involving measuring the similarity of structural elements such as walks or paths. Borgwardt and Kriegel proposed the all-paths kernel but emphasized that it is NP-hard to compute and infeasible in practice, favouring instead the shortest-path kernel. In this paper, we introduce a new algorithm for computing the all-paths kernel which is very efficient and enrich it further by including the simple cycles as well. We demonstrate how it is feasible even on large datasets to compute all the paths and simple cycles up to a moderate length. We show how to count labelled paths/simple cycles between vertices of a graph and evaluate a labelled path and simple cycles kernel. Extensive evaluations on a variety of graph datasets demonstrate that the all-paths and cycles kernel has superior performance to the shortest-path kernel and state-of-the-art performance overall.",0
"Due to the increase in structured data availability, there has been significant interest in machine learning methods involving graphs. Many of these approaches utilize kernel methods, which measure graph similarity by analyzing structural elements such as walks or paths. Borgwardt and Kriegel proposed the all-paths kernel, but due to its impracticality, they favored the shortest-path kernel. In this paper, we introduce a new algorithm for computing the all-paths kernel that efficiently includes simple cycles. We show that computing all paths and cycles up to a moderate length is feasible even on large datasets. We also demonstrate how to count labeled paths and cycles between vertices and evaluate a labeled path and cycle kernel. Extensive evaluations on various graph datasets reveal that the all-paths and cycles kernel outperforms the shortest-path kernel and achieves state-of-the-art performance.",1
"Class imbalance is a challenging issue in practical classification problems for deep learning models as well as traditional models. Traditionally successful countermeasures such as synthetic over-sampling have had limited success with complex, structured data handled by deep learning models. In this paper, we propose Deep Over-sampling (DOS), a framework for extending the synthetic over-sampling method to exploit the deep feature space acquired by a convolutional neural network (CNN). Its key feature is an explicit, supervised representation learning, for which the training data presents each raw input sample with a synthetic embedding target in the deep feature space, which is sampled from the linear subspace of in-class neighbors. We implement an iterative process of training the CNN and updating the targets, which induces smaller in-class variance among the embeddings, to increase the discriminative power of the deep representation. We present an empirical study using public benchmarks, which shows that the DOS framework not only counteracts class imbalance better than the existing method, but also improves the performance of the CNN in the standard, balanced settings.",0
"The problem of class imbalance poses a difficult challenge for both deep learning models and traditional models in practical classification tasks. While synthetic over-sampling has been a successful solution in the past, it has proven to be less effective with complex and structured data handled by deep learning models. To address this issue, we introduce Deep Over-sampling (DOS), a framework that leverages the deep feature space acquired by a convolutional neural network (CNN) to extend the synthetic over-sampling method. Our framework employs supervised representation learning, where each raw input sample is presented with a synthetic embedding target in the deep feature space, sampled from the linear subspace of in-class neighbors. We use an iterative process to train the CNN and update the targets, resulting in smaller in-class variance among the embeddings and increased discriminative power of the deep representation. Our empirical study, conducted using public benchmarks, demonstrates that the DOS framework not only outperforms existing methods in addressing class imbalance, but also enhances the performance of the CNN in standard, balanced settings.",1
"We tackle the problem of collaborative filtering (CF) with side information, through the lens of Gaussian Process (GP) regression. Driven by the idea of using the kernel to explicitly model user-item similarities, we formulate the GP in a way that allows the incorporation of low-rank matrix factorisation, arriving at our model, the Tucker Gaussian Process (TGP). Consequently, TGP generalises classical Bayesian matrix factorisation models, and goes beyond them to give a natural and elegant method for incorporating side information, giving enhanced predictive performance for CF problems. Moreover we show that it is a novel model for regression, especially well-suited to grid-structured data and problems where the dependence on covariates is close to being separable.",0
"Our focus is on addressing the challenge of collaborative filtering (CF) with side information by utilizing Gaussian Process (GP) regression. Our approach is based on the concept of utilizing the kernel to explicitly model user-item similarities, which has led us to develop the Tucker Gaussian Process (TGP) model. By incorporating low-rank matrix factorisation, TGP is capable of enhancing predictive performance for CF problems and surpasses traditional Bayesian matrix factorisation models. Additionally, TGP is a unique regression model that is particularly well-suited to grid-structured data and problems where the dependence on covariates is almost separable.",1
"We propose a novel method to fit and segment multi-structural data via convex relaxation. Unlike greedy methods --which maximise the number of inliers-- this approach efficiently searches for a soft assignment of points to models by minimising the energy of the overall classification. Our approach is similar to state-of-the-art energy minimisation techniques which use a global energy. However, we deal with the scaling factor (as the number of models increases) of the original combinatorial problem by relaxing the solution. This relaxation brings two advantages: first, by operating in the continuous domain we can parallelize the calculations. Second, it allows for the use of different metrics which results in a more general formulation.   We demonstrate the versatility of our technique on two different problems of estimating structure from images: plane extraction from RGB-D data and homography estimation from pairs of images. In both cases, we report accurate results on publicly available datasets, in most of the cases outperforming the state-of-the-art.",0
"Our proposed method employs convex relaxation to fit and segment multi-structural data. Unlike greedy techniques that prioritize the number of inliers, our approach efficiently discovers a soft assignment of points to models by minimizing the energy of the overall classification. Our method is comparable to contemporary energy minimization techniques that use a global energy. However, we handle the scaling factor of the original combinatorial problem by relaxing the solution. This relaxation provides two benefits: firstly, by working in the continuous domain, we can parallelize the calculations. Secondly, it allows for the use of various metrics, resulting in a more comprehensive formulation. We demonstrate the adaptability of our methodology in two distinct problems of estimating structure from images. These are plane extraction from RGB-D data and homography estimation from pairs of images. In both cases, we report precise outcomes on publicly available datasets, surpassing the state-of-the-art in most cases.",1
"Neural embeddings have been used with great success in Natural Language Processing (NLP). They provide compact representations that encapsulate word similarity and attain state-of-the-art performance in a range of linguistic tasks. The success of neural embeddings has prompted significant amounts of research into applications in domains other than language. One such domain is graph-structured data, where embeddings of vertices can be learned that encapsulate vertex similarity and improve performance on tasks including edge prediction and vertex labelling. For both NLP and graph based tasks, embeddings have been learned in high-dimensional Euclidean spaces. However, recent work has shown that the appropriate isometric space for embedding complex networks is not the flat Euclidean space, but negatively curved, hyperbolic space. We present a new concept that exploits these recent insights and propose learning neural embeddings of graphs in hyperbolic space. We provide experimental evidence that embedding graphs in their natural geometry significantly improves performance on downstream tasks for several real-world public datasets.",0
"Natural Language Processing (NLP) has experienced great success through the use of neural embeddings, which offer condensed representations that capture word similarity and achieve top-notch performance in various linguistic tasks. The triumph of neural embeddings has led to extensive research into their applications across different domains beyond language. One such domain is graph-structured data, where vertex embeddings can be learned to encapsulate vertex similarity and enhance performance in tasks such as edge prediction and vertex labelling. Previous studies have utilized high-dimensional Euclidean spaces to learn embeddings for both NLP and graph-based tasks. However, recent findings have revealed that negatively curved, hyperbolic space is the appropriate isometric space for embedding complex networks. Based on these insights, we introduce a novel concept that involves learning neural embeddings in hyperbolic space for graphs. Our experiments provide evidence that embedding graphs in their natural geometry significantly boosts performance in downstream tasks for several real-world public datasets.",1
"Graph embedding provides an efficient solution for graph analysis by converting the graph into a low-dimensional space which preserves the structure information. In contrast to the graph structure data, the i.i.d. node embedding can be processed efficiently in terms of both time and space. Current semi-supervised graph embedding algorithms assume the labelled nodes are given, which may not be always true in the real world. While manually label all training data is inapplicable, how to select the subset of training data to label so as to maximize the graph analysis task performance is of great importance. This motivates our proposed active graph embedding (AGE) framework, in which we design a general active learning query strategy for any semi-supervised graph embedding algorithm. AGE selects the most informative nodes as the training labelled nodes based on the graphical information (i.e., node centrality) as well as the learnt node embedding (i.e., node classification uncertainty and node embedding representativeness). Different query criteria are combined with the time-sensitive parameters which shift the focus from graph based query criteria to embedding based criteria as the learning progresses. Experiments have been conducted on three public data sets and the results verified the effectiveness of each component of our query strategy and the power of combining them using time-sensitive parameters. Our code is available online at: https://github.com/vwz/AGE.",0
"Graph embedding is a useful technique for analyzing graphs. By converting graphs into a low-dimensional space, structure information is retained and processing time and space are reduced. However, current semi-supervised graph embedding algorithms assume that labelled nodes are given, which is not always practical in real-world scenarios. Therefore, selecting the most informative nodes to label is important for maximizing graph analysis task performance. To address this issue, we propose an active graph embedding (AGE) framework that employs a general active learning query strategy. AGE selects informative training labelled nodes based on node centrality, node classification uncertainty, and node embedding representativeness. Different query criteria are combined with time-sensitive parameters to shift the focus from graph based query criteria to embedding based criteria as learning progresses. We conducted experiments on three public data sets to verify the effectiveness of our query strategy and demonstrate the power of combining different criteria using time-sensitive parameters. Our code is available online at: https://github.com/vwz/AGE.",1
"Brain imaging data such as EEG or MEG are high-dimensional spatiotemporal data often degraded by complex, non-Gaussian noise. For reliable analysis of brain imaging data, it is important to extract discriminative, low-dimensional intrinsic representation of the recorded data. This work proposes a new method to learn the low-dimensional representations from the noise-degraded measurements. In particular, our work proposes a new deep neural network design that integrates graph information such as brain connectivity with fully-connected layers. Our work leverages efficient graph filter design using Chebyshev polynomial and recent work on convolutional nets on graph-structured data. Our approach exploits graph structure as the prior side information, localized graph filter for feature extraction and neural networks for high capacity learning. Experiments on real MEG datasets show that our approach can extract more discriminative representations, leading to improved accuracy in a supervised classification task.",0
"High-dimensional spatiotemporal brain imaging data, such as EEG or MEG, often contain complex, non-Gaussian noise that can hinder reliable analysis. To address this issue, it is crucial to obtain low-dimensional intrinsic representations of the data that are discriminative. This study proposes a novel approach that uses a deep neural network design, incorporating brain connectivity information with fully-connected layers. The method employs efficient graph filter design, utilizing Chebyshev polynomial and recent convolutional net research on graph-structured data. The approach leverages the graph structure as prior information, localized graph filter for feature extraction and neural networks for high capacity learning. The results of experiments conducted on real MEG data demonstrate that the proposed method can extract more discriminative representations, leading to better accuracy in supervised classification.",1
"This paper introduces a generalization of Convolutional Neural Networks (CNNs) from low-dimensional grid data, such as images, to graph-structured data. We propose a novel spatial convolution utilizing a random walk to uncover the relations within the input, analogous to the way the standard convolution uses the spatial neighborhood of a pixel on the grid. The convolution has an intuitive interpretation, is efficient and scalable and can also be used on data with varying graph structure. Furthermore, this generalization can be applied to many standard regression or classification problems, by learning the the underlying graph. We empirically demonstrate the performance of the proposed CNN on MNIST, and challenge the state-of-the-art on Merck molecular activity data set.",0
"In this paper, a new version of Convolutional Neural Networks (CNNs) is presented that can be applied to graph-structured data, in contrast to just low-dimensional grid data like images. The proposed spatial convolution uses a random walk to analyze connections within the input, similar to how standard convolution uses spatial proximity of pixels on a grid. This convolution is easy to understand, efficient, scalable, and can be used with graph structures that vary. Additionally, this new version can be used for many standard regression or classification problems by learning the graph underneath. To demonstrate its effectiveness, we test the proposed CNN on MNIST and show that it outperforms the state-of-the-art on the Merck molecular activity data set.",1
"Many different classification tasks need to manage structured data, which are usually modeled as graphs. Moreover, these graphs can be dynamic, meaning that the vertices/edges of each graph may change during time. Our goal is to jointly exploit structured data and temporal information through the use of a neural network model. To the best of our knowledge, this task has not been addressed using these kind of architectures. For this reason, we propose two novel approaches, which combine Long Short-Term Memory networks and Graph Convolutional Networks to learn long short-term dependencies together with graph structure. The quality of our methods is confirmed by the promising results achieved.",0
"Structured data used in various classification tasks are often represented as graphs, which may also be dynamic with changing vertices or edges over time. Our objective is to utilize neural network models to simultaneously leverage temporal information and structured data. This specific task has not yet been tackled using such architectures, prompting our proposal of two innovative approaches. By combining Graph Convolutional Networks with Long Short-Term Memory networks, we can effectively learn long short-term dependencies alongside graph structure. The favorable results attained serve as confirmation of our approaches' quality.",1
"This paper describes structuring data and constructing plots to explore forest classification models interactively. A forest classifier is an example of an ensemble, produced by bagging multiple trees. The process of bagging and combining results from multiple trees, produces numerous diagnostics which, with interactive graphics, can provide a lot of insight into class structure in high dimensions. Various aspects are explored in this paper, to assess model complexity, individual model contributions, variable importance and dimension reduction, and uncertainty in prediction associated with individual observations. The ideas are applied to the random forest algorithm, and to the projection pursuit forest, but could be more broadly applied to other bagged ensembles. Interactive graphics are built in R, using the ggplot2, plotly, and shiny packages.",0
"In this paper, the focus is on the exploration of forest classification models through the structuring of data and the creation of plots. A forest classifier, which is an ensemble generated by bagging several trees, is used as an example. The process of bagging and merging the results of several trees creates multiple diagnostic tools that can provide valuable insights into the class structure in high dimensions. The paper explores various aspects of the model, such as model complexity, individual contributions, variable importance, dimension reduction, and uncertainty in prediction for individual observations. While the ideas are applied to the random forest algorithm and projection pursuit forest, they can be adapted to other bagged ensembles. The paper utilizes interactive graphics created in R using the ggplot2, plotly, and shiny packages.",1
"This paper presents a novel method for structural data recognition using a large number of graph models. In general, prevalent methods for structural data recognition have two shortcomings: 1) Only a single model is used to capture structural variation. 2) Naive recognition methods are used, such as the nearest neighbor method. In this paper, we propose strengthening the recognition performance of these models as well as their ability to capture structural variation. The proposed method constructs a large number of graph models and trains decision trees using the models. This paper makes two main contributions. The first is a novel graph model that can quickly perform calculations, which allows us to construct several models in a feasible amount of time. The second contribution is a novel approach to structural data recognition: graph model boosting. Comprehensive structural variations can be captured with a large number of graph models constructed in a boosting framework, and a sophisticated classifier can be formed by aggregating the decision trees. Consequently, we can carry out structural data recognition with powerful recognition capability in the face of comprehensive structural variation. The experiments shows that the proposed method achieves impressive results and outperforms existing methods on datasets of IAM graph database repository.",0
"This paper introduces a new technique for recognizing structural data by utilizing numerous graph models. Traditional methods for structural data recognition suffer from two major limitations: first, they only employ a single model to capture structural variations, and second, they rely on basic recognition techniques, such as the nearest neighbor method. To overcome these shortcomings, this study aims to enhance the recognition performance and structural variation capturing abilities of the models. This is achieved by constructing multiple graph models and training decision trees using them. The paper proposes two major contributions: first, a novel graph model capable of fast calculations, which enables the creation of several models in a reasonable amount of time, and second, graph model boosting as a novel approach to structural data recognition. By constructing a large number of graph models within a boosting framework, the proposed method can capture comprehensive structural variations and generate a sophisticated classifier by aggregating decision trees. As a result, the proposed method can perform structural data recognition with powerful recognition capabilities against comprehensive structural variation. The experiments demonstrate that the proposed method achieves impressive results and outperforms existing methods on datasets of IAM graph database repository.",1
"We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.",0
"Our efficient variant of convolutional neural networks operates directly on graphs and presents a scalable approach for semi-supervised learning on graph-structured data. We chose our convolutional architecture based on a localized first-order approximation of spectral graph convolutions and our model can learn hidden layer representations that encode both local graph structure and node features. Through experiments on citation networks and a knowledge graph dataset, we show that our approach surpasses related methods by a significant margin, while scaling linearly in the number of graph edges.",1
"The success of kernel methods has initiated the design of novel positive semidefinite functions, in particular for structured data. A leading design paradigm for this is the convolution kernel, which decomposes structured objects into their parts and sums over all pairs of parts. Assignment kernels, in contrast, are obtained from an optimal bijection between parts, which can provide a more valid notion of similarity. In general however, optimal assignments yield indefinite functions, which complicates their use in kernel methods. We characterize a class of base kernels used to compare parts that guarantees positive semidefinite optimal assignment kernels. These base kernels give rise to hierarchies from which the optimal assignment kernels are computed in linear time by histogram intersection. We apply these results by developing the Weisfeiler-Lehman optimal assignment kernel for graphs. It provides high classification accuracy on widely-used benchmark data sets improving over the original Weisfeiler-Lehman kernel.",0
"Novel positive semidefinite functions have been designed for structured data, with the success of kernel methods serving as the inspiration. One such design paradigm is the convolution kernel, which breaks down structured objects into their individual parts and computes the sum of all part pairs. Conversely, optimal bijections between parts yield assignment kernels that offer a more accurate measure of similarity. However, optimal assignments often result in indefinite functions, complicating their use in kernel methods. We identified a class of base kernels that ensure positive semidefinite optimal assignment kernels when comparing parts. These base kernels generate hierarchies from which optimal assignment kernels can be computed in linear time using histogram intersection. Our findings are applied to the development of the Weisfeiler-Lehman optimal assignment kernel for graphs, which achieves high classification accuracy on commonly used benchmark datasets, surpassing the performance of the original Weisfeiler-Lehman kernel.",1
"Computational approaches to drug discovery can reduce the time and cost associated with experimental assays and enable the screening of novel chemotypes. Structure-based drug design methods rely on scoring functions to rank and predict binding affinities and poses. The ever-expanding amount of protein-ligand binding and structural data enables the use of deep machine learning techniques for protein-ligand scoring.   We describe convolutional neural network (CNN) scoring functions that take as input a comprehensive 3D representation of a protein-ligand interaction. A CNN scoring function automatically learns the key features of protein-ligand interactions that correlate with binding. We train and optimize our CNN scoring functions to discriminate between correct and incorrect binding poses and known binders and non-binders. We find that our CNN scoring function outperforms the AutoDock Vina scoring function when ranking poses both for pose prediction and virtual screening.",0
"Drug discovery can be expedited and made more cost-effective by using computational approaches, which can also facilitate the identification of novel chemotypes. One of the approaches used is structure-based drug design, which utilizes scoring functions to anticipate binding affinities and poses. The abundance of protein-ligand binding and structural data has made it possible to employ deep machine learning methods for protein-ligand scoring. In this study, we introduce convolutional neural network (CNN) scoring functions that use a comprehensive 3D representation of protein-ligand interaction as input. The CNN scoring function automatically identifies the essential characteristics of protein-ligand interactions that are related to binding. We train and optimize this scoring function to differentiate between correct and incorrect binding poses and to distinguish between known binders and non-binders. Our findings indicate that our CNN scoring function is superior to the AutoDock Vina scoring function in terms of pose prediction and virtual screening.",1
"Deep learning has achieved a remarkable performance breakthrough in several fields, most notably in speech recognition, natural language processing, and computer vision. In particular, convolutional neural network (CNN) architectures currently produce state-of-the-art performance on a variety of image analysis tasks such as object detection and recognition. Most of deep learning research has so far focused on dealing with 1D, 2D, or 3D Euclidean-structured data such as acoustic signals, images, or videos. Recently, there has been an increasing interest in geometric deep learning, attempting to generalize deep learning methods to non-Euclidean structured data such as graphs and manifolds, with a variety of applications from the domains of network analysis, computational social science, or computer graphics. In this paper, we propose a unified framework allowing to generalize CNN architectures to non-Euclidean domains (graphs and manifolds) and learn local, stationary, and compositional task-specific features. We show that various non-Euclidean CNN methods previously proposed in the literature can be considered as particular instances of our framework. We test the proposed method on standard tasks from the realms of image-, graph- and 3D shape analysis and show that it consistently outperforms previous approaches.",0
"Several fields have seen significant progress due to deep learning, particularly in speech recognition, natural language processing, and computer vision. Convolutional neural network (CNN) architectures have produced state-of-the-art results in image analysis tasks such as object recognition and detection. Initially, deep learning research focused on Euclidean-structured data like images, videos, and acoustic signals. Recently, there has been a growing interest in geometric deep learning, which aims to extend deep learning methods to non-Euclidean data like graphs and manifolds. Our paper presents a unified framework that generalizes CNN architectures to non-Euclidean domains and enables learning of local, stationary, and compositional task-specific features. We demonstrate that various non-Euclidean CNN approaches proposed in the literature can be considered as particular instances of our framework. Our proposed method outperforms previous approaches consistently in standard tasks from image, graph, and 3D shape analysis.",1
"We introduce the variational graph auto-encoder (VGAE), a framework for unsupervised learning on graph-structured data based on the variational auto-encoder (VAE). This model makes use of latent variables and is capable of learning interpretable latent representations for undirected graphs. We demonstrate this model using a graph convolutional network (GCN) encoder and a simple inner product decoder. Our model achieves competitive results on a link prediction task in citation networks. In contrast to most existing models for unsupervised learning on graph-structured data and link prediction, our model can naturally incorporate node features, which significantly improves predictive performance on a number of benchmark datasets.",0
"The variational graph auto-encoder (VGAE) is a framework that utilizes latent variables for unsupervised learning on graph-structured data. It is based on the variational auto-encoder (VAE) and can effectively learn interpretable latent representations for undirected graphs. In our approach, we use a graph convolutional network (GCN) encoder and a simple inner product decoder to showcase the capabilities of our model. Our results on a citation network link prediction task demonstrate that our model achieves competitive performance. Notably, our model stands out from other unsupervised learning models and link prediction models for graph-structured data since it can seamlessly include node features, resulting in a marked improvement in predictive accuracy across a range of benchmark datasets.",1
"We propose a new algorithm for fast approximate nearest neighbor search based on the properties of ordered vectors. Data vectors are classified based on the index and sign of their largest components, thereby partitioning the space in a number of cones centered in the origin. The query is itself classified, and the search starts from the selected cone and proceeds to neighboring ones. Overall, the proposed algorithm corresponds to locality sensitive hashing in the space of directions, with hashing based on the order of components. Thanks to the statistical features emerging through ordering, it deals very well with the challenging case of unstructured data, and is a valuable building block for more complex techniques dealing with structured data. Experiments on both simulated and real-world data prove the proposed algorithm to provide a state-of-the-art performance.",0
"Our proposal presents a novel algorithm that enables rapid and approximate searches for nearest neighbors by utilizing ordered vectors. To achieve this, we sort the data vectors based on their largest component's index and sign, which enables the partitioning of the space into multiple cones centered at the origin. The query is also classified, and the search begins from the chosen cone and moves on to adjacent ones. Our algorithm is essentially locality sensitive hashing, but in the direction space, using component ordering for hashing. By leveraging the statistical properties that arise from ordering, our approach effectively handles the challenging problem of unstructured data. Furthermore, it serves as a crucial building block for more advanced techniques that deal with structured data. Our experiments on both real-world and simulated data demonstrate that our proposed algorithm delivers state-of-the-art performance.",1
"Current approaches for fine-grained recognition do the following: First, recruit experts to annotate a dataset of images, optionally also collecting more structured data in the form of part annotations and bounding boxes. Second, train a model utilizing this data. Toward the goal of solving fine-grained recognition, we introduce an alternative approach, leveraging free, noisy data from the web and simple, generic methods of recognition. This approach has benefits in both performance and scalability. We demonstrate its efficacy on four fine-grained datasets, greatly exceeding existing state of the art without the manual collection of even a single label, and furthermore show first results at scaling to more than 10,000 fine-grained categories. Quantitatively, we achieve top-1 accuracies of 92.3% on CUB-200-2011, 85.4% on Birdsnap, 93.4% on FGVC-Aircraft, and 80.8% on Stanford Dogs without using their annotated training sets. We compare our approach to an active learning approach for expanding fine-grained datasets.",0
"The typical method for fine-grained recognition involves hiring experts to annotate images and collecting structured data, followed by model training. However, we introduce an alternative approach that utilizes free, noisy data from the internet and basic recognition methods. This approach offers advantages in both performance and scalability. Our method surpasses existing state-of-the-art results on four fine-grained datasets without the need for manual labeling, and we also demonstrate potential for scaling to over 10,000 categories. We achieve top-1 accuracies of 92.3% on CUB-200-2011, 85.4% on Birdsnap, 93.4% on FGVC-Aircraft, and 80.8% on Stanford Dogs without using their annotated training sets. In comparison to an active learning approach for expanding fine-grained datasets, our approach proves to be more effective.",1
"Multivariate time series naturally exist in many fields, like energy, bioinformatics, signal processing, and finance. Most of these applications need to be able to compare these structured data. In this context, dynamic time warping (DTW) is probably the most common comparison measure. However, not much research effort has been put into improving it by learning. In this paper, we propose a novel method for learning similarities based on DTW, in order to improve time series classification. Making use of the uniform stability framework, we provide the first theoretical guarantees in the form of a generalization bound for linear classification. The experimental study shows that the proposed approach is efficient, while yielding sparse classifiers.",0
"Multivariate time series occur naturally in various fields such as energy, bioinformatics, signal processing, and finance. Comparing these structured data is necessary for most of these applications. Dynamic time warping (DTW) is a commonly used measure for such comparisons. However, not much research has been dedicated to enhancing DTW through learning. In the present study, we introduce a new method for learning similarities based on DTW to improve time series classification. Utilizing the uniform stability framework, we provide the first theoretical assurances in the form of a generalization bound for linear classification. Our experimental analysis proves that the proposed approach is effective and produces sparse classifiers.",1
"We propose a novel kernel based post selection inference (PSI) algorithm, which can not only handle non-linearity in data but also structured output such as multi-dimensional and multi-label outputs. Specifically, we develop a PSI algorithm for independence measures, and propose the Hilbert-Schmidt Independence Criterion (HSIC) based PSI algorithm (hsicInf). The novelty of the proposed algorithm is that it can handle non-linearity and/or structured data through kernels. Namely, the proposed algorithm can be used for wider range of applications including nonlinear multi-class classification and multi-variate regressions, while existing PSI algorithms cannot handle them. Through synthetic experiments, we show that the proposed approach can find a set of statistically significant features for both regression and classification problems. Moreover, we apply the hsicInf algorithm to a real-world data, and show that hsicInf can successfully identify important features.",0
"Our proposed algorithm, the Hilbert-Schmidt Independence Criterion (HSIC) based PSI algorithm (hsicInf), is a kernel-based method that can handle non-linearity and structured output, such as multi-dimensional and multi-label outputs. Unlike existing PSI algorithms, hsicInf can be used for a wider range of applications, including nonlinear multi-class classification and multi-variate regressions. We demonstrate the effectiveness of our approach through synthetic experiments, where we show that it can identify statistically significant features for both regression and classification problems. Additionally, we apply the hsicInf algorithm to real-world data and show that it can successfully identify important features.",1
"This paper proposes a two-view deterministic geometric model fitting method, termed Superpixel-based Deterministic Fitting (SDF), for multiple-structure data. SDF starts from superpixel segmentation, which effectively captures prior information of feature appearances. The feature appearances are beneficial to reduce the computational complexity for deterministic fitting methods. SDF also includes two original elements, i.e., a deterministic sampling algorithm and a novel model selection algorithm. The two algorithms are tightly coupled to boost the performance of SDF in both speed and accuracy. Specifically, the proposed sampling algorithm leverages the grouping cues of superpixels to generate reliable and consistent hypotheses. The proposed model selection algorithm further makes use of desirable properties of the generated hypotheses, to improve the conventional fit-and-remove framework for more efficient and effective performance. The key characteristic of SDF is that it can efficiently and deterministically estimate the parameters of model instances in multi-structure data. Experimental results demonstrate that the proposed SDF shows superiority over several state-of-the-art fitting methods for real images with single-structure and multiple-structure data.",0
"In this article, we introduce a new approach for fitting geometric models to multiple-structure data called Superpixel-based Deterministic Fitting (SDF). Our method employs superpixel segmentation to capture prior information of feature appearances, which reduces computational complexity for deterministic fitting. Additionally, we incorporate two unique algorithms, a deterministic sampling algorithm and a novel model selection algorithm, which work together to improve SDF's speed and accuracy. The sampling algorithm uses grouping cues from superpixels to generate reliable hypotheses, while the model selection algorithm utilizes desirable properties of these hypotheses to enhance the conventional fit-and-remove framework. SDF is able to efficiently and deterministically estimate model parameters for multi-structure data. Our experimental results demonstrate the superiority of SDF over several state-of-the-art fitting methods for real images with single-structure and multiple-structure data.",1
"In this paper, we propose a novel hypergraph based method (called HF) to fit and segment multi-structural data. The proposed HF formulates the geometric model fitting problem as a hypergraph partition problem based on a novel hypergraph model. In the hypergraph model, vertices represent data points and hyperedges denote model hypotheses. The hypergraph, with large and ""data-determined"" degrees of hyperedges, can express the complex relationships between model hypotheses and data points. In addition, we develop a robust hypergraph partition algorithm to detect sub-hypergraphs for model fitting. HF can effectively and efficiently estimate the number of, and the parameters of, model instances in multi-structural data heavily corrupted with outliers simultaneously. Experimental results show the advantages of the proposed method over previous methods on both synthetic data and real images.",0
"This paper introduces a new approach, named HF, for fitting and segmenting multi-structural data. The method uses a hypergraph model to solve the geometric model fitting problem. In this model, data points are represented as vertices and model hypotheses are represented as hyperedges. The hypergraph allows for complex relationships between the model hypotheses and data points. Additionally, a robust hypergraph partition algorithm is developed to detect sub-hypergraphs for model fitting. HF is capable of estimating the number and parameters of model instances in data that is heavily corrupted with outliers. The proposed method is compared to previous methods using both synthetic data and real images, and demonstrates superior performance.",1
"We present diffusion-convolutional neural networks (DCNNs), a new model for graph-structured data. Through the introduction of a diffusion-convolution operation, we show how diffusion-based representations can be learned from graph-structured data and used as an effective basis for node classification. DCNNs have several attractive qualities, including a latent representation for graphical data that is invariant under isomorphism, as well as polynomial-time prediction and learning that can be represented as tensor operations and efficiently implemented on the GPU. Through several experiments with real structured datasets, we demonstrate that DCNNs are able to outperform probabilistic relational models and kernel-on-graph methods at relational node classification tasks.",0
"Our study introduces diffusion-convolutional neural networks (DCNNs) as a novel approach for graph-structured data. By incorporating a diffusion-convolution procedure, we illustrate how graph-structured data can generate diffusion-based representations that are useful for node classification. DCNNs have multiple advantages, including an isomorphism-invariant latent representation for graphical data and polynomial-time learning and prediction that are tensor operations and GPU compatible. We conducted various experiments using actual structured datasets, and our findings demonstrate that DCNNs are more efficient than probabilistic relational models and kernel-on-graph techniques in relational node classification tasks.",1
"A number of applications in engineering, social sciences, physics, and biology involve inference over networks. In this context, graph signals are widely encountered as descriptors of vertex attributes or features in graph-structured data. Estimating such signals in all vertices given noisy observations of their values on a subset of vertices has been extensively analyzed in the literature of signal processing on graphs (SPoG). This paper advocates kernel regression as a framework generalizing popular SPoG modeling and reconstruction and expanding their capabilities. Formulating signal reconstruction as a regression task on reproducing kernel Hilbert spaces of graph signals permeates benefits from statistical learning, offers fresh insights, and allows for estimators to leverage richer forms of prior information than existing alternatives. A number of SPoG notions such as bandlimitedness, graph filters, and the graph Fourier transform are naturally accommodated in the kernel framework. Additionally, this paper capitalizes on the so-called representer theorem to devise simpler versions of existing Thikhonov regularized estimators, and offers a novel probabilistic interpretation of kernel methods on graphs based on graphical models. Motivated by the challenges of selecting the bandwidth parameter in SPoG estimators or the kernel map in kernel-based methods, the present paper further proposes two multi-kernel approaches with complementary strengths. Whereas the first enables estimation of the unknown bandwidth of bandlimited signals, the second allows for efficient graph filter selection. Numerical tests with synthetic as well as real data demonstrate the merits of the proposed methods relative to state-of-the-art alternatives.",0
"In various fields such as engineering, social sciences, physics, and biology, there is a need for inference over networks. Graph signals are commonly used to describe vertex attributes or features in graph-structured data in this context. The literature of signal processing on graphs (SPoG) has extensively analyzed the estimation of these signals in all vertices, given noisy observations of their values on a subset of vertices. This paper suggests using kernel regression as a framework to generalize popular SPoG modeling and reconstruction and enhance their capabilities. By formulating signal reconstruction as a regression task on reproducing kernel Hilbert spaces of graph signals, statistical learning benefits can be utilized, new insights can be gained, and richer forms of prior information can be leveraged. The kernel framework naturally accommodates various SPoG concepts such as bandlimitedness, graph filters, and the graph Fourier transform. The paper also introduces simpler versions of existing Thikhonov regularized estimators using the representer theorem and offers a novel probabilistic interpretation of kernel methods on graphs based on graphical models. To address the challenges of selecting the bandwidth parameter in SPoG estimators or the kernel map in kernel-based methods, the paper proposes two multi-kernel approaches with complementary strengths. The first approach enables the estimation of the unknown bandwidth of bandlimited signals, while the second allows for efficient graph filter selection. Numerical tests using both synthetic and real data showcase the advantages of the proposed methods compared to state-of-the-art alternatives.",1
"In this paper, we propose a novel geometric model fitting method, called Mode-Seeking on Hypergraphs (MSH),to deal with multi-structure data even in the presence of severe outliers. The proposed method formulates geometric model fitting as a mode seeking problem on a hypergraph in which vertices represent model hypotheses and hyperedges denote data points. MSH intuitively detects model instances by a simple and effective mode seeking algorithm. In addition to the mode seeking algorithm, MSH includes a similarity measure between vertices on the hypergraph and a weight-aware sampling technique. The proposed method not only alleviates sensitivity to the data distribution, but also is scalable to large scale problems. Experimental results further demonstrate that the proposed method has significant superiority over the state-of-the-art fitting methods on both synthetic data and real images.",0
"We introduce a new approach for fitting geometric models, which we call Mode-Seeking on Hypergraphs (MSH), that can handle multi-structure data and outliers. Our method views geometric model fitting as a mode seeking problem on a hypergraph, where vertices represent model hypotheses and hyperedges denote data points. MSH uses a straightforward and effective mode seeking algorithm to detect model instances, along with a similarity measure between hypergraph vertices and a weight-aware sampling technique. This method reduces sensitivity to data distribution and is scalable to large-scale problems. Our experimental results show that MSH outperforms existing fitting methods on both synthetic and real image data.",1
"By taking the semantic object parsing task as an exemplar application scenario, we propose the Graph Long Short-Term Memory (Graph LSTM) network, which is the generalization of LSTM from sequential data or multi-dimensional data to general graph-structured data. Particularly, instead of evenly and fixedly dividing an image to pixels or patches in existing multi-dimensional LSTM structures (e.g., Row, Grid and Diagonal LSTMs), we take each arbitrary-shaped superpixel as a semantically consistent node, and adaptively construct an undirected graph for each image, where the spatial relations of the superpixels are naturally used as edges. Constructed on such an adaptive graph topology, the Graph LSTM is more naturally aligned with the visual patterns in the image (e.g., object boundaries or appearance similarities) and provides a more economical information propagation route. Furthermore, for each optimization step over Graph LSTM, we propose to use a confidence-driven scheme to update the hidden and memory states of nodes progressively till all nodes are updated. In addition, for each node, the forgets gates are adaptively learned to capture different degrees of semantic correlation with neighboring nodes. Comprehensive evaluations on four diverse semantic object parsing datasets well demonstrate the significant superiority of our Graph LSTM over other state-of-the-art solutions.",0
"We introduce the Graph Long Short-Term Memory (Graph LSTM) network, which is a more generalized version of LSTM that can process graph-structured data. Our proposed model is particularly suited for semantic object parsing tasks. Instead of dividing images into evenly fixed pixels or patches as in previous multi-dimensional LSTM structures (e.g. Row, Grid and Diagonal LSTMs), we treat arbitrary-shaped superpixels as semantically consistent nodes and use the spatial relations between these nodes as edges to build an undirected graph. The Graph LSTM network is more aligned with visual patterns in the image, such as object boundaries or appearance similarities, and provides an efficient information propagation route. We also propose a confidence-driven scheme to progressively update the hidden and memory states of nodes during optimization. Additionally, the forget gates are adaptively learned for each node to capture different degrees of semantic correlation with neighboring nodes. Our comprehensive evaluations on four different semantic object parsing datasets show that our Graph LSTM outperforms other state-of-the-art solutions.",1
"The construction of a meaningful graph plays a crucial role in the success of many graph-based representations and algorithms for handling structured data, especially in the emerging field of graph signal processing. However, a meaningful graph is not always readily available from the data, nor easy to define depending on the application domain. In particular, it is often desirable in graph signal processing applications that a graph is chosen such that the data admit certain regularity or smoothness on the graph. In this paper, we address the problem of learning graph Laplacians, which is equivalent to learning graph topologies, such that the input data form graph signals with smooth variations on the resulting topology. To this end, we adopt a factor analysis model for the graph signals and impose a Gaussian probabilistic prior on the latent variables that control these signals. We show that the Gaussian prior leads to an efficient representation that favors the smoothness property of the graph signals. We then propose an algorithm for learning graphs that enforces such property and is based on minimizing the variations of the signals on the learned graph. Experiments on both synthetic and real world data demonstrate that the proposed graph learning framework can efficiently infer meaningful graph topologies from signal observations under the smoothness prior.",0
"The success of graph-based representations and algorithms for structured data, especially in graph signal processing, relies heavily on meaningful graph construction. However, defining a meaningful graph is not always straightforward and can be challenging depending on the application domain. Graph signal processing applications often require a graph that exhibits certain regularity or smoothness properties. In this study, we address the task of learning graph Laplacians, which is equivalent to learning graph topologies that enable input data to form graph signals with smooth variations. To achieve this, we introduce a factor analysis model for the graph signals and impose a Gaussian probabilistic prior on the latent variables that govern these signals. The Gaussian prior promotes an efficient representation that favors the smoothness property of the graph signals. We propose an algorithm that learns graphs by minimizing signal variations and enforcing smoothness properties. Our experiments on both synthetic and real-world data demonstrate that our proposed graph learning framework efficiently infers meaningful graph topologies from signal observations under the smoothness prior.",1
"Sparse representations have been successfully applied to signal processing, computer vision and machine learning. Currently there is a trend to learn sparse models directly on structure data, such as region covariance. However, such methods when combined with region covariance often require complex computation. We present an approach to transform a structured sparse model learning problem to a traditional vectorized sparse modeling problem by constructing a Euclidean space representation for region covariance matrices. Our new representation has multiple advantages. Experiments on several vision tasks demonstrate competitive performance with the state-of-the-art methods.",0
"Sparse models have proven effective in signal processing, machine learning, and computer vision. Recently, there has been a movement towards training sparse models on structured data, like region covariance. Unfortunately, this approach can be computationally challenging. To address this issue, we propose a method that transforms structured sparse model learning into a traditional vectorized sparse modeling problem. This transformation is achieved by constructing a Euclidean space representation for region covariance matrices. Our new representation offers multiple benefits and, when tested on various vision tasks, performs competitively with state-of-the-art methods.",1
"Learning the kernel functions used in kernel methods has been a vastly explored area in machine learning. It is now widely accepted that to obtain 'good' performance, learning a kernel function is the key challenge. In this work we focus on learning kernel representations for structured regression. We propose use of polynomials expansion of kernels, referred to as Schoenberg transforms and Gegenbaur transforms, which arise from the seminal result of Schoenberg (1938). These kernels can be thought of as polynomial combination of input features in a high dimensional reproducing kernel Hilbert space (RKHS). We learn kernels over input and output for structured data, such that, dependency between kernel features is maximized. We use Hilbert-Schmidt Independence Criterion (HSIC) to measure this. We also give an efficient, matrix decomposition-based algorithm to learn these kernel transformations, and demonstrate state-of-the-art results on several real-world datasets.",0
"The field of machine learning has extensively explored the learning of kernel functions used in kernel methods. It is widely acknowledged that mastering the art of kernel function learning is crucial for achieving good performance. This study concentrates on the acquisition of kernel representations for structured regression. Our approach involves using polynomial expansions of kernels, namely Schoenberg transforms and Gegenbaur transforms, which stem from Schoenberg's seminal work in 1938. These kernels can be seen as polynomial combinations of input features in a high-dimensional reproducing kernel Hilbert space (RKHS). Our aim is to learn kernels for structured data, considering both input and output, maximizing the dependency between kernel features, and using the Hilbert-Schmidt Independence Criterion (HSIC) to measure this. We also present an efficient algorithm based on matrix decomposition to learn these kernel transformations and demonstrate their superior performance on various real-world datasets.",1
"The complexity of the visual world creates significant challenges for comprehensive visual understanding. In spite of recent successes in visual recognition, today's vision systems would still struggle to deal with visual queries that require a deeper reasoning. We propose a knowledge base (KB) framework to handle an assortment of visual queries, without the need to train new classifiers for new tasks. Building such a large-scale multimodal KB presents a major challenge of scalability. We cast a large-scale MRF into a KB representation, incorporating visual, textual and structured data, as well as their diverse relations. We introduce a scalable knowledge base construction system that is capable of building a KB with half billion variables and millions of parameters in a few hours. Our system achieves competitive results compared to purpose-built models on standard recognition and retrieval tasks, while exhibiting greater flexibility in answering richer visual queries.",0
"Comprehending the intricate visual world is a formidable task, and even though visual recognition has made some strides, there are still challenges in tackling complex visual queries that demand deeper reasoning. A feasible solution is the knowledge base (KB) framework, which can manage a range of visual queries without requiring new classifiers for individual tasks. However, constructing a large-scale multimodal KB presents scalability issues. To address this, we have transformed a large-scale MRF into a KB representation that includes visual, textual, and structured data, along with their varied relationships. Furthermore, we have developed a scalable knowledge base construction system that can build a KB with half a billion variables and millions of parameters in just a few hours. Our system yields competitive outcomes on standard recognition and retrieval tasks compared to purpose-built models, while also providing greater flexibility in answering complex visual queries.",1
"Relational machine learning studies methods for the statistical analysis of relational, or graph-structured, data. In this paper, we provide a review of how such statistical models can be ""trained"" on large knowledge graphs, and then used to predict new facts about the world (which is equivalent to predicting new edges in the graph). In particular, we discuss two fundamentally different kinds of statistical relational models, both of which can scale to massive datasets. The first is based on latent feature models such as tensor factorization and multiway neural networks. The second is based on mining observable patterns in the graph. We also show how to combine these latent and observable models to get improved modeling power at decreased computational cost. Finally, we discuss how such statistical models of graphs can be combined with text-based information extraction methods for automatically constructing knowledge graphs from the Web. To this end, we also discuss Google's Knowledge Vault project as an example of such combination.",0
"The topic of relational machine learning involves statistical analysis techniques for data that is graph-structured or relational. This article provides an overview of how statistical models can be ""trained"" on large knowledge graphs and used to predict new facts, which are equivalent to new edges in the graph. The discussion focuses on two types of statistical relational models that can scale to massive datasets: latent feature models, including tensor factorization and multiway neural networks, and models based on observable patterns in the graph. The article also explores how a combination of these models can improve modeling power while reducing computational cost. Additionally, the article explains how such statistical models can be combined with text-based information extraction methods to automatically construct knowledge graphs from the Web, using Google's Knowledge Vault project as an example.",1
"Tree-structured data usually contain both topological and geometrical information, and are necessarily considered on manifold instead of Euclidean space for appropriate data parameterization and analysis. In this study, we propose a novel tree-structured data parameterization, called Topology-Attribute matrix (T-A matrix), so the data clustering task can be conducted on matrix manifold. We incorporate the structure constraints embedded in data into the negative matrix factorization method to determine meta-trees from the T-A matrix, and the signature vector of each single tree can then be extracted by meta-tree decomposition. The meta-tree space turns out to be a cone space, in which we explore the distance metric and implement the clustering algorithm based on the concepts like Fr\'echet mean. Finally, the T-A matrix based clustering (TAMBAC) framework is evaluated and compared using both simulated data and real retinal images to illustrate its efficiency and accuracy.",0
"Data in a tree structure typically contains both topological and geometrical information, which requires analysis on a manifold instead of Euclidean space to achieve proper data parameterization. This study introduces a new tree-structured data parameterization, referred to as Topology-Attribute matrix (T-A matrix), which permits conducting data clustering on a matrix manifold. We integrate the structure constraints inherent in the data into the negative matrix factorization method to determine meta-trees from the T-A matrix, and subsequently extract the signature vector of each individual tree by means of meta-tree decomposition. The meta-tree space is a cone space in which we explore the distance metric and implement the clustering algorithm based on concepts such as Fr\'echet mean. Lastly, we evaluate and compare the efficiency and accuracy of the T-A matrix based clustering (TAMBAC) framework using both simulated data and real retinal images.",1
"The modelling of data on a spherical surface requires the consideration of directional probability distributions. To model asymmetrically distributed data on a three-dimensional sphere, Kent distributions are often used. The moment estimates of the parameters are typically used in modelling tasks involving Kent distributions. However, these lack a rigorous statistical treatment. The focus of the paper is to introduce a Bayesian estimation of the parameters of the Kent distribution which has not been carried out in the literature, partly because of its complex mathematical form. We employ the Bayesian information-theoretic paradigm of Minimum Message Length (MML) to bridge this gap and derive reliable estimators. The inferred parameters are subsequently used in mixture modelling of Kent distributions. The problem of inferring the suitable number of mixture components is also addressed using the MML criterion. We demonstrate the superior performance of the derived MML-based parameter estimates against the traditional estimators. We apply the MML principle to infer mixtures of Kent distributions to model empirical data corresponding to protein conformations. We demonstrate the effectiveness of Kent models to act as improved descriptors of protein structural data as compared to commonly used von Mises-Fisher distributions.",0
"When modelling data on a spherical surface, it is necessary to consider directional probability distributions. Kent distributions are often used to model asymmetrically distributed data on a three-dimensional sphere. However, the moment estimates of the parameters used in modelling lack a rigorous statistical treatment. This paper aims to introduce a Bayesian estimation of the parameters of the Kent distribution, which has not been done before due to its complex mathematical form. The Minimum Message Length (MML) paradigm is employed to bridge this gap and derive reliable estimators. The inferred parameters are then used in mixture modelling of Kent distributions. Additionally, the problem of inferring the appropriate number of mixture components is addressed using the MML criterion. The MML-based parameter estimates outperform traditional estimators. Empirical data of protein conformations is modelled using mixtures of Kent distributions based on the MML principle. The effectiveness of Kent models as descriptors of protein structural data is demonstrated, outperforming commonly used von Mises-Fisher distributions.",1
"Deep Learning's recent successes have mostly relied on Convolutional Networks, which exploit fundamental statistical properties of images, sounds and video data: the local stationarity and multi-scale compositional structure, that allows expressing long range interactions in terms of shorter, localized interactions. However, there exist other important examples, such as text documents or bioinformatic data, that may lack some or all of these strong statistical regularities.   In this paper we consider the general question of how to construct deep architectures with small learning complexity on general non-Euclidean domains, which are typically unknown and need to be estimated from the data. In particular, we develop an extension of Spectral Networks which incorporates a Graph Estimation procedure, that we test on large-scale classification problems, matching or improving over Dropout Networks with far less parameters to estimate.",0
"Convolutional Networks have been the driving force behind the recent successes of Deep Learning, as they take advantage of the statistical properties of images, sounds, and video data. These properties include local stationarity and multi-scale compositional structure, which enable long range interactions to be expressed in terms of shorter, localized interactions. However, there are other types of data, such as text documents or bioinformatic data, that may not possess these strong statistical regularities. This paper addresses the challenge of constructing deep architectures with small learning complexity for non-Euclidean domains, which are typically unknown and must be estimated from the data. The authors propose an extension of Spectral Networks that includes a Graph Estimation procedure, which they test on large-scale classification problems. The results show that this approach matches or even outperforms Dropout Networks, while requiring far fewer parameters to estimate.",1
"Many real-world applications are associated with structured data, where not only input but also output has interplay. However, typical classification and regression models often lack the ability of simultaneously exploring high-order interaction within input and that within output. In this paper, we present a deep learning model aiming to generate a powerful nonlinear functional mapping from structured input to structured output. More specifically, we propose to integrate high-order hidden units, guided discriminative pretraining, and high-order auto-encoders for this purpose. We evaluate the model with three datasets, and obtain state-of-the-art performances among competitive methods. Our current work focuses on structured output regression, which is a less explored area, although the model can be extended to handle structured label classification.",0
"Structured data is commonly used in real-world applications, where input and output are interconnected. However, classification and regression models often lack the ability to explore high-order interactions within both input and output simultaneously. In this article, we introduce a deep learning model that generates a powerful nonlinear functional mapping from structured input to structured output. Our approach integrates high-order hidden units, guided discriminative pretraining, and high-order auto-encoders. We tested the model on three datasets and achieved state-of-the-art performance compared to other methods. Our research focuses on structured output regression, which is an area that has received less exploration, although our model can also handle structured label classification.",1
"In many applications data is naturally presented in terms of orderings of some basic elements or symbols. Reasoning about such data requires a notion of similarity capable of handling sequences of different lengths. In this paper we describe a family of Mercer kernel functions for such sequentially structured data. The family is characterized by a decomposable structure in terms of symbol-level and structure-level similarities, representing a specific combination of kernels which allows for efficient computation. We provide an experimental evaluation on sequential classification tasks comparing kernels from our family of kernels to a state of the art sequence kernel called the Global Alignment kernel which has been shown to outperform Dynamic Time Warping",0
"Data in many applications is often presented as orderings of basic elements or symbols, requiring a similarity concept capable of handling various sequence lengths for reasoning about such data. This paper presents a family of Mercer kernel functions suitable for sequentially structured data. The family is characterized by a decomposable structure based on symbol-level and structure-level similarities, which enables efficient computation. We conducted an experimental evaluation on sequential classification tasks, comparing our family of kernels to the state-of-the-art Global Alignment kernel, which has been proven to outperform Dynamic Time Warping.",1
"We evaluate a version of the recently-proposed classification system named Optimized Dissimilarity Space Embedding (ODSE) that operates in the input space of sequences of generic objects. The ODSE system has been originally presented as a classification system for patterns represented as labeled graphs. However, since ODSE is founded on the dissimilarity space representation of the input data, the classifier can be easily adapted to any input domain where it is possible to define a meaningful dissimilarity measure. Here we demonstrate the effectiveness of the ODSE classifier for sequences by considering an application dealing with the recognition of the solubility degree of the Escherichia coli proteome. Solubility, or analogously aggregation propensity, is an important property of protein molecules, which is intimately related to the mechanisms underlying the chemico-physical process of folding. Each protein of our dataset is initially associated with a solubility degree and it is represented as a sequence of symbols, denoting the 20 amino acid residues. The herein obtained computational results, which we stress that have been achieved with no context-dependent tuning of the ODSE system, confirm the validity and generality of the ODSE-based approach for structured data classification.",0
"We examine the Optimized Dissimilarity Space Embedding (ODSE) classification system, which was recently proposed and operates in the input space of sequences of generic objects. While ODSE was originally presented as a classification system for labeled graphs, it can be easily adapted to any input domain that has a meaningful dissimilarity measure. In this study, we demonstrate the effectiveness of the ODSE classifier for sequences by using it to recognize the solubility degree of the Escherichia coli proteome. Solubility is a critical property of protein molecules that is closely linked to the folding process. Each protein in our dataset is initially associated with a solubility degree and represented as a sequence of symbols denoting the 20 amino acid residues. Our computational results, achieved without context-dependent tuning of the ODSE system, validate the generality and usefulness of the ODSE-based approach for structured data classification.",1
"Deep learning is a broad set of techniques that uses multiple layers of representation to automatically learn relevant features directly from structured data. Recently, such techniques have yielded record-breaking results on a diverse set of difficult machine learning tasks in computer vision, speech recognition, and natural language processing. Despite the enormous success of deep learning, relatively little is understood theoretically about why these techniques are so successful at feature learning and compression. Here, we show that deep learning is intimately related to one of the most important and successful techniques in theoretical physics, the renormalization group (RG). RG is an iterative coarse-graining scheme that allows for the extraction of relevant features (i.e. operators) as a physical system is examined at different length scales. We construct an exact mapping from the variational renormalization group, first introduced by Kadanoff, and deep learning architectures based on Restricted Boltzmann Machines (RBMs). We illustrate these ideas using the nearest-neighbor Ising Model in one and two-dimensions. Our results suggests that deep learning algorithms may be employing a generalized RG-like scheme to learn relevant features from data.",0
"The use of multiple layers of representation to automatically learn relevant features from structured data is known as deep learning. This technique has shown unprecedented success in a range of difficult machine learning tasks such as computer vision, speech recognition, and natural language processing. However, despite its success, there is little theoretical understanding of why deep learning is so effective at feature learning and compression. The authors of this paper propose a connection between deep learning and the renormalization group (RG), a popular technique in theoretical physics. RG is an iterative coarse-graining scheme that allows for the extraction of relevant features as a physical system is examined at different length scales. The authors construct a mapping between the variational renormalization group and deep learning architectures based on Restricted Boltzmann Machines (RBMs). They demonstrate the validity of their idea using the nearest-neighbor Ising Model in one and two-dimensions, suggesting that deep learning algorithms could be using a generalized RG-like scheme to learn relevant features from data.",1
"We introduce propagation kernels, a general graph-kernel framework for efficiently measuring the similarity of structured data. Propagation kernels are based on monitoring how information spreads through a set of given graphs. They leverage early-stage distributions from propagation schemes such as random walks to capture structural information encoded in node labels, attributes, and edge information. This has two benefits. First, off-the-shelf propagation schemes can be used to naturally construct kernels for many graph types, including labeled, partially labeled, unlabeled, directed, and attributed graphs. Second, by leveraging existing efficient and informative propagation schemes, propagation kernels can be considerably faster than state-of-the-art approaches without sacrificing predictive performance. We will also show that if the graphs at hand have a regular structure, for instance when modeling image or video data, one can exploit this regularity to scale the kernel computation to large databases of graphs with thousands of nodes. We support our contributions by exhaustive experiments on a number of real-world graphs from a variety of application domains.",0
"Propagation kernels are a versatile graph-kernel framework that efficiently measures the similarity of structured data. These kernels monitor the spread of information across a set of graphs and utilize early-stage distributions from propagation methods like random walks to capture structural information in node labels, attributes, and edge information. This approach has two advantages. Firstly, it allows for the construction of kernels for different types of graphs, including labeled, partially labeled, unlabeled, directed, and attributed graphs, using readily available propagation schemes. Secondly, by utilizing existing efficient and informative propagation schemes, propagation kernels can deliver faster processing times than current methods without sacrificing predictive performance. Additionally, we demonstrate that the regularity of graphs, such as those used in image and video data modeling, can be exploited to scale kernel computation to large databases of graphs with thousands of nodes. Through extensive experiments on real-world graphs from multiple application domains, we validate our contributions.",1
"Unsupervised ranking faces one critical challenge in evaluation applications, that is, no ground truth is available. When PageRank and its variants show a good solution in related subjects, they are applicable only for ranking from link-structure data. In this work, we focus on unsupervised ranking from multi-attribute data which is also common in evaluation tasks. To overcome the challenge, we propose five essential meta-rules for the design and assessment of unsupervised ranking approaches: scale and translation invariance, strict monotonicity, linear/nonlinear capacities, smoothness, and explicitness of parameter size. These meta-rules are regarded as high level knowledge for unsupervised ranking tasks. Inspired by the works in [8] and [14], we propose a ranking principal curve (RPC) model, which learns a one-dimensional manifold function to perform unsupervised ranking tasks on multi-attribute observations. Furthermore, the RPC is modeled to be a cubic B\'ezier curve with control points restricted in the interior of a hypercube, thereby complying with all the five meta-rules to infer a reasonable ranking list. With control points as the model parameters, one is able to understand the learned manifold and to interpret the ranking list semantically. Numerical experiments of the presented RPC model are conducted on two open datasets of different ranking applications. In comparison with the state-of-the-art approaches, the new model is able to show more reasonable ranking lists.",0
"Unsupervised ranking is a challenging task in evaluation applications due to the absence of ground truth. While PageRank and its variants are effective for ranking from link-structure data, they are not suitable for multi-attribute data commonly encountered in evaluation tasks. To address this challenge, we propose five essential meta-rules for designing and evaluating unsupervised ranking approaches, including scale and translation invariance, strict monotonicity, linear/nonlinear capacities, smoothness, and explicitness of parameter size. These meta-rules serve as high-level knowledge for unsupervised ranking tasks. Inspired by previous works, we introduce a Ranking Principal Curve (RPC) model, which learns a one-dimensional manifold function to perform unsupervised ranking on multi-attribute observations. The RPC model is a cubic B\'ezier curve with control points restricted in a hypercube, adhering to all five meta-rules to produce a reasonable ranking list. By interpreting the ranking list semantically, one can understand the learned manifold and the model parameters. We conduct numerical experiments on two open datasets, demonstrating that the RPC model outperforms state-of-the-art approaches and generates more reasonable ranking lists.",1
"The need for appropriate ways to measure the distance or similarity between data is ubiquitous in machine learning, pattern recognition and data mining, but handcrafting such good metrics for specific problems is generally difficult. This has led to the emergence of metric learning, which aims at automatically learning a metric from data and has attracted a lot of interest in machine learning and related fields for the past ten years. This survey paper proposes a systematic review of the metric learning literature, highlighting the pros and cons of each approach. We pay particular attention to Mahalanobis distance metric learning, a well-studied and successful framework, but additionally present a wide range of methods that have recently emerged as powerful alternatives, including nonlinear metric learning, similarity learning and local metric learning. Recent trends and extensions, such as semi-supervised metric learning, metric learning for histogram data and the derivation of generalization guarantees, are also covered. Finally, this survey addresses metric learning for structured data, in particular edit distance learning, and attempts to give an overview of the remaining challenges in metric learning for the years to come.",0
"In machine learning, pattern recognition, and data mining, there is a constant need for effective ways to measure the similarity or distance between data. However, creating suitable metrics for specific problems can be challenging. As a solution, metric learning has emerged, which involves automatically learning metrics from data and has gained significant attention in the past decade. This paper presents a comprehensive review of the metric learning literature, focusing on Mahalanobis distance metric learning, as well as various emerging methods, such as nonlinear metric learning and similarity learning. The survey also covers recent trends and extensions, including semi-supervised metric learning and metric learning for histogram data, and outlines the remaining challenges in metric learning, particularly in structured data and edit distance learning.",1
"Dealing with structured data needs the use of expressive representation formalisms that, however, puts the problem to deal with the computational complexity of the machine learning process. Furthermore, real world domains require tools able to manage their typical uncertainty. Many statistical relational learning approaches try to deal with these problems by combining the construction of relevant relational features with a probabilistic tool. When the combination is static (static propositionalization), the constructed features are considered as boolean features and used offline as input to a statistical learner; while, when the combination is dynamic (dynamic propositionalization), the feature construction and probabilistic tool are combined into a single process. In this paper we propose a selective propositionalization method that search the optimal set of relational features to be used by a probabilistic learner in order to minimize a loss function. The new propositionalization approach has been combined with the random subspace ensemble method. Experiments on real-world datasets shows the validity of the proposed method.",0
"To handle structured data, one requires expressive representation formalisms, which pose a challenge in dealing with the computational complexity of machine learning. Additionally, actual domains necessitate tools that can handle their characteristic uncertainty. Several statistical relational learning methods attempt to tackle these issues by integrating the creation of pertinent relational features with a probabilistic tool. When the combination is static (static propositionalization), the constructed features are treated as boolean features and utilized offline as input to a statistical learner. On the other hand, when the combination is dynamic (dynamic propositionalization), the feature construction and probabilistic tool are fused into a single process. Our research proposes a selective propositionalization method that searches for the optimal set of relational features to be used by a probabilistic learner in minimizing a loss function. The novel propositionalization approach has been merged with the random subspace ensemble method. Testing on real-world datasets demonstrates the effectiveness of the suggested method.",1
"In structured output learning, obtaining labelled data for real-world applications is usually costly, while unlabelled examples are available in abundance. Semi-supervised structured classification has been developed to handle large amounts of unlabelled structured data. In this work, we consider semi-supervised structural SVMs with domain constraints. The optimization problem, which in general is not convex, contains the loss terms associated with the labelled and unlabelled examples along with the domain constraints. We propose a simple optimization approach, which alternates between solving a supervised learning problem and a constraint matching problem. Solving the constraint matching problem is difficult for structured prediction, and we propose an efficient and effective hill-climbing method to solve it. The alternating optimization is carried out within a deterministic annealing framework, which helps in effective constraint matching, and avoiding local minima which are not very useful. The algorithm is simple to implement and achieves comparable generalization performance on benchmark datasets.",0
"Structured output learning often involves high costs for obtaining labelled data in real-world applications, while unlabelled examples are abundant. To tackle this challenge, semi-supervised structured classification has been developed to handle large volumes of unlabelled structured data. This paper focuses on semi-supervised structural SVMs with domain constraints, where the optimization problem is non-convex and includes loss terms from both labelled and unlabelled examples, as well as domain constraints. We propose an optimization approach that alternates between a supervised learning problem and a constraint matching problem. The latter is difficult for structured prediction, but we introduce an efficient hill-climbing method. The alternating optimization is carried out within a deterministic annealing framework to improve constraint matching and avoid less useful local minima. Our algorithm is easy to implement and achieves comparable generalization performance on benchmark datasets.",1
"In this paper, we present two localized graph filtering based methods for interpolating graph signals defined on the vertices of arbitrary graphs from only a partial set of samples. The first method is an extension of previous work on reconstructing bandlimited graph signals from partially observed samples. The iterative graph filtering approach very closely approximates the solution proposed in the that work, while being computationally more efficient. As an alternative, we propose a regularization based framework in which we define the cost of reconstruction to be a combination of smoothness of the graph signal and the reconstruction error with respect to the known samples, and find solutions that minimize this cost. We provide both a closed form solution and a computationally efficient iterative solution of the optimization problem. The experimental results on the recommendation system datasets demonstrate effectiveness of the proposed methods.",0
"This paper introduces two methods for interpolating graph signals on vertices of arbitrary graphs using only a partial set of samples. One method is an extension of a previous approach for reconstructing bandlimited graph signals from partially observed samples, but with improved computational efficiency. Alternatively, we propose a regularization-based framework that combines the smoothness of the graph signal with the reconstruction error to minimize the cost of reconstruction. We provide both a closed form solution and an iterative solution for the optimization problem. The effectiveness of the proposed methods is demonstrated through experimental results on recommendation system datasets.",1
"Tensor factorizations have become increasingly popular approaches for various learning tasks on structured data. In this work, we extend the RESCAL tensor factorization, which has shown state-of-the-art results for multi-relational learning, to account for the binary nature of adjacency tensors. We study the improvements that can be gained via this approach on various benchmark datasets and show that the logistic extension can improve the prediction results significantly.",0
"Structured data learning tasks have seen a rise in the use of tensor factorizations. The RESCAL tensor factorization has proved to be the best method for multi-relational learning. In this study, we have enhanced the RESCAL tensor factorization to consider the binary nature of adjacency tensors. Our investigation focused on the benefits of this modification on multiple benchmark datasets. The results indicated that the logistic extension significantly enhanced the prediction outcomes.",1
"Large-scale relational learning becomes crucial for handling the huge amounts of structured data generated daily in many application domains ranging from computational biology or information retrieval, to natural language processing. In this paper, we present a new neural network architecture designed to embed multi-relational graphs into a flexible continuous vector space in which the original data is kept and enhanced. The network is trained to encode the semantics of these graphs in order to assign high probabilities to plausible components. We empirically show that it reaches competitive performance in link prediction on standard datasets from the literature.",0
"The need for large-scale relational learning has become increasingly important due to the immense amounts of structured data being produced daily in various application domains such as computational biology, natural language processing, and information retrieval. Our paper introduces a novel neural network architecture that can embed multi-relational graphs into a versatile continuous vector space while preserving and enhancing the original data. Through training, the network learns to encode the meaning of these graphs and assigns high probabilities to potential components. Our empirical findings demonstrate that this approach achieves notable results in link prediction on conventional datasets from published literature.",1
"This paper investigates graph clustering in the planted cluster model in the presence of {\em small clusters}. Traditional results dictate that for an algorithm to provably correctly recover the clusters, {\em all} clusters must be sufficiently large (in particular, $\tilde{\Omega}(\sqrt{n})$ where $n$ is the number of nodes of the graph). We show that this is not really a restriction: by a more refined analysis of the trace-norm based recovery approach proposed in Jalali et al. (2011) and Chen et al. (2012), we prove that small clusters, under certain mild assumptions, do not hinder recovery of large ones.   Based on this result, we further devise an iterative algorithm to recover {\em almost all clusters} via a ""peeling strategy"", i.e., recover large clusters first, leading to a reduced problem, and repeat this procedure. These results are extended to the {\em partial observation} setting, in which only a (chosen) part of the graph is observed.The peeling strategy gives rise to an active learning algorithm, in which edges adjacent to smaller clusters are queried more often as large clusters are learned (and removed).   From a high level, this paper sheds novel insights on high-dimensional statistics and learning structured data, by presenting a structured matrix learning problem for which a one shot convex relaxation approach necessarily fails, but a carefully constructed sequence of convex relaxationsdoes the job.",0
"The main focus of this study is to analyze graph clustering in the planted cluster model with the presence of small clusters. Traditional findings suggest that for an algorithm to accurately recover clusters, all clusters must be of sufficient size, specifically at least $\tilde{\Omega}(\sqrt{n})$ where $n$ is the number of nodes in the graph. However, this study proves otherwise by conducting a more detailed analysis of the trace-norm based recovery approach proposed in Jalali et al. (2011) and Chen et al. (2012). It establishes that small clusters do not impede the recovery of large ones, given certain mild assumptions. The study also introduces an iterative algorithm that uses a ""peeling strategy"" to recover almost all clusters by first detecting and removing large clusters, then repeating the process. These findings are further extended to the partial observation setting, where only a part of the graph is observed. The peeling strategy leads to an active learning algorithm where edges adjacent to smaller clusters are queried more often as large clusters are learned and removed. Overall, this paper offers new insights into high-dimensional statistics and structured data learning by presenting a structured matrix learning problem that requires a carefully constructed sequence of convex relaxations instead of a one-shot convex relaxation approach.",1
"Traditional relation extraction predicts relations within some fixed and finite target schema. Machine learning approaches to this task require either manual annotation or, in the case of distant supervision, existing structured sources of the same schema. The need for existing datasets can be avoided by using a universal schema: the union of all involved schemas (surface form predicates as in OpenIE, and relations in the schemas of pre-existing databases). This schema has an almost unlimited set of relations (due to surface forms), and supports integration with existing structured data (through the relation types of existing databases). To populate a database of such schema we present a family of matrix factorization models that predict affinity between database tuples and relations. We show that this achieves substantially higher accuracy than the traditional classification approach. More importantly, by operating simultaneously on relations observed in text and in pre-existing structured DBs such as Freebase, we are able to reason about unstructured and structured data in mutually-supporting ways. By doing so our approach outperforms state-of-the-art distant supervision systems.",0
"The conventional method of relation extraction is limited to a predetermined schema and requires either manual annotation or existing structured sources for machine learning. However, a universal schema that combines all relevant schemas, including surface form predicates and pre-existing database relations, can overcome the need for existing datasets and expand the range of potential relations. To build a database using this schema, we propose a set of matrix factorization models that predict the affinity between database tuples and relations, resulting in higher accuracy than traditional classification methods. Our approach integrates unstructured and structured data from both text and pre-existing databases, enabling mutual support and outperforming current distant supervision systems.",1
"In practical machine learning systems, graph based data representation has been widely used in various learning paradigms, ranging from unsupervised clustering to supervised classification. Besides those applications with natural graph or network structure data, such as social network analysis and relational learning, many other applications often involve a critical step in converting data vectors to an adjacency graph. In particular, a sparse subgraph extracted from the original graph is often required due to both theoretic and practical needs. Previous study clearly shows that the performance of different learning algorithms, e.g., clustering and classification, benefits from such sparse subgraphs with balanced node connectivity. However, the existing graph construction methods are either computationally expensive or with unsatisfactory performance. In this paper, we utilize a scalable method called auction algorithm and its parallel extension to recover a sparse yet nearly balanced subgraph with significantly reduced computational cost. Empirical study and comparison with the state-ofart approaches clearly demonstrate the superiority of the proposed method in both efficiency and accuracy.",0
"Graph based data representation is commonly used in various machine learning systems, from unsupervised clustering to supervised classification. While certain applications have natural graph or network structure data, such as social network analysis and relational learning, many others require converting data vectors to an adjacency graph. Typically, a sparse subgraph extracted from the original graph is necessary for theoretical and practical reasons, as previous studies have shown that learning algorithms benefit from such subgraphs with balanced node connectivity. However, existing graph construction methods are either computationally expensive or lack satisfactory performance. To address this, we present a scalable method utilizing the auction algorithm and its parallel extension to recover a nearly balanced, sparse subgraph with significantly reduced computational cost. Empirical studies and comparisons with state-of-the-art approaches demonstrate the proposed method's superiority in both efficiency and accuracy.",1
"Collective classification models attempt to improve classification performance by taking into account the class labels of related instances. However, they tend not to learn patterns of interactions between classes and/or make the assumption that instances of the same class link to each other (assortativity assumption). Blockmodels provide a solution to these issues, being capable of modelling assortative and disassortative interactions, and learning the pattern of interactions in the form of a summary network. The Supervised Blockmodel provides good classification performance using link structure alone, whilst simultaneously providing an interpretable summary of network interactions to allow a better understanding of the data. This work explores three variants of supervised blockmodels of varying complexity and tests them on four structurally different real world networks.",0
"The aim of collective classification models is to enhance classification accuracy by considering the class labels of associated instances. However, these models do not typically identify interaction patterns between classes, and often make the assumption that instances belonging to the same class are linked (assortativity assumption). Blockmodels offer a solution to these issues, as they can model assortative and disassortative interactions and generate a summary network that highlights interaction patterns. The Supervised Blockmodel achieves effective classification performance using only link structure, while providing an understandable summary of network interactions for improved data comprehension. This study investigates three variations of supervised blockmodels with different levels of complexity and evaluates their performance on four diverse real-world networks.",1
"High dimensional structured data such as text and images is often poorly understood and misrepresented in statistical modeling. The standard histogram representation suffers from high variance and performs poorly in general. We explore novel connections between statistical translation, heat kernels on manifolds and graphs, and expected distances. These connections provide a new framework for unsupervised metric learning for text documents. Experiments indicate that the resulting distances are generally superior to their more standard counterparts.",0
"Statistical modeling often struggles to comprehend and accurately represent high dimensional structured data, such as text and images. The conventional histogram representation is unreliable due to its high variance and subpar performance. Our research delves into innovative links between statistical translation, heat kernels on manifolds and graphs, and expected distances. These links establish a fresh approach to unsupervised metric learning for text documents. Empirical tests demonstrate that the resulting distances are typically superior to their traditional equivalents.",1
"High-dimensional data common in genomics, proteomics, and chemometrics often contains complicated correlation structures. Recently, partial least squares (PLS) and Sparse PLS methods have gained attention in these areas as dimension reduction techniques in the context of supervised data analysis. We introduce a framework for Regularized PLS by solving a relaxation of the SIMPLS optimization problem with penalties on the PLS loadings vectors. Our approach enjoys many advantages including flexibility, general penalties, easy interpretation of results, and fast computation in high-dimensional settings. We also outline extensions of our methods leading to novel methods for Non-negative PLS and Generalized PLS, an adaption of PLS for structured data. We demonstrate the utility of our methods through simulations and a case study on proton Nuclear Magnetic Resonance (NMR) spectroscopy data.",0
"Complicated correlation structures are often found in high-dimensional data from genomics, proteomics, and chemometrics. To address this, dimension reduction techniques such as partial least squares (PLS) and Sparse PLS have gained popularity in supervised data analysis. In this study, we propose a Regularized PLS framework by penalizing the PLS loadings vectors and solving a relaxation of the SIMPLS optimization problem. Our approach has many benefits, including flexibility, general penalties, easy interpretation of results, and fast computation in high-dimensional settings. We also present extensions of our methods that lead to new techniques for Non-negative PLS and Generalized PLS, which adapt PLS for structured data. To demonstrate the effectiveness of our methods, we conduct simulations and a case study on proton Nuclear Magnetic Resonance (NMR) spectroscopy data.",1
"High-dimensional tensors or multi-way data are becoming prevalent in areas such as biomedical imaging, chemometrics, networking and bibliometrics. Traditional approaches to finding lower dimensional representations of tensor data include flattening the data and applying matrix factorizations such as principal components analysis (PCA) or employing tensor decompositions such as the CANDECOMP / PARAFAC (CP) and Tucker decompositions. The former can lose important structure in the data, while the latter Higher-Order PCA (HOPCA) methods can be problematic in high-dimensions with many irrelevant features. We introduce frameworks for sparse tensor factorizations or Sparse HOPCA based on heuristic algorithmic approaches and by solving penalized optimization problems related to the CP decomposition. Extensions of these approaches lead to methods for general regularized tensor factorizations, multi-way Functional HOPCA and generalizations of HOPCA for structured data. We illustrate the utility of our methods for dimension reduction, feature selection, and signal recovery on simulated data and multi-dimensional microarrays and functional MRIs.",0
"Multi-way data or high-dimensional tensors are increasingly common in various fields, including biomedical imaging, chemometrics, networking, and bibliometrics. Traditional methods for finding lower dimensional representations of tensor data involve flattening the data and utilizing matrix factorizations such as principal components analysis (PCA) or tensor decompositions such as the CANDECOMP/PARAFAC (CP) and Tucker decompositions. However, flattening the data can result in the loss of vital structure, and HOPCA methods can pose issues in high-dimensions with numerous irrelevant features. To address these challenges, we propose frameworks for sparse tensor factorizations and Sparse HOPCA using heuristic algorithms and solving penalized optimization problems related to the CP decomposition. Our approaches can be extended to general regularized tensor factorizations, multi-way Functional HOPCA, and structured data generalizations of HOPCA. We demonstrate the effectiveness of our techniques for dimension reduction, feature selection, and signal recovery using simulated data, multi-dimensional microarrays, and functional MRIs.",1
"Recent results in Compressive Sensing have shown that, under certain conditions, the solution to an underdetermined system of linear equations with sparsity-based regularization can be accurately recovered by solving convex relaxations of the original problem. In this work, we present a novel primal-dual analysis on a class of sparsity minimization problems. We show that the Lagrangian bidual (i.e., the Lagrangian dual of the Lagrangian dual) of the sparsity minimization problems can be used to derive interesting convex relaxations: the bidual of the $\ell_0$-minimization problem is the $\ell_1$-minimization problem; and the bidual of the $\ell_{0,1}$-minimization problem for enforcing group sparsity on structured data is the $\ell_{1,\infty}$-minimization problem. The analysis provides a means to compute per-instance non-trivial lower bounds on the (group) sparsity of the desired solutions. In a real-world application, the bidual relaxation improves the performance of a sparsity-based classification framework applied to robust face recognition.",0
"Recent findings in Compressive Sensing have revealed that, in specific circumstances, solving convex relaxations of an underdetermined system of linear equations with sparsity-based regularization can result in an accurate recovery of the solution. This paper introduces a new primal-dual analysis on a category of sparsity minimization problems. The study demonstrates that the Lagrangian bidual of the sparsity minimization problems can produce interesting convex relaxations. For instance, the bidual of the $\ell_0$-minimization problem is the $\ell_1$-minimization problem, while the bidual of the $\ell_{0,1}$-minimization problem for structured data group sparsity is the $\ell_{1,\infty}$-minimization problem. By using this analysis, it is possible to calculate non-trivial lower bounds on the (group) sparsity of the intended solutions. Furthermore, in a practical application, the bidual relaxation enhances the performance of a sparsity-based classification system used in robust facial recognition.",1
"We describe many vantage points on the Baire metric and its use in clustering data, or its use in preprocessing and structuring data in order to support search and retrieval operations. In some cases, we proceed directly to clusters and do not directly determine the distances. We show how a hierarchical clustering can be read directly from one pass through the data. We offer insights also on practical implications of precision of data measurement. As a mechanism for treating multidimensional data, including very high dimensional data, we use random projections.",0
"Numerous perspectives are provided on the Baire metric and its role in data clustering, preprocessing, and structuring for facilitating search and retrieval tasks. The direct computation of distances is sometimes omitted, and we demonstrate how hierarchical clustering can be obtained from a single pass through the data. Additionally, we shed light on the practical implications of data measurement accuracy. To deal with multidimensional data, particularly those with high dimensions, random projections are utilized.",1
"Recently there is a line of research work proposing to employ Spectral Clustering (SC) to segment (group){Throughout the paper, we use segmentation, clustering, and grouping, and their verb forms, interchangeably.} high-dimensional structural data such as those (approximately) lying on subspaces {We follow {liu2010robust} and use the term ""subspace"" to denote both linear subspaces and affine subspaces. There is a trivial conversion between linear subspaces and affine subspaces as mentioned therein.} or low-dimensional manifolds. By learning the affinity matrix in the form of sparse reconstruction, techniques proposed in this vein often considerably boost the performance in subspace settings where traditional SC can fail. Despite the success, there are fundamental problems that have been left unsolved: the spectrum property of the learned affinity matrix cannot be gauged in advance, and there is often one ugly symmetrization step that post-processes the affinity for SC input. Hence we advocate to enforce the symmetric positive semidefinite constraint explicitly during learning (Low-Rank Representation with Positive SemiDefinite constraint, or LRR-PSD), and show that factually it can be solved in an exquisite scheme efficiently instead of general-purpose SDP solvers that usually scale up poorly. We provide rigorous mathematical derivations to show that, in its canonical form, LRR-PSD is equivalent to the recently proposed Low-Rank Representation (LRR) scheme {liu2010robust}, and hence offer theoretic and practical insights to both LRR-PSD and LRR, inviting future research. As per the computational cost, our proposal is at most comparable to that of LRR, if not less. We validate our theoretic analysis and optimization scheme by experiments on both synthetic and real data sets.",0
"A recent research study has suggested using Spectral Clustering (SC) to segment high-dimensional structural data, such as those found on subspaces or low-dimensional manifolds. By utilizing sparse reconstruction to learn the affinity matrix, this approach has proven successful in subspace settings where traditional SC fails. However, there are still unresolved issues, such as the inability to gauge the spectrum property of the learned affinity matrix beforehand and the need for an unsightly symmetrization step. To address these problems, we propose enforcing a symmetric positive semidefinite constraint during learning, which we refer to as Low-Rank Representation with Positive SemiDefinite constraint (LRR-PSD). Our rigorous mathematical derivations demonstrate that LRR-PSD is equivalent to Low-Rank Representation (LRR), which offers both theoretical and practical insights for future research. Additionally, our proposed optimization scheme is computationally efficient and comparable to that of LRR. We verify our findings through experiments on synthetic and real datasets.",1
"The major challenge in designing a discriminative learning algorithm for predicting structured data is to address the computational issues arising from the exponential size of the output space. Existing algorithms make different assumptions to ensure efficient, polynomial time estimation of model parameters. For several combinatorial structures, including cycles, partially ordered sets, permutations and other graph classes, these assumptions do not hold. In this thesis, we address the problem of designing learning algorithms for predicting combinatorial structures by introducing two new assumptions: (i) The first assumption is that a particular counting problem can be solved efficiently. The consequence is a generalisation of the classical ridge regression for structured prediction. (ii) The second assumption is that a particular sampling problem can be solved efficiently. The consequence is a new technique for designing and analysing probabilistic structured prediction models. These results can be applied to solve several complex learning problems including but not limited to multi-label classification, multi-category hierarchical classification, and label ranking.",0
"When creating a discriminative learning algorithm for predicting structured data, the main challenge is dealing with computational issues caused by the vast size of the output space. Current algorithms rely on various assumptions to ensure that model parameters can be estimated efficiently within a polynomial time frame. However, these assumptions do not apply to certain combinatorial structures, such as cycles, partially ordered sets, permutations, and other graph classes. In this thesis, we tackle the issue of creating learning algorithms for predicting combinatorial structures by introducing two new assumptions. The first assumption is that a specific counting problem can be solved swiftly, which leads to a generalization of classical ridge regression for structured prediction. The second assumption is that a particular sampling problem can be solved efficiently, which results in a novel technique for designing and analyzing probabilistic structured prediction models. These findings can be used to solve complex learning problems, including but not limited to multi-label classification, multi-category hierarchical classification, and label ranking.",1
"Deep learning on graphs has recently achieved remarkable success on a variety of tasks, while such success relies heavily on the massive and carefully labeled data. However, precise annotations are generally very expensive and time-consuming. To address this problem, self-supervised learning (SSL) is emerging as a new paradigm for extracting informative knowledge through well-designed pretext tasks without relying on manual labels. In this survey, we extend the concept of SSL, which first emerged in the fields of computer vision and natural language processing, to present a timely and comprehensive review of existing SSL techniques for graph data. Specifically, we divide existing graph SSL methods into three categories: contrastive, generative, and predictive. More importantly, unlike other surveys that only provide a high-level description of published research, we present an additional mathematical summary of existing works in a unified framework. Furthermore, to facilitate methodological development and empirical comparisons, we also summarize the commonly used datasets, evaluation metrics, downstream tasks, open-source implementations, and experimental study of various algorithms. Finally, we discuss the technical challenges and potential future directions for improving graph self-supervised learning. Latest advances in graph SSL are summarized in a GitHub repository https://github.com/LirongWu/awesome-graph-self-supervised-learning.",0
"The success of deep learning on graphs has been impressive, but it relies heavily on large amounts of accurately labeled data. However, labeling data is a costly and time-consuming process. To address this issue, self-supervised learning (SSL) has emerged as a new approach to extract informative knowledge from pretext tasks without the need for manual labels. In this review, we apply the concept of SSL, which was first used in computer vision and natural language processing, to graph data. We categorize existing graph SSL methods into three groups: contrastive, generative, and predictive, and provide a mathematical summary of their works in a unified framework. We also summarize commonly used datasets, evaluation metrics, downstream tasks, open-source implementations, and experimental studies of various algorithms to aid in methodological development and empirical comparisons. Finally, we discuss technical challenges and future directions to improve graph self-supervised learning. A GitHub repository, https://github.com/LirongWu/awesome-graph-self-supervised-learning, summarizes the latest advances in graph SSL.",1
"We focus on the problem of segmenting a certain object referred by a natural language sentence in video content, at the core of formulating a pinpoint vision-language relation. While existing attempts mainly construct such relation in an implicit way, i.e., grid-level multi-modal feature fusion, it has been proven problematic to distinguish semantically similar objects under this paradigm. In this work, we propose to interwind the visual and linguistic modalities in an explicit way via the contrastive learning objective, which directly aligns the referred object and the language description and separates the unreferred content apart across frames. Moreover, to remedy for the degradation problem, we present two complementary hard instance mining strategies, i.e., Language-relevant Channel Filter and Relative Hard Instance Construction. They encourage the network to exclude visual-distinguishable feature and to focus on easy-confused objects during the contrastive training. Extensive experiments on two benchmarks, i.e., A2D Sentences and J-HMDB Sentences, quantitatively demonstrate the state-of-the-arts performance of our method and qualitatively show the more accurate distinguishment between semantically similar objects over baselines.",0
"Our main objective is to address the issue of segmenting specific objects mentioned in natural language sentences within video content, which is crucial for establishing a precise vision-language relationship. While previous approaches have tackled this problem through an implicit approach, which involves multi-modal feature fusion at the grid level, it has proven to be challenging to differentiate between objects that are semantically similar. In this study, we propose an explicit approach that intertwines visual and linguistic modalities through the contrastive learning objective. This aligns the referred object and language description and separates irrelevant content across frames. To address the degradation issue, we introduce two hard instance mining strategies - Language-relevant Channel Filter and Relative Hard Instance Construction. These encourage the network to exclude visually distinguishable features and focus on easy-to-confuse objects during contrastive training. Our experimental results on two benchmarks, A2D Sentences and J-HMDB Sentences, demonstrate the superior performance of our method and its ability to distinguish between semantically similar objects accurately compared to baselines.",1
"Managing large numbers of incoming bug reports and finding the most critical issues in hardware development is time consuming, but crucial in order to reduce development costs. In this paper, we present an approach to predict the time to fix, the risk and the complexity of debugging and resolution of a bug report using different supervised machine learning algorithms, namely Random Forest, Naive Bayes, SVM, MLP and XGBoost. Further, we investigate the effect of the application of active learning and we evaluate the impact of different text representation techniques, namely TF-IDF, Word2Vec, Universal Sentence Encoder and XLNet on the model's performance. The evaluation shows that a combination of text embeddings generated through the Universal Sentence Encoder and MLP as classifier outperforms all other methods, and is well suited to predict the risk and complexity of bug tickets.",0
"Reducing development costs requires effectively managing numerous bug reports and identifying the most critical issues in hardware development, which is a time-consuming process. In this study, we introduce a method to forecast the time required to fix, as well as the level of risk and complexity involved in debugging and resolving a bug report. This is achieved by utilizing a range of supervised machine learning algorithms, including Naive Bayes, Random Forest, SVM, MLP, and XGBoost. Additionally, we explore the impact of active learning on this process and evaluate the performance of different text representation techniques, such as TF-IDF, Word2Vec, Universal Sentence Encoder, and XLNet. Our findings indicate that employing a combination of text embeddings generated through the Universal Sentence Encoder and MLP as the classifier yields superior results, making it suitable for predicting the risk and complexity of bug tickets.",1
"Robots are becoming everyday devices, increasing their interaction with humans. To make human-machine interaction more natural, cognitive features like Visual Voice Activity Detection (VVAD), which can detect whether a person is speaking or not, given visual input of a camera, need to be implemented. Neural networks are state of the art for tasks in Image Processing, Time Series Prediction, Natural Language Processing and other domains. Those Networks require large quantities of labeled data. Currently there are not many datasets for the task of VVAD. In this work we created a large scale dataset called the VVAD-LRS3 dataset, derived by automatic annotations from the LRS3 dataset. The VVAD-LRS3 dataset contains over 44K samples, over three times the next competitive dataset (WildVVAD). We evaluate different baselines on four kinds of features: facial and lip images, and facial and lip landmark features. With a Convolutional Neural Network Long Short Term Memory (CNN LSTM) on facial images an accuracy of 92% was reached on the test set. A study with humans showed that they reach an accuracy of 87.93% on the test set.",0
"The interaction between humans and robots is increasing as robots become more commonplace. In order to improve the naturalness of this interaction, cognitive features like Visual Voice Activity Detection (VVAD) should be implemented. VVAD can detect whether a person is speaking or not based on visual input from a camera. Neural networks are currently the most advanced technology for tasks like Image Processing, Time Series Prediction, and Natural Language Processing, but they require large amounts of labeled data. Currently, there are few datasets available for the task of VVAD. This study presents the VVAD-LRS3 dataset, which contains over 44K samples - more than three times the size of the next largest dataset (WildVVAD) - derived from automatic annotations of the LRS3 dataset. Baselines were evaluated using facial and lip images, as well as facial and lip landmark features, and a Convolutional Neural Network Long Short Term Memory (CNN LSTM) was able to achieve an accuracy of 92% on the test set. A human study found that participants achieved an accuracy of 87.93% on the same test set.",1
"Transformer architecture has emerged to be successful in a number of natural language processing tasks. However, its applications to medical vision remain largely unexplored. In this study, we present UTNet, a simple yet powerful hybrid Transformer architecture that integrates self-attention into a convolutional neural network for enhancing medical image segmentation. UTNet applies self-attention modules in both encoder and decoder for capturing long-range dependency at different scales with minimal overhead. To this end, we propose an efficient self-attention mechanism along with relative position encoding that reduces the complexity of self-attention operation significantly from $O(n^2)$ to approximate $O(n)$. A new self-attention decoder is also proposed to recover fine-grained details from the skipped connections in the encoder. Our approach addresses the dilemma that Transformer requires huge amounts of data to learn vision inductive bias. Our hybrid layer design allows the initialization of Transformer into convolutional networks without a need of pre-training. We have evaluated UTNet on the multi-label, multi-vendor cardiac magnetic resonance imaging cohort. UTNet demonstrates superior segmentation performance and robustness against the state-of-the-art approaches, holding the promise to generalize well on other medical image segmentations.",0
"Although Transformer architecture has proven to be successful in several natural language processing tasks, its potential applications in medical vision have yet to be extensively explored. In this study, we introduce UTNet, a powerful, yet straightforward hybrid Transformer architecture that combines self-attention and convolutional neural network to enhance medical image segmentation. UTNet employs self-attention modules in both encoder and decoder to capture long-range dependencies at different scales while minimizing overhead. To achieve this, we propose an efficient self-attention mechanism and relative position encoding that significantly reduces the complexity of self-attention operation from $O(n^2)$ to approximately $O(n)$. Additionally, we introduce a new self-attention decoder to recover fine-grained details from the skipped connections in the encoder. Our approach addresses the requirement of vast amounts of data for Transformer to learn vision inductive bias. The hybrid layer design makes it possible to initialize Transformer into convolutional networks without pre-training. We tested UTNet on a multi-label, multi-vendor cardiac magnetic resonance imaging cohort, which demonstrated superior segmentation performance and robustness compared to state-of-the-art approaches, suggesting its potential to generalize well on other medical image segmentations.",1
"Transformers are state-of-the-art deep learning models that are composed of stacked attention and point-wise, fully connected layers designed for handling sequential data. Transformers are not only ubiquitous throughout Natural Language Processing (NLP), but, recently, they have inspired a new wave of Computer Vision (CV) applications research. In this work, a Vision Transformer (ViT) is applied to predict the state variables of 2-dimensional Ising model simulations. Our experiments show that ViT outperform state-of-the-art Convolutional Neural Networks (CNN) when using a small number of microstate images from the Ising model corresponding to various boundary conditions and temperatures. This work opens the possibility of applying ViT to other simulations, and raises interesting research directions on how attention maps can learn about the underlying physics governing different phenomena.",0
"Transformers are advanced deep learning models that consist of attention and fully connected layers, specifically designed for handling sequential data. They are widely used in Natural Language Processing (NLP) and have recently sparked a new wave of research in Computer Vision (CV). This study applies a Vision Transformer (ViT) to predict the state variables of 2-dimensional Ising model simulations. Results show that ViT outperforms state-of-the-art Convolutional Neural Networks (CNN) when using a small number of microstate images from the Ising model under various boundary conditions and temperatures. This research opens up the possibility of using ViT in other simulations, and presents interesting research opportunities to explore how attention maps can learn about the underlying physics governing different phenomena.",1
"Transformer-based architectures have become the de-facto standard models for a wide range of Natural Language Processing tasks. However, their memory footprint and high latency are prohibitive for efficient deployment and inference on resource-limited devices. In this work, we explore quantization for transformers. We show that transformers have unique quantization challenges -- namely, high dynamic activation ranges that are difficult to represent with a low bit fixed-point format. We establish that these activations contain structured outliers in the residual connections that encourage specific attention patterns, such as attending to the special separator token. To combat these challenges, we present three solutions based on post-training quantization and quantization-aware training, each with a different set of compromises for accuracy, model size, and ease of use. In particular, we introduce a novel quantization scheme -- per-embedding-group quantization. We demonstrate the effectiveness of our methods on the GLUE benchmark using BERT, establishing state-of-the-art results for post-training quantization. Finally, we show that transformer weights and embeddings can be quantized to ultra-low bit-widths, leading to significant memory savings with a minimum accuracy loss. Our source code is available at~\url{https://github.com/qualcomm-ai-research/transformer-quantization}.",0
"Transformer-based architectures have become the widely accepted models for various Natural Language Processing tasks. However, their high latency and memory usage pose challenges for efficient deployment and inference on devices with limited resources. This study focuses on quantization for transformers and identifies the unique quantization challenges they present, such as high dynamic activation ranges that are difficult to represent with low bit fixed-point format. The study establishes that residual connections contain structured outliers that encourage specific attention patterns, which can be tackled by post-training quantization and quantization-aware training. The study presents three solutions, each with a different set of trade-offs for accuracy, model size, and ease of use, including a novel quantization scheme called per-embedding-group quantization. The study demonstrates the effectiveness of these methods on the GLUE benchmark using BERT, achieving state-of-the-art results for post-training quantization. Finally, the study suggests that transformer weights and embeddings can be quantized to ultra-low bit-widths, resulting in significant memory savings with minimal accuracy loss. The source code for this study is available at \url{https://github.com/qualcomm-ai-research/transformer-quantization}.",1
"Standard training via empirical risk minimization (ERM) can produce models that achieve high accuracy on average but low accuracy on certain groups, especially in the presence of spurious correlations between the input and label. Prior approaches that achieve high worst-group accuracy, like group distributionally robust optimization (group DRO) require expensive group annotations for each training point, whereas approaches that do not use such group annotations typically achieve unsatisfactory worst-group accuracy. In this paper, we propose a simple two-stage approach, JTT, that first trains a standard ERM model for several epochs, and then trains a second model that upweights the training examples that the first model misclassified. Intuitively, this upweights examples from groups on which standard ERM models perform poorly, leading to improved worst-group performance. Averaged over four image classification and natural language processing tasks with spurious correlations, JTT closes 75% of the gap in worst-group accuracy between standard ERM and group DRO, while only requiring group annotations on a small validation set in order to tune hyperparameters.",0
"Training models using standard empirical risk minimization (ERM) may result in high average accuracy but low accuracy for specific groups, especially when there are spurious correlations between the input and label. Group distributionally robust optimization (group DRO) has been used to achieve high worst-group accuracy, but it requires costly group annotations for each training point. Approaches without group annotations often result in unsatisfactory worst-group accuracy. In this study, we introduce a two-stage approach called JTT. First, a standard ERM model is trained for several epochs, followed by a second model that assigns more significance to training examples that were misclassified by the first model. This approach prioritizes examples from groups that standard ERM models perform poorly on, resulting in improved worst-group performance. JTT was evaluated on four image classification and natural language processing tasks with spurious correlations and reduced the gap in worst-group accuracy between standard ERM and group DRO by 75%. Only a small validation set was required for hyperparameter tuning, which was used to obtain group annotations.",1
"Internet of Things (IoT)-based indoor localization has gained significant popularity recently to satisfy the ever-increasing requirements of indoor Location-based Services (LBS). In this context, Inertial Measurement Unit (IMU)-based localization is of interest as it provides a scalable solution independent of any proprietary sensors/modules. Existing IMU-based methodologies, however, are mainly developed based on statistical heading and step length estimation techniques that suffer from cumulative error issues and have extensive computational time requirements limiting their application for real-time indoor positioning. To address the aforementioned issues, we propose the Online Dynamic Window (ODW)-assisted two-stage Long Short Term Memory (LSTM) localization framework. Three ODWs are proposed, where the first model uses a Natural Language Processing (NLP)-inspired Dynamic Window (DW) approach, which significantly reduces the required computational time. The second framework is developed based on a Signal Processing Dynamic Windowing (SP-DW) approach to further reduce the required processing time of the two-stage LSTM-based model. The third ODW, referred to as the SP-NLP, combines the first two windowing mechanisms to further improve the overall achieved accuracy. Compared to the traditional LSTM-based positioning approaches, which suffer from either high tensor computation requirements or low accuracy, the proposed ODW-assisted models can perform indoor localization in a near-real time fashion with high accuracy. Performances of the proposed ODW-assisted models are evaluated based on a real Pedestrian Dead Reckoning (PDR) dataset. The results illustrate potentials of the proposed ODW-assisted techniques in achieving high classification accuracy with significantly reduced computational time, making them applicable for near real-time implementations.",0
"The need for indoor Location-based Services (LBS) has led to a surge in popularity for Internet of Things (IoT)-based indoor localization. Inertial Measurement Unit (IMU) based localization is an interesting option as it is scalable and not reliant on proprietary sensors/modules. However, current IMU-based methodologies suffer from cumulative error issues and require extensive computational time, which limits their real-time indoor positioning applications. To address these issues, we propose the use of Online Dynamic Window (ODW)-assisted two-stage Long Short Term Memory (LSTM) localization framework. Our framework offers three ODWs: the first model uses a Natural Language Processing (NLP)-inspired Dynamic Window (DW) approach to reduce computational time, the second framework uses a Signal Processing Dynamic Windowing (SP-DW) approach to further reduce processing time, and the third ODW, the SP-NLP, combines the first two windowing mechanisms to improve overall accuracy. Compared to traditional LSTM-based positioning approaches, which suffer from either high computation requirements or low accuracy, our proposed ODW-assisted models can perform indoor localization in near real-time with high accuracy. We evaluated the performance of our proposed ODW-assisted models using a real Pedestrian Dead Reckoning (PDR) dataset and our results demonstrate the potential of our techniques in achieving high classification accuracy with significantly reduced computational time, making them suitable for near-real-time implementations.",1
"Self-attention networks have revolutionized natural language processing and are making impressive strides in image analysis tasks such as image classification and object detection. Inspired by this success, we investigate the application of self-attention networks to 3D point cloud processing. We design self-attention layers for point clouds and use these to construct self-attention networks for tasks such as semantic scene segmentation, object part segmentation, and object classification. Our Point Transformer design improves upon prior work across domains and tasks. For example, on the challenging S3DIS dataset for large-scale semantic scene segmentation, the Point Transformer attains an mIoU of 70.4% on Area 5, outperforming the strongest prior model by 3.3 absolute percentage points and crossing the 70% mIoU threshold for the first time.",0
"The utilization of self-attention networks has transformed natural language processing and is currently advancing image analysis tasks, including image classification and object detection. With inspiration from this success, we have explored the potential of self-attention networks in 3D point cloud processing. Our approach involves creating self-attention layers for point clouds, which are utilized to build self-attention networks for various tasks such as semantic scene segmentation, object part segmentation, and object classification. Our Point Transformer design has shown significant improvement compared to previous work in different domains and tasks. For instance, in the challenging S3DIS dataset, the Point Transformer achieved an mIoU of 70.4% on Area 5 for large-scale semantic scene segmentation. This outperforms the strongest prior model by 3.3 absolute percentage points and marks the first time that the 70% mIoU threshold has been surpassed.",1
"Complex, multi-task problems have proven to be difficult to solve efficiently in a sparse-reward reinforcement learning setting. In order to be sample efficient, multi-task learning requires reuse and sharing of low-level policies. To facilitate the automatic decomposition of hierarchical tasks, we propose the use of step-by-step human demonstrations in the form of natural language instructions and action trajectories. We introduce a dataset of such demonstrations in a crafting-based grid world. Our model consists of a high-level language generator and low-level policy, conditioned on language. We find that human demonstrations help solve the most complex tasks. We also find that incorporating natural language allows the model to generalize to unseen tasks in a zero-shot setting and to learn quickly from a few demonstrations. Generalization is not only reflected in the actions of the agent, but also in the generated natural language instructions in unseen tasks. Our approach also gives our trained agent interpretable behaviors because it is able to generate a sequence of high-level descriptions of its actions.",0
"Sparse-reward reinforcement learning settings have proven to struggle with solving complex, multi-task problems efficiently. To achieve sample efficiency, multi-task learning requires the reuse and sharing of low-level policies. Our proposed solution involves using step-by-step human demonstrations in the form of natural language instructions and action trajectories to facilitate the automatic decomposition of hierarchical tasks. We have created a dataset of such demonstrations in a crafting-based grid world. Our model consists of a high-level language generator and low-level policy, which are conditioned on language. Our findings suggest that human demonstrations are particularly effective in solving the most complex tasks. Additionally, incorporating natural language enables the model to generalize to unseen tasks in a zero-shot setting and learn quickly from a few demonstrations. The model's generalization is demonstrated not only in its actions but also in the generated natural language instructions for unseen tasks. Our approach also allows the trained agent to exhibit interpretable behaviors as it can generate a sequence of high-level descriptions of its actions.",1
"Recurrent neural networks (RNNs) are powerful in the tasks oriented to sequential data, such as natural language processing and video recognition. However, since the modern RNNs, including long-short term memory (LSTM) and gated recurrent unit (GRU) networks, have complex topologies and expensive space/computation complexity, compressing them becomes a hot and promising topic in recent years. Among plenty of compression methods, tensor decomposition, e.g., tensor train (TT), block term (BT), tensor ring (TR) and hierarchical Tucker (HT), appears to be the most amazing approach since a very high compression ratio might be obtained. Nevertheless, none of these tensor decomposition formats can provide both the space and computation efficiency. In this paper, we consider to compress RNNs based on a novel Kronecker CANDECOMP/PARAFAC (KCP) decomposition, which is derived from Kronecker tensor (KT) decomposition, by proposing two fast algorithms of multiplication between the input and the tensor-decomposed weight. According to our experiments based on UCF11, Youtube Celebrities Face and UCF50 datasets, it can be verified that the proposed KCP-RNNs have comparable performance of accuracy with those in other tensor-decomposed formats, and even 278,219x compression ratio could be obtained by the low rank KCP. More importantly, KCP-RNNs are efficient in both space and computation complexity compared with other tensor-decomposed ones under similar ranks. Besides, we find KCP has the best potential for parallel computing to accelerate the calculations in neural networks.",0
"Recurrent neural networks (RNNs) are adept at handling sequential data tasks like natural language processing and video recognition. However, modern RNNs, such as long-short term memory (LSTM) and gated recurrent unit (GRU) networks, have complex topologies and are computationally expensive. Therefore, compressing them has become a popular and promising topic in recent years, with tensor decomposition being the most effective method. Tensor train (TT), block term (BT), tensor ring (TR), and hierarchical Tucker (HT) are some of the most commonly used tensor decomposition formats. However, none of these formats can offer both space and computation efficiency. Hence, this paper proposes a new Kronecker CANDECOMP/PARAFAC (KCP) decomposition method for compressing RNNs. The KCP is derived from Kronecker tensor (KT) decomposition and involves two fast algorithms for multiplying input and tensor-decomposed weights. The experiments conducted on UCF11, Youtube Celebrities Face, and UCF50 datasets show that KCP-RNNs have comparable accuracy performance to other tensor-decomposed formats, and can achieve up to a 278,219x compression ratio with low rank KCP. Importantly, KCP-RNNs have both space and computation efficiency compared to other tensor-decomposed formats with similar ranks. Additionally, KCP has the potential for parallel computing, which could accelerate the calculations in neural networks.",1
"Visual question answering (VQA) has recently been introduced to remote sensing to make information extraction from overhead imagery more accessible to everyone. VQA considers a question (in natural language, therefore easy to formulate) about an image and aims at providing an answer through a model based on computer vision and natural language processing methods. As such, a VQA model needs to jointly consider visual and textual features, which is frequently done through a fusion step. In this work, we study three different fusion methodologies in the context of VQA for remote sensing and analyse the gains in accuracy with respect to the model complexity. Our findings indicate that more complex fusion mechanisms yield an improved performance, yet that seeking a trade-of between model complexity and performance is worthwhile in practice.",0
"The integration of visual question answering (VQA) into remote sensing has made it easier for individuals to extract information from aerial images. VQA utilizes a natural language question to provide an answer through a computer vision and natural language processing model. To effectively achieve this, VQA models must consider both visual and textual features, which is typically accomplished through fusion. This study explores three fusion methodologies in the context of VQA for remote sensing and evaluates the impact on accuracy in relation to model complexity. The results indicate that more complex fusion mechanisms lead to better performance, but balancing model complexity and performance is beneficial in practical use.",1
"Pre-Trained Vision-Language Models (VL-PTMs) have shown promising capabilities in grounding natural language in image data, facilitating a broad variety of cross-modal tasks. However, we note that there exists a significant gap between the objective forms of model pre-training and fine-tuning, resulting in a need for quantities of labeled data to stimulate the visual grounding capability of VL-PTMs for downstream tasks. To address the challenge, we present Cross-modal Prompt Tuning (CPT, alternatively, Colorful Prompt Tuning), a novel paradigm for tuning VL-PTMs, which reformulates visual grounding into a fill-in-the-blank problem with color-based co-referential markers in image and text, maximally mitigating the gap. In this way, our prompt tuning approach enables strong few-shot and even zero-shot visual grounding capabilities of VL-PTMs. Comprehensive experimental results show that prompt tuned VL-PTMs outperform their fine-tuned counterparts by a large margin (e.g., 17.3% absolute accuracy improvement, and 73.8% relative standard deviation reduction on average with one shot in RefCOCO evaluation). All the data and code will be available to facilitate future research.",0
"Vision-Language Models that are pre-trained (VL-PTMs) have displayed potential in connecting natural language with image data, which aids in accomplishing a wide range of cross-modal tasks. Nonetheless, there is a considerable gap between the forms of model pre-training and fine-tuning, leading to the requirement of considerable labeled data to stimulate the visual grounding ability of VL-PTMs for downstream tasks. To combat this challenge, we propose a new approach called Cross-modal Prompt Tuning (CPT), also known as Colorful Prompt Tuning, which transforms visual grounding into a fill-in-the-blank problem with color-based co-referential markers in both image and text, substantially lessening the gap. By applying this prompt tuning technique, our VL-PTMs exhibit strong few-shot and even zero-shot visual grounding capabilities. Our extensive experimental results demonstrate that prompt-tuned VL-PTMs surpass their fine-tuned counterparts by a significant margin (e.g., 17.3% absolute accuracy improvement and 73.8% relative standard deviation reduction on average with a single shot in RefCOCO evaluation). All data and code will be accessible to facilitate future research.",1
"Generating fine-grained, realistic images from text has many applications in the visual and semantic realm. Considering that, we propose Bangla Attentional Generative Adversarial Network (AttnGAN) that allows intensified, multi-stage processing for high-resolution Bangla text-to-image generation. Our model can integrate the most specific details at different sub-regions of the image. We distinctively concentrate on the relevant words in the natural language description. This framework has achieved a better inception score on the CUB dataset. For the first time, a fine-grained image is generated from Bangla text using attentional GAN. Bangla has achieved 7th position among 100 most spoken languages. This inspires us to explicitly focus on this language, which will ensure the inevitable need of many people. Moreover, Bangla has a more complex syntactic structure and less natural language processing resource that validates our work more.",0
"The creation of detailed and lifelike images based on text has numerous practical uses in both visual and semantic fields. To address this, we have developed the Bangla Attentional Generative Adversarial Network (AttnGAN), which offers multi-stage processing capabilities for generating high-resolution Bangla text-to-image conversions. Our model can incorporate specific details from various sub-regions of the image, with a particular emphasis on relevant words in natural language descriptions. Our framework has produced superior results on the CUB dataset, marking the first instance of fine-grained Bangla text-to-image generation using attentional GAN. Bangla ranks as the 7th most commonly spoken language globally, making it an important focus for our work and ensuring its relevance to a broad audience. Furthermore, the language's intricate syntactic structure and limited natural language processing resources further validate our efforts.",1
"We investigate the incorporation of visual relationships into the task of supervised image caption generation by proposing a model that leverages detected objects and auto-generated visual relationships to describe images in natural language. To do so, we first generate a scene graph from raw image pixels by identifying individual objects and visual relationships between them. This scene graph then serves as input to our graph-to-text model, which generates the final caption. In contrast to previous approaches, our model thus explicitly models the detection of objects and visual relationships in the image. For our experiments we construct a new dataset from the intersection of Visual Genome and MS COCO, consisting of images with both a corresponding gold scene graph and human-authored caption. Our results show that our methods outperform existing state-of-the-art end-to-end models that generate image descriptions directly from raw input pixels when compared in terms of the BLEU and METEOR evaluation metrics.",0
"Our study explores the integration of visual relationships into supervised image caption generation. We propose a model that utilizes detected objects and auto-generated visual relationships to produce image descriptions in natural language. Initially, we create a scene graph by identifying individual objects and visual relationships between them from raw image pixels. This scene graph is then used as input for our graph-to-text model, which generates the final caption. Our approach differs from previous methods as it explicitly models object detection and visual relationships in the image. We develop a new dataset by merging Visual Genome and MS COCO, which includes images with corresponding gold scene graphs and human-authored captions. Our results display that our strategies surpass existing state-of-the-art end-to-end models generating image descriptions from raw input pixels when evaluated using BLEU and METEOR metrics.",1
"The longitudinal modeling of neuroanatomical changes related to Alzheimer's disease (AD) is crucial for studying the progression of the disease. To this end, we introduce TransforMesh, a spatio-temporal network based on transformers that models longitudinal shape changes on 3D anatomical meshes. While transformer and mesh networks have recently shown impressive performances in natural language processing and computer vision, their application to medical image analysis has been very limited. To the best of our knowledge, this is the first work that combines transformer and mesh networks. Our results show that TransforMesh can model shape trajectories better than other baseline architectures that do not capture temporal dependencies. Moreover, we also explore the capabilities of TransforMesh in detecting structural anomalies of the hippocampus in patients developing AD.",0
"Studying the progression of Alzheimer's disease (AD) requires a thorough understanding of the neuroanatomical changes over time. In order to achieve this, we have developed TransforMesh, a spatio-temporal network that utilizes transformers to model longitudinal shape changes on 3D anatomical meshes. Although transformers and mesh networks have demonstrated remarkable performances in natural language processing and computer vision, their application to medical image analysis has been limited. Our study is the first to combine transformer and mesh networks, and our results indicate that TransforMesh is superior to other baseline architectures that fail to capture temporal dependencies when modeling shape trajectories. Additionally, we have investigated the capability of TransforMesh in identifying structural anomalies of the hippocampus in patients with AD.",1
"Recent progress in the Natural Language Processing domain has given us several State-of-the-Art (SOTA) pretrained models which can be finetuned for specific tasks. These large models with billions of parameters trained on numerous GPUs/TPUs over weeks are leading in the benchmark leaderboards. In this paper, we discuss the need for a benchmark for cost and time effective smaller models trained on a single GPU. This will enable researchers with resource constraints experiment with novel and innovative ideas on tokenization, pretraining tasks, architecture, fine tuning methods etc. We set up Small-Bench NLP, a benchmark for small efficient neural language models trained on a single GPU. Small-Bench NLP benchmark comprises of eight NLP tasks on the publicly available GLUE datasets and a leaderboard to track the progress of the community. Our ELECTRA-DeBERTa (15M parameters) small model architecture achieves an average score of 81.53 which is comparable to that of BERT-Base's 82.20 (110M parameters). Our models, code and leaderboard are available at https://github.com/smallbenchnlp",0
"The Natural Language Processing field has made significant advancements, resulting in State-of-the-Art pretrained models that can be fine-tuned for specific tasks. These large models, trained on multiple GPUs/TPUs for weeks, are currently dominating benchmark leaderboards. This paper proposes the need for a benchmark focused on smaller, cost and time effective models trained on a single GPU. Such a benchmark would enable researchers with limited resources to experiment with novel ideas on tokenization, pretraining tasks, architecture, fine-tuning methods, and more. To address this need, we have established Small-Bench NLP, a benchmark comprising of eight NLP tasks based on the publicly available GLUE datasets, along with a leaderboard to track community progress. Our ELECTRA-DeBERTa model, with only 15M parameters, achieved an average score of 81.53, comparable to BERT-Base's score of 82.20 with 110M parameters. Our models, code, and leaderboard are available at https://github.com/smallbenchnlp.",1
"Given an untrimmed video and a natural language query, Natural Language Video Localization (NLVL) aims to identify the video moment described by the query. To address this task, existing methods can be roughly grouped into two groups: 1) propose-and-rank models first define a set of hand-designed moment candidates and then find out the best-matching one. 2) proposal-free models directly predict two temporal boundaries of the referential moment from frames. Currently, almost all the propose-and-rank methods have inferior performance than proposal-free counterparts. In this paper, we argue that propose-and-rank approach is underestimated due to the predefined manners: 1) Hand-designed rules are hard to guarantee the complete coverage of targeted segments. 2) Densely sampled candidate moments cause redundant computation and degrade the performance of ranking process. To this end, we propose a novel model termed LPNet (Learnable Proposal Network for NLVL) with a fixed set of learnable moment proposals. The position and length of these proposals are dynamically adjusted during training process. Moreover, a boundary-aware loss has been proposed to leverage frame-level information and further improve the performance. Extensive ablations on two challenging NLVL benchmarks have demonstrated the effectiveness of LPNet over existing state-of-the-art methods.",0
"The objective of Natural Language Video Localization (NLVL) is to identify the moment in an untrimmed video that corresponds to a given natural language query. Two types of methods have been developed for this task: 1) propose-and-rank models, which select a set of predetermined moment candidates and rank them to identify the best match, and 2) proposal-free models, which directly predict the temporal boundaries of the referential moment from frames. Currently, proposal-free methods outperform propose-and-rank methods. However, we argue that this is due to two problems with the propose-and-rank approach: 1) hand-designed rules do not always cover all targeted segments, and 2) densely sampled candidates lead to redundant computation and degraded ranking performance. To address these issues, we propose a new model called LPNet (Learnable Proposal Network for NLVL), which uses a fixed set of learnable moment proposals that are dynamically adjusted during training. Additionally, we introduce a boundary-aware loss to leverage frame-level information and improve performance. Our extensive experiments on two challenging benchmarks demonstrate that LPNet outperforms existing state-of-the-art methods.",1
"Temporal Sentence Grounding in Videos (TSGV), i.e., grounding a natural language sentence which indicates complex human activities in a long and untrimmed video sequence, has received unprecedented attentions over the last few years. Although each newly proposed method plausibly can achieve better performance than previous ones, current TSGV models still tend to capture the moment annotation biases and fail to take full advantage of multi-modal inputs. Even more incredibly, several extremely simple baselines without training can also achieve state-of-the-art performance. In this paper, we take a closer look at the existing evaluation protocols for TSGV, and find that both the prevailing dataset splits and evaluation metrics are the devils to cause unreliable benchmarking. To this end, we propose to re-organize two widely-used TSGV benchmarks (ActivityNet Captions and Charades-STA). Specifically, we deliberately make the ground-truth moment distribution different in the training and test splits, i.e., out-of-distribution (OOD) testing. Meanwhile, we introduce a new evaluation metric dR@n,IoU@m to calibrate the basic IoU scores by penalizing on the bias-influenced moment predictions and alleviate the inflating evaluations caused by the dataset annotation biases such as overlong ground-truth moments. Under our new evaluation protocol, we conduct extensive experiments and ablation studies on eight state-of-the-art TSGV methods. All the results demonstrate that the re-organized dataset splits and new metric can better monitor the progress in TSGV. Our reorganized datsets are available at https://github.com/yytzsy/grounding_changing_distribution.",0
"Over the past few years, there has been a surge in attention towards Temporal Sentence Grounding in Videos (TSGV), which involves identifying complex human activities in a lengthy and untrimmed video sequence based on a given natural language sentence. While various proposed methods have shown improved performance, they still tend to be influenced by moment annotation biases and fail to fully utilize multi-modal inputs. Surprisingly, some simple baselines have also achieved state-of-the-art results without training. This paper aims to scrutinize the existing evaluation protocols for TSGV and reveal the unreliability caused by both dataset splits and evaluation metrics. To address this issue, the authors propose reorganizing the widely-used TSGV benchmarks, ActivityNet Captions and Charades-STA, by introducing out-of-distribution (OOD) testing and a new evaluation metric, dR@n,IoU@m. The latter metric adjusts the basic IoU scores by penalizing the bias-influenced moment predictions and mitigating the impact of annotation biases. The authors then conduct extensive experiments and ablation studies on eight state-of-the-art TSGV methods under the new evaluation protocol, and show that the reorganized datasets and metric better monitor the progress of TSGV. The reorganized datasets are available on GitHub at https://github.com/yytzsy/grounding_changing_distribution.",1
"3D visual grounding aims at grounding a natural language description about a 3D scene, usually represented in the form of 3D point clouds, to the targeted object region. Point clouds are sparse, noisy, and contain limited semantic information compared with 2D images. These inherent limitations make the 3D visual grounding problem more challenging. In this study, we propose 2D Semantics Assisted Training (SAT) that utilizes 2D image semantics in the training stage to ease point-cloud-language joint representation learning and assist 3D visual grounding. The main idea is to learn auxiliary alignments between rich, clean 2D object representations and the corresponding objects or mentioned entities in 3D scenes. SAT takes 2D object semantics, i.e., object label, image feature, and 2D geometric feature, as the extra input in training but does not require such inputs during inference. By effectively utilizing 2D semantics in training, our approach boosts the accuracy on the Nr3D dataset from 37.7% to 49.2%, which significantly surpasses the non-SAT baseline with the identical network architecture and inference input. Our approach outperforms the state of the art by large margins on multiple 3D visual grounding datasets, i.e., +10.4% absolute accuracy on Nr3D, +9.9% on Sr3D, and +5.6% on ScanRef.",0
"The objective of 3D visual grounding is to establish a connection between a natural language description of a 3D scene, which is typically presented as 3D point clouds, and the specific region of interest. Due to their sparsity, noise, and limited semantic information, point clouds are less informative than 2D images and present a greater challenge for 3D visual grounding. To address this challenge, we introduce the concept of 2D Semantics Assisted Training (SAT), which employs the semantics of 2D images during the training stage to facilitate joint representation learning between point clouds and language and improve 3D visual grounding. Our approach trains auxiliary alignments between high-quality 2D object representations and the corresponding objects or entities in 3D scenes, utilizing object labels, image features, and 2D geometric features as additional input during training but not during inference. By leveraging 2D semantics during training, our method enhances the accuracy of the 3D visual grounding model significantly, outperforming the non-SAT baseline and achieving state-of-the-art performance on multiple 3D visual grounding datasets. Specifically, our approach improves the accuracy on Nr3D by 11.5%, on Sr3D by 9.9%, and on ScanRef by 5.6%.",1
"Transformers have seen an unprecedented rise in Natural Language Processing and Computer Vision tasks. However, in audio tasks, they are either infeasible to train due to extremely large sequence length of audio waveforms or reach competitive performance after feature extraction through Fourier-based methods, incurring a loss-floor. In this work, we introduce an architecture, Audiomer, where we combine 1D Residual Networks with Performer Attention to achieve state-of-the-art performance in Keyword Spotting with raw audio waveforms, out-performing all previous methods while also being computationally cheaper, much more parameter and data-efficient. Audiomer allows for deployment in compute-constrained devices and training on smaller datasets.",0
"The use of Transformers has significantly increased in Natural Language Processing and Computer Vision, but they have faced limitations in audio tasks due to either impractical training caused by the lengthy sequence of audio waveforms or suboptimal performance after Fourier-based feature extraction, resulting in a loss-floor. This study proposes Audiomer, an architecture that integrates 1D Residual Networks and Performer Attention to achieve exceptional performance in Keyword Spotting with raw audio waveforms. Audiomer outperforms prior approaches, requires less computation, and is more efficient in terms of parameters and data, making it suitable for deployment on resource-limited devices and training with smaller datasets.",1
"Transformers, the default model of choices in natural language processing, have drawn scant attention from the medical imaging community. Given the ability to exploit long-term dependencies, transformers are promising to help atypical convolutional neural networks (convnets) to overcome its inherent shortcomings of spatial inductive bias. However, most of recently proposed transformer-based segmentation approaches simply treated transformers as assisted modules to help encode global context into convolutional representations without investigating how to optimally combine self-attention (i.e., the core of transformers) with convolution. To address this issue, in this paper, we introduce nnFormer (i.e., Not-aNother transFormer), a powerful segmentation model with an interleaved architecture based on empirical combination of self-attention and convolution. In practice, nnFormer learns volumetric representations from 3D local volumes. Compared to the naive voxel-level self-attention implementation, such volume-based operations help to reduce the computational complexity by approximate 98% and 99.5% on Synapse and ACDC datasets, respectively. In comparison to prior-art network configurations, nnFormer achieves tremendous improvements over previous transformer-based methods on two commonly used datasets Synapse and ACDC. For instance, nnFormer outperforms Swin-UNet by over 7 percents on Synapse. Even when compared to nnUNet, currently the best performing fully-convolutional medical segmentation network, nnFormer still provides slightly better performance on Synapse and ACDC.",0
"Although transformers are a popular choice for natural language processing, they have not received much attention from the medical imaging community. Transformers have the potential to overcome the limitations of convnets' spatial inductive bias by exploiting long-term dependencies. However, recent transformer-based segmentation approaches have only used transformers as modules to assist in encoding global context into convolutional representations without exploring the optimal combination of self-attention (the core of transformers) with convolution. To address this issue, we introduce nnFormer, a powerful segmentation model that uses an interleaved architecture based on an empirical combination of self-attention and convolution. nnFormer learns volumetric representations from 3D local volumes, reducing computational complexity by approximately 98% and 99.5% on Synapse and ACDC datasets, respectively. Compared to previous transformer-based methods, nnFormer achieves significant improvements on Synapse and ACDC datasets, outperforming Swin-UNet by over 7% on Synapse and providing slightly better performance than nnUNet, the best-performing fully-convolutional medical segmentation network, on both datasets.",1
"Vision-language pre-training has recently emerged as a promising alternative for representation learning. It shifts from the tradition of using images and discrete labels for learning a fixed set of weights, seen as visual concepts, to aligning images and raw text for two separate encoders. Such a paradigm benefits from a broader source of supervision and allows zero-shot transfer to downstream tasks since visual concepts can be diametrically generated from natural language, known as prompt. In this paper, we identify that a major challenge of deploying such models in practice is prompt engineering. This is because designing a proper prompt, especially for context words surrounding a class name, requires domain expertise and typically takes a significant amount of time for words tuning since a slight change in wording could have a huge impact on performance. Moreover, different downstream tasks require specific designs, further hampering the efficiency of deployment. To overcome this challenge, we propose a novel approach named context optimization (CoOp). The main idea is to model context in prompts using continuous representations and perform end-to-end learning from data while keeping the pre-trained parameters fixed. In this way, the design of task-relevant prompts can be fully automated. Experiments on 11 datasets show that CoOp effectively turns pre-trained vision-language models into data-efficient visual learners, requiring as few as one or two shots to beat hand-crafted prompts with a decent margin and able to gain significant improvements when using more shots (e.g., at 16 shots the average gain is around 17% with the highest reaching over 50%). CoOp also exhibits strong robustness to distribution shift.",0
"The use of vision-language pre-training is a promising method for representation learning, which involves aligning images and raw text for two encoders rather than relying on discrete labels. This approach offers a wider range of supervision and allows for zero-shot transfer to downstream tasks. However, deploying such models in practice is challenging due to the need for prompt engineering, which involves designing a suitable prompt that can take a significant amount of time and requires domain expertise. Additionally, different downstream tasks require specific prompt designs, further complicating the process. To address this issue, we propose a new method called context optimization (CoOp), which utilizes continuous representations to model context in prompts and performs end-to-end learning while keeping pre-trained parameters fixed. This approach allows for the automation of task-relevant prompt design, making the process more efficient. Our experiments on 11 datasets demonstrate that CoOp effectively improves the performance of pre-trained vision-language models, requiring as few as one or two shots to outperform hand-crafted prompts, and exhibiting strong robustness to distribution shift.",1
"Inspired by the success of transformer-based pre-training methods on natural language tasks and further computer vision tasks, researchers have begun to apply transformer to video processing. This survey aims to give a comprehensive overview on transformer-based pre-training methods for Video-Language learning. We first briefly introduce the transformer tructure as the background knowledge, including attention mechanism, position encoding etc. We then describe the typical paradigm of pre-training & fine-tuning on Video-Language processing in terms of proxy tasks, downstream tasks and commonly used video datasets. Next, we categorize transformer models into Single-Stream and Multi-Stream structures, highlight their innovations and compare their performances. Finally, we analyze and discuss the current challenges and possible future research directions for Video-Language pre-training.",0
"Researchers have been inspired by the success of transformer-based pre-training methods in natural language and computer vision tasks, and have started using transformers for video processing. This survey provides a comprehensive overview of transformer-based pre-training methods for Video-Language learning. The survey initially introduces the transformer structure, which includes attention mechanism, position encoding, and other background knowledge. The pre-training and fine-tuning paradigm for Video-Language processing is then described in terms of proxy tasks, downstream tasks, and commonly used video datasets. The transformer models are categorized as Single-Stream and Multi-Stream structures, highlighting their innovations and comparing their performances. Finally, the survey analyzes and discusses the current challenges and possible future research directions for Video-Language pre-training.",1
"We introduce a new type of programming challenge called programming puzzles, as an objective and comprehensive evaluation of program synthesis, and release an open-source dataset of Python Programming Puzzles (P3). Each puzzle is defined by a short Python program $f$, and the goal is to find an input $x$ which makes $f$ output ""True"". The puzzles are objective in that each one is specified entirely by the source code of its verifier $f$, so evaluating $f(x)$ is all that is needed to test a candidate solution $x$. They do not require an answer key or input/output examples, nor do they depend on natural language understanding. The dataset is comprehensive in that it spans problems of a range of difficulties and domains, ranging from trivial string manipulation problems that are immediately obvious to human programmers (but not necessarily to AI), to classic programming puzzles (e.g., Towers of Hanoi), to interview/competitive-programming problems (e.g., dynamic programming), to longstanding open problems in algorithms and mathematics (e.g., factoring). The objective nature of P3 readily supports self-supervised bootstrapping. We develop baseline enumerative program synthesis and GPT-3 solvers that are capable of solving easy puzzles -- even without access to any reference solutions -- by learning from their own past solutions. Based on a small user study, we find puzzle difficulty to correlate between human programmers and the baseline AI solvers.",0
"We have created a new programming challenge called programming puzzles, which is an unbiased and all-encompassing method of evaluating program synthesis. We have also created an open-source dataset called Python Programming Puzzles (P3). Each puzzle consists of a short Python program, with the objective of finding an input that will make the program output ""True"". These puzzles are unbiased because they are defined solely by the source code of their verifier program, and evaluating the program with a candidate solution is the only requirement for testing. They do not rely on a key or input/output examples, nor do they require natural language understanding. The dataset covers a wide range of difficulties and domains, from simple string manipulation problems to classic programming puzzles and even longstanding open problems in mathematics and algorithms. P3's objective nature facilitates self-supervised bootstrapping, and we have developed baseline enumerative program synthesis and GPT-3 solvers that can solve easy puzzles without any reference solutions by learning from their own past solutions. Our small user study revealed that puzzle difficulty correlates between human programmers and baseline AI solvers.",1
"Patients associated with multiple co-occurring health conditions often face aggravated complications and less favorable outcomes. Co-occurring conditions are especially prevalent among individuals suffering from kidney disease, an increasingly widespread condition affecting 13% of the general population in the US. This study aims to identify and characterize patterns of co-occurring medical conditions in patients employing a probabilistic framework. Specifically, we apply topic modeling in a non-traditional way to find associations across SNOMEDCT codes assigned and recorded in the EHRs of>13,000 patients diagnosed with kidney disease. Unlike most prior work on topic modeling, we apply the method to codes rather than to natural language. Moreover, we quantitatively evaluate the topics, assessing their tightness and distinctiveness, and also assess the medical validity of our results. Our experiments show that each topic is succinctly characterized by a few highly probable and unique disease codes, indicating that the topics are tight. Furthermore, inter-topic distance between each pair of topics is typically high, illustrating distinctiveness. Last, most coded conditions grouped together within a topic, are indeed reported to co-occur in the medical literature. Notably, our results uncover a few indirect associations among conditions that have hitherto not been reported as correlated in the medical literature.",0
"Individuals with kidney disease, which affects 13% of the US population, are particularly prone to experiencing multiple co-occurring health conditions that can lead to worsened complications and outcomes. This study uses a probabilistic framework and topic modeling to identify and analyze patterns of co-occurring medical conditions in over 13,000 patients diagnosed with kidney disease. Unlike previous studies, the method is applied to codes rather than natural language. The resulting topics are evaluated for tightness, distinctiveness, and medical validity. The findings show that each topic is characterized by a few highly probable and unique disease codes, indicating tightness, and that the inter-topic distance is high, indicating distinctiveness. Additionally, most coded conditions grouped together within a topic are reported to co-occur in the medical literature. The study also uncovers previously unreported indirect associations among conditions.",1
"Statistical language modeling and translation with transformers have found many successful applications in program understanding and generation tasks, setting high benchmarks for tools in modern software development environments. The finite context window of these neural models means, however, that they will be unable to leverage the entire relevant context of large files and packages for any given task. While there are many efforts to extend the context window, we introduce an architecture-independent approach for leveraging the syntactic hierarchies of source code for incorporating entire file-level context into a fixed-length window. Using concrete syntax trees of each source file we extract syntactic hierarchies and integrate them into context window by selectively removing from view more specific, less relevant scopes for a given task. We evaluate this approach on code generation tasks and joint translation of natural language and source code in Python programming language, achieving a new state-of-the-art in code completion and summarization for Python in the CodeXGLUE benchmark. We also introduce new CodeXGLUE benchmarks for user-experience-motivated tasks: code completion with normalized literals, method body completion/code summarization conditioned on file-level context.",0
"The use of statistical language modeling and transformers in program comprehension and generation tasks has been highly successful, setting impressive standards for modern software development tools. However, the limited context window of these neural models poses a challenge, as they cannot fully utilize the relevant context of large files and packages for a given task. Despite ongoing efforts to extend the context window, we propose an architecture-independent method that leverages the syntactic hierarchies of source code to incorporate the entire file-level context into a fixed-length window. By extracting the concrete syntax trees of each source file, we selectively remove less relevant scopes, integrating the syntactic hierarchies into the context window. Our approach achieves a new state-of-the-art in Python code completion and summarization for the CodeXGLUE benchmark, and we also introduce new benchmarks for user-experience-motivated tasks, such as code completion with normalized literals and method body completion/summary conditioned on file-level context.",1
"Large Transformer models have been central to recent advances in natural language processing. The training and inference costs of these models, however, have grown rapidly and become prohibitively expensive. Here we aim to reduce the costs of Transformers by searching for a more efficient variant. Compared to previous approaches, our search is performed at a lower level, over the primitives that define a Transformer TensorFlow program. We identify an architecture, named Primer, that has a smaller training cost than the original Transformer and other variants for auto-regressive language modeling. Primer's improvements can be mostly attributed to two simple modifications: squaring ReLU activations and adding a depthwise convolution layer after each Q, K, and V projection in self-attention.   Experiments show Primer's gains over Transformer increase as compute scale grows and follow a power law with respect to quality at optimal model sizes. We also verify empirically that Primer can be dropped into different codebases to significantly speed up training without additional tuning. For example, at a 500M parameter size, Primer improves the original T5 architecture on C4 auto-regressive language modeling, reducing the training cost by 4X. Furthermore, the reduced training cost means Primer needs much less compute to reach a target one-shot performance. For instance, in a 1.9B parameter configuration similar to GPT-3 XL, Primer uses 1/3 of the training compute to achieve the same one-shot performance as Transformer. We open source our models and several comparisons in T5 to help with reproducibility.",0
"Recent breakthroughs in natural language processing have largely relied on large Transformer models. However, the cost of training and inference for these models has become excessively high. To address this issue, we have developed a more efficient variant of the Transformer called Primer. Our approach involves searching for an architecture at a lower level, focusing on the primitives that define a Transformer TensorFlow program. Primer's improvements are mainly due to two simple modifications: squaring ReLU activations and adding a depthwise convolution layer after each Q, K, and V projection in self-attention. Our experiments demonstrate that Primer outperforms the original Transformer and other variants for auto-regressive language modeling, especially as compute scale grows. Additionally, Primer can be easily integrated into different codebases to significantly speed up training without any additional tuning. For example, at a 500M parameter size, Primer reduces the training cost by 4X compared to the original T5 architecture on C4 auto-regressive language modeling. Furthermore, Primer requires much less compute to achieve a target one-shot performance, as demonstrated in a 1.9B parameter configuration similar to GPT-3 XL. We have made our models and comparisons in T5 available as open source to facilitate reproducibility.",1
"Temporal sentence grounding in videos(TSGV), which aims to localize one target segment from an untrimmed video with respect to a given sentence query, has drawn increasing attentions in the research community over the past few years. Different from the task of temporal action localization, TSGV is more flexible since it can locate complicated activities via natural languages, without restrictions from predefined action categories. Meanwhile, TSGV is more challenging since it requires both textual and visual understanding for semantic alignment between two modalities(i.e., text and video). In this survey, we give a comprehensive overview for TSGV, which i) summarizes the taxonomy of existing methods, ii) provides a detailed description of the evaluation protocols(i.e., datasets and metrics) to be used in TSGV, and iii) in-depth discusses potential problems of current benchmarking designs and research directions for further investigations. To the best of our knowledge, this is the first systematic survey on temporal sentence grounding. More specifically, we first discuss existing TSGV approaches by grouping them into four categories, i.e., two-stage methods, end-to-end methods, reinforcement learning-based methods, and weakly supervised methods. Then we present the benchmark datasets and evaluation metrics to assess current research progress. Finally, we discuss some limitations in TSGV through pointing out potential problems improperly resolved in the current evaluation protocols, which may push forwards more cutting edge research in TSGV. Besides, we also share our insights on several promising directions, including three typical tasks with new and practical settings based on TSGV.",0
"Over the past few years, there has been a growing interest in temporal sentence grounding in videos (TSGV). This task aims to locate a specific segment in an untrimmed video based on a given sentence query. Unlike temporal action localization, TSGV is more flexible as it can identify complex activities using natural language, without being limited by pre-defined action categories. However, TSGV is also more challenging as it requires both textual and visual comprehension to align the semantics of the two modalities. This survey provides a comprehensive overview of TSGV. It summarizes existing methods, describes evaluation protocols, and discusses potential issues with benchmarking designs and future research directions. This is the first systematic survey of TSGV. The survey categorizes existing TSGV approaches into four groups: two-stage methods, end-to-end methods, reinforcement learning-based methods, and weakly supervised methods. It also presents benchmark datasets and evaluation metrics to measure current research progress. The survey highlights potential problems with the current evaluation protocols and suggests new research directions. Furthermore, it proposes three new and practical tasks based on TSGV.",1
"Estimating the performance of a machine learning system is a longstanding challenge in artificial intelligence research. Today, this challenge is especially relevant given the emergence of systems which appear to increasingly outperform human beings. In some cases, this ""superhuman"" performance is readily demonstrated; for example by defeating legendary human players in traditional two player games. On the other hand, it can be challenging to evaluate classification models that potentially surpass human performance. Indeed, human annotations are often treated as a ground truth, which implicitly assumes the superiority of the human over any models trained on human annotations. In reality, human annotators can make mistakes and be subjective. Evaluating the performance with respect to a genuine oracle may be more objective and reliable, even when querying the oracle is expensive or impossible. In this paper, we first raise the challenge of evaluating the performance of both humans and models with respect to an oracle which is unobserved. We develop a theory for estimating the accuracy compared to the oracle, using only imperfect human annotations for reference. Our analysis provides a simple recipe for detecting and certifying superhuman performance in this setting, which we believe will assist in understanding the stage of current research on classification. We validate the convergence of the bounds and the assumptions of our theory on carefully designed toy experiments with known oracles. Moreover, we demonstrate the utility of our theory by meta-analyzing large-scale natural language processing tasks, for which an oracle does not exist, and show that under our assumptions a number of models from recent years are with high probability superhuman.",0
"Assessing the effectiveness of a machine learning system has always been a difficult task in the field of artificial intelligence research. This is particularly pertinent today due to the emergence of systems that appear to be surpassing human performance. Although some systems have demonstrated ""superhuman"" performance by defeating renowned human players in conventional two player games, it can be challenging to evaluate classification models that have the potential to surpass humans. Typically, human annotations are considered the ground truth, which assumes the superiority of humans over models trained on human annotations. However, human annotators can make mistakes and be subjective. Therefore, evaluating the performance with respect to an authentic oracle may be more objective and reliable, even if querying the oracle is expensive or impossible. This research paper introduces the challenge of evaluating the performance of both humans and models with respect to an unobserved oracle. A theory is developed for estimating accuracy compared to the oracle, using only imperfect human annotations for reference. The analysis provides a simple approach for identifying and verifying superhuman performance in this scenario, which is hoped to assist in understanding the current state of classification research. The validity of the theory is confirmed through carefully designed toy experiments with known oracles. Additionally, the usefulness of the theory is demonstrated by meta-analyzing large-scale natural language processing tasks, for which an oracle does not exist, and it is shown that under the assumptions of the theory, several models from recent years are highly likely to be superhuman.",1
"EEG signals are usually simple to obtain but expensive to label. Although supervised learning has been widely used in the field of EEG signal analysis, its generalization performance is limited by the amount of annotated data. Self-supervised learning (SSL), as a popular learning paradigm in computer vision (CV) and natural language processing (NLP), can employ unlabeled data to make up for the data shortage of supervised learning. In this paper, we propose a self-supervised contrastive learning method of EEG signals for sleep stage classification. During the training process, we set up a pretext task for the network in order to match the right transformation pairs generated from EEG signals. In this way, the network improves the representation ability by learning the general features of EEG signals. The robustness of the network also gets improved in dealing with diverse data, that is, extracting constant features from changing data. In detail, the network's performance depends on the choice of transformations and the amount of unlabeled data used in the training process of self-supervised learning. Empirical evaluations on the Sleep-edf dataset demonstrate the competitive performance of our method on sleep staging (88.16% accuracy and 81.96% F1 score) and verify the effectiveness of SSL strategy for EEG signal analysis in limited labeled data regimes. All codes are provided publicly online.",0
"Obtaining EEG signals is usually easy, but labeling them is costly. While supervised learning is commonly used in EEG signal analysis, its effectiveness is limited by the amount of annotated data available. To address this issue, we propose using self-supervised learning, a popular learning approach in computer vision and natural language processing, to leverage unlabeled data and overcome the shortage of labeled data. Specifically, we propose a self-supervised contrastive learning method for sleep stage classification using EEG signals. During training, we design a pretext task for the network to match pairs of transformations generated from the EEG signals, which enables the network to learn general features of EEG signals and improve its robustness in dealing with diverse data. Our method's performance depends on the choice of transformations and the amount of unlabeled data used in the training process. Empirical evaluations on the Sleep-edf dataset demonstrate our method's competitive performance in sleep staging and verify the effectiveness of the self-supervised learning strategy in limited labeled data scenarios. Our code is publicly available.",1
"Chest radiographs are one of the most common diagnostic modalities in clinical routine. It can be done cheaply, requires minimal equipment, and the image can be diagnosed by every radiologists. However, the number of chest radiographs obtained on a daily basis can easily overwhelm the available clinical capacities. We propose RATCHET: RAdiological Text Captioning for Human Examined Thoraces. RATCHET is a CNN-RNN-based medical transformer that is trained end-to-end. It is capable of extracting image features from chest radiographs, and generates medically accurate text reports that fit seamlessly into clinical work flows. The model is evaluated for its natural language generation ability using common metrics from NLP literature, as well as its medically accuracy through a surrogate report classification task. The model is available for download at: http://www.github.com/farrell236/RATCHET.",0
"The use of chest radiographs is widespread in clinical practice due to its affordability, minimal equipment requirement, and ease of interpretation by radiologists. However, the high volume of daily chest radiographs can exceed the clinical capacity. To address this issue, we introduce RATCHET: RAdiological Text Captioning for Human Examined Thoraces, a CNN-RNN-based medical transformer that is trained end-to-end. RATCHET can extract image features from chest radiographs and generate medically accurate text reports that can be seamlessly integrated into clinical workflows. Our model's natural language generation ability is evaluated using metrics from NLP literature, and its medical accuracy is assessed through a surrogate report classification task. Interested users can download the model at http://www.github.com/farrell236/RATCHET.",1
"Adversarial regularization has been shown to improve the generalization performance of deep learning models in various natural language processing tasks. Existing works usually formulate the method as a zero-sum game, which is solved by alternating gradient descent/ascent algorithms. Such a formulation treats the adversarial and the defending players equally, which is undesirable because only the defending player contributes to the generalization performance. To address this issue, we propose Stackelberg Adversarial Regularization (SALT), which formulates adversarial regularization as a Stackelberg game. This formulation induces a competition between a leader and a follower, where the follower generates perturbations, and the leader trains the model subject to the perturbations. Different from conventional approaches, in SALT, the leader is in an advantageous position. When the leader moves, it recognizes the strategy of the follower and takes the anticipated follower's outcomes into consideration. Such a leader's advantage enables us to improve the model fitting to the unperturbed data. The leader's strategic information is captured by the Stackelberg gradient, which is obtained using an unrolling algorithm. Our experimental results on a set of machine translation and natural language understanding tasks show that SALT outperforms existing adversarial regularization baselines across all tasks. Our code is publicly available.",0
"Various natural language processing tasks have shown that adversarial regularization can enhance the generalization performance of deep learning models. Typically, this method is formulated as a zero-sum game, solved by alternating gradient descent/ascent algorithms. However, this formulation treats the adversarial and defending players equally, which is not ideal as only the defending player contributes to the generalization performance. To address this issue, we propose Stackelberg Adversarial Regularization (SALT), which formulates adversarial regularization as a Stackelberg game. SALT induces a competition between a leader and a follower, where the follower generates perturbations, and the leader trains the model subject to the perturbations. In contrast to conventional methods, in SALT, the leader has an advantage. The leader's advantage is that it recognizes the strategy of the follower and takes the anticipated follower's outcomes into account when making a move. This advantage enables us to enhance the model's fitting to the unperturbed data. The Stackelberg gradient captures the leader's strategic information, which is obtained using an unrolling algorithm. Our experimental results on machine translation and natural language understanding tasks demonstrate that SALT outperforms existing adversarial regularization baselines across all tasks. Our code is available to the public.",1
"Change captioning tasks aim to detect changes in image pairs observed before and after a scene change and generate a natural language description of the changes. Existing change captioning studies have mainly focused on a single change.However, detecting and describing multiple changed parts in image pairs is essential for enhancing adaptability to complex scenarios. We solve the above issues from three aspects: (i) We propose a simulation-based multi-change captioning dataset; (ii) We benchmark existing state-of-the-art methods of single change captioning on multi-change captioning; (iii) We further propose Multi-Change Captioning transformers (MCCFormers) that identify change regions by densely correlating different regions in image pairs and dynamically determines the related change regions with words in sentences. The proposed method obtained the highest scores on four conventional change captioning evaluation metrics for multi-change captioning. Additionally, our proposed method can separate attention maps for each change and performs well with respect to change localization. Moreover, the proposed framework outperformed the previous state-of-the-art methods on an existing change captioning benchmark, CLEVR-Change, by a large margin (+6.1 on BLEU-4 and +9.7 on CIDEr scores), indicating its general ability in change captioning tasks.",0
"The objective of change captioning tasks is to identify alterations between two images captured before and after a scene transition and generate a natural language description of the changes. Although previous studies have mainly focused on detecting a single change, it is crucial to detect and describe multiple changes in image pairs to enhance adaptability to complicated scenarios. To tackle these issues, we address them from three perspectives: (i) We introduce a simulation-based dataset for multi-change captioning; (ii) We evaluate existing state-of-the-art methods for single change captioning on multi-change captioning; (iii) We propose Multi-Change Captioning transformers (MCCFormers) that densely correlate different regions in image pairs to identify change regions and dynamically determine the corresponding change regions with words in sentences. Our proposed approach achieved the highest scores on four conventional evaluation metrics for multi-change captioning. Furthermore, our method can generate attention maps for each change and performs well in terms of change localization. Additionally, our proposed framework outperformed previous state-of-the-art methods on an established change captioning benchmark, CLEVR-Change, by a significant margin (+6.1 on BLEU-4 and +9.7 on CIDEr scores), which demonstrates its general capability in change captioning tasks.",1
"Due to the advances in computing and sensing, deep learning (DL) has widely been applied in smart energy systems (SESs). These DL-based solutions have proved their potentials in improving the effectiveness and adaptiveness of the control systems. However, in recent years, increasing evidence shows that DL techniques can be manipulated by adversarial attacks with carefully-crafted perturbations. Adversarial attacks have been studied in computer vision and natural language processing. However, there is very limited work focusing on the adversarial attack deployment and mitigation in energy systems. In this regard, to better prepare the SESs against potential adversarial attacks, we propose an innovative adversarial attack model that can practically compromise dynamical controls of energy system. We also optimize the deployment of the proposed adversarial attack model by employing deep reinforcement learning (RL) techniques. In this paper, we present our first-stage work in this direction. In simulation section, we evaluate the performance of our proposed adversarial attack model using standard IEEE 9-bus system.",0
"The application of deep learning (DL) in smart energy systems (SESs) has become widespread thanks to advances in computing and sensing technologies. DL-based solutions have been proven to enhance the effectiveness and adaptiveness of control systems. Nevertheless, recent research suggests that DL techniques can be manipulated by adversarial attacks through carefully-crafted perturbations. Although adversarial attacks have been extensively studied in computer vision and natural language processing, there is limited work focusing on their deployment and mitigation in energy systems. To address this gap, we propose an innovative adversarial attack model that can potentially compromise the dynamical controls of an energy system. We also employ deep reinforcement learning (RL) techniques to optimize the deployment of our proposed adversarial attack model. This paper presents our initial work in this area and evaluates the performance of our proposed model using the standard IEEE 9-bus system in the simulation section. Our goal is to better equip SESs against potential adversarial attacks.",1
"Motivated by the success of masked language modeling~(MLM) in pre-training natural language processing models, we propose w2v-BERT that explores MLM for self-supervised speech representation learning. w2v-BERT is a framework that combines contrastive learning and MLM, where the former trains the model to discretize input continuous speech signals into a finite set of discriminative speech tokens, and the latter trains the model to learn contextualized speech representations via solving a masked prediction task consuming the discretized tokens. In contrast to existing MLM-based speech pre-training frameworks such as HuBERT, which relies on an iterative re-clustering and re-training process, or vq-wav2vec, which concatenates two separately trained modules, w2v-BERT can be optimized in an end-to-end fashion by solving the two self-supervised tasks~(the contrastive task and MLM) simultaneously. Our experiments show that w2v-BERT achieves competitive results compared to current state-of-the-art pre-trained models on the LibriSpeech benchmarks when using the Libri-Light~60k corpus as the unsupervised data. In particular, when compared to published models such as conformer-based wav2vec~2.0 and HuBERT, our model shows~5\% to~10\% relative WER reduction on the test-clean and test-other subsets. When applied to the Google's Voice Search traffic dataset, w2v-BERT outperforms our internal conformer-based wav2vec~2.0 by more than~30\% relatively.",0
"Inspired by the success of masked language modeling (MLM) in pre-training natural language processing models, we present w2v-BERT, a framework that employs MLM for self-supervised speech representation learning. Our approach combines contrastive learning and MLM, with the former training the model to convert continuous speech signals into a finite set of discriminative speech tokens and the latter training the model to learn contextualized speech representations by solving a masked prediction task using the discretized tokens. Unlike existing MLM-based speech pre-training frameworks, w2v-BERT can be optimized end-to-end by simultaneously solving both self-supervised tasks, rather than relying on an iterative re-clustering and re-training process or concatenating separately trained modules. Our experiments demonstrate that w2v-BERT achieves competitive results compared to current state-of-the-art pre-trained models on the LibriSpeech benchmarks using the Libri-Light 60k corpus as unsupervised data. Specifically, our model shows a 5-10% relative WER reduction on the test-clean and test-other subsets compared to published models such as conformer-based wav2vec 2.0 and HuBERT. Furthermore, when applied to the Google Voice Search traffic dataset, w2v-BERT outperforms our internal conformer-based wav2vec 2.0 by over 30% relatively.",1
"Video grounding aims to localize the temporal segment corresponding to a sentence query from an untrimmed video. Almost all existing video grounding methods fall into two frameworks: 1) Top-down model: It predefines a set of segment candidates and then conducts segment classification and regression. 2) Bottom-up model: It directly predicts frame-wise probabilities of the referential segment boundaries. However, all these methods are not end-to-end, \ie, they always rely on some time-consuming post-processing steps to refine predictions. To this end, we reformulate video grounding as a set prediction task and propose a novel end-to-end multi-modal Transformer model, dubbed as \textbf{GTR}. Specifically, GTR has two encoders for video and language encoding, and a cross-modal decoder for grounding prediction. To facilitate the end-to-end training, we use a Cubic Embedding layer to transform the raw videos into a set of visual tokens. To better fuse these two modalities in the decoder, we design a new Multi-head Cross-Modal Attention. The whole GTR is optimized via a Many-to-One matching loss. Furthermore, we conduct comprehensive studies to investigate different model design choices. Extensive results on three benchmarks have validated the superiority of GTR. All three typical GTR variants achieve record-breaking performance on all datasets and metrics, with several times faster inference speed.",0
"The objective of video grounding is to pinpoint the specific time segment of an untrimmed video that corresponds to a given sentence query. The majority of current video grounding methods can be categorized into two frameworks: the top-down model, which predefines a set of segment candidates and performs segment classification and regression, and the bottom-up model, which directly predicts frame-wise probabilities of the referential segment boundaries. However, these approaches are not end-to-end, meaning that they require time-consuming post-processing steps to refine their predictions. In order to address this issue, we propose a new end-to-end multi-modal Transformer model called GTR, formulated as a set prediction task. GTR comprises two encoders for video and language encoding, and a cross-modal decoder for grounding prediction. To enable end-to-end training, we employ a Cubic Embedding layer to convert raw videos into visual tokens. To better integrate the two modalities in the decoder, we introduce a new Multi-head Cross-Modal Attention. The entire GTR is optimized through a Many-to-One matching loss. Additionally, we conduct extensive studies to analyze various model design choices. Our results on three benchmarks demonstrate that GTR outperforms other methods, achieving record-breaking performance on all datasets and metrics while also having a faster inference speed.",1
"Term weighting schemes are widely used in Natural Language Processing and Information Retrieval. In particular, term weighting is the basis for keyword extraction. However, there are relatively few evaluation studies that shed light about the strengths and shortcomings of each weighting scheme. In fact, in most cases researchers and practitioners resort to the well-known tf-idf as default, despite the existence of other suitable alternatives, including graph-based models. In this paper, we perform an exhaustive and large-scale empirical comparison of both statistical and graph-based term weighting methods in the context of keyword extraction. Our analysis reveals some interesting findings such as the advantages of the less-known lexical specificity with respect to tf-idf, or the qualitative differences between statistical and graph-based methods. Finally, based on our findings we discuss and devise some suggestions for practitioners. Source code to reproduce our experimental results, including a keyword extraction library, are available in the following repository: https://github.com/asahi417/kex",0
"Term weighting schemes are extensively utilized in the fields of Natural Language Processing and Information Retrieval, forming the foundation of keyword extraction. Despite the existence of other suitable alternatives, researchers and practitioners tend to rely on tf-idf as their default option, as there are limited evaluation studies that compare the strengths and weaknesses of each weighting method. In this paper, we conduct a comprehensive and large-scale empirical comparison of both statistical and graph-based term weighting methods within the context of keyword extraction. Our analysis uncovers several intriguing findings, such as the benefits of the lesser-known lexical specificity compared to tf-idf, as well as the qualitative differences between statistical and graph-based approaches. Based on our results, we provide recommendations for practitioners. Our experimental results, including a keyword extraction library, can be found in the following repository: https://github.com/asahi417/kex",1
"A key problem in multi-task learning (MTL) research is how to select high-quality auxiliary tasks automatically. This paper presents GradTS, an automatic auxiliary task selection method based on gradient calculation in Transformer-based models. Compared to AUTOSEM, a strong baseline method, GradTS improves the performance of MT-DNN with a bert-base-cased backend model, from 0.33% to 17.93% on 8 natural language understanding (NLU) tasks in the GLUE benchmarks. GradTS is also time-saving since (1) its gradient calculations are based on single-task experiments and (2) the gradients are re-used without additional experiments when the candidate task set changes. On the 8 GLUE classification tasks, for example, GradTS costs on average 21.32% less time than AUTOSEM with comparable GPU consumption. Further, we show the robustness of GradTS across various task settings and model selections, e.g. mixed objectives among candidate tasks. The efficiency and efficacy of GradTS in these case studies illustrate its general applicability in MTL research without requiring manual task filtering or costly parameter tuning.",0
"Multi-task learning (MTL) research faces a challenge in automatically choosing high-quality auxiliary tasks. This paper introduces GradTS, a method for selecting auxiliary tasks automatically in Transformer-based models using gradient calculation. Compared to the baseline method AUTOSEM, GradTS improves MT-DNN performance with a bert-base-cased backend model by up to 17.93% on 8 natural language understanding (NLU) tasks in the GLUE benchmarks. In addition, GradTS saves time by using single-task experiments for gradient calculations and re-using gradients when the candidate task set changes. On average, GradTS costs 21.32% less time than AUTOSEM with comparable GPU consumption. GradTS is also robust in various task settings and model selections, making it a general and efficient method for MTL research without requiring manual task filtering or costly parameter tuning.",1
"The cross entropy loss is widely used due to its effectiveness and solid theoretical grounding. However, as training progresses, the loss tends to focus on hard to classify samples, which may prevent the network from obtaining gains in performance. While most work in the field suggest ways to classify hard negatives, we suggest to strategically leave hard negatives behind, in order to focus on misclassified samples with higher probabilities. We show that adding to the optimization goal the expectation loss, which is a better approximation of the zero-one loss, helps the network to achieve better accuracy. We, therefore, propose to shift between the two losses during training, focusing more on the expectation loss gradually during the later stages of training. Our experiments show that the new training protocol improves performance across a diverse set of classification domains, including computer vision, natural language processing, tabular data, and sequences. Our code and scripts are available at supplementary.",0
"The cross entropy loss is commonly used because of its effectiveness and strong theoretical foundation. However, during training, the loss may concentrate on challenging-to-classify samples, hindering the network's ability to improve its performance. While many studies suggest methods for classifying difficult negatives, we propose leaving them behind and prioritizing misclassified samples with higher probabilities. By including the expectation loss, which approximates the zero-one loss better, in the optimization objective, we demonstrate that the network can achieve higher accuracy. Therefore, we propose alternating between the two losses during training, gradually shifting focus to the expectation loss in the later stages. Our experimental results demonstrate that this new training protocol enhances performance across a range of classification domains such as computer vision, natural language processing, tabular data, and sequences. Supplementary code and scripts are available.",1
"The progress of some AI paradigms such as deep learning is said to be linked to an exponential growth in the number of parameters. There are many studies corroborating these trends, but does this translate into an exponential increase in energy consumption? In order to answer this question we focus on inference costs rather than training costs, as the former account for most of the computing effort, solely because of the multiplicative factors. Also, apart from algorithmic innovations, we account for more specific and powerful hardware (leading to higher FLOPS) that is usually accompanied with important energy efficiency optimisations. We also move the focus from the first implementation of a breakthrough paper towards the consolidated version of the techniques one or two year later. Under this distinctive and comprehensive perspective, we study relevant models in the areas of computer vision and natural language processing: for a sustained increase in performance we see a much softer growth in energy consumption than previously anticipated. The only caveat is, yet again, the multiplicative factor, as future AI increases penetration and becomes more pervasive.",0
"It has been suggested that the advancement of certain AI paradigms, such as deep learning, is attributed to the exponential rise in parameters. While numerous studies support this notion, the question remains whether this leads to a corresponding increase in energy consumption. To address this, we examine inference costs instead of training costs, as the former constitutes the bulk of computing effort due to multiplicative factors. Additionally, we consider advancements in hardware, which yield higher FLOPS and greater energy efficiency. Our analysis concentrates on established techniques, rather than initial implementations, in computer vision and natural language processing. Our findings indicate that, for sustained performance improvement, energy consumption growth is more gradual than previously expected. Nonetheless, it is important to note that the multiplicative factor may pose challenges as AI increasingly penetrates and becomes more ubiquitous.",1
"Internet search affects people's cognition of the world, so mitigating biases in search results and learning fair models is imperative for social good. We study a unique gender bias in image search in this work: the search images are often gender-imbalanced for gender-neutral natural language queries. We diagnose two typical image search models, the specialized model trained on in-domain datasets and the generalized representation model pre-trained on massive image and text data across the internet. Both models suffer from severe gender bias. Therefore, we introduce two novel debiasing approaches: an in-processing fair sampling method to address the gender imbalance issue for training models, and a post-processing feature clipping method base on mutual information to debias multimodal representations of pre-trained models. Extensive experiments on MS-COCO and Flickr30K benchmarks show that our methods significantly reduce the gender bias in image search models.",0
"It is crucial for the betterment of society to address biases in internet search results and develop fair models. This is because such biases can influence people's perception of the world. The focus of this research is to examine the gender bias that exists in image search results when using gender-neutral language queries. We analyze two commonly used models in image search, namely the specialized model and the generalized representation model. Both models display significant gender bias. Hence, we propose two new methods to mitigate such biases: an in-processing fair sampling method to address gender imbalance during model training and a post-processing feature clipping method based on mutual information to debias multimodal representations in pre-trained models. Our experiments on MS-COCO and Flickr30K benchmarks demonstrate that our approaches significantly reduce gender bias in image search models.",1
"The research community has proposed copious modifications to the Transformer architecture since it was introduced over three years ago, relatively few of which have seen widespread adoption. In this paper, we comprehensively evaluate many of these modifications in a shared experimental setting that covers most of the common uses of the Transformer in natural language processing. Surprisingly, we find that most modifications do not meaningfully improve performance. Furthermore, most of the Transformer variants we found beneficial were either developed in the same codebase that we used or are relatively minor changes. We conjecture that performance improvements may strongly depend on implementation details and correspondingly make some recommendations for improving the generality of experimental results.",0
"Numerous adjustments to the Transformer architecture have been proposed by the research community since its introduction more than three years ago. However, only a few of them have been widely adopted. This paper exhaustively assesses many of these modifications under a common experimental environment that covers most of the Transformer's typical applications in natural language processing. Surprisingly, we discovered that most of the adjustments do not meaningfully enhance performance. Additionally, we discovered that the Transformer variants that were advantageous were either developed using the same codebase we used or were minor alterations. We speculate that the improvements in performance may rely heavily on the implementation's specifics and thus provide recommendations for improving the universality of experimental findings.",1
"This paper proposes Panoptic Narrative Grounding, a spatially fine and general formulation of the natural language visual grounding problem. We establish an experimental framework for the study of this new task, including new ground truth and metrics, and we propose a strong baseline method to serve as stepping stone for future work. We exploit the intrinsic semantic richness in an image by including panoptic categories, and we approach visual grounding at a fine-grained level by using segmentations. In terms of ground truth, we propose an algorithm to automatically transfer Localized Narratives annotations to specific regions in the panoptic segmentations of the MS COCO dataset. To guarantee the quality of our annotations, we take advantage of the semantic structure contained in WordNet to exclusively incorporate noun phrases that are grounded to a meaningfully related panoptic segmentation region. The proposed baseline achieves a performance of 55.4 absolute Average Recall points. This result is a suitable foundation to push the envelope further in the development of methods for Panoptic Narrative Grounding.",0
"The natural language visual grounding problem can be addressed with Panoptic Narrative Grounding, which is a spatially fine and general approach. To study this new task, we have established an experimental framework that includes new ground truth and metrics. Additionally, we have proposed a strong baseline method to serve as a starting point for future work. Our approach leverages the semantic richness in an image by incorporating panoptic categories and segmentations to fine-tune visual grounding. To ensure high-quality annotations, we have devised an algorithm that automatically transfers Localized Narratives annotations to specific regions in the panoptic segmentations of the MS COCO dataset. We have also used the semantic structure contained in WordNet to exclusively incorporate noun phrases that are grounded to a meaningfully related panoptic segmentation region. Our proposed baseline achieves a performance of 55.4 absolute Average Recall points, which provides a solid foundation for further advancement in the development of methods for Panoptic Narrative Grounding.",1
"Temporal grounding aims to temporally localize a video moment in the video whose semantics are related to a given natural language query. Existing methods typically apply a detection or regression pipeline on the fused representation with a focus on designing complicated heads and fusion strategies. Instead, from a perspective on temporal grounding as a metric-learning problem, we present a Dual Matching Network (DMN), to directly model the relations between language queries and video moments in a joint embedding space. This new metric-learning framework enables fully exploiting negative samples from two new aspects: constructing negative cross-modal pairs from a dual matching scheme and mining negative pairs across different videos. These new negative samples could enhance the joint representation learning of two modalities via cross-modal pair discrimination to maximize their mutual information. Experiments show that DMN achieves highly competitive performance compared with state-of-the-art methods on four video grounding benchmarks. Based on DMN, we present a winner solution for STVG challenge of the 3rd PIC workshop. This suggests that metric-learning is still a promising method for temporal grounding via capturing the essential cross-modal correlation in a joint embedding space.",0
"The goal of temporal grounding is to pinpoint a specific moment in a video that is semantically related to a given natural language query. Current approaches rely on complex head and fusion strategies using detection or regression pipelines. However, we propose a new perspective on temporal grounding as a metric-learning problem and introduce the Dual Matching Network (DMN). This framework models the relationship between language queries and video moments in a joint embedding space, allowing for the exploitation of negative samples. These negative samples are generated through a dual matching scheme and by mining across different videos, improving cross-modal pair discrimination and maximizing mutual information. Our experiments show that DMN achieves competitive performance on four video grounding benchmarks and is the winner solution for the STVG challenge of the 3rd PIC workshop. This suggests that metric-learning remains a promising approach for temporal grounding by capturing the essential cross-modal correlation in a joint embedding space.",1
"Narrated instructional videos often show and describe manipulations of similar objects, e.g., repairing a particular model of a car or laptop. In this work we aim to reconstruct such objects and to localize associated narrations in 3D. Contrary to the standard scenario of instance-level 3D reconstruction, where identical objects or scenes are present in all views, objects in different instructional videos may have large appearance variations given varying conditions and versions of the same product. Narrations may also have large variation in natural language expressions. We address these challenges by three contributions. First, we propose an approach for correspondence estimation combining learnt local features and dense flow. Second, we design a two-step divide and conquer reconstruction approach where the initial 3D reconstructions of individual videos are combined into a 3D alignment graph. Finally, we propose an unsupervised approach to ground natural language in obtained 3D reconstructions. We demonstrate the effectiveness of our approach for the domain of car maintenance. Given raw instructional videos and no manual supervision, our method successfully reconstructs engines of different car models and associates textual descriptions with corresponding objects in 3D.",0
"The purpose of this study is to reconstruct objects and localize narrations in 3D for instructional videos that demonstrate the manipulation of similar objects, such as repairing a specific car or laptop model. This differs from instance-level 3D reconstruction because objects in instructional videos may have varying appearances due to differences in product versions and conditions, while narrations may use different language expressions. To overcome these challenges, we propose three contributions: a correspondence estimation approach that utilizes local features and dense flow, a two-step divide and conquer reconstruction method that combines individual video reconstructions into a 3D alignment graph, and an unsupervised approach to ground natural language in obtained 3D reconstructions. We demonstrate the effectiveness of our approach in car maintenance by successfully reconstructing engines of different car models and associating textual descriptions with corresponding objects in 3D, without any manual supervision.",1
"Along with feature points for image matching, line features provide additional constraints to solve visual geometric problems in robotics and computer vision (CV). Although recent convolutional neural network (CNN)-based line descriptors are promising for viewpoint changes or dynamic environments, we claim that the CNN architecture has innate disadvantages to abstract variable line length into the fixed-dimensional descriptor. In this paper, we effectively introduce Line-Transformers dealing with variable lines. Inspired by natural language processing (NLP) tasks where sentences can be understood and abstracted well in neural nets, we view a line segment as a sentence that contains points (words). By attending to well-describable points on aline dynamically, our descriptor performs excellently on variable line length. We also propose line signature networks sharing the line's geometric attributes to neighborhoods. Performing as group descriptors, the networks enhance line descriptors by understanding lines' relative geometries. Finally, we present the proposed line descriptor and matching in a Point and Line Localization (PL-Loc). We show that the visual localization with feature points can be improved using our line features. We validate the proposed method for homography estimation and visual localization.",0
"Line features are another tool in robotics and computer vision that can be used in conjunction with feature points for image matching. While recent CNN-based line descriptors are promising for dynamic environments, we argue that CNN architecture has limitations when it comes to abstracting variable line length into a fixed-dimensional descriptor. In this study, we introduce Line-Transformers that can effectively handle variable line length. We were inspired by natural language processing, where sentences can be abstracted well in neural nets, and we view a line segment as a sentence that contains points. Our descriptor performs well on variable line length by dynamically attending to well-describable points on a line. Additionally, we propose line signature networks that enhance line descriptors by understanding the relative geometries of lines. Finally, we present our proposed line descriptor and matching in a Point and Line Localization (PL-Loc) framework, which improves visual localization with feature points using our line features. We validate our method for homography estimation and visual localization.",1
"Deep learning models exhibit a preference for statistical fitting over logical reasoning. Spurious correlations might be memorized when there exists statistical bias in training data, which severely limits the model performance especially in small data scenarios. In this work, we introduce Counterfactual Adversarial Training framework (CAT) to tackle the problem from a causality perspective. Particularly, for a specific sample, CAT first generates a counterfactual representation through latent space interpolation in an adversarial manner, and then performs Counterfactual Risk Minimization (CRM) on each original-counterfactual pair to adjust sample-wise loss weight dynamically, which encourages the model to explore the true causal effect. Extensive experiments demonstrate that CAT achieves substantial performance improvement over SOTA across different downstream tasks, including sentence classification, natural language inference and question answering.",0
"The preference of deep learning models for statistical fitting over logical reasoning can lead to memorization of spurious correlations in the presence of statistical bias in training data. This limitation is particularly significant in small data scenarios. To address this problem from a causality perspective, we propose the Counterfactual Adversarial Training framework (CAT). CAT generates a counterfactual representation for a specific sample using latent space interpolation in an adversarial manner and then performs Counterfactual Risk Minimization (CRM) on each original-counterfactual pair to adjust sample-wise loss weight dynamically. This approach encourages the model to explore the true causal effect. Our extensive experiments show that CAT significantly improves performance over the state-of-the-art for various downstream tasks, including sentence classification, natural language inference, and question answering.",1
"Video question answering (VideoQA) is challenging given its multimodal combination of visual understanding and natural language understanding. While existing approaches seldom leverage the appearance-motion information in the video at multiple temporal scales, the interaction between the question and the visual information for textual semantics extraction is frequently ignored. Targeting these issues, this paper proposes a novel Temporal Pyramid Transformer (TPT) model with multimodal interaction for VideoQA. The TPT model comprises two modules, namely Question-specific Transformer (QT) and Visual Inference (VI). Given the temporal pyramid constructed from a video, QT builds the question semantics from the coarse-to-fine multimodal co-occurrence between each word and the visual content. Under the guidance of such question-specific semantics, VI infers the visual clues from the local-to-global multi-level interactions between the question and the video. Within each module, we introduce a multimodal attention mechanism to aid the extraction of question-video interactions, with residual connections adopted for the information passing across different levels. Through extensive experiments on three VideoQA datasets, we demonstrate better performances of the proposed method in comparison with the state-of-the-arts.",0
"The combination of visual and natural language understanding makes VideoQA a challenging task. However, current approaches do not utilize appearance-motion information in videos at various temporal scales, and they often overlook the interaction between questions and visual information for textual semantics extraction. To address these challenges, a Temporal Pyramid Transformer (TPT) model with multimodal interaction is proposed in this paper. The TPT model consists of two modules: Question-specific Transformer (QT) and Visual Inference (VI). QT constructs question semantics from coarse-to-fine multimodal co-occurrence between each word and the visual content, while VI infers visual clues from local-to-global multi-level interactions between questions and videos. Multimodal attention mechanisms aid in the extraction of question-video interactions within each module, and residual connections facilitate information passing across different levels. Experimental results on three VideoQA datasets demonstrate the superior performance of the proposed method compared to existing state-of-the-art approaches.",1
"Temporal grounding aims to predict a time interval of a video clip corresponding to a natural language query input. In this work, we present EVOQUER, a temporal grounding framework incorporating an existing text-to-video grounding model and a video-assisted query generation network. Given a query and an untrimmed video, the temporal grounding model predicts the target interval, and the predicted video clip is fed into a video translation task by generating a simplified version of the input query. EVOQUER forms closed-loop learning by incorporating loss functions from both temporal grounding and query generation serving as feedback. Our experiments on two widely used datasets, Charades-STA and ActivityNet, show that EVOQUER achieves promising improvements by 1.05 and 1.31 at R@0.7. We also discuss how the query generation task could facilitate error analysis by explaining temporal grounding model behavior.",0
"Temporal grounding is the process of determining the time interval in a video clip that corresponds to a natural language query input. Our study introduces EVOQUER, which is a temporal grounding framework that includes a pre-existing text-to-video grounding model and a video-assisted query generation network. The temporal grounding model predicts the target interval based on the query and untrimmed video, and the resulting video clip is used in a video translation task by generating a simplified version of the input query. EVOQUER combines temporal grounding and query generation loss functions to form a closed-loop learning system. Our experiments on Charades-STA and ActivityNet datasets demonstrate that EVOQUER produces promising results, with R@0.7 improvements of 1.05 and 1.31, respectively. We also explore how the query generation task can help analyze errors by explaining the behavior of the temporal grounding model.",1
"Facial editing is an important task in vision and graphics with numerous applications. However, existing works are incapable to deliver a continuous and fine-grained editing mode (e.g., editing a slightly smiling face to a big laughing one) with natural interactions with users. In this work, we propose Talk-to-Edit, an interactive facial editing framework that performs fine-grained attribute manipulation through dialog between the user and the system. Our key insight is to model a continual ""semantic field"" in the GAN latent space. 1) Unlike previous works that regard the editing as traversing straight lines in the latent space, here the fine-grained editing is formulated as finding a curving trajectory that respects fine-grained attribute landscape on the semantic field. 2) The curvature at each step is location-specific and determined by the input image as well as the users' language requests. 3) To engage the users in a meaningful dialog, our system generates language feedback by considering both the user request and the current state of the semantic field.   We also contribute CelebA-Dialog, a visual-language facial editing dataset to facilitate large-scale study. Specifically, each image has manually annotated fine-grained attribute annotations as well as template-based textual descriptions in natural language. Extensive quantitative and qualitative experiments demonstrate the superiority of our framework in terms of 1) the smoothness of fine-grained editing, 2) the identity/attribute preservation, and 3) the visual photorealism and dialog fluency. Notably, user study validates that our overall system is consistently favored by around 80% of the participants. Our project page is https://www.mmlab-ntu.com/project/talkedit/.",0
"In the field of vision and graphics, facial editing is a crucial task with a range of practical applications. However, current techniques are limited in their ability to provide a seamless and detailed editing experience, such as transforming a slightly smiling face into one that is laughing heartily, while also maintaining natural interactions with users. To address this issue, we propose Talk-to-Edit, an interactive framework for facial editing that enables fine-grained attribute manipulation through dialogue between the user and the system. Our approach involves creating a continuous ""semantic field"" in the GAN latent space, which allows for more nuanced editing than previous methods that only move along straight lines. The curvature of the trajectory is determined by the input image and user requests, and the system generates language feedback based on both the user's input and the current state of the semantic field. To facilitate large-scale study, we have also created CelebA-Dialog, a dataset with manually annotated fine-grained attribute annotations and natural language descriptions. Our experiments demonstrate that our framework offers superior editing smoothness, identity/attribute preservation, and visual photorealism and dialogue fluency. In user studies, around 80% of participants consistently preferred our system. More information can be found on our project page at https://www.mmlab-ntu.com/project/talkedit/.",1
"Machine learning has been utilized to perform tasks in many different domains such as classification, object detection, image segmentation and natural language analysis. Data labeling has always been one of the most important tasks in machine learning. However, labeling large amounts of data increases the monetary cost in machine learning. As a result, researchers started to focus on reducing data annotation and labeling costs. Transfer learning was designed and widely used as an efficient approach that can reasonably reduce the negative impact of limited data, which in turn, reduces the data preparation cost. Even transferring previous knowledge from a source domain reduces the amount of data needed in a target domain. However, large amounts of annotated data are still demanded to build robust models and improve the prediction accuracy of the model. Therefore, researchers started to pay more attention on auto annotation and labeling. In this survey paper, we provide a review of previous techniques that focuses on optimized data annotation and labeling for video, audio, and text data.",0
"Machine learning has proven to be useful in various fields such as object detection, classification, natural language analysis, and image segmentation. Nonetheless, data labeling is an essential aspect of machine learning that requires a significant amount of money. To address this issue, researchers have been looking for ways to reduce data annotation and labeling costs. Transfer learning has been introduced to reduce the effect of limited data and the related preparation cost. This approach involves transferring knowledge from a source domain to a target domain, which cuts down the amount of data required. However, building robust models and improving prediction accuracy still requires a large amount of annotated data. Therefore, researchers have shifted their attention to auto annotation and labeling. This survey paper reviews previous techniques that have focused on optimizing data annotation and labeling for text, audio, and video data.",1
"Generating images according to natural language descriptions is a challenging task. Prior research has mainly focused to enhance the quality of generation by investigating the use of spatial attention and/or textual attention thereby neglecting the relationship between channels. In this work, we propose the Combined Attention Generative Adversarial Network (CAGAN) to generate photo-realistic images according to textual descriptions. The proposed CAGAN utilises two attention models: word attention to draw different sub-regions conditioned on related words; and squeeze-and-excitation attention to capture non-linear interaction among channels. With spectral normalisation to stabilise training, our proposed CAGAN improves the state of the art on the IS and FID on the CUB dataset and the FID on the more challenging COCO dataset. Furthermore, we demonstrate that judging a model by a single evaluation metric can be misleading by developing an additional model adding local self-attention which scores a higher IS, outperforming the state of the art on the CUB dataset, but generates unrealistic images through feature repetition.",0
"The task of creating images based on natural language descriptions is difficult. Previous research has focused on improving the quality of image generation by exploring the use of spatial and textual attention, while neglecting the correlation between channels. Our proposed solution, the Combined Attention Generative Adversarial Network (CAGAN), incorporates two attention models: word attention, which draws sub-regions based on related words, and squeeze-and-excitation attention, which captures nonlinear interaction among channels. By incorporating spectral normalization to stabilize training, our CAGAN produces high-quality, realistic images and outperforms previous models on the CUB and COCO datasets. However, we also found that evaluating models based on a single metric can be misleading. We developed an additional model with local self-attention that scored higher on the IS metric on the CUB dataset but produced unrealistic images due to feature repetition.",1
"In the last decade or so, we have witnessed deep learning reinvigorating the machine learning field. It has solved many problems in the domains of computer vision, speech recognition, natural language processing, and various other tasks with state-of-the-art performance. The data is generally represented in the Euclidean space in these domains. Various other domains conform to non-Euclidean space, for which graph is an ideal representation. Graphs are suitable for representing the dependencies and interrelationships between various entities. Traditionally, handcrafted features for graphs are incapable of providing the necessary inference for various tasks from this complex data representation. Recently, there is an emergence of employing various advances in deep learning to graph data-based tasks. This article provides a comprehensive survey of graph neural networks (GNNs) in each learning setting: supervised, unsupervised, semi-supervised, and self-supervised learning. Taxonomy of each graph based learning setting is provided with logical divisions of methods falling in the given learning setting. The approaches for each learning task are analyzed from both theoretical as well as empirical standpoints. Further, we provide general architecture guidelines for building GNNs. Various applications and benchmark datasets are also provided, along with open challenges still plaguing the general applicability of GNNs.",0
"Over the past decade, deep learning has reinvigorated the field of machine learning, solving numerous problems in computer vision, speech recognition, natural language processing, and other areas with exceptional performance. While data in these domains is typically represented in Euclidean space, non-Euclidean domains require a graph representation to capture dependencies and relationships between entities. However, traditional handcrafted features for graphs lack the necessary inference for complex data representation. As a result, recent advances in deep learning have been employed to graph data-based tasks, leading to the emergence of graph neural networks (GNNs). This article presents a comprehensive survey of GNNs in various learning settings, including supervised, unsupervised, semi-supervised, and self-supervised learning. We provide a taxonomy of each learning setting, with logical divisions of methods falling in the given setting, and analyze approaches from theoretical and empirical perspectives. Additionally, we offer general architecture guidelines for building GNNs, highlight various applications and benchmark datasets, and discuss open challenges that still hinder the general applicability of GNNs.",1
"Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks e.g., Long short-term memory (LSTM). Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers i.e., self-attention, large-scale pre-training, and bidirectional encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization) and 3D analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works.",0
"The impressive performance of Transformer models in natural language tasks has piqued the interest of the vision community, who are now exploring their potential use in computer vision problems. Transformers have several advantages over recurrent networks, such as the ability to model long dependencies and process sequences in parallel. They also require minimal inductive biases and are suitable for processing multiple modalities. These strengths have led to exciting progress in vision tasks using Transformer networks. This survey provides a comprehensive overview of Transformer models in computer vision, including an introduction to key concepts like self-attention and bidirectional encoding, as well as their applications in various vision tasks. The advantages and limitations of popular techniques are compared, and future research directions are discussed.",1
"Deep neural networks for natural language processing are fragile in the face of adversarial examples -- small input perturbations, like synonym substitution or word duplication, which cause a neural network to change its prediction. We present an approach to certifying the robustness of LSTMs (and extensions of LSTMs) and training models that can be efficiently certified. Our approach can certify robustness to intractably large perturbation spaces defined programmatically in a language of string transformations. Our evaluation shows that (1) our approach can train models that are more robust to combinations of string transformations than those produced using existing techniques; (2) our approach can show high certification accuracy of the resulting models.",0
"Adversarial examples, such as small input modifications, can easily disrupt the predictions of deep neural networks used for natural language processing. To address this issue, we propose a method for verifying the stability of LSTMs and their variations, as well as training models that can be certified efficiently. Our technique can guarantee resistance to complex, programmatically defined string transformations that are too large to handle manually. Our experiments demonstrate that our approach can create more resilient models than current methods and deliver high certification accuracy.",1
"More transformer blocks with residual connections have recently achieved impressive results on various tasks. To achieve better performance with fewer trainable parameters, recent methods are proposed to go shallower by parameter sharing or model compressing along with the depth. However, weak modeling capacity limits their performance. Contrastively, going wider by inducing more trainable matrixes and parameters would produce a huge model requiring advanced parallelism to train and inference.   In this paper, we propose a parameter-efficient framework, going wider instead of deeper. Specially, following existing works, we adapt parameter sharing to compress along depth. But, such deployment would limit the performance. To maximize modeling capacity, we scale along model width by replacing feed-forward network (FFN) with mixture-of-experts (MoE). Across transformer blocks, instead of sharing normalization layers, we propose to use individual layernorms to transform various semantic representations in a more parameter-efficient way. To evaluate our plug-and-run framework, we design WideNet and conduct comprehensive experiments on popular computer vision and natural language processing benchmarks. On ImageNet-1K, our best model outperforms Vision Transformer (ViT) by $1.5\%$ with $0.72 \times$ trainable parameters. Using $0.46 \times$ and $0.13 \times$ parameters, our WideNet can still surpass ViT and ViT-MoE by $0.8\%$ and $2.1\%$, respectively. On four natural language processing datasets, WideNet outperforms ALBERT by $1.8\%$ on average and surpass BERT using factorized embedding parameterization by $0.8\%$ with fewer parameters.",0
"Recent methods have shown impressive results on various tasks by using transformer blocks with residual connections, but they often have limited modeling capacity. To address this, some propose going shallower by sharing parameters or compressing the model along the depth, but this can limit performance. Conversely, going wider by increasing the number of trainable parameters can lead to a large model that requires advanced parallelism. This paper proposes a parameter-efficient framework that goes wider instead of deeper, using parameter sharing to compress along depth and replacing the feed-forward network with a mixture-of-experts to maximize modeling capacity. Individual layernorms are used to transform various semantic representations more efficiently, and the resulting WideNet outperforms state-of-the-art models on computer vision and natural language processing benchmarks while using fewer parameters.",1
"Crop and weed monitoring is an important challenge for agriculture and food production nowadays. Thanks to recent advances in data acquisition and computation technologies, agriculture is evolving to a more smart and precision farming to meet with the high yield and high quality crop production. Classification and recognition in Unmanned Aerial Vehicles (UAV) images are important phases for crop monitoring. Advances in deep learning models relying on Convolutional Neural Network (CNN) have achieved high performances in image classification in the agricultural domain. Despite the success of this architecture, CNN still faces many challenges such as high computation cost, the need of large labelled datasets, ... Natural language processing's transformer architecture can be an alternative approach to deal with CNN's limitations. Making use of the self-attention paradigm, Vision Transformer (ViT) models can achieve competitive or better results without applying any convolution operations. In this paper, we adopt the self-attention mechanism via the ViT models for plant classification of weeds and crops: red beet, off-type beet (green leaves), parsley and spinach. Our experiments show that with small set of labelled training data, ViT models perform better compared to state-of-the-art CNN-based models EfficientNet and ResNet, with a top accuracy of 99.8\% achieved by the ViT model.",0
"Nowadays, agriculture and food production face a significant challenge with crop and weed monitoring. However, recent advancements in data acquisition and computation technologies have led to the emergence of smart and precision farming. Unmanned Aerial Vehicles (UAV) images play a crucial role in crop monitoring, where classification and recognition are critical phases. While Convolutional Neural Network (CNN) has achieved significant success in image classification in agriculture, it still faces challenges such as high computation cost and the need for large labelled datasets. Luckily, the transformer architecture of natural language processing offers an alternative approach to deal with CNN's limitations. Vision Transformer (ViT) models, which rely on the self-attention paradigm, can achieve better results without any convolution operations. In this paper, we utilize ViT models for plant classification of crops and weeds, including red beet, off-type beet, parsley, and spinach. Our experiments demonstrate that ViT models outperform state-of-the-art CNN-based models like EfficientNet and ResNet, with a top accuracy of 99.8\% achieved by ViT models, even with minimal labelled training data.",1
"Currently nearing human-level performance, Visual Question Answering (VQA) is an emerging area in artificial intelligence.   Established as a multi-disciplinary field in machine learning, both computer vision and natural language processing communities are working together to achieve state-of-the-art (SOTA) performance.   However, there is a gap between the SOTA results and real world applications.   This is due to the lack of model generalisation.   The RAMEN model \cite{Shrestha2019} aimed to achieve domain generalization by obtaining the highest score across two main types of VQA datasets.   This study provides two major improvements to the early/late fusion module and aggregation module of the RAMEN architecture, with the objective of further strengthening domain generalization.   Vector operations based fusion strategies are introduced for the fusion module and the transformer architecture is introduced for the aggregation module.   Improvements of up to five VQA datasets from the experiments conducted are evident.   Following the results, this study analyses the effects of both the improvements on the domain generalization problem.   The code is available on GitHub though the following link \url{https://github.com/bhanukaManesha/ramen}.",0
"Visual Question Answering (VQA) is a growing field in artificial intelligence that is approaching human-level performance. It involves collaboration between the computer vision and natural language processing communities to achieve state-of-the-art (SOTA) results. However, despite these advancements, there is still a gap between SOTA performance and real-world applications due to a lack of model generalization. To address this issue, the RAMEN model \cite{Shrestha2019} was developed to achieve domain generalization across two main types of VQA datasets. This study focuses on improving the early/late fusion module and aggregation module of the RAMEN architecture. The proposed improvements include vector operations-based fusion strategies for the fusion module and the introduction of the transformer architecture for the aggregation module. These improvements have resulted in up to five VQA datasets showing improvement in performance. Additionally, this study analyzes the effects of the improvements on the domain generalization problem. The code for the RAMEN model is publicly available on GitHub at \url{https://github.com/bhanukaManesha/ramen}.",1
"Learning from image-text data has demonstrated recent success for many recognition tasks, yet is currently limited to visual features or individual visual concepts such as objects. In this paper, we propose one of the first methods that learn from image-sentence pairs to extract a graphical representation of localized objects and their relationships within an image, known as scene graph. To bridge the gap between images and texts, we leverage an off-the-shelf object detector to identify and localize object instances, match labels of detected regions to concepts parsed from captions, and thus create ""pseudo"" labels for learning scene graph. Further, we design a Transformer-based model to predict these ""pseudo"" labels via a masked token prediction task. Learning from only image-sentence pairs, our model achieves 30% relative gain over a latest method trained with human-annotated unlocalized scene graphs. Our model also shows strong results for weakly and fully supervised scene graph generation. In addition, we explore an open-vocabulary setting for detecting scene graphs, and present the first result for open-set scene graph generation. Our code is available at https://github.com/YiwuZhong/SGG_from_NLS.",0
"Recently, learning from image-text data has been successful for many recognition tasks, but it has been limited to visual features or individual concepts such as objects. This paper proposes a method that learns from image-sentence pairs to extract a graphical representation of localized objects and their relationships within an image called a scene graph. To connect images and texts, an off-the-shelf object detector is used to identify and localize object instances, then labels of detected regions are matched to concepts parsed from captions, creating ""pseudo"" labels for learning scene graph. A Transformer-based model is designed to predict these ""pseudo"" labels through a masked token prediction task. The model achieves a 30% relative gain over a recent method trained with human-annotated unlocalized scene graphs, and strong results are shown for weakly and fully supervised scene graph generation. Additionally, an open-vocabulary setting for detecting scene graphs is explored, and the first result for open-set scene graph generation is presented. The code is available on GitHub at https://github.com/YiwuZhong/SGG_from_NLS.",1
"In recent years Deep Learning reached significant results in many practical problems, such as computer vision, natural language processing, speech recognition and many others. For many years the main goal of the research was to improve the quality of models, even if the complexity was impractically high. However, for the production solutions, which often require real-time work, the latency of the model plays a very important role. Current state-of-the-art architectures are found with neural architecture search (NAS) taking model complexity into account. However, designing of the search space suitable for specific hardware is still a challenging task. To address this problem we propose a measure of hardware efficiency of neural architecture search space - matrix efficiency measure (MEM); a search space comprising of hardware-efficient operations; a latency-aware scaling method; and ISyNet - a set of architectures designed to be fast on the specialized neural processing unit (NPU) hardware and accurate at the same time. We show the advantage of the designed architectures for the NPU devices on ImageNet and the generalization ability for the downstream classification and detection tasks.",0
"Deep Learning has made significant advancements in various practical applications such as computer vision, natural language processing, and speech recognition in recent years. While models' quality has been the primary focus of research, the complexity of these models often makes them impractical for real-time solutions. The latest neural architecture search (NAS) approaches consider model complexity, leading to state-of-the-art architectures. However, designing a search space suitable for specific hardware is still a challenging task. To address this issue, we propose a matrix efficiency measure (MEM) to evaluate hardware efficiency, a search space that comprises hardware-efficient operations, a latency-aware scaling method, and ISyNet, a set of architectures designed for fast and accurate performance on specialized neural processing unit (NPU) hardware. Our designed architectures showcase superior performance on ImageNet and generalization ability for downstream classification and detection tasks on NPU devices.",1
"Success of deep neural networks in diverse tasks across domains of computer vision, speech recognition and natural language processing, has necessitated understanding the dynamics of training process and also working of trained models. Two independent contributions of this paper are 1) Novel activation function for faster training convergence 2) Systematic pruning of filters of models trained irrespective of activation function. We analyze the topological transformation of the space of training samples as it gets transformed by each successive layer during training, by changing the activation function. The impact of changing activation function on the convergence during training is reported for the task of binary classification. A novel activation function aimed at faster convergence for classification tasks is proposed. Here, Betti numbers are used to quantify topological complexity of data. Results of experiments on popular synthetic binary classification datasets with large Betti numbers(>150) using MLPs are reported. Results show that the proposed activation function results in faster convergence requiring fewer epochs by a factor of 1.5 to 2, since Betti numbers reduce faster across layers with the proposed activation function. The proposed methodology was verified on benchmark image datasets: fashion MNIST, CIFAR-10 and cat-vs-dog images, using CNNs. Based on empirical results, we propose a novel method for pruning a trained model. The trained model was pruned by eliminating filters that transform data to a topological space with large Betti numbers. All filters with Betti numbers greater than 300 were removed from each layer without significant reduction in accuracy. This resulted in faster prediction time and reduced memory size of the model.",0
"The success of deep neural networks in various domains, including computer vision, speech recognition, and natural language processing, has made it necessary to comprehend the training process dynamics and the functionality of trained models. This paper presents two independent contributions: 1) a new activation function to enhance training convergence speed, and 2) a systematic approach to pruning filters in trained models, irrespective of the activation function. The authors analyze the topological transformations of the training sample space as it progresses through each layer of the network, by changing the activation function. They report the impact of changing the activation function on training convergence in binary classification tasks and propose a novel activation function that leads to faster convergence for such tasks. The authors use Betti numbers to measure the topological complexity of data and report results from experiments using MLPs on synthetic binary classification datasets with large Betti numbers (>150). The proposed activation function reduces the Betti numbers faster across layers, leading to faster convergence requiring fewer epochs by a factor of 1.5 to 2. The authors also propose a new method for pruning a trained model, which involves removing filters that transform data into a topological space with large Betti numbers. They verify the methodology on benchmark image datasets using CNNs and report faster prediction time and reduced memory size of the model without significant accuracy reduction.",1
"Data-to-text (D2T) generation in the biomedical domain is a promising - yet mostly unexplored - field of research. Here, we apply neural models for D2T generation to a real-world dataset consisting of package leaflets of European medicines. We show that fine-tuned transformers are able to generate realistic, multisentence text from data in the biomedical domain, yet have important limitations. We also release a new dataset (BioLeaflets) for benchmarking D2T generation models in the biomedical domain.",0
"Research on data-to-text generation in the biomedical domain shows great promise, but has not been extensively studied. In this study, we utilize neural models to generate text from European medicine package leaflets, which demonstrates that fine-tuned transformers can produce realistic, multi-sentence text in the biomedical field. However, these models have some limitations. Additionally, we introduce a new dataset called BioLeaflets that can be used to evaluate the performance of data-to-text generation models in the biomedical domain.",1
"Text-to-image synthesis aims to generate a photo-realistic image from a given natural language description. Previous works have made significant progress with Generative Adversarial Networks (GANs). Nonetheless, it is still hard to generate intact objects or clear textures (Fig 1). To address this issue, we propose Feature-Aware Generative Adversarial Network (FA-GAN) to synthesize a high-quality image by integrating two techniques: a self-supervised discriminator and a feature-aware loss. First, we design a self-supervised discriminator with an auxiliary decoder so that the discriminator can extract better representation. Secondly, we introduce a feature-aware loss to provide the generator more direct supervision by employing the feature representation from the self-supervised discriminator. Experiments on the MS-COCO dataset show that our proposed method significantly advances the state-of-the-art FID score from 28.92 to 24.58.",0
"The objective of text-to-image synthesis is to create a realistic photograph based on a given natural language description. Prior research has utilized Generative Adversarial Networks (GANs) and made notable advancements. However, generating complete objects or clear textures remains challenging, as depicted in Figure 1. To tackle this problem, we suggest utilizing Feature-Aware Generative Adversarial Network (FA-GAN) that combines two techniques: a self-supervised discriminator and a feature-aware loss. Firstly, we develop a self-supervised discriminator with an auxiliary decoder to enhance the discriminator's representation. Secondly, we introduce a feature-aware loss to provide the generator with more direct supervision by using the feature representation from the self-supervised discriminator. Our experiments on the MS-COCO dataset demonstrate that our proposed technique significantly improves the state-of-the-art FID score, from 28.92 to 24.58.",1
"Deep neural networks usually require large labeled datasets for training to achieve state-of-the-art performance in many tasks, such as image classification and natural language processing. Although a lot of data is created each day by active Internet users, most of these data are unlabeled and are vulnerable to data poisoning attacks. In this paper, we develop an efficient active learning method that requires fewer labeled instances and incorporates the technique of adversarial retraining in which additional labeled artificial data are generated without increasing the budget of the labeling. The generated adversarial examples also provide a way to measure the vulnerability of the model. To check the performance of the proposed method under an adversarial setting, i.e., malicious mislabeling and data poisoning attacks, we perform an extensive evaluation on the reduced CIFAR-10 dataset, which contains only two classes: airplane and frog. Our experimental results demonstrate that the proposed active learning method is efficient for defending against malicious mislabeling and data poisoning attacks. Specifically, whereas the baseline active learning method based on the random sampling strategy performs poorly (about 50%) under a malicious mislabeling attack, the proposed active learning method can achieve the desired accuracy of 89% using only one-third of the dataset on average.",0
"To achieve exceptional results in tasks such as image classification and natural language processing, deep neural networks often require large labeled datasets for training. Unfortunately, most of the data created by active internet users daily are unlabeled and susceptible to data poisoning attacks. In this article, we introduce an active learning method that is efficient and requires fewer labeled instances. This method incorporates adversarial retraining, which generates additional labeled artificial data without increasing the labeling budget, and the generated adversarial examples help measure the model's vulnerability. To evaluate the proposed method's performance under malicious mislabeling and data poisoning attacks, we conduct a comprehensive evaluation on the reduced CIFAR-10 dataset, which includes only two classes: airplane and frog. Our experimental results reveal that the proposed active learning method is successful in defending against these attacks. While the baseline active learning method based on random sampling performs poorly (about 50%) under a malicious mislabeling attack, the proposed active learning method can achieve the desired accuracy of 89% using only one-third of the dataset on average.",1
"Vision Transformer (ViT) demonstrates that Transformer for natural language processing can be applied to computer vision tasks and result in comparable performance to convolutional neural networks (CNN), which have been studied and adopted in computer vision for years. This naturally raises the question of how the performance of ViT can be advanced with design techniques of CNN. To this end, we propose to incorporate two techniques and present ViT-ResNAS, an efficient multi-stage ViT architecture designed with neural architecture search (NAS). First, we propose residual spatial reduction to decrease sequence lengths for deeper layers and utilize a multi-stage architecture. When reducing lengths, we add skip connections to improve performance and stabilize training deeper networks. Second, we propose weight-sharing NAS with multi-architectural sampling. We enlarge a network and utilize its sub-networks to define a search space. A super-network covering all sub-networks is then trained for fast evaluation of their performance. To efficiently train the super-network, we propose to sample and train multiple sub-networks with one forward-backward pass. After that, evolutionary search is performed to discover high-performance network architectures. Experiments on ImageNet demonstrate that ViT-ResNAS achieves better accuracy-MACs and accuracy-throughput trade-offs than the original DeiT and other strong baselines of ViT. Code is available at https://github.com/yilunliao/vit-search.",0
"The Vision Transformer (ViT) has proven that the Transformer, which is commonly used for natural language processing, can also be applied to computer vision tasks and produce comparable results to the well-established convolutional neural networks (CNN) that have been used in computer vision for a long time. This achievement has led researchers to question how ViT's performance can be improved by incorporating CNN design techniques. In response to this, we introduce ViT-ResNAS, an efficient multi-stage ViT architecture designed with neural architecture search (NAS). Our proposed techniques include residual spatial reduction to decrease sequence lengths for deeper layers and utilize a multi-stage architecture, as well as weight-sharing NAS with multi-architectural sampling. We demonstrate through experiments on ImageNet that ViT-ResNAS outperforms the original DeiT and other strong ViT baselines in terms of accuracy-MACs and accuracy-throughput trade-offs. The code for ViT-ResNAS is available on our Github repository at https://github.com/yilunliao/vit-search.",1
"An agent that can understand natural-language instruction and carry out corresponding actions in the visual world is one of the long-term challenges of Artificial Intelligent (AI). Due to multifarious instructions from humans, it requires the agent can link natural language to vision and action in unstructured, previously unseen environments. If the instruction given by human is a navigation task, this challenge is called Visual-and-Language Navigation (VLN). It is a booming multi-disciplinary field of increasing importance and with extraordinary practicality. Instead of focusing on the details of specific methods, this paper provides a comprehensive survey on VLN tasks and makes a classification carefully according the different characteristics of language instructions in these tasks. According to when the instructions are given, the tasks can be divided into single-turn and multi-turn. For single-turn tasks, we further divided them into goal-orientation and route-orientation based on whether the instructions contain a route. For multi-turn tasks, we divided them into imperative task and interactive task based on whether the agent responses to the instructions. This taxonomy enable researchers to better grasp the key point of a specific task and identify directions for future research.",0
"Artificial Intelligence (AI) faces the challenge of creating an agent capable of comprehending natural-language commands and executing corresponding actions in an unstructured, visual environment. Visual-and-Language Navigation (VLN) is a rapidly growing, interdisciplinary field that addresses this challenge in navigation tasks. This paper presents a comprehensive survey of VLN tasks and classifies them based on their language-instruction characteristics. The tasks are divided into single-turn and multi-turn based on when instructions are given, further classified into goal-orientation and route-orientation for single-turn tasks, and imperative task and interactive task for multi-turn tasks based on agent response. This taxonomy facilitates researchers in understanding specific tasks and identifying future research directions, rather than focusing on specific methods.",1
"Deep learning approaches have produced substantial breakthroughs in fields such as image classification and natural language processing and are making rapid inroads in the area of protein design. Many generative models of proteins have been developed that encompass all known protein sequences, model specific protein families, or extrapolate the dynamics of individual proteins. Those generative models can learn protein representations that are often more informative of protein structure and function than hand-engineered features. Furthermore, they can be used to quickly propose millions of novel proteins that resemble the native counterparts in terms of expression level, stability, or other attributes. The protein design process can further be guided by discriminative oracles to select candidates with the highest probability of having the desired properties. In this review, we discuss five classes of generative models that have been most successful at modeling proteins and provide a framework for model guided protein design.",0
"The utilization of deep learning has resulted in significant advancements in areas like image classification and natural language processing, with its influence expanding into protein design. Numerous generative models of proteins have been developed, encompassing all known protein sequences, modeling specific protein families, or extrapolating the dynamics of individual proteins. These models have the capacity to learn protein representations that are more informative of protein structure and function than hand-engineered features. They can also swiftly suggest millions of novel proteins that resemble the native counterparts in terms of expression level, stability, or other attributes. Discriminative oracles can then be employed to guide the protein design process and select candidates with the highest probability of possessing the desired properties. In this review, we examine five classes of generative models that have proven to be the most successful at modeling proteins, and we provide a framework for model-guided protein design.",1
"Adversarial training has been proven to be a powerful regularization method to improve the generalization of models. However, current adversarial training methods only attack the original input sample or the embedding vectors, and their attacks lack coverage and diversity. To further enhance the breadth and depth of attack, we propose a novel masked weight adversarial training method called DropAttack, which enhances generalization of model by adding intentionally worst-case adversarial perturbations to both the input and hidden layers in different dimensions and minimize the adversarial risks generated by each layer. DropAttack is a general technique and can be adopt to a wide variety of neural networks with different architectures. To validate the effectiveness of the proposed method, we used five public datasets in the fields of natural language processing (NLP) and computer vision (CV) for experimental evaluating. We compare the proposed method with other adversarial training methods and regularization methods, and our method achieves state-of-the-art on all datasets. In addition, Dropattack can achieve the same performance when it use only a half training data compared to other standard training method. Theoretical analysis reveals that DropAttack can perform gradient regularization at random on some of the input and wight parameters of the model. Further visualization experiments show that DropAttack can push the minimum risk of the model to a lower and flatter loss landscapes. Our source code is publicly available on https://github.com/nishiwen1214/DropAttack.",0
"Adversarial training is a regularization technique that has been proven effective in improving model generalization. However, current methods only attack the original input or embedding vectors, resulting in limited coverage and diversity. To address this, we propose DropAttack, a new masked weight adversarial training method that adds intentionally worst-case perturbations to both input and hidden layers, minimizing adversarial risk in each layer. This approach is versatile and can be applied to various neural network architectures. We tested DropAttack on five public datasets from natural language processing and computer vision fields, achieving state-of-the-art performance compared to other adversarial and regularization methods. Furthermore, DropAttack can achieve comparable results using only half the training data. Theoretical analysis shows that DropAttack can conduct gradient regularization randomly on some input and weight parameters, while visualization experiments demonstrate its ability to push the model's minimum risk towards a lower and flatter loss landscape. Our source code is available at https://github.com/nishiwen1214/DropAttack.",1
"Multimodal sentiment analysis aims to recognize people's attitudes from multiple communication channels such as verbal content (i.e., text), voice, and facial expressions. It has become a vibrant and important research topic in natural language processing. Much research focuses on modeling the complex intra- and inter-modal interactions between different communication channels. However, current multimodal models with strong performance are often deep-learning-based techniques and work like black boxes. It is not clear how models utilize multimodal information for sentiment predictions. Despite recent advances in techniques for enhancing the explainability of machine learning models, they often target unimodal scenarios (e.g., images, sentences), and little research has been done on explaining multimodal models. In this paper, we present an interactive visual analytics system, M2Lens, to visualize and explain multimodal models for sentiment analysis. M2Lens provides explanations on intra- and inter-modal interactions at the global, subset, and local levels. Specifically, it summarizes the influence of three typical interaction types (i.e., dominance, complement, and conflict) on the model predictions. Moreover, M2Lens identifies frequent and influential multimodal features and supports the multi-faceted exploration of model behaviors from language, acoustic, and visual modalities. Through two case studies and expert interviews, we demonstrate our system can help users gain deep insights into the multimodal models for sentiment analysis.",0
"The objective of multimodal sentiment analysis is to identify individuals' attitudes through various communication channels such as text, voice, and facial expressions. This has become a crucial area of research in natural language processing, with much focus on the intricate intra- and inter-modal interactions between diverse communication channels. However, current high-performance multimodal models are mostly deep-learning-based, and their use of multimodal information for sentiment predictions is unclear. Despite recent advancements in enhancing the explainability of machine learning models, little attention has been given to explaining multimodal models. In this paper, we introduce M2Lens, an interactive visual analytics system that provides explanations on intra- and inter-modal interactions at global, subset, and local levels. It summarizes the impact of three typical interaction types on model predictions and identifies influential multimodal features while supporting multi-faceted exploration of model behaviors. Through two case studies and expert interviews, we demonstrate how our system can help users gain profound insights into multimodal models for sentiment analysis.",1
"Deep neural network (DNN) model compression for efficient on-device inference is becoming increasingly important to reduce memory requirements and keep user data on-device. To this end, we propose a novel differentiable k-means clustering layer (DKM) and its application to train-time weight clustering-based DNN model compression. DKM casts k-means clustering as an attention problem and enables joint optimization of the parameters and clustering centroids. Unlike prior works that rely on additional regularizers and parameters, DKM-based compression keeps the original loss function and model architecture fixed. We evaluated DKM-based compression on various DNN models for computer vision and natural language processing (NLP) tasks. Our results demonstrate that DMK delivers superior compression and accuracy trade-off on ImageNet1k and GLUE benchmarks. For example, DKM-based compression can offer 74.5% top-1 ImageNet1k accuracy on ResNet50 DNN model with 3.3MB model size (29.4x model compression factor). For MobileNet-v1, which is a challenging DNN to compress, DKM delivers 62.8% top-1 ImageNet1k accuracy with 0.74 MB model size (22.4x model compression factor). This result is 6.8% higher top-1 accuracy and 33% relatively smaller model size than the current state-of-the-art DNN compression algorithms. Additionally, DKM enables compression of DistilBERT model by 11.8x with minimal (1.1%) accuracy loss on GLUE NLP benchmarks.",0
"Efficient on-device inference is becoming increasingly important, and Deep Neural Network (DNN) model compression is a promising approach to reduce memory requirements and keep user data on-device. In this regard, we propose a novel differentiable k-means clustering layer (DKM) for DNN model compression at train-time. DKM treats k-means clustering as an attention problem and allows for joint optimization of the parameters and clustering centroids. It is distinct from prior works that rely on additional regularizers and parameters, as it retains the original loss function and model architecture. We evaluated DKM-based compression on various DNN models for computer vision and natural language processing (NLP) tasks. Our results show that DKM achieves superior compression and accuracy trade-off on ImageNet1k and GLUE benchmarks. For instance, DKM-based compression can offer a 74.5% top-1 ImageNet1k accuracy on ResNet50 DNN model with a 3.3MB model size (29.4x model compression factor). DKM also delivers 62.8% top-1 ImageNet1k accuracy with a 0.74 MB model size (22.4x model compression factor) for challenging DNN models like MobileNet-v1. Moreover, DKM enables compression of DistilBERT model by 11.8x with minimal (1.1%) accuracy loss on GLUE NLP benchmarks.",1
"Spatio-temporal representational learning has been widely adopted in various fields such as action recognition, video object segmentation, and action anticipation. Previous spatio-temporal representational learning approaches primarily employ ConvNets or sequential models,e.g., LSTM, to learn the intra-frame and inter-frame features. Recently, Transformer models have successfully dominated the study of natural language processing (NLP), image classification, etc. However, the pure-Transformer based spatio-temporal learning can be prohibitively costly on memory and computation to extract fine-grained features from a tiny patch. To tackle the training difficulty and enhance the spatio-temporal learning, we construct a shifted chunk Transformer with pure self-attention blocks. Leveraging the recent efficient Transformer design in NLP, this shifted chunk Transformer can learn hierarchical spatio-temporal features from a local tiny patch to a global video clip. Our shifted self-attention can also effectively model complicated inter-frame variances. Furthermore, we build a clip encoder based on Transformer to model long-term temporal dependencies. We conduct thorough ablation studies to validate each component and hyper-parameters in our shifted chunk Transformer, and it outperforms previous state-of-the-art approaches on Kinetics-400, Kinetics-600, UCF101, and HMDB51. Code and trained models will be released.",0
"The use of spatio-temporal representational learning is prevalent in several fields, including action recognition, video object segmentation, and action anticipation. In the past, ConvNets and sequential models (e.g., LSTM) were primarily used to learn intra-frame and inter-frame features. However, Transformer models have recently become the dominant approach in natural language processing and image classification. Nonetheless, using a pure Transformer for spatio-temporal learning can be expensive in terms of memory and computation, making it difficult to extract fine-grained features from a small patch. To address this challenge, we developed a shifted chunk Transformer with pure self-attention blocks that can learn hierarchical spatio-temporal features from a local tiny patch to a global video clip. Our shifted self-attention can also model complex inter-frame variances, and we use a clip encoder based on Transformer to model long-term temporal dependencies. We conducted thorough ablation studies to validate each component and hyper-parameter of our shifted chunk Transformer, which outperformed previous state-of-the-art approaches on Kinetics-400, Kinetics-600, UCF101, and HMDB51. We will make our code and trained models available.",1
"We introduce MADGRAD, a novel optimization method in the family of AdaGrad adaptive gradient methods. MADGRAD shows excellent performance on deep learning optimization problems from multiple fields, including classification and image-to-image tasks in vision, and recurrent and bidirectionally-masked models in natural language processing. For each of these tasks, MADGRAD matches or outperforms both SGD and ADAM in test set performance, even on problems for which adaptive methods normally perform poorly.",0
"MADGRAD is a new optimization technique that belongs to the family of AdaGrad adaptive gradient methods. It has demonstrated exceptional performance on deep learning optimization problems across various domains, such as vision tasks like classification and image-to-image, and natural language processing tasks like recurrent and bidirectionally-masked models. In all these tasks, MADGRAD has either matched or outperformed both SGD and ADAM in test set performance, even on problems that adaptive methods typically struggle with.",1
"Transformers have shown impressive performance in various natural language processing and computer vision tasks, due to the capability of modeling long-range dependencies. Recent progress has demonstrated to combine such transformers with CNN-based semantic image segmentation models is very promising. However, it is not well studied yet on how well a pure transformer based approach can achieve for image segmentation. In this work, we explore a novel framework for semantic image segmentation, which is encoder-decoder based Fully Transformer Networks (FTN). Specifically, we first propose a Pyramid Group Transformer (PGT) as the encoder for progressively learning hierarchical features, while reducing the computation complexity of the standard visual transformer(ViT). Then, we propose a Feature Pyramid Transformer (FPT) to fuse semantic-level and spatial-level information from multiple levels of the PGT encoder for semantic image segmentation. Surprisingly, this simple baseline can achieve new state-of-the-art results on multiple challenging semantic segmentation benchmarks, including PASCAL Context, ADE20K and COCO-Stuff. The source code will be released upon the publication of this work.",0
"Transformers have proven to be highly effective in a range of natural language processing and computer vision tasks, owing to their ability to model long-range dependencies. Recent advances have shown that combining transformers and CNN-based semantic image segmentation models holds great promise. However, there is still limited research on the potential of a purely transformer-based approach for image segmentation. In this study, we propose a new framework for semantic image segmentation called Fully Transformer Networks (FTN), which employs an encoder-decoder approach. Specifically, we introduce a Pyramid Group Transformer (PGT) as the encoder to learn hierarchical features progressively while minimizing the computation complexity of the standard visual transformer (ViT). Additionally, we propose a Feature Pyramid Transformer (FPT) to merge semantic-level and spatial-level information from multiple levels of the PGT encoder for semantic image segmentation. Remarkably, this uncomplicated method has achieved state-of-the-art results on various challenging semantic segmentation benchmarks, including PASCAL Context, ADE20K, and COCO-Stuff. The source code will be made public upon publication of this study.",1
"Web Image Context Extraction (WICE) consists in obtaining the textual information describing an image using the content of the surrounding webpage. A common preprocessing step before performing WICE is to render the content of the webpage. When done at a large scale (e.g., for search engine indexation), it may become very computationally costly (up to several seconds per page). To avoid this cost, we introduce a novel WICE approach that combines Graph Neural Networks (GNNs) and Natural Language Processing models. Our method relies on a graph model containing both node types and text as features. The model is fed through several blocks of GNNs to extract the textual context. Since no labeled WICE dataset with ground truth exists, we train and evaluate the GNNs on a proxy task that consists in finding the semantically closest text to the image caption. We then interpret importance weights to find the most relevant text nodes and define them as the image context. Thanks to GNNs, our model is able to encode both structural and semantic information from the webpage. We show that our approach gives promising results to help address the large-scale WICE problem using only HTML data.",0
"WICE, which involves obtaining textual information that describes an image by using the content of the surrounding webpage, typically requires rendering the webpage as a preprocessing step. However, this can be computationally expensive when done on a large scale. To address this issue, we propose a new approach that combines Graph Neural Networks (GNNs) and Natural Language Processing models. Our method utilizes a graph model that includes node types and text as features, and passes through multiple GNN blocks to extract the textual context. As there is no labeled WICE dataset with ground truth, we train and evaluate the GNNs on a proxy task that involves finding the most semantically similar text to the image caption. We then use importance weights to identify the most relevant text nodes and define them as the image context. Our model is capable of encoding both structural and semantic information from the webpage, and shows promising results for addressing large-scale WICE problems using HTML data alone.",1
"There is an increasing demand for scalable algorithms capable of clustering and analyzing large time series datasets. The Kohonen self-organizing map (SOM) is a type of unsupervised artificial neural network for visualizing and clustering complex data, reducing the dimensionality of data, and selecting influential features. Like all clustering methods, the SOM requires a measure of similarity between input data (in this work time series). Dynamic time warping (DTW) is one such measure, and a top performer given that it accommodates the distortions when aligning time series. Despite its use in clustering, DTW is limited in practice because it is quadratic in runtime complexity with the length of the time series data. To address this, we present a new DTW-based clustering method, called SOMTimeS (a Self-Organizing Map for TIME Series), that scales better and runs faster than other DTW-based clustering algorithms, and has similar performance accuracy. The computational performance of SOMTimeS stems from its ability to prune unnecessary DTW computations during the SOM's training phase. We also implemented a similar pruning strategy for K-means for comparison with one of the top performing clustering algorithms. We evaluated the pruning effectiveness, accuracy, execution time and scalability on 112 benchmark time series datasets from the University of California, Riverside classification archive. We showed that for similar accuracy, the speed-up achieved for SOMTimeS and K-means was 1.8x on average; however, rates varied between 1x and 18x depending on the dataset. SOMTimeS and K-means pruned 43% and 50% of the total DTW computations, respectively. We applied SOMtimeS to natural language conversation data collected as part of a large healthcare cohort study of patient-clinician serious illness conversations to demonstrate the algorithm's utility with complex, temporally sequenced phenomena.",0
"The demand for scalable algorithms that can cluster and analyze large time series datasets is on the rise. The Kohonen self-organizing map (SOM) is an unsupervised artificial neural network that can visualize and cluster complex data, reduce dimensionality, and select influential features. Similar to other clustering methods, SOM also requires a measure of similarity between input data, such as time series. One such measure is dynamic time warping (DTW), which accommodates distortions when aligning time series. However, DTW has a quadratic runtime complexity with the length of the time series data, limiting its practical use. To address this, we introduce a new DTW-based clustering method, SOMTimeS, that scales better and runs faster than other DTW-based clustering algorithms while maintaining similar performance accuracy. SOMTimeS prunes unnecessary DTW computations during the SOM's training phase, resulting in improved computational performance. We also implemented a similar pruning strategy for K-means and evaluated the pruning effectiveness, accuracy, execution time, and scalability on 112 benchmark time series datasets. The results showed that SOMTimeS and K-means pruned 43% and 50% of the total DTW computations, respectively, achieving a speed-up of 1.8x on average for similar accuracy. This speed-up varied between 1x and 18x depending on the dataset. Finally, we demonstrated the utility of SOMTimeS with complex, temporally sequenced natural language conversation data collected from a large healthcare cohort study of patient-clinician serious illness conversations.",1
"Interaction and navigation defined by natural language instructions in dynamic environments pose significant challenges for neural agents. This paper focuses on addressing two challenges: handling long sequence of subtasks, and understanding complex human instructions. We propose Episodic Transformer (E.T.), a multimodal transformer that encodes language inputs and the full episode history of visual observations and actions. To improve training, we leverage synthetic instructions as an intermediate representation that decouples understanding the visual appearance of an environment from the variations of natural language instructions. We demonstrate that encoding the history with a transformer is critical to solve compositional tasks, and that pretraining and joint training with synthetic instructions further improve the performance. Our approach sets a new state of the art on the challenging ALFRED benchmark, achieving 38.4% and 8.5% task success rates on seen and unseen test splits.",0
"Neural agents face significant obstacles when it comes to interacting and navigating in dynamic environments through natural language instructions. This research paper concentrates on addressing two of these challenges: managing lengthy sequences of subtasks and comprehending intricate human instructions. Our proposed solution is the Episodic Transformer (E.T.), a multimodal transformer that encodes language inputs and the complete history of visual observations and actions. To enhance the training process, we use synthetic instructions as an intermediate representation, which separates the understanding of the visual environment from variations in natural language instructions. Through our experiments, we have discovered that encoding history with a transformer is crucial for solving compositional tasks, and pretraining and joint training with synthetic instructions lead to improved performance. Our approach achieves a new state-of-the-art on the ALFRED benchmark, with task success rates of 38.4% and 8.5% on seen and unseen test splits, respectively.",1
"We investigate Referring Image Segmentation (RIS), which outputs a segmentation map corresponding to the given natural language description. To solve RIS efficiently, we need to understand each word's relationship with other words, each region in the image to other regions, and cross-modal alignment between linguistic and visual domains. We argue that one of the limiting factors in the recent methods is that they do not handle these interactions simultaneously. To this end, we propose a novel architecture called JRNet, which uses a Joint Reasoning Module(JRM) to concurrently capture the inter-modal and intra-modal interactions. The output of JRM is passed through a novel Cross-Modal Multi-Level Fusion (CMMLF) module which further refines the segmentation masks by exchanging contextual information across visual hierarchy through linguistic features acting as a bridge. We present thorough ablation studies and validate our approach's performance on four benchmark datasets, showing considerable performance gains over the existing state-of-the-art methods.",0
"Our research focuses on Referring Image Segmentation (RIS), a process that generates a segmentation map based on a natural language description. To effectively address RIS, it is essential to comprehend the relationships between words, regions within the image, and the alignment between linguistic and visual domains. Previous methods have not adequately handled these interactions simultaneously, limiting their efficacy. To overcome this issue, we propose a new model called JRNet, which employs a Joint Reasoning Module (JRM) to capture inter-modal and intra-modal interactions concurrently. Our Cross-Modal Multi-Level Fusion (CMMLF) module enhances the segmentation masks by exchanging contextual information across visual hierarchy through linguistic features. We validate our approach through extensive experimentation on four benchmark datasets, demonstrating significant performance improvement over existing state-of-the-art methods.",1
"Activation functions play a pivotal role in determining the training dynamics and neural network performance. The widely adopted activation function ReLU despite being simple and effective has few disadvantages including the Dying ReLU problem. In order to tackle such problems, we propose a novel activation function called Serf which is self-regularized and nonmonotonic in nature. Like Mish, Serf also belongs to the Swish family of functions. Based on several experiments on computer vision (image classification and object detection) and natural language processing (machine translation, sentiment classification and multimodal entailment) tasks with different state-of-the-art architectures, it is observed that Serf vastly outperforms ReLU (baseline) and other activation functions including both Swish and Mish, with a markedly bigger margin on deeper architectures. Ablation studies further demonstrate that Serf based architectures perform better than those of Swish and Mish in varying scenarios, validating the effectiveness and compatibility of Serf with varying depth, complexity, optimizers, learning rates, batch sizes, initializers and dropout rates. Finally, we investigate the mathematical relation between Swish and Serf, thereby showing the impact of preconditioner function ingrained in the first derivative of Serf which provides a regularization effect making gradients smoother and optimization faster.",0
"The role of activation functions is crucial in determining neural network performance and training dynamics. Although ReLU is a widely used and effective activation function, it has drawbacks including the Dying ReLU problem. To address these limitations, we introduce a new activation function called Serf. Serf is nonmonotonic and self-regularized and belongs to the Swish family of functions, like Mish. Through experiments on image classification, object detection, machine translation, sentiment classification, and multimodal entailment tasks using various state-of-the-art architectures, we found that Serf significantly outperforms ReLU and other activation functions, especially in deeper architectures. Ablation studies demonstrate that Serf-based architectures outperform those of Swish and Mish in different scenarios, confirming the compatibility and effectiveness of Serf with varying parameters. Furthermore, we investigate the mathematical relationship between Swish and Serf, revealing the impact of the preconditioner function's first derivative on Serf's regularization effect, leading to smoother gradients and faster optimization.",1
"Most existing image retrieval systems use text queries as a way for the user to express what they are looking for. However, fine-grained image retrieval often requires the ability to also express where in the image the content they are looking for is. The text modality can only cumbersomely express such localization preferences, whereas pointing is a more natural fit. In this paper, we propose an image retrieval setup with a new form of multimodal queries, where the user simultaneously uses both spoken natural language (the what) and mouse traces over an empty canvas (the where) to express the characteristics of the desired target image. We then describe simple modifications to an existing image retrieval model, enabling it to operate in this setup. Qualitative and quantitative experiments show that our model effectively takes this spatial guidance into account, and provides significantly more accurate retrieval results compared to text-only equivalent systems.",0
"The majority of current image retrieval systems utilize text-based queries as a means for users to specify their desired content. However, for precise image retrieval, it is often necessary to indicate the location of the desired content within the image. This is a difficult task using textual input, whereas using pointing gestures is more intuitive. In this article, we propose a novel image retrieval approach that incorporates a multimodal query system. This system combines speech and mouse tracing on a blank canvas to specify both the content and location of the desired target image. We also outline straightforward adjustments to an existing image retrieval model, allowing it to function within this framework. Our experiments, both qualitative and quantitative, demonstrate that our model effectively incorporates spatial guidance, resulting in more accurate retrieval outcomes compared to systems that rely solely on text-based queries.",1
"Facial action unit (FAU) intensities are popular descriptors for the analysis of facial behavior. However, FAUs are sparsely represented when only a few are activated at a time. In this study, we explore the possibility of representing the dynamics of facial expressions by adopting algorithms used for word representation in natural language processing. Specifically, we perform clustering on a large dataset of temporal facial expressions with 5.3M frames before applying the Global Vector representation (GloVe) algorithm to learn the embeddings of the facial clusters. We evaluate the usefulness of our learned representations on two downstream tasks: schizophrenia symptom estimation and depression severity regression. These experimental results show the potential effectiveness of our approach for improving the assessment of mental health symptoms over baseline models that use FAU intensities alone.",0
"The use of Facial Action Unit (FAU) intensities is a popular way to analyze facial behavior, but it is limited when only a few FAUs are activated at a time. This study aims to explore a new way of representing the dynamics of facial expressions by using algorithms from natural language processing that cluster a large dataset of temporal facial expressions with 5.3M frames. We then apply the Global Vector representation (GloVe) algorithm to learn the embeddings of the facial clusters. Our study evaluates the effectiveness of these learned representations on two downstream tasks: schizophrenia symptom estimation and depression severity regression. Our experimental results demonstrate the potential usefulness of our approach in improving the assessment of mental health symptoms, surpassing baseline models that use FAU intensities alone.",1
"Infographics are documents designed to effectively communicate information using a combination of textual, graphical and visual elements. In this work, we explore the automatic understanding of infographic images by using Visual Question Answering technique.To this end, we present InfographicVQA, a new dataset that comprises a diverse collection of infographics along with natural language questions and answers annotations. The collected questions require methods to jointly reason over the document layout, textual content, graphical elements, and data visualizations. We curate the dataset with emphasis on questions that require elementary reasoning and basic arithmetic skills. Finally, we evaluate two strong baselines based on state of the art multi-modal VQA models, and establish baseline performance for the new task. The dataset, code and leaderboard will be made available at http://docvqa.org",0
"The aim of our work is to investigate the automated comprehension of infographic images through the application of Visual Question Answering techniques. Infographics are documents that employ a combination of textual, graphical and visual elements to effectively convey information. To achieve this goal, we introduce InfographicVQA, a novel dataset that consists of a varied collection of infographics with natural language questions and answer annotations. The questions we collect require methods that can reason across the document layout, textual content, graphical components, and data visualizations. Our focus is on elementary reasoning and basic arithmetic skills. Finally, we use strong baselines based on state-of-the-art multi-modal VQA models to evaluate the dataset and establish baseline performance for the new task. The dataset, code, and leaderboard can be accessed at http://docvqa.org.",1
"Transformer architectures have become the model of choice in natural language processing and are now being introduced into computer vision tasks such as image classification, object detection, and semantic segmentation. However, in the field of human pose estimation, convolutional architectures still remain dominant. In this work, we present PoseFormer, a purely transformer-based approach for 3D human pose estimation in videos without convolutional architectures involved. Inspired by recent developments in vision transformers, we design a spatial-temporal transformer structure to comprehensively model the human joint relations within each frame as well as the temporal correlations across frames, then output an accurate 3D human pose of the center frame. We quantitatively and qualitatively evaluate our method on two popular and standard benchmark datasets: Human3.6M and MPI-INF-3DHP. Extensive experiments show that PoseFormer achieves state-of-the-art performance on both datasets. Code is available at \url{https://github.com/zczcwh/PoseFormer}",0
"Transformer models have gained popularity in natural language processing and are now being incorporated into computer vision tasks such as image classification, object detection, and semantic segmentation. However, convolutional architectures still dominate in the field of human pose estimation. In this study, we introduce PoseFormer, a transformer-based approach for 3D human pose estimation in videos that does not involve convolutional architectures. We design a spatial-temporal transformer structure inspired by recent developments in vision transformers to model human joint relations within each frame and temporal correlations across frames, resulting in an accurate 3D human pose estimation for the center frame. We evaluate our method on two standard benchmark datasets, Human3.6M and MPI-INF-3DHP, and demonstrate state-of-the-art performance. Code is available at \url{https://github.com/zczcwh/PoseFormer}.",1
"The concern regarding users' data privacy has risen to its highest level due to the massive increase in communication platforms, social networking sites, and greater users' participation in online public discourse. An increasing number of people exchange private information via emails, text messages, and social media without being aware of the risks and implications. Researchers in the field of Natural Language Processing (NLP) have concentrated on creating tools and strategies to identify, categorize, and sanitize private information in text data since a substantial amount of data is exchanged in textual form. However, most of the detection methods solely rely on the existence of pre-identified keywords in the text and disregard the inference of the underlying meaning of the utterance in a specific context. Hence, in some situations, these tools and algorithms fail to detect disclosure, or the produced results are miss-classified. In this paper, we propose a multi-input, multi-output hybrid neural network which utilizes transfer-learning, linguistics, and metadata to learn the hidden patterns. Our goal is to better classify disclosure/non-disclosure content in terms of the context of situation. We trained and evaluated our model on a human-annotated ground truth dataset, containing a total of 5,400 tweets. The results show that the proposed model was able to identify privacy disclosure through tweets with an accuracy of 77.4% while classifying the information type of those tweets with an impressive accuracy of 99%, by jointly learning for two separate tasks.",0
"Due to the proliferation of communication platforms and social networking sites, as well as increased participation in online public discourse, concerns about user data privacy have reached an all-time high. Many individuals unknowingly share private information through emails, text messages, and social media, putting themselves at risk. Researchers in Natural Language Processing (NLP) have focused on developing tools to identify and sanitize private information in text data. However, existing detection methods rely solely on pre-identified keywords, often overlooking the underlying meaning in context and leading to misclassification of data. To address this, we propose a multi-input, multi-output hybrid neural network that incorporates transfer-learning, linguistics, and metadata to learn hidden patterns and better classify disclosure/non-disclosure content based on context. Our model was trained on a human-annotated dataset of 5,400 tweets and achieved an accuracy of 77.4% in identifying privacy disclosure and 99% in classifying information type by jointly learning for two separate tasks.",1
"Describing images using natural language is widely known as image captioning, which has made consistent progress due to the development of computer vision and natural language generation techniques. Though conventional captioning models achieve high accuracy based on popular metrics, i.e., BLEU, CIDEr, and SPICE, the ability of captions to distinguish the target image from other similar images is under-explored. To generate distinctive captions, a few pioneers employ contrastive learning or re-weighted the ground-truth captions, which focuses on one single input image. However, the relationships between objects in a similar image group (e.g., items or properties within the same album or fine-grained events) are neglected. In this paper, we improve the distinctiveness of image captions using a Group-based Distinctive Captioning Model (GdisCap), which compares each image with other images in one similar group and highlights the uniqueness of each image. In particular, we propose a group-based memory attention (GMA) module, which stores object features that are unique among the image group (i.e., with low similarity to objects in other images). These unique object features are highlighted when generating captions, resulting in more distinctive captions. Furthermore, the distinctive words in the ground-truth captions are selected to supervise the language decoder and GMA. Finally, we propose a new evaluation metric, distinctive word rate (DisWordRate) to measure the distinctiveness of captions. Quantitative results indicate that the proposed method significantly improves the distinctiveness of several baseline models, and achieves the state-of-the-art performance on both accuracy and distinctiveness. Results of a user study agree with the quantitative evaluation and demonstrate the rationality of the new metric DisWordRate.",0
"The process of describing images using natural language is widely known as image captioning. This field has made significant progress due to the development of computer vision and natural language generation techniques. However, while conventional captioning models achieve high accuracy based on popular metrics such as BLEU, CIDEr, and SPICE, their ability to differentiate between target images and similar ones has not been fully explored. While some pioneers have used contrastive learning or re-weighted ground-truth captions to generate distinctive captions, these approaches only focus on one input image and ignore the relationships between objects in similar image groups. To address this gap, we propose a Group-based Distinctive Captioning Model (GdisCap) that compares each image with other images in a similar group and highlights their uniqueness. We introduce a group-based memory attention (GMA) module to store object features that are unique among the image group, which are then highlighted when generating captions. We also propose a new evaluation metric, distinctive word rate (DisWordRate), to measure the distinctiveness of captions. Our quantitative and user study results confirm that the proposed method significantly improves the distinctiveness of several baseline models and achieves state-of-the-art performance on both accuracy and distinctiveness.",1
"Vision-and-language navigation (VLN) aims to enable embodied agents to navigate in realistic environments using natural language instructions. Given the scarcity of domain-specific training data and the high diversity of image and language inputs, the generalization of VLN agents to unseen environments remains challenging. Recent methods explore pretraining to improve generalization, however, the use of generic image-caption datasets or existing small-scale VLN environments is suboptimal and results in limited improvements. In this work, we introduce BnB, a large-scale and diverse in-domain VLN dataset. We first collect image-caption (IC) pairs from hundreds of thousands of listings from online rental marketplaces. Using IC pairs we next propose automatic strategies to generate millions of VLN path-instruction (PI) pairs. We further propose a shuffling loss that improves the learning of temporal order inside PI pairs. We use BnB pretrain our Airbert model that can be adapted to discriminative and generative settings and show that it outperforms state of the art for Room-to-Room (R2R) navigation and Remote Referring Expression (REVERIE) benchmarks. Moreover, our in-domain pretraining significantly increases performance on a challenging few-shot VLN evaluation, where we train the model only on VLN instructions from a few houses.",0
"The goal of Vision-and-language navigation (VLN) is to allow physical agents to navigate through realistic environments using natural language commands. However, due to the lack of specific training data and the variety of language and image inputs, it is difficult for VLN agents to adapt to new environments. Previous methods have attempted to address this issue through pretraining, but using generic image-caption datasets or small-scale VLN environments has resulted in limited success. This paper introduces BnB, a large and diverse VLN dataset created by collecting image-caption pairs from online rental marketplaces and using automatic strategies to generate path-instruction pairs. The authors propose a shuffling loss to improve the learning of temporal order within these pairs. Using BnB data, the Airbert model is pretrained and can be adapted to both discriminative and generative settings, outperforming state-of-the-art benchmarks in Room-to-Room navigation and Remote Referring Expression tasks. Additionally, the in-domain pretraining improves performance on a challenging few-shot VLN evaluation.",1
"Neural networks are well-known to be vulnerable to imperceptible perturbations in the input, called adversarial examples, that result in misclassification. Generating adversarial examples for source code poses an additional challenge compared to the domains of images and natural language, because source code perturbations must retain the functional meaning of the code. We identify a striking relationship between token frequency statistics and learned token embeddings: the L2 norm of learned token embeddings increases with the frequency of the token except for the highest-frequnecy tokens. We leverage this relationship to construct a simple and efficient gradient-free method for generating state-of-the-art adversarial examples on models of code. Our method empirically outperforms competing gradient-based methods with less information and less computational effort.",0
"Adversarial examples, imperceptible perturbations in the input that cause misclassification, are a known vulnerability of neural networks. However, generating adversarial examples for source code is more challenging than for images or natural language because the perturbations must preserve the code's functional meaning. We discovered that the L2 norm of learned token embeddings increases with the token's frequency, except for the highest-frequency tokens, and used this relationship to develop a gradient-free method for generating advanced adversarial examples on code models. Our method outperforms competing gradient-based methods with less information and computational effort.",1
"Recent deep-learning models have achieved impressive predictive performance by learning complex functions of many variables, often at the cost of interpretability. This chapter covers recent work aiming to interpret models by attributing importance to features and feature groups for a single prediction. Importantly, the proposed attributions assign importance to interactions between features, in addition to features in isolation. These attributions are shown to yield insights across real-world domains, including bio-imaging, cosmology image and natural-language processing. We then show how these attributions can be used to directly improve the generalization of a neural network or to distill it into a simple model. Throughout the chapter, we emphasize the use of reality checks to scrutinize the proposed interpretation techniques.",0
"In recent times, deep-learning models have exhibited impressive predictive capabilities by learning intricate functions of numerous variables, albeit at the expense of interpretability. This section delves into the latest research endeavors that aim to interpret models by assigning significance to features and feature groups for a single prediction. It is noteworthy that these attributions accord importance to interactions between features and not just isolated features. Such attributions have been observed to offer insights in various practical fields, including bio-imaging, cosmology image, and natural-language processing. Additionally, we demonstrate how these attributions can enhance the generalization of a neural network or simplify it into a basic model. The chapter underscores the importance of reality checks to scrutinize the proposed interpretation techniques.",1
"Recently, there has been an increasing number of efforts to introduce models capable of generating natural language explanations (NLEs) for their predictions on vision-language (VL) tasks. Such models are appealing, because they can provide human-friendly and comprehensive explanations. However, there is a lack of comparison between existing methods, which is due to a lack of re-usable evaluation frameworks and a scarcity of datasets. In this work, we introduce e-ViL and e-SNLI-VE. e-ViL is a benchmark for explainable vision-language tasks that establishes a unified evaluation framework and provides the first comprehensive comparison of existing approaches that generate NLEs for VL tasks. It spans four models and three datasets and both automatic metrics and human evaluation are used to assess model-generated explanations. e-SNLI-VE is currently the largest existing VL dataset with NLEs (over 430k instances). We also propose a new model that combines UNITER, which learns joint embeddings of images and text, and GPT-2, a pre-trained language model that is well-suited for text generation. It surpasses the previous state of the art by a large margin across all datasets. Code and data are available here: https://github.com/maximek3/e-ViL.",0
"In recent times, there has been a growing interest in the development of models that can generate natural language explanations (NLEs) for their predictions on vision-language (VL) tasks. These models are attractive because they can provide explanations that are easy for humans to understand and offer comprehensive insights. However, the lack of a common evaluation framework and limited datasets have made it difficult to compare existing methods. To address this issue, we present e-ViL and e-SNLI-VE. e-ViL is a standardized benchmark for explainable vision-language tasks that provides a comprehensive comparison of NLE-generating approaches across four models and three datasets. Both automatic metrics and human evaluation are used to assess the quality of model-generated explanations. e-SNLI-VE is currently the largest VL dataset with NLEs, comprising over 430k instances. We also introduce a novel model that combines UNITER and GPT-2 to generate superior results compared to previous state-of-the-art models across all datasets. Our code and data are available at https://github.com/maximek3/e-ViL.",1
"We propose UniT, a Unified Transformer model to simultaneously learn the most prominent tasks across different domains, ranging from object detection to natural language understanding and multimodal reasoning. Based on the transformer encoder-decoder architecture, our UniT model encodes each input modality with an encoder and makes predictions on each task with a shared decoder over the encoded input representations, followed by task-specific output heads. The entire model is jointly trained end-to-end with losses from each task. Compared to previous efforts on multi-task learning with transformers, we share the same model parameters across all tasks instead of separately fine-tuning task-specific models and handle a much higher variety of tasks across different domains. In our experiments, we learn 7 tasks jointly over 8 datasets, achieving strong performance on each task with significantly fewer parameters. Our code is available in MMF at https://mmf.sh.",0
"Our proposal, UniT, is a Unified Transformer model that can learn various tasks across different domains all at once. This includes tasks such as object detection, natural language understanding, and multimodal reasoning. With our UniT model, input modalities are encoded using an encoder and predictions are made on each task using a shared decoder over the encoded input representations, followed by task-specific output heads. Unlike previous efforts on multi-task learning with transformers, we share the same model parameters across all tasks instead of separately fine-tuning task-specific models. We handle a much wider variety of tasks across different domains. The entire model is jointly trained end-to-end with losses from each task. In our experiments, we learned 7 tasks simultaneously over 8 datasets, achieving strong performance on each task with significantly fewer parameters. Our code is available in MMF at https://mmf.sh.",1
"For many fundamental scene understanding tasks, it is difficult or impossible to obtain per-pixel ground truth labels from real images. We address this challenge by introducing Hypersim, a photorealistic synthetic dataset for holistic indoor scene understanding. To create our dataset, we leverage a large repository of synthetic scenes created by professional artists, and we generate 77,400 images of 461 indoor scenes with detailed per-pixel labels and corresponding ground truth geometry. Our dataset: (1) relies exclusively on publicly available 3D assets; (2) includes complete scene geometry, material information, and lighting information for every scene; (3) includes dense per-pixel semantic instance segmentations and complete camera information for every image; and (4) factors every image into diffuse reflectance, diffuse illumination, and a non-diffuse residual term that captures view-dependent lighting effects.   We analyze our dataset at the level of scenes, objects, and pixels, and we analyze costs in terms of money, computation time, and annotation effort. Remarkably, we find that it is possible to generate our entire dataset from scratch, for roughly half the cost of training a popular open-source natural language processing model. We also evaluate sim-to-real transfer performance on two real-world scene understanding tasks - semantic segmentation and 3D shape prediction - where we find that pre-training on our dataset significantly improves performance on both tasks, and achieves state-of-the-art performance on the most challenging Pix3D test set. All of our rendered image data, as well as all the code we used to generate our dataset and perform our experiments, is available online.",0
"The task of obtaining per-pixel ground truth labels from real images for fundamental scene understanding tasks is a challenging and sometimes impossible feat. To overcome this difficulty, we have developed Hypersim, a synthetic dataset for indoor scene understanding that is photorealistic. Our dataset includes 77,400 images of 461 indoor scenes with detailed per-pixel labels and ground truth geometry generated from a large repository of synthetic scenes created by professional artists. The dataset is created using publicly available 3D assets and includes complete scene geometry, material and lighting information, per-pixel semantic instance segmentations, and complete camera information for every image. Additionally, each image is factored into diffuse reflectance, diffuse illumination, and a non-diffuse residual term that captures view-dependent lighting effects. We analyzed the dataset in terms of scenes, objects, and pixels and evaluated costs in terms of money, computation time, and annotation effort. We found that generating the dataset from scratch was roughly half the cost of training a popular open-source natural language processing model. We also evaluated the sim-to-real transfer performance on two real-world scene understanding tasks and found that pre-training on our dataset significantly improved performance on both tasks, achieving state-of-the-art performance on the most challenging Pix3D test set. Our dataset and code are available online.",1
"Protein is linked to almost every life process. Therefore, analyzing the biological structure and property of protein sequences is critical to the exploration of life, as well as disease detection and drug discovery. Traditional protein analysis methods tend to be labor-intensive and time-consuming. The emergence of deep learning models makes modeling data patterns in large quantities of data possible. Interdisciplinary researchers have begun to leverage deep learning methods to model large biological datasets, e.g. using long short-term memory and convolutional neural network for protein sequence classification. After millions of years of evolution, evolutionary information is encoded in protein sequences. Inspired by the similarity between natural language and protein sequences, we use large-scale language models to model evolutionary-scale protein sequences, encoding protein biology information in representation. Significant improvements are observed in both token-level and sequence-level tasks, demonstrating that our large-scale model can accurately capture evolution information from pretraining on evolutionary-scale individual sequences. Our code and model are available at https://github.com/THUDM/ProteinLM.",0
"The analysis of protein sequences is crucial for understanding various life processes and detecting diseases and developing drugs. However, traditional methods of protein analysis are time-consuming and require a lot of effort. The emergence of deep learning models has made it possible to analyze large amounts of data more efficiently. Interdisciplinary researchers are now using deep learning methods, such as long short-term memory and convolutional neural network, for protein sequence classification. Protein sequences contain evolutionary information that has been encoded over millions of years. Inspired by the similarities between natural language and protein sequences, large-scale language models are being used to model evolutionary-scale protein sequences and encode biological information. Our large-scale model has shown significant improvements in both token-level and sequence-level tasks, accurately capturing evolution information. The code and model are available at https://github.com/THUDM/ProteinLM.",1
"Deep learning research has recently witnessed an impressively fast-paced progress in a wide range of tasks including computer vision, natural language processing, and reinforcement learning. The extraordinary performance of these systems often gives the impression that they can be used to revolutionise our lives for the better. However, as recent works point out, these systems suffer from several issues that make them unreliable for use in the real world, including vulnerability to adversarial attacks (Szegedy et al. [248]), tendency to memorise noise (Zhang et al. [292]), being over-confident on incorrect predictions (miscalibration) (Guo et al. [99]), and unsuitability for handling private data (Gilad-Bachrach et al. [88]). In this thesis, we look at each of these issues in detail, investigate their causes, and propose computationally cheap algorithms for mitigating them in practice. To do this, we identify structures in deep neural networks that can be exploited to mitigate the above causes of unreliability of deep learning algorithms.",0
"In recent times, deep learning research has made remarkable progress in a variety of tasks such as computer vision, natural language processing, and reinforcement learning. While these systems exhibit exceptional performance, it is often believed that they can bring about a revolution in our lives for the better. However, recent studies indicate that these systems are plagued by a multitude of issues rendering them unreliable for real-world usage. These issues include susceptibility to adversarial attacks, noise memorization, miscalibration, and unsuitability for sensitive data handling. This thesis delves into each of these problems, their underlying causes, and recommends cost-effective algorithms to tackle them. We explore deep neural network structures that can be leveraged to mitigate these issues and ensure the reliability of deep learning algorithms in practice.",1
"Deep Neural Networks (DNNs) have transformed the field of machine learning and are widely deployed in many applications involving image, video, speech and natural language processing. The increasing compute demands of DNNs have been widely addressed through Graphics Processing Units (GPUs) and specialized accelerators. However, as model sizes grow, these von Neumann architectures require very high memory bandwidth to keep the processing elements utilized as a majority of the data resides in the main memory. Processing in memory has been proposed as a promising solution for the memory wall bottleneck for ML workloads. In this work, we propose a new DRAM-based processing-in-memory (PIM) multiplication primitive coupled with intra-bank accumulation to accelerate matrix vector operations in ML workloads. The proposed multiplication primitive adds < 1% area overhead and does not require any change in the DRAM peripherals. Therefore, the proposed multiplication can be easily adopted in commodity DRAM chips. Subsequently, we design a DRAM-based PIM architecture, data mapping scheme and dataflow for executing DNNs within DRAM. System evaluations performed on networks like AlexNet, VGG16 and ResNet18 show that the proposed architecture, mapping, and data flow can provide up to 19.5x speedup over an NVIDIA Titan Xp GPU highlighting the need to overcome the memory bottleneck in future generations of DNN hardware.",0
"The application of Deep Neural Networks (DNNs) has revolutionized machine learning, with their widespread use in various domains such as image, video, speech, and natural language processing. To address the rising computational demands of DNNs, Graphics Processing Units (GPUs) and specialized accelerators have been employed. However, as the size of the models increases, the von Neumann architectures require high memory bandwidth to keep the processing units utilized, as most of the data resides in the main memory. To overcome this memory wall bottleneck for ML workloads, processing in memory has been suggested as a promising solution. This work proposes a DRAM-based processing-in-memory (PIM) multiplication primitive for matrix vector operations in ML workloads, coupled with intra-bank accumulation. The proposed multiplication primitive entails < 1% area overhead and does not require any change in the DRAM peripherals, allowing for easy adoption in commodity DRAM chips. Furthermore, a DRAM-based PIM architecture, data mapping scheme, and data flow have been designed for executing DNNs within DRAM. System evaluations performed on networks such as AlexNet, VGG16, and ResNet18 demonstrate that the proposed architecture, mapping, and data flow can deliver up to 19.5x speedup over an NVIDIA Titan Xp GPU, highlighting the need to address the memory bottleneck in future generations of DNN hardware.",1
"Deep generative models of 3D shapes have received a great deal of research interest. Yet, almost all of them generate discrete shape representations, such as voxels, point clouds, and polygon meshes. We present the first 3D generative model for a drastically different shape representation --- describing a shape as a sequence of computer-aided design (CAD) operations. Unlike meshes and point clouds, CAD models encode the user creation process of 3D shapes, widely used in numerous industrial and engineering design tasks. However, the sequential and irregular structure of CAD operations poses significant challenges for existing 3D generative models. Drawing an analogy between CAD operations and natural language, we propose a CAD generative network based on the Transformer. We demonstrate the performance of our model for both shape autoencoding and random shape generation. To train our network, we create a new CAD dataset consisting of 178,238 models and their CAD construction sequences. We have made this dataset publicly available to promote future research on this topic.",0
"Research interest in deep generative models for 3D shapes has been significant, but most of these models generate shape representations that are discrete, such as point clouds, voxels, and polygon meshes. However, we have developed the first 3D generative model that describes a shape as a sequence of computer-aided design (CAD) operations. Unlike meshes and point clouds, CAD models capture the user's creation process of 3D shapes, which is widely used in various industrial and engineering design tasks. However, the sequential and irregular structure of CAD operations has posed significant challenges for existing 3D generative models. To overcome this, we have proposed a CAD generative network based on the Transformer, drawing an analogy between CAD operations and natural language. Our model performs well for both shape autoencoding and random shape generation, and to train it, we created a new CAD dataset containing 178,238 models and their CAD construction sequences. We have made this dataset publicly available to encourage further research in this area.",1
"Vision-language Navigation (VLN) tasks require an agent to navigate step-by-step while perceiving the visual observations and comprehending a natural language instruction. Large data bias, which is caused by the disparity ratio between the small data scale and large navigation space, makes the VLN task challenging. Previous works have proposed various data augmentation methods to reduce data bias. However, these works do not explicitly reduce the data bias across different house scenes. Therefore, the agent would overfit to the seen scenes and achieve poor navigation performance in the unseen scenes. To tackle this problem, we propose the Random Environmental Mixup (REM) method, which generates cross-connected house scenes as augmented data via mixuping environment. Specifically, we first select key viewpoints according to the room connection graph for each scene. Then, we cross-connect the key views of different scenes to construct augmented scenes. Finally, we generate augmented instruction-path pairs in the cross-connected scenes. The experimental results on benchmark datasets demonstrate that our augmentation data via REM help the agent reduce its performance gap between the seen and unseen environment and improve the overall performance, making our model the best existing approach on the standard VLN benchmark.",0
"The Vision-language Navigation (VLN) task involves an agent navigating through a space while receiving visual input and understanding natural language instructions. However, this task is challenging due to large data bias caused by the difference in scale between the small data set and large navigation space. While previous studies have suggested data augmentation methods to reduce data bias, they have not addressed the issue of data bias across different house scenes, which results in poor navigation performance in unseen scenes. To address this, we propose the Random Environmental Mixup (REM) method, which generates augmented data by cross-connecting house scenes through mixuping the environment. Our method involves selecting key viewpoints based on the room connection graph for each scene and cross-connecting them to construct new scenes. We then generate augmented instruction-path pairs in the cross-connected scenes. Our experimental results show that the REM method reduces the performance gap between the seen and unseen environment and improves overall performance, making it the best approach on the standard VLN benchmark.",1
"Image caption generation is one of the most challenging problems at the intersection of visual recognition and natural language modeling domains. In this work, we propose and study a practically important variant of this problem where test images may contain visual objects with no corresponding visual or textual training examples. For this problem, we propose a detection-driven approach based on a generalized zero-shot detection model and a template-based sentence generation model. In order to improve the detection component, we jointly define a class-to-class similarity based class representation and a practical score calibration mechanism. We also propose a novel evaluation metric that provides complimentary insights to the captioning outputs, by separately handling the visual and non-visual components of the captions. Our experiments show that the proposed zero-shot detection model obtains state-of-the-art performance on the MS-COCO dataset and the zero-shot captioning approach yields promising results.",0
"Generating captions for images is a challenging task that requires expertise in both visual recognition and natural language modeling. This paper focuses on a specific aspect of this problem, where images used for testing may contain visual objects that were not included in the training data. To address this issue, the authors propose a two-fold solution that combines a generalized zero-shot detection model with a template-based sentence generation model. The detection component is enhanced by using class-to-class similarity and a practical score calibration mechanism. Additionally, the authors introduce a novel evaluation metric that provides a more comprehensive analysis of the captioning outputs. Experimental results based on the MS-COCO dataset demonstrate that the proposed zero-shot detection model achieves state-of-the-art performance, while the zero-shot captioning approach shows promising results.",1
"Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the self-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent networks. Given its high performance and less need for vision-specific inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these vision transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also include efficient transformer methods for pushing transformer into real device-based applications. Furthermore, we also take a brief look at the self-attention mechanism in computer vision, as it is the base component in transformer. Toward the end of this paper, we discuss the challenges and provide several further research directions for vision transformers.",0
"The transformer, initially developed for natural language processing, is a deep neural network that relies on the self-attention mechanism. Researchers are now investigating ways to apply this technology to computer vision tasks due to its robust representation capabilities. Transformer models have proven to be as good as, if not better than, traditional convolutional and recurrent networks on various visual benchmarks. This has led to a surge of interest in the computer vision community, as transformer models require less vision-specific inductive bias and perform well. This paper categorizes vision transformer models into different tasks, such as backbone network, high/mid-level vision, low-level vision, and video processing. The authors also examine efficient transformer methods for use in real device-based applications. They briefly discuss the self-attention mechanism in computer vision, which serves as the foundation of the transformer. Finally, the paper outlines the challenges and proposes directions for further research in vision transformers.",1
"Data processing and analytics are fundamental and pervasive. Algorithms play a vital role in data processing and analytics where many algorithm designs have incorporated heuristics and general rules from human knowledge and experience to improve their effectiveness. Recently, reinforcement learning, deep reinforcement learning (DRL) in particular, is increasingly explored and exploited in many areas because it can learn better strategies in complicated environments it is interacting with than statically designed algorithms. Motivated by this trend, we provide a comprehensive review of recent works focusing on utilizing deep reinforcement learning to improve data processing and analytics. First, we present an introduction to key concepts, theories, and methods in deep reinforcement learning. Next, we discuss deep reinforcement learning deployment on database systems, facilitating data processing and analytics in various aspects, including data organization, scheduling, tuning, and indexing. Then, we survey the application of deep reinforcement learning in data processing and analytics, ranging from data preparation, natural language interface to healthcare, fintech, etc. Finally, we discuss important open challenges and future research directions of using deep reinforcement learning in data processing and analytics.",0
"Data processing and analytics are essential and ubiquitous processes. Algorithms are crucial in these processes, and many algorithm designs have incorporated heuristics and general rules from human knowledge and experience to enhance their effectiveness. Deep reinforcement learning (DRL) is increasingly being explored and exploited in various fields due to its ability to learn better strategies in complex environments than statically designed algorithms. In light of this trend, we present a comprehensive review of recent works that focus on using DRL to improve data processing and analytics. Our review includes an introduction to key concepts, theories, and methods in DRL, followed by a discussion of its deployment on database systems to facilitate data processing and analytics in different aspects such as data organization, scheduling, tuning, and indexing. We also survey the applications of DRL in data processing and analytics, including data preparation, natural language interface, healthcare, fintech, and more. Lastly, we discuss the open challenges and future research directions of utilizing DRL in data processing and analytics.",1
"Recently proposed fine-grained 3D visual grounding is an essential and challenging task, whose goal is to identify the 3D object referred by a natural language sentence from other distractive objects of the same category. Existing works usually adopt dynamic graph networks to indirectly model the intra/inter-modal interactions, making the model difficult to distinguish the referred object from distractors due to the monolithic representations of visual and linguistic contents. In this work, we exploit Transformer for its natural suitability on permutation-invariant 3D point clouds data and propose a TransRefer3D network to extract entity-and-relation aware multimodal context among objects for more discriminative feature learning. Concretely, we devise an Entity-aware Attention (EA) module and a Relation-aware Attention (RA) module to conduct fine-grained cross-modal feature matching. Facilitated by co-attention operation, our EA module matches visual entity features with linguistic entity features while RA module matches pair-wise visual relation features with linguistic relation features, respectively. We further integrate EA and RA modules into an Entity-and-Relation aware Contextual Block (ERCB) and stack several ERCBs to form our TransRefer3D for hierarchical multimodal context modeling. Extensive experiments on both Nr3D and Sr3D datasets demonstrate that our proposed model significantly outperforms existing approaches by up to 10.6% and claims the new state-of-the-art. To the best of our knowledge, this is the first work investigating Transformer architecture for fine-grained 3D visual grounding task.",0
"The task of fine-grained 3D visual grounding has recently been proposed as an important and difficult challenge. Its objective is to identify the 3D object mentioned in a natural language sentence, amid other similar objects. Previous approaches have used dynamic graph networks to model intra and inter-modal interactions, resulting in difficulty distinguishing the referred object from distractors due to the use of monolithic visual and linguistic representations. In this study, we propose a TransRefer3D network that uses the Transformer model, which is well-suited for permutation-invariant 3D point clouds data. Our network is designed to extract entity-and-relation aware multimodal context among objects for more discriminative feature learning. We achieve this through the use of an Entity-aware Attention (EA) module and a Relation-aware Attention (RA) module, which conduct fine-grained cross-modal feature matching. Our EA module matches visual entity features with linguistic entity features, while the RA module matches pair-wise visual relation features with linguistic relation features. We integrate the EA and RA modules into an Entity-and-Relation aware Contextual Block (ERCB) and stack several ERCBs to form our TransRefer3D for hierarchical multimodal context modeling. Our experiments on both Nr3D and Sr3D datasets show that our proposed model significantly outperforms existing approaches by up to 10.6%, claiming the new state-of-the-art. To the best of our knowledge, this is the first work to investigate the use of the Transformer architecture for the task of fine-grained 3D visual grounding.",1
"Video captioning aims to automatically generate natural language sentences that can describe the visual contents of a given video. Existing generative models like encoder-decoder frameworks cannot explicitly explore the object-level interactions and frame-level information from complex spatio-temporal data to generate semantic-rich captions. Our main contribution is to identify three key problems in a joint framework for future video summarization tasks. 1) Enhanced Object Proposal: we propose a novel Conditional Graph that can fuse spatio-temporal information into latent object proposal. 2) Visual Knowledge: Latent Proposal Aggregation is proposed to dynamically extract visual words with higher semantic levels. 3) Sentence Validation: A novel Discriminative Language Validator is proposed to verify generated captions so that key semantic concepts can be effectively preserved. Our experiments on two public datasets (MVSD and MSR-VTT) manifest significant improvements over state-of-the-art approaches on all metrics, especially for BLEU-4 and CIDEr. Our code is available at https://github.com/baiyang4/D-LSG-Video-Caption.",0
"The goal of video captioning is to generate natural language sentences that describe the visual content of a video automatically. However, current generative models such as encoder-decoder frameworks struggle to explore the object-level interactions and frame-level information present in complex spatio-temporal data, which limits their ability to produce captions that are semantically rich. To address this issue, we introduce a joint framework that addresses three key problems for future video summarization tasks. Firstly, we propose an Enhanced Object Proposal using a novel Conditional Graph that can fuse spatio-temporal information into latent object proposals. Secondly, we introduce Visual Knowledge through Latent Proposal Aggregation, which dynamically extracts visual words with higher semantic levels. Lastly, we propose a Discriminative Language Validator to verify generated captions and preserve key semantic concepts effectively. Our experiments on two public datasets (MVSD and MSR-VTT) demonstrate significant improvements over state-of-the-art approaches on all metrics, particularly for BLEU-4 and CIDEr. Our code is available at https://github.com/baiyang4/D-LSG-Video-Caption.",1
"Deep learning is a group of exciting new technologies for neural networks. Through a combination of advanced training techniques and neural network architectural components, it is now possible to create neural networks that can handle tabular data, images, text, and audio as both input and output. Deep learning allows a neural network to learn hierarchies of information in a way that is like the function of the human brain. This course will introduce the student to classic neural network structures, Convolution Neural Networks (CNN), Long Short-Term Memory (LSTM), Gated Recurrent Neural Networks (GRU), General Adversarial Networks (GAN), and reinforcement learning. Application of these architectures to computer vision, time series, security, natural language processing (NLP), and data generation will be covered. High-Performance Computing (HPC) aspects will demonstrate how deep learning can be leveraged both on graphical processing units (GPUs), as well as grids. Focus is primarily upon the application of deep learning to problems, with some introduction to mathematical foundations. Readers will use the Python programming language to implement deep learning using Google TensorFlow and Keras. It is not necessary to know Python prior to this book; however, familiarity with at least one programming language is assumed.",0
"The field of deep learning comprises innovative technologies for neural networks that enable them to process various types of data, such as tabular information, images, text, and audio, as both input and output. This approach facilitates the learning of hierarchical information, akin to the human brain, by employing advanced training techniques and neural network architectural components. This course covers classic neural network structures, such as CNN, LSTM, GRU, GAN, and reinforcement learning, and their applications in computer vision, time series, security, NLP, and data generation. HPC aspects demonstrate how deep learning can be utilized on GPUs and grids. The focus is on solving problems using deep learning, with some introduction to mathematical foundations. The reader will implement deep learning using Python, TensorFlow, and Keras. Prior knowledge of Python is not essential, but familiarity with at least one programming language is assumed.",1
"We extend the task of composed image retrieval, where an input query consists of an image and short textual description of how to modify the image. Existing methods have only been applied to non-complex images within narrow domains, such as fashion products, thereby limiting the scope of study on in-depth visual reasoning in rich image and language contexts. To address this issue, we collect the Compose Image Retrieval on Real-life images (CIRR) dataset, which consists of over 36,000 pairs of crowd-sourced, open-domain images with human-generated modifying text. To extend current methods to the open-domain, we propose CIRPLANT, a transformer based model that leverages rich pre-trained vision-and-language (V&L) knowledge for modifying visual features conditioned on natural language. Retrieval is then done by nearest neighbor lookup on the modified features. We demonstrate that with a relatively simple architecture, CIRPLANT outperforms existing methods on open-domain images, while matching state-of-the-art accuracy on the existing narrow datasets, such as fashion. Together with the release of CIRR, we believe this work will inspire further research on composed image retrieval.",0
"The current methods for composed image retrieval are limited in their applicability to non-complex images in narrow domains, such as fashion products. This hinders the potential for in-depth visual reasoning in rich image and language contexts. To overcome this limitation, we have developed the Compose Image Retrieval on Real-life images (CIRR) dataset, featuring over 36,000 pairs of open-domain images with human-generated modifying text. To enable retrieval of these images, we have proposed the CIRPLANT model, which utilizes a transformer-based approach that leverages pre-trained vision-and-language knowledge to modify visual features based on natural language. Retrieval is then achieved via nearest neighbor lookup on the modified features. Our results show that CIRPLANT surpasses existing methods in open-domain image retrieval, while maintaining state-of-the-art accuracy on narrow datasets like fashion. We hope that the release of CIRR and our work on CIRPLANT will encourage further research in the field of composed image retrieval.",1
"Video-and-Language Inference is a recently proposed task for joint video-and-language understanding. This new task requires a model to draw inference on whether a natural language statement entails or contradicts a given video clip. In this paper, we study how to address three critical challenges for this task: judging the global correctness of the statement involved multiple semantic meanings, joint reasoning over video and subtitles, and modeling long-range relationships and complex social interactions. First, we propose an adaptive hierarchical graph network that achieves in-depth understanding of the video over complex interactions. Specifically, it performs joint reasoning over video and subtitles in three hierarchies, where the graph structure is adaptively adjusted according to the semantic structures of the statement. Secondly, we introduce semantic coherence learning to explicitly encourage the semantic coherence of the adaptive hierarchical graph network from three hierarchies. The semantic coherence learning can further improve the alignment between vision and linguistics, and the coherence across a sequence of video segments. Experimental results show that our method significantly outperforms the baseline by a large margin.",0
"The task of Video-and-Language Inference involves understanding both video and language and has recently been proposed. To successfully complete this task, a model must determine if a given video clip contradicts or entails a natural language statement. This paper focuses on addressing three major challenges: determining the overall accuracy of the statement, reasoning jointly over video and subtitles, and modeling complex social interactions and long-range relationships. To tackle these challenges, an adaptive hierarchical graph network is proposed. This network performs joint reasoning over video and subtitles in three hierarchies, with the graph structure being adjusted based on the semantic structures of the statement. Additionally, semantic coherence learning is introduced to enhance the alignment between vision and language and improve coherence across a sequence of video segments. Experimental results demonstrate that this method outperforms the baseline by a significant margin.",1
"To continuously improve quality and reflect changes in data, machine learning applications have to regularly retrain and update their core models. We show that a differential analysis of language model snapshots before and after an update can reveal a surprising amount of detailed information about changes in the training data. We propose two new metrics---\emph{differential score} and \emph{differential rank}---for analyzing the leakage due to updates of natural language models. We perform leakage analysis using these metrics across models trained on several different datasets using different methods and configurations. We discuss the privacy implications of our findings, propose mitigation strategies and evaluate their effect.",0
"In order to maintain quality and adapt to changing data, it is necessary for machine learning applications to regularly update and retrain their core models. Our research demonstrates that a differential analysis of language model snapshots can provide valuable insights into the modifications made to the training data. We introduce two new metrics, namely the differential score and differential rank, to assess the extent of information leakage caused by updates to natural language models. Our analysis covers a range of models trained on diverse datasets using various methods and configurations. We also examine the privacy implications of our findings, suggest ways to mitigate potential risks, and assess the effectiveness of these measures.",1
"Recently, there have been breakthroughs in computer vision (""CV"") models that are more generalizable with the advent of models such as CLIP and ALIGN. In this paper, we analyze CLIP and highlight some of the challenges such models pose. CLIP reduces the need for task specific training data, potentially opening up many niche tasks to automation. CLIP also allows its users to flexibly specify image classification classes in natural language, which we find can shift how biases manifest. Additionally, through some preliminary probes we find that CLIP can inherit biases found in prior computer vision systems. Given the wide and unpredictable domain of uses for such models, this raises questions regarding what sufficiently safe behaviour for such systems may look like. These results add evidence to the growing body of work calling for a change in the notion of a 'better' model--to move beyond simply looking at higher accuracy at task-oriented capability evaluations, and towards a broader 'better' that takes into account deployment-critical features such as different use contexts, and people who interact with the model when thinking about model deployment.",0
"In recent times, computer vision models like CLIP and ALIGN have emerged, which are more generalizable. This paper focuses on analyzing CLIP and the challenges it presents. CLIP eliminates the need for specific training data for tasks, which could automate many niche tasks. Moreover, users can specify image classification classes in natural language, which alters how biases are expressed. Our preliminary tests indicate that CLIP may inherit biases from earlier computer vision systems. As these models have diverse applications, it prompts us to consider what constitutes safe behavior for such systems. These findings support the growing call for a broader definition of a 'better' model that goes beyond accuracy and task-oriented capability and accounts for deployment-critical features and people's interactions with the model.",1
"In many sequence learning tasks, such as program synthesis and document summarization, a key problem is searching over a large space of possible output sequences. We propose to learn representations of the outputs that are specifically meant for search: rich enough to specify the desired output but compact enough to make search more efficient. Discrete latent codes are appealing for this purpose, as they naturally allow sophisticated combinatorial search strategies. The latent codes are learned using a self-supervised learning principle, in which first a discrete autoencoder is trained on the output sequences, and then the resulting latent codes are used as intermediate targets for the end-to-end sequence prediction task. Based on these insights, we introduce the \emph{Latent Programmer}, a program synthesis method that first predicts a discrete latent code from input/output examples, and then generates the program in the target language. We evaluate the Latent Programmer on two domains: synthesis of string transformation programs, and generation of programs from natural language descriptions. We demonstrate that the discrete latent representation significantly improves synthesis accuracy.",0
"When dealing with tasks that involve learning sequences, such as document summarization and program synthesis, a major issue is sifting through a vast array of potential output sequences. To address this, we suggest developing representations of outputs designed specifically for search: ones that are detailed enough to define the desired output, but also concise enough to expedite the search process. Discrete latent codes are an attractive option for this purpose, as they naturally permit complex combinatorial search methods. We use a self-supervised learning approach to learn the latent codes: first, a discrete autoencoder is trained on the output sequences, and then the resulting latent codes are utilized as intermediate targets for the end-to-end sequence prediction task. Using these insights, we present the ""Latent Programmer,"" a program synthesis approach that predicts a discrete latent code from input/output examples before generating the program in the target language. We assess the Latent Programmer on two domains: the synthesis of string transformation programs and the generation of programs from natural language descriptions. Our results show that the discrete latent representation significantly enhances synthesis accuracy.",1
"Inspired by how the human brain employs a higher number of neural pathways when describing a highly focused subject, we show that deep attentive models used for the main vision-language task of image captioning, could be extended to achieve better performance. Image captioning bridges a gap between computer vision and natural language processing. Automated image captioning is used as a tool to eliminate the need for human agent for creating descriptive captions for unseen images.Automated image captioning is challenging and yet interesting. One reason is that AI based systems capable of generating sentences that describe an input image could be used in a wide variety of tasks beyond generating captions for unseen images found on web or uploaded to social media. For example, in biology and medical sciences, these systems could provide researchers and physicians with a brief linguistic description of relevant images, potentially expediting their work.",0
"By taking inspiration from the human brain's use of numerous neural pathways when describing a highly focused subject, we propose extending deep attentive models to enhance the performance of image captioning, which bridges the gap between computer vision and natural language processing. Automated image captioning is a useful tool for creating descriptive captions for unseen images without the need for human intervention. Although challenging, AI-based systems capable of generating sentences to describe input images have vast potential uses in various fields beyond generating captions for social media or web images. For instance, in medical and biological sciences, such systems could provide researchers and physicians with a succinct linguistic description of relevant images, ultimately speeding up their work.",1
"While convolutional neural networks have shown a tremendous impact on various computer vision tasks, they generally demonstrate limitations in explicitly modeling long-range dependencies due to the intrinsic locality of the convolution operation. Initially designed for natural language processing tasks, Transformers have emerged as alternative architectures with innate global self-attention mechanisms to capture long-range dependencies. In this paper, we propose TransDepth, an architecture that benefits from both convolutional neural networks and transformers. To avoid the network losing its ability to capture local-level details due to the adoption of transformers, we propose a novel decoder that employs attention mechanisms based on gates. Notably, this is the first paper that applies transformers to pixel-wise prediction problems involving continuous labels (i.e., monocular depth prediction and surface normal estimation). Extensive experiments demonstrate that the proposed TransDepth achieves state-of-the-art performance on three challenging datasets. Our code is available at: https://github.com/ygjwd12345/TransDepth.",0
"Although convolutional neural networks have been successful in various computer vision tasks, they struggle to model long-range dependencies due to the convolution operation's intrinsic locality. On the other hand, Transformers were initially designed for natural language processing and have a global self-attention mechanism that captures long-range dependencies. This paper introduces TransDepth, an architecture that combines the strengths of convolutional neural networks and Transformers. To preserve the network's ability to capture local-level details, a novel decoder with attention mechanisms based on gates is proposed. Furthermore, this is the first paper to apply Transformers to pixel-wise prediction problems with continuous labels, including monocular depth prediction and surface normal estimation. Extensive experiments show that TransDepth outperforms existing methods on three challenging datasets. The code is available at: https://github.com/ygjwd12345/TransDepth.",1
"Neural agents trained in reinforcement learning settings can learn to communicate among themselves via discrete tokens, accomplishing as a team what agents would be unable to do alone. However, the current standard of using one-hot vectors as discrete communication tokens prevents agents from acquiring more desirable aspects of communication such as zero-shot understanding. Inspired by word embedding techniques from natural language processing, we propose neural agent architectures that enables them to communicate via discrete tokens derived from a learned, continuous space. We show in a decision theoretic framework that our technique optimizes communication over a wide range of scenarios, whereas one-hot tokens are only optimal under restrictive assumptions. In self-play experiments, we validate that our trained agents learn to cluster tokens in semantically-meaningful ways, allowing them communicate in noisy environments where other techniques fail. Lastly, we demonstrate both that agents using our method can effectively respond to novel human communication and that humans can understand unlabeled emergent agent communication, outperforming the use of one-hot communication.",0
"Neural agents can work together through the use of discrete tokens learned in reinforcement learning settings. However, the current practice of using one-hot vectors as communication tokens limits agents from developing more advanced communication skills, like zero-shot understanding. To address this, we introduce neural agent architectures that allow agents to communicate through discrete tokens derived from a continuous space. Our approach optimizes communication in a range of scenarios, unlike one-hot tokens, which only work in specific conditions. In self-play experiments, we demonstrate that our agents learn to group tokens in semantically meaningful ways, which allows them to communicate even in noisy environments where other methods fail. Additionally, we show that our approach enables agents to respond effectively to novel human communication and that humans can understand emergent agent communication without labels, surpassing the use of one-hot communication.",1
"Meter-level load forecasting is crucial for efficient energy management and power system planning for Smart Grids (SGs), in tasks associated with regulation, dispatching, scheduling, and unit commitment of power grids. Although a variety of algorithms have been proposed and applied on the field, more accurate and robust models are still required: the overall utility cost of operations in SGs increases 10 million currency units if the load forecasting error increases 1%, and the mean absolute percentage error (MAPE) in forecasting is still much higher than 1%. Transformers have become the new state-of-the-art in a variety of tasks, including the ones in computer vision, natural language processing and time series forecasting, surpassing alternative neural models such as convolutional and recurrent neural networks. In this letter, we present a new state-of-the-art Transformer-based algorithm for the meter-level load forecasting task, which has surpassed the former state-of-the-art, LSTM, and the traditional benchmark, vanilla RNN, in all experiments by a margin of at least 13% in MAPE.",0
"Accurate and robust models for meter-level load forecasting are essential for efficient energy management and power system planning in Smart Grids (SGs). These models are utilized in tasks related to regulation, dispatching, scheduling, and unit commitment of power grids. Despite the existence of various algorithms in this field, the mean absolute percentage error (MAPE) in forecasting remains high, and more precise and reliable models are required. The cost of SGs operations increases by 10 million currency units with every 1% increase in load forecasting error. Transformers are currently the state-of-the-art models in computer vision, natural language processing, and time series forecasting, surpassing alternative neural models like convolutional and recurrent neural networks. In this letter, we introduce a new Transformer-based algorithm for meter-level load forecasting. Our algorithm has surpassed the former state-of-the-art model, LSTM, and the traditional benchmark, vanilla RNN, in all experiments by a margin of at least 13% in MAPE.",1
"Deep learning on graphs has attracted significant interests recently. However, most of the works have focused on (semi-) supervised learning, resulting in shortcomings including heavy label reliance, poor generalization, and weak robustness. To address these issues, self-supervised learning (SSL), which extracts informative knowledge through well-designed pretext tasks without relying on manual labels, has become a promising and trending learning paradigm for graph data. Different from SSL on other domains like computer vision and natural language processing, SSL on graphs has an exclusive background, design ideas, and taxonomies. Under the umbrella of graph self-supervised learning, we present a timely and comprehensive review of the existing approaches which employ SSL techniques for graph data. We construct a unified framework that mathematically formalizes the paradigm of graph SSL. According to the objectives of pretext tasks, we divide these approaches into four categories: generation-based, auxiliary property-based, contrast-based, and hybrid approaches. We further conclude the applications of graph SSL across various research fields and summarize the commonly used datasets, evaluation benchmark, performance comparison and open-source codes of graph SSL. Finally, we discuss the remaining challenges and potential future directions in this research field.",0
"Recently, there has been significant interest in deep learning on graphs. However, most of the existing research has focused on (semi-) supervised learning, which has limitations such as heavy reliance on labels, poor generalization, and weak robustness. To overcome these shortcomings, self-supervised learning (SSL) has emerged as a promising learning paradigm for graph data, where informative knowledge is extracted through well-designed pretext tasks without relying on manual labels. SSL on graphs has its own unique background, design ideas, and taxonomies, unlike SSL on other domains such as computer vision and natural language processing. In this review paper, we provide a comprehensive overview of existing approaches that utilize SSL techniques for graph data, and we present a unified framework that mathematically formalizes the paradigm of graph SSL. Based on the objectives of pretext tasks, we categorize these approaches into four groups: generation-based, auxiliary property-based, contrast-based, and hybrid approaches. We also discuss the applications of graph SSL in various research fields, summarize commonly used datasets, evaluation benchmarks, performance comparisons, and open-source codes of graph SSL. Finally, we highlight the remaining challenges and potential future directions in this research area.",1
"Progress of machine learning in critical care has been difficult to track, in part due to absence of public benchmarks. Other fields of research (such as computer vision and natural language processing) have established various competitions and public benchmarks. Recent availability of large clinical datasets has enabled the possibility of establishing public benchmarks. Taking advantage of this opportunity, we propose a public benchmark suite to address four areas of critical care, namely mortality prediction, estimation of length of stay, patient phenotyping and risk of decompensation. We define each task and compare the performance of both clinical models as well as baseline and deep learning models using eICU critical care dataset of around 73,000 patients. This is the first public benchmark on a multi-centre critical care dataset, comparing the performance of clinical gold standard with our predictive model. We also investigate the impact of numerical variables as well as handling of categorical variables on each of the defined tasks. The source code, detailing our methods and experiments is publicly available such that anyone can replicate our results and build upon our work.",0
"The progress of machine learning in critical care has been challenging to monitor due to the absence of public benchmarks. However, other fields of research such as computer vision and natural language processing have established various competitions and public benchmarks. With the recent availability of large clinical datasets, the establishment of public benchmarks is now possible. To take advantage of this opportunity, we propose a public benchmark suite that focuses on four areas of critical care: mortality prediction, estimation of length of stay, patient phenotyping, and risk of decompensation. We provide a definition for each task and compare the performance of clinical models, baseline models, and deep learning models using the eICU critical care dataset, which includes approximately 73,000 patients. This is the first public benchmark on a multi-center critical care dataset that compares the performance of the clinical gold standard with our predictive model. Additionally, we investigate the impact of numerical and categorical variables on each of the defined tasks. We have made the source code publicly available, detailing our methods and experiments, so that anyone can replicate our results and build upon our work.",1
"Interventional causal models describe several joint distributions over some variables used to describe a system, one for each intervention setting. They provide a formal recipe for how to move between the different joint distributions and make predictions about the variables upon intervening on the system. Yet, it is difficult to formalise how we may change the underlying variables used to describe the system, say moving from fine-grained to coarse-grained variables. Here, we argue that compositionality is a desideratum for such model transformations and the associated errors: When abstracting a reference model M iteratively, first obtaining M' and then further simplifying that to obtain M'', we expect the composite transformation from M to M'' to exist and its error to be bounded by the errors incurred by each individual transformation step. Category theory, the study of mathematical objects via compositional transformations between them, offers a natural language to develop our framework for model transformations and abstractions. We introduce a category of finite interventional causal models and, leveraging theory of enriched categories, prove the desired compositionality properties for our framework.",0
"Interventional causal models are a set of joint distributions that describe a system's variables for each intervention setting. These models provide a systematic approach to navigate between the different joint distributions and make predictions about the variables following an intervention. However, changing the underlying variables used to describe the system, such as shifting from fine-grained to coarse-grained variables, is challenging to formalize. In this paper, we propose that compositionality is a crucial factor in model transformations and their associated errors. We argue that when we abstract a reference model M iteratively, first obtaining M' and then further simplifying it to M'', we expect the composite transformation from M to M'' to exist, with its error bounded by the errors incurred by each individual transformation step. Category theory provides a natural language to develop our framework for model transformations and abstractions, as it studies mathematical objects via compositional transformations. Therefore, we introduce a category of finite interventional causal models and, using the theory of enriched categories, prove the desired compositionality properties for our framework.",1
"Video captioning is an essential technology to understand scenes and describe events in natural language. To apply it to real-time monitoring, a system needs not only to describe events accurately but also to produce the captions as soon as possible. Low-latency captioning is needed to realize such functionality, but this research area for online video captioning has not been pursued yet. This paper proposes a novel approach to optimize each caption's output timing based on a trade-off between latency and caption quality. An audio-visual Trans-former is trained to generate ground-truth captions using only a small portion of all video frames, and to mimic outputs of a pre-trained Transformer to which all the frames are given. A CNN-based timing detector is also trained to detect a proper output timing, where the captions generated by the two Trans-formers become sufficiently close to each other. With the jointly trained Transformer and timing detector, a caption can be generated in the early stages of an event-triggered video clip, as soon as an event happens or when it can be forecasted. Experiments with the ActivityNet Captions dataset show that our approach achieves 94% of the caption quality of the upper bound given by the pre-trained Transformer using the entire video clips, using only 28% of frames from the beginning.",0
"In order to comprehend scenes and describe events in natural language, video captioning is a crucial technology. For real-time monitoring, the system must produce captions both accurately and swiftly, which requires low-latency captioning. Unfortunately, research in this area for online video captioning has not yet been explored. This paper introduces a new technique that optimizes each caption's output timing by balancing latency and caption quality. A small portion of all video frames is used to train an audio-visual Transformer to generate ground-truth captions and mimic the output of a pre-trained Transformer that uses all frames. Additionally, a CNN-based timing detector is trained to identify the appropriate output timing where the captions generated by both Transformers are sufficiently similar. With this jointly trained system, captions can be generated early in event-triggered video clips when an event occurs or can be predicted. Experiments with the ActivityNet Captions dataset demonstrate that this approach produces captions with 94% of the quality of the pre-trained Transformer that uses the entire video clip, but only requires 28% of the frames from the start.",1
"Most existing neural architecture search (NAS) algorithms are dedicated to the downstream tasks, e.g., image classification in computer vision. However, extensive experiments have shown that, prominent neural architectures, such as ResNet in computer vision and LSTM in natural language processing, are generally good at extracting patterns from the input data and perform well on different downstream tasks. These observations inspire us to ask: Is it necessary to use the performance of specific downstream tasks to evaluate and search for good neural architectures? Can we perform NAS effectively and efficiently while being agnostic to the downstream task? In this work, we attempt to affirmatively answer the above two questions and improve the state-of-the-art NAS solution by proposing a novel and generic NAS framework, termed Generic NAS (GenNAS). GenNAS does not use task-specific labels but instead adopts \textit{regression} on a set of manually designed synthetic signal bases for architecture evaluation. Such a self-supervised regression task can effectively evaluate the intrinsic power of an architecture to capture and transform the input signal patterns, and allow more sufficient usage of training samples. We then propose an automatic task search to optimize the combination of synthetic signals using limited downstream-task-specific labels, further improving the performance of GenNAS. We also thoroughly evaluate GenNAS's generality and end-to-end NAS performance on all search spaces, which outperforms almost all existing works with significant speedup.",0
"Currently, most neural architecture search (NAS) algorithms focus on downstream tasks, such as image classification in computer vision. However, research has demonstrated that prominent neural architectures, like ResNet in computer vision and LSTM in natural language processing, excel at discovering patterns in input data and performing well across various downstream tasks. This has led to the question of whether evaluating and searching for good neural architectures requires task-specific performance metrics. Is it possible to conduct effective and efficient NAS without consideration for downstream tasks? This study aims to confirm that it is, and proposes a novel and universal NAS framework called Generic NAS (GenNAS). GenNAS does not rely on task-specific labels, but instead utilizes regression on a set of manually designed synthetic signal bases to assess architecture performance. This self-supervised regression task can accurately evaluate a neural architecture's intrinsic ability to capture and transform input signal patterns, allowing for more efficient use of training samples. Additionally, the study proposes an automatic task search to optimize the combination of synthetic signals using limited downstream-task-specific labels, further enhancing the performance of GenNAS. The study thoroughly evaluates GenNAS's generality and end-to-end NAS performance across all search spaces, demonstrating its superiority over almost all existing works with significant speedup.",1
"Attention-based transformer networks have demonstrated promising potential as their applications extend from natural language processing to vision. However, despite the recent improvements, such as sub-quadratic attention approximation and various training enhancements, the compact vision transformers to date using the regular attention still fall short in comparison with its convnet counterparts, in terms of \textit{accuracy,} \textit{model size}, \textit{and} \textit{throughput}. This paper introduces a compact self-attention mechanism that is fundamental and highly generalizable. The proposed method reduces redundancy and improves efficiency on top of the existing attention optimizations. We show its drop-in applicability for both the regular attention mechanism and some most recent variants in vision transformers. As a result, we produced smaller and faster models with the same or better accuracies.",0
"Transformers with attention mechanisms have shown great potential, with applications in natural language processing and vision. Despite recent improvements, including sub-quadratic attention approximation and various training enhancements, compact vision transformers using regular attention still lag behind convnet counterparts in terms of accuracy, model size, and throughput. This paper presents a novel, highly generalizable self-attention mechanism that reduces redundancy and enhances efficiency beyond existing attention optimizations. Our approach can be applied to regular attention and recent variants in vision transformers, resulting in smaller and faster models with the same or better accuracies.",1
"Hypertext transfer protocol (HTTP) is one of the most widely used protocols on the Internet. As a consequence, most attacks (i.e., SQL injection, XSS) use HTTP as the transport mechanism. Therefore, it is crucial to develop an intelligent solution that would allow to effectively detect and filter out anomalies in HTTP traffic. Currently, most of the anomaly detection systems are either rule-based or trained using manually selected features. We propose utilizing modern unsupervised language representation model for embedding HTTP requests and then using it to classify anomalies in the traffic. The solution is motivated by methods used in Natural Language Processing (NLP) such as Doc2Vec which could potentially capture the true understanding of HTTP messages, and therefore improve the efficiency of Intrusion Detection System. In our work, we not only aim at generating a suitable embedding space, but also at the interpretability of the proposed model. We decided to use the current state-of-the-art RoBERTa, which, as far as we know, has never been used in a similar problem. To verify how the solution would work in real word conditions, we train the model using only legitimate traffic. We also try to explain the results based on clusters that occur in the vectorized requests space and a simple logistic regression classifier. We compared our approach with the similar, previously proposed methods. We evaluate the feasibility of our method on three different datasets: CSIC2010, CSE-CIC-IDS2018 and one that we prepared ourselves. The results we show are comparable to others or better, and most importantly - interpretable.",0
"HTTP is a widely used protocol on the internet, which makes it a popular choice for attacks like SQL injection and XSS. To effectively detect and filter out anomalies in HTTP traffic, it's crucial to develop an intelligent solution. However, most current anomaly detection systems are either rule-based or trained using manually selected features. To address this, we propose using modern unsupervised language representation models to embed HTTP requests and classify anomalies in the traffic. Our approach is motivated by NLP methods like Doc2Vec, which can capture the true understanding of HTTP messages and improve the efficiency of Intrusion Detection Systems. We use RoBERTa, the state-of-the-art language model, and train it only on legitimate traffic to test its real-world performance. Our model's interpretability is also important, so we explain the results based on vectorized request clusters and a simple logistic regression classifier. We compare our approach to similar methods and evaluate it on three datasets: CSIC2010, CSE-CIC-IDS2018, and one we prepared ourselves. Our results are either comparable or better than others, and most importantly, interpretable.",1
"Deep learning has achieved great success in a wide spectrum of multimedia applications such as image classification, natural language processing and multimodal data analysis. Recent years have seen the development of many deep learning frameworks that provide a high-level programming interface for users to design models, conduct training and deploy inference. However, it remains challenging to build an efficient end-to-end multimedia application with most existing frameworks. Specifically, in terms of usability, it is demanding for non-experts to implement deep learning models, obtain the right settings for the entire machine learning pipeline, manage models and datasets, and exploit external data sources all together. Further, in terms of adaptability, elastic computation solutions are much needed as the actual serving workload fluctuates constantly, and scaling the hardware resources to handle the fluctuating workload is typically infeasible. To address these challenges, we introduce SINGA-Easy, a new deep learning framework that provides distributed hyper-parameter tuning at the training stage, dynamic computational cost control at the inference stage, and intuitive user interactions with multimedia contents facilitated by model explanation. Our experiments on the training and deployment of multi-modality data analysis applications show that the framework is both usable and adaptable to dynamic inference loads. We implement SINGA-Easy on top of Apache SINGA and demonstrate our system with the entire machine learning life cycle.",0
"A wide range of multimedia applications, including image classification, natural language processing, and multimodal data analysis, have experienced significant success through deep learning. While many deep learning frameworks have been developed in recent years to enable users to design models, train them, and deploy inference, building an efficient end-to-end multimedia application with most existing frameworks remains a challenge. The usability of these frameworks is particularly demanding for non-experts, who may struggle to implement deep learning models, obtain the right machine learning pipeline settings, manage models and datasets, and integrate external data sources. Additionally, adaptability is a major concern, as elastic computation solutions are required to handle constantly fluctuating serving workloads, and scaling hardware resources to meet these workloads is often impractical. To address these challenges, we present SINGA-Easy, a new deep learning framework that offers distributed hyper-parameter tuning during the training stage, dynamic computational cost control during inference, and intuitive user interactions with multimedia content through model explanation. Our testing shows that SINGA-Easy is both usable and adaptable to dynamic inference loads, and we implement the framework on top of Apache SINGA to demonstrate the full machine learning life cycle.",1
"The recently-proposed Perceiver model obtains good results on several domains (images, audio, multimodal, point clouds) while scaling linearly in compute and memory with the input size. While the Perceiver supports many kinds of inputs, it can only produce very simple outputs such as class scores. Perceiver IO overcomes this limitation without sacrificing the original's appealing properties by learning to flexibly query the model's latent space to produce outputs of arbitrary size and semantics. Perceiver IO still decouples model depth from data size and still scales linearly with data size, but now with respect to both input and output sizes. The full Perceiver IO model achieves strong results on tasks with highly structured output spaces, such as natural language and visual understanding, StarCraft II, and multi-task and multi-modal domains. As highlights, Perceiver IO matches a Transformer-based BERT baseline on the GLUE language benchmark without the need for input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation.",0
"The Perceiver model, which has been recently proposed, achieves good results across multiple domains including images, audio, multimodal, and point clouds, while maintaining a linear relationship between compute and memory with input size. Despite its versatility in handling various input types, the Perceiver model can only generate simple outputs such as class scores. To address this limitation, Perceiver IO has been introduced, which allows for flexible querying of the model's latent space to produce outputs of varying sizes and semantics without compromising the original model's desirable qualities. Perceiver IO still maintains a linear relationship between model depth and data size, while scaling linearly with both input and output sizes. The complete Perceiver IO model demonstrates impressive performance in tasks with highly structured output spaces, including natural language and visual understanding, StarCraft II, and multi-task and multi-modal domains. Notably, Perceiver IO achieves results that are on par with a Transformer-based BERT baseline in the GLUE language benchmark without requiring input tokenization and sets a new state-of-the-art performance benchmark for Sintel optical flow estimation.",1
"Can a generative model be trained to produce images from a specific domain, guided by a text prompt only, without seeing any image? In other words: can an image generator be trained blindly? Leveraging the semantic power of large scale Contrastive-Language-Image-Pre-training (CLIP) models, we present a text-driven method that allows shifting a generative model to new domains, without having to collect even a single image from those domains. We show that through natural language prompts and a few minutes of training, our method can adapt a generator across a multitude of domains characterized by diverse styles and shapes. Notably, many of these modifications would be difficult or outright impossible to reach with existing methods. We conduct an extensive set of experiments and comparisons across a wide range of domains. These demonstrate the effectiveness of our approach and show that our shifted models maintain the latent-space properties that make generative models appealing for downstream tasks.",0
"Is it possible to train a generative model to produce images in a specific domain solely based on text prompts, without any image input? In simpler terms, can an image generator be trained without visual assistance? Our approach utilizes the semantic capabilities of Contrastive-Language-Image-Pre-training (CLIP) models, allowing us to shift a generative model to new domains without the need for image collection. By using natural language prompts and a brief training session, our method can adapt a generator to a variety of domains with diverse styles and shapes, some of which are difficult or impossible to achieve with existing methods. We conducted numerous experiments and comparisons across multiple domains, demonstrating the effectiveness of our approach while maintaining the latent-space properties that make generative models ideal for downstream tasks.",1
"Human-in-the-loop aims to train an accurate prediction model with minimum cost by integrating human knowledge and experience. Humans can provide training data for machine learning applications and directly accomplish some tasks that are hard for computers in the pipeline with the help of machine-based approaches. In this paper, we survey existing works on human-in-the-loop from a data perspective and classify them into three categories with a progressive relationship: (1) the work of improving model performance from data processing, (2) the work of improving model performance through interventional model training, and (3) the design of the system independent human-in-the-loop. Using the above categorization, we summarize major approaches in the field, along with their technical strengths/ weaknesses, we have simple classification and discussion in natural language processing, computer vision, and others. Besides, we provide some open challenges and opportunities. This survey intends to provide a high-level summarization for human-in-the-loop and motivates interested readers to consider approaches for designing effective human-in-the-loop solutions.",0
"The objective of human-in-the-loop is to create a precise prediction model with minimal expenditure by merging human knowledge and experience. Humans can contribute training data to machine learning applications and execute certain complex tasks that computers struggle with, with the aid of machine-based methods. This article surveys current studies on human-in-the-loop from a data perspective and categorizes them into three progressive groups: (1) improving model performance through data processing, (2) enhancing model performance through interventional model training, and (3) designing a system independent human-in-the-loop. We outline the primary approaches in the field, including their technical strengths and weaknesses, and discuss them in natural language processing, computer vision, and other domains. Additionally, we present some open challenges and prospects. This review serves as a general overview of human-in-the-loop and encourages readers to consider effective solutions for designing human-in-the-loop systems.",1
"Learning continuously during all model lifetime is fundamental to deploy machine learning solutions robust to drifts in the data distribution. Advances in Continual Learning (CL) with recurrent neural networks could pave the way to a large number of applications where incoming data is non stationary, like natural language processing and robotics. However, the existing body of work on the topic is still fragmented, with approaches which are application-specific and whose assessment is based on heterogeneous learning protocols and datasets. In this paper, we organize the literature on CL for sequential data processing by providing a categorization of the contributions and a review of the benchmarks. We propose two new benchmarks for CL with sequential data based on existing datasets, whose characteristics resemble real-world applications. We also provide a broad empirical evaluation of CL and Recurrent Neural Networks in class-incremental scenario, by testing their ability to mitigate forgetting with a number of different strategies which are not specific to sequential data processing. Our results highlight the key role played by the sequence length and the importance of a clear specification of the CL scenario.",0
"Continuous learning throughout the entire lifespan of a model is crucial to create machine learning solutions that can withstand changes in data distribution. The development of Continual Learning (CL) using recurrent neural networks has the potential to revolutionize various applications, such as natural language processing and robotics, where incoming data is unpredictable. However, the current research on CL is fragmented, with application-specific approaches and assessments based on different learning protocols and datasets. This paper aims to organize the literature on CL for sequential data processing by categorizing contributions and reviewing benchmarks. Two new benchmarks for CL with sequential data are proposed using existing datasets that resemble real-world applications. The study also includes a comprehensive empirical evaluation of CL and Recurrent Neural Networks in a class-incremental scenario, testing their ability to mitigate forgetting using various strategies applicable to sequential data processing. The findings underscore the importance of specifying the CL scenario and the sequence length.",1
"""How can we animate 3D-characters from a movie script or move robots by simply telling them what we would like them to do?"" ""How unstructured and complex can we make a sentence and still generate plausible movements from it?"" These are questions that need to be answered in the long-run, as the field is still in its infancy. Inspired by these problems, we present a new technique for generating compositional actions, which handles complex input sentences. Our output is a 3D pose sequence depicting the actions in the input sentence. We propose a hierarchical two-stream sequential model to explore a finer joint-level mapping between natural language sentences and 3D pose sequences corresponding to the given motion. We learn two manifold representations of the motion -- one each for the upper body and the lower body movements. Our model can generate plausible pose sequences for short sentences describing single actions as well as long compositional sentences describing multiple sequential and superimposed actions. We evaluate our proposed model on the publicly available KIT Motion-Language Dataset containing 3D pose data with human-annotated sentences. Experimental results show that our model advances the state-of-the-art on text-based motion synthesis in objective evaluations by a margin of 50%. Qualitative evaluations based on a user study indicate that our synthesized motions are perceived to be the closest to the ground-truth motion captures for both short and compositional sentences.",0
"The field of generating movements for 3D characters and robots through natural language commands and complex sentences is still in its early stages, and we must address questions such as how to generate plausible movements from unstructured and intricate input. To tackle these issues, we propose a new technique for generating compositional actions using a hierarchical two-stream sequential model. Our model can generate 3D pose sequences for both short and long sentences describing single or multiple sequential actions. We evaluate our model on the KIT Motion-Language Dataset, where it outperforms the state-of-the-art by 50% and is perceived to be the closest to the ground-truth motion captures in user studies.",1
"Despite the progress in automatic detection of radiologic findings from chest X-ray (CXR) images in recent years, a quantitative evaluation of the explainability of these models is hampered by the lack of locally labeled datasets for different findings. With the exception of a few expert-labeled small-scale datasets for specific findings, such as pneumonia and pneumothorax, most of the CXR deep learning models to date are trained on global ""weak"" labels extracted from text reports, or trained via a joint image and unstructured text learning strategy. Inspired by the Visual Genome effort in the computer vision community, we constructed the first Chest ImaGenome dataset with a scene graph data structure to describe $242,072$ images. Local annotations are automatically produced using a joint rule-based natural language processing (NLP) and atlas-based bounding box detection pipeline. Through a radiologist constructed CXR ontology, the annotations for each CXR are connected as an anatomy-centered scene graph, useful for image-level reasoning and multimodal fusion applications. Overall, we provide: i) $1,256$ combinations of relation annotations between $29$ CXR anatomical locations (objects with bounding box coordinates) and their attributes, structured as a scene graph per image, ii) over $670,000$ localized comparison relations (for improved, worsened, or no change) between the anatomical locations across sequential exams, as well as ii) a manually annotated gold standard scene graph dataset from $500$ unique patients.",0
"The lack of locally labeled datasets for different radiologic findings has hindered the quantitative evaluation of the explainability of deep learning models used for automatic detection of chest X-ray (CXR) images. Although some small-scale datasets exist for specific findings, most CXR deep learning models are trained using global ""weak"" labels extracted from text reports or a joint image and unstructured text learning strategy. To address this issue, we created the first Chest ImaGenome dataset using a scene graph data structure to describe 242,072 images. Local annotations were produced using a joint rule-based natural language processing and atlas-based bounding box detection pipeline, connected as an anatomy-centered scene graph through a CXR ontology. The dataset provides over 1,256 combinations of relation annotations and over 670,000 localized comparison relations, as well as a manually annotated gold standard scene graph dataset from 500 unique patients. This dataset can be useful for image-level reasoning and multimodal fusion applications.",1
"Connecting Vision and Language plays an essential role in Generative Intelligence. For this reason, large research efforts have been devoted to image captioning, i.e. describing images with syntactically and semantically meaningful sentences. Starting from 2015 the task has generally been addressed with pipelines composed of a visual encoder and a language model for text generation. During these years, both components have evolved considerably through the exploitation of object regions, attributes, the introduction of multi-modal connections, fully-attentive approaches, and BERT-like early-fusion strategies. However, regardless of the impressive results, research in image captioning has not reached a conclusive answer yet. This work aims at providing a comprehensive overview of image captioning approaches, from visual encoding and text generation to training strategies, datasets, and evaluation metrics. In this respect, we quantitatively compare many relevant state-of-the-art approaches to identify the most impactful technical innovations in architectures and training strategies. Moreover, many variants of the problem and its open challenges are discussed. The final goal of this work is to serve as a tool for understanding the existing literature and highlighting the future directions for a research area where Computer Vision and Natural Language Processing can find an optimal synergy.",0
"Generative Intelligence relies heavily on the connection between Vision and Language, which has led to extensive research on image captioning. This involves creating sentences that accurately and meaningfully describe images. Since 2015, image captioning has typically been approached using a visual encoder and a language model for text generation. Over the years, these components have been enhanced through the use of object regions, attributes, multi-modal connections, fully-attentive approaches, and BERT-like early-fusion strategies. Despite the impressive progress, there is still no definitive solution in image captioning. This study aims to comprehensively review image captioning methods, including visual encoding, text generation, training strategies, datasets, and evaluation metrics. We compare numerous state-of-the-art approaches quantitatively to identify the most impactful technical innovations in architectures and training strategies. Furthermore, we discuss various problem variations and open challenges. The ultimate objective is to provide a tool for understanding the existing literature and identifying future research directions for a field where Computer Vision and Natural Language Processing can synergize optimally.",1
"Gradient quantization is an emerging technique in reducing communication costs in distributed learning. Existing gradient quantization algorithms often rely on engineering heuristics or empirical observations, lacking a systematic approach to dynamically quantize gradients. This paper addresses this issue by proposing a novel dynamically quantized SGD (DQ-SGD) framework, enabling us to dynamically adjust the quantization scheme for each gradient descent step by exploring the trade-off between communication cost and convergence error. We derive an upper bound, tight in some cases, of the convergence error for a restricted family of quantization schemes and loss functions. We design our DQ-SGD algorithm via minimizing the communication cost under the convergence error constraints. Finally, through extensive experiments on large-scale natural language processing and computer vision tasks on AG-News, CIFAR-10, and CIFAR-100 datasets, we demonstrate that our quantization scheme achieves better tradeoffs between the communication cost and learning performance than other state-of-the-art gradient quantization methods.",0
"The use of gradient quantization has become increasingly popular in distributed learning as a means of reducing communication costs. However, current gradient quantization techniques are often based on empirical observations or heuristics and lack a systematic approach to quantizing gradients dynamically. To address this issue, this paper proposes a new framework called dynamically quantized SGD (DQ-SGD), which allows for the adjustment of the quantization scheme for each gradient descent step by balancing the trade-off between communication cost and convergence error. We establish an upper bound, which is sometimes tight, for the convergence error for a limited range of quantization schemes and loss functions. Our DQ-SGD algorithm is designed to minimize communication costs while adhering to convergence error constraints. Finally, we conducted extensive experiments on AG-News, CIFAR-10, and CIFAR-100 datasets for large-scale natural language processing and computer vision tasks, demonstrating that our quantization scheme offers better trade-offs between communication cost and learning performance compared to other state-of-the-art gradient quantization methods.",1
"Vision-and-Language Navigation (VLN) is a challenging task in which an agent needs to follow a language-specified path to reach a target destination. The goal gets even harder as the actions available to the agent get simpler and move towards low-level, atomic interactions with the environment. This setting takes the name of low-level VLN. In this paper, we strive for the creation of an agent able to tackle three key issues: multi-modality, long-term dependencies, and adaptability towards different locomotive settings. To that end, we devise ""Perceive, Transform, and Act"" (PTA): a fully-attentive VLN architecture that leaves the recurrent approach behind and the first Transformer-like architecture incorporating three different modalities - natural language, images, and low-level actions for the agent control. In particular, we adopt an early fusion strategy to merge lingual and visual information efficiently in our encoder. We then propose to refine the decoding phase with a late fusion extension between the agent's history of actions and the perceptual modalities. We experimentally validate our model on two datasets: PTA achieves promising results in low-level VLN on R2R and achieves good performance in the recently proposed R4R benchmark. Our code is publicly available at https://github.com/aimagelab/perceive-transform-and-act.",0
"The task of Vision-and-Language Navigation (VLN) is difficult as it requires an agent to follow a path described in language to reach a destination. The challenge increases as the agent's actions become simpler and more focused on interacting with the environment. This is known as low-level VLN. This paper aims to address three major issues in VLN: multi-modality, long-term dependencies, and adaptability to different locomotive settings. Our solution is ""Perceive, Transform, and Act"" (PTA), a fully-attentive architecture that incorporates natural language, images, and low-level actions for agent control. Our encoder efficiently merges lingual and visual information, while the decoding phase is improved with a late fusion extension between the agent's action history and perceptual modalities. We validate our model on two datasets, achieving promising results in low-level VLN on R2R and good performance on the R4R benchmark. Our code is available at https://github.com/aimagelab/perceive-transform-and-act.",1
"Relative position encoding (RPE) is important for transformer to capture sequence ordering of input tokens. General efficacy has been proven in natural language processing. However, in computer vision, its efficacy is not well studied and even remains controversial, e.g., whether relative position encoding can work equally well as absolute position? In order to clarify this, we first review existing relative position encoding methods and analyze their pros and cons when applied in vision transformers. We then propose new relative position encoding methods dedicated to 2D images, called image RPE (iRPE). Our methods consider directional relative distance modeling as well as the interactions between queries and relative position embeddings in self-attention mechanism. The proposed iRPE methods are simple and lightweight. They can be easily plugged into transformer blocks. Experiments demonstrate that solely due to the proposed encoding methods, DeiT and DETR obtain up to 1.5% (top-1 Acc) and 1.3% (mAP) stable improvements over their original versions on ImageNet and COCO respectively, without tuning any extra hyperparameters such as learning rate and weight decay. Our ablation and analysis also yield interesting findings, some of which run counter to previous understanding. Code and models are open-sourced at https://github.com/microsoft/Cream/tree/main/iRPE.",0
"Relative position encoding (RPE) is crucial for transformers to comprehend the ordering of input tokens in a sequence. It has been successfully employed in natural language processing, but its effectiveness in computer vision is controversial. The question remains whether RPE can function as effectively as absolute position. To address this issue, this study examines current RPE methods and their advantages and drawbacks when applied to vision transformers. The study presents novel RPE methods, called image RPE (iRPE), specifically designed for 2D images. These methods incorporate directional relative distance modeling and the interactions between queries and relative position embeddings in the self-attention mechanism. The iRPE methods are straightforward and lightweight and can be easily incorporated into transformer blocks. Experiments demonstrate that the proposed encoding methods improve DeiT and DETR by up to 1.5% (top-1 Acc) and 1.3% (mAP), respectively, on ImageNet and COCO, without the need to adjust any extra hyperparameters, such as learning rate and weight decay. The study's ablation and analysis reveal interesting results, some of which contradict previous beliefs. The code and models are available at https://github.com/microsoft/Cream/tree/main/iRPE.",1
"Compared with the visual grounding on 2D images, the natural-language-guided 3D object localization on point clouds is more challenging. In this paper, we propose a new model, named InstanceRefer, to achieve a superior 3D visual grounding through the grounding-by-matching strategy. In practice, our model first predicts the target category from the language descriptions using a simple language classification model. Then, based on the category, our model sifts out a small number of instance candidates (usually less than 20) from the panoptic segmentation of point clouds. Thus, the non-trivial 3D visual grounding task has been effectively re-formulated as a simplified instance-matching problem, considering that instance-level candidates are more rational than the redundant 3D object proposals. Subsequently, for each candidate, we perform the multi-level contextual inference, i.e., referring from instance attribute perception, instance-to-instance relation perception, and instance-to-background global localization perception, respectively. Eventually, the most relevant candidate is selected and localized by ranking confidence scores, which are obtained by the cooperative holistic visual-language feature matching. Experiments confirm that our method outperforms previous state-of-the-arts on ScanRefer online benchmark and Nr3D/Sr3D datasets.",0
"The task of 3D object localization on point clouds with natural language guidance is more difficult than on 2D images. In this study, we introduce a new model, InstanceRefer, which utilizes a grounding-by-matching strategy to achieve superior 3D visual grounding. Our model begins by predicting the target category from language descriptions using a straightforward language classification model. It then selects a small number of instance candidates, usually fewer than 20, from the panoptic segmentation of point clouds based on the category. By doing so, the complex 3D visual grounding task is simplified into an instance-matching problem, which is more rational than redundant 3D object proposals. We then perform multi-level contextual inference for each candidate, including instance attribute perception, instance-to-instance relation perception, and instance-to-background global localization perception. Finally, we use cooperative holistic visual-language feature matching to rank confidence scores and select the most relevant candidate for localization. Our experiments on ScanRefer online benchmark and Nr3D/Sr3D datasets demonstrate that our approach outperforms previous state-of-the-art methods.",1
"Video generation is one of the most challenging tasks in Machine Learning and Computer Vision fields of study. In this paper, we tackle the text to video generation problem, which is a conditional form of video generation. Humans can listen/read natural language sentences, and can imagine or visualize what is being described; therefore, we believe that video generation from natural language sentences will have an important impact on Artificial Intelligence. Video generation is relatively a new field of study in Computer Vision, which is far from being solved. The majority of recent works deal with synthetic datasets or real datasets with very limited types of objects, scenes, and emotions. To the best of our knowledge, this is the very first work on the text (free-form sentences) to video generation on more realistic video datasets like Actor and Action Dataset (A2D) or UCF101. We tackle the complicated problem of video generation by regressing the latent representations of the first and last frames and employing a context-aware interpolation method to build the latent representations of in-between frames. We propose a stacking ``upPooling'' block to sequentially generate RGB frames out of each latent representation and progressively increase the resolution. Moreover, our proposed Discriminator encodes videos based on single and multiple frames. We provide quantitative and qualitative results to support our arguments and show the superiority of our method over well-known baselines like Recurrent Neural Network (RNN) and Deconvolution (as known as Convolutional Transpose) based video generation methods.",0
"The task of video generation is an arduous one in the fields of Machine Learning and Computer Vision. This study focuses on the problem of generating videos based on textual input, which involves a conditional form of video generation. Humans possess the ability to comprehend natural language sentences and create mental images of the described scenario. Consequently, we believe that generating videos from such sentences will have a significant impact on Artificial Intelligence. Video generation is a relatively new area of research in Computer Vision, and its challenges are far from resolved. Recent works dealt with synthetic datasets or limited real datasets that incorporate only a few objects, scenes, and emotions. This study aims to generate videos from free-form sentences using more realistic datasets such as Actor and Action Dataset (A2D) or UCF101. We address the complex problem of video generation by regressing the latent representations of the first and last frames and utilizing a context-aware interpolation method to construct the latent representations of intermediate frames. We introduce a stacking ""upPooling"" block to sequentially generate RGB frames from each latent representation and progressively increase the resolution. Additionally, our proposed Discriminator encodes videos based on single or multiple frames. We present quantitative and qualitative results to support our claims and demonstrate the superiority of our approach over popular baselines such as Recurrent Neural Network (RNN) and Deconvolution (also known as Convolutional Transpose) based video generation methods.",1
"Softmax classifiers with a very large number of classes naturally occur in many applications such as natural language processing and information retrieval. The calculation of full softmax is costly from the computational and energy perspective. There have been various sampling approaches to overcome this challenge, popularly known as negative sampling (NS). Ideally, NS should sample negative classes from a distribution that is dependent on the input data, the current parameters, and the correct positive class. Unfortunately, due to the dynamically updated parameters and data samples, there is no sampling scheme that is provably adaptive and samples the negative classes efficiently. Therefore, alternative heuristics like random sampling, static frequency-based sampling, or learning-based biased sampling, which primarily trade either the sampling cost or the adaptivity of samples per iteration are adopted. In this paper, we show two classes of distributions where the sampling scheme is truly adaptive and provably generates negative samples in near-constant time. Our implementation in C++ on CPU is significantly superior, both in terms of wall-clock time and accuracy, compared to the most optimized TensorFlow implementations of other popular negative sampling approaches on powerful NVIDIA V100 GPU.",0
"Many applications, including natural language processing and information retrieval, require softmax classifiers with a large number of classes. However, the computational and energy cost of calculating full softmax can be significant. To address this challenge, negative sampling (NS) has been proposed. Ideally, NS should sample negative classes from a distribution that depends on the input data, current parameters, and positive class. However, due to the constantly changing parameters and data samples, no sampling scheme can efficiently and provably adapt to sample negative classes. Therefore, alternative heuristics such as random sampling, static frequency-based sampling, or learning-based biased sampling are used, which trade off either sampling cost or adaptivity per iteration. This paper introduces two adaptive sampling schemes that generate negative samples in near-constant time, which outperform other popular NS approaches in terms of both wall-clock time and accuracy. The implementation is in C++ on CPU.",1
"Image Captioning is a task that combines computer vision and natural language processing, where it aims to generate descriptive legends for images. It is a two-fold process relying on accurate image understanding and correct language understanding both syntactically and semantically. It is becoming increasingly difficult to keep up with the latest research and findings in the field of image captioning due to the growing amount of knowledge available on the topic. There is not, however, enough coverage of those findings in the available review papers. We perform in this paper a run-through of the current techniques, datasets, benchmarks and evaluation metrics used in image captioning. The current research on the field is mostly focused on deep learning-based methods, where attention mechanisms along with deep reinforcement and adversarial learning appear to be in the forefront of this research topic. In this paper, we review recent methodologies such as UpDown, OSCAR, VIVO, Meta Learning and a model that uses conditional generative adversarial nets. Although the GAN-based model achieves the highest score, UpDown represents an important basis for image captioning and OSCAR and VIVO are more useful as they use novel object captioning. This review paper serves as a roadmap for researchers to keep up to date with the latest contributions made in the field of image caption generation.",0
"The task of Image Captioning combines computer vision and natural language processing to generate descriptive captions for images. This process requires accurate understanding of both the image and language, including syntax and semantics. Keeping up with the latest research in this field is increasingly challenging due to the vast amount of available knowledge, and existing review papers do not provide sufficient coverage of recent findings. In this paper, we provide an overview of current techniques, datasets, benchmarks, and evaluation metrics used in Image Captioning. Deep learning-based methods, particularly those incorporating attention mechanisms, deep reinforcement, and adversarial learning, are the focus of recent research. We review several methodologies, including UpDown, OSCAR, VIVO, Meta Learning, and a conditional generative adversarial nets model, with the GAN-based model achieving the highest score. However, UpDown is a fundamental basis for Image Captioning, while OSCAR and VIVO are more useful due to their novel object captioning. This review paper serves as a roadmap for researchers to stay up-to-date with the latest contributions in Image Caption Generation.",1
"Image captioning is a task in the field of Artificial Intelligence that merges between computer vision and natural language processing. It is responsible for generating legends that describe images, and has various applications like descriptions used by assistive technology or indexing images (for search engines for instance). This makes it a crucial topic in AI that is undergoing a lot of research. This task however, like many others, is trained on large images labeled via human annotation, which can be very cumbersome: it needs manual effort, both financial and temporal costs, it is error-prone and potentially difficult to execute in some cases (e.g. medical images). To mitigate the need for labels, we attempt to use self-supervised learning, a type of learning where models use the data contained within the images themselves as labels. It is challenging to accomplish though, since the task is two-fold: the images and captions come from two different modalities and usually handled by different types of networks. It is thus not obvious what a completely self-supervised solution would look like. How it would achieve captioning in a comparable way to how self-supervision is applied today on image recognition tasks is still an ongoing research topic. In this project, we are using an encoder-decoder architecture where the encoder is a convolutional neural network (CNN) trained on OpenImages dataset and learns image features in a self-supervised fashion using the rotation pretext task. The decoder is a Long Short-Term Memory (LSTM), and it is trained, along within the image captioning model, on MS COCO dataset and is responsible of generating captions. Our GitHub repository can be found: https://github.com/elhagry1/SSL_ImageCaptioning_RotationPrediction",0
"In the field of Artificial Intelligence, image captioning combines computer vision and natural language processing to generate descriptive legends for various applications such as assistive technology or image indexing. However, the current method of training the model requires large images labeled through human annotation, which is costly, time-consuming, and prone to error. To overcome this issue, we explore self-supervised learning, where the model uses the image data itself as labels. This is a difficult task as images and captions are from different modalities and require different networks. In our project, we use an encoder-decoder architecture with a self-supervised convolutional neural network (CNN) trained on OpenImages dataset and a Long Short-Term Memory (LSTM) decoder trained on MS COCO dataset to generate captions. Our research on achieving self-supervised image captioning is ongoing and our GitHub repository can be found at https://github.com/elhagry1/SSL_ImageCaptioning_RotationPrediction.",1
"Recent advances in the areas of multimodal machine learning and artificial intelligence (AI) have led to the development of challenging tasks at the intersection of Computer Vision, Natural Language Processing, and Embodied AI. Whereas many approaches and previous survey pursuits have characterised one or two of these dimensions, there has not been a holistic analysis at the center of all three. Moreover, even when combinations of these topics are considered, more focus is placed on describing, e.g., current architectural methods, as opposed to also illustrating high-level challenges and opportunities for the field. In this survey paper, we discuss Embodied Vision-Language Planning (EVLP) tasks, a family of prominent embodied navigation and manipulation problems that jointly use computer vision and natural language. We propose a taxonomy to unify these tasks and provide an in-depth analysis and comparison of the new and current algorithmic approaches, metrics, simulated environments, as well as the datasets used for EVLP tasks. Finally, we present the core challenges that we believe new EVLP works should seek to address, and we advocate for task construction that enables model generalizability and furthers real-world deployment.",0
"Advancements in multimodal machine learning and artificial intelligence have given rise to complex tasks at the intersection of Computer Vision, Natural Language Processing, and Embodied AI. While previous studies have focused on one or two of these dimensions, there has been a lack of holistic analysis covering all three. Additionally, existing research has emphasized architectural methods instead of exploring high-level challenges and opportunities in the field. This survey paper discusses Embodied Vision-Language Planning (EVLP) tasks, which involve embodied navigation and manipulation problems that utilize computer vision and natural language. The paper proposes a taxonomy to unify these tasks and provides an in-depth analysis and comparison of algorithmic approaches, metrics, simulated environments, and datasets used for EVLP tasks. The paper also highlights core challenges for new EVLP works and emphasizes the importance of task construction that promotes model generalizability and real-world deployment.",1
"Image manipulation with natural language, which aims to manipulate images with the guidance of language descriptions, has been a challenging problem in the fields of computer vision and natural language processing (NLP). Currently, a number of efforts have been made for this task, but their performances are still distant away from generating realistic and text-conformed manipulated images. Therefore, in this paper, we propose a memory-based Image Manipulation Network (MIM-Net), where a set of memories learned from images is introduced to synthesize the texture information with the guidance of the textual description. We propose a two-stage network with an additional reconstruction stage to learn the latent memories efficiently. To avoid the unnecessary background changes, we propose a Target Localization Unit (TLU) to focus on the manipulation of the region mentioned by the text. Moreover, to learn a robust memory, we further propose a novel randomized memory training loss. Experiments on the four popular datasets show the better performance of our method compared to the existing ones.",0
"Manipulating images using natural language has been a challenging task in the fields of computer vision and natural language processing. Although there have been some attempts to achieve this, the results have been far from satisfactory in terms of generating realistic manipulated images that match the text descriptions. This paper proposes a Memory-based Image Manipulation Network (MIM-Net) that utilizes a set of previously learned memories from images to synthesize texture information according to the text description. The proposed network consists of two stages, including an additional reconstruction stage for efficient memory learning. A Target Localization Unit (TLU) is introduced to focus on the manipulation of the region mentioned in the text, while a novel randomized memory training loss is proposed to train a more robust memory. Experimental results on four popular datasets demonstrate the superior performance of our method over existing ones.",1
"We introduce a method that allows to automatically segment images into semantically meaningful regions without human supervision. Derived regions are consistent across different images and coincide with human-defined semantic classes on some datasets. In cases where semantic regions might be hard for human to define and consistently label, our method is still able to find meaningful and consistent semantic classes. In our work, we use pretrained StyleGAN2~\cite{karras2020analyzing} generative model: clustering in the feature space of the generative model allows to discover semantic classes. Once classes are discovered, a synthetic dataset with generated images and corresponding segmentation masks can be created. After that a segmentation model is trained on the synthetic dataset and is able to generalize to real images. Additionally, by using CLIP~\cite{radford2021learning} we are able to use prompts defined in a natural language to discover some desired semantic classes. We test our method on publicly available datasets and show state-of-the-art results.",0
"We present a technique for segmenting images into meaningful regions based on semantics, without the need for human input. The resulting regions are consistent across various images and align with human-defined semantic categories in some datasets. Our method is capable of identifying meaningful and consistent semantic classes even in cases where it may be challenging for humans to define and consistently label these regions. Our approach utilizes a pre-trained StyleGAN2 generative model, which facilitates clustering in the feature space to detect semantic classes. This discovery is then used to generate a synthetic dataset with segmentation masks, which is subsequently employed to train a segmentation model. This model can then generalize to real images. Furthermore, by utilizing CLIP, we can use natural language prompts to discover desired semantic classes. We evaluate our method on publicly available datasets and achieve state-of-the-art results.",1
"Self-supervised pre-training of large-scale transformer models on text corpora followed by finetuning has achieved state-of-the-art on a number of natural language processing tasks. Recently, Lu et al. (2021, arXiv:2103.05247) claimed that frozen pretrained transformers (FPTs) match or outperform training from scratch as well as unfrozen (fine-tuned) pretrained transformers in a set of transfer tasks to other modalities. In our work, we find that this result is, in fact, an artifact of not tuning the learning rates. After carefully redesigning the empirical setup, we find that when tuning learning rates properly, pretrained transformers do outperform or match training from scratch in all of our tasks, but only as long as the entire model is finetuned. Thus, while transfer from pretrained language models to other modalities does indeed provide gains and hints at exciting possibilities for future work, properly tuning hyperparameters is important for arriving at robust findings.",0
"Recent studies have shown that pre-training large-scale transformer models on text corpora followed by finetuning has achieved state-of-the-art results in natural language processing tasks. Lu et al. (2021, arXiv:2103.05247) claimed that frozen pretrained transformers (FPTs) perform as well as or better than models trained from scratch or unfrozen pretrained transformers in transfer tasks to other modalities. However, our research discovered that this conclusion was due to improper tuning of learning rates. By redesigning the experimental setup and properly tuning the hyperparameters, we found that pretrained transformers do outperform or match training from scratch in all tasks, but only when the entire model is finetuned. Therefore, while transfer learning shows promise for future work, it is important to properly tune hyperparameters to arrive at robust findings.",1
"Transformer with self-attention has led to the revolutionizing of natural language processing field, and recently inspires the emergence of Transformer-style architecture design with competitive results in numerous computer vision tasks. Nevertheless, most of existing designs directly employ self-attention over a 2D feature map to obtain the attention matrix based on pairs of isolated queries and keys at each spatial location, but leave the rich contexts among neighbor keys under-exploited. In this work, we design a novel Transformer-style module, i.e., Contextual Transformer (CoT) block, for visual recognition. Such design fully capitalizes on the contextual information among input keys to guide the learning of dynamic attention matrix and thus strengthens the capacity of visual representation. Technically, CoT block first contextually encodes input keys via a $3\times3$ convolution, leading to a static contextual representation of inputs. We further concatenate the encoded keys with input queries to learn the dynamic multi-head attention matrix through two consecutive $1\times1$ convolutions. The learnt attention matrix is multiplied by input values to achieve the dynamic contextual representation of inputs. The fusion of the static and dynamic contextual representations are finally taken as outputs. Our CoT block is appealing in the view that it can readily replace each $3\times3$ convolution in ResNet architectures, yielding a Transformer-style backbone named as Contextual Transformer Networks (CoTNet). Through extensive experiments over a wide range of applications (e.g., image recognition, object detection and instance segmentation), we validate the superiority of CoTNet as a stronger backbone. Source code is available at \url{https://github.com/JDAI-CV/CoTNet}.",0
"The introduction of self-attention in transformers has revolutionized the field of natural language processing and has recently inspired the creation of transformer-style architecture designs that show competitive results in many computer vision tasks. However, most designs currently in use employ self-attention over a 2D feature map to obtain an attention matrix based on isolated queries and keys at each spatial location, which leaves the rich contextual information among neighboring keys largely untapped. To address this, we have developed a novel transformer-style module called the Contextual Transformer (CoT) block for visual recognition. The CoT block makes full use of the contextual information among input keys to guide the learning of a dynamic attention matrix, thus strengthening the capacity of visual representation. The CoT block first contextually encodes input keys through a 3x3 convolution to achieve a static contextual representation. We then concatenate the encoded keys with input queries to learn the dynamic multi-head attention matrix through two consecutive 1x1 convolutions. The attention matrix is then multiplied by input values to achieve the dynamic contextual representation of inputs, and the fusion of static and dynamic contextual representations is taken as outputs. Our CoT block can replace each 3x3 convolution in ResNet architectures to create a transformer-style backbone called Contextual Transformer Networks (CoTNet). Through extensive experimentation on a range of applications, such as image recognition, object detection, and instance segmentation, we have shown that CoTNet is a stronger backbone. The source code for CoTNet is available at https://github.com/JDAI-CV/CoTNet.",1
"When humans solve complex problems, they rarely come up with a decision right-away. Instead, they start with an intuitive decision, reflect upon it, spot mistakes, resolve contradictions and jump between different hypotheses. Thus, they create a sequence of ideas and follow a train of thought that ultimately reaches a conclusive decision. Contrary to this, today's neural classification models are mostly trained to map an input to one single and fixed output. In this paper, we investigate how we can give models the opportunity of a second, third and $k$-th thought. We take inspiration from Hegel's dialectics and propose a method that turns an existing classifier's class prediction (such as the image class forest) into a sequence of predictions (such as forest $\rightarrow$ tree $\rightarrow$ mushroom). Concretely, we propose a correction module that is trained to estimate the model's correctness as well as an iterative prediction update based on the prediction's gradient. Our approach results in a dynamic system over class probability distributions $\unicode{x2014}$ the thought flow. We evaluate our method on diverse datasets and tasks from computer vision and natural language processing. We observe surprisingly complex but intuitive behavior and demonstrate that our method (i) can correct misclassifications, (ii) strengthens model performance, (iii) is robust to high levels of adversarial attacks, (iv) can increase accuracy up to 4% in a label-distribution-shift setting and (iv) provides a tool for model interpretability that uncovers model knowledge which otherwise remains invisible in a single distribution prediction.",0
"Humans do not usually arrive at a solution for complex problems right away. They typically begin with an intuitive decision, reflect on it, identify mistakes, resolve contradictions, and switch between various hypotheses, thereby creating a sequence of ideas that leads to a conclusive decision. However, modern neural classification models are primarily trained to map an input to one fixed output. This paper explores how to give models the ability to generate second, third, and k-th thoughts. Drawing inspiration from Hegel's dialectics, we propose a method that transforms a classifier's class prediction into a prediction sequence, such as forest to tree to mushroom. Our approach involves a correction module that estimates the model's correctness and an iterative prediction update based on the prediction's gradient. Our method results in a dynamic system over class probability distributions. We evaluate our approach on diverse datasets and tasks from computer vision and natural language processing and observe impressive and intuitive behavior. Our method can correct misclassifications, enhance model performance, withstand high levels of adversarial attacks, increase accuracy up to 4% in a label-distribution-shift setting, and provide a tool for model interpretability that reveals hidden model knowledge.",1
"We describe an efficient hierarchical method to compute attention in the Transformer architecture. The proposed attention mechanism exploits a matrix structure similar to the Hierarchical Matrix (H-Matrix) developed by the numerical analysis community, and has linear run time and memory complexity. We perform extensive experiments to show that the inductive bias embodied by our hierarchical attention is effective in capturing the hierarchical structure in the sequences typical for natural language and vision tasks. Our method is superior to alternative sub-quadratic proposals by over +6 points on average on the Long Range Arena benchmark. It also sets a new SOTA test perplexity on One-Billion Word dataset with 5x fewer model parameters than that of the previous-best Transformer-based models.",0
"An effective method for computing attention in the Transformer architecture is presented, utilizing a hierarchical approach. The proposed attention mechanism employs a matrix structure similar to the Hierarchical Matrix developed by the numerical analysis community, resulting in linear run time and memory complexity. Through extensive experiments, it is demonstrated that the hierarchical attention effectively captures the hierarchical structure in natural language and vision tasks. Compared to alternative sub-quadratic proposals, our method outperforms by over 6 points on average on the Long Range Arena benchmark and achieves a new state-of-the-art test perplexity on the One-Billion Word dataset with 5 times fewer model parameters than the previous-best Transformer-based models.",1
"This paper is a presentation of a new method for denoising images using Haralick features and further segmenting the characters using artificial neural networks. The image is divided into kernels, each of which is converted to a GLCM (Gray Level Co-Occurrence Matrix) on which a Haralick Feature generation function is called, the result of which is an array with fourteen elements corresponding to fourteen features The Haralick values and the corresponding noise/text classification form a dictionary, which is then used to de-noise the image through kernel comparison. Segmentation is the process of extracting characters from a document and can be used when letters are separated by white space, which is an explicit boundary marker. Segmentation is the first step in many Natural Language Processing problems. This paper explores the process of segmentation using Neural Networks. While there have been numerous methods to segment characters of a document, this paper is only concerned with the accuracy of doing so using neural networks. It is imperative that the characters be segmented correctly, for failing to do so will lead to incorrect recognition by Natural language processing tools. Artificial Neural Networks was used to attain accuracy of upto 89%. This method is suitable for languages where the characters are delimited by white space. However, this method will fail to provide acceptable results when the language heavily uses connected letters. An example would be the Devanagari script, which is predominantly used in northern India.",0
"This article presents a novel approach to denoising images through the utilization of Haralick features and artificial neural networks for character segmentation. The process involves dividing the image into kernels and converting each kernel into a Gray Level Co-Occurrence Matrix (GLCM). A Haralick Feature generation function is called to generate an array of fourteen elements corresponding to fourteen features. The resulting Haralick values and their associated noise/text classification are stored in a dictionary and used to de-noise the image by comparing kernels. Segmentation, which is the extraction of characters from a document, is crucial for many Natural Language Processing problems and is explored in this paper using Neural Networks. Although there are many methods for character segmentation, this paper focuses solely on the accuracy of using neural networks. The accuracy achieved through the use of Artificial Neural Networks was up to 89%. This approach is suitable for languages with white space as a delimiter; however, it may not perform well for languages with connected letters, such as the Devanagari script, commonly used in Northern India.",1
"Vision and Language Navigation (VLN) requires an agent to navigate to a target location by following natural language instructions. Most of existing works represent a navigation candidate by the feature of the corresponding single view where the candidate lies in. However, an instruction may mention landmarks out of the single view as references, which might lead to failures of textual-visual matching of existing methods. In this work, we propose a multi-module Neighbor-View Enhanced Model (NvEM) to adaptively incorporate visual contexts from neighbor views for better textual-visual matching. Specifically, our NvEM utilizes a subject module and a reference module to collect contexts from neighbor views. The subject module fuses neighbor views at a global level, and the reference module fuses neighbor objects at a local level. Subjects and references are adaptively determined via attention me'chanisms. Our model also includes an action module to utilize the strong orientation guidance (e.g., ""turn left"") in instructions. Each module predicts navigation action separately and their weighted sum is used for predicting the final action. Extensive experimental results demonstrate the effectiveness of the proposed method on the R2R and R4R benchmarks against several state-of-the-art navigators, and NvEM even beats some pre-training ones. Our code is available at https://github.com/MarSaKi/NvEM.",0
"The task of Vision and Language Navigation (VLN) involves guiding an agent to a specific location using natural language instructions. Many existing methods rely on representing a navigation candidate based on the features of the single view in which it is located. However, this approach may fail when instructions refer to landmarks outside of the current view. To address this limitation, we propose a Neighbor-View Enhanced Model (NvEM) that can incorporate visual contexts from neighboring views to improve textual-visual matching. Our NvEM consists of three modules: a subject module, a reference module, and an action module. The subject module fuses visual information from neighboring views at a global level, while the reference module focuses on local objects in neighboring views. Subjects and references are selected using attention mechanisms. The action module utilizes orientation guidance in the instructions to predict the navigation actions. Our model predicts navigation actions separately for each module, and the weighted sum of these predictions is used for the final action. We evaluate our model on the R2R and R4R benchmarks and show that it outperforms several state-of-the-art navigation models, including some that use pre-training. Our code is available on GitHub at https://github.com/MarSaKi/NvEM.",1
"Language instruction plays an essential role in the natural language grounded navigation tasks. However, navigators trained with limited human-annotated instructions may have difficulties in accurately capturing key information from the complicated instruction at different timesteps, leading to poor navigation performance. In this paper, we exploit to train a more robust navigator which is capable of dynamically extracting crucial factors from the long instruction, by using an adversarial attacking paradigm. Specifically, we propose a Dynamic Reinforced Instruction Attacker (DR-Attacker), which learns to mislead the navigator to move to the wrong target by destroying the most instructive information in instructions at different timesteps. By formulating the perturbation generation as a Markov Decision Process, DR-Attacker is optimized by the reinforcement learning algorithm to generate perturbed instructions sequentially during the navigation, according to a learnable attack score. Then, the perturbed instructions, which serve as hard samples, are used for improving the robustness of the navigator with an effective adversarial training strategy and an auxiliary self-supervised reasoning task. Experimental results on both Vision-and-Language Navigation (VLN) and Navigation from Dialog History (NDH) tasks show the superiority of our proposed method over state-of-the-art methods. Moreover, the visualization analysis shows the effectiveness of the proposed DR-Attacker, which can successfully attack crucial information in the instructions at different timesteps. Code is available at https://github.com/expectorlin/DR-Attacker.",0
"The role of language instruction is crucial for natural language-based navigation tasks, but navigators trained with limited human-annotated instructions may struggle to capture key information from complex instructions at different points in time. This can result in poor navigation performance. To address this, our paper proposes using an adversarial attacking paradigm to train a more robust navigator capable of dynamically extracting vital factors from lengthy instructions. We introduce the Dynamic Reinforced Instruction Attacker (DR-Attacker), which learns to mislead the navigator by destroying the most instructive information in instructions at various timesteps. DR-Attacker generates perturbed instructions sequentially during navigation, using reinforcement learning to optimize the process and generate hard samples for adversarial training. Our approach outperforms state-of-the-art methods on both Vision-and-Language Navigation (VLN) and Navigation from Dialog History (NDH) tasks. Visualization analysis demonstrates the effectiveness of DR-Attacker in successfully attacking crucial information in instructions. The code is available at https://github.com/expectorlin/DR-Attacker.",1
"We define disentanglement as how far class-different data points from each other are, relative to the distances among class-similar data points. When maximizing disentanglement during representation learning, we obtain a transformed feature representation where the class memberships of the data points are preserved. If the class memberships of the data points are preserved, we would have a feature representation space in which a nearest neighbour classifier or a clustering algorithm would perform well. We take advantage of this method to learn better natural language representation, and employ it on text classification and text clustering tasks. Through disentanglement, we obtain text representations with better-defined clusters and improve text classification performance. Our approach had a test classification accuracy of as high as 90.11% and test clustering accuracy of 88% on the AG News dataset, outperforming our baseline models -- without any other training tricks or regularization.",0
"Disentanglement is defined by the relative distances between data points of different classes compared to those of the same class. By maximizing disentanglement in representation learning, we can preserve the class memberships of the data points, resulting in a feature representation space that is conducive to well-performing nearest neighbor classifiers or clustering algorithms. This approach has been applied to natural language representation learning, specifically in text classification and clustering tasks, resulting in better-defined clusters and improved classification performance. Our approach achieved a high test classification accuracy of 90.11% and test clustering accuracy of 88% on the AG News dataset, outperforming baseline models without additional training tricks or regularization.",1
"We present a new four-pronged approach to build firefighter's situational awareness for the first time in the literature. We construct a series of deep learning frameworks built on top of one another to enhance the safety, efficiency, and successful completion of rescue missions conducted by firefighters in emergency first response settings. First, we used a deep Convolutional Neural Network (CNN) system to classify and identify objects of interest from thermal imagery in real-time. Next, we extended this CNN framework for object detection, tracking, segmentation with a Mask RCNN framework, and scene description with a multimodal natural language processing(NLP) framework. Third, we built a deep Q-learning-based agent, immune to stress-induced disorientation and anxiety, capable of making clear navigation decisions based on the observed and stored facts in live-fire environments. Finally, we used a low computational unsupervised learning technique called tensor decomposition to perform meaningful feature extraction for anomaly detection in real-time. With these ad-hoc deep learning structures, we built the artificial intelligence system's backbone for firefighters' situational awareness. To bring the designed system into usage by firefighters, we designed a physical structure where the processed results are used as inputs in the creation of an augmented reality capable of advising firefighters of their location and key features around them, which are vital to the rescue operation at hand, as well as a path planning feature that acts as a virtual guide to assist disoriented first responders in getting back to safety. When combined, these four approaches present a novel approach to information understanding, transfer, and synthesis that could dramatically improve firefighter response and efficacy and reduce life loss.",0
"Our research introduces a novel approach to enhancing firefighter's situational awareness through a four-pronged strategy that has not been explored in the literature before. We leverage a series of deep learning frameworks that aim to enhance safety, efficiency, and increase the likelihood of successful rescue missions carried out by firefighters in emergency first response settings. Firstly, we employ a deep Convolutional Neural Network (CNN) system that can classify and identify objects of interest from thermal imagery in real-time. Subsequently, we extend this CNN framework by integrating object detection, tracking, segmentation with a Mask RCNN framework, and scene description with a multimodal natural language processing (NLP) framework. Thirdly, we develop a deep Q-learning-based agent that is impervious to stress-induced disorientation and anxiety, capable of making clear navigation decisions based on the observed and stored facts in live-fire environments. Finally, we use a low computational unsupervised learning technique called tensor decomposition to perform meaningful feature extraction for real-time anomaly detection. These ad-hoc deep learning structures form the backbone of our artificial intelligence system that can enhance firefighters' situational awareness. To facilitate the use of our designed system by firefighters, we developed a physical structure that uses the processed results as inputs to create an augmented reality capable of advising firefighters of their location and key features around them, which are crucial to the rescue operation at hand, as well as a path planning feature that acts as a virtual guide to assist disoriented first responders in returning to safety. By combining these four approaches, we present a novel strategy to information understanding, transfer, and synthesis that has the potential to significantly improve firefighter response and efficacy and reduce life loss.",1
"Deep learning (DL) techniques have achieved great success in predictive accuracy in a variety of tasks, but deep neural networks (DNNs) are shown to produce highly overconfident scores for even abnormal samples. Well-defined uncertainty indicates whether a model's output should (or should not) be trusted and thus becomes critical in real-world scenarios which typically involves shifted input distributions due to many factors. Existing uncertainty approaches assume that testing samples from a different data distribution would induce unreliable model predictions thus have higher uncertainty scores. They quantify model uncertainty by calibrating DL model's confidence of a given input and evaluate the effectiveness in computer vision (CV) and natural language processing (NLP)-related tasks. However, their methodologies' reliability may be compromised under programming tasks due to difference in data representations and shift patterns. In this paper, we first define three different types of distribution shift in program data and build a large-scale shifted Java dataset. We implement two common programming language tasks on our dataset to study the effect of each distribution shift on DL model performance. We also propose a large-scale benchmark of existing state-of-the-art predictive uncertainty on programming tasks and investigate their effectiveness under data distribution shift. Experiments show that program distribution shift does degrade the DL model performance to varying degrees and that existing uncertainty methods all present certain limitations in quantifying uncertainty on program dataset.",0
"Predictive accuracy has been greatly improved by deep learning (DL) techniques, but deep neural networks (DNNs) can produce overconfident scores for abnormal samples. Real-world scenarios involve shifted input distributions, and well-defined uncertainty is critical for determining whether a model's output can be trusted. Existing uncertainty approaches assume that testing samples from different data distributions would lead to unreliable model predictions and thus have higher uncertainty scores. They quantify model uncertainty by calibrating DL model confidence and evaluating effectiveness in computer vision (CV) and natural language processing (NLP). However, these methodologies' reliability may be compromised under programming tasks due to differences in data representations and shift patterns. This paper defines three types of distribution shift in program data and builds a large-scale Java dataset. Two common programming language tasks are implemented to study the effect of each distribution shift on DL model performance. A large-scale benchmark of existing state-of-the-art predictive uncertainty on programming tasks is proposed, and their effectiveness under data distribution shift is investigated. The experiments show that program distribution shift degrades DL model performance to varying degrees and that existing uncertainty methods present limitations in quantifying uncertainty on program datasets.",1
"Faces generated using generative adversarial networks (GANs) have reached unprecedented realism. These faces, also known as ""Deep Fakes"", appear as realistic photographs with very little pixel-level distortions. While some work has enabled the training of models that lead to the generation of specific properties of the subject, generating a facial image based on a natural language description has not been fully explored. For security and criminal identification, the ability to provide a GAN-based system that works like a sketch artist would be incredibly useful. In this paper, we present a novel approach to generate facial images from semantic text descriptions. The learned model is provided with a text description and an outline of the type of face, which the model uses to sketch the features. Our models are trained using an Affine Combination Module (ACM) mechanism to combine the text embedding from BERT and the GAN latent space using a self-attention matrix. This avoids the loss of features due to inadequate ""attention"", which may happen if text embedding and latent vector are simply concatenated. Our approach is capable of generating images that are very accurately aligned to the exhaustive textual descriptions of faces with many fine detail features of the face and helps in generating better images. The proposed method is also capable of making incremental changes to a previously generated image if it is provided with additional textual descriptions or sentences.",0
"Unprecedented realism has been achieved in faces produced by generative adversarial networks (GANs), also known as ""Deep Fakes"". These images resemble authentic photographs with minimal distortion at the pixel level. Although models have been developed to generate specific properties of a subject, the potential of generating facial images based on natural language descriptions remains largely unexplored. A GAN-based system that functions like a sketch artist would be invaluable for security and criminal identification purposes. The present study introduces a new approach to creating facial images from semantic text descriptions. The model utilizes a combination of the BERT text embedding and the GAN latent space through a self-attention matrix, which is achieved by an Affine Combination Module (ACM) mechanism. This ensures that features are not lost due to inadequate attention, as can occur when text embedding and latent vectors are simply concatenated. Our approach produces highly detailed facial images that accurately match exhaustive textual descriptions and can be refined by additional textual input.",1
"Deep learning's success has been widely recognized in a variety of machine learning tasks, including image classification, audio recognition, and natural language processing. As an extension of deep learning beyond these domains, graph neural networks (GNNs) are designed to handle the non-Euclidean graph-structure which is intractable to previous deep learning techniques. Existing GNNs are presented using various techniques, making direct comparison and cross-reference more complex. Although existing studies categorize GNNs into spatial-based and spectral-based techniques, there hasn't been a thorough examination of their relationship. To close this gap, this study presents a single framework that systematically incorporates most GNNs. We organize existing GNNs into spatial and spectral domains, as well as expose the connections within each domain. A review of spectral graph theory and approximation theory builds a strong relationship across the spatial and spectral domains in further investigation.",0
"The success of deep learning has been widely acknowledged in different machine learning tasks such as image classification, audio recognition, and natural language processing. Graph neural networks (GNNs) have been developed as an extension of deep learning to handle non-Euclidean graph-structure that previous deep learning techniques could not handle. However, existing GNNs have been presented using various techniques, making it difficult to compare them directly. Although the categorization of GNNs into spatial-based and spectral-based techniques exists, no thorough examination has been made on their relationship. To address this gap, this study presents a single framework that systematically includes most GNNs. Existing GNNs are organized into spatial and spectral domains, and connections within each domain are exposed. Through a review of spectral graph theory and approximation theory, a strong relationship is established across the spatial and spectral domains, which further investigation can build upon.",1
"Communication between agents in collaborative multi-agent settings is in general implicit or a direct data stream. This paper considers text-based natural language as a novel form of communication between multiple agents trained with reinforcement learning. This could be considered first steps toward a truly autonomous communication without the need to define a limited set of instructions, and natural collaboration between humans and robots. Inspired by the game of Blind Leads, we propose an environment where one agent uses natural language instructions to guide another through a maze. We test the ability of reinforcement learning agents to effectively communicate through discrete word-level symbols and show that the agents are able to sufficiently communicate through natural language with a limited vocabulary. Although the communication is not always perfect English, the agents are still able to navigate the maze. We achieve a BLEU score of 0.85, which is an improvement of 0.61 over randomly generated sequences while maintaining a 100% maze completion rate. This is a 3.5 times the performance of the random baseline using our reference set.",0
"In collaborative multi-agent settings, communication between agents is typically implicit or in the form of a direct data stream. However, this paper introduces a new approach to communication using text-based natural language, which allows for truly autonomous communication without the need for a limited set of instructions. This approach also promotes natural collaboration between humans and robots. Inspired by the game of Blind Leads, the paper proposes an environment where one agent guides another through a maze using natural language instructions. The study evaluates the effectiveness of reinforcement learning agents in communicating through discrete word-level symbols and demonstrates that the agents are able to navigate the maze with a limited vocabulary. While the communication may not always be perfect English, the agents still achieve a 100% maze completion rate and a BLEU score of 0.85 – an improvement of 0.61 over randomly generated sequences. Our reference set also shows a 3.5 times better performance than the random baseline.",1
"Reinforcement learning (RL) is typically concerned with estimating single-step policies or single-step models, leveraging the Markov property to factorize the problem in time. However, we can also view RL as a sequence modeling problem, with the goal being to predict a sequence of actions that leads to a sequence of high rewards. Viewed in this way, it is tempting to consider whether powerful, high-capacity sequence prediction models that work well in other domains, such as natural-language processing, can also provide simple and effective solutions to the RL problem. To this end, we explore how RL can be reframed as ""one big sequence modeling"" problem, using state-of-the-art Transformer architectures to model distributions over sequences of states, actions, and rewards. Addressing RL as a sequence modeling problem significantly simplifies a range of design decisions: we no longer require separate behavior policy constraints, as is common in prior work on offline model-free RL, and we no longer require ensembles or other epistemic uncertainty estimators, as is common in prior work on model-based RL. All of these roles are filled by the same Transformer sequence model. In our experiments, we demonstrate the flexibility of this approach across long-horizon dynamics prediction, imitation learning, goal-conditioned RL, and offline RL.",0
"Reinforcement learning (RL) typically involves estimating policies or models for a single step, using the Markov property to break down the problem over time. However, RL can also be seen as a problem of modeling sequences to predict actions leading to high rewards. This perspective raises the question of whether sequence prediction models, which are effective in other domains such as natural language processing, can provide solutions to RL problems. In this study, we explore how RL can be reframed as a single large sequence modeling problem using state-of-the-art Transformer architectures. This approach simplifies design decisions as behavior policy constraints and epistemic uncertainty estimators are no longer required. Our experiments demonstrate the flexibility of this approach in various RL scenarios, including long-horizon dynamics prediction, imitation learning, goal-conditioned RL, and offline RL.",1
"Developing video understanding intelligence is quite challenging because it requires holistic integration of images, scripts, and sounds based on natural language processing, temporal dependency, and reasoning. Recently, substantial attempts have been made on several video datasets with associated question answering (QA) on a large scale. However, existing evaluation metrics for video question answering (VideoQA) do not provide meaningful analysis. To make progress, we argue that a well-made framework, established on the way humans understand, is required to explain and evaluate the performance of understanding in detail. Then we propose a top-down evaluation system for VideoQA, based on the cognitive process of humans and story elements: Cognitive Modules for Evaluation (CogME). CogME is composed of three cognitive modules: targets, contents, and thinking. The interaction among the modules in the understanding procedure can be expressed in one sentence as follows: ""I understand the CONTENT of the TARGET through a way of THINKING."" Each module has sub-components derived from the story elements. We can specify the required aspects of understanding by annotating the sub-components to individual questions. CogME thus provides a framework for an elaborated specification of VideoQA datasets. To examine the suitability of a VideoQA dataset for validating video understanding intelligence, we evaluated the baseline model of the DramaQA dataset by applying CogME. The evaluation reveals that story elements are unevenly reflected in the existing dataset, and the model based on the dataset may cause biased predictions. Although this study has only been able to grasp a narrow range of stories, we expect that it offers the first step in considering the cognitive process of humans on the video understanding intelligence of humans and AI.",0
"The development of video understanding intelligence is a difficult task that requires the integration of images, scripts, and sounds using natural language processing, temporal dependency, and reasoning. Although there have been significant efforts to create video datasets with associated question answering (QA), the existing evaluation metrics for VideoQA are inadequate. To address this issue, we propose a top-down evaluation system called Cognitive Modules for Evaluation (CogME), which is based on the cognitive process of humans and story elements. CogME consists of three cognitive modules: targets, contents, and thinking, and provides a framework for an elaborated specification of VideoQA datasets. By annotating sub-components to individual questions, we can specify the required aspects of understanding. We evaluated the baseline model of the DramaQA dataset using CogME and found that story elements are unevenly reflected in the existing dataset, which may cause biased predictions. This study is a first step towards considering the cognitive process of humans and AI in video understanding intelligence.",1
"Detecting customized moments and highlights from videos given natural language (NL) user queries is an important but under-studied topic. One of the challenges in pursuing this direction is the lack of annotated data. To address this issue, we present the Query-based Video Highlights (QVHighlights) dataset. It consists of over 10,000 YouTube videos, covering a wide range of topics, from everyday activities and travel in lifestyle vlog videos to social and political activities in news videos. Each video in the dataset is annotated with: (1) a human-written free-form NL query, (2) relevant moments in the video w.r.t. the query, and (3) five-point scale saliency scores for all query-relevant clips. This comprehensive annotation enables us to develop and evaluate systems that detect relevant moments as well as salient highlights for diverse, flexible user queries. We also present a strong baseline for this task, Moment-DETR, a transformer encoder-decoder model that views moment retrieval as a direct set prediction problem, taking extracted video and query representations as inputs and predicting moment coordinates and saliency scores end-to-end. While our model does not utilize any human prior, we show that it performs competitively when compared to well-engineered architectures. With weakly supervised pretraining using ASR captions, Moment-DETR substantially outperforms previous methods. Lastly, we present several ablations and visualizations of Moment-DETR. Data and code is publicly available at https://github.com/jayleicn/moment_detr",0
"The identification of personalized moments and highlights from videos based on natural language user queries is a significant yet under-researched area. A major challenge in pursuing this direction is the unavailability of annotated data. To address this issue, we have introduced the Query-based Video Highlights (QVHighlights) dataset, which comprises more than 10,000 YouTube videos representing various topics, from everyday activities to political events. Each video in the dataset is labeled with a human-written free-form NL query, relevant moments in the video with respect to the query, and a five-point scale saliency score for all query-relevant clips. Such comprehensive annotation allows us to develop and assess systems that detect relevant moments and salient highlights for diverse user queries. Additionally, we present a strong baseline for this task, Moment-DETR, which is a transformer encoder-decoder model that views moment retrieval as a direct set prediction problem and performs competitively compared to well-engineered architectures. We also demonstrate that Moment-DETR outperforms previous methods with weakly supervised pretraining using ASR captions. Finally, we provide various ablations and visualizations of Moment-DETR. The data and code are publicly available at https://github.com/jayleicn/moment_detr.",1
"We develop a novel approach to conformal prediction when the target task has limited data available for training. Conformal prediction identifies a small set of promising output candidates in place of a single prediction, with guarantees that the set contains the correct answer with high probability. When training data is limited, however, the predicted set can easily become unusably large. In this work, we obtain substantially tighter prediction sets while maintaining desirable marginal guarantees by casting conformal prediction as a meta-learning paradigm over exchangeable collections of auxiliary tasks. Our conformalization algorithm is simple, fast, and agnostic to the choice of underlying model, learning algorithm, or dataset. We demonstrate the effectiveness of this approach across a number of few-shot classification and regression tasks in natural language processing, computer vision, and computational chemistry for drug discovery.",0
"Our innovative approach to conformal prediction addresses situations where there is insufficient data to train the target task. Conformal prediction offers a small set of output options instead of a single prediction, with a high probability that the correct answer is included. Unfortunately, a limited dataset can result in an impractical number of predicted options. Our solution involves utilizing conformal prediction as a meta-learning model for exchangeable auxiliary tasks. This approach yields significantly more precise prediction sets while maintaining the desired level of accuracy. Our algorithm is uncomplicated, efficient, and adaptable to any model, learning algorithm, or dataset. We successfully apply our method to various few-shot classification and regression tasks in natural language processing, computer vision, and computational chemistry for drug discovery.",1
"Transformers have been successful for many natural language processing tasks. However, applying transformers to the video domain for tasks such as long-term video generation and scene understanding has remained elusive due to the high computational complexity and the lack of natural tokenization. In this paper, we propose the Object-Centric Video Transformer (OCVT) which utilizes an object-centric approach for decomposing scenes into tokens suitable for use in a generative video transformer. By factoring the video into objects, our fully unsupervised model is able to learn complex spatio-temporal dynamics of multiple interacting objects in a scene and generate future frames of the video. Our model is also significantly more memory-efficient than pixel-based models and thus able to train on videos of length up to 70 frames with a single 48GB GPU. We compare our model with previous RNN-based approaches as well as other possible video transformer baselines. We demonstrate OCVT performs well when compared to baselines in generating future frames. OCVT also develops useful representations for video reasoning, achieving start-of-the-art performance on the CATER task.",0
"Although transformers have proven to be effective for many natural language processing tasks, their application to the video domain for long-term video generation and scene comprehension has been challenging due to the complexity of computation and the absence of natural tokenization. In this study, we introduce the Object-Centric Video Transformer (OCVT) which utilizes an object-centric approach to break down scenes into tokens that are suitable for use in a generative video transformer. By breaking the video into objects, our unsupervised model can learn intricate spatio-temporal dynamics of multiple interacting objects in a scene and produce future frames of the video. Additionally, our model uses significantly less memory than pixel-based models, enabling it to train on videos up to 70 frames long with just one 48GB GPU. We compare our model with previous RNN-based approaches and other possible video transformer baselines and demonstrate that OCVT performs well in generating future frames. OCVT also produces useful representations for video reasoning, achieving state-of-the-art performance on the CATER task.",1
"Video captioning, i.e. the task of generating captions from video sequences creates a bridge between the Natural Language Processing and Computer Vision domains of computer science. The task of generating a semantically accurate description of a video is quite complex. Considering the complexity, of the problem, the results obtained in recent research works are praiseworthy. However, there is plenty of scope for further investigation. This paper addresses this scope and proposes a novel solution. Most video captioning models comprise two sequential/recurrent layers - one as a video-to-context encoder and the other as a context-to-caption decoder. This paper proposes a novel architecture, namely Semantically Sensible Video Captioning (SSVC) which modifies the context generation mechanism by using two novel approaches - ""stacked attention"" and ""spatial hard pull"". As there are no exclusive metrics for evaluating video captioning models, we emphasize both quantitative and qualitative analysis of our model. Hence, we have used the BLEU scoring metric for quantitative analysis and have proposed a human evaluation metric for qualitative analysis, namely the Semantic Sensibility (SS) scoring metric. SS Score overcomes the shortcomings of common automated scoring metrics. This paper reports that the use of the aforementioned novelties improves the performance of state-of-the-art architectures.",0
"The process of creating captions from video sequences, known as video captioning, serves as a link between the fields of Natural Language Processing and Computer Vision in computer science. Generating an accurate and meaningful description of a video is a complex task, and recent research has produced impressive results, but there is still room for further exploration. This study proposes a novel solution called Semantically Sensible Video Captioning (SSVC), which utilizes two innovative techniques called ""stacked attention"" and ""spatial hard pull"" to modify the context generation mechanism. Most video captioning models consist of two layers, a video-to-context encoder and a context-to-caption decoder, but the SSVC approach enhances performance by incorporating these new approaches. To assess the effectiveness of the SSVC model, both quantitative and qualitative analyses were conducted. The BLEU scoring metric was used for quantitative analysis, while a new human evaluation metric called the Semantic Sensibility (SS) scoring metric was proposed for qualitative analysis. The SS metric addresses the limitations of automated scoring metrics. Results show that the SSVC approach improves state-of-the-art architectures.",1
"Academic advances of AI models in high-precision domains, like healthcare, need to be made explainable in order to enhance real-world adoption. Our past studies and ongoing interactions indicate that medical experts can use AI systems with greater trust if there are ways to connect the model inferences about patients to explanations that are tied back to the context of use. Specifically, risk prediction is a complex problem of diagnostic and interventional importance to clinicians wherein they consult different sources to make decisions. To enable the adoption of the ever improving AI risk prediction models in practice, we have begun to explore techniques to contextualize such models along three dimensions of interest: the patients' clinical state, AI predictions about their risk of complications, and algorithmic explanations supporting the predictions. We validate the importance of these dimensions by implementing a proof-of-concept (POC) in type-2 diabetes (T2DM) use case where we assess the risk of chronic kidney disease (CKD) - a common T2DM comorbidity. Within the POC, we include risk prediction models for CKD, post-hoc explainers of the predictions, and other natural-language modules which operationalize domain knowledge and CPGs to provide context. With primary care physicians (PCP) as our end-users, we present our initial results and clinician feedback in this paper. Our POC approach covers multiple knowledge sources and clinical scenarios, blends knowledge to explain data and predictions to PCPs, and received an enthusiastic response from our medical expert.",0
"In order to increase the practical use of AI models in high-precision fields such as healthcare, it is essential that they are explainable. Our previous research and ongoing interactions indicate that medical professionals would be more comfortable using AI systems if they could connect the model's inferences about patients to explanations that are relevant to the context of use. Risk prediction is a complex problem that requires clinicians to consult different sources to make decisions, which is of great diagnostic and interventional importance. To facilitate the adoption of AI risk prediction models in practice, we are exploring techniques to contextualize these models based on three dimensions: the patient's clinical state, AI predictions about their risk of complications, and algorithmic explanations that support these predictions. We conducted a proof-of-concept study on type-2 diabetes (T2DM) to assess the risk of chronic kidney disease (CKD), a common T2DM comorbidity. Our approach includes risk prediction models for CKD, post-hoc explainers of the predictions, and other natural-language modules that operationalize domain knowledge and CPGs to provide context. We presented our initial results and clinician feedback to primary care physicians, who responded enthusiastically to our approach. Our POC approach covers multiple knowledge sources and clinical scenarios, blending knowledge to explain data and predictions to PCPs.",1
"Black-box machine learning learning methods are now routinely used in high-risk settings, like medical diagnostics, which demand uncertainty quantification to avoid consequential model failures. Distribution-free uncertainty quantification (distribution-free UQ) is a user-friendly paradigm for creating statistically rigorous confidence intervals/sets for such predictions. Critically, the intervals/sets are valid without distributional assumptions or model assumptions, with explicit guarantees with finitely many datapoints. Moreover, they adapt to the difficulty of the input; when the input example is difficult, the uncertainty intervals/sets are large, signaling that the model might be wrong. Without much work, one can use distribution-free methods on any underlying algorithm, such as a neural network, to produce confidence sets guaranteed to contain the ground truth with a user-specified probability, such as 90%. Indeed, the methods are easy-to-understand and general, applying to many modern prediction problems arising in the fields of computer vision, natural language processing, deep reinforcement learning, and so on. This hands-on introduction is aimed at a reader interested in the practical implementation of distribution-free UQ, including conformal prediction and related methods, who is not necessarily a statistician. We will include many explanatory illustrations, examples, and code samples in Python, with PyTorch syntax. The goal is to provide the reader a working understanding of distribution-free UQ, allowing them to put confidence intervals on their algorithms, with one self-contained document.",0
"Black-box machine learning techniques are commonly used in risky settings, such as medical diagnostics, where uncertainty assessment is crucial to avoid model errors. Distribution-free uncertainty quantification (UQ) is a user-friendly approach that generates statistically sound confidence intervals/sets for predictions without any distributional or model assumptions. This method provides explicit guarantees even with limited data points and adjusts the uncertainty intervals/sets according to the complexity of the input. One can easily apply distribution-free methods on any underlying algorithm, including neural networks, to produce confidence sets that contain the ground truth with a specified probability. The approach is widely applicable to modern prediction problems in various fields, including computer vision, natural language processing, and deep reinforcement learning. This practical guide caters to readers with an interest in implementing distribution-free UQ, including conformal prediction and related methods, without requiring a background in statistics. It includes many explanatory illustrations, examples, and Python-based code samples using PyTorch syntax. The aim is to provide the reader a working knowledge of distribution-free UQ to enable them to confidently put confidence intervals on their algorithms with a single comprehensive document.",1
"Natural language often exhibits inherent hierarchical structure ingrained with complex syntax and semantics. However, most state-of-the-art deep generative models learn embeddings only in Euclidean vector space, without accounting for this structural property of language. In this paper, we investigate text generation in a hyperbolic latent space to learn continuous hierarchical representations. An Adversarial Poincare Variational Autoencoder (APo-VAE) is presented, where both the prior and variational posterior of latent variables are defined over a Poincare ball via wrapped normal distributions. By adopting the primal-dual formulation of KL divergence, an adversarial learning procedure is introduced to empower robust model training. Extensive experiments in language modeling and dialog-response generation tasks demonstrate the winning effectiveness of the proposed APo-VAE model over VAEs in Euclidean latent space, thanks to its superb capabilities in capturing latent language hierarchies in hyperbolic space.",0
"The structure of natural language is complex, with intricate syntax and semantics that exhibit a hierarchical pattern. However, current deep generative models only learn embeddings in a Euclidean vector space and do not consider this property of language. This paper explores the use of a hyperbolic latent space for text generation to develop continuous hierarchical representations. The Adversarial Poincare Variational Autoencoder (APo-VAE) is introduced, which defines both the prior and variational posterior of latent variables using a Poincare ball and wrapped normal distributions. To ensure robust model training, an adversarial learning procedure is implemented using the primal-dual formulation of KL divergence. Extensive experiments in language modeling and dialog-response generation tasks prove that APo-VAE is more effective than VAEs in a Euclidean latent space, as it can capture latent language hierarchies more accurately in hyperbolic space.",1
"The world of empirical machine learning (ML) strongly relies on benchmarks in order to determine the relative effectiveness of different algorithms and methods. This paper proposes the notion of ""a benchmark lottery"" that describes the overall fragility of the ML benchmarking process. The benchmark lottery postulates that many factors, other than fundamental algorithmic superiority, may lead to a method being perceived as superior. On multiple benchmark setups that are prevalent in the ML community, we show that the relative performance of algorithms may be altered significantly simply by choosing different benchmark tasks, highlighting the fragility of the current paradigms and potential fallacious interpretation derived from benchmarking ML methods. Given that every benchmark makes a statement about what it perceives to be important, we argue that this might lead to biased progress in the community. We discuss the implications of the observed phenomena and provide recommendations on mitigating them using multiple machine learning domains and communities as use cases, including natural language processing, computer vision, information retrieval, recommender systems, and reinforcement learning.",0
"Empirical machine learning (ML) heavily relies on benchmarks to determine the relative effectiveness of various algorithms and methods. This paper introduces the concept of a ""benchmark lottery"" to describe the overall fragility of the ML benchmarking process. The benchmark lottery suggests that multiple factors, beyond algorithmic superiority, may contribute to a method being deemed superior. We demonstrate that on prevalent benchmark setups in the ML community, the relative performance of algorithms can be significantly altered by selecting different benchmark tasks. This highlights the frailty of current paradigms and the potential for flawed interpretations derived from benchmarking ML methods. As each benchmark makes a statement about what it deems important, we argue that this may lead to biased progress within the community. We discuss the implications of these phenomena and provide recommendations for mitigating them using various machine learning domains and communities, such as natural language processing, computer vision, information retrieval, recommender systems, and reinforcement learning.",1
"Visual Question Answering (VQA) is concerned with answering free-form questions about an image. Since it requires a deep semantic and linguistic understanding of the question and the ability to associate it with various objects that are present in the image, it is an ambitious task and requires multi-modal reasoning from both computer vision and natural language processing. We propose Graphhopper, a novel method that approaches the task by integrating knowledge graph reasoning, computer vision, and natural language processing techniques. Concretely, our method is based on performing context-driven, sequential reasoning based on the scene entities and their semantic and spatial relationships. As a first step, we derive a scene graph that describes the objects in the image, as well as their attributes and their mutual relationships. Subsequently, a reinforcement learning agent is trained to autonomously navigate in a multi-hop manner over the extracted scene graph to generate reasoning paths, which are the basis for deriving answers. We conduct an experimental study on the challenging dataset GQA, based on both manually curated and automatically generated scene graphs. Our results show that we keep up with a human performance on manually curated scene graphs. Moreover, we find that Graphhopper outperforms another state-of-the-art scene graph reasoning model on both manually curated and automatically generated scene graphs by a significant margin.",0
"The task of Visual Question Answering (VQA) involves answering questions about an image, which requires a deep understanding of the question and the ability to associate it with objects in the image. It is a challenging task that requires multi-modal reasoning from both computer vision and natural language processing. In this study, we propose a new method called Graphhopper that integrates knowledge graph reasoning, computer vision, and natural language processing techniques. Our approach involves context-driven, sequential reasoning based on the semantic and spatial relationships of the scene entities. We first derive a scene graph that describes the objects in the image and their attributes. We then use a reinforcement learning agent to navigate the scene graph and generate reasoning paths to derive answers. We conducted experiments on the GQA dataset using both manually curated and automatically generated scene graphs. Our results show that Graphhopper performs at a human level on manually curated scene graphs and outperforms another state-of-the-art model on both manually curated and automatically generated scene graphs.",1
"We address the problem of text-guided video temporal grounding, which aims to identify the time interval of certain event based on a natural language description. Different from most existing methods that only consider RGB images as visual features, we propose a multi-modal framework to extract complementary information from videos. Specifically, we adopt RGB images for appearance, optical flow for motion, and depth maps for image structure. While RGB images provide abundant visual cues of certain event, the performance may be affected by background clutters. Therefore, we use optical flow to focus on large motion and depth maps to infer the scene configuration when the action is related to objects recognizable with their shapes. To integrate the three modalities more effectively and enable inter-modal learning, we design a dynamic fusion scheme with transformers to model the interactions between modalities. Furthermore, we apply intra-modal self-supervised learning to enhance feature representations across videos for each modality, which also facilitates multi-modal learning. We conduct extensive experiments on the Charades-STA and ActivityNet Captions datasets, and show that the proposed method performs favorably against state-of-the-art approaches.",0
"Our focus is on solving the issue of identifying the time interval of a certain event by utilizing a natural language description in text-guided video temporal grounding. Most current methods only use RGB images as visual features, which limits their effectiveness. To address this limitation, we propose a multi-modal framework that incorporates RGB images for appearance, optical flow for motion, and depth maps for image structure. While RGB images provide useful visual information, they can be affected by background clutter. To overcome this, we use optical flow to focus on large motion and depth maps to infer the scene configuration when the action is related to objects recognizable with their shapes. We also use a dynamic fusion scheme with transformers to model the interactions between modalities, and intra-modal self-supervised learning to enhance feature representations across videos for each modality. Our experiments on the Charades-STA and ActivityNet Captions datasets demonstrate that our approach performs favorably against state-of-the-art methods.",1
"With the rise of voice chat rooms, a gigantic resource of data can be exposed to the research community for natural language processing tasks. Moderators in voice chat rooms actively monitor the discussions and remove the participants with offensive language. However, it makes the hate speech detection even more difficult since some participants try to find creative ways to articulate hate speech. This makes the hate speech detection challenging in new social media like Clubhouse. To the best of our knowledge all the hate speech datasets have been collected from text resources like Twitter. In this paper, we take the first step to collect a significant dataset from Clubhouse as the rising star in social media industry. We analyze the collected instances from statistical point of view using the Google Perspective Scores. Our experiments show that, the Perspective Scores can outperform Bag of Words and Word2Vec as high level text features.",0
"The emergence of voice chat rooms presents a vast amount of data that can aid the research community in natural language processing tasks. While moderators actively monitor discussions and remove users who use offensive language, hate speech detection has become more challenging as some participants use creative ways to articulate their hate speech. This difficulty is especially pronounced in new social media platforms such as Clubhouse. To date, all hate speech datasets have been gathered from text sources like Twitter. This paper takes the first step in collecting a significant dataset from Clubhouse, a rising star in the social media industry. We analyze the collected data using Google Perspective Scores from a statistical perspective. Our experiments demonstrate that Perspective Scores can outperform Bag of Words and Word2Vec as high-level text features.",1
"Compared to consumer lending, Micro, Small and Medium Enterprise (mSME) credit risk modelling is particularly challenging, as, often, the same sources of information are not available. Therefore, it is standard policy for a loan officer to provide a textual loan assessment to mitigate limited data availability. In turn, this statement is analysed by a credit expert alongside any available standard credit data. In our paper, we exploit recent advances from the field of Deep Learning and Natural Language Processing (NLP), including the BERT (Bidirectional Encoder Representations from Transformers) model, to extract information from 60 000 textual assessments provided by a lender. We consider the performance in terms of the AUC (Area Under the receiver operating characteristic Curve) and Brier Score metrics and find that the text alone is surprisingly effective for predicting default. However, when combined with traditional data, it yields no additional predictive capability, with performance dependent on the text's length. Our proposed deep learning model does, however, appear to be robust to the quality of the text and therefore suitable for partly automating the mSME lending process. We also demonstrate how the content of loan assessments influences performance, leading us to a series of recommendations on a new strategy for collecting future mSME loan assessments.",0
"The complexity of Micro, Small and Medium Enterprise (mSME) credit risk modelling is greater than that of consumer lending due to the unavailability of the same sources of information. To address this limitation, loan officers typically provide a textual loan assessment which is then analyzed by a credit expert in combination with any available standard credit data. Utilizing recent advancements in Deep Learning and Natural Language Processing (NLP), we examine the effectiveness of the BERT model in extracting information from 60,000 textual assessments provided by a lender. Our study evaluates the model's performance using the AUC and Brier Score metrics and finds that the text alone is surprisingly effective for predicting default, but its length influences its performance. The combination of text and traditional data does not enhance predictive capability. Our deep learning model appears to be robust to low-quality text, making it suitable for automating the mSME lending process. Finally, we recommend a new strategy for collecting future mSME loan assessments based on our findings on how the content of loan assessments impacts performance.",1
"Medical Visual Question Answering (VQA) is a multi-modal challenging task widely considered by research communities of the computer vision and natural language processing. Since most current medical VQA models focus on visual content, ignoring the importance of text, this paper proposes a multi-view attention-based model(MuVAM) for medical visual question answering which integrates the high-level semantics of medical images on the basis of text description. Firstly, different methods are utilized to extract the features of the image and the question for the two modalities of vision and text. Secondly, this paper proposes a multi-view attention mechanism that include Image-to-Question (I2Q) attention and Word-to-Text (W2T) attention. Multi-view attention can correlate the question with image and word in order to better analyze the question and get an accurate answer. Thirdly, a composite loss is presented to predict the answer accurately after multi-modal feature fusion and improve the similarity between visual and textual cross-modal features. It consists of classification loss and image-question complementary (IQC) loss. Finally, for data errors and missing labels in the VQA-RAD dataset, we collaborate with medical experts to correct and complete this dataset and then construct an enhanced dataset, VQA-RADPh. The experiments on these two datasets show that the effectiveness of MuVAM surpasses the state-of-the-art method.",0
"The task of Medical Visual Question Answering (VQA) is considered a challenging multi-modal task by both computer vision and natural language processing research communities. However, current medical VQA models primarily focus on visual content, neglecting the importance of text. To address this, our paper proposes a multi-view attention-based model (MuVAM) for medical visual question answering that integrates high-level semantics of medical images based on text descriptions. We employ various methods to extract features of the image and the question for the vision and text modalities. Our proposed multi-view attention mechanism includes Image-to-Question (I2Q) and Word-to-Text (W2T) attention to correlate the question with the image and word, respectively, for an accurate answer. We present a composite loss consisting of classification loss and image-question complementary (IQC) loss to predict the answer accurately after multi-modal feature fusion and improve similarity between visual and textual cross-modal features. Additionally, we collaborated with medical experts to correct and complete the VQA-RAD dataset and construct an enhanced dataset, VQA-RADPh. Our experiments on both datasets demonstrate the superior effectiveness of MuVAM compared to the state-of-the-art method.",1
"Attribute extrapolation in sample generation is challenging for deep neural networks operating beyond the training distribution. We formulate a new task for extrapolation in sequence generation, focusing on natural language and proteins, and propose GENhance, a generative framework that enhances attributes through a learned latent space. Trained on movie reviews and a computed protein stability dataset, GENhance can generate strongly-positive text reviews and highly stable protein sequences without being exposed to similar data during training. We release our benchmark tasks and models to contribute to the study of generative modeling extrapolation and data-driven design in biology and chemistry.",0
"Generating samples with extrapolated attributes poses a challenge to deep neural networks that operate beyond their training distribution. To address this issue, we introduce a new task for extrapolation in sequence generation, with a focus on natural language and proteins. We propose GENhance, a generative framework that enhances attributes through a learned latent space. GENhance is trained on movie reviews and a computed protein stability dataset, and it can generate strongly-positive text reviews and highly stable protein sequences, even without exposure to similar data during training. We have made our benchmark tasks and models available to contribute to the study of generative modeling extrapolation and data-driven design in biology and chemistry.",1
"In this article, we present a Shell Language Preprocessing (SLP) library, which implements tokenization and encoding directed on the parsing of Unix and Linux shell commands. We describe the rationale behind the need for a new approach with specific examples when conventional Natural Language Processing (NLP) pipelines fail. Furthermore, we evaluate our methodology on a security classification task against widely accepted information and communications technology (ICT) tokenization techniques and achieve significant improvement of an F1-score from 0.392 to 0.874.",0
"The focus of this article is the introduction of the Shell Language Preprocessing (SLP) library that performs tokenization and encoding for parsing Unix and Linux shell commands. The need for this approach is explained with examples where traditional Natural Language Processing (NLP) methods do not suffice. Additionally, we assess our approach for a security classification task relative to standard information and communications technology (ICT) tokenization approaches. The results show a substantial increase in F1-score from 0.392 to 0.874.",1
"Humans and other intelligent animals evolved highly sophisticated perception systems that combine multiple sensory modalities. On the other hand, state-of-the-art artificial agents rely mostly on visual inputs or structured low-dimensional observations provided by instrumented environments. Learning to act based on combined visual and auditory inputs is still a new topic of research that has not been explored beyond simple scenarios. To facilitate progress in this area we introduce a new version of VizDoom simulator to create a highly efficient learning environment that provides raw audio observations. We study the performance of different model architectures in a series of tasks that require the agent to recognize sounds and execute instructions given in natural language. Finally, we train our agent to play the full game of Doom and find that it can consistently defeat a traditional vision-based adversary. We are currently in the process of merging the augmented simulator with the main ViZDoom code repository. Video demonstrations and experiment code can be found at https://sites.google.com/view/sound-rl.",0
"Sophisticated perception systems that combine multiple sensory modalities have evolved in humans and other intelligent animals. However, current state-of-the-art artificial agents primarily rely on visual inputs or structured low-dimensional observations from instrumented environments. Research on learning to act based on combined visual and auditory inputs is still in its early stages and has yet to progress beyond simple scenarios. To advance this area of research, we present a new version of the VizDoom simulator that provides raw audio observations and creates a highly efficient learning environment. Our study evaluates different model architectures in tasks that require the agent to recognize sounds and execute instructions in natural language. Furthermore, we train our agent to play the full game of Doom and show that it can consistently defeat a traditional vision-based adversary. We are currently merging the augmented simulator with the main ViZDoom code repository. You can find video demonstrations and experiment code at https://sites.google.com/view/sound-rl.",1
"In recent years, artificial intelligence (AI) systems have come to the forefront. These systems, mostly based on Deep learning (DL), achieve excellent results in areas such as image processing, natural language processing, or speech recognition. Despite the statistically high accuracy of deep learning models, their output is often a decision of ""black box"". Thus, Interpretability methods have become a popular way to gain insight into the decision-making process of deep learning models. Explanation of a deep learning model is desirable in the medical domain since the experts have to justify their judgments to the patient. In this work, we proposed a method for explanation-guided training that uses a Layer-wise relevance propagation (LRP) technique to force the model to focus only on the relevant part of the image. We experimentally verified our method on a convolutional neural network (CNN) model for low-grade and high-grade glioma classification problems. Our experiments show promising results in a way to use interpretation techniques in the model training process.",0
"Artificial intelligence (AI) systems, mostly based on Deep learning (DL), have become prevalent in recent years and excel in fields such as image processing, natural language processing, and speech recognition. However, despite their high accuracy, the decisions made by deep learning models often remain a mystery, leading to the need for Interpretability methods. In the medical domain, where experts must justify their judgments to patients, understanding the decision-making process is particularly important. In this study, we propose a method for explanation-guided training that focuses only on the relevant part of the image using Layer-wise relevance propagation (LRP) technique. We tested our approach on a convolutional neural network (CNN) model for low-grade and high-grade glioma classification, demonstrating promising results for using interpretation techniques in the model training process.",1
"The last decade has seen a significant increase of interest in deep learning research, with many public successes that have demonstrated its potential. As such, these systems are now being incorporated into commercial products. With this comes an additional challenge: how can we build AI systems that solve tasks where there is not a crisp, well-defined specification? While multiple solutions have been proposed, in this competition we focus on one in particular: learning from human feedback. Rather than training AI systems using a predefined reward function or using a labeled dataset with a predefined set of categories, we instead train the AI system using a learning signal derived from some form of human feedback, which can evolve over time as the understanding of the task changes, or as the capabilities of the AI system improve.   The MineRL BASALT competition aims to spur forward research on this important class of techniques. We design a suite of four tasks in Minecraft for which we expect it will be hard to write down hardcoded reward functions. These tasks are defined by a paragraph of natural language: for example, ""create a waterfall and take a scenic picture of it"", with additional clarifying details. Participants must train a separate agent for each task, using any method they want. Agents are then evaluated by humans who have read the task description. To help participants get started, we provide a dataset of human demonstrations on each of the four tasks, as well as an imitation learning baseline that leverages these demonstrations.   Our hope is that this competition will improve our ability to build AI systems that do what their designers intend them to do, even when the intent cannot be easily formalized. Besides allowing AI to solve more tasks, this can also enable more effective regulation of AI systems, as well as making progress on the value alignment problem.",0
"Over the past decade, there has been a surge in interest in deep learning research, resulting in numerous successful applications that showcase its potential. As a result, these systems are now being incorporated into commercial products. However, a new challenge arises: how can we develop AI systems that can perform tasks without clearly defined specifications? While various solutions have been proposed, this competition focuses on one approach: training AI systems using human feedback instead of predefined reward functions or labeled datasets. The MineRL BASALT competition presents four Minecraft tasks that are difficult to solve with hardcoded reward functions, each defined by a natural language paragraph with additional details. Participants can train an agent for each task using any method, and the agents are then evaluated by humans who have read the task description. To aid participants, a dataset of human demonstrations and an imitation learning baseline are provided. The goal is to enhance our ability to create AI systems that align with their intended purpose, even in ambiguous situations. This will facilitate better AI regulation and progress in value alignment.",1
"Adaptive gradient methods such as RMSProp and Adam use exponential moving estimate of the squared gradient to compute adaptive step sizes, achieving better convergence than SGD in face of noisy objectives. However, Adam can have undesirable convergence behaviors due to unstable or extreme adaptive learning rates. Methods such as AMSGrad and AdaBound have been proposed to stabilize the adaptive learning rates of Adam in the later stage of training, but they do not outperform Adam in some practical tasks such as training Transformers \cite{transformer}. In this paper, we propose an adaptive learning rate principle, in which the running mean of squared gradient in Adam is replaced by a weighted mean, with weights chosen to maximize the estimated variance of each coordinate. This results in a faster adaptation to the local gradient variance, which leads to more desirable empirical convergence behaviors than Adam. We prove the proposed algorithm converges under mild assumptions for nonconvex stochastic optimization problems, and demonstrate the improved efficacy of our adaptive averaging approach on machine translation, natural language understanding and large-batch pretraining of BERT. The code is available at https://github.com/zhuchen03/MaxVA.",0
"Adaptive gradient techniques such as RMSProp and Adam use an exponential moving estimate of the squared gradient to calculate adaptive step sizes, which can result in superior convergence when dealing with noisy objectives compared to SGD. However, Adam can exhibit unfavorable convergence behaviors due to erratic or extreme adaptive learning rates. Although AMSGrad and AdaBound have been suggested to stabilize the adaptive learning rates of Adam in later stages of training, they do not perform better than Adam in certain practical tasks like training Transformers. In this study, we propose an adaptive learning rate principle in which the running mean of squared gradient in Adam is replaced with a weighted mean. The weights are selected to maximize the estimated variance of each coordinate, resulting in a faster adaptation to the local gradient variance and more favorable empirical convergence behaviors than Adam. We prove that the proposed algorithm converges for nonconvex stochastic optimization problems under mild assumptions and demonstrate the improved effectiveness of our adaptive averaging approach on machine translation, natural language understanding, and large-batch pretraining of BERT. The code for our proposed algorithm is accessible at https://github.com/zhuchen03/MaxVA.",1
"Deep learning provides a promising way to extract effective representations from raw data in an end-to-end fashion and has proven its effectiveness in various domains such as computer vision, natural language processing, etc. However, in domains such as content/product recommendation and risk management, where sequence of event data is the most used raw data form and experts derived features are more commonly used, deep learning models struggle to dominate the game. In this paper, we propose a symbolic testing framework that helps to answer the question of what kinds of expert-derived features could be learned by a neural network. Inspired by this testing framework, we introduce an efficient architecture named SHORING, which contains two components: \textit{event network} and \textit{sequence network}. The \textit{event} network learns arbitrarily yet efficiently high-order \textit{event-level} embeddings via a provable reparameterization trick, the \textit{sequence} network aggregates from sequence of \textit{event-level} embeddings. We argue that SHORING is capable of learning certain standard symbolic expressions which the standard multi-head self-attention network fails to learn, and conduct comprehensive experiments and ablation studies on four synthetic datasets and three real-world datasets. The results show that SHORING empirically outperforms the state-of-the-art methods.",0
"Deep learning has proven to be effective in various domains such as computer vision and natural language processing by extracting useful representations from raw data in an end-to-end manner. However, in domains like content/product recommendation and risk management where expert-derived features and event sequences are prevalent, deep learning models face challenges. To address this, we propose a symbolic testing framework to determine which expert-derived features can be learned by a neural network. Based on this framework, we introduce an efficient architecture called SHORING comprising two components: the event network and the sequence network. The event network learns high-order event-level embeddings efficiently using a reparameterization trick, while the sequence network aggregates these embeddings. We demonstrate that SHORING can learn certain symbolic expressions that standard multi-head self-attention networks cannot, through comprehensive experiments and ablation studies on synthetic and real-world datasets. The results indicate that SHORING outperforms state-of-the-art methods.",1
"Braille has empowered visually challenged community to read and write. But at the same time, it has created a gap due to widespread inability of non-Braille users to understand Braille scripts. This gap has fuelled researchers to propose Optical Braille Recognition techniques to convert Braille documents to natural language. The main motivation of this work is to cement the communication gap at academic institutions by translating personal documents of blind students. This has been accomplished by proposing an economical and effective technique which digitizes Braille documents using a smartphone camera. For any given Braille image, a dot detection mechanism based on Hough transform is proposed which is invariant to skewness, noise and other deterrents. The detected dots are then clustered into Braille cells using distance-based clustering algorithm. In succession, the standard physical parameters of each Braille cells are estimated for feature extraction and classification as natural language characters. The comprehensive evaluation of this technique on the proposed dataset of 54 Braille scripts has yielded into accuracy of 98.71%.",0
"The visually impaired community has been enabled to read and write through Braille. However, there is a gap between Braille users and non-Braille users, causing researchers to propose Optical Braille Recognition techniques that convert Braille documents to natural language. The aim of this research is to bridge the communication gap in academic institutions by translating personal documents of blind students. An economical and effective technique that digitizes Braille documents using a smartphone camera has been proposed. This technique uses a dot detection mechanism based on Hough transform that is immune to skewness, noise, and other impediments. The detected dots are then clustered into Braille cells using a distance-based clustering algorithm. The standard physical parameters of each Braille cell are estimated for feature extraction and classification as natural language characters. The proposed technique has been evaluated on a dataset of 54 Braille scripts, resulting in an accuracy of 98.71%.",1
"A generic video summary is an abridged version of a video that conveys the whole story and features the most important scenes. Yet the importance of scenes in a video is often subjective, and users should have the option of customizing the summary by using natural language to specify what is important to them. Further, existing models for fully automatic generic summarization have not exploited available language models, which can serve as an effective prior for saliency. This work introduces CLIP-It, a single framework for addressing both generic and query-focused video summarization, typically approached separately in the literature. We propose a language-guided multimodal transformer that learns to score frames in a video based on their importance relative to one another and their correlation with a user-defined query (for query-focused summarization) or an automatically generated dense video caption (for generic video summarization). Our model can be extended to the unsupervised setting by training without ground-truth supervision. We outperform baselines and prior work by a significant margin on both standard video summarization datasets (TVSum and SumMe) and a query-focused video summarization dataset (QFVS). Particularly, we achieve large improvements in the transfer setting, attesting to our method's strong generalization capabilities.",0
"A generic video summary is a shortened version of a video that includes the most important scenes and conveys the entire story. However, determining the significance of scenes in a video can be subjective, and users should be able to personalize the summary by specifying what is important to them using natural language. Additionally, current models for automatic generic summarization have not utilized available language models, which can serve as an effective prior for identifying important scenes. This study presents CLIP-It, a unified framework that addresses both generic and query-focused video summarization, which are typically approached independently. The proposed language-guided multimodal transformer learns to score frames in a video based on their importance relative to each other and their correlation with a user-defined query or an automatically generated dense video caption. Our model can be extended to unsupervised settings by training without ground-truth supervision. We significantly outperform baselines and prior work on standard video summarization datasets (TVSum and SumMe) and a query-focused video summarization dataset (QFVS), with significant improvements in the transfer setting, indicating strong generalization capabilities.",1
"Transformer-based models are popularly used in natural language processing (NLP). Its core component, self-attention, has aroused widespread interest. To understand the self-attention mechanism, a direct method is to visualize the attention map of a pre-trained model. Based on the patterns observed, a series of efficient Transformers with different sparse attention masks have been proposed. From a theoretical perspective, universal approximability of Transformer-based models is also recently proved. However, the above understanding and analysis of self-attention is based on a pre-trained model. To rethink the importance analysis in self-attention, we study the significance of different positions in attention matrix during pre-training. A surprising result is that diagonal elements in the attention map are the least important compared with other attention positions. We provide a proof showing that these diagonal elements can indeed be removed without deteriorating model performance. Furthermore, we propose a Differentiable Attention Mask (DAM) algorithm, which further guides the design of the SparseBERT. Extensive experiments verify our interesting findings and illustrate the effect of the proposed algorithm.",0
"In natural language processing (NLP), transformer-based models are commonly used and have gained widespread interest due to their core component, self-attention. To gain a better understanding of the self-attention mechanism, it is possible to visualize the attention map of a pre-trained model. This has led to the development of various efficient transformers with different sparse attention masks based on observed patterns. Recently, the universal approximability of transformer-based models has also been proven. However, the current understanding and analysis of self-attention are based on pre-trained models, and it is important to analyze the significance of different positions in the attention matrix during pre-training. Surprisingly, our study shows that diagonal elements in the attention map are the least important compared to other attention positions. We prove that these diagonal elements can be safely removed without affecting model performance. Additionally, we propose the Differentiable Attention Mask (DAM) algorithm, which guides the design of the SparseBERT and leads to interesting findings. Extensive experiments confirm the effectiveness of our proposed algorithm.",1
"We investigate the capability of a transformer pretrained on natural language to generalize to other modalities with minimal finetuning -- in particular, without finetuning of the self-attention and feedforward layers of the residual blocks. We consider such a model, which we call a Frozen Pretrained Transformer (FPT), and study finetuning it on a variety of sequence classification tasks spanning numerical computation, vision, and protein fold prediction. In contrast to prior works which investigate finetuning on the same modality as the pretraining dataset, we show that pretraining on natural language can improve performance and compute efficiency on non-language downstream tasks. Additionally, we perform an analysis of the architecture, comparing the performance of a random initialized transformer to a random LSTM. Combining the two insights, we find language-pretrained transformers can obtain strong performance on a variety of non-language tasks.",0
"Our research focuses on exploring the potential of a transformer model trained on natural language to adapt to other modalities with minimal finetuning, specifically without adjusting the self-attention and feedforward layers of the residual blocks. We refer to this model as a Frozen Pretrained Transformer (FPT) and examine its performance on various sequence classification tasks related to numerical computation, vision, and protein fold prediction. Unlike previous studies that only examine finetuning on the same modality as the pretraining dataset, we demonstrate that pretraining on natural language can enhance performance and computational efficiency for non-language tasks. Furthermore, we conduct an analysis of the architecture and compare the performance of a randomly initialized transformer to that of a random LSTM. Our findings indicate that language-pretrained transformers can achieve excellent results on diverse non-language tasks.",1
"This paper addresses the problem of media retrieval using a multimodal query (a query which combines visual input with additional semantic information in natural language feedback). We propose a SynthTriplet GAN framework which resolves this task by expanding the multimodal query with a synthetically generated image that captures semantic information from both image and text input. We introduce a novel triplet mining method that uses a synthetic image as an anchor to directly optimize for embedding distances of generated and target images. We demonstrate that apart from the added value of retrieval illustration with synthetic image with the focus on customization and user feedback, the proposed method greatly surpasses other multimodal generation methods and achieves state of the art results in the multimodal retrieval task. We also show that in contrast to other retrieval methods, our method provides explainable embeddings.",0
"The focus of this article is on solving the issue of media retrieval through the use of a multimodal query, which combines visual and semantic information in natural language feedback. To address this problem, we propose the SynthTriplet GAN framework, which expands the multimodal query by generating a synthetic image that incorporates semantic information from both the image and text input. Our approach includes a novel triplet mining method that uses a synthetic image as an anchor to optimize for the embedding distances of generated and target images. Our method not only improves retrieval illustration with synthetic images that are customizable and informed by user feedback, but it also outperforms other multimodal generation methods and achieves state-of-the-art outcomes in multimodal retrieval. Furthermore, our method provides explainable embeddings, which sets it apart from other retrieval methods.",1
"Transformers provide promising accuracy and have become popular and used in various domains such as natural language processing and computer vision. However, due to their massive number of model parameters, memory and computation requirements, they are not suitable for resource-constrained low-power devices. Even with high-performance and specialized devices, the memory bandwidth can become a performance-limiting bottleneck. In this paper, we present a performance analysis of state-of-the-art vision transformers on several devices. We propose to reduce the overall memory footprint and memory transfers by clustering the model parameters. We show that by using only 64 clusters to represent model parameters, it is possible to reduce the data transfer from the main memory by more than 4x, achieve up to 22% speedup and 39% energy savings on mobile devices with less than 0.1% accuracy loss.",0
"Transformers are widely used for natural language processing and computer vision and are known for their accuracy. However, their large number of model parameters makes them unsuitable for low-power devices due to memory and computation requirements. Even high-performance devices can be limited by memory bandwidth. This paper analyzes the performance of top-performing vision transformers on various devices. To reduce memory requirements, we propose clustering model parameters, leading to a reduction in data transfer from main memory by over 4x. This results in a 22% speedup and 39% energy savings on mobile devices with less than 0.1% accuracy loss using only 64 clusters.",1
"Transformer attention architectures, similar to those developed for natural language processing, have recently proved efficient also in vision, either in conjunction with or as a replacement for convolutional layers. Typically, visual attention is inserted in the network architecture as a (series of) feedforward self-attention module(s), with mutual key-query agreement as the main selection and routing operation. However efficient, this strategy is only vaguely compatible with the way that attention is implemented in biological brains: as a separate and unified network of attentional selection regions, receiving inputs from and exerting modulatory influence on the entire hierarchy of visual regions. Here, we report experiments with a simple such attention system that can improve the performance of standard convolutional networks, with relatively few additional parameters. Each spatial position in each layer of the network produces a key-query vector pair; all queries are then pooled into a global attention query. On the next iteration, the match between each key and the global attention query modulates the network's activations -- emphasizing or silencing the locations that agree or disagree (respectively) with the global attention system. We demonstrate the usefulness of this brain-inspired Global Attention Agreement network (GAttANet) for various convolutional backbones (from a simple 5-layer toy model to a standard ResNet50 architecture) and datasets (CIFAR10, CIFAR100, Imagenet-1k). Each time, our global attention system improves accuracy over the corresponding baseline.",0
"Recently, transformer attention architectures, which were originally developed for natural language processing, have shown to be effective in vision tasks. They can be used in conjunction with or as a replacement for convolutional layers. Typically, visual attention is incorporated into the network as a feedforward self-attention module. However, this approach is not fully compatible with the biological implementation of attention in the brain, which involves a separate and unified network of attentional selection regions. To address this, we present a simple attention system that can enhance the performance of standard convolutional networks with few additional parameters. In this system, each spatial position in each layer generates a key-query vector pair that is pooled into a global attention query. The match between each key and the global attention query modulates the network's activations, emphasizing or silencing the locations that agree or disagree with the global attention system. We call this approach the Global Attention Agreement network (GAttANet), and we demonstrate its effectiveness on various convolutional backbones and datasets, including CIFAR10, CIFAR100, and Imagenet-1k. In each case, our system improves accuracy over the corresponding baseline.",1
"Deep neural network (DNN) is a popular model implemented in many systems to handle complex tasks such as image classification, object recognition, natural language processing etc. Consequently DNN structural vulnerabilities become part of the security vulnerabilities in those systems. In this paper we study the root cause of DNN adversarial examples. We examine the DNN response surface to understand its classification boundary. Our study reveals the structural problem of DNN classification boundary that leads to the adversarial examples. Existing attack algorithms can generate from a handful to a few hundred adversarial examples given one clean image. We show there are infinitely many adversarial images given one clean sample, all within a small neighborhood of the clean sample. We then define DNN uncertainty regions and show transferability of adversarial examples is not universal. We also argue that generalization error, the large sample theoretical guarantee established for DNN, cannot adequately capture the phenomenon of adversarial examples. We need new theory to measure DNN robustness.",0
"The use of deep neural networks (DNNs) has become widespread in many systems for handling complex tasks like image classification, object recognition, and natural language processing. However, the vulnerabilities in DNN structures also pose a risk to the security of these systems. This paper delves into the root cause of DNN adversarial examples and examines the response surface of DNN to understand its classification boundary. Our study reveals a problem in the structure of DNN's classification boundary, which leads to the emergence of adversarial examples. Existing attack algorithms can produce a range of adversarial examples from a few to hundreds from a single clean image. We demonstrate that there are an infinite number of adversarial images within a small neighborhood of the clean sample. We also introduce the concept of DNN uncertainty regions and show that the transferability of adversarial examples is not universal. Furthermore, we argue that generalization error, which is a theoretical guarantee for DNNs based on large sample sizes, cannot adequately capture the phenomenon of adversarial examples. Therefore, a new theory is required to measure DNN robustness.",1
"Deep Learning (DL) is considered the state-of-the-art in computer vision, speech recognition and natural language processing. Until recently, it was also widely accepted that DL is irrelevant for learning tasks on tabular data, especially in the small sample regime where ensemble methods are acknowledged as the gold standard. We present a new end-to-end differentiable method to train a standard FFNN. Our method, \textbf{Muddling labels for Regularization} (\texttt{MLR}), penalizes memorization through the generation of uninformative labels and the application of a differentiable close-form regularization scheme on the last hidden layer during training. \texttt{MLR} outperforms classical NN and the gold standard (GBDT, RF) for regression and classification tasks on several datasets from the UCI database and Kaggle covering a large range of sample sizes and feature to sample ratios. Researchers and practitioners can use \texttt{MLR} on its own as an off-the-shelf \DL{} solution or integrate it into the most advanced ML pipelines.",0
"DL, which is the current leader in computer vision, speech recognition, and natural language processing, was once believed to be unsuitable for tabular data learning tasks, particularly in cases with small sample sizes where ensemble methods were known to be the best option. However, we have introduced a new method for training a standard FFNN called ""Muddling labels for Regularization"" (MLR), which is an end-to-end differentiable approach that uses uninformative labels to penalize memorization and a differentiable close-form regularization scheme on the last hidden layer during training. Our results indicate that MLR surpasses classical NN and the gold standard (GBDT, RF) in regression and classification tasks on various datasets from the UCI database and Kaggle, covering a wide range of sample sizes and feature to sample ratios. Researchers and practitioners can utilize MLR as a standalone DL solution or incorporate it into the most sophisticated ML pipelines.",1
"Self-supervised contrastive representation learning has proved incredibly successful in the vision and natural language domains, enabling state-of-the-art performance with orders of magnitude less labeled data. However, such methods are domain-specific and little has been done to leverage this technique on real-world tabular datasets. We propose SCARF, a simple, widely-applicable technique for contrastive learning, where views are formed by corrupting a random subset of features. When applied to pre-train deep neural networks on the 69 real-world, tabular classification datasets from the OpenML-CC18 benchmark, SCARF not only improves classification accuracy in the fully-supervised setting but does so also in the presence of label noise and in the semi-supervised setting where only a fraction of the available training data is labeled. We show that SCARF complements existing strategies and outperforms alternatives like autoencoders. We conduct comprehensive ablations, detailing the importance of a range of factors.",0
"The use of self-supervised contrastive representation learning has been highly effective in improving performance in both vision and natural language areas, requiring significantly less labeled data. However, these methods are limited to specific domains and have not been widely applied to tabular datasets. Our proposed technique, SCARF, is a straightforward and adaptable approach to contrastive learning, using corrupted feature subsets to create views. By applying SCARF to pre-train deep neural networks on 69 actual tabular classification datasets from OpenML-CC18, we demonstrate improved classification accuracy in fully-supervised, label noise, and semi-supervised settings where only a portion of training data is labeled. SCARF complements existing methods and surpasses alternatives such as autoencoders. We perform thorough evaluations, highlighting the significance of various factors.",1
"This work presents CLIPDraw, an algorithm that synthesizes novel drawings based on natural language input. CLIPDraw does not require any training; rather a pre-trained CLIP language-image encoder is used as a metric for maximizing similarity between the given description and a generated drawing. Crucially, CLIPDraw operates over vector strokes rather than pixel images, a constraint that biases drawings towards simpler human-recognizable shapes. Results compare between CLIPDraw and other synthesis-through-optimization methods, as well as highlight various interesting behaviors of CLIPDraw, such as satisfying ambiguous text in multiple ways, reliably producing drawings in diverse artistic styles, and scaling from simple to complex visual representations as stroke count is increased. Code for experimenting with the method is available at: https://colab.research.google.com/github/kvfrans/clipdraw/blob/main/clipdraw.ipynb",0
"The article introduces CLIPDraw, an algorithm that generates new drawings based on natural language input. Unlike other methods, CLIPDraw does not require training because it uses a pre-trained CLIP language-image encoder to maximize the similarity between the description and the created drawing. CLIPDraw works with vector strokes instead of pixel images, which leads to simpler shapes that are easily recognizable by humans. The research shows that CLIPDraw outperforms other optimization techniques in terms of producing diverse styles and satisfying ambiguous text in multiple ways. Moreover, the algorithm can handle both simple and complex visual representations by increasing the number of strokes. Those interested in experimenting with the method can access the code at https://colab.research.google.com/github/kvfrans/clipdraw/blob/main/clipdraw.ipynb.",1
"In large technology companies, the requirements for managing and organizing technical documents created by engineers and managers in supporting relevant decision making have increased dramatically in recent years, which has led to a higher demand for more scalable, accurate, and automated document classification. Prior studies have primarily focused on processing text for classification and small-scale databases. This paper describes a novel multimodal deep learning architecture, called TechDoc, for technical document classification, which utilizes both natural language and descriptive images to train hierarchical classifiers. The architecture synthesizes convolutional neural networks and recurrent neural networks through an integrated training process. We applied the architecture to a large multimodal technical document database and trained the model for classifying documents based on the hierarchical International Patent Classification system. Our results show that the trained neural network presents a greater classification accuracy than those using a single modality and several earlier text classification methods. The trained model can potentially be scaled to millions of real-world technical documents with both text and figures, which is useful for data and knowledge management in large technology companies and organizations.",0
"The need for effective management and organization of technical documents in large technology companies has significantly increased in recent years, resulting in a higher demand for automated document classification that is scalable and accurate. While previous studies have focused on text processing and small-scale databases, this paper introduces a novel deep learning architecture called TechDoc, which uses both natural language and descriptive images to train hierarchical classifiers. The architecture combines convolutional neural networks and recurrent neural networks in an integrated training process. The model was applied to a large multimodal technical document database and trained to classify documents based on the hierarchical International Patent Classification system. The results show that the TechDoc architecture outperforms single-modality and earlier text classification methods in terms of accuracy. The trained model can be scaled to handle millions of technical documents with text and figures, which is beneficial for data and knowledge management in large technology companies and organizations.",1
"In this paper, we propose an architecture to solve a novel problem statement that has stemmed more so in recent times with an increase in demand for virtual content delivery due to the COVID-19 pandemic. All educational institutions, workplaces, research centers, etc. are trying to bridge the gap of communication during these socially distanced times with the use of online content delivery. The trend now is to create presentations, and then subsequently deliver the same using various virtual meeting platforms. The time being spent in such creation of presentations and delivering is what we try to reduce and eliminate through this paper which aims to use Machine Learning (ML) algorithms and Natural Language Processing (NLP) modules to automate the process of creating a slides-based presentation from a document, and then use state-of-the-art voice cloning models to deliver the content in the desired author's voice. We consider a structured document such as a research paper to be the content that has to be presented. The research paper is first summarized using BERT summarization techniques and condensed into bullet points that go into the slides. Tacotron inspired architecture with Encoder, Synthesizer, and a Generative Adversarial Network (GAN) based vocoder, is used to convey the contents of the slides in the author's voice (or any customized voice). Almost all learning has now been shifted to online mode, and professionals are now working from the comfort of their homes. Due to the current situation, teachers and professionals have shifted to presentations to help them in imparting information. In this paper, we aim to reduce the considerable amount of time that is taken in creating a presentation by automating this process and subsequently delivering this presentation in a customized voice, using a content delivery mechanism that can clone any voice using a short audio clip.",0
"The COVID-19 pandemic has caused an increase in demand for virtual content delivery, particularly in educational institutions, workplaces, and research centers. As a result, creating and delivering presentations using virtual meeting platforms has become a popular trend. However, this process can be time-consuming. To address this issue, we propose an architecture that utilizes Machine Learning (ML) algorithms and Natural Language Processing (NLP) modules to automate the process of creating a presentation from a structured document, such as a research paper. The paper is first summarized using BERT summarization techniques and condensed into bullet points for the slides. The content is then delivered using a state-of-the-art voice cloning model in the desired author's voice. Our architecture uses a Tacotron-inspired design with an Encoder, Synthesizer, and a Generative Adversarial Network (GAN)-based vocoder. Our goal is to reduce the time and effort required to create and deliver presentations in the current online learning and working environment.",1
"Transformer, which can benefit from global (long-range) information modeling using self-attention mechanisms, has been successful in natural language processing and 2D image classification recently. However, both local and global features are crucial for dense prediction tasks, especially for 3D medical image segmentation. In this paper, we for the first time exploit Transformer in 3D CNN for MRI Brain Tumor Segmentation and propose a novel network named TransBTS based on the encoder-decoder structure. To capture the local 3D context information, the encoder first utilizes 3D CNN to extract the volumetric spatial feature maps. Meanwhile, the feature maps are reformed elaborately for tokens that are fed into Transformer for global feature modeling. The decoder leverages the features embedded by Transformer and performs progressive upsampling to predict the detailed segmentation map. Extensive experimental results on both BraTS 2019 and 2020 datasets show that TransBTS achieves comparable or higher results than previous state-of-the-art 3D methods for brain tumor segmentation on 3D MRI scans. The source code is available at https://github.com/Wenxuan-1119/TransBTS",0
"The Transformer has proven to be successful in natural language processing and 2D image classification thanks to its ability to utilize global (long-range) information modeling through self-attention mechanisms. However, dense prediction tasks, particularly 3D medical image segmentation, require both local and global features. This paper introduces the TransBTS network, which combines Transformer with 3D CNN for MRI Brain Tumor Segmentation. The encoder first uses 3D CNN to extract volumetric spatial feature maps, which are then carefully transformed into tokens for global feature modeling by Transformer. The decoder utilizes the features embedded by Transformer to predict a detailed segmentation map through progressive upsampling. Experimental results on BraTS 2019 and 2020 datasets show that TransBTS achieves comparable or better results than previous 3D methods for brain tumor segmentation on 3D MRI scans. The source code for TransBTS is available at https://github.com/Wenxuan-1119/TransBTS.",1
"Current vision and language tasks usually take complete visual data (e.g., raw images or videos) as input, however, practical scenarios may often consist the situations where part of the visual information becomes inaccessible due to various reasons e.g., restricted view with fixed camera or intentional vision block for security concerns. As a step towards the more practical application scenarios, we introduce a novel task that aims to describe a video using the natural language dialog between two agents as a supplementary information source given incomplete visual data. Different from most existing vision-language tasks where AI systems have full access to images or video clips, which may reveal sensitive information such as recognizable human faces or voices, we intentionally limit the visual input for AI systems and seek a more secure and transparent information medium, i.e., the natural language dialog, to supplement the missing visual information. Specifically, one of the intelligent agents - Q-BOT - is given two semantic segmented frames from the beginning and the end of the video, as well as a finite number of opportunities to ask relevant natural language questions before describing the unseen video. A-BOT, the other agent who has access to the entire video, assists Q-BOT to accomplish the goal by answering the asked questions. We introduce two different experimental settings with either a generative (i.e., agents generate questions and answers freely) or a discriminative (i.e., agents select the questions and answers from candidates) internal dialog generation process. With the proposed unified QA-Cooperative networks, we experimentally demonstrate the knowledge transfer process between the two dialog agents and the effectiveness of using the natural language dialog as a supplement for incomplete implicit visions.",0
"Typically, vision and language tasks require complete visual data as input, but in real-life situations, part of the visual information may be inaccessible. This can occur due to restricted view with fixed cameras or intentional vision block for security reasons. To address this issue, we propose a new task that utilizes natural language dialog between two agents as a supplementary information source to describe a video with incomplete visual data. Unlike traditional vision-language tasks where AI systems have full access to images or video clips, our approach intentionally limits the visual input for security and transparency reasons. Q-BOT, one of the intelligent agents, is provided with two semantic segmented frames from the beginning and end of the video and a limited number of opportunities to ask relevant natural language questions. A-BOT, the other agent, has access to the entire video and assists Q-BOT by answering the questions asked. We experiment with two different settings, generative and discriminative, for the internal dialog generation process. By using unified QA-Cooperative networks, we demonstrate the effectiveness of using natural language dialog as a supplement for incomplete visual data and the knowledge transfer process between the two dialog agents.",1
"When tuning the architecture and hyperparameters of large machine learning models for on-device deployment, it is desirable to understand the optimal trade-offs between on-device latency and model accuracy. In this work, we leverage recent methodological advances in Bayesian optimization over high-dimensional search spaces and multi-objective Bayesian optimization to efficiently explore these trade-offs for a production-scale on-device natural language understanding model at Facebook.",0
"To ensure efficient on-device deployment of large machine learning models, it is important to comprehend the ideal balance between on-device latency and model accuracy during architecture and hyperparameter tuning. This study utilizes the latest developments in Bayesian optimization for exploring these trade-offs in a production-scale on-device natural language understanding model at Facebook, employing both high-dimensional search spaces and multi-objective Bayesian optimization.",1
"This work improves the quality of automated machine learning (AutoML) systems by using dataset and function descriptions while significantly decreasing computation time from minutes to milliseconds by using a zero-shot approach. Given a new dataset and a well-defined machine learning task, humans begin by reading a description of the dataset and documentation for the algorithms to be used. This work is the first to use these textual descriptions, which we call privileged information, for AutoML. We use a pre-trained Transformer model to process the privileged text and demonstrate that using this information improves AutoML performance. Thus, our approach leverages the progress of unsupervised representation learning in natural language processing to provide a significant boost to AutoML. We demonstrate that using only textual descriptions of the data and functions achieves reasonable classification performance, and adding textual descriptions to data meta-features improves classification across tabular datasets. To achieve zero-shot AutoML we train a graph neural network with these description embeddings and the data meta-features. Each node represents a training dataset, which we use to predict the best machine learning pipeline for a new test dataset in a zero-shot fashion. Our zero-shot approach rapidly predicts a high-quality pipeline for a supervised learning task and dataset. In contrast, most AutoML systems require tens or hundreds of pipeline evaluations. We show that zero-shot AutoML reduces running and prediction times from minutes to milliseconds, consistently across datasets. By speeding up AutoML by orders of magnitude this work demonstrates real-time AutoML.",0
"This study enhances the effectiveness of automated machine learning (AutoML) systems by utilizing dataset and function descriptions, while also significantly decreasing computation time from minutes to milliseconds through a zero-shot technique. The process begins with humans reading a dataset description and algorithm documentation for a given machine learning task. This research is the first to employ this textual information, known as privileged data, in AutoML. By utilizing a pre-trained Transformer model, the privileged text is processed, and it is proven that incorporating this information improves AutoML performance. This method takes advantage of the advances in unsupervised representation learning in natural language processing to give AutoML a substantial boost. The study demonstrates that employing only textual descriptions of data and functions can provide reasonable classification performance, and the addition of textual descriptions to data meta-features enhances classification across tabular datasets. To achieve zero-shot AutoML, a graph neural network is trained with these description embeddings and data meta-features. The network predicts the best machine learning pipeline for a new test dataset in a zero-shot fashion, with each training dataset represented as a node. This zero-shot method quickly predicts a high-quality pipeline for a supervised learning task and dataset, whereas most AutoML systems require numerous pipeline evaluations. The research proves that zero-shot AutoML reduces running and prediction times from minutes to milliseconds, which leads to real-time AutoML.",1
"Recently, transformers have shown great superiority in solving computer vision tasks by modeling images as a sequence of manually-split patches with self-attention mechanism. However, current architectures of vision transformers (ViTs) are simply inherited from natural language processing (NLP) tasks and have not been sufficiently investigated and optimized. In this paper, we make a further step by examining the intrinsic structure of transformers for vision tasks and propose an architecture search method, dubbed ViTAS, to search for the optimal architecture with similar hardware budgets. Concretely, we design a new effective yet efficient weight sharing paradigm for ViTs, such that architectures with different token embedding, sequence size, number of heads, width, and depth can be derived from a single super-transformer. Moreover, to cater for the variance of distinct architectures, we introduce \textit{private} class token and self-attention maps in the super-transformer. In addition, to adapt the searching for different budgets, we propose to search the sampling probability of identity operation. Experimental results show that our ViTAS attains excellent results compared to existing pure transformer architectures. For example, with $1.3$G FLOPs budget, our searched architecture achieves $74.7\%$ top-$1$ accuracy on ImageNet and is $2.5\%$ superior than the current baseline ViT architecture. Code is available at \url{https://github.com/xiusu/ViTAS}.",0
"Transformers have recently demonstrated exceptional capabilities in addressing computer vision tasks by representing images as a series of manually separated patches using self-attention mechanisms. However, the present designs of Vision Transformers (ViTs) are primarily borrowed from Natural Language Processing (NLP) tasks and lack sufficient investigation and optimization. This study aims to investigate the inherent structure of transformers in vision tasks and proposes a novel architecture search method, ViTAS, to identify the optimal architecture while adhering to similar hardware budgets. ViTAS incorporates a new weight sharing paradigm for ViTs that facilitates the derivation of various architectures with different token embedding, sequence size, number of heads, width, and depth from a single super-transformer. To cater to the diversity of different architectures, private class token and self-attention maps are introduced in the super-transformer. Additionally, to adapt the search for different budgets, the sampling probability of identity operation is searched. Experimental results indicate that ViTAS outperforms existing transformer architectures, achieving a top-1 accuracy of 74.7% on ImageNet with a 1.3G FLOPs budget, which is 2.5% superior to the current ViT baseline architecture. The code for ViTAS is accessible at https://github.com/xiusu/ViTAS.",1
"Breakthroughs in machine learning in the last decade have led to `digital intelligence', i.e. machine learning models capable of learning from vast amounts of labeled data to perform several digital tasks such as speech recognition, face recognition, machine translation and so on. The goal of this thesis is to make progress towards designing algorithms capable of `physical intelligence', i.e. building intelligent autonomous navigation agents capable of learning to perform complex navigation tasks in the physical world involving visual perception, natural language understanding, reasoning, planning, and sequential decision making. Despite several advances in classical navigation methods in the last few decades, current navigation agents struggle at long-term semantic navigation tasks. In the first part of the thesis, we discuss our work on short-term navigation using end-to-end reinforcement learning to tackle challenges such as obstacle avoidance, semantic perception, language grounding, and reasoning. In the second part, we present a new class of navigation methods based on modular learning and structured explicit map representations, which leverage the strengths of both classical and end-to-end learning methods, to tackle long-term navigation tasks. We show that these methods are able to effectively tackle challenges such as localization, mapping, long-term planning, exploration and learning semantic priors. These modular learning methods are capable of long-term spatial and semantic understanding and achieve state-of-the-art results on various navigation tasks.",0
"Over the last ten years, advancements in machine learning have resulted in the development of ""digital intelligence."" This refers to machine learning models that can learn from vast amounts of labeled data to perform various digital tasks like speech recognition, face recognition, and machine translation. The objective of this thesis is to advance the design of algorithms that are capable of ""physical intelligence."" This means creating intelligent autonomous navigation agents that can learn to perform complex navigation tasks in the physical world. These tasks involve visual perception, natural language understanding, reasoning, planning, and sequential decision making. While classical navigation methods have made considerable progress in recent decades, current navigation agents struggle with long-term semantic navigation tasks. The thesis is divided into two parts. The first part discusses our work on short-term navigation using end-to-end reinforcement learning to overcome challenges like obstacle avoidance, semantic perception, language grounding, and reasoning. The second part presents a new class of navigation methods based on modular learning and structured explicit map representations. These methods combine the strengths of classical and end-to-end learning techniques to deal with long-term navigation tasks, such as localization, mapping, long-term planning, exploration, and learning semantic priors. These modular learning methods are proficient in long-term spatial and semantic understanding and deliver state-of-the-art results on various navigation tasks.",1
"Twitter is a useful resource to analyze peoples' opinions on various topics. Often these topics are correlated or associated with locations from where these Tweet posts are made. For example, restaurant owners may need to know where their target customers eat with respect to the sentiment of the posts made related to food, policy planners may need to analyze citizens' opinion on relevant issues such as crime, safety, congestion, etc. with respect to specific parts of the city, or county or state. As promising as this is, less than $1\%$ of the crawled Tweet posts come with geolocation tags. That makes accurate prediction of Tweet posts for the non geo-tagged tweets very critical to analyze data in various domains. In this research, we utilized millions of Twitter posts and end-users domain expertise to build a set of deep neural network models using natural language processing (NLP) techniques, that predicts the geolocation of non geo-tagged Tweet posts at various level of granularities such as neighborhood, zipcode, and longitude with latitudes. With multiple neural architecture experiments, and a collaborative human-machine workflow design, our ongoing work on geolocation detection shows promising results that empower end-users to correlate relationship between variables of choice with the location information.",0
"Analyzing people's opinions on different topics is possible with the help of Twitter, which is often associated with specific locations. This information can be useful for various purposes, such as restaurant owners who want to know where their customers eat and policy planners who need to analyze citizens' views on issues like crime, safety, and congestion. However, less than 1% of Tweet posts come with geolocation tags, making accurate prediction of non-geo-tagged tweets crucial. To address this, we used millions of Twitter posts and deep neural network models with natural language processing techniques to predict geolocation at various levels, such as neighborhood, zipcode, and longitude with latitudes. Through multiple neural architecture experiments and a collaborative human-machine workflow, our ongoing research on geolocation detection has promising results that allow end-users to correlate variables with location information.",1
"Causality knowledge is vital to building robust AI systems. Deep learning models often perform poorly on tasks that require causal reasoning, which is often derived using some form of commonsense knowledge not immediately available in the input but implicitly inferred by humans. Prior work has unraveled spurious observational biases that models fall prey to in the absence of causality. While language representation models preserve contextual knowledge within learned embeddings, they do not factor in causal relationships during training. By blending causal relationships with the input features to an existing model that performs visual cognition tasks (such as scene understanding, video captioning, video question-answering, etc.), better performance can be achieved owing to the insight causal relationships bring about. Recently, several models have been proposed that have tackled the task of mining causal data from either the visual or textual modality. However, there does not exist widespread research that mines causal relationships by juxtaposing the visual and language modalities. While images offer a rich and easy-to-process resource for us to mine causality knowledge from, videos are denser and consist of naturally time-ordered events. Also, textual information offers details that could be implicit in videos. We propose iReason, a framework that infers visual-semantic commonsense knowledge using both videos and natural language captions. Furthermore, iReason's architecture integrates a causal rationalization module to aid the process of interpretability, error analysis and bias detection. We demonstrate the effectiveness of iReason using a two-pronged comparative analysis with language representation learning models (BERT, GPT-2) as well as current state-of-the-art multimodal causality models.",0
"Having knowledge of causality is crucial for creating strong AI systems. Deep learning models often struggle with tasks that require causal reasoning, which humans infer from commonsense knowledge not immediately present in the input. Previous research has identified observational biases that models fall victim to in the absence of causality. Although language representation models preserve contextual knowledge, they do not incorporate causal relationships into training. By incorporating causal relationships with existing models that perform visual cognition tasks, better performance can be achieved. Some models have mined causal data from either visual or textual modalities, but few have juxtaposed both modalities. Images provide a rich resource for mining causality knowledge, while videos offer dense, time-ordered events and textual information provides implicit details. We propose iReason, a framework that uses both videos and natural language captions to infer visual-semantic commonsense knowledge and integrates a causal rationalization module to aid interpretability, error analysis, and bias detection. We compare iReason to language representation learning models (BERT, GPT-2) and state-of-the-art multimodal causality models and demonstrate its effectiveness.",1
"Initially developed for natural language processing (NLP), Transformers are now widely used for source code processing, due to the format similarity between source code and text. In contrast to natural language, source code is strictly structured, i.e., it follows the syntax of the programming language. Several recent works develop Transformer modifications for capturing syntactic information in source code. The drawback of these works is that they do not compare to each other and consider different tasks. In this work, we conduct a thorough empirical study of the capabilities of Transformers to utilize syntactic information in different tasks. We consider three tasks (code completion, function naming and bug fixing) and re-implement different syntax-capturing modifications in a unified framework. We show that Transformers are able to make meaningful predictions based purely on syntactic information and underline the best practices of taking the syntactic information into account for improving the performance of the model.",0
"Transformers were initially designed for natural language processing but have now become popular for processing source code due to the similarities between the two formats. Unlike natural language, source code is structured and adheres to the syntax of the programming language. To capture syntactic information in source code, recent works have introduced Transformer modifications. However, these modifications are not compared to each other and are designed for different tasks. This study thoroughly examines the ability of Transformers to utilize syntactic information in various tasks, such as code completion, function naming, and bug fixing. We implement different syntax-capturing modifications within a unified framework and demonstrate that Transformers can make meaningful predictions based solely on syntactic information. Furthermore, we highlight best practices for incorporating syntactic information to optimize model performance.",1
"Transformers recently are adapted from the community of natural language processing as a promising substitute of convolution-based neural networks for visual learning tasks. However, its supremacy degenerates given an insufficient amount of training data (e.g., ImageNet). To make it into practical utility, we propose a novel distillation-based method to train vision transformers. Unlike previous works, where merely heavy convolution-based teachers are provided, we introduce lightweight teachers with different architectural inductive biases (e.g., convolution and involution) to co-advise the student transformer. The key is that teachers with different inductive biases attain different knowledge despite that they are trained on the same dataset, and such different knowledge compounds and boosts the student's performance during distillation. Equipped with this cross inductive bias distillation method, our vision transformers (termed as CivT) outperform all previous transformers of the same architecture on ImageNet.",0
"Transformers have become a promising alternative to convolution-based neural networks for visual learning tasks, having been adapted from the natural language processing community. However, their superiority diminishes when there is insufficient training data, such as in the case of ImageNet. In order to make transformers practically useful, we suggest a novel distillation-based technique for their training. Unlike previous methods, we introduce lightweight teachers with varying architectural inductive biases, including convolution and involution, to co-teach the student transformer. Teachers with different biases gain distinct knowledge, even when trained on the same dataset, and this knowledge compounds and enhances the student's performance during distillation. Our vision transformers, called CivT, outperform previous transformers with the same architecture on ImageNet thanks to this cross-inductive-bias distillation approach.",1
"Automatic code synthesis from natural language descriptions is a challenging task. We witness massive progress in developing code generation systems for domain-specific languages (DSLs) employing sequence-to-sequence deep learning techniques in the recent past. In this paper, we specifically experiment with \textsc{AlgoLisp} DSL-based generative models and showcase the existence of significant dataset bias through different classes of adversarial examples. We also experiment with two variants of Transformer-based models that outperform all existing \textsc{AlgoLisp} DSL-based code generation baselines. Consistent with the current state-of-the-art systems, our proposed models, too, achieve poor performance under adversarial settings. Therefore, we propose several dataset augmentation techniques to reduce bias and showcase their efficacy using robust experimentation.",0
"Generating code automatically from natural language descriptions is a challenging task. However, there have been considerable advancements in developing code generation systems for domain-specific languages (DSLs) using sequence-to-sequence deep learning techniques. This paper focuses on experimenting with \textsc{AlgoLisp} DSL-based generative models and exposes significant dataset bias through various classes of adversarial examples. Additionally, two variants of Transformer-based models are tested and found to outperform all existing \textsc{AlgoLisp} DSL-based code generation baselines. Despite the proposed models being consistent with current state-of-the-art systems, they still demonstrate poor performance under adversarial settings. Therefore, several dataset augmentation techniques are proposed to reduce bias and their effectiveness is demonstrated through robust experimentation.",1
"Hierarchical Agglomerative Clustering (HAC) algorithms are extensively utilized in modern data science, and seek to partition the dataset into clusters while generating a hierarchical relationship between the data samples. HAC algorithms are employed in many applications, such as biology, natural language processing, and recommender systems. Thus, it is imperative to ensure that these algorithms are fair -- even if the dataset contains biases against certain protected groups, the cluster outputs generated should not discriminate against samples from any of these groups. However, recent work in clustering fairness has mostly focused on center-based clustering algorithms, such as k-median and k-means clustering. In this paper, we propose fair algorithms for performing HAC that enforce fairness constraints 1) irrespective of the distance linkage criteria used, 2) generalize to any natural measures of clustering fairness for HAC, 3) work for multiple protected groups, and 4) have competitive running times to vanilla HAC. Through extensive experiments on multiple real-world UCI datasets, we show that our proposed algorithm finds fairer clusterings compared to vanilla HAC as well as other state-of-the-art fair clustering approaches.",0
"Modern data science commonly employs Hierarchical Agglomerative Clustering (HAC) algorithms, which aim to create clusters in the dataset while establishing a hierarchical relationship between data samples. HAC algorithms are widely used in various fields, including biology, natural language processing, and recommender systems. It is crucial to ensure that these algorithms are unbiased and do not discriminate against any protected group, even if the dataset contains biases. However, previous research on clustering fairness has mainly focused on center-based clustering algorithms like k-median and k-means clustering. This paper proposes fair algorithms for HAC that enforce fairness constraints, regardless of the distance linkage criteria used, and apply to multiple natural measures of clustering fairness for HAC, with a competitive running time to vanilla HAC. The proposed algorithm's efficacy is demonstrated through experiments on several real-world UCI datasets, showing that it produces fairer clusterings than vanilla HAC and other state-of-the-art fair clustering approaches.",1
"Pruning is an effective method to reduce the memory footprint and FLOPs associated with neural network models. However, existing structured-pruning methods often result in significant accuracy degradation for moderate pruning levels. To address this problem, we introduce a new Hessian Aware Pruning (HAP) method coupled with a Neural Implant approach that uses second-order sensitivity as a metric for structured pruning. The basic idea is to prune insensitive components and to use a Neural Implant for moderately sensitive components, instead of completely pruning them. For the latter approach, the moderately sensitive components are replaced with with a low rank implant that is smaller and less computationally expensive than the original component. We use the relative Hessian trace to measure sensitivity, as opposed to the magnitude based sensitivity metric commonly used in the literature. We test HAP for both computer vision tasks and natural language tasks, and we achieve new state-of-the-art results. Specifically, HAP achieves less than $0.1\%$/$0.5\%$ degradation on PreResNet29/ResNet50 (CIFAR-10/ImageNet) with more than 70\%/50\% of parameters pruned. Meanwhile, HAP also achieves significantly better performance (up to 0.8\% with 60\% of parameters pruned) as compared to gradient based method for head pruning on transformer-based models. The framework has been open sourced and available online.",0
"Pruning neural network models is a useful way to reduce memory usage and FLOPs, but commonly used structured-pruning methods often lead to notable accuracy reduction with moderate pruning levels. To solve this issue, we propose a new approach called Hessian Aware Pruning (HAP), which utilizes second-order sensitivity to guide structured pruning. We identify insensitive components to be pruned and use a Neural Implant technique for moderately sensitive components, instead of fully pruning them. For the latter, we substitute the moderately sensitive components with a smaller, less computationally intensive low rank implant. We measure sensitivity using the relative Hessian trace instead of the magnitude-based sensitivity metric commonly used. We test HAP on computer vision and natural language tasks, achieving state-of-the-art results. HAP achieves less than $0.1\%$/$0.5\%$ degradation on PreResNet29/ResNet50 (CIFAR-10/ImageNet) with more than 70\%/50\% of parameters pruned. Moreover, HAP outperforms the gradient-based method for head pruning on transformer-based models, with up to 0.8\% improvement and 60\% of parameters pruned. Our framework is available online as open source.",1
"As soon as abstract mathematical computations were adapted to computation on digital computers, the problem of efficient representation, manipulation, and communication of the numerical values in those computations arose. Strongly related to the problem of numerical representation is the problem of quantization: in what manner should a set of continuous real-valued numbers be distributed over a fixed discrete set of numbers to minimize the number of bits required and also to maximize the accuracy of the attendant computations? This perennial problem of quantization is particularly relevant whenever memory and/or computational resources are severely restricted, and it has come to the forefront in recent years due to the remarkable performance of Neural Network models in computer vision, natural language processing, and related areas. Moving from floating-point representations to low-precision fixed integer values represented in four bits or less holds the potential to reduce the memory footprint and latency by a factor of 16x; and, in fact, reductions of 4x to 8x are often realized in practice in these applications. Thus, it is not surprising that quantization has emerged recently as an important and very active sub-area of research in the efficient implementation of computations associated with Neural Networks. In this article, we survey approaches to the problem of quantizing the numerical values in deep Neural Network computations, covering the advantages/disadvantages of current methods. With this survey and its organization, we hope to have presented a useful snapshot of the current research in quantization for Neural Networks and to have given an intelligent organization to ease the evaluation of future research in this area.",0
"The advent of digital computers led to the application of abstract mathematical computations, but with it came the challenge of efficiently representing, manipulating, and communicating numerical values. The issue of quantization arose as a result, which involves distributing continuous real-valued numbers over a fixed set of discrete numbers to minimize the number of bits required and maximize accuracy. This problem is especially pertinent when dealing with limited memory and computational resources, and has gained prominence due to the impressive performance of Neural Network models in various fields. By transitioning from floating-point to low-precision fixed integer values represented in four bits or less, memory footprint and latency can be reduced by a factor of 16x, with reductions of 4x to 8x being common in practice. Consequently, quantization has become a crucial area of research in the efficient implementation of Neural Network computations. This article provides an overview of approaches to quantizing numerical values in deep Neural Network computations, including the advantages and disadvantages of current methods. Our hope is to offer a valuable snapshot of current research in quantization for Neural Networks and provide a framework for evaluating future research in this area.",1
"Transfer learning with pre-training on large-scale datasets has played an increasingly significant role in computer vision and natural language processing recently. However, as there exist numerous application scenarios that have distinctive demands such as certain latency constraints and specialized data distributions, it is prohibitively expensive to take advantage of large-scale pre-training for per-task requirements. In this paper, we focus on the area of object detection and present a transfer learning system named GAIA, which could automatically and efficiently give birth to customized solutions according to heterogeneous downstream needs. GAIA is capable of providing powerful pre-trained weights, selecting models that conform to downstream demands such as latency constraints and specified data domains, and collecting relevant data for practitioners who have very few datapoints for their tasks. With GAIA, we achieve promising results on COCO, Objects365, Open Images, Caltech, CityPersons, and UODB which is a collection of datasets including KITTI, VOC, WiderFace, DOTA, Clipart, Comic, and more. Taking COCO as an example, GAIA is able to efficiently produce models covering a wide range of latency from 16ms to 53ms, and yields AP from 38.2 to 46.5 without whistles and bells. To benefit every practitioner in the community of object detection, GAIA is released at https://github.com/GAIA-vision.",0
"Recently, transfer learning has become increasingly important in computer vision and natural language processing through pre-training on large-scale datasets. However, due to unique demands in various application scenarios, it is impractical to use large-scale pre-training for per-task requirements. In this paper, our focus is on object detection and introducing a transfer learning system, named GAIA, that can automatically and efficiently generate customized solutions based on downstream needs. GAIA provides pre-trained weights, selects models that meet downstream demands such as latency constraints and specified data domains, and collects relevant data for practitioners with limited datapoints. We achieve promising results on various datasets, including COCO, Objects365, and Open Images. For example, GAIA can efficiently produce models for COCO with latency ranging from 16ms to 53ms and yields AP from 38.2 to 46.5. To benefit the object detection community, GAIA is available at https://github.com/GAIA-vision.",1
"Unsupervised domain adaptation is used in many machine learning applications where, during training, a model has access to unlabeled data in the target domain, and a related labeled dataset. In this paper, we introduce a novel and general domain-adversarial framework. Specifically, we derive a novel generalization bound for domain adaptation that exploits a new measure of discrepancy between distributions based on a variational characterization of f-divergences. It recovers the theoretical results from Ben-David et al. (2010a) as a special case and supports divergences used in practice. Based on this bound, we derive a new algorithmic framework that introduces a key correction in the original adversarial training method of Ganin et al. (2016). We show that many regularizers and ad-hoc objectives introduced over the last years in this framework are then not required to achieve performance comparable to (if not better than) state-of-the-art domain-adversarial methods. Experimental analysis conducted on real-world natural language and computer vision datasets show that our framework outperforms existing baselines, and obtains the best results for f-divergences that were not considered previously in domain-adversarial learning.",0
"The use of unsupervised domain adaptation is prevalent in various machine learning applications. During training, a model is provided access to an unlabeled dataset in the target domain, along with a related labeled dataset. This paper introduces a new and general domain-adversarial framework that utilizes a novel generalization bound for domain adaptation. This bound capitalizes on a new measure of discrepancy between distributions, which is based on a variational characterization of f-divergences. The framework incorporates a correction to the original adversarial training method of Ganin et al. (2016) based on this bound. The proposed approach yields performance comparable to (if not better than) state-of-the-art domain-adversarial methods without the need for many regularizers and ad-hoc objectives. Real-world experiments conducted on natural language and computer vision datasets demonstrate that our framework outperforms existing baselines and achieves the best results for f-divergences that were not previously considered in domain-adversarial learning.",1
"Recent advances in document image analysis (DIA) have been primarily driven by the application of neural networks. Ideally, research outcomes could be easily deployed in production and extended for further investigation. However, various factors like loosely organized codebases and sophisticated model configurations complicate the easy reuse of important innovations by a wide audience. Though there have been on-going efforts to improve reusability and simplify deep learning (DL) model development in disciplines like natural language processing and computer vision, none of them are optimized for challenges in the domain of DIA. This represents a major gap in the existing toolkit, as DIA is central to academic research across a wide range of disciplines in the social sciences and humanities. This paper introduces layoutparser, an open-source library for streamlining the usage of DL in DIA research and applications. The core layoutparser library comes with a set of simple and intuitive interfaces for applying and customizing DL models for layout detection, character recognition, and many other document processing tasks. To promote extensibility, layoutparser also incorporates a community platform for sharing both pre-trained models and full document digitization pipelines. We demonstrate that layoutparser is helpful for both lightweight and large-scale digitization pipelines in real-word use cases. The library is publicly available at https://layout-parser.github.io/.",0
"The progress made in document image analysis (DIA) has been largely spurred by the use of neural networks. The ultimate goal is to make research easily applicable in production and expandable for further research. However, various hurdles such as disorganized codebases and complex model configurations make it difficult for a broad audience to reuse important innovations. While attempts have been made to simplify the development of deep learning (DL) models in fields like natural language processing and computer vision, none have been optimized for DIA challenges. This is a significant gap in the toolkit as DIA is critical to academic research across various disciplines. In response, this article introduces layoutparser, an open-source library that simplifies the use of DL in DIA research and applications. The core layoutparser library features easy-to-use interfaces for applying and customizing DL models for layout detection, character recognition, and other document processing tasks. To promote flexibility, layoutparser also includes a community platform for sharing pre-trained models and full document digitization pipelines. We demonstrate that layoutparser is useful for both small and large-scale digitization pipelines in real-world scenarios. The library is available to the public at https://layout-parser.github.io/.",1
"Recently introduced self-supervised methods for image representation learning provide on par or superior results to their fully supervised competitors, yet the corresponding efforts to explain the self-supervised approaches lag behind. Motivated by this observation, we introduce a novel visual probing framework for explaining the self-supervised models by leveraging probing tasks employed previously in natural language processing. The probing tasks require knowledge about semantic relationships between image parts. Hence, we propose a systematic approach to obtain analogs of natural language in vision, such as visual words, context, and taxonomy. Our proposal is grounded in Marr's computational theory of vision and concerns features like textures, shapes, and lines. We show the effectiveness and applicability of those analogs in the context of explaining self-supervised representations. Our key findings emphasize that relations between language and vision can serve as an effective yet intuitive tool for discovering how machine learning models work, independently of data modality. Our work opens a plethora of research pathways towards more explainable and transparent AI.",0
"Self-supervised methods for image representation learning have been recently introduced and have shown comparable or better results than their fully supervised counterparts. However, explanations for these self-supervised approaches are lacking. To address this issue, we propose a novel visual probing framework that utilizes probing tasks from natural language processing to explain self-supervised models. These tasks require knowledge of semantic relationships between image parts, which we obtain through a systematic approach that includes visual words, context, and taxonomy. Our approach is based on Marr's computational theory of vision and focuses on features such as textures, shapes, and lines. By applying these analogs to explain self-supervised representations, we demonstrate their effectiveness and applicability. Our findings indicate that the relationship between language and vision can serve as an intuitive tool for understanding machine learning models, regardless of data modality. Our work suggests new avenues for research towards explainable and transparent AI.",1
"Deep Learning has revolutionized the fields of computer vision, natural language understanding, speech recognition, information retrieval and more. However, with the progressive improvements in deep learning models, their number of parameters, latency, resources required to train, etc. have all have increased significantly. Consequently, it has become important to pay attention to these footprint metrics of a model as well, not just its quality. We present and motivate the problem of efficiency in deep learning, followed by a thorough survey of the five core areas of model efficiency (spanning modeling techniques, infrastructure, and hardware) and the seminal work there. We also present an experiment-based guide along with code, for practitioners to optimize their model training and deployment. We believe this is the first comprehensive survey in the efficient deep learning space that covers the landscape of model efficiency from modeling techniques to hardware support. Our hope is that this survey would provide the reader with the mental model and the necessary understanding of the field to apply generic efficiency techniques to immediately get significant improvements, and also equip them with ideas for further research and experimentation to achieve additional gains.",0
"The emergence of Deep Learning has brought about significant changes in various fields such as computer vision, natural language understanding, speech recognition, and information retrieval. However, as deep learning models continue to advance, their parameters, latency, and resource requirements have also increased considerably. Therefore, it is essential to consider these factors, in addition to the model's quality, when evaluating its effectiveness. Our study focuses on the issue of efficiency in deep learning and provides an in-depth analysis of the five core areas of model efficiency, which include modeling techniques, infrastructure, and hardware. We also offer a practical guide, complete with code, to help practitioners optimize their model training and deployment. This is the first comprehensive survey in the efficient deep learning space that covers the entire spectrum of model efficiency, including hardware support. Our goal is to equip readers with the necessary knowledge and understanding of the field to apply generic efficiency techniques and achieve significant improvements. Additionally, we hope to provide ideas for further research and experimentation to achieve even greater gains.",1
"We propose a method for efficiently incorporating constraints into a stochastic gradient Langevin framework for the training of deep neural networks. Constraints allow direct control of the parameter space of the model. Appropriately designed, they reduce the vanishing/exploding gradient problem, control weight magnitudes and stabilize deep neural networks and thus improve the robustness of training algorithms and the generalization capabilities of the trained neural network. We present examples of constrained training methods motivated by orthogonality preservation for weight matrices and explicit weight normalizations. We describe the methods in the overdamped formulation of Langevin dynamics and the underdamped form, in which momenta help to improve sampling efficiency. The methods are explored in test examples in image classification and natural language processing.",0
"Our proposed approach efficiently integrates constraints into a stochastic gradient Langevin framework, which is used for training deep neural networks. Constraints provide direct control over the model's parameter space, and when designed appropriately, they can minimize the vanishing/exploding gradient problem, regulate weight magnitudes, and stabilize deep neural networks. This improves the resilience of training algorithms and the overall generalization capabilities of the neural network. We offer examples of constrained training methods, including orthogonality preservation for weight matrices and explicit weight normalizations. We explain these techniques in both the overdamped and underdamped formulations of Langevin dynamics, where momentum improves sampling efficiency. Furthermore, we test these methods in image classification and natural language processing examples.",1
"Compositional generalization is the ability to generalize systematically to a new data distribution by combining known components. Although humans seem to have a great ability to generalize compositionally, state-of-the-art neural models struggle to do so. In this work, we study compositional generalization in classification tasks and present two main contributions. First, we study ways to convert a natural language sequence-to-sequence dataset to a classification dataset that also requires compositional generalization. Second, we show that providing structural hints (specifically, providing parse trees and entity links as attention masks for a Transformer model) helps compositional generalization.",0
"The capacity to systematically generalize to a new set of data by combining familiar components is known as compositional generalization. While humans appear to possess exceptional compositional generalization abilities, current neural models have difficulties with this task. In this study, we examine compositional generalization in classification tasks and make two primary contributions. Initially, we explore methods for converting a natural language sequence-to-sequence dataset into a classification dataset that necessitates compositional generalization. Secondly, we demonstrate that giving structural cues, such as parse trees and entity links, as attention masks to a Transformer model enhances compositional generalization.",1
"Following their success in natural language processing, transformers have recently shown much promise for computer vision. The self-attention operation underlying transformers yields global interactions between all tokens ,i.e. words or image patches, and enables flexible modelling of image data beyond the local interactions of convolutions. This flexibility, however, comes with a quadratic complexity in time and memory, hindering application to long sequences and high-resolution images. We propose a ""transposed"" version of self-attention that operates across feature channels rather than tokens, where the interactions are based on the cross-covariance matrix between keys and queries. The resulting cross-covariance attention (XCA) has linear complexity in the number of tokens, and allows efficient processing of high-resolution images. Our cross-covariance image transformer (XCiT) is built upon XCA. It combines the accuracy of conventional transformers with the scalability of convolutional architectures. We validate the effectiveness and generality of XCiT by reporting excellent results on multiple vision benchmarks, including image classification and self-supervised feature learning on ImageNet-1k, object detection and instance segmentation on COCO, and semantic segmentation on ADE20k.",0
"Transformers, which have been successful in natural language processing, have recently shown potential in computer vision. The self-attention operation in transformers enables global interactions between all tokens, such as words or image patches, allowing for flexible modeling of image data beyond local interactions of convolutions. However, this flexibility comes at the cost of quadratic complexity in time and memory, making it difficult to apply to lengthy sequences and high-resolution images. To overcome this, we propose a ""transposed"" version of self-attention that operates across feature channels instead of tokens, based on the cross-covariance matrix between keys and queries. This results in cross-covariance attention (XCA), which has linear complexity in the number of tokens and enables efficient processing of high-resolution images. Our cross-covariance image transformer (XCiT) leverages XCA, combining the accuracy of conventional transformers with the scalability of convolutional architectures. We demonstrate the effectiveness and versatility of XCiT through exceptional results on various vision benchmarks, including image classification, object detection, instance segmentation, and semantic segmentation. These benchmarks include ImageNet-1k, COCO, and ADE20k.",1
"Inductive program synthesis, or inferring programs from examples of desired behavior, offers a general paradigm for building interpretable, robust, and generalizable machine learning systems. Effective program synthesis depends on two key ingredients: a strong library of functions from which to build programs, and an efficient search strategy for finding programs that solve a given task. We introduce LAPS (Language for Abstraction and Program Search), a technique for using natural language annotations to guide joint learning of libraries and neurally-guided search models for synthesis. When integrated into a state-of-the-art library learning system (DreamCoder), LAPS produces higher-quality libraries and improves search efficiency and generalization on three domains -- string editing, image composition, and abstract reasoning about scenes -- even when no natural language hints are available at test time.",0
"The process of inductive program synthesis, which involves deriving programs from examples of desired behavior, has the potential to create machine learning systems that are easy to understand, robust, and adaptable. To achieve effective program synthesis, two important factors are required: a comprehensive library of functions for constructing programs, and an efficient search approach for identifying programs that address a given task. Our research introduces LAPS (Language for Abstraction and Program Search), a method that employs natural language annotations to facilitate the collaborative learning of libraries and search models for synthesis. When integrated into the DreamCoder library learning system, LAPS enhances the quality of libraries and improves search efficiency and generalization across three domains - string editing, image composition, and abstract scene analysis - even in the absence of natural language hints during testing.",1
"Predicting gender by the name is not a simple task. In many applications, especially in the natural language processing (NLP) field, this task may be necessary, mainly when considering foreign names. Some machine learning algorithms can satisfactorily perform the prediction. In this paper, we examined and implemented feedforward and recurrent deep neural network models, such as MLP, RNN, GRU, CNN, and BiLSTM, to classify gender through the first name. A dataset of Brazilian names is used to train and evaluate the models. We analyzed the accuracy, recall, precision, and confusion matrix to measure the models' performances. The results indicate that the gender prediction can be performed from the feature extraction strategy looking at the names as a set of strings. Some models accurately predict the gender in more than 90% of the cases. The recurrent models overcome the feedforward models in this binary classification problem.",0
"Identifying gender based on a person's name is a complex task, particularly when dealing with foreign names in the field of natural language processing (NLP). Machine learning algorithms can effectively perform this task, and in this article, we explore the implementation of deep neural network models, including MLP, RNN, GRU, CNN, and BiLSTM, for gender classification using first names from a dataset of Brazilian names. We evaluate the models using accuracy, recall, precision, and confusion matrix to determine their performance. Our findings suggest that predicting gender from names can be achieved through feature extraction strategies that treat names as sets of strings. Some models can predict gender with over 90% accuracy, with recurrent models outperforming feedforward models in this binary classification problem.",1
"This work introduces World-GAN, the first method to perform data-driven Procedural Content Generation via Machine Learning in Minecraft from a single example. Based on a 3D Generative Adversarial Network (GAN) architecture, we are able to create arbitrarily sized world snippets from a given sample. We evaluate our approach on creations from the community as well as structures generated with the Minecraft World Generator. Our method is motivated by the dense representations used in Natural Language Processing (NLP) introduced with word2vec [1]. The proposed block2vec representations make World-GAN independent from the number of different blocks, which can vary a lot in Minecraft, and enable the generation of larger levels. Finally, we demonstrate that changing this new representation space allows us to change the generated style of an already trained generator. World-GAN enables its users to generate Minecraft worlds based on parts of their creations.",0
"The first-ever data-driven Procedural Content Generation method via Machine Learning in Minecraft from a single example is introduced in this work, called World-GAN. By utilizing a 3D Generative Adversarial Network (GAN) architecture, our approach can produce world snippets of any size from a given sample. Our method is evaluated on community creations and structures that were generated with the Minecraft World Generator. Inspired by the dense representations used in Natural Language Processing (NLP), we introduced block2vec representations that make World-GAN independent from the varying number of blocks in Minecraft, allowing the generation of larger levels. Moreover, we demonstrate that changing this new representation space can modify the generated style of an already trained generator. World-GAN empowers users to create Minecraft worlds based on parts of their own creations.",1
"Combining Natural Language with Vision represents a unique and interesting challenge in the domain of Artificial Intelligence. The AI City Challenge Track 5 for Natural Language-Based Vehicle Retrieval focuses on the problem of combining visual and textual information, applied to a smart-city use case. In this paper, we present All You Can Embed (AYCE), a modular solution to correlate single-vehicle tracking sequences with natural language. The main building blocks of the proposed architecture are (i) BERT to provide an embedding of the textual descriptions, (ii) a convolutional backbone along with a Transformer model to embed the visual information. For the training of the retrieval model, a variation of the Triplet Margin Loss is proposed to learn a distance measure between the visual and language embeddings. The code is publicly available at https://github.com/cscribano/AYCE_2021.",0
"Artificial Intelligence faces an interesting and unique challenge when combining Natural Language with Vision. The AI City Challenge Track 5 aims to tackle the issue of merging visual and textual information in a smart-city use case for Natural Language-Based Vehicle Retrieval. Our paper introduces a modular solution called All You Can Embed (AYCE), which correlates single-vehicle tracking sequences with natural language. The proposed architecture includes BERT to provide an embedding of textual descriptions and a convolutional backbone along with a Transformer model to embed visual information. To train the retrieval model, we propose a variation of the Triplet Margin Loss to learn the distance measure between visual and language embeddings. The code for AYCE is available to the public on https://github.com/cscribano/AYCE_2021.",1
"Purpose: Image classification is perhaps the most fundamental task in imaging AI. However, labeling images is time-consuming and tedious. We have recently demonstrated that reinforcement learning (RL) can classify 2D slices of MRI brain images with high accuracy. Here we make two important steps toward speeding image classification: Firstly, we automatically extract class labels from the clinical reports. Secondly, we extend our prior 2D classification work to fully 3D image volumes from our institution. Hence, we proceed as follows: in Part 1, we extract labels from reports automatically using the SBERT natural language processing approach. Then, in Part 2, we use these labels with RL to train a classification Deep-Q Network (DQN) for 3D image volumes.   Methods: For Part 1, we trained SBERT with 90 radiology report impressions. We then used the trained SBERT to predict class labels for use in Part 2. In Part 2, we applied multi-step image classification to allow for combined Deep-Q learning using 3D convolutions and TD(0) Q learning. We trained on a set of 90 images. We tested on a separate set of 61 images, again using the classes predicted from patient reports by the trained SBERT in Part 1. For comparison, we also trained and tested a supervised deep learning classification network on the same set of training and testing images using the same labels.   Results: Part 1: Upon training with the corpus of radiology reports, the SBERT model had 100% accuracy for both normal and metastasis-containing scans. Part 2: Then, using these labels, whereas the supervised approach quickly overfit the training data and as expected performed poorly on the testing set (66% accuracy, just over random guessing), the reinforcement learning approach achieved an accuracy of 92%. The results were found to be statistically significant, with a p-value of 3.1 x 10^-5.",0
"The main objective of imaging AI is to classify images, which is a tedious and time-consuming task that involves labeling. However, we have recently demonstrated that reinforcement learning (RL) can efficiently classify 2D MRI brain image slices with high accuracy. To speed up the image classification process, we have taken two significant steps. Firstly, we have used the SBERT natural language processing approach to automatically extract class labels from clinical reports. Secondly, we have extended our prior work on 2D classification to fully 3D image volumes from our institution. The process involves two parts. In Part 1, we trained SBERT using 90 radiology report impressions to automatically extract class labels. In Part 2, we used these labels along with RL to train a classification Deep-Q Network (DQN) for 3D image volumes. We applied multi-step image classification to allow for combined Deep-Q learning using 3D convolutions and TD(0) Q learning. We trained on a set of 90 images and tested on a separate set of 61 images using the same labels. For comparison, we also trained and tested a supervised deep learning classification network on the same set of training and testing images using the same labels. The results showed that SBERT had 100% accuracy in predicting class labels. Furthermore, the RL approach achieved an accuracy of 92%, which was statistically significant (p-value of 3.1 x 10^-5), while the supervised approach performed poorly.",1
"This paper is a submission to the Alzheimer's Dementia Recognition through Spontaneous Speech (ADReSS) challenge, which aims to develop methods that can assist in the automated prediction of severity of Alzheimer's Disease from speech data. We focus on acoustic and natural language features for cognitive impairment detection in spontaneous speech in the context of Alzheimer's Disease Diagnosis and the mini-mental state examination (MMSE) score prediction. We proposed a model that obtains unimodal decisions from different LSTMs, one for each modality of text and audio, and then combines them using a gating mechanism for the final prediction. We focused on sequential modelling of text and audio and investigated whether the disfluencies present in individuals' speech relate to the extent of their cognitive impairment. Our results show that the proposed classification and regression schemes obtain very promising results on both development and test sets. This suggests Alzheimer's Disease can be detected successfully with sequence modeling of the speech data of medical sessions.",0
"The aim of this submission is to participate in the ADReSS challenge, which seeks to create methods that can automatically predict the severity of Alzheimer's Disease by analyzing speech data. Our focus is on identifying cognitive impairment in spontaneous speech, in the context of Alzheimer's Disease Diagnosis and the mini-mental state examination (MMSE) score prediction, using acoustic and natural language features. To achieve this, we propose a model that uses separate LSTMs for the text and audio modalities, with a gating mechanism to combine their outputs for the final prediction. We also investigate the relationship between disfluencies in speech and cognitive impairment. Our results show that our proposed model performs well on both development and test sets, indicating that sequence modeling of medical session speech data can successfully detect Alzheimer's Disease.",1
"World models improve a learning agent's ability to efficiently operate in interactive and situated environments. This work focuses on the task of building world models of text-based game environments. Text-based games, or interactive narratives, are reinforcement learning environments in which agents perceive and interact with the world using textual natural language. These environments contain long, multi-step puzzles or quests woven through a world that is filled with hundreds of characters, locations, and objects. Our world model learns to simultaneously: (1) predict changes in the world caused by an agent's actions when representing the world as a knowledge graph; and (2) generate the set of contextually relevant natural language actions required to operate in the world. We frame this task as a Set of Sequences generation problem by exploiting the inherent structure of knowledge graphs and actions and introduce both a transformer-based multi-task architecture and a loss function to train it. A zero-shot ablation study on never-before-seen textual worlds shows that our methodology significantly outperforms existing textual world modeling techniques as well as the importance of each of our contributions.",0
"The ability of a learning agent to efficiently function in interactive and situated environments can be improved through the use of world models. This study focuses on developing world models for text-based game environments, which are reinforcement learning environments that utilize textual natural language for agents to perceive and interact with the world. These games feature long, multi-step puzzles and quests set in a world populated by numerous characters, locations, and objects. Our world model has a dual focus, predicting changes in the world caused by an agent's actions when represented as a knowledge graph, and generating contextually relevant natural language actions to operate in the world. To achieve this, we consider the task as a Set of Sequences generation problem and leverage the structure of knowledge graphs and actions, utilizing a transformer-based multi-task architecture and a loss function to train it. Our methodology is demonstrated to outperform existing textual world modeling techniques, as shown in a zero-shot ablation study on never-before-seen textual worlds. Additionally, we highlight the importance of each of our contributions.",1
"Predicting chemical properties from the structure of a molecule is of great importance in many applications including drug discovery and material design. Machine learning based molecular property prediction holds the promise of enabling accurate predictions at much less complexity, when compared to, for example Density Functional Theory (DFT) calculations. Features extracted from molecular graphs, using graph neural nets in a supervised manner, have emerged as strong baselines for such tasks. However, the vast chemical space together with the limited availability of labels makes supervised learning challenging, calling for learning a general-purpose molecular representation. Recently, pre-trained transformer-based language models (PTLMs) on large unlabeled corpus have produced state-of-the-art results in many downstream natural language processing tasks. Inspired by this development, here we present molecular embeddings obtained by training an efficient transformer encoder model, referred to as MoLFormer. This model was employed with a linear attention mechanism and highly paralleized training on 1D SMILES sequences of 1.1 billion unlabeled molecules from the PubChem and ZINC datasets. Experiments show that the learned molecular representation performs competitively, when compared to existing graph-based and fingerprint-based supervised learning baselines, on the challenging tasks of predicting properties of QM8 and QM9 molecules. Further task-specific fine-tuning of the MoLFormerr representation improves performance on several of those property prediction benchmarks. These results provide encouraging evidence that large-scale molecular language models can capture sufficient structural information to be able to accurately predict quantum chemical properties and beyond.",0
"The ability to predict chemical properties based on the structure of a molecule is crucial for various fields, including drug discovery and material design. Machine learning-based molecular property prediction offers the potential for more accurate predictions with less complexity compared to Density Functional Theory calculations. One effective approach involves using graph neural nets to extract features from molecular graphs in a supervised manner. However, the lack of labels and vast chemical space make supervised learning challenging, necessitating a general-purpose molecular representation. Recently, pre-trained transformer-based language models (PTLMs) have shown impressive results in natural language processing tasks. In this study, we introduce MoLFormer, an efficient transformer encoder model trained on 1.1 billion unlabeled molecules from the PubChem and ZINC datasets. Our experiments demonstrate that MoLFormer performs competitively with existing supervised learning baselines, such as graph-based and fingerprint-based methods, in predicting properties of QM8 and QM9 molecules. Further fine-tuning of MoLFormer improves performance on various property prediction benchmarks, suggesting that large-scale molecular language models can accurately predict quantum chemical properties and beyond.",1
"The Transformer architecture has become a dominant choice in many domains, such as natural language processing and computer vision. Yet, it has not achieved competitive performance on popular leaderboards of graph-level prediction compared to mainstream GNN variants. Therefore, it remains a mystery how Transformers could perform well for graph representation learning. In this paper, we solve this mystery by presenting Graphormer, which is built upon the standard Transformer architecture, and could attain excellent results on a broad range of graph representation learning tasks, especially on the recent OGB Large-Scale Challenge. Our key insight to utilizing Transformer in the graph is the necessity of effectively encoding the structural information of a graph into the model. To this end, we propose several simple yet effective structural encoding methods to help Graphormer better model graph-structured data. Besides, we mathematically characterize the expressive power of Graphormer and exhibit that with our ways of encoding the structural information of graphs, many popular GNN variants could be covered as the special cases of Graphormer.",0
"Despite its widespread use in natural language processing and computer vision, the Transformer architecture has not performed as well as mainstream GNN variants on leaderboards for graph-level prediction. This has left the question of how Transformers can excel in graph representation learning unanswered. In this paper, we introduce Graphormer, a model that builds on the standard Transformer architecture and achieves outstanding results on various graph representation learning tasks, including the OGB Large-Scale Challenge. Our key insight is that effectively encoding a graph's structural information into the model is essential for utilizing the Transformer in graph representation learning. To this end, we propose several straightforward yet effective structural encoding methods that help Graphormer model graph-structured data better. Furthermore, we mathematically describe the expressive power of Graphormer and show that our structural encoding methods cover many popular GNN variants as special cases.",1
"Pretrained language models have achieved state-of-the-art performance when adapted to a downstream NLP task. However, theoretical analysis of these models is scarce and challenging since the pretraining and downstream tasks can be very different. We propose an analysis framework that links the pretraining and downstream tasks with an underlying latent variable generative model of text -- the downstream classifier must recover a function of the posterior distribution over the latent variables. We analyze head tuning (learning a classifier on top of the frozen pretrained model) and prompt tuning in this setting. The generative model in our analysis is either a Hidden Markov Model (HMM) or an HMM augmented with a latent memory component, motivated by long-term dependencies in natural language. We show that 1) under certain non-degeneracy conditions on the HMM, simple classification heads can solve the downstream task, 2) prompt tuning obtains downstream guarantees with weaker non-degeneracy conditions, and 3) our recovery guarantees for the memory-augmented HMM are stronger than for the vanilla HMM because task-relevant information is easier to recover from the long-term memory. Experiments on synthetically generated data from HMMs back our theoretical findings.",0
"State-of-the-art performance in downstream NLP tasks has been achieved by pretrained language models. However, analyzing these models is difficult since the pretraining and downstream tasks can be vastly different. In order to address this issue, we propose an analysis framework that connects the two tasks through a latent variable generative model of text. The downstream classifier must recover a function of the posterior distribution over the latent variables. Our analysis considers head tuning and prompt tuning in this context, using a Hidden Markov Model (HMM) or an HMM with a latent memory component to model long-term dependencies in natural language. Our findings demonstrate that simple classification heads can solve the downstream task under certain non-degeneracy conditions on the HMM. Additionally, prompt tuning achieves downstream guarantees with weaker non-degeneracy conditions, and the memory-augmented HMM yields stronger recovery guarantees than the vanilla HMM due to the easier recovery of task-relevant information from long-term memory. Our theoretical results are supported by experiments on synthetically generated data from HMMs.",1
"The inductive biases of graph representation learning algorithms are often encoded in the background geometry of their embedding space. In this paper, we show that general directed graphs can be effectively represented by an embedding model that combines three components: a pseudo-Riemannian metric structure, a non-trivial global topology, and a unique likelihood function that explicitly incorporates a preferred direction in embedding space. We demonstrate the representational capabilities of this method by applying it to the task of link prediction on a series of synthetic and real directed graphs from natural language applications and biology. In particular, we show that low-dimensional cylindrical Minkowski and anti-de Sitter spacetimes can produce equal or better graph representations than curved Riemannian manifolds of higher dimensions.",0
"The geometry of the embedding space plays a crucial role in determining the inductive biases of graph representation learning algorithms. This study introduces an embedding model that combines three components to effectively represent general directed graphs: a pseudo-Riemannian metric structure, a non-trivial global topology, and a unique likelihood function that includes a preferred direction in the embedding space. The authors demonstrate the effectiveness of this approach by performing link prediction on synthetic and real directed graphs from natural language applications and biology. The study shows that low-dimensional cylindrical Minkowski and anti-de Sitter spacetimes can provide graph representations that are equal or better than those obtained from curved Riemannian manifolds of higher dimensions.",1
"In this work, we address the task of referring image segmentation (RIS), which aims at predicting a segmentation mask for the object described by a natural language expression. Most existing methods focus on establishing unidirectional or directional relationships between visual and linguistic features to associate two modalities together, while the multi-scale context is ignored or insufficiently modeled. Multi-scale context is crucial to localize and segment those objects that have large scale variations during the multi-modal fusion process. To solve this problem, we propose a simple yet effective Cascaded Multi-modal Fusion (CMF) module, which stacks multiple atrous convolutional layers in parallel and further introduces a cascaded branch to fuse visual and linguistic features. The cascaded branch can progressively integrate multi-scale contextual information and facilitate the alignment of two modalities during the multi-modal fusion process. Experimental results on four benchmark datasets demonstrate that our method outperforms most state-of-the-art methods. Code is available at https://github.com/jianhua2022/CMF-Refseg.",0
"The objective of our study is to tackle the task of referring image segmentation (RIS), wherein a natural language expression is utilized to predict a segmentation mask for the object. Current approaches concentrate on establishing unidirectional or directional associations between visual and linguistic features to combine the two modalities. However, they inadequately model or ignore the multi-scale context that is vital to localize and segment objects with large scale variations. To overcome this issue, we introduce the Cascaded Multi-modal Fusion (CMF) module that utilizes parallel atrous convolutional layers and a cascaded branch to fuse visual and linguistic features. The cascaded branch gradually integrates multi-scale contextual information and facilitates the alignment of both modalities during the multi-modal fusion process. Our method outperforms most state-of-the-art techniques, as shown by experimental results on four benchmark datasets. The code is accessible at https://github.com/jianhua2022/CMF-Refseg.",1
"Variational autoencoders have been widely applied for natural language generation, however, there are two long-standing problems: information under-representation and posterior collapse. The former arises from the fact that only the last hidden state from the encoder is transformed to the latent space, which is insufficient to summarize data. The latter comes as a result of the imbalanced scale between the reconstruction loss and the KL divergence in the objective function. To tackle these issues, in this paper we propose the discrete variational attention model with categorical distribution over the attention mechanism owing to the discrete nature in languages. Our approach is combined with an auto-regressive prior to capture the sequential dependency from observations, which can enhance the latent space for language generation. Moreover, thanks to the property of discreteness, the training of our proposed approach does not suffer from posterior collapse. Furthermore, we carefully analyze the superiority of discrete latent space over the continuous space with the common Gaussian distribution. Extensive experiments on language generation demonstrate superior advantages of our proposed approach in comparison with the state-of-the-art counterparts.",0
"Although variational autoencoders have been widely used for natural language generation, they face two problems: under-representation of information and posterior collapse. The former is due to the fact that only the last hidden state from the encoder is used to transform data into the latent space, which is not sufficient to summarize it. The latter occurs because the reconstruction loss and the KL divergence in the objective function have imbalanced scales. To address these issues, we propose the discrete variational attention model, which uses a categorical distribution over the attention mechanism to account for the discrete nature of language. We combine this approach with an auto-regressive prior to capture the sequential dependency in observations, thus improving the latent space for language generation. Our approach avoids posterior collapse thanks to the property of discreteness. We also compare the benefits of our discrete latent space with the common Gaussian distribution in the continuous space. Finally, extensive experiments show that our proposed approach outperforms state-of-the-art counterparts in language generation.",1
"We introduce a self-supervised vision representation model BEiT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e, image patches (such as 16x16 pixels), and visual tokens (i.e., discrete tokens). We first ""tokenize"" the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEiT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods. For example, base-size BEiT achieves 83.2% top-1 accuracy on ImageNet-1K, significantly outperforming from-scratch DeiT training (81.8%) with the same setup. Moreover, large-size BEiT obtains 86.3% only using ImageNet-1K, even outperforming ViT-L with supervised pre-training on ImageNet-22K (85.2%). The code and pretrained models are available at https://aka.ms/beit.",0
"BEiT, or Bidirectional Encoder representation from Image Transformers, is a self-supervised vision representation model that is similar to BERT in natural language processing. We propose a pretraining approach that involves a masked image modeling task for vision Transformers. Our approach involves two views of each image: image patches and visual tokens. We first tokenize the image into visual tokens and then randomly mask some image patches and feed them into the backbone Transformer. The objective is to recover the original visual tokens based on the corrupted image patches. BEiT is then fine-tuned on downstream tasks by appending task layers upon the pretrained encoder. Our experimental results on image classification and semantic segmentation show competitive results with previous pre-training methods. For instance, our base-size BEiT achieves 83.2% top-1 accuracy on ImageNet-1K, outperforming from-scratch DeiT training (81.8%). Even ViT-L with supervised pre-training on ImageNet-22K (85.2%) is outperformed by large-size BEiT, which obtains 86.3% using only ImageNet-1K. The code and pretrained models are available at https://aka.ms/beit.",1
"Transformer models have demonstrated superior performance in natural language processing. The dot product self-attention in Transformer allows us to model interactions between words. However, this modeling comes with significant computational overhead. In this work, we revisit the memory-compute trade-off associated with Transformer, particularly multi-head attention, and show a memory-heavy but significantly more compute-efficient alternative to Transformer. Our proposal, denoted as PairConnect, a multilayer perceptron (MLP), models the pairwise interaction between words by explicit pairwise word embeddings. As a result, PairConnect substitutes self dot product with a simple embedding lookup. We show mathematically that despite being an MLP, our compute-efficient PairConnect is strictly more expressive than Transformer. Our experiment on language modeling tasks suggests that PairConnect could achieve comparable results with Transformer while reducing the computational cost associated with inference significantly.",0
"The Transformer model has proven to be highly effective in natural language processing due to its ability to model word interactions through dot product self-attention. However, this approach requires significant computational resources. In this study, we explore the trade-off between memory and computation in Transformer, particularly in multi-head attention, and propose an alternative called PairConnect. PairConnect uses explicit pairwise word embeddings modeled by a multilayer perceptron (MLP) to replace self dot product with a simple embedding lookup. Despite being an MLP, PairConnect is more expressive than Transformer according to mathematical analysis. Our language modeling experiments indicate that PairConnect can achieve comparable results to Transformer while significantly reducing computational costs during inference.",1
"Transformers have achieved great success in many artificial intelligence fields, such as natural language processing, computer vision, and audio processing. Therefore, it is natural to attract lots of interest from academic and industry researchers. Up to the present, a great variety of Transformer variants (a.k.a. X-formers) have been proposed, however, a systematic and comprehensive literature review on these Transformer variants is still missing. In this survey, we provide a comprehensive review of various X-formers. We first briefly introduce the vanilla Transformer and then propose a new taxonomy of X-formers. Next, we introduce the various X-formers from three perspectives: architectural modification, pre-training, and applications. Finally, we outline some potential directions for future research.",0
"The success of Transformers in artificial intelligence has garnered significant attention from both academic and industry researchers due to their accomplishments in areas such as natural language processing, computer vision, and audio processing. Despite the numerous proposed Transformer variants, a comprehensive literature review on these X-formers is yet to be conducted. This survey aims to fill this gap by providing a comprehensive review of various X-formers. After briefly introducing the vanilla Transformer, we present a new taxonomy of X-formers and then discuss various X-formers from three perspectives: architectural modification, pre-training, and applications. Additionally, potential areas for future research are outlined.",1
"The spectacular success of deep generative models calls for quantitative tools to measure their statistical performance. Divergence frontiers have recently been proposed as an evaluation framework for generative models, due to their ability to measure the quality-diversity trade-off inherent to deep generative modeling. However, the statistical behavior of divergence frontiers estimated from data remains unknown to this day. In this paper, we establish non-asymptotic bounds on the sample complexity of the plug-in estimator of divergence frontiers. Along the way, we introduce a novel integral summary of divergence frontiers. We derive the corresponding non-asymptotic bounds and discuss the choice of the quantization level by balancing the two types of approximation errors arisen from its computation. We also augment the divergence frontier framework by investigating the statistical performance of smoothed distribution estimators such as the Good-Turing estimator. We illustrate the theoretical results with numerical examples from natural language processing and computer vision.",0
"There is a need for quantitative measures to evaluate the statistical performance of deep generative models, given their remarkable success. Divergence frontiers have emerged as a promising framework for assessing the quality-diversity trade-off inherent in these models. However, the statistical behavior of divergence frontiers estimated from data remains uncertain. This study establishes non-asymptotic bounds on the sample complexity of the plug-in estimator of divergence frontiers, while introducing a new integral summary of divergence frontiers. The authors also explore the impact of quantization level on approximation errors, and extend the framework to consider smoothed distribution estimators like the Good-Turing estimator. Numerical examples from natural language processing and computer vision are used to illustrate the theoretical results.",1
"Despite impressive performance on many text classification tasks, deep neural networks tend to learn frequent superficial patterns that are specific to the training data and do not always generalize well. In this work, we observe this limitation with respect to the task of native language identification. We find that standard text classifiers which perform well on the test set end up learning topical features which are confounds of the prediction task (e.g., if the input text mentions Sweden, the classifier predicts that the author's native language is Swedish). We propose a method that represents the latent topical confounds and a model which ""unlearns"" confounding features by predicting both the label of the input text and the confound; but we train the two predictors adversarially in an alternating fashion to learn a text representation that predicts the correct label but is less prone to using information about the confound. We show that this model generalizes better and learns features that are indicative of the writing style rather than the content.",0
"Although deep neural networks have shown impressive performance in various text classification tasks, they tend to learn frequent superficial patterns that are specific to the training data and may not always generalize well. The researchers note this limitation in the context of identifying a writer's native language. They discovered that standard text classifiers, which perform well on the test set, learn topical features that are confounds of the prediction task. For instance, if the input text mentions Sweden, the classifier predicts that the author's native language is Swedish. To address this issue, they propose a method that captures the latent topical confounds and a model that ""unlearns"" confounding features by predicting both the input text's label and the confound. The two predictors are trained adversarially in an alternating manner to learn a text representation that predicts the correct label while using less information about the confound. The researchers demonstrate that this approach leads to better generalization and the model learns features that reflect the writing style rather than the content.",1
"Despite significant improvements in natural language understanding models with the advent of models like BERT and XLNet, these neural-network based classifiers are vulnerable to blackbox adversarial attacks, where the attacker is only allowed to query the target model outputs. We add two more realistic restrictions on the attack methods, namely limiting the number of queries allowed (query budget) and crafting attacks that easily transfer across different pre-trained models (transferability), which render previous attack models impractical and ineffective. Here, we propose a target model agnostic adversarial attack method with a high degree of attack transferability across the attacked models. Our empirical studies show that in comparison to baseline methods, our method generates highly transferable adversarial sentences under the restriction of limited query budgets.",0
"Despite the significant advancements in natural language understanding models through the introduction of models such as BERT and XLNet, classifiers based on neural networks can still be vulnerable to blackbox adversarial attacks. These attacks involve the attacker being limited to querying the target model outputs. We have introduced two additional realistic restrictions on attack methods, namely, limiting the number of allowed queries (query budget) and crafting attacks that can easily transfer across different pre-trained models (transferability). These restrictions render previous attack models impractical and ineffective. Our proposed method for target model agnostic adversarial attacks has a high degree of attack transferability across various attacked models. Through empirical studies, our method has been shown to generate highly transferable adversarial sentences while adhering to the limited query budget restriction, in comparison to baseline methods.",1
"Recently, chest X-ray report generation, which aims to automatically generate descriptions of given chest X-ray images, has received growing research interests. The key challenge of chest X-ray report generation is to accurately capture and describe the abnormal regions. In most cases, the normal regions dominate the entire chest X-ray image, and the corresponding descriptions of these normal regions dominate the final report. Due to such data bias, learning-based models may fail to attend to abnormal regions. In this work, to effectively capture and describe abnormal regions, we propose the Contrastive Attention (CA) model. Instead of solely focusing on the current input image, the CA model compares the current input image with normal images to distill the contrastive information. The acquired contrastive information can better represent the visual features of abnormal regions. According to the experiments on the public IU-X-ray and MIMIC-CXR datasets, incorporating our CA into several existing models can boost their performance across most metrics. In addition, according to the analysis, the CA model can help existing models better attend to the abnormal regions and provide more accurate descriptions which are crucial for an interpretable diagnosis. Specifically, we achieve the state-of-the-art results on the two public datasets.",0
"There has been a growing interest in generating descriptions of chest X-ray images through automated means. However, accurately capturing and describing abnormal regions presents a challenge due to the dominance of normal regions in the images and their corresponding descriptions. This data bias can cause learning-based models to overlook abnormal regions. To address this issue, we propose the Contrastive Attention (CA) model, which compares the current input image with normal images to distill contrastive information and better represent visual features of abnormal regions. Our experiments on the IU-X-ray and MIMIC-CXR datasets demonstrate that incorporating the CA model into existing models improves their performance across most metrics. Furthermore, the CA model can help models better attend to abnormal regions and provide more accurate descriptions, which are crucial for interpretable diagnosis. Our approach achieves state-of-the-art results on both public datasets.",1
"Knowledge Graph (KG) completion research usually focuses on densely connected benchmark datasets that are not representative of real KGs. We curate two KG datasets that include biomedical and encyclopedic knowledge and use an existing commonsense KG dataset to explore KG completion in the more realistic setting where dense connectivity is not guaranteed. We develop a deep convolutional network that utilizes textual entity representations and demonstrate that our model outperforms recent KG completion methods in this challenging setting. We find that our model's performance improvements stem primarily from its robustness to sparsity. We then distill the knowledge from the convolutional network into a student network that re-ranks promising candidate entities. This re-ranking stage leads to further improvements in performance and demonstrates the effectiveness of entity re-ranking for KG completion.",0
"The focus of KG completion research is often limited to densely connected benchmark datasets that do not accurately reflect real KGs. To address this, we have compiled two KG datasets that encompass biomedical and encyclopedic knowledge and have utilized a commonsense KG dataset to examine KG completion in a more realistic setting where dense connectivity cannot be assumed. Our solution involves a deep convolutional network that employs textual entity representations and surpasses recent KG completion methods in this challenging scenario. Our model's strength lies in its ability to handle sparsity, resulting in a significant performance boost. To further enhance our approach, we have distilled the knowledge from the convolutional network into a student network that reorders promising candidate entities. This re-ranking step leads to even greater performance gains and highlights the effectiveness of entity re-ranking in KG completion.",1
"Sparsely-gated Mixture of Experts networks (MoEs) have demonstrated excellent scalability in Natural Language Processing. In Computer Vision, however, almost all performant networks are ""dense"", that is, every input is processed by every parameter. We present a Vision MoE (V-MoE), a sparse version of the Vision Transformer, that is scalable and competitive with the largest dense networks. When applied to image recognition, V-MoE matches the performance of state-of-the-art networks, while requiring as little as half of the compute at inference time. Further, we propose an extension to the routing algorithm that can prioritize subsets of each input across the entire batch, leading to adaptive per-image compute. This allows V-MoE to trade-off performance and compute smoothly at test-time. Finally, we demonstrate the potential of V-MoE to scale vision models, and train a 15B parameter model that attains 90.35% on ImageNet.",0
"Although sparse-gated Mixture of Experts networks (MoEs) have shown great scalability in Natural Language Processing, dense networks that process every input with every parameter are primarily relied upon in Computer Vision. Our team has developed a Vision MoE (V-MoE), which is a sparse version of the Vision Transformer that competes with the largest dense networks and is highly scalable. In image recognition, V-MoE performs as well as state-of-the-art networks, while using only half of the compute at inference time. Additionally, we suggest an improvement to the routing algorithm that can prioritize subsets of each input throughout the entire batch, allowing for adaptive per-image compute. This enables V-MoE to smoothly trade-off performance and compute at test-time. Lastly, we demonstrate V-MoE's potential to scale vision models and successfully train a 15B parameter model that achieves 90.35% on ImageNet.",1
"Transformer has been widely used for self-supervised pre-training in Natural Language Processing (NLP) and achieved great success. However, it has not been fully explored in visual self-supervised learning. Meanwhile, previous methods only consider the high-level feature and learning representation from a global perspective, which may fail to transfer to the downstream dense prediction tasks focusing on local features. In this paper, we present a novel Masked Self-supervised Transformer approach named MST, which can explicitly capture the local context of an image while preserving the global semantic information. Specifically, inspired by the Masked Language Modeling (MLM) in NLP, we propose a masked token strategy based on the multi-head self-attention map, which dynamically masks some tokens of local patches without damaging the crucial structure for self-supervised learning. More importantly, the masked tokens together with the remaining tokens are further recovered by a global image decoder, which preserves the spatial information of the image and is more friendly to the downstream dense prediction tasks. The experiments on multiple datasets demonstrate the effectiveness and generality of the proposed method. For instance, MST achieves Top-1 accuracy of 76.9% with DeiT-S only using 300-epoch pre-training by linear evaluation, which outperforms supervised methods with the same epoch by 0.4% and its comparable variant DINO by 1.0\%. For dense prediction tasks, MST also achieves 42.7% mAP on MS COCO object detection and 74.04% mIoU on Cityscapes segmentation only with 100-epoch pre-training.",0
"Although Transformer has been successful in self-supervised pre-training for Natural Language Processing (NLP), its potential in visual self-supervised learning has not been fully explored. Prior methods have only focused on high-level features and global perspectives, which may not transfer well to downstream dense prediction tasks that require local features. In this study, we introduce a new approach called Masked Self-supervised Transformer (MST) that captures both local and global information in images. We use a masked token strategy based on multi-head self-attention maps to preserve the crucial structure for self-supervised learning while masking some tokens of local patches. The masked tokens are then recovered by a global image decoder that maintains spatial information and is better suited for downstream dense prediction tasks. Our experiments on multiple datasets demonstrate the effectiveness and generality of MST, achieving high accuracy in both pre-training and dense prediction tasks. For example, MST achieves a Top-1 accuracy of 76.9% with DeiT-S and outperforms supervised methods with the same epoch, as well as its comparable variant DINO. Additionally, MST achieves high mAP on MS COCO object detection and mIoU on Cityscapes segmentation with minimal pre-training.",1
"The global spread of COVID-19, the disease caused by the novel coronavirus SARS-CoV-2, has cast a significant threat to mankind. As the COVID-19 situation continues to evolve, predicting localized disease severity is crucial for advanced resource allocation. This paper proposes a method named COURAGE (COUnty aggRegation mixup AuGmEntation) to generate a short-term prediction of 2-week-ahead COVID-19 related deaths for each county in the United States, leveraging modern deep learning techniques. Specifically, our method adopts a self-attention model from Natural Language Processing, known as the transformer model, to capture both short-term and long-term dependencies within the time series while enjoying computational efficiency. Our model fully utilizes publicly available information of COVID-19 related confirmed cases, deaths, community mobility trends and demographic information, and can produce state-level prediction as an aggregation of the corresponding county-level predictions. Our numerical experiments demonstrate that our model achieves the state-of-the-art performance among the publicly available benchmark models.",0
"The COVID-19 pandemic caused by SARS-CoV-2 has become a major threat globally. In order to manage the situation, accurate predictions of disease severity at the local level are crucial for resource allocation. This study introduces a method called COURAGE (COUnty aggRegation mixup AuGmEntation) which uses advanced deep learning techniques to produce short-term forecasts of COVID-19 related deaths for each county in the United States, up to two weeks in advance. The method employs a transformer model from Natural Language Processing to capture short and long-term dependencies within the time-series data, while also utilizing publicly available information on confirmed cases, deaths, community mobility trends, and demographic data. The model can produce state-level predictions based on county-level data. Numerical experiments show that the model outperforms other publicly available benchmark models.",1
"After their successful debut in natural language processing, Transformer architectures are now becoming the de-facto standard in many domains. An obstacle for their deployment over new modalities is the architectural configuration: the optimal depth-to-width ratio has been shown to dramatically vary across data types (e.g., $10$x larger over images than over language). We theoretically predict the existence of an embedding rank bottleneck that limits the contribution of self-attention width to the Transformer expressivity. We thus directly tie the input vocabulary size and rank to the optimal depth-to-width ratio, since a small vocabulary size or rank dictates an added advantage of depth over width. We empirically demonstrate the existence of this bottleneck and its implications on the depth-to-width interplay of Transformer architectures, linking the architecture variability across domains to the often glossed-over usage of different vocabulary sizes or embedding ranks in different domains. As an additional benefit, our rank bottlenecking framework allows us to identify size redundancies of $25\%-50\%$ in leading NLP models such as ALBERT and T5.",0
"Transformer architectures have become widely adopted in various fields following their successful implementation in natural language processing. However, their deployment in new domains is hindered by the challenge of determining the optimal depth-to-width ratio in different data types, which can vary significantly (e.g., $10$x larger over images than over language). Our research predicts that there is an embedding rank bottleneck that limits the self-attention width contribution to the Transformer's expressivity. Consequently, we link the input vocabulary size and rank to the optimal depth-to-width ratio, where a smaller vocabulary size or rank favors depth over width. We provide empirical evidence of this bottleneck and its impact on the depth-to-width interplay of Transformer architectures, highlighting the usage of different vocabulary sizes or embedding ranks in different domains. Our rank bottlenecking framework additionally identifies size redundancies of $25\%-50\%$ in leading NLP models, such as ALBERT and T5.",1
"Neural language models can be successfully trained on source code, leading to applications such as code completion. However, their versatile autoregressive self-supervision objective overlooks important global sequence-level features that are present in the data such as syntactic correctness or compilability. In this work, we pose the problem of learning to generate compilable code as constraint satisfaction. We define an Energy-Based Model (EBM) representing a pre-trained generative model with an imposed constraint of generating only compilable sequences. We then use the KL-Adaptive Distributional Policy Gradient algorithm (Khalifa et al., 2021) to train a generative model approximating the EBM. We conduct experiments showing that our proposed approach is able to improve compilability rates without sacrificing diversity and complexity of the generated samples.",0
"Applications such as code completion can be achieved by effectively training neural language models on source code. However, these models tend to disregard vital sequence-level features such as syntactic correctness and compilability, despite their versatile autoregressive self-supervision objective. In this study, we address this issue by framing the problem of learning to generate compilable code as a constraint satisfaction problem. We introduce an Energy-Based Model (EBM) that represents a pre-trained generative model with the restriction of producing only compilable sequences. Using the KL-Adaptive Distributional Policy Gradient algorithm (Khalifa et al., 2021), we then train a generative model that approximates the EBM. Our experiments demonstrate that our proposed method effectively enhances compilability rates without compromising the diversity and complexity of the generated samples.",1
"The recent advanced deep learning techniques have shown the promising results in various domains such as computer vision and natural language processing. The success of deep neural networks in supervised learning heavily relies on a large amount of labeled data. However, obtaining labeled data with target labels is often challenging due to various reasons such as cost of labeling and privacy issues, which challenges existing deep models. In spite of that, it is relatively easy to obtain data with \textit{inexact supervision}, i.e., having labels/tags related to the target task. For example, social media platforms are overwhelmed with billions of posts and images with self-customized tags, which are not the exact labels for target classification tasks but are usually related to the target labels. It is promising to leverage these tags (inexact supervision) and their relations with target classes to generate labeled data to facilitate the downstream classification tasks. However, the work on this is rather limited. Therefore, we study a novel problem of labeled data generation with inexact supervision. We propose a novel generative framework named as ADDES which can synthesize high-quality labeled data for target classification tasks by learning from data with inexact supervision and the relations between inexact supervision and target classes. Experimental results on image and text datasets demonstrate the effectiveness of the proposed ADDES for generating realistic labeled data from inexact supervision to facilitate the target classification task.",0
"Advanced deep learning techniques have delivered promising results in various domains, including computer vision and natural language processing. However, obtaining a large amount of labeled data with target labels remains a challenge due to issues such as labeling costs and privacy concerns. Nonetheless, data with inexact supervision, which includes labels related to the target task, is readily available. For instance, social media platforms have billions of posts and images with self-customized tags that are related to the target labels. Although limited research has been conducted in this area, leveraging these tags and their relations with target classes can generate labeled data to facilitate downstream classification tasks. Therefore, we propose a novel generative framework called ADDES, which synthesizes high-quality labeled data for target classification tasks by learning from data with inexact supervision and the relations between inexact supervision and target classes. Our experimental results on image and text datasets demonstrate the effectiveness of ADDES in generating realistic labeled data from inexact supervision to facilitate the target classification task.",1
"Transformers are widely used in natural language processing due to their ability to model longer-term dependencies in text. Although these models achieve state-of-the-art performance for many language related tasks, their applicability outside of the natural language processing field has been minimal. In this work, we propose the use of transformer models for the prediction of dynamical systems representative of physical phenomena. The use of Koopman based embeddings provide a unique and powerful method for projecting any dynamical system into a vector representation which can then be predicted by a transformer model. The proposed model is able to accurately predict various dynamical systems and outperform classical methods that are commonly used in the scientific machine learning literature.",0
"Due to their ability to model longer-term dependencies in text, transformers have become widely utilized in natural language processing. Although they are highly effective for language-related tasks, their usage beyond this field has been limited. Our research suggests the application of transformer models for predicting physical phenomena's dynamical systems. By utilizing Koopman-based embeddings, we can project any dynamical system into a vector representation, which a transformer model can then predict. This proposed model has proven to accurately forecast various dynamical systems and outperform traditional methods commonly used in scientific machine learning literature.",1
"We study the task of conversational fashion image retrieval via multiturn natural language feedback. Most previous studies are based on single-turn settings. Existing models on multiturn conversational fashion image retrieval have limitations, such as employing traditional models, and leading to ineffective performance. We propose a novel framework that can effectively handle conversational fashion image retrieval with multiturn natural language feedback texts. One characteristic of the framework is that it searches for candidate images based on exploitation of the encoded reference image and feedback text information together with the conversation history. Furthermore, the image fashion attribute information is leveraged via a mutual attention strategy. Since there is no existing fashion dataset suitable for the multiturn setting of our task, we derive a large-scale multiturn fashion dataset via additional manual annotation efforts on an existing single-turn dataset. The experiments show that our proposed model significantly outperforms existing state-of-the-art methods.",0
"Our focus is on multiturn natural language feedback for conversational fashion image retrieval, which differs from most studies that have concentrated on single-turn settings. Prior research on multiturn conversational fashion image retrieval has limitations, including the use of traditional models and poor performance. Our innovative framework tackles this issue by effectively managing conversational fashion image retrieval with multiturn natural language feedback texts. A unique feature of the framework is its ability to locate candidate images by exploiting the encoded reference image, feedback text information, and conversation history. Additionally, the framework utilizes image fashion attribute information through a mutual attention strategy. Since no fashion dataset is appropriate for our multiturn setting, we created a large-scale multiturn fashion dataset by manually annotating an existing single-turn dataset. Our experiments demonstrate that our proposed model outperforms existing state-of-the-art methods.",1
"Although deep learning models have driven state-of-the-art performance on a wide array of tasks, they are prone to learning spurious correlations that should not be learned as predictive clues. To mitigate this problem, we propose a causality-based training framework to reduce the spurious correlations caused by observable confounders. We give theoretical analysis on the underlying general Structural Causal Model (SCM) and propose to perform Maximum Likelihood Estimation (MLE) on the interventional distribution instead of the observational distribution, namely Counterfactual Maximum Likelihood Estimation (CMLE). As the interventional distribution, in general, is hidden from the observational data, we then derive two different upper bounds of the expected negative log-likelihood and propose two general algorithms, Implicit CMLE and Explicit CMLE, for causal predictions of deep learning models using observational data. We conduct experiments on two real-world tasks: Natural Language Inference (NLI) and Image Captioning. The results show that CMLE methods outperform the regular MLE method in terms of out-of-domain generalization performance and reducing spurious correlations, while maintaining comparable performance on the regular evaluations.",0
"Despite their success in achieving state-of-the-art performance in various tasks, deep learning models often learn irrelevant correlations that do not serve as predictive clues. To address this issue, we suggest a causality-based training approach that decreases the presence of spurious correlations resulting from observable confounders. We provide a theoretical analysis of the general Structural Causal Model (SCM) and propose Counterfactual Maximum Likelihood Estimation (CMLE) as an alternative to Maximum Likelihood Estimation (MLE) on the interventional distribution instead of the observational distribution. However, the interventional distribution is typically concealed in the observational data. Consequently, we develop two upper bounds for the anticipated negative log-likelihood and introduce Implicit CMLE and Explicit CMLE as two general algorithms for causal predictions with deep learning models using observational data. We also test our method on two real-world tasks: Natural Language Inference (NLI) and Image Captioning. The results demonstrate that CMLE techniques are superior to conventional MLE methods in terms of reducing spurious correlations and achieving out-of-domain generalization performance, while maintaining comparable performance on standard evaluations.",1
"Generative Adversarial Networks (GAN) have promoted a variety of applications in computer vision, natural language processing, etc. due to its generative model's compelling ability to generate realistic examples plausibly drawn from an existing distribution of samples. GAN not only provides impressive performance on data generation-based tasks but also stimulates fertilization for privacy and security oriented research because of its game theoretic optimization strategy. Unfortunately, there are no comprehensive surveys on GAN in privacy and security, which motivates this survey paper to summarize those state-of-the-art works systematically. The existing works are classified into proper categories based on privacy and security functions, and this survey paper conducts a comprehensive analysis of their advantages and drawbacks. Considering that GAN in privacy and security is still at a very initial stage and has imposed unique challenges that are yet to be well addressed, this paper also sheds light on some potential privacy and security applications with GAN and elaborates on some future research directions.",0
"The use of Generative Adversarial Networks (GAN) has led to a variety of applications in computer vision and natural language processing. This is due to its generative model's ability to create realistic examples from an existing distribution of samples. GAN has not only shown impressive performance on data generation tasks, but has also sparked interest in privacy and security research due to its game theoretic optimization strategy. However, there is a lack of comprehensive surveys on GAN in privacy and security. This survey paper aims to fill this gap by systematically summarizing the state-of-the-art works in this area and analyzing their advantages and drawbacks. The existing works are categorized based on their privacy and security functions. In addition, this paper explores potential privacy and security applications with GAN and identifies future research directions. Despite the unique challenges posed by GAN in privacy and security, this paper suggests that it has great potential in these fields.",1
"The irregular domain and lack of ordering make it challenging to design deep neural networks for point cloud processing. This paper presents a novel framework named Point Cloud Transformer(PCT) for point cloud learning. PCT is based on Transformer, which achieves huge success in natural language processing and displays great potential in image processing. It is inherently permutation invariant for processing a sequence of points, making it well-suited for point cloud learning. To better capture local context within the point cloud, we enhance input embedding with the support of farthest point sampling and nearest neighbor search. Extensive experiments demonstrate that the PCT achieves the state-of-the-art performance on shape classification, part segmentation and normal estimation tasks.",0
"Designing deep neural networks for point cloud processing is challenging due to the irregular domain and lack of ordering. However, a new framework called Point Cloud Transformer (PCT) has been introduced in this paper for point cloud learning. PCT is based on the successful Transformer, which has been widely used in natural language processing and image processing. It is particularly suitable for point cloud learning as it is inherently permutation invariant for processing a sequence of points. To capture local context within the point cloud more effectively, input embedding has been enhanced with the support of farthest point sampling and nearest neighbor search. Through extensive experiments, it has been shown that PCT achieves state-of-the-art performance on shape classification, part segmentation, and normal estimation tasks.",1
"Transformer-based deep learning models have increasingly demonstrated high accuracy on many natural language processing (NLP) tasks. In this paper, we propose a compression-compilation co-design framework that can guarantee the identified model to meet both resource and real-time specifications of mobile devices. Our framework applies a compiler-aware neural architecture optimization method (CANAO), which can generate the optimal compressed model that balances both accuracy and latency. We are able to achieve up to 7.8x speedup compared with TensorFlow-Lite with only minor accuracy loss. We present two types of BERT applications on mobile devices: Question Answering (QA) and Text Generation. Both can be executed in real-time with latency as low as 45ms. Videos for demonstrating the framework can be found on https://www.youtube.com/watch?v=_WIRvK_2PZI",0
"The accuracy of Transformer-based deep learning models in natural language processing (NLP) tasks has been increasingly demonstrated. This paper introduces a co-design framework for compression-compilation that ensures the model satisfies the resource and real-time requirements of mobile devices. Our framework employs a compiler-aware neural architecture optimization method (CANAO) that generates the ideal compressed model, balancing accuracy and latency. Our results show up to 7.8x speedup compared to TensorFlow-Lite with slight accuracy loss. We demonstrate two BERT applications, Question Answering (QA) and Text Generation, that can run in real-time with latency as low as 45ms. To see the framework in action, check out our videos at https://www.youtube.com/watch?v=_WIRvK_2PZI.",1
"Can we teach a robot to recognize and make predictions for activities that it has never seen before? We tackle this problem by learning models for video from text. This paper presents a hierarchical model that generalizes instructional knowledge from large-scale text-corpora and transfers the knowledge to video. Given a portion of an instructional video, our model recognizes and predicts coherent and plausible actions multiple steps into the future, all in rich natural language. To demonstrate the capabilities of our model, we introduce the \emph{Tasty Videos Dataset V2}, a collection of 4022 recipes for zero-shot learning, recognition and anticipation. Extensive experiments with various evaluation metrics demonstrate the potential of our method for generalization, given limited video data for training models.",0
"Is it possible to train a robot to identify and forecast actions it has not encountered previously? Our approach to this challenge involves developing video models based on textual information. This article describes a hierarchical model that extrapolates teaching knowledge from extensive text data and applies it to video. With our model, a portion of an instructional video can be analyzed, and the robot can anticipate and predict coherent and plausible actions several steps ahead, all in natural language. To showcase our model's capabilities, we present the \emph{Tasty Videos Dataset V2}, which contains 4022 recipes suitable for zero-shot learning, recognition, and prediction. Our method's potential for generalization, even with limited video data available for training models, is supported by extensive experiments using various evaluation metrics.",1
"There is a growing interest in the community in making an embodied AI agent perform a complicated task while interacting with an environment following natural language directives. Recent studies have tackled the problem using ALFRED, a well-designed dataset for the task, but achieved only very low accuracy. This paper proposes a new method, which outperforms the previous methods by a large margin. It is based on a combination of several new ideas. One is a two-stage interpretation of the provided instructions. The method first selects and interprets an instruction without using visual information, yielding a tentative action sequence prediction. It then integrates the prediction with the visual information etc., yielding the final prediction of an action and an object. As the object's class to interact is identified in the first stage, it can accurately select the correct object from the input image. Moreover, our method considers multiple egocentric views of the environment and extracts essential information by applying hierarchical attention conditioned on the current instruction. This contributes to the accurate prediction of actions for navigation. A preliminary version of the method won the ALFRED Challenge 2020. The current version achieves the unseen environment's success rate of 4.45% with a single view, which is further improved to 8.37% with multiple views.",0
"There is an increased interest among the community in having an embodied AI agent perform complex tasks while following natural language commands in an interactive environment. Previous studies have utilized the ALFRED dataset to approach this problem, but they have only achieved low accuracy. This paper proposes a new and more successful approach, which incorporates multiple innovative ideas. One of these ideas involves a two-stage interpretation of instructions, where the method first selects and interprets the instruction without using visual information to predict a tentative action sequence and then integrates this prediction with visual information to generate a final prediction of an action and an object. This method accurately identifies the object to interact with by detecting its class in the first stage and selecting the correct object from the input image. Furthermore, the method takes into account multiple egocentric views of the environment and extracts crucial information through hierarchical attention that is conditioned on the current instruction. This leads to accurate prediction of actions for navigation. The preliminary version of the method won the ALFRED Challenge 2020, and the current version achieves a success rate of 4.45% in an unseen environment with a single view, which further improves to 8.37% with multiple views.",1
"Machine learning is vulnerable to a wide variety of attacks. It is now well understood that by changing the underlying data distribution, an adversary can poison the model trained with it or introduce backdoors. In this paper we present a novel class of training-time attacks that require no changes to the underlying dataset or model architecture, but instead only change the order in which data are supplied to the model. In particular, we find that the attacker can either prevent the model from learning, or poison it to learn behaviours specified by the attacker. Furthermore, we find that even a single adversarially-ordered epoch can be enough to slow down model learning, or even to reset all of the learning progress. Indeed, the attacks presented here are not specific to the model or dataset, but rather target the stochastic nature of modern learning procedures. We extensively evaluate our attacks on computer vision and natural language benchmarks to find that the adversary can disrupt model training and even introduce backdoors.",0
"A variety of attacks can be launched against machine learning, including the poisoning of trained models or the introduction of backdoors by changing the data distribution. Our paper introduces a new type of training-time attack that does not require any changes to the dataset or model architecture, but instead alters the order in which data is presented to the model. This approach can prevent the model from learning or manipulate it to follow the attacker's desired behavior. We have found that even a single adversarial epoch can impede or reset learning progress. These attacks target the stochastic nature of modern learning procedures and can disrupt model training and introduce backdoors across different datasets and models, as demonstrated through our evaluation on computer vision and natural language benchmarks.",1
"Text-based video segmentation is a challenging task that segments out the natural language referred objects in videos. It essentially requires semantic comprehension and fine-grained video understanding. Existing methods introduce language representation into segmentation models in a bottom-up manner, which merely conducts vision-language interaction within local receptive fields of ConvNets. We argue that such interaction is not fulfilled since the model can barely construct region-level relationships given partial observations, which is contrary to the description logic of natural language/referring expressions. In fact, people usually describe a target object using relations with other objects, which may not be easily understood without seeing the whole video. To address the issue, we introduce a novel top-down approach by imitating how we human segment an object with the language guidance. We first figure out all candidate objects in videos and then choose the refereed one by parsing relations among those high-level objects. Three kinds of object-level relations are investigated for precise relationship understanding, i.e., positional relation, text-guided semantic relation, and temporal relation. Extensive experiments on A2D Sentences and J-HMDB Sentences show our method outperforms state-of-the-art methods by a large margin. Qualitative results also show our results are more explainable. Besides, based on the inspiration, we win the first place in CVPR2021 Referring Youtube-VOS challenge.",0
"Segmenting natural language referred objects in videos is a complex task that necessitates semantic comprehension and fine-grained video understanding. Existing methods have integrated language representation into segmentation models in a bottom-up manner, which limits vision-language interaction to local receptive fields of ConvNets. This approach is inadequate since the model struggles to establish region-level relationships based on partial observations, which is inconsistent with the description logic of natural language. Typically, people describe a target object using relations with other objects, which may require viewing the entire video for better comprehension. To tackle this challenge, we propose a top-down approach that emulates how humans segment objects with language guidance. We first identify all potential objects in videos and then select the referred one by analyzing relations among those high-level objects. We investigate three types of object-level relations for precise relationship understanding, including positional relation, text-guided semantic relation, and temporal relation. Our extensive experiments on A2D Sentences and J-HMDB Sentences demonstrate that our method significantly outperforms existing approaches, and our results are more interpretable. Moreover, based on this approach, we achieved the first place in the CVPR2021 Referring Youtube-VOS challenge.",1
"Vision transformer (ViT) has recently showed its strong capability in achieving comparable results to convolutional neural networks (CNNs) on image classification. However, vanilla ViT simply inherits the same architecture from the natural language processing directly, which is often not optimized for vision applications. Motivated by this, in this paper, we propose a new architecture that adopts the pyramid structure and employ a novel regional-to-local attention rather than global self-attention in vision transformers. More specifically, our model first generates regional tokens and local tokens from an image with different patch sizes, where each regional token is associated with a set of local tokens based on the spatial location. The regional-to-local attention includes two steps: first, the regional self-attention extract global information among all regional tokens and then the local self-attention exchanges the information among one regional token and the associated local tokens via self-attention. Therefore, even though local self-attention confines the scope in a local region but it can still receive global information. Extensive experiments on three vision tasks, including image classification, object detection and action recognition, show that our approach outperforms or is on par with state-of-the-art ViT variants including many concurrent works. Our source codes and models will be publicly available.",0
"The recent success of Vision transformer (ViT) in image classification has demonstrated its comparable performance to convolutional neural networks (CNNs). However, the vanilla ViT architecture is not optimized for vision tasks as it is directly inherited from natural language processing. To address this issue, we propose a novel architecture in this paper that incorporates a pyramid structure and a regional-to-local attention mechanism in ViT. Our model generates regional and local tokens from an image with different patch sizes, where each regional token is associated with a set of local tokens based on spatial location. The regional-to-local attention comprises two steps: first, the regional self-attention extracts global information among all regional tokens, and then the local self-attention exchanges information among one regional token and its associated local tokens via self-attention. Our approach achieves superior results compared to state-of-the-art ViT variants in image classification, object detection, and action recognition tasks. Our source codes and models are publicly available for future research.",1
"The self-supervised learning (SSL) paradigm is an essential exploration area, which tries to eliminate the need for expensive data labeling. Despite the great success of SSL methods in computer vision and natural language processing, most of them employ contrastive learning objectives that require negative samples, which are hard to define. This becomes even more challenging in the case of graphs and is a bottleneck for achieving robust representations. To overcome such limitations, we propose a framework for self-supervised graph representation learning -- Graph Barlow Twins, which utilizes a cross-correlation-based loss function instead of negative samples. Moreover, it does not rely on non-symmetric neural network architectures -- in contrast to state-of-the-art self-supervised graph representation learning method BGRL. We show that our method achieves as competitive results as BGRL, best self-supervised methods, and fully supervised ones while requiring substantially fewer hyperparameters and converging in an order of magnitude training steps earlier.",0
"The exploration of self-supervised learning (SSL) is crucial to eliminate the high cost of data labeling. Although SSL methods have been successful in computer vision and natural language processing, their use of contrastive learning objectives that require negative samples is difficult to define. This presents an even greater challenge for graphs, hindering the creation of reliable representations. To overcome this issue, we present Graph Barlow Twins, a self-supervised graph representation learning framework that utilizes a cross-correlation-based loss function instead of negative samples. Our method does not require non-symmetric neural network architectures, which is different from the current state-of-the-art self-supervised graph representation learning method BGRL. Our results show that our method has similar performance to BGRL, the best self-supervised and fully supervised methods, while requiring fewer hyperparameters and converging much earlier.",1
"Real-world machine learning systems are achieving remarkable performance in terms of coarse-grained metrics like overall accuracy and F-1 score. However, model improvement and development often require fine-grained modeling on individual data subsets or slices, for instance, the data slices where the models have unsatisfactory results. In practice, it gives tangible values for developing such models that can pay extra attention to critical or interested slices while retaining the original overall performance. This work extends the recent slice-based learning (SBL)~\cite{chen2019slice} with a mixture of attentions (MoA) to learn slice-aware dual attentive representations. We empirically show that the MoA approach outperforms the baseline method as well as the original SBL approach on monitored slices with two natural language understanding (NLU) tasks.",0
"Although real-world machine learning systems have achieved impressive results in terms of overall accuracy and F-1 score, improving and developing models often requires a more detailed approach to individual data subsets or slices. Specifically, models may have unsatisfactory results in certain data slices, making it necessary to focus on these critical or interesting slices while maintaining the original overall performance. This study builds upon the recent slice-based learning (SBL) approach by introducing a mixture of attentions (MoA) to learn slice-aware dual attentive representations. Our empirical results demonstrate that the MoA method surpasses both the baseline approach and the original SBL approach when applied to two natural language understanding (NLU) tasks.",1
"When a human asks questions online, or when a conversational virtual agent asks human questions, questions triggering emotions or with details might more likely to get responses or answers. we explore how to automatically rewrite natural language questions to improve the response rate from people. In particular, a new task of Visual Question Rewriting(VQR) task is introduced to explore how visual information can be used to improve the new questions. A data set containing around 4K bland questions, attractive questions and images triples is collected. We developed some baseline sequence to sequence models and more advanced transformer based models, which take a bland question and a related image as input and output a rewritten question that is expected to be more attractive. Offline experiments and mechanical Turk based evaluations show that it is possible to rewrite bland questions in a more detailed and attractive way to increase the response rate, and images can be helpful.",0
"The likelihood of receiving responses or answers increases when emotionally triggering or detailed questions are asked online by humans or virtual agents. Our study delves into the automatic rewriting of natural language questions to enhance response rates from people. We introduce a novel task, the Visual Question Rewriting (VQR) task, to explore the role of visual information in improving the quality of questions. Our dataset comprises approximately 4K bland questions, attractive questions, and image triples. We have developed baseline sequence-to-sequence models and advanced transformer-based models, which take bland questions and related images as input and output rewritten questions that are more appealing. Offline experiments and evaluations conducted using Mechanical Turk demonstrate that it is possible to rephrase bland questions to make them more detailed and captivating and to increase the response rate. Additionally, images can be useful in this process.",1
"We consider the problem of Visual Question Answering (VQA). Given an image and a free-form, open-ended, question, expressed in natural language, the goal of VQA system is to provide accurate answer to this question with respect to the image. The task is challenging because it requires simultaneous and intricate understanding of both visual and textual information. Attention, which captures intra- and inter-modal dependencies, has emerged as perhaps the most widely used mechanism for addressing these challenges. In this paper, we propose an improved attention-based architecture to solve VQA. We incorporate an Attention on Attention (AoA) module within encoder-decoder framework, which is able to determine the relation between attention results and queries. Attention module generates weighted average for each query. On the other hand, AoA module first generates an information vector and an attention gate using attention results and current context; and then adds another attention to generate final attended information by multiplying the two. We also propose multimodal fusion module to combine both visual and textual information. The goal of this fusion module is to dynamically decide how much information should be considered from each modality. Extensive experiments on VQA-v2 benchmark dataset show that our method achieves the state-of-the-art performance.",0
"The Visual Question Answering (VQA) problem involves providing accurate answers to open-ended questions about an image, expressed in natural language. This poses a challenge as it requires simultaneous understanding of both visual and textual information. Attention has become a widely used mechanism to address these challenges, capturing intra- and inter-modal dependencies. In this study, we propose an improved attention-based architecture for solving VQA. We introduce an Attention on Attention (AoA) module within an encoder-decoder framework, which determines the relation between attention results and queries. The AoA module generates an information vector and an attention gate using attention results and current context, and then adds another attention to generate final attended information by multiplying the two. We also propose a multimodal fusion module to combine visual and textual information, dynamically deciding how much information to consider from each modality. Our experiments on the VQA-v2 benchmark dataset show that our method achieves state-of-the-art performance.",1
"While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",0
"Despite the Transformer architecture being widely used for natural language processing, its application to computer vision has been limited. Typically, attention is used in conjunction with convolutional networks or to replace specific components while maintaining the network's overall structure. However, our research reveals that using CNNs is not necessary, and a pure transformer applied directly to image patch sequences can achieve excellent results in image classification tasks. By pre-training on large datasets and transferring to mid-sized or small image recognition benchmarks, such as ImageNet, CIFAR-100, and VTAB, our Vision Transformer (ViT) outperforms state-of-the-art convolutional networks while requiring significantly less computational resources to train.",1
"With the advent of state of the art nature-inspired pure attention based models i.e. transformers, and their success in natural language processing (NLP), their extension to machine vision (MV) tasks was inevitable and much felt. Subsequently, vision transformers (ViTs) were introduced which are giving quite a challenge to the established deep learning based machine vision techniques. However, pure attention based models/architectures like transformers require huge data, large training times and large computational resources. Some recent works suggest that combinations of these two varied fields can prove to build systems which have the advantages of both these fields. Accordingly, this state of the art survey paper is introduced which hopefully will help readers get useful information about this interesting and potential research area. A gentle introduction to attention mechanisms is given, followed by a discussion of the popular attention based deep architectures. Subsequently, the major categories of the intersection of attention mechanisms and deep learning for machine vision (MV) based are discussed. Afterwards, the major algorithms, issues and trends within the scope of the paper are discussed.",0
"With the emergence of advanced nature-inspired attention-based models, such as transformers, and their successful application in natural language processing (NLP), it was only a matter of time before they were extended to machine vision (MV) tasks. This resulted in the development of Vision Transformers (ViTs), which pose a significant challenge to established deep learning techniques in MV. However, attention-based models like transformers require vast amounts of data, long training times, and extensive computational resources. Recent works propose that the combination of these two fields could yield systems with the advantages of both. Therefore, this survey paper aims to provide readers with useful information on this exciting and promising research area. It begins with a gentle introduction to attention mechanisms, followed by a discussion of popular attention-based deep architectures. The paper then explores the intersection of attention mechanisms and deep learning for MV tasks, including major categories, algorithms, issues, and trends.",1
"Training a reinforcement learning agent to carry out natural language instructions is limited by the available supervision, i.e. knowing when the instruction has been carried out. We adapt the CLEVR visual question answering dataset to generate complex natural language navigation instructions and accompanying scene graphs, yielding an environment-agnostic supervised dataset. To demonstrate the use of this data set, we map the scenes to the VizDoom environment and use the architecture in \citet{gatedattention} to train an agent to carry out these more complex language instructions.",0
"The ability to train a reinforcement learning agent to follow natural language instructions is constrained by the availability of supervision in determining when the instruction has been executed. To overcome this limitation, we modify the CLEVR visual question answering dataset to generate intricate natural language navigation instructions and corresponding scene graphs, creating a supervised dataset that is not dependent on any particular environment. Our demonstration of this dataset involves mapping the scenes to the VizDoom environment and utilizing the architecture presented in the paper by \citet{gatedattention} to train an agent to execute these more intricate language instructions.",1
"Despite their ubiquity in core AI fields like natural language processing, the mechanics of deep attention-based neural networks like the Transformer model are not fully understood. In this article, we present a new perspective towards understanding how Transformers work. In particular, we show that the ""dot-product attention"" that is the core of the Transformer's operation can be characterized as a kernel learning method on a pair of Banach spaces. In particular, the Transformer's kernel is characterized as having an infinite feature dimension. Along the way we consider an extension of the standard kernel learning problem to a binary setting, where data come from two input domains and a response is defined for every cross-domain pair. We prove a new representer theorem for these binary kernel machines with non-Mercer (indefinite, asymmetric) kernels (implying that the functions learned are elements of reproducing kernel Banach spaces rather than Hilbert spaces), and also prove a new universal approximation theorem showing that the Transformer calculation can learn any binary non-Mercer reproducing kernel Banach space pair. We experiment with new kernels in Transformers, and obtain results that suggest the infinite dimensionality of the standard Transformer kernel is partially responsible for its performance. This paper's results provide a new theoretical understanding of a very important but poorly understood model in modern machine~learning.",0
"The mechanics of deep attention-based neural networks, such as the Transformer model, which are fundamental to AI fields like natural language processing, are not yet fully comprehended. This article offers a fresh perspective on understanding how Transformers operate. The article explains how the Transformer's ""dot-product attention,"" which is the core of its operation, can be regarded as a kernel learning method on a pair of Banach spaces. The Transformer's kernel is characterized as having an infinite feature dimension. Additionally, the article explores an extension of the standard kernel learning problem to a binary setting and proves a new representer theorem for binary kernel machines with non-Mercer kernels. The article also demonstrates a new universal approximation theorem, which indicates that the Transformer calculation can learn any binary non-Mercer reproducing kernel Banach space pair. The article experiments with new kernels in Transformers, and the results suggest that the infinite dimensionality of the standard Transformer kernel is partly responsible for its performance. The findings in this article provide a new theoretical understanding of an essential, yet poorly understood model in modern machine learning.",1
"Self-supervised or weakly supervised models trained on large-scale datasets have shown sample-efficient transfer to diverse datasets in few-shot settings. We consider how upstream pretrained models can be leveraged for downstream few-shot, multilabel, and continual learning tasks. Our model CLIPPER (CLIP PERsonalized) uses image representations from CLIP, a large-scale image representation learning model trained using weak natural language supervision. We developed a technique, called Multi-label Weight Imprinting (MWI), for multi-label, continual, and few-shot learning, and CLIPPER uses MWI with image representations from CLIP. We evaluated CLIPPER on 10 single-label and 5 multi-label datasets. Our model shows robust and competitive performance, and we set new benchmarks for few-shot, multi-label, and continual learning. Our lightweight technique is also compute-efficient and enables privacy-preserving applications as the data is not sent to the upstream model for fine-tuning.",0
"Large-scale datasets have enabled self-supervised or weakly supervised models to achieve sample-efficient transfer to diverse datasets in few-shot settings. Our focus is on utilizing upstream pretrained models for downstream few-shot, multilabel, and continual learning tasks. To accomplish this, we introduce CLIPPER (CLIP PERsonalized), which employs image representations from CLIP, a large-scale image representation learning model that was trained using weak natural language supervision. Our approach, Multi-label Weight Imprinting (MWI), is specifically designed for multi-label, continual, and few-shot learning. CLIPPER uses MWI in combination with image representations from CLIP. We conducted evaluations on 10 single-label and 5 multi-label datasets, which demonstrated the robust and competitive performance of our model. Additionally, we achieved new benchmarks for few-shot, multi-label, and continual learning. Finally, our technique is lightweight and compute-efficient, which makes it ideal for privacy-preserving applications as data is not sent to the upstream model for fine-tuning.",1
"Convolutional neural networks (CNNs) are ubiquitous in computer vision, with a myriad of effective and efficient variations. Recently, Transformers -- originally introduced in natural language processing -- have been increasingly adopted in computer vision. While early adopters continue to employ CNN backbones, the latest networks are end-to-end CNN-free Transformer solutions. A recent surprising finding shows that a simple MLP based solution without any traditional convolutional or Transformer components can produce effective visual representations. While CNNs, Transformers and MLP-Mixers may be considered as completely disparate architectures, we provide a unified view showing that they are in fact special cases of a more general method to aggregate spatial context in a neural network stack. We present the \model (CONText AggregatIon NEtwoRk), a general-purpose building block for multi-head context aggregation that can exploit long-range interactions \emph{a la} Transformers while still exploiting the inductive bias of the local convolution operation leading to faster convergence speeds, often seen in CNNs. In contrast to Transformer-based methods that do not scale well to downstream tasks that rely on larger input image resolutions, our efficient network, named \modellight, can be employed in object detection and instance segmentation networks such as DETR, RetinaNet and Mask-RCNN to obtain an impressive detection mAP of 38.9, 43.8, 45.1 and mask mAP of 41.3, providing large improvements of 6.6, 7.3, 6.9 and 6.6 pts respectively, compared to a ResNet-50 backbone with a comparable compute and parameter size. Our method also achieves promising results on self-supervised learning compared to DeiT on the DINO framework.",0
"In computer vision, Convolutional Neural Networks (CNNs) are widely used and have many effective and efficient variations. However, Transformers, originally introduced in natural language processing, have recently gained popularity in computer vision. While some still use CNN backbones, newer networks are now end-to-end Transformer solutions. Surprisingly, a simple MLP-based solution without traditional convolutional or Transformer components can produce effective visual representations. Despite the apparent architectural differences, we show that CNNs, Transformers, and MLP-Mixers are all special cases of a more general method to aggregate spatial context in neural networks. We present the CONText AggregatIon NEtwoRk (CONTAIR), a general-purpose building block for multi-head context aggregation that can exploit long-range interactions like Transformers while still utilizing the local convolution operation's inductive bias, leading to faster convergence speeds like CNNs. Our efficient network, called CONTAIRlight, outperforms a ResNet-50 backbone in object detection and instance segmentation tasks such as DETR, RetinaNet, and Mask-RCNN by achieving impressive detection and mask mAPs. Additionally, our method shows promise in self-supervised learning compared to DeiT on the DINO framework.",1
"In computer vision and natural language processing, innovations in model architecture that lead to increases in model capacity have reliably translated into gains in performance. In stark contrast with this trend, state-of-the-art reinforcement learning (RL) algorithms often use only small MLPs, and gains in performance typically originate from algorithmic innovations. It is natural to hypothesize that small datasets in RL necessitate simple models to avoid overfitting; however, this hypothesis is untested. In this paper we investigate how RL agents are affected by exchanging the small MLPs with larger modern networks with skip connections and normalization, focusing specifically on soft actor-critic (SAC) algorithms. We verify, empirically, that na\""ively adopting such architectures leads to instabilities and poor performance, likely contributing to the popularity of simple models in practice. However, we show that dataset size is not the limiting factor, and instead argue that intrinsic instability from the actor in SAC taking gradients through the critic is the culprit. We demonstrate that a simple smoothing method can mitigate this issue, which enables stable training with large modern architectures. After smoothing, larger models yield dramatic performance improvements for state-of-the-art agents -- suggesting that more ""easy"" gains may be had by focusing on model architectures in addition to algorithmic innovations.",0
"The use of advanced model architecture has consistently resulted in improved performance in computer vision and natural language processing. In contrast, state-of-the-art reinforcement learning (RL) algorithms typically utilize small MLPs, with performance gains stemming from algorithmic innovations rather than larger models. While it is commonly assumed that small datasets in RL require simple models to avoid overfitting, this hypothesis remains untested. This study examines the impact of replacing small MLPs with larger modern networks with skip connections and normalization on soft actor-critic (SAC) algorithms. Results indicate that the adoption of such architectures without modification leads to instabilities and poor performance, likely contributing to the continued use of simple models. However, the study finds that dataset size is not the limiting factor, and suggests that intrinsic instability from the actor in SAC taking gradients through the critic is the cause. The study proposes a simple smoothing method to mitigate this issue, enabling stable training with large modern architectures. After smoothing, larger models yield significant performance improvements for state-of-the-art agents, indicating that model architecture may be an additional area for improvement alongside algorithmic innovations.",1
"Referring video object segmentation (RVOS) aims to segment video objects with the guidance of natural language reference. Previous methods typically tackle RVOS through directly grounding linguistic reference over the image lattice. Such bottom-up strategy fails to explore object-level cues, easily leading to inferior results. In this work, we instead put forward a two-stage, top-down RVOS solution. First, an exhaustive set of object tracklets is constructed by propagating object masks detected from several sampled frames to the entire video. Second, a Transformer-based tracklet-language grounding module is proposed, which models instance-level visual relations and cross-modal interactions simultaneously and efficiently. Our model ranks first place on CVPR2021 Referring Youtube-VOS challenge.",0
"The main objective of referring video object segmentation (RVOS) is to segment video objects with the aid of natural language references. Previous methods have taken a bottom-up approach to RVOS by directly utilizing linguistic references on the image lattice. However, this approach fails to consider object-level cues, resulting in subpar outcomes. In this study, we propose a two-stage, top-down solution for RVOS. Initially, we generate a comprehensive set of object tracklets by extending object masks detected from multiple sampled frames throughout the entire video. Subsequently, we introduce a Transformer-based tracklet-language grounding module to model instance-level visual relationships and cross-modal interactions simultaneously and efficiently. Our model achieved first place in the CVPR2021 Referring Youtube-VOS challenge.",1
"Pre-trained Transformer-based models have achieved state-of-the-art performance for various Natural Language Processing (NLP) tasks. However, these models often have billions of parameters, and, thus, are too resource-hungry and computation-intensive to suit low-capability devices or applications with strict latency requirements. One potential remedy for this is model compression, which has attracted a lot of research attention. Here, we summarize the research in compressing Transformers, focusing on the especially popular BERT model. In particular, we survey the state of the art in compression for BERT, we clarify the current best practices for compressing large-scale Transformer models, and we provide insights into the workings of various methods. Our categorization and analysis also shed light on promising future research directions for achieving lightweight, accurate, and generic NLP models.",0
"Various Natural Language Processing (NLP) tasks have been dominated by state-of-the-art performance of pre-trained Transformer-based models. However, these models have billions of parameters, making them too demanding for low-capability devices or applications with strict latency requirements. To address this, model compression has received considerable research attention. In this article, we summarize research on compressing Transformers, with a focus on the widely used BERT model. Our survey covers the current best practices for compressing large-scale Transformer models, and we provide insights into the workings of different methods. We also categorize and analyze the state of the art in BERT compression, highlighting future research directions for creating lightweight, accurate, and generic NLP models.",1
"In traditional software programs, we take for granted how easy it is to debug code by tracing program logic from variables back to input, apply unit tests and assertion statements to block erroneous behavior, and compose programs together. But as the programs we write grow more complex, it becomes hard to apply traditional software to applications like computer vision or natural language. Although deep learning programs have demonstrated strong performance on these applications, they sacrifice many of the functionalities of traditional software programs. In this paper, we work towards bridging the benefits of traditional and deep learning programs by jointly training a generative model to constrain neural network activations to ""decode"" back to inputs. Doing so enables practitioners to probe and track information encoded in activation(s), apply assertion-like constraints on what information is encoded in an activation, and compose separate neural networks together in a plug-and-play fashion. In our experiments, we demonstrate applications of decodable representations to out-of-distribution detection, adversarial examples, calibration, and fairness -- while matching standard neural networks in accuracy.",0
"The ease of debugging code in traditional software programs is often taken for granted. Debugging involves tracing program logic from variables back to input, applying unit tests and assertion statements to block erroneous behavior, and composing programs together. However, as software programs become more complex, it becomes difficult to apply traditional software to applications such as computer vision or natural language. Although deep learning programs have shown strong performance in these applications, they sacrifice many functionalities of traditional software programs. In this paper, we aim to combine the benefits of traditional and deep learning programs by jointly training a generative model to constrain neural network activations to ""decode"" back to inputs. This approach allows practitioners to track and probe information encoded in activations, apply assertion-like constraints on what information is encoded, and effortlessly compose separate neural networks. Our experiments demonstrate the benefits of decodable representations in detecting out-of-distribution issues, adversarial examples, calibration, and fairness, while retaining accuracy similar to standard neural networks.",1
"Several papers argue that wide minima generalize better than narrow minima. In this paper, through detailed experiments that not only corroborate the generalization properties of wide minima, we also provide empirical evidence for a new hypothesis that the density of wide minima is likely lower than the density of narrow minima. Further, motivated by this hypothesis, we design a novel explore-exploit learning rate schedule. On a variety of image and natural language datasets, compared to their original hand-tuned learning rate baselines, we show that our explore-exploit schedule can result in either up to 0.84% higher absolute accuracy using the original training budget or up to 57% reduced training time while achieving the original reported accuracy. For example, we achieve state-of-the-art (SOTA) accuracy for IWSLT'14 (DE-EN) dataset by just modifying the learning rate schedule of a high performing model.",0
"According to various papers, wide minima are believed to have better generalization abilities than narrow minima. In this study, we conducted extensive experiments to confirm this hypothesis and additionally discovered that the density of wide minima is likely lower than that of narrow minima. Based on this finding, we developed a new learning rate schedule called explore-exploit. We evaluated this method on various image and natural language datasets and found that it can improve accuracy by up to 0.84% within the original training budget or reduce training time by up to 57% while maintaining the original reported accuracy. Notably, we achieved state-of-the-art accuracy on the IWSLT'14 (DE-EN) dataset by simply modifying the learning rate schedule of a highly performing model.",1
"Questions of fairness, robustness, and transparency are paramount to address before deploying NLP systems. Central to these concerns is the question of reliability: Can NLP systems reliably treat different demographics fairly and function correctly in diverse and noisy environments? To address this, we argue for the need for reliability testing and contextualize it among existing work on improving accountability. We show how adversarial attacks can be reframed for this goal, via a framework for developing reliability tests. We argue that reliability testing -- with an emphasis on interdisciplinary collaboration -- will enable rigorous and targeted testing, and aid in the enactment and enforcement of industry standards.",0
"Before deploying NLP systems, it is crucial to consider questions of fairness, robustness, and transparency. A critical aspect of these concerns is the reliability of NLP systems. Can they function effectively in different environments and treat all demographics equitably? To tackle this issue, we propose the importance of reliability testing and place it in the context of existing efforts to improve accountability. Our framework for developing reliability tests reframes the use of adversarial attacks to meet this goal. We advocate for interdisciplinary collaboration in reliability testing to enhance its accuracy and enable targeted testing. This approach will help to establish and enforce industry standards for NLP systems.",1
"Machine-learning systems such as self-driving cars or virtual assistants are composed of a large number of machine-learning models that recognize image content, transcribe speech, analyze natural language, infer preferences, rank options, etc. Models in these systems are often developed and trained independently, which raises an obvious concern: Can improving a machine-learning model make the overall system worse? We answer this question affirmatively by showing that improving a model can deteriorate the performance of downstream models, even after those downstream models are retrained. Such self-defeating improvements are the result of entanglement between the models in the system. We perform an error decomposition of systems with multiple machine-learning models, which sheds light on the types of errors that can lead to self-defeating improvements. We also present the results of experiments which show that self-defeating improvements emerge in a realistic stereo-based detection system for cars and pedestrians.",0
"Machine-learning systems like virtual assistants or self-driving cars are made up of numerous machine-learning models that can recognize image content, transcribe speech, analyze natural language, infer preferences, rank options, and more. These models are typically developed and trained independently, which raises concerns about whether improving one model can negatively impact the overall system. We demonstrate that such self-defeating improvements can occur due to the entanglement between the models in the system, even after downstream models are retrained. Our research includes an error decomposition of systems with multiple machine-learning models to identify the types of errors that can result in self-defeating improvements. We also present experiments conducted on a realistic stereo-based detection system for cars and pedestrians that show the emergence of self-defeating improvements.",1
"In machine learning we often encounter structured output prediction problems (SOPPs), i.e. problems where the output space admits a rich internal structure. Application domains where SOPPs naturally occur include natural language processing, speech recognition, and computer vision. Typical SOPPs have an extremely large label set, which grows exponentially as a function of the size of the output. Existing generalization analysis implies generalization bounds with at least a square-root dependency on the cardinality $d$ of the label set, which can be vacuous in practice. In this paper, we significantly improve the state of the art by developing novel high-probability bounds with a logarithmic dependency on $d$. Moreover, we leverage the lens of algorithmic stability to develop generalization bounds in expectation without any dependency on $d$. Our results therefore build a solid theoretical foundation for learning in large-scale SOPPs. Furthermore, we extend our results to learning with weakly dependent data.",0
"Structured output prediction problems (SOPPs) are frequently encountered in machine learning, whereby the output space has a complex internal structure. SOPPs naturally occur in domains such as natural language processing, speech recognition, and computer vision. These problems often have a large label set that grows exponentially with the output size. Existing generalization analysis provides generalization bounds with at least a square-root dependency on the cardinality of the label set, which can be impractical. This paper presents novel high-probability bounds that have a logarithmic dependency on the label set size. Additionally, we use algorithmic stability to develop generalization bounds without any dependence on the label set size. These results provide a strong theoretical basis for learning in large-scale SOPPs. We also extend our findings to learning with weakly dependent data.",1
"Vehicle search is one basic task for the efficient traffic management in terms of the AI City. Most existing practices focus on the image-based vehicle matching, including vehicle re-identification and vehicle tracking. In this paper, we apply one new modality, i.e., the language description, to search the vehicle of interest and explore the potential of this task in the real-world scenario. The natural language-based vehicle search poses one new challenge of fine-grained understanding of both vision and language modalities. To connect language and vision, we propose to jointly train the state-of-the-art vision models with the transformer-based language model in an end-to-end manner. Except for the network structure design and the training strategy, several optimization objectives are also re-visited in this work. The qualitative and quantitative experiments verify the effectiveness of the proposed method. Our proposed method has achieved the 1st place on the 5th AI City Challenge, yielding competitive performance 18.69% MRR accuracy on the private test set. We hope this work can pave the way for the future study on using language description effectively and efficiently for real-world vehicle retrieval systems. The code will be available at https://github.com/ShuaiBai623/AIC2021-T5-CLV.",0
"Efficient traffic management in an AI City requires vehicle search as a fundamental task. The focus of most existing practices is on image-based vehicle matching, which includes vehicle tracking and re-identification. This paper explores the potential of using language description as a new modality to search for a specific vehicle and the challenges it poses in terms of understanding vision and language modalities. To connect language and vision, we propose an end-to-end joint training of state-of-the-art vision models with a transformer-based language model. We revisit optimization objectives, network structure design, and training strategies in this work. Qualitative and quantitative experiments confirm the effectiveness of our proposed method, which achieved 18.69% MRR accuracy on the private test set and the 1st place in the 5th AI City Challenge. We hope our work will pave the way for future research on using language description in real-world vehicle retrieval systems. The code is available at https://github.com/ShuaiBai623/AIC2021-T5-CLV.",1
"In this article, we introduce a novel variant of the Tsetlin machine (TM) that randomly drops clauses, the key learning elements of a TM. In effect, TM with drop clause ignores a random selection of the clauses in each epoch, selected according to a predefined probability. In this way, additional stochasticity is introduced in the learning phase of TM. Along with producing more distinct and well-structured patterns that improve the performance, we also show that dropping clauses increases learning robustness. To explore the effects clause dropping has on accuracy, training time, and interpretability, we conduct extensive experiments on various benchmark datasets in natural language processing (NLP) (IMDb and SST2) as well as computer vision (MNIST and CIFAR10). In brief, we observe from +2% to +4% increase in accuracy and 2x to 4x faster learning. We further employ the Convolutional TM to document interpretable results on the CIFAR10 dataset. To the best of our knowledge, this is the first time an interpretable machine learning algorithm has been used to produce pixel-level human-interpretable results on CIFAR10. Also, unlike previous interpretable methods that focus on attention visualisation or gradient interpretability, we show that the TM is a more general interpretable method. That is, by producing rule-based propositional logic expressions that are \emph{human}-interpretable, the TM can explain how it classifies a particular instance at the pixel level for computer vision and at the word level for NLP.",0
"This article presents a new version of the Tsetlin machine (TM) called TM with drop clause. This variant randomly omits clauses during each learning epoch, which introduces more stochasticity and improves performance and learning robustness. The article conducts extensive experiments on natural language processing (NLP) datasets (IMDb and SST2) and computer vision datasets (MNIST and CIFAR10) to explore how clause dropping affects accuracy, training time, and interpretability. The results show a 2% to 4% increase in accuracy and 2x to 4x faster learning. Additionally, the article uses the Convolutional TM to provide interpretable results on the CIFAR10 dataset. This is the first time a machine learning algorithm has produced pixel-level human-interpretable results on CIFAR10. The TM is a more general interpretable method than previous ones, producing rule-based propositional logic expressions that are human-interpretable and explaining how it classifies at the pixel or word level.",1
"Although Transformer has made breakthrough success in widespread domains especially in Natural Language Processing (NLP), applying it to time series forecasting is still a great challenge. In time series forecasting, the autoregressive decoding of canonical Transformer models could introduce huge accumulative errors inevitably. Besides, utilizing Transformer to deal with spatial-temporal dependencies in the problem still faces tough difficulties.~To tackle these limitations, this work is the first attempt to propose a Non-Autoregressive Transformer architecture for time series forecasting, aiming at overcoming the time delay and accumulative error issues in the canonical Transformer. Moreover, we present a novel spatial-temporal attention mechanism, building a bridge by a learned temporal influence map to fill the gaps between the spatial and temporal attention, so that spatial and temporal dependencies can be processed integrally. Empirically, we evaluate our model on diversified ego-centric future localization datasets and demonstrate state-of-the-art performance on both real-time and accuracy.",0
"The Transformer has been successful in various fields, especially in Natural Language Processing (NLP), but there are still challenges in applying it to time series forecasting. The autoregressive decoding in traditional Transformer models can lead to significant errors in time series forecasting, and incorporating Transformer into spatial-temporal dependencies is also difficult. This study introduces a Non-Autoregressive Transformer architecture for time series forecasting to address these issues. Additionally, a new spatial-temporal attention mechanism is proposed to integrate spatial and temporal dependencies. The model is evaluated on diverse ego-centric future localization datasets and shows outstanding performance in both real-time and accuracy.",1
"A number of problems in the processing of sound and natural language, as well as in other areas, can be reduced to simultaneously reading an input sequence and writing an output sequence of generally different length. There are well developed methods that produce the output sequence based on the entirely known input. However, efficient methods that enable such transformations on-line do not exist. In this paper we introduce an architecture that learns with reinforcement to make decisions about whether to read a token or write another token. This architecture is able to transform potentially infinite sequences on-line. In an experimental study we compare it with state-of-the-art methods for neural machine translation. While it produces slightly worse translations than Transformer, it outperforms the autoencoder with attention, even though our architecture translates texts on-line thereby solving a more difficult problem than both reference methods.",0
"The processing of sound and natural language, along with other areas, can be simplified by simultaneously reading an input sequence and writing an output sequence of varying length. Established methods exist for generating output sequences from entirely known input, but efficient methods for on-line transformations do not exist. This paper presents an architecture that learns through reinforcement to decide when to read or write a token, allowing the transformation of potentially infinite sequences on-line. In an experiment, we compare our architecture to state-of-the-art methods for neural machine translation and find that it outperforms the autoencoder with attention, even though our architecture solves a more challenging problem by translating texts on-line. Although it produces slightly worse translations than the Transformer, our architecture offers a promising solution to the difficult task of on-line sequence transformation.",1
"In this paper, we introduce Target-Aware Weighted Training (TAWT), a weighted training algorithm for cross-task learning based on minimizing a representation-based task distance between the source and target tasks. We show that TAWT is easy to implement, is computationally efficient, requires little hyperparameter tuning, and enjoys non-asymptotic learning-theoretic guarantees. The effectiveness of TAWT is corroborated through extensive experiments with BERT on four sequence tagging tasks in natural language processing (NLP), including part-of-speech (PoS) tagging, chunking, predicate detection, and named entity recognition (NER). As a byproduct, the proposed representation-based task distance allows one to reason in a theoretically principled way about several critical aspects of cross-task learning, such as the choice of the source data and the impact of fine-tuning",0
"This paper introduces Target-Aware Weighted Training (TAWT), an algorithm for cross-task learning that minimizes a representation-based task distance between the source and target tasks. TAWT is easy to implement, computationally efficient, and requires minimal hyperparameter tuning. Additionally, it has non-asymptotic learning-theoretic guarantees. The effectiveness of TAWT is demonstrated through experiments with BERT on four natural language processing (NLP) sequence tagging tasks, including PoS tagging, chunking, predicate detection, and NER. The proposed representation-based task distance also enables principled reasoning about important aspects of cross-task learning, such as source data selection and fine-tuning impact.",1
"Borrowing from the transformer models that revolutionized the field of natural language processing, self-supervised feature learning for visual tasks has also seen state-of-the-art success using these extremely deep, isotropic networks. However, the typical AI researcher does not have the resources to evaluate, let alone train, a model with several billion parameters and quadratic self-attention activations. To facilitate further research, it is necessary to understand the features of these huge transformer models that can be adequately studied by the typical researcher. One interesting characteristic of these transformer models is that they remove most of the inductive biases present in classical convolutional networks. In this work, we analyze the effect of these and more inductive biases on small to moderately-sized isotropic networks used for unsupervised visual feature learning and show that their removal is not always ideal.",0
"Similar to the transformer models that transformed natural language processing, self-supervised feature learning for visual tasks has also achieved impressive results using extremely deep, isotropic networks. However, most AI researchers do not have the resources to evaluate or train models with billions of parameters and quadratic self-attention activations. To enable further research, it is important to identify the features of these large transformer models that can be studied by typical researchers. One notable characteristic of these models is their ability to eliminate most of the inductive biases found in classical convolutional networks. In this study, we examine the impact of these and other inductive biases on small to moderately-sized isotropic networks used for unsupervised visual feature learning, and demonstrate that their elimination is not always beneficial.",1
"Purpose: This study evaluates the effectiveness and impact of automated order-based protocol assignment for magnetic resonance imaging (MRI) exams using natural language processing (NLP) and deep learning (DL).   Methods: NLP tools were applied to retrospectively process orders from over 116,000 MRI exams with 200 unique sub-specialized protocols (""Local"" protocol class). Separate DL models were trained on 70\% of the processed data for ""Local"" protocols as well as 93 American College of Radiology (""ACR"") protocols and 48 ""General"" protocols. The DL Models were assessed in an ""auto-protocoling (AP)"" inference mode which returns the top recommendation and in a ""clinical decision support (CDS)"" inference mode which returns up to 10 protocols for radiologist review. The accuracy of each protocol recommendation was computed and analyzed based on the difference between the normalized output score of the corresponding neural net for the top two recommendations.   Results: The top predicted protocol in AP mode was correct for 82.8%, 73.8%, and 69.3% of the test cases for ""General"", ""ACR"", and ""Local"" protocol classes, respectively. Higher levels of accuracy over 96% were obtained for all protocol classes in CDS mode. However, at current validation performance levels, the proposed models offer modest, positive, financial impact on large-scale imaging networks.   Conclusions: DL-based protocol automation is feasible and can be tuned to route substantial fractions of exams for auto-protocoling, with higher accuracy with more general protocols. Economic analyses of the tested algorithms indicate that improved algorithm performance is required to yield a practical exam auto-protocoling tool for sub-specialized imaging exams.",0
"The objective of this research is to assess the efficiency and influence of using natural language processing (NLP) and deep learning (DL) to automatically assign protocols for magnetic resonance imaging (MRI) exams based on orders. To achieve this, NLP tools were applied to retrospectively analyze over 116,000 MRI exams with 200 unique sub-specialized protocols in the ""Local"" protocol category. DL models were then developed and trained on 70% of the processed data for ""Local,"" ""General,"" and 93 American College of Radiology (""ACR"") protocols. The models were evaluated in two modes: ""auto-protocoling"" (AP) and ""clinical decision support"" (CDS), which provide the top recommendation and up to 10 protocols for review by radiologists, respectively. The accuracy of each protocol recommendation was computed based on the difference between the normalized output score of the corresponding neural net for the top two recommendations. The study found that the accuracy of the top recommended protocol was 82.8%, 73.8%, and 69.3% for ""General,"" ""ACR,"" and ""Local"" protocol categories, respectively, in AP mode. However, in CDS mode, accuracy was over 96% for all protocol categories. The research concludes that DL-based protocol automation is feasible and can be adjusted to direct a significant number of exams for auto-protocoling, with greater accuracy for more general protocols. Nonetheless, the algorithms require improved performance to be financially practical for sub-specialized imaging exams.",1
"Natural Language Inference (NLI) datasets contain annotation artefacts resulting in spurious correlations between the natural language utterances and their respective entailment classes. These artefacts are exploited by neural networks even when only considering the hypothesis and ignoring the premise, leading to unwanted biases. Belinkov et al. (2019b) proposed tackling this problem via adversarial training, but this can lead to learned sentence representations that still suffer from the same biases. We show that the bias can be reduced in the sentence representations by using an ensemble of adversaries, encouraging the model to jointly decrease the accuracy of these different adversaries while fitting the data. This approach produces more robust NLI models, outperforming previous de-biasing efforts when generalised to 12 other datasets (Belinkov et al., 2019a; Mahabadi et al., 2020). In addition, we find that the optimal number of adversarial classifiers depends on the dimensionality of the sentence representations, with larger sentence representations being more difficult to de-bias while benefiting from using a greater number of adversaries.",0
"NLI datasets have annotation errors that create false connections between natural language utterances and their entailment categories. These errors are taken advantage of by neural networks, leading to unwanted biases even when only considering the hypothesis. Belinkov et al. suggested solving this issue via adversarial training, but this approach can still result in biased sentence representations. Our study shows that using an ensemble of adversaries can reduce bias in sentence representations by jointly decreasing the accuracy of multiple adversaries while fitting the data. This method produces more resilient NLI models, surpassing previous de-biasing efforts when applied to 12 other datasets. Moreover, the optimal number of adversarial classifiers depends on the dimensionality of sentence representations, with larger representations requiring more adversaries to de-bias effectively.",1
"This paper presents a new Vision Transformer (ViT) architecture Multi-Scale Vision Longformer, which significantly enhances the ViT of \cite{dosovitskiy2020image} for encoding high-resolution images using two techniques. The first is the multi-scale model structure, which provides image encodings at multiple scales with manageable computational cost. The second is the attention mechanism of vision Longformer, which is a variant of Longformer \cite{beltagy2020longformer}, originally developed for natural language processing, and achieves a linear complexity w.r.t. the number of input tokens. A comprehensive empirical study shows that the new ViT significantly outperforms several strong baselines, including the existing ViT models and their ResNet counterparts, and the Pyramid Vision Transformer from a concurrent work \cite{wang2021pyramid}, on a range of vision tasks, including image classification, object detection, and segmentation. The models and source code are released at \url{https://github.com/microsoft/vision-longformer}.",0
"In this article, a novel Multi-Scale Vision Longformer architecture for the Vision Transformer (ViT) is introduced, which greatly improves the encoding of high-resolution images by utilizing two methods. Firstly, the multi-scale model structure provides image encodings at different scales, while keeping computational costs manageable. Secondly, the attention mechanism of the vision Longformer is employed, which is a variation of the Longformer technique originally developed for natural language processing, and achieves a linear complexity with respect to the number of input tokens. The effectiveness of the new ViT is demonstrated through a comprehensive empirical study, which shows it outperforms various strong baselines including existing ViT models, their ResNet counterparts, and the Pyramid Vision Transformer from a concurrent study, across a range of vision tasks such as image classification, object detection, and segmentation. The models and source code are publicly available at \url{https://github.com/microsoft/vision-longformer}.",1
"Transformer networks are effective at modeling long-range contextual information and have recently demonstrated exemplary performance in the natural language processing domain. Conventionally, the temporal action proposal generation (TAPG) task is divided into two main sub-tasks: boundary prediction and proposal confidence prediction, which rely on the frame-level dependencies and proposal-level relationships separately. To capture the dependencies at different levels of granularity, this paper intuitively presents a unified temporal action proposal generation framework with original Transformers, called TAPG Transformer, which consists of a Boundary Transformer and a Proposal Transformer. Specifically, the Boundary Transformer captures long-term temporal dependencies to predict precise boundary information and the Proposal Transformer learns the rich inter-proposal relationships for reliable confidence evaluation. Extensive experiments are conducted on two popular benchmarks: ActivityNet-1.3 and THUMOS14, and the results demonstrate that TAPG Transformer outperforms state-of-the-art methods. Equipped with the existing action classifier, our method achieves remarkable performance on the temporal action localization task. Codes and models will be available.",0
"Recently, Transformer networks have displayed exceptional performance in natural language processing by effectively modeling contextual information over long distances. The temporal action proposal generation (TAPG) task typically involves two sub-tasks: boundary prediction and proposal confidence prediction, which separately rely on frame-level dependencies and proposal-level relationships. To capture dependencies at various levels of granularity, this paper proposes a unified TAPG Transformer framework utilizing original Transformers. This framework includes a Boundary Transformer to predict precise boundary information by capturing long-term temporal dependencies, and a Proposal Transformer to learn inter-proposal relationships for reliable confidence evaluation. Results from experiments conducted on two popular benchmarks, ActivityNet-1.3 and THUMOS14, demonstrate that the proposed TAPG Transformer outperforms state-of-the-art methods. When combined with an existing action classifier, our method achieves remarkable performance on the temporal action localization task. We will provide access to codes and models.",1
"Interactive robots navigating photo-realistic environments face challenges underlying vision-and-language navigation (VLN), but in addition, they need to be trained to handle the dynamic nature of dialogue. However, research in Cooperative Vision-and-Dialog Navigation (CVDN), where a navigator interacts with a guide in natural language in order to reach a goal, treats the dialogue history as a VLN-style static instruction. In this paper, we present VISITRON, a navigator better suited to the interactive regime inherent to CVDN by being trained to: i) identify and associate object-level concepts and semantics between the environment and dialogue history, ii) identify when to interact vs. navigate via imitation learning of a binary classification head. We perform extensive ablations with VISITRON to gain empirical insights and improve performance on CVDN. VISITRON is competitive with models on the static CVDN leaderboard. We also propose a generalized interactive regime to fine-tune and evaluate VISITRON and future such models with pre-trained guides for adaptability.",0
"Robots that navigate through realistic environments face challenges related to vision and language navigation. Furthermore, they need to be trained to handle dynamic dialogues. However, research in Cooperative Vision-and-Dialog Navigation treats dialogue history as a static instruction, which is not suitable for interactive navigation. This paper presents VISITRON, a navigator that is better suited for interactive navigation by being trained to identify object-level concepts and associate semantics between the environment and dialogue history. VISITRON also knows when to interact or navigate through imitation learning. Extensive ablations are performed to improve VISITRON's performance on Cooperative Vision-and-Dialog Navigation. VISITRON is competitive with models on the static leaderboard. Additionally, a generalized interactive regime is proposed to fine-tune and evaluate VISITRON and future models with pre-trained guides for adaptability.",1
"The AI City Challenge was created with two goals in mind: (1) pushing the boundaries of research and development in intelligent video analysis for smarter cities use cases, and (2) assessing tasks where the level of performance is enough to cause real-world adoption. Transportation is a segment ripe for such adoption. The fifth AI City Challenge attracted 305 participating teams across 38 countries, who leveraged city-scale real traffic data and high-quality synthetic data to compete in five challenge tracks. Track 1 addressed video-based automatic vehicle counting, where the evaluation being conducted on both algorithmic effectiveness and computational efficiency. Track 2 addressed city-scale vehicle re-identification with augmented synthetic data to substantially increase the training set for the task. Track 3 addressed city-scale multi-target multi-camera vehicle tracking. Track 4 addressed traffic anomaly detection. Track 5 was a new track addressing vehicle retrieval using natural language descriptions. The evaluation system shows a general leader board of all submitted results, and a public leader board of results limited to the contest participation rules, where teams are not allowed to use external data in their work. The public leader board shows results more close to real-world situations where annotated data is limited. Results show the promise of AI in Smarter Transportation. State-of-the-art performance for some tasks shows that these technologies are ready for adoption in real-world systems.",0
"The AI City Challenge aims to achieve two objectives: (1) advance research and development in intelligent video analysis for smarter cities use cases, and (2) evaluate tasks that can lead to real-world adoption. Transportation is a promising domain for such adoption. In the fifth AI City Challenge, 305 teams from 38 countries competed in five challenge tracks using real traffic data and high-quality synthetic data. Track 1 involved video-based automatic vehicle counting, while track 2 focused on city-scale vehicle re-identification with augmented synthetic data. Track 3 addressed city-scale multi-target multi-camera vehicle tracking, track 4 addressed traffic anomaly detection, and track 5 was a new track that involved vehicle retrieval using natural language descriptions. Results were evaluated based on algorithmic effectiveness and computational efficiency. The evaluation system had a general leaderboard of all submitted results and a public leaderboard that was limited to the contest participation rules. The public leaderboard was more representative of real-world situations where annotated data is limited. The results demonstrate the potential of AI in Smarter Transportation, with some tasks showing state-of-the-art performance that makes these technologies ready for adoption in real-world systems.",1
"Recently, learning a model that generalizes well on out-of-distribution (OOD) data has attracted great attention in the machine learning community. In this paper, after defining OOD generalization via Wasserstein distance, we theoretically show that a model robust to input perturbation generalizes well on OOD data. Inspired by previous findings that adversarial training helps improve input-robustness, we theoretically show that adversarially trained models have converged excess risk on OOD data, and empirically verify it on both image classification and natural language understanding tasks. Besides, in the paradigm of first pre-training and then fine-tuning, we theoretically show that a pre-trained model that is more robust to input perturbation provides a better initialization for generalization on downstream OOD data. Empirically, after fine-tuning, this better-initialized model from adversarial pre-training also has better OOD generalization.",0
"The machine learning community has recently taken an interest in developing models that can effectively handle out-of-distribution (OOD) data. This paper defines OOD generalization using Wasserstein distance and presents theoretical evidence that input-robust models perform well on OOD data. Adversarial training has been shown to improve input-robustness, and this paper demonstrates that models trained using this method have superior OOD generalization in both image classification and natural language understanding. Additionally, the authors propose that pre-training models to be more input-robust can improve their generalization on OOD data, and this theory is supported by empirical evidence. Overall, this paper shows that robustness to input perturbation is a crucial factor in achieving effective OOD generalization.",1
"Searching for objects in indoor organized environments such as homes or offices is part of our everyday activities. When looking for a target object, we jointly reason about the rooms and containers the object is likely to be in; the same type of container will have a different probability of having the target depending on the room it is in. We also combine geometric and semantic information to infer what container is best to search, or what other objects are best to move, if the target object is hidden from view. We propose to use a 3D scene graph representation to capture the hierarchical, semantic, and geometric aspects of this problem. To exploit this representation in a search process, we introduce Hierarchical Mechanical Search (HMS), a method that guides an agent's actions towards finding a target object specified with a natural language description. HMS is based on a novel neural network architecture that uses neural message passing of vectors with visual, geometric, and linguistic information to allow HMS to reason across layers of the graph while combining semantic and geometric cues. HMS is evaluated on a novel dataset of 500 3D scene graphs with dense placements of semantically related objects in storage locations, and is shown to be significantly better than several baselines at finding objects and close to the oracle policy in terms of the median number of actions required. Additional qualitative results can be found at https://ai.stanford.edu/mech-search/hms.",0
"In our daily routine, we often search for objects in organized indoor environments like homes or offices. Our search for a specific object involves reasoning about the rooms and containers where it might be located. The probability of finding the object within a container varies depending on the room it is in. To identify the best container to search or the best object to move when the target is not visible, we combine semantic and geometric information. We propose using a 3D scene graph representation to capture the hierarchical, semantic, and geometric aspects of the problem. To facilitate the search process, we introduce Hierarchical Mechanical Search (HMS), which guides an agent's actions to locate a target object described in natural language. HMS employs a novel neural network architecture that leverages neural message passing of vectors containing visual, geometric, and linguistic information to reason across the graph layers, combining semantic and geometric cues. We evaluate HMS on a dataset of 500 3D scene graphs with dense placements of semantically related objects in storage locations. The results show that HMS outperforms several baselines in finding objects and is close to the oracle policy in terms of the median number of actions required. Additional qualitative results can be viewed at https://ai.stanford.edu/mech-search/hms.",1
"Facial Expression Recognition (FER) in the wild is extremely challenging due to occlusions, variant head poses, face deformation and motion blur under unconstrained conditions. Although substantial progresses have been made in automatic FER in the past few decades, previous studies are mainly designed for lab-controlled FER. Real-world occlusions, variant head poses and other issues definitely increase the difficulty of FER on account of these information-deficient regions and complex backgrounds. Different from previous pure CNNs based methods, we argue that it is feasible and practical to translate facial images into sequences of visual words and perform expression recognition from a global perspective. Therefore, we propose Convolutional Visual Transformers to tackle FER in the wild by two main steps. First, we propose an attentional selective fusion (ASF) for leveraging the feature maps generated by two-branch CNNs. The ASF captures discriminative information by fusing multiple features with global-local attention. The fused feature maps are then flattened and projected into sequences of visual words. Second, inspired by the success of Transformers in natural language processing, we propose to model relationships between these visual words with global self-attention. The proposed method are evaluated on three public in-the-wild facial expression datasets (RAF-DB, FERPlus and AffectNet). Under the same settings, extensive experiments demonstrate that our method shows superior performance over other methods, setting new state of the art on RAF-DB with 88.14%, FERPlus with 88.81% and AffectNet with 61.85%. We also conduct cross-dataset evaluation on CK+ show the generalization capability of the proposed method.",0
"Recognizing facial expressions in the wild is a difficult task due to a variety of challenges such as occlusions, head poses, face deformation, and motion blur under unconstrained conditions. While there have been significant advancements in automatic facial expression recognition over the years, previous studies have mainly focused on lab-controlled environments. However, real-world scenarios with occlusions and complex backgrounds make it harder to recognize facial expressions. In this regard, we propose Convolutional Visual Transformers to address this issue. Our approach involves translating facial images into sequences of visual words and performing expression recognition from a global perspective. To achieve this, we introduce an attentional selective fusion (ASF) approach that captures discriminative information by fusing multiple features with global-local attention. The fused feature maps are then projected into sequences of visual words, and relationships between these words are modeled using global self-attention inspired by Transformers used in natural language processing. We evaluate our method on three public datasets and demonstrate its superior performance compared to other methods, setting new state-of-the-art results. Additionally, our proposed method shows good generalization capabilities on cross-dataset evaluation.",1
"Automated image captioning is one of the applications of Deep Learning which involves fusion of work done in computer vision and natural language processing, and it is typically performed using Encoder-Decoder architectures. In this project, we have implemented and experimented with various flavors of multi-modal image captioning networks where ResNet101, DenseNet121 and VGG19 based CNN Encoders and Attention based LSTM Decoders were explored. We have studied the effect of beam size and the use of pretrained word embeddings and compared them to baseline CNN encoder and RNN decoder architecture. The goal is to analyze the performance of each approach using various evaluation metrics including BLEU, CIDEr, ROUGE and METEOR. We have also explored model explainability using Visual Attention Maps (VAM) to highlight parts of the images which has maximum contribution for predicting each word of the generated caption.",0
"One application of Deep Learning is the automation of image captioning, which combines work from computer vision and natural language processing. This is done through the use of Encoder-Decoder architectures. Our project focused on implementing and experimenting with different types of multi-modal image captioning networks, including those that utilize ResNet101, DenseNet121, and VGG19 based CNN Encoders, as well as Attention based LSTM Decoders. We examined the effects of beam size and the use of pretrained word embeddings, comparing them to a baseline CNN encoder and RNN decoder architecture. Our aim was to evaluate each approach using various metrics such as BLEU, CIDEr, ROUGE, and METEOR. Additionally, we explored model explainability by using Visual Attention Maps (VAM) to identify which parts of the images had the greatest impact on predicting each word of the generated caption.",1
"In this paper, we consider hybrid parallelism -- a paradigm that employs both Data Parallelism (DP) and Model Parallelism (MP) -- to scale distributed training of large recommendation models. We propose a compression framework called Dynamic Communication Thresholding (DCT) for communication-efficient hybrid training. DCT filters the entities to be communicated across the network through a simple hard-thresholding function, allowing only the most relevant information to pass through. For communication efficient DP, DCT compresses the parameter gradients sent to the parameter server during model synchronization. The threshold is updated only once every few thousand iterations to reduce the computational overhead of compression. For communication efficient MP, DCT incorporates a novel technique to compress the activations and gradients sent across the network during the forward and backward propagation, respectively. This is done by identifying and updating only the most relevant neurons of the neural network for each training sample in the data. We evaluate DCT on publicly available natural language processing and recommender models and datasets, as well as recommendation systems used in production at Facebook. DCT reduces communication by at least $100\times$ and $20\times$ during DP and MP, respectively. The algorithm has been deployed in production, and it improves end-to-end training time for a state-of-the-art industrial recommender model by 37\%, without any loss in performance.",0
"This paper focuses on hybrid parallelism, which combines Data Parallelism (DP) and Model Parallelism (MP) to scale distributed training of large recommendation models. To achieve communication-efficient hybrid training, we propose a compression framework called Dynamic Communication Thresholding (DCT), which filters the entities to be communicated across the network using a hard-thresholding function. For DP, DCT compresses the parameter gradients sent to the parameter server during model synchronization, with the threshold updated only once every few thousand iterations. For MP, DCT compresses the activations and gradients sent across the network during forward and backward propagation by identifying and updating only the most relevant neurons of the neural network for each training sample. We evaluate DCT on various natural language processing and recommender models and datasets, as well as recommendation systems used in production at Facebook. Our results show that DCT reduces communication by at least $100\times$ and $20\times$ during DP and MP, respectively. The algorithm has been deployed in production and has improved the end-to-end training time for a state-of-the-art industrial recommender model by 37\%, without any loss in performance.",1
"Attention layers are widely used in natural language processing (NLP) and are beginning to influence computer vision architectures. Training very large transformer models allowed significant improvement in both fields, but once trained, these networks show symptoms of over-parameterization. For instance, it is known that many attention heads can be pruned without impacting accuracy. This work aims to enhance current understanding on how multiple heads interact. Motivated by the observation that attention heads learn redundant key/query projections, we propose a collaborative multi-head attention layer that enables heads to learn shared projections. Our scheme decreases the number of parameters in an attention layer and can be used as a drop-in replacement in any transformer architecture. Our experiments confirm that sharing key/query dimensions can be exploited in language understanding, machine translation and vision. We also show that it is possible to re-parametrize a pre-trained multi-head attention layer into our collaborative attention layer. Collaborative multi-head attention reduces the size of the key and query projections by 4 for same accuracy and speed. Our code is public.",0
"Attention layers are commonly utilized in natural language processing (NLP) and are gradually influencing computer vision designs. The use of large transformer models has resulted in significant advancements in both fields; however, these networks demonstrate signs of over-parameterization after being trained. For example, it is established that numerous attention heads can be removed without affecting accuracy. This study aims to broaden current comprehension of how multiple heads interact. With the understanding that attention heads learn redundant key/query projections, we suggest a collaborative multi-head attention layer that allows heads to learn shared projections. Our plan curtails the number of parameters in an attention layer and can be used as a substitute in any transformer architecture. Our experiments validate that sharing key/query dimensions can be utilized in language comprehension, machine translation, and vision. Furthermore, we demonstrate that it is feasible to restructure a pre-trained multi-head attention layer into our collaborative attention layer. Collaborative multi-head attention reduces the size of the key and query projections by 4, while retaining the same accuracy and speed. The public can access our code.",1
"Smart home environments are designed to provide services that help improve the quality of life for the occupant via a variety of sensors and actuators installed throughout the space. Many automated actions taken by a smart home are governed by the output of an underlying activity recognition system. However, activity recognition systems may not be perfectly accurate and therefore inconsistencies in smart home operations can lead a user to wonder ""why did the smart home do that?"" In this work, we build on insights from Explainable Artificial Intelligence (XAI) techniques to contribute computational methods for explainable activity recognition. Specifically, we generate explanations for smart home activity recognition systems that explain what about an activity led to the given classification. To do so, we introduce four computational techniques for generating natural language explanations of smart home data and compare their effectiveness at generating meaningful explanations. Through a study with everyday users, we evaluate user preferences towards the four explanation types. Our results show that the leading approach, SHAP, has a 92% success rate in generating accurate explanations. Moreover, 84% of sampled scenarios users preferred natural language explanations over a simple activity label, underscoring the need for explainable activity recognition systems. Finally, we show that explanations generated by some XAI methods can lead users to lose confidence in the accuracy of the underlying activity recognition model, while others lead users to gain confidence. Taking all studied factors into consideration, we make a recommendation regarding which existing XAI method leads to the best performance in the domain of smart home automation, and discuss a range of topics for future work in this area.",0
"Smart home environments aim to enhance the occupant's quality of life through various sensors and actuators placed throughout the space. However, the accuracy of the underlying activity recognition system may affect the consistency of automated actions, leading users to question the system's decision-making process. This study utilizes Explainable Artificial Intelligence (XAI) techniques to develop computational methods for explainable activity recognition in smart homes. The researchers introduce four computational techniques for generating natural language explanations of smart home data and compare their effectiveness in generating meaningful explanations. Through a study with everyday users, the researchers evaluate user preferences towards the four explanation types. The leading approach, SHAP, has a 92% success rate in generating accurate explanations, and 84% of sampled scenarios preferred natural language explanations over a simple activity label, highlighting the need for explainable activity recognition systems. Additionally, the researchers show that certain XAI methods can lead users to lose confidence in the accuracy of the underlying activity recognition model, while others lead users to gain confidence. Based on their findings, the researchers recommend the best-performing XAI method for smart home automation and suggest future areas of research in this field.",1
"Chinese word segmentation (CWS) is the basic of Chinese natural language processing (NLP). The quality of word segmentation will directly affect the rest of NLP tasks. Recently, with the artificial intelligence tide rising again, Long Short-Term Memory (LSTM) neural network, as one of easily modeling in sequence, has been widely utilized in various kinds of NLP tasks, and functions well. Attention mechanism is an ingenious method to solve the memory compression problem on LSTM. Furthermore, inspired by the powerful abilities of bidirectional LSTM models for modeling sequence and CRF model for decoding, we propose a Bidirectional LSTM-CRF Attention-based Model in this paper. Experiments on PKU and MSRA benchmark datasets show that our model performs better than the baseline methods modeling by other neural networks.",0
"The foundation of Chinese natural language processing (NLP) is Chinese word segmentation (CWS), and the accuracy of CWS has a direct impact on the success of other NLP tasks. As the field of artificial intelligence continues to grow, Long Short-Term Memory (LSTM) neural networks have become a popular tool for modeling sequences in various NLP tasks. The use of Attention mechanism has also proven effective in addressing memory compression issues within LSTM. Building on the strengths of bidirectional LSTM models for sequence modeling and CRF models for decoding, we present a Bidirectional LSTM-CRF Attention-based Model in this study. Our experiments on PKU and MSRA datasets demonstrate that our model outperforms other neural network-based baseline methods.",1
"Leveraging the advances of natural language processing, most recent scene text recognizers adopt an encoder-decoder architecture where text images are first converted to representative features and then a sequence of characters via `direct decoding'. However, scene text images suffer from rich noises of different sources such as complex background and geometric distortions which often confuse the decoder and lead to incorrect alignment of visual features at noisy decoding time steps. This paper presents I2C2W, a novel scene text recognizer that is accurate and tolerant to various noises in scenes. I2C2W consists of an image-to-character module (I2C) and a character-to-word module (C2W) which are complementary and can be trained end-to-end. I2C detects characters and predicts their relative positions in a word. It strives to detect all possible characters including incorrect and redundant ones based on different alignments of visual features without the restriction of time steps. Taking the detected characters as input, C2W learns from character semantics and their positions to filter out incorrect and redundant detection and produce the final word recognition. Extensive experiments over seven public datasets show that I2C2W achieves superior recognition performances and outperforms the state-of-the-art by large margins on challenging irregular scene text datasets.",0
"Recent scene text recognizers typically employ an encoder-decoder architecture utilizing natural language processing to convert text images into character sequences through ""direct decoding."" However, scene text images are often riddled with various noise sources, such as complex backgrounds and geometric distortions, which can lead to erroneous decoding and misalignment of visual features. This paper introduces I2C2W, a novel scene text recognizer that is both accurate and tolerant of noisy scenes. I2C2W comprises an image-to-character module (I2C) and a character-to-word module (C2W), which work in tandem and can be trained end-to-end. I2C identifies characters and predicts their relative positions within a word, aiming to detect all possible characters regardless of time steps, including incorrect and redundant ones. C2W leverages character semantics and their positions to filter out incorrect and redundant detections and produce the final word recognition. Extensive experiments conducted on seven public datasets demonstrate that I2C2W achieves superior recognition performance and outperforms state-of-the-art techniques by large margins, particularly on challenging and irregular scene text datasets.",1
"Performance of recommender systems (RS) relies heavily on the amount of training data available. This poses a chicken-and-egg problem for early-stage products, whose amount of data, in turn, relies on the performance of their RS. On the other hand, zero-shot learning promises some degree of generalization from an old dataset to an entirely new dataset. In this paper, we explore the possibility of zero-shot learning in RS. We develop an algorithm, dubbed ZEro-Shot Recommenders (ZESRec), that is trained on an old dataset and generalize to a new one where there are neither overlapping users nor overlapping items, a setting that contrasts typical cross-domain RS that has either overlapping users or items. Different from categorical item indices, i.e., item ID, in previous methods, ZESRec uses items' natural-language descriptions (or description embeddings) as their continuous indices, and therefore naturally generalize to any unseen items. In terms of users, ZESRec builds upon recent advances on sequential RS to represent users using their interactions with items, thereby generalizing to unseen users as well. We study two pairs of real-world RS datasets and demonstrate that ZESRec can successfully enable recommendations in such a zero-shot setting, opening up new opportunities for resolving the chicken-and-egg problem for data-scarce startups or early-stage products.",0
"The effectiveness of recommender systems relies heavily on the amount of training data available, creating a problem for new products that lack data, which in turn affects the performance of their RS. However, zero-shot learning offers a degree of generalization from an old dataset to a new one. In this study, we investigate the potential of zero-shot learning in RS by creating an algorithm called ZEro-Shot Recommenders (ZESRec), which is trained on an old dataset and can generalize to a new one without overlapping users or items. Unlike previous methods that use categorical item indices, ZESRec uses items' natural-language descriptions or embeddings, allowing it to generalize to any unseen items. ZESRec also represents users using their interactions with items, enabling it to generalize to unseen users. We tested ZESRec on two real-world RS datasets and demonstrated its effectiveness in enabling recommendations in a zero-shot setting, providing new opportunities for data-scarce startups or early-stage products to overcome the chicken-and-egg problem.",1
"When working to understand usage of a data format, examples of the data format are often more representative than the format's specification. For example, two different applications might use very different JSON representations, or two PDF-writing applications might make use of very different areas of the PDF specification to realize the same rendered content. The complexity arising from these distinct origins can lead to large, difficult-to-understand attack surfaces, presenting a security concern when considering both exfiltration and data schizophrenia. Grammar inference can aid in describing the practical language generator behind examples of a data format. However, most grammar inference research focuses on natural language, not data formats, and fails to support crucial features such as type recursion. We propose a novel set of mechanisms for grammar inference, RL-GRIT, and apply them to understanding de facto data formats. After reviewing existing grammar inference solutions, it was determined that a new, more flexible scaffold could be found in Reinforcement Learning (RL). Within this work, we lay out the many algorithmic changes required to adapt RL from its traditional, sequential-time environment to the highly interdependent environment of parsing. The result is an algorithm which can demonstrably learn recursive control structures in simple data formats, and can extract meaningful structure from fragments of the PDF format. Whereas prior work in grammar inference focused on either regular languages or constituency parsing, we show that RL can be used to surpass the expressiveness of both classes, and offers a clear path to learning context-sensitive languages. The proposed algorithm can serve as a building block for understanding the ecosystems of de facto data formats.",0
"To comprehend a data format's usage, examples are often more representative than its specification. For instance, different applications may use different JSON representations or PDF-writing applications may utilize distinct areas of the PDF specification to achieve the same outcome. These variations result in complicated and hard-to-understand attack surfaces, posing a security risk for data schizophrenia and exfiltration. Grammar inference can assist in describing the practical language generator behind examples of a data format, but current research on grammar inference mainly focuses on natural language and lacks vital features like type recursion. To address this limitation, we present a new mechanism for grammar inference called RL-GRIT, which we apply to understand de facto data formats. Our approach adapts Reinforcement Learning (RL) to the highly interdependent environment of parsing, resulting in an algorithm that can learn recursive control structures in simple data formats and extract meaningful structure from fragments of the PDF format. Our proposed algorithm surpasses the expressiveness of both regular languages and constituency parsing and offers a clear path to learning context-sensitive languages. It can serve as a building block for understanding the ecosystems of de facto data formats.",1
"Given a natural language expression and an image/video, the goal of referring segmentation is to produce the pixel-level masks of the entities described by the subject of the expression. Previous approaches tackle this problem by implicit feature interaction and fusion between visual and linguistic modalities in a one-stage manner. However, human tends to solve the referring problem in a progressive manner based on informative words in the expression, i.e., first roughly locating candidate entities and then distinguishing the target one. In this paper, we propose a Cross-Modal Progressive Comprehension (CMPC) scheme to effectively mimic human behaviors and implement it as a CMPC-I (Image) module and a CMPC-V (Video) module to improve referring image and video segmentation models. For image data, our CMPC-I module first employs entity and attribute words to perceive all the related entities that might be considered by the expression. Then, the relational words are adopted to highlight the target entity as well as suppress other irrelevant ones by spatial graph reasoning. For video data, our CMPC-V module further exploits action words based on CMPC-I to highlight the correct entity matched with the action cues by temporal graph reasoning. In addition to the CMPC, we also introduce a simple yet effective Text-Guided Feature Exchange (TGFE) module to integrate the reasoned multimodal features corresponding to different levels in the visual backbone under the guidance of textual information. In this way, multi-level features can communicate with each other and be mutually refined based on the textual context. Combining CMPC-I or CMPC-V with TGFE can form our image or video version referring segmentation frameworks and our frameworks achieve new state-of-the-art performances on four referring image segmentation benchmarks and three referring video segmentation benchmarks respectively.",0
"The objective of referring segmentation is to generate pixel-level masks of entities depicted in an image/video based on a natural language expression. Prior methods address this issue by combining visual and linguistic modalities in a one-step approach using implicit feature interaction and fusion. Conversely, humans tend to solve referring problems progressively by first identifying potential entities and then selecting the target one based on informative words in the expression. To emulate human behavior, we propose a Cross-Modal Progressive Comprehension (CMPC) scheme, which we implement as a CMPC-I (Image) and CMPC-V (Video) module to enhance image and video segmentation models. The CMPC-I module uses entity and attribute words to determine related entities, then employs relational words for spatial graph reasoning to highlight the target entity and suppress irrelevant ones. The CMPC-V module builds on CMPC-I and utilizes action words for temporal graph reasoning to identify the correct entity matching the action cues. Additionally, we introduce a Text-Guided Feature Exchange (TGFE) module to combine multimodal features from different levels in the visual backbone under textual guidance. The CMPC-I/CMPC-V with TGFE forms our image/video version referring segmentation frameworks, which outperform existing methods on four referring image segmentation benchmarks and three referring video segmentation benchmarks.",1
"Explaining the behavior of black box machine learning models through human interpretable rules is an important research area. Recent work has focused on explaining model behavior locally i.e. for specific predictions as well as globally across the fields of vision, natural language, reinforcement learning and data science. We present a novel model-agnostic approach that derives rules to globally explain the behavior of classification models trained on numerical and/or categorical data. Our approach builds on top of existing local model explanation methods to extract conditions important for explaining model behavior for specific instances followed by an evolutionary algorithm that optimizes an information theory based fitness function to construct rules that explain global model behavior. We show how our approach outperforms existing approaches on a variety of datasets. Further, we introduce a parameter to evaluate the quality of interpretation under the scenario of distributional shift. This parameter evaluates how well the interpretation can predict model behavior for previously unseen data distributions. We show how existing approaches for interpreting models globally lack distributional robustness. Finally, we show how the quality of the interpretation can be improved under the scenario of distributional shift by adding out of distribution samples to the dataset used to learn the interpretation and thereby, increase robustness. All of the datasets used in our paper are open and publicly available. Our approach has been deployed in a leading digital marketing suite of products.",0
"An important area of research is explaining the behavior of black box machine learning models through human interpretable rules. Recent studies have concentrated on explaining model behavior locally, for specific predictions, as well as globally across various fields such as vision, natural language, reinforcement learning, and data science. We have introduced a novel model-agnostic approach that can derive rules to globally explain the behavior of classification models trained on categorical and/or numerical data. Our approach is built on existing local model explanation methods, which extract conditions vital for explaining model behavior for specific instances. We then use an evolutionary algorithm that optimizes an information theory-based fitness function to construct rules that explain global model behavior. Our approach surpasses existing approaches on a variety of datasets, and we have introduced a parameter to evaluate interpretation quality under the scenario of distributional shift. This parameter assesses how well the interpretation can predict model behavior for previously unseen data distributions. We have demonstrated how existing approaches lack distributional robustness and how adding out-of-distribution samples to the dataset used to learn the interpretation can increase robustness and improve interpretation quality under the scenario of distributional shift. All datasets used in our study are publicly available, and our approach has been implemented in a top digital marketing suite of products.",1
"Language-queried video actor segmentation aims to predict the pixel-level mask of the actor which performs the actions described by a natural language query in the target frames. Existing methods adopt 3D CNNs over the video clip as a general encoder to extract a mixed spatio-temporal feature for the target frame. Though 3D convolutions are amenable to recognizing which actor is performing the queried actions, it also inevitably introduces misaligned spatial information from adjacent frames, which confuses features of the target frame and yields inaccurate segmentation. Therefore, we propose a collaborative spatial-temporal encoder-decoder framework which contains a 3D temporal encoder over the video clip to recognize the queried actions, and a 2D spatial encoder over the target frame to accurately segment the queried actors. In the decoder, a Language-Guided Feature Selection (LGFS) module is proposed to flexibly integrate spatial and temporal features from the two encoders. We also propose a Cross-Modal Adaptive Modulation (CMAM) module to dynamically recombine spatial- and temporal-relevant linguistic features for multimodal feature interaction in each stage of the two encoders. Our method achieves new state-of-the-art performance on two popular benchmarks with less computational overhead than previous approaches.",0
"The objective of video actor segmentation using language queries is to anticipate the pixel-level mask of the actor who executes the actions specified by a natural language query in the target frames. Previous methodologies use 3D CNNs over the video clip to generate a combined spatio-temporal feature for the target frame. However, this approach introduces spatial information from neighboring frames, leading to inaccurate segmentation. To address this issue, we propose a collaborative spatial-temporal encoder-decoder framework that includes a 3D temporal encoder to identify the queried actions and a 2D spatial encoder for precise segmentation of the queried actors. Our decoder incorporates a Language-Guided Feature Selection (LGFS) module that flexibly integrates spatial and temporal features from the two encoders. We also introduce a Cross-Modal Adaptive Modulation (CMAM) module that dynamically recombines spatial- and temporal-relevant linguistic features for multimodal feature interaction in each stage of the two encoders. Our approach outperforms previous methods on two popular benchmarks with less computational overhead.",1
"This paper is based on a machine learning project at the Norwegian University of Science and Technology, fall 2020. The project was initiated with a literature review on the latest developments within time-series forecasting methods in the scientific community over the past five years. The paper summarizes the essential aspects of this research. Furthermore, in this paper, we introduce an LSTM cell's architecture, and explain how different components go together to alter the cell's memory and predict the output. Also, the paper provides the necessary formulas and foundations to calculate a forward iteration through an LSTM. Then, the paper refers to some practical applications and research that emphasize the strength and weaknesses of LSTMs, shown within the time-series domain and the natural language processing (NLP) domain. Finally, alternative statistical methods for time series predictions are highlighted, where the paper outline ARIMA and exponential smoothing. Nevertheless, as LSTMs can be viewed as a complex architecture, the paper assumes that the reader has some knowledge of essential machine learning aspects, such as the multi-layer perceptron, activation functions, overfitting, backpropagation, bias, over- and underfitting, and more.",0
"This paper presents a machine learning project undertaken at the Norwegian University of Science and Technology during fall 2020. The project's objective was to explore the latest developments in time-series forecasting methods by reviewing relevant literature published in the scientific community over the past five years. The paper provides a summary of the essential aspects of this research. It also introduces the architecture of an LSTM cell and explains how its different components work together to modify the cell's memory and predict the output. The paper presents the necessary formulas and foundations for calculating a forward iteration through an LSTM. Additionally, it highlights practical applications and research that demonstrate the strengths and weaknesses of LSTMs in the time-series and natural language processing domains. The paper also outlines alternative statistical methods for time series predictions, such as ARIMA and exponential smoothing. However, due to the complexity of LSTMs, the paper assumes that the reader has some knowledge of essential machine learning aspects, such as the multi-layer perceptron, activation functions, overfitting, backpropagation, bias, over- and underfitting, and more.",1
"Real-world data often exhibit imbalanced distributions, where certain target values have significantly fewer observations. Existing techniques for dealing with imbalanced data focus on targets with categorical indices, i.e., different classes. However, many tasks involve continuous targets, where hard boundaries between classes do not exist. We define Deep Imbalanced Regression (DIR) as learning from such imbalanced data with continuous targets, dealing with potential missing data for certain target values, and generalizing to the entire target range. Motivated by the intrinsic difference between categorical and continuous label space, we propose distribution smoothing for both labels and features, which explicitly acknowledges the effects of nearby targets, and calibrates both label and learned feature distributions. We curate and benchmark large-scale DIR datasets from common real-world tasks in computer vision, natural language processing, and healthcare domains. Extensive experiments verify the superior performance of our strategies. Our work fills the gap in benchmarks and techniques for practical imbalanced regression problems. Code and data are available at https://github.com/YyzHarry/imbalanced-regression.",0
"It is common for real-world data to have imbalanced distributions where certain target values are observed significantly less. Existing methods for addressing imbalanced data focus on categorical targets with different classes. However, many tasks involve continuous targets where there are no clear boundaries between classes. We introduce Deep Imbalanced Regression (DIR) as a method for learning from imbalanced data with continuous targets, accounting for missing data for certain target values, and generalizing across the entire target range. Our approach uses distribution smoothing for both labels and features, which considers the impact of nearby targets and adjusts both label and feature distributions. We create and evaluate large-scale DIR datasets from various domains, including computer vision, natural language processing, and healthcare. Our experiments demonstrate the effectiveness of our strategies, which fill the gap in techniques and benchmarks for imbalanced regression problems. Code and data are available on our GitHub page at https://github.com/YyzHarry/imbalanced-regression.",1
"Spherical data is distributed on the sphere. The data appears in various fields such as meteorology, biology, and natural language processing. However, a method for analysis of spherical data does not develop enough yet. One of the important issues is an estimation of the number of clusters in spherical data. To address the issue, I propose a new method called the Spherical X-means (SX-means) that can estimate the number of clusters on d-dimensional sphere. The SX-means is the model-based method assuming that the data is generated from a mixture of von Mises-Fisher distributions. The present paper explains the proposed method and shows its performance of estimation of the number of clusters.",0
"Data that is distributed on a sphere is known as spherical data and is commonly found in various fields such as meteorology, biology, and natural language processing. Unfortunately, there is currently a lack of developed methods for analyzing this type of data. One significant issue is determining the number of clusters present in the spherical data. To address this problem, a new method called the Spherical X-means (SX-means) has been proposed. The SX-means is a model-based method that assumes the data is generated from a mixture of von Mises-Fisher distributions. This paper will explain the proposed method and showcase its ability to estimate the number of clusters.",1
"Recurrent neural networks (RNNs) have been applied to a broad range of applications, including natural language processing, drug discovery, and video recognition. Their vulnerability to input perturbation is also known. Aligning with a view from software defect detection, this paper aims to develop a coverage guided testing approach to systematically exploit the internal behaviour of RNNs, with the expectation that such testing can detect defects with high possibility. Technically, the long short term memory network (LSTM), a major class of RNNs, is thoroughly studied. A family of three test metrics are designed to quantify not only the values but also the temporal relations (including both step-wise and bounded-length) exhibited when LSTM processing inputs. A genetic algorithm is applied to efficiently generate test cases. The test metrics and test case generation algorithm are implemented into a tool TestRNN, which is then evaluated on a set of LSTM benchmarks. Experiments confirm that TestRNN has advantages over the state-of-art tool DeepStellar and attack-based defect detection methods, owing to its working with finer temporal semantics and the consideration of the naturalness of input perturbation. Furthermore, TestRNN enables meaningful information to be collected and exhibited for users to understand the testing results, which is an important step towards interpretable neural network testing.",0
"Recurrent neural networks (RNNs) have found numerous applications, such as natural language processing, drug discovery, and video recognition. However, RNNs are susceptible to input perturbations. This paper proposes a coverage guided testing approach to systematically explore the internal behavior of RNNs. The focus is on the long short term memory network (LSTM), a major class of RNNs. To quantify both the values and temporal relations exhibited during LSTM processing, a family of three test metrics is designed. A genetic algorithm is used to generate test cases efficiently. The resulting tool, TestRNN, is evaluated on a set of LSTM benchmarks and outperforms existing tools in terms of temporal semantics and natural input perturbation. TestRNN also provides meaningful information for users to understand the testing results, which is an important step towards interpretable neural network testing.",1
"Source code summarization aims to generate natural language summaries from structured code snippets for better understanding code functionalities. However, automatic code summarization is challenging due to the complexity of the source code and the language gap between the source code and natural language summaries. Most previous approaches either rely on retrieval-based (which can take advantage of similar examples seen from the retrieval database, but have low generalization performance) or generation-based methods (which have better generalization performance, but cannot take advantage of similar examples). This paper proposes a novel retrieval-augmented mechanism to combine the benefits of both worlds. Furthermore, to mitigate the limitation of Graph Neural Networks (GNNs) on capturing global graph structure information of source code, we propose a novel attention-based dynamic graph to complement the static graph representation of the source code, and design a hybrid message passing GNN for capturing both the local and global structural information. To evaluate the proposed approach, we release a new challenging benchmark, crawled from diversified large-scale open-source C projects (total 95k+ unique functions in the dataset). Our method achieves the state-of-the-art performance, improving existing methods by 1.42, 2.44 and 1.29 in terms of BLEU-4, ROUGE-L and METEOR.",0
"The goal of source code summarization is to produce natural language summaries from structured code snippets to enhance comprehension of code functionality. However, automatic code summarization presents challenges due to the complexity of the source code and the language differences between the source code and natural language. Previous methods have relied on either retrieval-based or generation-based techniques, each with their own limitations. This study proposes a novel retrieval-augmented mechanism that combines the advantages of both approaches. Additionally, to address limitations in Graph Neural Networks (GNNs) with respect to capturing global graph structure information of source code, the authors propose an attention-based dynamic graph and a hybrid message passing GNN that can capture both local and global structural information. The authors evaluate their approach with a new benchmark dataset consisting of more than 95k unique functions from diverse large-scale open-source C projects. Their method achieves state-of-the-art performance, surpassing existing methods in BLEU-4, ROUGE-L, and METEOR by 1.42, 2.44, and 1.29, respectively.",1
"Graph Neural Networks have revolutionized many machine learning tasks in recent years, ranging from drug discovery, recommendation systems, image classification, social network analysis to natural language understanding. This paper shows their efficacy in modeling relationships between products and making predictions for unseen product networks. By representing products as nodes and their relationships as edges of a graph, we show how an inductive graph neural network approach, named GraphSAGE, can efficiently learn continuous representations for nodes and edges. These representations also capture product feature information such as price, brand, or engineering attributes. They are combined with a classification model for predicting the existence of the relationship between products. Using a case study of the Chinese car market, we find that our method yields double the prediction performance compared to an Exponential Random Graph Model-based method for predicting the co-consideration relationship between cars. While a vanilla GraphSAGE requires a partial network to make predictions, we introduce an `adjacency prediction model' to circumvent this limitation. This enables us to predict product relationships when no neighborhood information is known. Finally, we demonstrate how a permutation-based interpretability analysis can provide insights on how design attributes impact the predictions of relationships between products. This work provides a systematic method to predict the relationships between products in many different markets.",0
"In recent years, Graph Neural Networks have transformed various machine learning tasks including drug discovery, recommendation systems, image classification, social network analysis, and natural language understanding. This research showcases the potential of Graph Neural Networks in modeling relationships among products and making predictions for unidentified product networks. By presenting products as nodes and their relationships as edges of a graph, the study illustrates how GraphSAGE, an inductive graph neural network technique, can effectively learn continuous representations for nodes and edges, which also includes product feature information such as price, brand, or engineering attributes. These representations are then integrated with a classification model to forecast the existence of product relationships. The study conducted on the Chinese car market reveals that the proposed method doubles the prediction performance compared to an Exponential Random Graph Model-based method for predicting co-consideration relationships between cars. To overcome the limitation of a partial network, an ""adjacency prediction model"" is also introduced to predict product relationships when no neighborhood information is available. Finally, a permutation-based interpretability analysis is utilized to understand how design attributes impact the predictions of relationships between products. This research offers a structured approach to predict product relationships in various markets.",1
"Customers of machine learning systems demand accountability from the companies employing these algorithms for various prediction tasks. Accountability requires understanding of system limit and condition of erroneous predictions, as customers are often interested in understanding the incorrect predictions, and model developers are absorbed in finding methods that can be used to get incremental improvements to an existing system. Therefore, we propose an accountable error characterization method, AEC, to understand when and where errors occur within the existing black-box models. AEC, as constructed with human-understandable linguistic features, allows the model developers to automatically identify the main sources of errors for a given classification system. It can also be used to sample for the set of most informative input points for a next round of training. We perform error detection for a sentiment analysis task using AEC as a case study. Our results on the sample sentiment task show that AEC is able to characterize erroneous predictions into human understandable categories and also achieves promising results on selecting erroneous samples when compared with the uncertainty-based sampling.",0
"Companies using machine learning systems face demands from their customers for accountability regarding the accuracy of the algorithms used to make predictions. To satisfy these demands, it is necessary to understand the limitations of the system and the conditions under which incorrect predictions occur. While customers want to know why mistakes were made, model developers want to improve the system. Our solution is the Accountable Error Characterization (AEC) method, which uses understandable linguistic features to identify the main sources of errors in an existing black-box model. This method can also be used to identify the most informative input points for the next round of training. We applied AEC to a sentiment analysis task and found that it was effective in categorizing erroneous predictions and selecting samples for training. Our results were promising when compared to uncertainty-based sampling.",1
"Keeping in mind the necessity of intelligent system in educational sector, this paper proposes a text analysis based automated approach for automatic evaluation of the descriptive answers in an examination. In particular, the research focuses on the use of intelligent concepts of Natural Language Processing and Data Mining for computer aided examination evaluation system. The paper present an architecture for fair evaluation of answer sheet. In this architecture, the examiner creates a sample answer sheet for given sets of question. By using the concept of text summarization, text semantics and keywords summarization, the final score for each answer is calculated. The text similarity model is based on Siamese Manhattan LSTM (MaLSTM). The results of this research were compared to manually graded assignments and other existing system. This approach was found to be very efficient in order to be implemented in an institution or in an university.",0
"This paper suggests an automated approach for evaluating descriptive answers in exams using text analysis. The focus is on incorporating Natural Language Processing and Data Mining concepts to create a computer aided evaluation system. The proposed architecture allows examiners to create a sample answer sheet for a set of questions, and uses text summarization, semantics, and keywords to calculate the final score for each answer. The text similarity model, based on Siamese Manhattan LSTM, is used to compare the results with manually graded assignments and existing systems. The study demonstrates the efficiency of this approach, making it suitable for implementation in institutions or universities.",1
"Recent advancement of research in biometrics, computer vision, and natural language processing has discovered opportunities for person retrieval from surveillance videos using textual query. The prime objective of a surveillance system is to locate a person using a description, e.g., a short woman with a pink t-shirt and white skirt carrying a black purse. She has brown hair. Such a description contains attributes like gender, height, type of clothing, colour of clothing, hair colour, and accessories. Such attributes are formally known as soft biometrics. They help bridge the semantic gap between a human description and a machine as a textual query contains the person's soft biometric attributes. It is also not feasible to manually search through huge volumes of surveillance footage to retrieve a specific person. Hence, automatic person retrieval using vision and language-based algorithms is becoming popular. In comparison to other state-of-the-art reviews, the contribution of the paper is as follows: 1. Recommends most discriminative soft biometrics for specifiic challenging conditions. 2. Integrates benchmark datasets and retrieval methods for objective performance evaluation. 3. A complete snapshot of techniques based on features, classifiers, number of soft biometric attributes, type of the deep neural networks, and performance measures. 4. The comprehensive coverage of person retrieval from handcrafted features based methods to end-to-end approaches based on natural language description.",0
"Advancements in biometrics, computer vision, and natural language processing have revealed possibilities for identifying individuals in surveillance videos using textual queries. The primary objective of a surveillance system is to locate individuals based on their characteristics, such as gender, height, clothing type and color, hair color, and accessories, which are collectively known as soft biometrics. This helps to bridge the gap between human descriptions and machine queries. The manual search of vast amounts of surveillance footage to locate a specific individual is impractical, so automatic person retrieval using vision and language-based algorithms is gaining popularity. This paper's contribution includes recommending the most discriminative soft biometrics for challenging conditions, integrating benchmark datasets and retrieval methods for objective performance evaluation, offering a complete overview of techniques based on various features, classifiers, number of soft biometric attributes, type of deep neural networks, and performance measures, and providing comprehensive coverage of person retrieval methods from handcrafted features-based to end-to-end approaches based on natural language descriptions, compared to other state-of-the-art reviews.",1
"Image captioning is one of the most challenging tasks in AI, which aims to automatically generate textual sentences for an image. Recent methods for image captioning follow encoder-decoder framework that transforms the sequence of salient regions in an image into natural language descriptions. However, these models usually lack the comprehensive understanding of the contextual interactions reflected on various visual relationships between objects. In this paper, we explore explicit and implicit visual relationships to enrich region-level representations for image captioning. Explicitly, we build semantic graph over object pairs and exploit gated graph convolutional networks (Gated GCN) to selectively aggregate local neighbors' information. Implicitly, we draw global interactions among the detected objects through region-based bidirectional encoder representations from transformers (Region BERT) without extra relational annotations. To evaluate the effectiveness and superiority of our proposed method, we conduct extensive experiments on Microsoft COCO benchmark and achieve remarkable improvements compared with strong baselines.",0
"Generating textual descriptions for images, known as image captioning, is a complex task in the field of AI. Existing methods for this task use an encoder-decoder framework to convert salient regions in an image into natural language descriptions. However, these models often lack a comprehensive understanding of contextual interactions between objects. Therefore, in this study, we have explored explicit and implicit visual relationships to enhance region-level representations for image captioning. Our approach involves building a semantic graph over object pairs and utilizing gated graph convolutional networks to selectively aggregate local neighbors' information. We also draw global interactions among detected objects through region-based bidirectional encoder representations from transformers, without requiring extra relational annotations. We have evaluated the effectiveness of our method on the Microsoft COCO benchmark and achieved remarkable improvements compared to strong baselines.",1
"In most real-world applications, it is seldom the case that a given observable evolves independently of its environment. In social networks, users' behavior results from the people they interact with, news in their feed, or trending topics. In natural language, the meaning of phrases emerges from the combination of words. In general medicine, a diagnosis is established on the basis of the interaction of symptoms. Here, we propose a new model, the Interactive Mixed Membership Stochastic Block Model (IMMSBM), which investigates the role of interactions between entities (hashtags, words, memes, etc.) and quantifies their importance within the aforementioned corpora. We find that interactions play an important role in those corpora. In inference tasks, taking them into account leads to average relative changes with respect to non-interactive models of up to 150\% in the probability of an outcome. Furthermore, their role greatly improves the predictive power of the model. Our findings suggest that neglecting interactions when modeling real-world phenomena might lead to incorrect conclusions being drawn.",0
"Observable phenomena in real-world applications rarely evolve independently of their environment. For example, users' behavior in social networks is influenced by their interactions with other users, news in their feed, and popular trends. Similarly, the meaning of phrases in natural language is derived from the combination of words, and in general medicine, a diagnosis is based on the interaction of symptoms. Our proposed model, the Interactive Mixed Membership Stochastic Block Model (IMMSBM), examines the impact of interactions between entities such as hashtags, words, and memes, and quantifies their significance in these contexts. Our research indicates that interactions play a crucial role in these scenarios, and considering them in inference tasks can result in average relative changes of up to 150\% in the probability of an outcome, and significantly enhance the predictive power of the model. Therefore, ignoring interactions when modeling real-world phenomena may lead to erroneous conclusions.",1
"In recent years, sentiment analysis and emotion classification are two of the most abundantly used techniques in the field of Natural Language Processing (NLP). Although sentiment analysis and emotion classification are used commonly in applications such as analyzing customer reviews, the popularity of candidates contesting in elections, and comments about various sporting events; however, in this study, we have examined their application for epidemic outbreak detection. Early outbreak detection is the key to deal with epidemics effectively, however, the traditional ways of outbreak detection are time-consuming which inhibits prompt response from the respective departments. Social media platforms such as Twitter, Facebook, Instagram, etc. allow the users to express their thoughts related to different aspects of life, and therefore, serve as a substantial source of information in such situations. The proposed study exploits the bilingual (Urdu and English) data from Twitter and NEWS websites related to the dengue epidemic in Pakistan, and sentiment analysis and emotion classification are performed to acquire deep insights from the data set for gaining a fair idea related to an epidemic outbreak. Machine learning and deep learning algorithms have been used to train and implement the models for the execution of both tasks. The comparative performance of each model has been evaluated using accuracy, precision, recall, and f1-measure.",0
"The field of Natural Language Processing (NLP) has seen an increase in the use of sentiment analysis and emotion classification in recent years. While these techniques are commonly used for analyzing customer reviews, candidate popularity during elections, and comments about sporting events, this study focuses on their application for early epidemic outbreak detection. Traditional outbreak detection methods are time-consuming and hinder prompt response from relevant departments. Social media platforms like Twitter, Facebook, and Instagram provide a significant source of information as users share their thoughts on various aspects of life. This study uses bilingual (Urdu and English) data from Twitter and NEWS websites related to the dengue epidemic in Pakistan. Machine learning and deep learning algorithms are used to train and implement models for sentiment analysis and emotion classification. The models are evaluated based on their accuracy, precision, recall, and f1-measure performances.",1
"Visual question answering (VQA) is a task that combines both the techniques of computer vision and natural language processing. It requires models to answer a text-based question according to the information contained in a visual. In recent years, the research field of VQA has been expanded. Research that focuses on the VQA, examining the reasoning ability and VQA on scientific diagrams, has also been explored more. Meanwhile, more multimodal feature fusion mechanisms have been proposed. This paper will review and analyze existing datasets, metrics, and models proposed for the VQA task.",0
"The task of visual question answering (VQA) involves merging computer vision techniques with natural language processing to enable models to respond to text-based queries based on visual content. Over the years, the VQA research domain has grown, with more emphasis on examining reasoning abilities and VQA on scientific diagrams, as well as proposing multimodal feature fusion mechanisms. This article aims to evaluate and scrutinize the existing datasets, metrics, and models put forth for the VQA task.",1
"There has been recently a growing interest in studying adversarial examples on natural language models in the black-box setting. These methods attack natural language classifiers by perturbing certain important words until the classifier label is changed. In order to find these important words, these methods rank all words by importance by querying the target model word by word for each input sentence, resulting in high query inefficiency. A new interesting approach was introduced that addresses this problem through interpretable learning to learn the word ranking instead of previous expensive search. The main advantage of using this approach is that it achieves comparable attack rates to the state-of-the-art methods, yet faster and with fewer queries, where fewer queries are desirable to avoid suspicion towards the attacking agent. Nonetheless, this approach sacrificed the useful information that could be leveraged from the target classifier for that sake of query efficiency. In this paper we study the effect of leveraging the target model outputs and data on both attack rates and average number of queries, and we show that both can be improved, with a limited overhead of additional queries.",0
"Recently, there has been a surge of interest in studying adversarial examples on natural language models in the black-box setting. These methods involve perturbing specific crucial words in order to alter the classifier label. However, the traditional approach of ranking the importance of words by querying the target model for each input sentence is highly query-inefficient. A novel approach using interpretable learning has been introduced to address this, which learns the word ranking instead of relying on an expensive search. This approach achieves comparable attack rates to state-of-the-art methods with fewer queries, thereby avoiding suspicion towards the attacking agent. However, this approach sacrifices the useful information that can be obtained from the target classifier for the sake of query efficiency. In this paper, we examine the impact of leveraging the target model outputs and data on both attack rates and average number of queries, and demonstrate that both can be improved with minimal additional queries.",1
"Person search has drawn increasing attention due to its real-world applications and research significance. Person search aims to find a probe person in a gallery of scene images with a wide range of applications, such as criminals search, multicamera tracking, missing person search, etc. Early person search works focused on image-based person search, which uses person image as the search query. Text-based person search is another major person search category that uses free-form natural language as the search query. Person search is challenging, and corresponding solutions are diverse and complex. Therefore, systematic surveys on this topic are essential. This paper surveyed the recent works on image-based and text-based person search from the perspective of challenges and solutions. Specifically, we provide a brief analysis of highly influential person search methods considering the three significant challenges: the discriminative person features, the query-person gap, and the detection-identification inconsistency. We summarise and compare evaluation results. Finally, we discuss open issues and some promising future research directions.",0
"The search for individuals has gained attention for its practical applications and research importance. The objective of person search is to locate a specific person in a collection of scene images, with various uses such as tracking criminals, multi-camera monitoring, and locating missing individuals. Earlier studies on person search focused on image-based search, where the person's image is used as the search query. Another major category of person search is text-based, which employs natural language as the search query. Person search is a challenging task, and there are diverse and complex solutions to this problem. As a result, comprehensive surveys on this subject are necessary. This article surveys the recent developments in image-based and text-based person search and analyzes influential person search techniques in light of three significant challenges: the discriminative person features, the query-person gap, and the detection-identification inconsistency. We also summarize and compare evaluation results and discuss open issues and future research directions with promising potential.",1
"As one of the most fundamental tasks in graph theory, subgraph matching is a crucial task in many fields, ranging from information retrieval, computer vision, biology, chemistry and natural language processing. Yet subgraph matching problem remains to be an NP-complete problem. This study proposes an end-to-end learning-based approximate method for subgraph matching task, called subgraph matching network (Sub-GMN). The proposed Sub-GMN firstly uses graph representation learning to map nodes to node-level embedding. It then combines metric learning and attention mechanisms to model the relationship between matched nodes in the data graph and query graph. To test the performance of the proposed method, we applied our method on two databases. We used two existing methods, GNN and FGNN as baseline for comparison. Our experiment shows that, on dataset 1, on average the accuracy of Sub-GMN are 12.21\% and 3.2\% higher than that of GNN and FGNN respectively. On average running time Sub-GMN runs 20-40 times faster than FGNN. In addition, the average F1-score of Sub-GMN on all experiments with dataset 2 reached 0.95, which demonstrates that Sub-GMN outputs more correct node-to-node matches.   Comparing with the previous GNNs-based methods for subgraph matching task, our proposed Sub-GMN allows varying query and data graphes in the test/application stage, while most previous GNNs-based methods can only find a matched subgraph in the data graph during the test/application for the same query graph used in the training stage. Another advantage of our proposed Sub-GMN is that it can output a list of node-to-node matches, while most existing end-to-end GNNs based methods cannot provide the matched node pairs.",0
"The task of subgraph matching is essential in graph theory and has numerous applications in fields such as information retrieval, computer vision, biology, chemistry, and natural language processing. Despite its importance, the subgraph matching problem is still NP-complete. In this study, we propose a learning-based approximate method for subgraph matching called Sub-GMN. The Sub-GMN utilizes graph representation learning to map nodes to node-level embedding and employs metric learning and attention mechanisms to model the relationship between matched nodes in the data graph and query graph. We evaluated the performance of Sub-GMN on two databases and compared it with two existing methods, GNN and FGNN. Our experiments show that on average, Sub-GMN outperforms both GNN and FGNN in terms of accuracy and runs 20-40 times faster than FGNN. Additionally, Sub-GMN outputs a list of node-to-node matches, which is an advantage over existing end-to-end GNNs-based methods that cannot provide matched node pairs. Another advantage is that Sub-GMN allows for varying query and data graphs in the test/application stage, while previous GNNs-based methods can only find a matched subgraph in the data graph for the same query graph used in the training stage. The average F1-score of Sub-GMN on all experiments with dataset 2 reached 0.95, demonstrating that Sub-GMN outputs more correct node-to-node matches.",1
"Remarkable performance from Transformer networks in Natural Language Processing promote the development of these models in dealing with computer vision tasks such as image recognition and segmentation. In this paper, we introduce a novel framework, called Multi-level Multi-scale Point Transformer (MLMSPT) that works directly on the irregular point clouds for representation learning. Specifically, a point pyramid transformer is investigated to model features with diverse resolutions or scales we defined, followed by a multi-level transformer module to aggregate contextual information from different levels of each scale and enhance their interactions. While a multi-scale transformer module is designed to capture the dependencies among representations across different scales. Extensive evaluation on public benchmark datasets demonstrate the effectiveness and the competitive performance of our methods on 3D shape classification, part segmentation and semantic segmentation tasks.",0
"The impressive performance of Transformer networks in Natural Language Processing has led to their application in computer vision tasks like image recognition and segmentation. In this paper, we present a new approach called Multi-level Multi-scale Point Transformer (MLMSPT) that operates directly on irregular point clouds for representation learning. Our framework includes a point pyramid transformer to model features with varying resolutions or scales, a multi-level transformer module to gather contextual information from different levels of each scale and enhance their interactions, and a multi-scale transformer module to capture dependencies among representations across different scales. Our methods have been extensively evaluated on public benchmark datasets, and the results demonstrate their effectiveness and competitive performance in 3D shape classification, part segmentation, and semantic segmentation tasks.",1
"Social Media Platforms (SMPs) like Facebook, Twitter, Instagram etc. have large user base all around the world that generates huge amount of data every second. This includes a lot of posts by fake and spam users, typically used by many organisations around the globe to have competitive edge over others. In this work, we aim at detecting such user accounts in Twitter using a novel approach. We show how to distinguish between Genuine and Spam accounts in Twitter using a combination of Graph Representation Learning and Natural Language Processing techniques.",0
"Social Media Platforms (SMPs) such as Facebook, Twitter, and Instagram are widely used across the globe, resulting in a massive amount of data generation every second. However, the data includes numerous posts by fake and spam users used by various organizations worldwide to gain an advantage over their competitors. This study focuses on identifying such user accounts in Twitter by implementing a unique approach. We demonstrate the ability to differentiate between authentic and spam accounts on Twitter by utilizing a combination of Graph Representation Learning and Natural Language Processing techniques.",1
"Self-attention mechanism recently achieves impressive advancement in Natural Language Processing (NLP) and Image Processing domains. And its permutation invariance property makes it ideally suitable for point cloud processing. Inspired by this remarkable success, we propose an end-to-end architecture, dubbed Cross-Level Cross-Scale Cross-Attention Network (CLCSCANet), for point cloud representation learning. First, a point-wise feature pyramid module is introduced to hierarchically extract features from different scales or resolutions. Then a cross-level cross-attention is designed to model long-range inter-level and intra-level dependencies. Finally, we develop a cross-scale cross-attention module to capture interactions between-and-within scales for representation enhancement. Compared with state-of-the-art approaches, our network can obtain competitive performance on challenging 3D object classification, point cloud segmentation tasks via comprehensive experimental evaluation.",0
"The self-attention mechanism has made significant progress in the fields of Natural Language Processing (NLP) and Image Processing, and its permutation invariance feature makes it beneficial for point cloud processing. We were inspired by this success and developed an end-to-end architecture, called the Cross-Level Cross-Scale Cross-Attention Network (CLCSCANet), for point cloud representation learning. Our approach includes a point-wise feature pyramid module to extract features from multiple scales, a cross-level cross-attention module to model long-range dependencies, and a cross-scale cross-attention module to enhance representation. Our network achieved competitive performance on challenging 3D object classification and point cloud segmentation tasks compared to state-of-the-art methods, as evaluated through comprehensive experiments.",1
"Following the tremendous success of transformer in natural language processing and image understanding tasks, in this paper, we present a novel point cloud representation learning architecture, named Dual Transformer Network (DTNet), which mainly consists of Dual Point Cloud Transformer (DPCT) module. Specifically, by aggregating the well-designed point-wise and channel-wise multi-head self-attention models simultaneously, DPCT module can capture much richer contextual dependencies semantically from the perspective of position and channel. With the DPCT module as a fundamental component, we construct the DTNet for performing point cloud analysis in an end-to-end manner. Extensive quantitative and qualitative experiments on publicly available benchmarks demonstrate the effectiveness of our proposed transformer framework for the tasks of 3D point cloud classification and segmentation, achieving highly competitive performance in comparison with the state-of-the-art approaches.",0
"In this paper, we introduce the Dual Transformer Network (DTNet) as a point cloud representation learning architecture. This is based on the success of transformer models in natural language processing and image understanding. The DTNet is primarily composed of the Dual Point Cloud Transformer (DPCT) module, which combines point-wise and channel-wise multi-head self-attention models to capture richer contextual dependencies from both position and channel perspectives. We use the DPCT module to construct an end-to-end DTNet for point cloud analysis. Our proposed transformer framework achieves highly competitive performance in 3D point cloud classification and segmentation tasks, as demonstrated by extensive quantitative and qualitative experiments on publicly available benchmarks.",1
"Deep learning has yielded extraordinary results in vision and natural language processing, but this achievement comes at a cost. Most deep learning models require enormous resources during training, both in terms of computation and in human labeling effort. In this paper, we show that one can achieve similar accuracy to traditional deep-learning models, while using less training data. Much of the previous work in this area relies on using uncertainty or some form of diversity to select subsets of a larger training set. Submodularity, a discrete analogue of convexity, has been exploited to model diversity in various settings including data subset selection. In contrast to prior methods, we propose a novel diversity driven objective function, and balancing constraints on class labels and decision boundaries using matroids. This allows us to use efficient greedy algorithms with approximation guarantees for subset selection. We outperform baselines on standard image classification datasets such as CIFAR-10, CIFAR-100, and ImageNet. In addition, we also show that the proposed balancing constraints can play a key role in boosting the performance in long-tailed datasets such as CIFAR-100-LT.",0
"Although deep learning has delivered impressive outcomes in vision and natural language processing, it comes with a high price tag. The majority of deep learning models necessitate copious resources during training, in terms of both computation and labeling effort. This article introduces a technique that achieves similar accuracy to conventional deep-learning models while using less training data. Previous research in this field has relied on utilizing uncertainty or some form of diversity to choose subsets of a larger training set. Submodularity, which is a discrete analog of convexity, has been exploited to model diversity in various settings, including data subset selection. Unlike previous methods, this research proposes a new diversity-driven objective function and balancing constraints on class labels and decision boundaries using matroids. This approach enables the use of efficient greedy algorithms with approximation guarantees for subset selection. On standard image classification datasets such as CIFAR-10, CIFAR-100, and ImageNet, this technique outperforms baseline methods. Furthermore, the proposed balancing constraints play a crucial role in enhancing performance in long-tailed datasets, such as CIFAR-100-LT.",1
"In many real-world scenarios where extrinsic rewards to the agent are extremely sparse, curiosity has emerged as a useful concept providing intrinsic rewards that enable the agent to explore its environment and acquire information to achieve its goals. Despite their strong performance on many sparse-reward tasks, existing curiosity approaches rely on an overly holistic view of state transitions, and do not allow for a structured understanding of specific aspects of the environment. In this paper, we formulate curiosity based on grounded question answering by encouraging the agent to ask questions about the environment and be curious when the answers to these questions change. We show that natural language questions encourage the agent to uncover specific knowledge about their environment such as the physical properties of objects as well as their spatial relationships with other objects, which serve as valuable curiosity rewards to solve sparse-reward tasks more efficiently.",0
"Many scenarios in the real world involve sparse extrinsic rewards for agents. In such cases, curiosity has proven to be a useful concept as it provides intrinsic rewards that allow the agent to explore its environment and gather information to achieve its objectives. Although existing curiosity approaches have performed well in many tasks with sparse rewards, they take an overly holistic view of state transitions and lack a structured understanding of specific aspects of the environment. This paper proposes a curiosity framework based on grounded question answering, which encourages the agent to ask questions about its environment and remain curious when the answers to these questions change. Our approach demonstrates that natural language questions encourage the agent to discover specific knowledge about its environment, including the physical properties of objects and their spatial relationships with other objects. These insights serve as valuable curiosity rewards that enable the agent to solve sparse-reward tasks more efficiently.",1
"Natural language-based vehicle retrieval is a task to find a target vehicle within a given image based on a natural language description as a query. This technology can be applied to various areas including police searching for a suspect vehicle. However, it is challenging due to the ambiguity of language descriptions and the difficulty of processing multi-modal data. To tackle this problem, we propose a deep neural network called SBNet that performs natural language-based segmentation for vehicle retrieval. We also propose two task-specific modules to improve performance: a substitution module that helps features from different domains to be embedded in the same space and a future prediction module that learns temporal information. SBnet has been trained using the CityFlow-NL dataset that contains 2,498 tracks of vehicles with three unique natural language descriptions each and tested 530 unique vehicle tracks and their corresponding query sets. SBNet achieved a significant improvement over the baseline in the natural language-based vehicle tracking track in the AI City Challenge 2021.",0
"The task of natural language-based vehicle retrieval involves locating a specific vehicle within an image using a natural language description as a search query. This technology has numerous applications, such as aiding the police in finding a suspect vehicle. However, it is a challenging task due to the imprecise nature of language descriptions and the difficulty of processing multi-modal data. To address these issues, we propose SBNet, a deep neural network that performs natural language-based segmentation for vehicle retrieval. Additionally, we introduce two task-specific modules to enhance performance: a substitution module that facilitates embedding features from various domains into the same space, and a future prediction module that learns temporal information. SBNet was trained using the CityFlow-NL dataset, which comprises 2,498 vehicle tracks with three distinct natural language descriptions each. We tested the network on 530 unique vehicle tracks and their corresponding query sets. SBNet demonstrated a significant improvement over the baseline in the natural language-based vehicle tracking track in the AI City Challenge 2021.",1
"Chest radiography is the most common radiographic examination performed in daily clinical practice for the detection of various heart and lung abnormalities. The large amount of data to be read and reported, with more than 100 studies per day for a single radiologist, poses a challenge in consistently maintaining high interpretation accuracy. The introduction of large-scale public datasets has led to a series of novel systems for automated abnormality classification. However, the labels of these datasets were obtained using natural language processed medical reports, yielding a large degree of label noise that can impact the performance. In this study, we propose novel training strategies that handle label noise from such suboptimal data. Prior label probabilities were measured on a subset of training data re-read by 4 board-certified radiologists and were used during training to increase the robustness of the training model to the label noise. Furthermore, we exploit the high comorbidity of abnormalities observed in chest radiography and incorporate this information to further reduce the impact of label noise. Additionally, anatomical knowledge is incorporated by training the system to predict lung and heart segmentation, as well as spatial knowledge labels. To deal with multiple datasets and images derived from various scanners that apply different post-processing techniques, we introduce a novel image normalization strategy. Experiments were performed on an extensive collection of 297,541 chest radiographs from 86,876 patients, leading to a state-of-the-art performance level for 17 abnormalities from 2 datasets. With an average AUC score of 0.880 across all abnormalities, our proposed training strategies can be used to significantly improve performance scores.",0
"In everyday clinical practice, chest radiography is commonly used to detect heart and lung abnormalities. However, interpreting the large amount of data generated from over 100 studies per day per radiologist can be challenging, and maintaining high interpretation accuracy is important. While automated abnormality classification systems using large-scale public datasets have been introduced, the labels obtained from natural language processed medical reports can contain a significant amount of label noise that may affect performance. To address this issue, our study proposes novel training strategies that incorporate prior label probabilities from a subset of training data read by four board-certified radiologists. We also incorporate anatomical and spatial knowledge by training the system to predict lung and heart segmentation and spatial knowledge labels. In addition, we introduce a novel image normalization strategy to deal with multiple datasets and images derived from various scanners applying different post-processing techniques. Our experiments on a large collection of chest radiographs show that our proposed training strategies can significantly improve performance scores, achieving state-of-the-art levels for 17 abnormalities from two datasets with an average AUC score of 0.880 across all abnormalities.",1
"Referring expression comprehension aims to localize objects identified by natural language descriptions. This is a challenging task as it requires understanding of both visual and language domains. One nature is that each object can be described by synonymous sentences with paraphrases, and such varieties in languages have critical impact on learning a comprehension model. While prior work usually treats each sentence and attends it to an object separately, we focus on learning a referring expression comprehension model that considers the property in synonymous sentences. To this end, we develop an end-to-end trainable framework to learn contrastive features on the image and object instance levels, where features extracted from synonymous sentences to describe the same object should be closer to each other after mapping to the visual domain. We conduct extensive experiments to evaluate the proposed algorithm on several benchmark datasets, and demonstrate that our method performs favorably against the state-of-the-art approaches. Furthermore, since the varieties in expressions become larger across datasets when they describe objects in different ways, we present the cross-dataset and transfer learning settings to validate the ability of our learned transferable features.",0
"The comprehension of referring expressions is a difficult task that involves understanding both language and visual domains in order to localize objects described by natural language. One challenge is the fact that multiple sentences can describe the same object using different words, which has a significant impact on comprehension models. Instead of treating each sentence separately, our approach focuses on learning a model that takes into account synonymous sentences. We have developed a framework that can be trained end-to-end to extract contrastive features at both the image and object instance levels. Using this framework, we aim to ensure that synonymous sentences describing the same object have similar features after being mapped to the visual domain. We have conducted extensive experiments on several benchmark datasets and have shown that our approach performs better than current state-of-the-art methods. Additionally, we have explored cross-dataset and transfer learning settings to demonstrate the transferability of our learned features, particularly in cases where objects are described in different ways.",1
"Text-video retrieval is a challenging task that aims to search relevant video contents based on natural language descriptions. The key to this problem is to measure text-video similarities in a joint embedding space. However, most existing methods only consider the global cross-modal similarity and overlook the local details. Some works incorporate the local comparisons through cross-modal local matching and reasoning. These complex operations introduce tremendous computation. In this paper, we design an efficient global-local alignment method. The multi-modal video sequences and text features are adaptively aggregated with a set of shared semantic centers. The local cross-modal similarities are computed between the video feature and text feature within the same center. This design enables the meticulous local comparison and reduces the computational cost of the interaction between each text-video pair. Moreover, a global alignment method is proposed to provide a global cross-modal measurement that is complementary to the local perspective. The global aggregated visual features also provide additional supervision, which is indispensable to the optimization of the learnable semantic centers. We achieve consistent improvements on three standard text-video retrieval benchmarks and outperform the state-of-the-art by a clear margin.",0
"Searching for relevant video content based on natural language descriptions is a difficult task known as text-video retrieval. The key challenge is to measure text-video similarities in a joint embedding space. However, existing methods tend to overlook local details and focus solely on global cross-modal similarity. Some works have tried to incorporate local comparisons through cross-modal local matching and reasoning, but this introduces a significant computational burden. In this paper, we address these issues by proposing an efficient global-local alignment method. Our approach adaptively aggregates multi-modal video sequences and text features with shared semantic centers. Local cross-modal similarities are computed between the video and text features within the same center. This design enables meticulous local comparisons while reducing the computational cost of pairing each text-video. Additionally, we propose a global alignment method that provides a complementary perspective to the local approach. The global aggregated visual features provide valuable supervision, which is essential to optimizing the learnable semantic centers. Our approach achieves consistent improvements on three standard text-video retrieval benchmarks and outperforms the state-of-the-art by a significant margin.",1
"Motivated by the success of Transformers in natural language processing (NLP) tasks, there emerge some attempts (e.g., ViT and DeiT) to apply Transformers to the vision domain. However, pure Transformer architectures often require a large amount of training data or extra supervision to obtain comparable performance with convolutional neural networks (CNNs). To overcome these limitations, we analyze the potential drawbacks when directly borrowing Transformer architectures from NLP. Then we propose a new \textbf{Convolution-enhanced image Transformer (CeiT)} which combines the advantages of CNNs in extracting low-level features, strengthening locality, and the advantages of Transformers in establishing long-range dependencies. Three modifications are made to the original Transformer: \textbf{1)} instead of the straightforward tokenization from raw input images, we design an \textbf{Image-to-Tokens (I2T)} module that extracts patches from generated low-level features; \textbf{2)} the feed-froward network in each encoder block is replaced with a \textbf{Locally-enhanced Feed-Forward (LeFF)} layer that promotes the correlation among neighboring tokens in the spatial dimension; \textbf{3)} a \textbf{Layer-wise Class token Attention (LCA)} is attached at the top of the Transformer that utilizes the multi-level representations.   Experimental results on ImageNet and seven downstream tasks show the effectiveness and generalization ability of CeiT compared with previous Transformers and state-of-the-art CNNs, without requiring a large amount of training data and extra CNN teachers. Besides, CeiT models also demonstrate better convergence with $3\times$ fewer training iterations, which can reduce the training cost significantly\footnote{Code and models will be released upon acceptance.}.",0
"After Transformers achieved success in natural language processing (NLP) tasks, there have been attempts to apply them to the vision domain, such as ViT and DeiT. However, pure Transformer architectures often need a lot of training data or additional supervision to match the performance of convolutional neural networks (CNNs). To address these limitations, we analyze the potential drawbacks of directly borrowing Transformers from NLP, and propose a new model called Convolution-enhanced image Transformer (CeiT). CeiT combines the benefits of CNNs in extracting low-level features and strengthening locality, and the advantages of Transformers in establishing long-range dependencies. We make three modifications to the original Transformer: 1) we design an Image-to-Tokens (I2T) module that extracts patches from generated low-level features instead of using a straightforward tokenization from raw input images, 2) we replace the feed-forward network in each encoder block with a Locally-enhanced Feed-Forward (LeFF) layer that promotes correlation among neighboring tokens in the spatial dimension, and 3) we attach a Layer-wise Class token Attention (LCA) to the top of the Transformer, which utilizes multi-level representations. Experimental results on ImageNet and seven downstream tasks demonstrate the effectiveness and generalization ability of CeiT compared to previous Transformers and state-of-the-art CNNs, without requiring a large amount of training data and additional CNN teachers. Furthermore, CeiT models show better convergence with only one-third of the training iterations, which significantly reduces training costs. (Code and models will be released upon acceptance.)",1
"Traditional computer vision models are trained to predict a fixed set of predefined categories. Recently, natural language has been shown to be a broader and richer source of supervision that provides finer descriptions to visual concepts than supervised ""gold"" labels. Previous works, such as CLIP, use a simple pretraining task of predicting the pairings between images and text captions. CLIP, however, is data hungry and requires more than 400M image text pairs for training. We propose a data-efficient contrastive distillation method that uses soft labels to learn from noisy image-text pairs. Our model transfers knowledge from pretrained image and sentence encoders and achieves strong performance with only 3M image text pairs, 133x smaller than CLIP. Our method exceeds the previous SoTA of general zero-shot learning on ImageNet 21k+1k by 73% relatively with a ResNet50 image encoder and DeCLUTR text encoder. We also beat CLIP by 10.5% relatively on zero-shot evaluation on Google Open Images (19,958 classes).",0
"Conventional computer vision models are designed to anticipate a fixed set of predetermined categories. However, natural language has recently been found to provide more comprehensive and detailed descriptions for visual concepts than supervised ""gold"" labels. Prior research, such as CLIP, employs a straightforward pretraining task that involves predicting image-text caption pairings. Nonetheless, CLIP is data-intensive and requires over 400M image-text pairs for training. To tackle this issue, we present a data-efficient contrastive distillation approach that utilizes soft labels to learn from noisy image-text pairs. Our model leverages knowledge from pretrained image and sentence encoders, and it achieves impressive performance with only 3M image-text pairs, which is 133 times smaller than CLIP. Our method surpasses the previous state-of-the-art in general zero-shot learning on ImageNet 21k+1k by 73% relatively, using a ResNet50 image encoder and DeCLUTR text encoder. Additionally, we outperform CLIP by 10.5% relatively on zero-shot evaluation on Google Open Images (19,958 classes).",1
"Recent advances in using retrieval components over external knowledge sources have shown impressive results for a variety of downstream tasks in natural language processing. Here, we explore the use of unstructured external knowledge sources of images and their corresponding captions for improving visual question answering (VQA). First, we train a novel alignment model for embedding images and captions in the same space, which achieves substantial improvement in performance on image-caption retrieval w.r.t. similar methods. Second, we show that retrieval-augmented multi-modal transformers using the trained alignment model improve results on VQA over strong baselines. We further conduct extensive experiments to establish the promise of this approach, and examine novel applications for inference time such as hot-swapping indices.",0
"The use of retrieval components from external knowledge sources has yielded notable advancements in natural language processing for a range of downstream tasks. In this study, we investigate the potential of unstructured external knowledge sources of images and their corresponding captions in enhancing visual question answering (VQA). Firstly, we develop a new alignment model to embed images and captions in the same space, which leads to significant improvements in image-caption retrieval compared to similar techniques. Secondly, we demonstrate that retrieval-augmented multi-modal transformers, employing the trained alignment model, enhance VQA results beyond strong baselines. Additionally, we perform extensive experiments to establish the potential of this approach and explore new applications for inference time, such as hot-swapping indices.",1
"People's visual experiences of the world are easy to carve up and examine along natural language boundaries, e.g., by category labels, attribute labels, etc. However, it is more difficult to elicit detailed visuospatial information about what a person attends to, e.g., the specific shape of a tree. Paying attention to the shapes of things not only feeds into well defined tasks like visual category learning, but it is also what enables us to differentiate similarly named objects and to take on creative visual pursuits, like poetically describing the shape of a thing, or finding shapes in the clouds or stars. We use a new data collection method that elicits people's prioritized attention to shapes during visual photo inspection by asking them to trace important parts of the image under varying time constraints. Using data collected via crowdsourcing over a set of 187 photographs, we examine changes in patterns of visual attention across individuals, across image types, and across time constraints.",0
"It is simple to split and analyze people's visual encounters of the world based on natural language boundaries such as category and attribute labels. However, obtaining specific visuospatial details, such as the shape of a tree that an individual focuses on, is more challenging. Recognizing shapes of objects is crucial for visual category learning and distinguishing identical objects. Additionally, it allows for creative activities such as describing the shape of an object poetically or finding shapes in the stars or clouds. To gather information on individuals' prioritized attention to shapes during visual photo inspection, we have developed a new data collection method. We asked participants to trace crucial parts of an image under varying time restrictions. Crowdsourcing was used to collect data from 187 photographs, and we analyzed changes in patterns of visual attention across different individuals, image types, and time constraints.",1
"Machine learning applications to symbolic mathematics are becoming increasingly popular, yet there lacks a centralized source of real-world symbolic expressions to be used as training data. In contrast, the field of natural language processing leverages resources like Wikipedia that provide enormous amounts of real-world textual data. Adopting the philosophy of ""mathematics as language,"" we bridge this gap by introducing a pipeline for distilling mathematical expressions embedded in Wikipedia into symbolic encodings to be used in downstream machine learning tasks. We demonstrate that a $\textit{mathematical}$ $\textit{language}$ $\textit{model}$ trained on this ""corpus"" of expressions can be used as a prior to improve the performance of neural-guided search for the task of symbolic regression.",0
"The use of machine learning in symbolic mathematics is becoming more popular, but there is no centralized source of real-world symbolic expressions for training data. In contrast, natural language processing has access to vast amounts of textual data from resources like Wikipedia. To bridge this gap, we propose a pipeline that converts mathematical expressions from Wikipedia into symbolic encodings for downstream machine learning tasks. By training a mathematical language model on this ""corpus"" of expressions, we show that it can improve the performance of neural-guided search for symbolic regression.",1
"In recent years, graph neural networks (GNNs) have been widely adopted in the representation learning of graph-structured data and provided state-of-the-art performance in various applications such as link prediction, node classification, and recommendation. Motivated by recent advances of self-supervision for representation learning in natural language processing and computer vision, self-supervised learning has been recently studied to leverage unlabeled graph-structured data. However, employing self-supervision tasks as auxiliary tasks to assist a primary task has been less explored in the literature on graphs. In this paper, we propose a novel self-supervised auxiliary learning framework to effectively learn graph neural networks. Moreover, this work is the first study showing that a meta-path prediction is beneficial as a self-supervised auxiliary task for heterogeneous graphs. Our method is learning to learn a primary task with various auxiliary tasks to improve generalization performance. The proposed method identifies an effective combination of auxiliary tasks and automatically balances them to improve the primary task. Our methods can be applied to any graph neural network in a plug-in manner without manual labeling or additional data. Also, it can be extended to any other auxiliary tasks. Our experiments demonstrate that the proposed method consistently improves the performance of node classification and link prediction.",0
"Graph neural networks (GNNs) have become increasingly popular for representation learning of graph-structured data, achieving state-of-the-art performance in tasks such as link prediction, node classification, and recommendation. Recently, researchers have explored self-supervised learning techniques, inspired by advancements in natural language processing and computer vision, to leverage unlabeled graph data. However, the use of self-supervised tasks as auxiliary tasks to support a primary task has not been extensively studied in graph literature. This paper proposes a novel self-supervised auxiliary learning framework to enhance the learning of GNNs, and shows that meta-path prediction is an effective self-supervised auxiliary task for heterogeneous graphs. Our approach learns to learn a primary task with multiple auxiliary tasks that are balanced automatically for optimal performance. The method can be applied to any GNN without requiring manual labeling or additional data, and can be extended to other auxiliary tasks. Our experiments demonstrate that the proposed method consistently improves node classification and link prediction performance.",1
"The computational prediction algorithm of neural network, or deep learning, has drawn much attention recently in statistics as well as in image recognition and natural language processing. Particularly in statistical application for censored survival data, the loss function used for optimization has been mainly based on the partial likelihood from Cox's model and its variations to utilize existing neural network library such as Keras, which was built upon the open source library of TensorFlow. This paper presents a novel application of the neural network to the quantile regression for survival data with right censoring, which is adjusted by the inverse of the estimated censoring distribution in the check function. The main purpose of this work is to show that the deep learning method could be flexible enough to predict nonlinear patterns more accurately compared to existing quantile regression methods such as traditional linear quantile regression and nonparametric quantile regression with total variation regularization, emphasizing practicality of the method for censored survival data. Simulation studies were performed to generate nonlinear censored survival data and compare the deep learning method with existing quantile regression methods in terms of prediction accuracy. The proposed method is illustrated with two publicly available breast cancer data sets with gene signatures. The method has been built into a package and is freely available at \url{https://github.com/yicjia/DeepQuantreg}.",0
"Recently, the neural network's computational prediction algorithm, also known as deep learning, has gained significant attention in statistics, image recognition, and natural language processing. In statistical applications for censored survival data, the loss function used for optimization has mainly relied on the partial likelihood from Cox's model and its variations. Existing neural network libraries such as Keras, built on TensorFlow's open-source library, have been utilized for this purpose. This paper introduces a new application of neural networks to quantile regression for survival data with right censoring, adjusted by the inverse of the estimated censoring distribution in the check function. The goal is to demonstrate that deep learning is flexible enough to predict nonlinear patterns more accurately than traditional linear quantile regression and nonparametric quantile regression with total variation regularization. The proposed method's practicality for censored survival data is emphasized through simulation studies and comparison with existing quantile regression methods. Two publicly available breast cancer data sets with gene signatures are used to illustrate the method, which has been built into a package freely available at \url{https://github.com/yicjia/DeepQuantreg}.",1
"Compressing large neural networks is an important step for their deployment in resource-constrained computational platforms. In this context, vector quantization is an appealing framework that expresses multiple parameters using a single code, and has recently achieved state-of-the-art network compression on a range of core vision and natural language processing tasks. Key to the success of vector quantization is deciding which parameter groups should be compressed together. Previous work has relied on heuristics that group the spatial dimension of individual convolutional filters, but a general solution remains unaddressed. This is desirable for pointwise convolutions (which dominate modern architectures), linear layers (which have no notion of spatial dimension), and convolutions (when more than one filter is compressed to the same codeword). In this paper we make the observation that the weights of two adjacent layers can be permuted while expressing the same function. We then establish a connection to rate-distortion theory and search for permutations that result in networks that are easier to compress. Finally, we rely on an annealed quantization algorithm to better compress the network and achieve higher final accuracy. We show results on image classification, object detection, and segmentation, reducing the gap with the uncompressed model by 40 to 70% with respect to the current state of the art.",0
"An important step in deploying large neural networks on resource-constrained computational platforms is compressing them. Vector quantization, which expresses multiple parameters using a single code, has become a popular framework for network compression in various core vision and natural language processing tasks. However, a general solution for grouping parameter groups together remains unaddressed, especially for pointwise convolutions, linear layers, and convolutions where multiple filters are compressed into the same codeword. To address this, we observe that the weights of two adjacent layers can be permuted while expressing the same function, and establish a connection to rate-distortion theory. We search for permutations that make networks easier to compress and use an annealed quantization algorithm to achieve higher final accuracy. Our results show a reduction in the gap with the uncompressed model by 40 to 70% for image classification, object detection, and segmentation, compared to the current state of the art.",1
"A full-fledged data exploration system must combine different access modalities with a powerful concept of guiding the user in the exploration process, by being reactive and anticipative both for data discovery and for data linking. Such systems are a real opportunity for our community to cater to users with different domain and data science expertise. We introduce INODE -- an end-to-end data exploration system -- that leverages, on the one hand, Machine Learning and, on the other hand, semantics for the purpose of Data Management (DM). Our vision is to develop a classic unified, comprehensive platform that provides extensive access to open datasets, and we demonstrate it in three significant use cases in the fields of Cancer Biomarker Reearch, Research and Innovation Policy Making, and Astrophysics. INODE offers sustainable services in (a) data modeling and linking, (b) integrated query processing using natural language, (c) guidance, and (d) data exploration through visualization, thus facilitating the user in discovering new insights. We demonstrate that our system is uniquely accessible to a wide range of users from larger scientific communities to the public. Finally, we briefly illustrate how this work paves the way for new research opportunities in DM.",0
"To create a comprehensive data exploration system, various access modes must be combined with a user-guidance concept that is both responsive and predictive in terms of data discovery and linking. This will allow for the provision of services to users with diverse domain knowledge and data science expertise. INODE is an end-to-end system for data exploration that uses Machine Learning and semantics for Data Management. Our goal is to develop a unified platform that provides extensive access to open datasets, and we showcase it in three significant use cases in cancer biomarker research, research and innovation policy-making, and astrophysics. INODE offers sustainable services in data modeling and linking, integrated query processing using natural language, guidance, and data exploration through visualization, making it easy for users to uncover new insights. We demonstrate that our system is accessible to a wide range of users, from larger scientific communities to the public. Finally, we briefly describe how this work creates new research opportunities in Data Management.",1
"We algorithmically identify label errors in the test sets of 10 of the most commonly-used computer vision, natural language, and audio datasets, and subsequently study the potential for these label errors to affect benchmark results. Errors in test sets are numerous and widespread: we estimate an average of 3.4% errors across the 10 datasets, where for example 2916 label errors comprise 6% of the ImageNet validation set. Putative label errors are found using confident learning and then human-validated via crowdsourcing (54% of the algorithmically-flagged candidates are indeed erroneously labeled). Surprisingly, we find that lower capacity models may be practically more useful than higher capacity models in real-world datasets with high proportions of erroneously labeled data. For example, on ImageNet with corrected labels: ResNet-18 outperforms ResNet-50 if the prevalence of originally mislabeled test examples increases by just 6%. On CIFAR-10 with corrected labels: VGG-11 outperforms VGG-19 if the prevalence of originally mislabeled test examples increases by 5%. Traditionally, ML practitioners choose which model to deploy based on test accuracy -- our findings advise caution here, proposing that judging models over correctly labeled test sets may be more useful, especially for noisy real-world datasets.",0
"Through the use of confident learning, we have identified label errors in the test sets of 10 popular computer vision, natural language, and audio datasets. Our investigation explores how these errors impact benchmark results. We have discovered that errors in test sets are quite common, with an average of 3.4% errors across the 10 datasets. The ImageNet validation set, for instance, contains 2916 label errors, which make up 6% of the set. We have employed crowdsourcing to validate the putative label errors, and it was found that 54% of the algorithmically-flagged candidates were indeed erroneously labeled. Our surprising discovery is that lower capacity models are more practical than higher capacity models in real-world datasets with high proportions of erroneously labeled data. For example, with corrected labels on ImageNet, ResNet-18 outperforms ResNet-50 if the prevalence of originally mislabeled test examples increases by just 6%. Similarly, with corrected labels on CIFAR-10, VGG-11 outperforms VGG-19 if the prevalence of originally mislabeled test examples increases by 5%. Our findings suggest caution when selecting models based solely on test accuracy, and instead propose that models should be judged over correctly labeled test sets, particularly for noisy real-world datasets.",1
"Self-supervised learning methods are gaining increasing traction in computer vision due to their recent success in reducing the gap with supervised learning. In natural language processing (NLP) self-supervised learning and transformers are already the methods of choice. The recent literature suggests that the transformers are becoming increasingly popular also in computer vision. So far, the vision transformers have been shown to work well when pretrained either using a large scale supervised data or with some kind of co-supervision, e.g. in terms of teacher network. These supervised pretrained vision transformers achieve very good results in downstream tasks with minimal changes. In this work we investigate the merits of self-supervised learning for pretraining image/vision transformers and then using them for downstream classification tasks. We propose Self-supervised vIsion Transformers (SiT) and discuss several self-supervised training mechanisms to obtain a pretext model. The architectural flexibility of SiT allows us to use it as an autoencoder and work with multiple self-supervised tasks seamlessly. We show that a pretrained SiT can be finetuned for a downstream classification task on small scale datasets, consisting of a few thousand images rather than several millions. The proposed approach is evaluated on standard datasets using common protocols. The results demonstrate the strength of the transformers and their suitability for self-supervised learning. We outperformed existing self-supervised learning methods by large margin. We also observed that SiT is good for few shot learning and also showed that it is learning useful representation by simply training a linear classifier on top of the learned features from SiT. Pretraining, finetuning, and evaluation codes will be available under: https://github.com/Sara-Ahmed/SiT.",0
"Due to their success in reducing the gap with supervised learning, self-supervised learning methods are gaining popularity in computer vision. In the field of natural language processing (NLP), self-supervised learning and transformers have already become the preferred methods. Recent literature suggests that transformers are also gaining popularity in computer vision. Currently, vision transformers have been proven to work well when pre-trained using supervised data or co-supervised data, such as teacher networks. These pre-trained vision transformers achieve excellent results in downstream tasks with minimal changes. In this study, we investigate the benefits of self-supervised learning for pre-training image/vision transformers, followed by their use in downstream classification tasks. We introduce Self-supervised vIsion Transformers (SiT) and explore several self-supervised training mechanisms to obtain a pretext model. SiT's architectural flexibility allows us to use it as an autoencoder, working with multiple self-supervised tasks seamlessly. We demonstrate that a pre-trained SiT can be fine-tuned for a downstream classification task on small-scale datasets containing only a few thousand images instead of several million. We evaluate our approach on standard datasets using common protocols and show that the transformers are strong and suitable for self-supervised learning. We outperform existing self-supervised learning methods by a large margin and observe that SiT is excellent for few-shot learning. We also demonstrate that it learns useful representations by simply training a linear classifier on top of the learned features from SiT. Pre-training, fine-tuning, and evaluation codes will be available on https://github.com/Sara-Ahmed/SiT.",1
"We study joint learning of Convolutional Neural Network (CNN) and Transformer for vision-language pre-training (VLPT) which aims to learn cross-modal alignments from millions of image-text pairs. State-of-the-art approaches extract salient image regions and align regions with words step-by-step. As region-based visual features usually represent parts of an image, it is challenging for existing vision-language models to fully understand the semantics from paired natural languages. In this paper, we propose SOHO to ""See Out of tHe bOx"" that takes a whole image as input, and learns vision-language representation in an end-to-end manner. SOHO does not require bounding box annotations which enables inference 10 times faster than region-based approaches. In particular, SOHO learns to extract comprehensive yet compact image features through a visual dictionary (VD) that facilitates cross-modal understanding. VD is designed to represent consistent visual abstractions of similar semantics. It is updated on-the-fly and utilized in our proposed pre-training task Masked Visual Modeling (MVM). We conduct experiments on four well-established vision-language tasks by following standard VLPT settings. In particular, SOHO achieves absolute gains of 2.0% R@1 score on MSCOCO text retrieval 5k test split, 1.5% accuracy on NLVR$^2$ test-P split, 6.7% accuracy on SNLI-VE test split, respectively.",0
"In this study, we investigate the joint learning of Convolutional Neural Network (CNN) and Transformer for vision-language pre-training (VLPT), which focuses on learning cross-modal alignments from millions of image-text pairs. Currently, the most advanced techniques extract important image regions and align them with words in a step-by-step manner. However, as these region-based visual features usually only represent parts of an image, existing vision-language models struggle to fully comprehend the semantics from paired natural languages. Therefore, we introduce SOHO, which stands for ""See Out of tHe bOx"". SOHO takes a whole image as input and learns vision-language representation in an end-to-end manner without the need for bounding box annotations. This approach enables inference 10 times faster than region-based methods. SOHO learns to extract comprehensive yet compact image features through a visual dictionary (VD), which helps facilitate cross-modal understanding. The VD is designed to represent consistent visual abstractions of similar semantics and is updated on-the-fly and utilized in our proposed pre-training task called Masked Visual Modeling (MVM). We evaluate our approach on four well-established vision-language tasks using standard VLPT settings. Our results show that SOHO achieves significant gains in performance on all tasks, including a 2.0% increase in R@1 score on MSCOCO text retrieval 5k test split, a 1.5% increase in accuracy on NLVR$^2$ test-P split, and a 6.7% increase in accuracy on SNLI-VE test split.",1
"Over the last decade, the use of Deep Learning in many applications produced results that are comparable to and in some cases surpassing human expert performance. The application domains include diagnosing diseases, finance, agriculture, search engines, robot vision, and many others. In this paper, we are proposing an architecture that utilizes image/video captioning methods and Natural Language Processing systems to generate a title and a concise abstract for a video. Such a system can potentially be utilized in many application domains, including, the cinema industry, video search engines, security surveillance, video databases/warehouses, data centers, and others. The proposed system functions and operates as followed: it reads a video; representative image frames are identified and selected; the image frames are captioned; NLP is applied to all generated captions together with text summarization; and finally, a title and an abstract are generated for the video. All functions are performed automatically. Preliminary results are provided in this paper using publicly available datasets. This paper is not concerned about the efficiency of the system at the execution time. We hope to be able to address execution efficiency issues in our subsequent publications.",0
"In recent years, Deep Learning has proved to be increasingly successful in numerous fields, achieving results that are comparable to or even surpassing those of human experts. Its applications span a wide range of areas, such as disease diagnosis, finance, agriculture, robot vision, search engines, and many others. This paper presents an architecture that combines image and video captioning techniques with Natural Language Processing systems to generate a brief abstract and title for a video. The proposed system could be used in various settings, such as the cinema industry, video search engines, security surveillance, video databases/warehouses, and data centers. The system operates automatically by analyzing the video, selecting representative image frames, and captioning them. NLP algorithms are then used to summarize the captions and generate a title and abstract for the video. Preliminary results using publicly available datasets are presented. The study does not address the system's efficiency during execution, but the authors hope to address this in future publications.",1
"Object detection with transformers (DETR) reaches competitive performance with Faster R-CNN via a transformer encoder-decoder architecture. Inspired by the great success of pre-training transformers in natural language processing, we propose a pretext task named random query patch detection to Unsupervisedly Pre-train DETR (UP-DETR) for object detection. Specifically, we randomly crop patches from the given image and then feed them as queries to the decoder. The model is pre-trained to detect these query patches from the original image. During the pre-training, we address two critical issues: multi-task learning and multi-query localization. (1) To trade off classification and localization preferences in the pretext task, we freeze the CNN backbone and propose a patch feature reconstruction branch which is jointly optimized with patch detection. (2) To perform multi-query localization, we introduce UP-DETR from single-query patch and extend it to multi-query patches with object query shuffle and attention mask. In our experiments, UP-DETR significantly boosts the performance of DETR with faster convergence and higher average precision on object detection, one-shot detection and panoptic segmentation. Code and pre-training models: https://github.com/dddzg/up-detr.",0
"The transformer encoder-decoder architecture used in object detection with DETR has achieved comparable performance with Faster R-CNN. Taking inspiration from the success of pre-training transformers in natural language processing, we have proposed a pretext task called random query patch detection for Unsupervisedly Pre-training DETR (UP-DETR) in object detection. To detect these query patches from the original image, we randomly crop patches from the image and use them as queries to the decoder. During the pre-training, we address two crucial issues: multi-task learning and multi-query localization. To balance classification and localization preferences in the pretext task, we have introduced a patch feature reconstruction branch that is jointly optimized with patch detection while freezing the CNN backbone. Additionally, to perform multi-query localization, we have extended UP-DETR from single-query patch to multi-query patches using object query shuffle and attention mask. Our experiments have demonstrated that UP-DETR has significantly improved the performance of DETR, resulting in faster convergence and higher average precision on object detection, one-shot detection, and panoptic segmentation. The code and pre-training models can be found at https://github.com/dddzg/up-detr.",1
"While deep learning is a powerful tool for natural language processing (NLP) problems, successful solutions to these problems rely heavily on large amounts of annotated samples. However, manually annotating data is expensive and time-consuming. Active Learning (AL) strategies reduce the need for huge volumes of labeled data by iteratively selecting a small number of examples for manual annotation based on their estimated utility in training the given model. In this paper, we argue that since AL strategies choose examples independently, they may potentially select similar examples, all of which may not contribute significantly to the learning process. Our proposed approach, Active$\mathbf{^2}$ Learning (A$\mathbf{^2}$L), actively adapts to the deep learning model being trained to eliminate such redundant examples chosen by an AL strategy. We show that A$\mathbf{^2}$L is widely applicable by using it in conjunction with several different AL strategies and NLP tasks. We empirically demonstrate that the proposed approach is further able to reduce the data requirements of state-of-the-art AL strategies by $\approx \mathbf{3-25\%}$ on an absolute scale on multiple NLP tasks while achieving the same performance with virtually no additional computation overhead.",0
"Even though deep learning is a robust tool for solving natural language processing (NLP) problems, it heavily relies on a vast amount of annotated samples to be successful. However, annotating data manually is both time-consuming and expensive. To address this issue, Active Learning (AL) strategies are used to iteratively select a small number of examples for manual annotation based on their estimated utility in training the given model. Nonetheless, AL strategies may select similar examples independently, resulting in redundant examples that do not contribute significantly to the learning process. In this paper, we introduce Active$\mathbf{^2}$ Learning (A$\mathbf{^2}$L), which adapts to the deep learning model being trained to eliminate such redundancy. Our approach is widely applicable and can be used with various AL strategies and NLP tasks. We demonstrate empirically that A$\mathbf{^2}$L can reduce the data requirements of state-of-the-art AL strategies by $\approx \mathbf{3-25\%}$ on an absolute scale on multiple NLP tasks while achieving the same performance with virtually no additional computation overhead.",1
"COVID-19 pandemic has an unprecedented impact all over the world since early 2020. During this public health crisis, reliable forecasting of the disease becomes critical for resource allocation and administrative planning. The results from compartmental models such as SIR and SEIR are popularly referred by CDC and news media. With more and more COVID-19 data becoming available, we examine the following question: Can a direct data-driven approach without modeling the disease spreading dynamics outperform the well referred compartmental models and their variants? In this paper, we show the possibility. It is observed that as COVID-19 spreads at different speed and scale in different geographic regions, it is highly likely that similar progression patterns are shared among these regions within different time periods. This intuition lead us to develop a new neural forecasting model, called Attention Crossing Time Series (\textbf{ACTS}), that makes forecasts via comparing patterns across time series obtained from multiple regions. The attention mechanism originally developed for natural language processing can be leveraged and generalized to materialize this idea. Among 13 out of 18 testings including forecasting newly confirmed cases, hospitalizations and deaths, \textbf{ACTS} outperforms all the leading COVID-19 forecasters highlighted by CDC.",0
"Since early 2020, the COVID-19 pandemic has had an unprecedented global impact. During this public health crisis, it is crucial to have reliable disease forecasting for resource allocation and administrative planning. The Centers for Disease Control and Prevention (CDC) and news media often refer to the results from compartmental models such as SIR and SEIR. With the increasing availability of COVID-19 data, we explore whether a direct data-driven approach can outperform these compartmental models and their variants. In this paper, we demonstrate that this is possible. As COVID-19 spreads at varying speeds and scales in different geographic regions, similar progression patterns are likely to be shared among these regions at different time periods. This insight led us to develop a novel neural forecasting model, called Attention Crossing Time Series (ACTS), which compares patterns across time series from multiple regions to make forecasts. The attention mechanism, originally developed for natural language processing, can be generalized to realize this idea. In 13 out of 18 tests, including forecasting newly confirmed cases, hospitalizations, and deaths, ACTS outperforms all the leading COVID-19 forecasters highlighted by the CDC.",1
"Natural Language (NL) descriptions can be one of the most convenient or the only way to interact with systems built to understand and detect city scale traffic patterns and vehicle-related events. In this paper, we extend the widely adopted CityFlow Benchmark with NL descriptions for vehicle targets and introduce the CityFlow-NL Benchmark. The CityFlow-NL contains more than 5,000 unique and precise NL descriptions of vehicle targets, making it the first multi-target multi-camera tracking with NL descriptions dataset to our knowledge. Moreover, the dataset facilitates research at the intersection of multi-object tracking, retrieval by NL descriptions, and temporal localization of events. In this paper, we focus on two foundational tasks: the Vehicle Retrieval by NL task and the Vehicle Tracking by NL task, which take advantage of the proposed CityFlow-NL benchmark and provide a strong basis for future research on the multi-target multi-camera tracking by NL description task.",0
"Interacting with systems designed to recognize city-wide traffic patterns and vehicle-related events can be made easier with Natural Language (NL) descriptions. The CityFlow Benchmark has been extended with NL descriptions for vehicle targets and is now known as the CityFlow-NL Benchmark. The benchmark contains over 5,000 precise and unique NL descriptions of vehicle targets, which is the first multi-target multi-camera tracking dataset that includes NL descriptions to our knowledge. The dataset is beneficial for research on multi-object tracking, retrieval by NL descriptions, and temporal localization of events. The paper focuses on two fundamental tasks: Vehicle Retrieval by NL and Vehicle Tracking by NL, which utilize the proposed CityFlow-NL benchmark and provide a strong basis for future research on the multi-target multi-camera tracking by NL description task.",1
"We propose a novel Siamese Natural Language Tracker (SNLT), which brings the advancements in visual tracking to the tracking by natural language (NL) descriptions task. The proposed SNLT is applicable to a wide range of Siamese trackers, providing a new class of baselines for the tracking by NL task and promising future improvements from the advancements of Siamese trackers. The carefully designed architecture of the Siamese Natural Language Region Proposal Network (SNL-RPN), together with the Dynamic Aggregation of vision and language modalities, is introduced to perform the tracking by NL task. Empirical results over tracking benchmarks with NL annotations show that the proposed SNLT improves Siamese trackers by 3 to 7 percentage points with a slight tradeoff of speed. The proposed SNLT outperforms all NL trackers to-date and is competitive among state-of-the-art real-time trackers on LaSOT benchmarks while running at 50 frames per second on a single GPU.",0
"We present the Siamese Natural Language Tracker (SNLT), a new approach that incorporates advancements in visual tracking to improve the tracking by natural language (NL) descriptions task. This innovative technique can be applied to a variety of Siamese trackers, providing a new baseline for the tracking by NL task, as well as offering potential for future improvements. The SNLT utilizes a carefully designed architecture called the Siamese Natural Language Region Proposal Network (SNL-RPN) combined with Dynamic Aggregation of vision and language modalities to achieve tracking by NL. Empirical results from tracking benchmarks with NL annotations show that the SNLT enhances Siamese trackers by 3 to 7 percentage points, with a slight reduction in speed. The SNLT outperforms all previous NL trackers and is competitive with state-of-the-art real-time trackers on LaSOT benchmarks, running at 50 frames per second on a single GPU.",1
"Video editing tools are widely used nowadays for digital design. Although the demand for these tools is high, the prior knowledge required makes it difficult for novices to get started. Systems that could follow natural language instructions to perform automatic editing would significantly improve accessibility. This paper introduces the language-based video editing (LBVE) task, which allows the model to edit, guided by text instruction, a source video into a target video. LBVE contains two features: 1) the scenario of the source video is preserved instead of generating a completely different video; 2) the semantic is presented differently in the target video, and all changes are controlled by the given instruction. We propose a Multi-Modal Multi-Level Transformer (M$^3$L-Transformer) to carry out LBVE. The M$^3$L-Transformer dynamically learns the correspondence between video perception and language semantic at different levels, which benefits both the video understanding and video frame synthesis. We build three new datasets for evaluation, including two diagnostic and one from natural videos with human-labeled text. Extensive experimental results show that M$^3$L-Transformer is effective for video editing and that LBVE can lead to a new field toward vision-and-language research.",0
"Nowadays, video editing tools are widely used in digital design. However, novices face difficulty in getting started due to the prior knowledge required. Therefore, there is a need for systems that could perform automatic editing based on natural language instructions to increase accessibility. The language-based video editing (LBVE) task is introduced in this paper, which allows the model to edit a source video into a target video while being guided by text instructions. LBVE has two key features: preserving the scenario of the source video and presenting the semantic differently in the target video while being controlled by the given instruction. To carry out LBVE, we propose a Multi-Modal Multi-Level Transformer (M$^3$L-Transformer) that dynamically learns the correspondence between video perception and language semantic at different levels. This benefits both video understanding and frame synthesis. Three new datasets are built for evaluation, including two diagnostic and one from natural videos with human-labeled text. Extensive experimental results show that M$^3$L-Transformer is effective for video editing and that LBVE can lead to a new field of research in vision and language.",1
"Most of state of the art methods applied on time series consist of deep learning methods that are too complex to be interpreted. This lack of interpretability is a major drawback, as several applications in the real world are critical tasks, such as the medical field or the autonomous driving field. The explainability of models applied on time series has not gather much attention compared to the computer vision or the natural language processing fields. In this paper, we present an overview of existing explainable AI (XAI) methods applied on time series and illustrate the type of explanations they produce. We also provide a reflection on the impact of these explanation methods to provide confidence and trust in the AI systems.",0
"The majority of advanced techniques utilized on time series involve deep learning methods that are excessively intricate to comprehend. This absence of comprehensibility is a significant disadvantage, considering the importance of various applications in the real world, such as the medical and autonomous driving fields. The level of attention given to the interpretability of models applied on time series is not comparable to the attention given to the computer vision or natural language processing fields. This article presents an overview of existing methods for explainable AI (XAI) applied on time series, outlining the types of explanations they generate. Furthermore, we reflect on the impact of these explanation methods in providing confidence and trust in AI systems.",1
"The Transformer architecture has become increasingly popular over the past two years, owing to its impressive performance on a number of natural language processing (NLP) tasks. However, all Transformer computations occur at the level of word representations and therefore, it may be argued that Transformer models do not explicitly attempt to learn hierarchical structure which is widely assumed to be integral to language. In the present work, we introduce hierarchical processing into the Transformer model, taking inspiration from the U-Net architecture, popular in computer vision for its hierarchical view of natural images. We empirically demonstrate that the proposed architecture outperforms both the vanilla Transformer and some strong baselines in the domain of chit-chat dialogue.",0
"Over the last couple of years, the Transformer architecture has gained popularity for its impressive performance in various natural language processing (NLP) tasks. However, there is a debate about whether Transformer models can learn a hierarchical structure that is essential to language since they only compute at the word representation level. In this study, we incorporate hierarchical processing into the Transformer model using U-Net architecture, which is famous in computer vision for its hierarchical perspective of natural images. Through experimentation, we prove that our proposed architecture outperforms the vanilla Transformer and some robust baselines in chit-chat dialogue.",1
"A special purpose learning system assumes knowledge of admissible tasks at design time. Adapting such a system to unforeseen tasks requires architecture manipulation such as adding an output head for each new task or dataset. In this work, we propose a task-agnostic vision-language system that accepts an image and a natural language task description and outputs bounding boxes, confidences, and text. The system supports a wide range of vision tasks such as classification, localization, question answering, captioning, and more. We evaluate the system's ability to learn multiple skills simultaneously, to perform tasks with novel skill-concept combinations, and to learn new skills efficiently and without forgetting.",0
"The use of a specific learning system assumes that the tasks to be performed are known at the time of design. If new tasks arise, the system must be modified by adding an output head for each new task or dataset. In this study, we introduce a vision-language system that is not specific to any particular task and can accept both an image and a natural language task description. The system produces bounding boxes, confidences, and text and can be used for various vision tasks, including classification, localization, question answering, and captioning. We assess the system's capability to learn several skills at once, perform tasks that combine new skill-concepts, and learn new skills efficiently without forgetting.",1
"We present DietNeRF, a 3D neural scene representation estimated from a few images. Neural Radiance Fields (NeRF) learn a continuous volumetric representation of a scene through multi-view consistency, and can be rendered from novel viewpoints by ray casting. While NeRF has an impressive ability to reconstruct geometry and fine details given many images, up to 100 for challenging 360{\deg} scenes, it often finds a degenerate solution to its image reconstruction objective when only a few input views are available. To improve few-shot quality, we propose DietNeRF. We introduce an auxiliary semantic consistency loss that encourages realistic renderings at novel poses. DietNeRF is trained on individual scenes to (1) correctly render given input views from the same pose, and (2) match high-level semantic attributes across different, random poses. Our semantic loss allows us to supervise DietNeRF from arbitrary poses. We extract these semantics using a pre-trained visual encoder such as CLIP, a Vision Transformer trained on hundreds of millions of diverse single-view, 2D photographs mined from the web with natural language supervision. In experiments, DietNeRF improves the perceptual quality of few-shot view synthesis when learned from scratch, can render novel views with as few as one observed image when pre-trained on a multi-view dataset, and produces plausible completions of completely unobserved regions.",0
"DietNeRF is a method for generating a 3D neural scene representation from a limited number of images. The Neural Radiance Fields (NeRF) technique creates a continuous volumetric representation of a scene using multiple views and can be rendered from new angles through ray casting. Although NeRF is capable of reconstructing fine details and geometry with up to 100 images for complex 360{\deg} scenes, it often produces a suboptimal solution when only a few input views are available. To address this issue, DietNeRF proposes an auxiliary semantic consistency loss that promotes realistic renderings at novel poses. DietNeRF is trained on individual scenes to correctly render input views from the same pose and match high-level semantic attributes across different, random poses. This semantic loss allows for supervision from arbitrary viewpoints and is extracted using a pre-trained visual encoder like CLIP. In experiments, DietNeRF improves the quality of few-shot view synthesis and can generate plausible completions of unobserved regions with as few as one observed image when pre-trained on a multi-view dataset.",1
"We consider the task of grasping a target object based on a natural language command query. Previous work primarily focused on localizing the object given the query, which requires a separate grasp detection module to grasp it. The cascaded application of two pipelines incurs errors in overlapping multi-object cases due to ambiguity in the individual outputs. This work proposes a model named Command Grasping Network(CGNet) to directly output command satisficing grasps from RGB image and textual command inputs. A dataset with ground truth (image, command, grasps) tuple is generated based on the VMRD dataset to train the proposed network. Experimental results on the generated test set show that CGNet outperforms a cascaded object-retrieval and grasp detection baseline by a large margin. Three physical experiments demonstrate the functionality and performance of CGNet.",0
"The focus of our study is the act of grasping a desired object based on a command given in natural language. While previous research has concentrated on pinpointing the object mentioned in the command, this approach requires an additional module to detect the grasp required to pick it up. This two-step process often results in errors when dealing with multiple objects that overlap, due to the ambiguity of the separate outputs. To address this issue, we propose a new model called the Command Grasping Network (CGNet) that can directly output satisfactory grasping commands from both an RGB image and a written command. To train this model, we created a dataset consisting of (image, command, grasp) tuples, generated from the VMRD dataset. Our experimental results show that CGNet significantly outperforms the baseline approach of object-retrieval and grasp detection. Additionally, we conducted three physical experiments that demonstrate the effectiveness and performance of CGNet.",1
"The video captioning task is to describe the video contents with natural language by the machine. Many methods have been proposed for solving this task. A large dataset called MSR Video to Text (MSR-VTT) is often used as the benckmark dataset for testing the performance of the methods. However, we found that the human annotations, i.e., the descriptions of video contents in the dataset are quite noisy, e.g., there are many duplicate captions and many captions contain grammatical problems. These problems may pose difficulties to video captioning models for learning. We cleaned the MSR-VTT annotations by removing these problems, then tested several typical video captioning models on the cleaned dataset. Experimental results showed that data cleaning boosted the performances of the models measured by popular quantitative metrics. We recruited subjects to evaluate the results of a model trained on the original and cleaned datasets. The human behavior experiment demonstrated that trained on the cleaned dataset, the model generated captions that were more coherent and more relevant to contents of the video clips. The cleaned dataset is publicly available.",0
"The machine's task of video captioning involves using natural language to describe the content of a video. Various methods have been proposed to address this task, with the MSR Video to Text (MSR-VTT) dataset being commonly used as a benchmark for evaluating performance. However, we discovered that the human annotations in the dataset contain numerous errors, such as duplicate captions and grammatical issues, which could impede learning for video captioning models. To address this, we cleaned the MSR-VTT annotations by removing these issues and then tested several video captioning models on the cleaned dataset. Our experiments revealed that cleaning the data enhanced the models' performance, as measured by popular metrics. Additionally, we conducted a human behavior experiment to compare the results of a model trained on the original and cleaned datasets, and found that the captions generated from the cleaned dataset were more coherent and relevant to the video content. The cleaned dataset is publicly available for use.",1
"Natural language video localization (NLVL), which aims to locate a target moment from a video that semantically corresponds to a text query, is a novel and challenging task. Toward this end, in this paper, we present a comprehensive survey of the NLVL algorithms, where we first propose the pipeline of NLVL, and then categorize them into supervised and weakly-supervised methods, following by the analysis of the strengths and weaknesses of each kind of methods. Subsequently, we present the dataset, evaluation protocols and the general performance analysis. Finally, the possible perspectives are obtained by summarizing the existing methods.",0
"The task of natural language video localization (NLVL) is to find a specific moment in a video that matches a given text query, which is a complex and innovative challenge. In this paper, we conduct a thorough review of NLVL algorithms. We begin by outlining the NLVL pipeline and then organize the algorithms into two categories: supervised and weakly-supervised methods. We analyze the advantages and disadvantages of each approach. We also introduce the dataset, evaluation procedures, and overall performance analysis. Lastly, we summarize the existing methods and discuss potential future directions.",1
"Natural Human-Robot Interaction (HRI) is one of the key components for service robots to be able to work in human-centric environments. In such dynamic environments, the robot needs to understand the intention of the user to accomplish a task successfully. Towards addressing this point, we propose a software architecture that segments a target object from a crowded scene, indicated verbally by a human user. At the core of our system, we employ a multi-modal deep neural network for visual grounding. Unlike most grounding methods that tackle the challenge using pre-trained object detectors via a two-stepped process, we develop a single stage zero-shot model that is able to provide predictions in unseen data. We evaluate the performance of the proposed model on real RGB-D data collected from public scene datasets. Experimental results showed that the proposed model performs well in terms of accuracy and speed, while showcasing robustness to variation in the natural language input.",0
"Service robots require Natural Human-Robot Interaction (HRI) to function effectively in human-centric environments. In such settings, the robot must comprehend the user's intention to complete tasks successfully. To address this issue, we suggest a software architecture that verbally identifies a target object in a crowded scene. Our system employs a multi-modal deep neural network for visual grounding, which is different from most grounding methods that use pre-trained object detectors. We develop a single-stage zero-shot model that can make predictions in unseen data. We evaluate the performance of our model using real RGB-D data from public scene datasets. Our experimental results demonstrate that our proposed model is accurate, fast, and robust to natural language input variations.",1
"Tracking by natural language specification is a new rising research topic that aims at locating the target object in the video sequence based on its language description. Compared with traditional bounding box (BBox) based tracking, this setting guides object tracking with high-level semantic information, addresses the ambiguity of BBox, and links local and global search organically together. Those benefits may bring more flexible, robust and accurate tracking performance in practical scenarios. However, existing natural language initialized trackers are developed and compared on benchmark datasets proposed for tracking-by-BBox, which can't reflect the true power of tracking-by-language. In this work, we propose a new benchmark specifically dedicated to the tracking-by-language, including a large scale dataset, strong and diverse baseline methods. Specifically, we collect 2k video sequences (contains a total of 1,244,340 frames, 663 words) and split 1300/700 for the train/testing respectively. We densely annotate one sentence in English and corresponding bounding boxes of the target object for each video. We also introduce two new challenges into TNL2K for the object tracking task, i.e., adversarial samples and modality switch. A strong baseline method based on an adaptive local-global-search scheme is proposed for future works to compare. We believe this benchmark will greatly boost related researches on natural language guided tracking.",0
"The emerging research topic of tracking by natural language specification aims to locate an object in a video sequence using language description instead of traditional bounding box-based tracking. This approach utilizes high-level semantic information, resolves the ambiguity of bounding boxes, and seamlessly integrates local and global search. These advantages promise more adaptable, robust, and precise tracking in practical scenarios. However, current natural language initialized trackers are evaluated on tracking-by-bounding-box benchmark datasets that cannot fully exhibit the potential of tracking-by-language. To address this limitation, we introduce a new benchmark dedicated to tracking-by-language. This benchmark includes a vast dataset with 2k video sequences, strong and diverse baseline methods, and two new challenges: adversarial samples and modality switch. Each video is densely annotated with one English sentence and the corresponding bounding boxes of the target object. We propose a robust baseline method based on an adaptive local-global-search scheme for future comparisons. We anticipate that this benchmark will significantly advance research on natural language-guided tracking.",1
"Localized Narratives is a dataset with detailed natural language descriptions of images paired with mouse traces that provide a sparse, fine-grained visual grounding for phrases. We propose TReCS, a sequential model that exploits this grounding to generate images. TReCS uses descriptions to retrieve segmentation masks and predict object labels aligned with mouse traces. These alignments are used to select and position masks to generate a fully covered segmentation canvas; the final image is produced by a segmentation-to-image generator using this canvas. This multi-step, retrieval-based approach outperforms existing direct text-to-image generation models on both automatic metrics and human evaluations: overall, its generated images are more photo-realistic and better match descriptions.",0
"The dataset called Localized Narratives contains detailed natural language descriptions of images, which are paired with mouse traces to provide specific visual information for phrases. Our proposed model, TReCS, utilizes this visual information to generate images. TReCS retrieves segmentation masks and predicts object labels from the descriptions aligned with mouse traces. These alignments are then used to position masks to create a fully covered segmentation canvas, and the final image is generated using a segmentation-to-image generator. This retrieval-based approach outperforms existing direct text-to-image generation models in both automatic metrics and human evaluations, producing more photo-realistic images that match descriptions better.",1
"Vision-and-language navigation (VLN) is a multimodal task where an agent follows natural language instructions and navigates in visual environments. Multiple setups have been proposed, and researchers apply new model architectures or training techniques to boost navigation performance. However, recent studies witness a slow-down in the performance improvements in both indoor and outdoor VLN tasks, and the agents' inner mechanisms for making navigation decisions remain unclear. To the best of our knowledge, the way the agents perceive the multimodal input is under-studied and clearly needs investigations. In this work, we conduct a series of diagnostic experiments to unveil agents' focus during navigation. Results show that indoor navigation agents refer to both object tokens and direction tokens in the instruction when making decisions. In contrast, outdoor navigation agents heavily rely on direction tokens and have a poor understanding of the object tokens. Furthermore, instead of merely staring at surrounding objects, indoor navigation agents can set their sights on objects further from the current viewpoint. When it comes to vision-and-language alignments, many models claim that they are able to align object tokens with certain visual targets, but we cast doubt on the reliability of such alignments.",0
"VLN is a task wherein agents navigate through visual environments by following natural language instructions. Researchers have proposed various setups and techniques to improve navigation performance. However, recent studies show a decrease in performance improvements for both indoor and outdoor VLN tasks, and the agents' decision-making mechanisms remain unclear. Investigations on how agents perceive multimodal input are needed. This study conducts diagnostic experiments to determine agents' focus during navigation. Results reveal that indoor navigation agents rely on both object and direction tokens, while outdoor navigation agents heavily depend on direction tokens and have poor understanding of object tokens. Indoor navigation agents can also focus on distant objects. Additionally, the reliability of object-token and visual-target alignment claims is uncertain.",1
"Recently, the Transformer module has been transplanted from natural language processing to computer vision. This paper applies the Transformer to video-based person re-identification, where the key issue is to extract the discriminative information from a tracklet. We show that, despite the strong learning ability, the vanilla Transformer suffers from an increased risk of over-fitting, arguably due to a large number of attention parameters and insufficient training data. To solve this problem, we propose a novel pipeline where the model is pre-trained on a set of synthesized video data and then transferred to the downstream domains with the perception-constrained Spatiotemporal Transformer (STT) module and Global Transformer (GT) module. The derived algorithm achieves significant accuracy gain on three popular video-based person re-identification benchmarks, MARS, DukeMTMC-VideoReID, and LS-VID, especially when the training and testing data are from different domains. More importantly, our research sheds light on the application of the Transformer on highly-structured visual data.",0
"The Transformer module has recently been adapted from natural language processing to computer vision. This study employs the Transformer in video-based person re-identification, with the main challenge being the extraction of discriminative information from a tracklet. Although the vanilla Transformer has strong learning capabilities, it is prone to over-fitting due to a large number of attention parameters and insufficient training data. To address this issue, a novel pipeline is proposed where the model is pre-trained on synthesized video data before being transferred to downstream domains using the Spatiotemporal Transformer (STT) module and Global Transformer (GT) module. The resulting algorithm demonstrates a significant improvement in accuracy on three popular video-based person re-identification benchmarks, namely MARS, DukeMTMC-VideoReID, and LS-VID, particularly when the training and testing data belong to different domains. Furthermore, this study provides insights into the Transformer's potential applications in highly-structured visual data.",1
"The advent of larger machine learning (ML) models have improved state-of-the-art (SOTA) performance in various modeling tasks, ranging from computer vision to natural language. As ML models continue increasing in size, so does their respective energy consumption and computational requirements. However, the methods for tracking, reporting, and comparing energy consumption remain limited. We presentEnergyVis, an interactive energy consumption tracker for ML models. Consisting of multiple coordinated views, EnergyVis enables researchers to interactively track, visualize and compare model energy consumption across key energy consumption and carbon footprint metrics (kWh and CO2), helping users explore alternative deployment locations and hardware that may reduce carbon footprints. EnergyVis aims to raise awareness concerning computational sustainability by interactively highlighting excessive energy usage during model training; and by providing alternative training options to reduce energy usage.",0
"The state-of-the-art performance in various modeling tasks, such as computer vision and natural language, has been improved with the emergence of larger machine learning (ML) models. However, with the increase in model size comes a rise in energy consumption and computational demands, creating a need for better methods for tracking, reporting, and comparing energy usage. We introduce EnergyVis, an interactive tool that allows researchers to track, visualize, and compare energy consumption across key metrics like kWh and CO2. This tool can help users explore alternative deployment locations and hardware to reduce carbon footprints. By highlighting excessive energy usage during model training, EnergyVis aims to promote computational sustainability and provide alternative training options to reduce energy consumption.",1
"Referring image segmentation aims to segment the objects referred by a natural language expression. Previous methods usually focus on designing an implicit and recurrent feature interaction mechanism to fuse the visual-linguistic features to directly generate the final segmentation mask without explicitly modeling the localization information of the referent instances. To tackle these problems, we view this task from another perspective by decoupling it into a ""Locate-Then-Segment"" (LTS) scheme. Given a language expression, people generally first perform attention to the corresponding target image regions, then generate a fine segmentation mask about the object based on its context. The LTS first extracts and fuses both visual and textual features to get a cross-modal representation, then applies a cross-model interaction on the visual-textual features to locate the referred object with position prior, and finally generates the segmentation result with a light-weight segmentation network. Our LTS is simple but surprisingly effective. On three popular benchmark datasets, the LTS outperforms all the previous state-of-the-art methods by a large margin (e.g., +3.2% on RefCOCO+ and +3.4% on RefCOCOg). In addition, our model is more interpretable with explicitly locating the object, which is also proved by visualization experiments. We believe this framework is promising to serve as a strong baseline for referring image segmentation.",0
"The goal of referring image segmentation is to segment objects referenced in natural language expressions. Current methods rely on an implicit and recurrent feature interaction mechanism to fuse visual and linguistic features and generate the segmentation mask. However, this approach does not explicitly model the localization information of the referent instances. To address this issue, we propose a ""Locate-Then-Segment"" (LTS) scheme that separates the task into two steps. Firstly, attention is directed to the corresponding target image regions, followed by the generation of a segmentation mask based on contextual information. The LTS method extracts and fuses both visual and textual features to obtain a cross-modal representation, applies cross-model interaction to locate the referred object with position prior, and generates the segmentation result using a lightweight segmentation network. Our LTS method is simple yet highly effective, outperforming all previous state-of-the-art methods by a large margin on three benchmark datasets (e.g., +3.2% on RefCOCO+ and +3.4% on RefCOCOg). Moreover, our model is more interpretable, as it explicitly locates the object, as demonstrated by visualization experiments. We believe that our framework holds promise as a strong baseline for referring image segmentation.",1
"Knowledge is captured in the form of entities and their relationships and stored in knowledge graphs. Knowledge graphs enhance the capabilities of applications in many different areas including Web search, recommendation, and natural language understanding. This is mainly because, entities enable machines to understand things that go beyond simple tokens. Many modern algorithms use learned entity embeddings from these structured representations. However, building a knowledge graph takes time and effort, hence very costly and nontrivial. On the other hand, many Web sources describe entities in some structured format and therefore, finding ways to get them into useful entity knowledge is advantageous. We propose an approach that processes entity centric textual knowledge sources to learn entity embeddings and in turn avoids the need for a traditional knowledge graph. We first extract triples into the new representation format that does not use traditional complex triple extraction methods defined by pre-determined relationship labels. Then we learn entity embeddings through this new type of triples. We show that the embeddings learned from our approach are: (i) high quality and comparable to a known knowledge graph-based embeddings and can be used to improve them further, (ii) better than a contextual language model-based entity embeddings, and (iii) easy to compute and versatile in domain-specific applications where a knowledge graph is not readily available",0
"Storing knowledge in knowledge graphs involves representing entities and their relationships. This improves the capabilities of various applications, including natural language understanding, recommendation, and web search. Entities enable machines to understand complex concepts beyond simple tokens, making them useful for modern algorithms that use learned entity embeddings from structured representations. However, creating a knowledge graph is time-consuming and expensive. Instead, we propose an approach that processes entity-centric textual knowledge sources to learn entity embeddings, eliminating the need for a traditional knowledge graph. We extract triples into a new representation format and learn entity embeddings through them. Our approach produces high-quality embeddings comparable to knowledge graph-based embeddings and superior to contextual language model-based entity embeddings. Additionally, it is easy to compute and adaptable for domain-specific applications where a knowledge graph is unavailable.",1
"Following the success in advancing natural language processing and understanding, transformers are expected to bring revolutionary changes to computer vision. This work provides the first and comprehensive study on the robustness of vision transformers (ViTs) against adversarial perturbations. Tested on various white-box and transfer attack settings, we find that ViTs possess better adversarial robustness when compared with convolutional neural networks (CNNs). We summarize the following main observations contributing to the improved robustness of ViTs:   1) Features learned by ViTs contain less low-level information and are more generalizable, which contributes to superior robustness against adversarial perturbations.   2) Introducing convolutional or tokens-to-token blocks for learning low-level features in ViTs can improve classification accuracy but at the cost of adversarial robustness.   3) Increasing the proportion of transformers in the model structure (when the model consists of both transformer and CNN blocks) leads to better robustness. But for a pure transformer model, simply increasing the size or adding layers cannot guarantee a similar effect.   4) Pre-training on larger datasets does not significantly improve adversarial robustness though it is critical for training ViTs.   5) Adversarial training is also applicable to ViT for training robust models.   Furthermore, feature visualization and frequency analysis are conducted for explanation. The results show that ViTs are less sensitive to high-frequency perturbations than CNNs and there is a high correlation between how well the model learns low-level features and its robustness against different frequency-based perturbations.",0
"Transformers are anticipated to revolutionize computer vision after their success in natural language processing and understanding. This study is the first to comprehensively examine the robustness of vision transformers (ViTs) against adversarial perturbations. ViTs are shown to possess better adversarial robustness than convolutional neural networks (CNNs) in various white-box and transfer attack settings. The study reveals that ViTs' improved robustness can be attributed to the following observations: 1) ViTs learn features that contain less low-level information and are more generalizable, resulting in superior resistance to adversarial perturbations. 2) While introducing convolutional or tokens-to-token blocks to ViTs can enhance classification accuracy, it comes at the expense of adversarial robustness. 3) Increasing the proportion of transformers in the model structure enhances robustness. However, this effect cannot be guaranteed by solely increasing the size or adding layers to a pure transformer model. 4) Larger pre-training datasets do not significantly improve ViTs' adversarial robustness, although they are critical for their training. 5) Adversarial training can be used to train robust ViT models. Additionally, feature visualization and frequency analysis reveal that ViTs are less sensitive to high-frequency perturbations than CNNs. There is also a high correlation between the model's ability to learn low-level features and its robustness against different frequency-based perturbations.",1
"Research in the area of Vision and Language encompasses challenging topics that seek to connect visual and textual information. The video-to-text problem is one of these topics, in which the goal is to connect an input video with its textual description. This connection can be mainly made by retrieving the most significant descriptions from a corpus or generating a new one given a context video. These two ways represent essential tasks for Computer Vision and Natural Language Processing communities, called text retrieval from video task and video captioning/description task. These two tasks are substantially more complex than predicting or retrieving a single sentence from an image. The spatiotemporal information present in videos introduces diversity and complexity regarding the visual content and the structure of associated language descriptions. This review categorizes and describes the state-of-the-art techniques for the video-to-text problem. It covers the main video-to-text methods and the ways to evaluate their performance. We analyze how the most reported benchmark datasets have been created, showing their drawbacks and strengths for the problem requirements. We also show the impressive progress that researchers have made on each dataset, and we analyze why, despite this progress, the video-to-text conversion is still unsolved. State-of-the-art techniques are still a long way from achieving human-like performance in generating or retrieving video descriptions. We cover several significant challenges in the field and discuss future research directions.",0
"The field of Vision and Language research deals with complex topics that aim to link visual and textual information. One such topic is the video-to-text problem, which involves connecting a video input with its corresponding textual description. This can be accomplished by retrieving relevant descriptions from a database or generating new ones based on the context of the video. These tasks are crucial for the Computer Vision and Natural Language Processing communities and are more challenging than predicting a single sentence from an image due to the spatiotemporal information present in videos. This paper reviews the current state-of-the-art techniques for the video-to-text problem, including methods and performance evaluation. The authors also examine the limitations and strengths of benchmark datasets used for the problem and discuss significant challenges in the field and future research directions. Despite progress made, the video-to-text conversion remains far from achieving human-like performance.",1
"Neural networks training on edge terminals is essential for edge AI computing, which needs to be adaptive to evolving environment. Quantised models can efficiently run on edge devices, but existing training methods for these compact models are designed to run on powerful servers with abundant memory and energy budget. For example, quantisation-aware training (QAT) method involves two copies of model parameters, which is usually beyond the capacity of on-chip memory in edge devices. Data movement between off-chip and on-chip memory is energy demanding as well. The resource requirements are trivial for powerful servers, but critical for edge devices. To mitigate these issues, We propose Resource Constrained Training (RCT). RCT only keeps a quantised model throughout the training, so that the memory requirements for model parameters in training is reduced. It adjusts per-layer bitwidth dynamically in order to save energy when a model can learn effectively with lower precision. We carry out experiments with representative models and tasks in image application and natural language processing. Experiments show that RCT saves more than 86\% energy for General Matrix Multiply (GEMM) and saves more than 46\% memory for model parameters, with limited accuracy loss. Comparing with QAT-based method, RCT saves about half of energy on moving model parameters.",0
"Training neural networks on edge terminals is crucial for edge AI computing, which must be adaptable to a changing environment. Although quantised models can run efficiently on edge devices, the current training methods for these models are designed for high-performance servers with sufficient memory and energy resources. For instance, quantisation-aware training (QAT) requires two copies of model parameters, which typically exceed the on-chip memory capacity of edge devices. Additionally, data transfer between off-chip and on-chip memory consumes a significant amount of energy. While these resource requirements are insignificant for powerful servers, they are critical for edge devices. To address these challenges, we propose Resource Constrained Training (RCT), which employs only a quantised model throughout the training to reduce the memory requirements for model parameters during training. It also adjusts the bitwidth of each layer dynamically to conserve energy when the model can learn effectively with lower precision. We conducted experiments with typical models and tasks in image processing and natural language processing. The results demonstrated that RCT saved more than 86% energy for General Matrix Multiply (GEMM) and more than 46% memory for model parameters with limited accuracy loss. Compared to QAT-based methods, RCT saved about half the energy required to transfer model parameters.",1
"Deep learning model (primarily convolutional networks and LSTM) for time series classification has been studied broadly by the community with the wide applications in different domains like healthcare, finance, industrial engineering and IoT. Meanwhile, Transformer Networks recently achieved frontier performance on various natural language processing and computer vision tasks. In this work, we explored a simple extension of the current Transformer Networks with gating, named Gated Transformer Networks (GTN) for the multivariate time series classification problem. With the gating that merges two towers of Transformer which model the channel-wise and step-wise correlations respectively, we show how GTN is naturally and effectively suitable for the multivariate time series classification task. We conduct comprehensive experiments on thirteen dataset with full ablation study. Our results show that GTN is able to achieve competing results with current state-of-the-art deep learning models. We also explored the attention map for the natural interpretability of GTN on time series modeling. Our preliminary results provide a strong baseline for the Transformer Networks on multivariate time series classification task and grounds the foundation for future research.",0
"The community has extensively researched the use of deep learning models, specifically convolutional networks and LSTM, for time series classification. These models have been applied in various domains such as healthcare, finance, industrial engineering, and IoT. Recently, Transformer Networks have shown exceptional performance in natural language processing and computer vision tasks. In this study, we introduce Gated Transformer Networks (GTN), a simple extension of Transformer Networks with gating, for multivariate time series classification. By merging two Transformer towers that model channel-wise and step-wise correlations, GTN is a natural fit for multivariate time series classification. We conducted experiments on thirteen datasets with a full ablation study and found that GTN is competitive with current state-of-the-art deep learning models. In addition, we investigated the attention map for natural interpretability of GTN in time series modeling. Our results provide a strong baseline for Transformer Networks in multivariate time series classification and lay the groundwork for future research.",1
"Although transfer learning is proven to be effective in computer vision and natural language processing applications, it is rarely investigated in forecasting financial time series. Majority of existing works on transfer learning are based on single-source transfer learning due to the availability of open-access large-scale datasets. However, in financial domain, the lengths of individual time series are relatively short and single-source transfer learning models are less effective. Therefore, in this paper, we investigate multi-source deep transfer learning for financial time series. We propose two multi-source transfer learning methods namely Weighted Average Ensemble for Transfer Learning (WAETL) and Tree-structured Parzen Estimator Ensemble Selection (TPEES). The effectiveness of our approach is evaluated on financial time series extracted from stock markets. Experiment results reveal that TPEES outperforms other baseline methods on majority of multi-source transfer tasks.",0
"Despite the success of transfer learning in computer vision and natural language processing, its application in financial time series forecasting has been limited. This is due to the short length of individual time series in the financial domain, making single-source transfer learning less effective. Existing works have mainly focused on single-source transfer learning using large-scale datasets. In this study, we explore the use of multi-source deep transfer learning for financial time series and propose two methods: Weighted Average Ensemble for Transfer Learning (WAETL) and Tree-structured Parzen Estimator Ensemble Selection (TPEES). Our evaluation on stock market data shows that TPEES outperforms other baseline methods for multi-source transfer tasks.",1
"The dominant approaches to text representation in natural language rely on learning embeddings on massive corpora which have convenient properties such as compositionality and distance preservation. In this paper, we develop a novel method to learn a heavy-tailed embedding with desirable regularity properties regarding the distributional tails, which allows to analyze the points far away from the distribution bulk using the framework of multivariate extreme value theory. In particular, a classifier dedicated to the tails of the proposed embedding is obtained which performance outperforms the baseline. This classifier exhibits a scale invariance property which we leverage by introducing a novel text generation method for label preserving dataset augmentation. Numerical experiments on synthetic and real text data demonstrate the relevance of the proposed framework and confirm that this method generates meaningful sentences with controllable attribute, e.g. positive or negative sentiment.",0
"Text representation in natural language is typically achieved through learning embeddings on large corpora with convenient properties like compositionality and distance preservation. However, we present a new method in this paper that focuses on learning a heavy-tailed embedding with desirable regularity properties concerning distributional tails. This allows for the analysis of points further from the distribution bulk using multivariate extreme value theory. By creating a classifier dedicated to the tails of this embedding, we outperform the baseline in terms of performance. This classifier also exhibits scale invariance, which we utilize to introduce a new text generation approach for label preserving dataset augmentation. Numerical experiments on both synthetic and real text data confirm the relevance of this framework and the ability to generate meaningful sentences with controllable attributes such as positive or negative sentiment.",1
"Curriculum learning (CL) is a training strategy that trains a machine learning model from easier data to harder data, which imitates the meaningful learning order in human curricula. As an easy-to-use plug-in, the CL strategy has demonstrated its power in improving the generalization capacity and convergence rate of various models in a wide range of scenarios such as computer vision and natural language processing etc. In this survey article, we comprehensively review CL from various aspects including motivations, definitions, theories, and applications. We discuss works on curriculum learning within a general CL framework, elaborating on how to design a manually predefined curriculum or an automatic curriculum. In particular, we summarize existing CL designs based on the general framework of Difficulty Measurer+Training Scheduler and further categorize the methodologies for automatic CL into four groups, i.e., Self-paced Learning, Transfer Teacher, RL Teacher, and Other Automatic CL. We also analyze principles to select different CL designs that may benefit practical applications. Finally, we present our insights on the relationships connecting CL and other machine learning concepts including transfer learning, meta-learning, continual learning and active learning, etc., then point out challenges in CL as well as potential future research directions deserving further investigations.",0
"The training strategy known as Curriculum Learning (CL) involves training a machine learning model with progressively more difficult data, following the order in which humans typically learn. This approach has been shown to improve the generalization capacity and convergence rate of various models across different fields, such as computer vision and natural language processing. In this survey, we provide a comprehensive overview of CL, covering its motivations, definitions, theories, and applications. We explore the different ways in which a curriculum can be designed, either through manual or automatic means, with the latter further categorized into Self-paced Learning, Transfer Teacher, RL Teacher, and Other Automatic CL. We also analyze the principles that underlie selecting different CL designs based on their potential benefits to practical applications. Finally, we discuss the relationships between CL and other machine learning concepts, such as transfer learning, meta-learning, continual learning, and active learning, while identifying the challenges and potential research directions for CL going forward.",1
"In video object tracking, there exist rich temporal contexts among successive frames, which have been largely overlooked in existing trackers. In this work, we bridge the individual video frames and explore the temporal contexts across them via a transformer architecture for robust object tracking. Different from classic usage of the transformer in natural language processing tasks, we separate its encoder and decoder into two parallel branches and carefully design them within the Siamese-like tracking pipelines. The transformer encoder promotes the target templates via attention-based feature reinforcement, which benefits the high-quality tracking model generation. The transformer decoder propagates the tracking cues from previous templates to the current frame, which facilitates the object searching process. Our transformer-assisted tracking framework is neat and trained in an end-to-end manner. With the proposed transformer, a simple Siamese matching approach is able to outperform the current top-performing trackers. By combining our transformer with the recent discriminative tracking pipeline, our method sets several new state-of-the-art records on prevalent tracking benchmarks.",0
"The current video object tracking methods tend to ignore the rich temporal contexts present among successive frames, which can affect the accuracy of the tracking. This study proposes a transformer architecture to bridge the gap between individual frames and explore the temporal contexts across them for more robust tracking. In contrast to the traditional use of the transformer in natural language processing tasks, the encoder and decoder are separated into two parallel branches and carefully designed within Siamese-like tracking pipelines. The transformer encoder reinforces the target templates through attention-based feature enhancement, leading to the creation of high-quality tracking models. Meanwhile, the transformer decoder propagates the tracking cues from previous templates to the current frame, facilitating the object searching process. The proposed transformer-assisted tracking framework is trained in an end-to-end manner and provides better results than the current top-performing trackers. When combined with the recent discriminative tracking pipeline, our method sets several new state-of-the-art records on prevalent tracking benchmarks.",1
"It is encouraged to see that progress has been made to bridge videos and natural language. However, mainstream video captioning methods suffer from slow inference speed due to the sequential manner of autoregressive decoding, and prefer generating generic descriptions due to the insufficient training of visual words (e.g., nouns and verbs) and inadequate decoding paradigm. In this paper, we propose a non-autoregressive decoding based model with a coarse-to-fine captioning procedure to alleviate these defects. In implementations, we employ a bi-directional self-attention based network as our language model for achieving inference speedup, based on which we decompose the captioning procedure into two stages, where the model has different focuses. Specifically, given that visual words determine the semantic correctness of captions, we design a mechanism of generating visual words to not only promote the training of scene-related words but also capture relevant details from videos to construct a coarse-grained sentence ""template"". Thereafter, we devise dedicated decoding algorithms that fill in the ""template"" with suitable words and modify inappropriate phrasing via iterative refinement to obtain a fine-grained description. Extensive experiments on two mainstream video captioning benchmarks, i.e., MSVD and MSR-VTT, demonstrate that our approach achieves state-of-the-art performance, generates diverse descriptions, and obtains high inference efficiency. Our code is available at https://github.com/yangbang18/Non-Autoregressive-Video-Captioning.",0
"Although progress has been made in bridging videos and natural language, mainstream video captioning methods face challenges such as slow inference speed due to autoregressive decoding and generating generic descriptions due to insufficient training of visual words and inadequate decoding paradigm. This paper proposes a non-autoregressive decoding based model with a coarse-to-fine captioning procedure to address these issues. A bi-directional self-attention based network is employed to achieve inference speedup, and the captioning procedure is decomposed into two stages. Visual words are generated to promote the training of scene-related words and capture relevant details from videos to construct a coarse-grained sentence ""template"". Dedicated decoding algorithms are then used to fill in the ""template"" with suitable words and refine inappropriate phrasing via iteration to obtain a fine-grained description. Extensive experiments on MSVD and MSR-VTT benchmarks demonstrate that this approach achieves state-of-the-art performance, generates diverse descriptions, and achieves high inference efficiency. The code is available at https://github.com/yangbang18/Non-Autoregressive-Video-Captioning.",1
"Cognitive grammar suggests that the acquisition of language grammar is grounded within visual structures. While grammar is an essential representation of natural language, it also exists ubiquitously in vision to represent the hierarchical part-whole structure. In this work, we study grounded grammar induction of vision and language in a joint learning framework. Specifically, we present VLGrammar, a method that uses compound probabilistic context-free grammars (compound PCFGs) to induce the language grammar and the image grammar simultaneously. We propose a novel contrastive learning framework to guide the joint learning of both modules. To provide a benchmark for the grounded grammar induction task, we collect a large-scale dataset, \textsc{PartIt}, which contains human-written sentences that describe part-level semantics for 3D objects. Experiments on the \textsc{PartIt} dataset show that VLGrammar outperforms all baselines in image grammar induction and language grammar induction. The learned VLGrammar naturally benefits related downstream tasks. Specifically, it improves the image unsupervised clustering accuracy by 30\%, and performs well in image retrieval and text retrieval. Notably, the induced grammar shows superior generalizability by easily generalizing to unseen categories.",0
"According to cognitive grammar, language grammar acquisition is rooted in visual structures. Grammar is a fundamental aspect of natural language and is also prevalent in vision as it represents the hierarchical part-whole structure. This study explores the joint learning of grounded grammar induction in vision and language. The method used is VLGrammar, which employs compound probabilistic context-free grammars to induce both language and image grammar simultaneously. The joint learning of both modules is guided by a novel contrastive learning framework. To establish a benchmark for the grounded grammar induction task, a large-scale dataset called \textsc{PartIt} is collected, which comprises human-written sentences describing part-level semantics for 3D objects. Results from experiments on the \textsc{PartIt} dataset demonstrate that VLGrammar outperforms all baselines in both image and language grammar induction. The learned VLGrammar also enhances related downstream tasks, such as image unsupervised clustering accuracy, image and text retrieval. Furthermore, the induced grammar exhibits superior generalizability by easily extending to unseen categories.",1
"Humans learn from life events to form intuitions towards the understanding of visual environments and languages. Envision that you are instructed by a high-level instruction, ""Go to the bathroom in the master bedroom and replace the blue towel on the left wall"", what would you possibly do to carry out the task? Intuitively, we comprehend the semantics of the instruction to form an overview of where a bathroom is and what a blue towel is in mind; then, we navigate to the target location by consistently matching the bathroom appearance in mind with the current scene. In this paper, we present an agent that mimics such human behaviors. Specifically, we focus on the Remote Embodied Visual Referring Expression in Real Indoor Environments task, called REVERIE, where an agent is asked to correctly localize a remote target object specified by a concise high-level natural language instruction, and propose a two-stage training pipeline. In the first stage, we pretrain the agent with two cross-modal alignment sub-tasks, namely the Scene Grounding task and the Object Grounding task. The agent learns where to stop in the Scene Grounding task and what to attend to in the Object Grounding task respectively. Then, to generate action sequences, we propose a memory-augmented attentive action decoder to smoothly fuse the pre-trained vision and language representations with the agent's past memory experiences. Without bells and whistles, experimental results show that our method outperforms previous state-of-the-art(SOTA) significantly, demonstrating the effectiveness of our method.",0
"Intuitions about visual environments and languages are formed by humans through learning from life events. Suppose you receive a high-level instruction to replace a blue towel in the master bedroom's left wall. What would be your approach to carry out the task? Our understanding of the instruction allows us to form a mental image of the bathroom's location and the blue towel's appearance. We then match this mental image with the current scene to navigate to the target location. In this research, we introduce an agent that imitates such human behavior. Our focus is on the REVERIE task, where the agent is required to locate a remote target object based on a concise high-level natural language instruction. We propose a two-stage training pipeline, starting with pretraining the agent with two cross-modal alignment sub-tasks called Scene Grounding and Object Grounding tasks. The agent learns where to stop in the former and what to attend to in the latter. To generate action sequences, we propose a memory-augmented attentive action decoder that fuses pre-trained vision and language representations with the agent's past memory experiences. Our experimental results show that our method significantly outperforms previous state-of-the-art (SOTA), demonstrating its effectiveness.",1
"Deep models trained in supervised mode have achieved remarkable success on a variety of tasks. When labeled samples are limited, self-supervised learning (SSL) is emerging as a new paradigm for making use of large amounts of unlabeled samples. SSL has achieved promising performance on natural language and image learning tasks. Recently, there is a trend to extend such success to graph data using graph neural networks (GNNs). In this survey, we provide a unified review of different ways of training GNNs using SSL. Specifically, we categorize SSL methods into contrastive and predictive models. In either category, we provide a unified framework for methods as well as how these methods differ in each component under the framework. Our unified treatment of SSL methods for GNNs sheds light on the similarities and differences of various methods, setting the stage for developing new methods and algorithms. We also summarize different SSL settings and the corresponding datasets used in each setting. To facilitate methodological development and empirical comparison, we develop a standardized testbed for SSL in GNNs, including implementations of common baseline methods, datasets, and evaluation metrics.",0
"Supervised deep models have achieved impressive results on a range of tasks, but when there are limited labeled samples available, self-supervised learning (SSL) is becoming a new approach to leverage vast amounts of unlabeled data. SSL has shown promise in natural language and image learning tasks, and now there is a growing trend in applying it to graph data using graph neural networks (GNNs). This survey offers a comprehensive review of different SSL techniques for training GNNs, which are divided into contrastive and predictive models. We provide a unified framework to compare methods and highlight differences in each component. Our analysis sheds light on similarities and differences between methods, paving the way for new algorithm development. We also summarize different SSL settings and corresponding datasets, and we develop a standardized testbed for SSL in GNNs, including common baseline methods, datasets, and evaluation metrics to facilitate methodological development and empirical comparison.",1
"Decompilation is the procedure of transforming binary programs into a high-level representation, such as source code, for human analysts to examine. While modern decompilers can reconstruct and recover much information that is discarded during compilation, inferring variable names is still extremely difficult. Inspired by recent advances in natural language processing, we propose a novel solution to infer variable names in decompiled code based on Masked Language Modeling, Byte-Pair Encoding, and neural architectures such as Transformers and BERT. Our solution takes \textit{raw} decompiler output, the less semantically meaningful code, as input, and enriches it using our proposed \textit{finetuning} technique, Constrained Masked Language Modeling. Using Constrained Masked Language Modeling introduces the challenge of predicting the number of masked tokens for the original variable name. We address this \textit{count of token prediction} challenge with our post-processing algorithm. Compared to the state-of-the-art approaches, our trained VarBERT model is simpler and of much better performance. We evaluated our model on an existing large-scale data set with 164,632 binaries and showed that it can predict variable names identical to the ones present in the original source code up to 84.15\% of the time.",0
"Decompilation refers to the conversion of binary programs into a high-level format, such as source code, which can be analyzed by human experts. Although modern decompilers can retrieve much of the information that is lost during compilation, inferring variable names remains a formidable task. To address this issue, we propose a novel approach that leverages recent advances in natural language processing, including Masked Language Modeling, Byte-Pair Encoding, and neural architectures like Transformers and BERT. Our method takes raw decompiler output, which contains less semantically meaningful code, as input and uses our proposed finetuning technique, Constrained Masked Language Modeling, to enhance it. However, using Constrained Masked Language Modeling poses a challenge in predicting the number of masked tokens for the original variable name. To overcome this hurdle, we developed a post-processing algorithm. Compared to state-of-the-art methods, our trained VarBERT model is simpler and more effective. We evaluated our model on a large-scale dataset comprising 164,632 binaries and demonstrated that it can predict variable names with an accuracy of up to 84.15%, which matches those in the original source code.",1
"This paper describes an approach to solving the next destination city recommendation problem for a travel reservation system. We propose a two stages approach: a heuristic approach for candidates selection and an attention neural network model for candidates re-ranking. Our method was inspired by listwise learning-to-rank methods and recent developments in natural language processing and the transformer architecture in particular. We used this approach to solve the Booking.com recommendations challenge Our team achieved 5th place on the challenge using this method, with 0.555 accuracy@4 value on the closed part of the dataset.",0
"In this article, we present a solution to the problem of providing recommendations for the next destination city in a travel reservation system. Our proposed method consists of two stages: first, a heuristic approach is used to select candidates, and then an attention neural network model is utilized to re-rank them. Our approach draws inspiration from listwise learning-to-rank techniques, as well as recent advancements in natural language processing and the transformer architecture specifically. To address the Booking.com recommendations challenge, we applied this method and achieved 5th place, with a closed dataset accuracy@4 value of 0.555.",1
"Among image classification, skip and densely-connection-based networks have dominated most leaderboards. Recently, from the successful development of multi-head attention in natural language processing, it is sure that now is a time of either using a Transformer-like model or hybrid CNNs with attention. However, the former need a tremendous resource to train, and the latter is in the perfect balance in this direction. In this work, to make CNNs handle global and local information, we proposed UPANets, which equips channel-wise attention with a hybrid skip-densely-connection structure. Also, the extreme-connection structure makes UPANets robust with a smoother loss landscape. In experiments, UPANets surpassed most well-known and widely-used SOTAs with an accuracy of 96.47% in Cifar-10, 80.29% in Cifar-100, and 67.67% in Tiny Imagenet. Most importantly, these performances have high parameters efficiency and only trained in one customer-based GPU. We share implementing code of UPANets in https://github.com/hanktseng131415go/UPANets.",0
"Skip and densely-connection-based networks have been dominating most image classification leaderboards. However, with the successful development of multi-head attention in natural language processing, it has become apparent that it is now the era of either using a Transformer-like model or hybrid CNNs with attention. The former requires a significant amount of resources to train, while the latter strikes a perfect balance in this regard. To enable CNNs to handle both global and local information, we have proposed UPANets, which utilize a hybrid skip-densely-connection structure equipped with channel-wise attention. Additionally, the extreme-connection structure of UPANets makes them robust with a smoother loss landscape. In experiments, UPANets have outperformed the majority of well-known and widely-used SOTAs with an accuracy of 96.47% in Cifar-10, 80.29% in Cifar-100, and 67.67% in Tiny Imagenet. Most importantly, these performances are highly parameter-efficient and can be trained on a single customer-based GPU. We have shared the implementation code of UPANets on https://github.com/hanktseng131415go/UPANets.",1
"Convolutional video models have an order of magnitude larger computational complexity than their counterpart image-level models. Constrained by computational resources, there is no model or training method that can train long video sequences end-to-end. Currently, the main-stream method is to split a raw video into clips, leading to incomplete fragmentary temporal information flow. Inspired by natural language processing techniques dealing with long sentences, we propose to treat videos as serial fragments satisfying Markov property, and train it as a whole by progressively propagating information through the temporal dimension in multiple steps. This progressive training (PGT) method is able to train long videos end-to-end with limited resources and ensures the effective transmission of information. As a general and robust training method, we empirically demonstrate that it yields significant performance improvements on different models and datasets. As an illustrative example, the proposed method improves SlowOnly network by 3.7 mAP on Charades and 1.9 top-1 accuracy on Kinetics with negligible parameter and computation overhead. Code is available at https://github.com/BoPang1996/PGT.",0
"The computational complexity of convolutional video models is much greater than that of image-level models, making it impossible to train long video sequences end-to-end due to resource constraints. To overcome this, videos are treated as serial fragments following the Markov property and trained progressively through the temporal dimension in multiple steps. This approach, called progressive training (PGT), effectively propagates information and allows for end-to-end training of long videos with limited resources. PGT is a robust training method that improves the performance of different models and datasets. For instance, PGT enhances the SlowOnly network by 3.7 mAP on Charades and 1.9 top-1 accuracy on Kinetics with minimal parameter and computation overhead. The code for PGT is available at https://github.com/BoPang1996/PGT.",1
"Deep supervised learning has achieved great success in the last decade. However, its deficiencies of dependence on manual labels and vulnerability to attacks have driven people to explore a better solution. As an alternative, self-supervised learning attracts many researchers for its soaring performance on representation learning in the last several years. Self-supervised representation learning leverages input data itself as supervision and benefits almost all types of downstream tasks. In this survey, we take a look into new self-supervised learning methods for representation in computer vision, natural language processing, and graph learning. We comprehensively review the existing empirical methods and summarize them into three main categories according to their objectives: generative, contrastive, and generative-contrastive (adversarial). We further investigate related theoretical analysis work to provide deeper thoughts on how self-supervised learning works. Finally, we briefly discuss open problems and future directions for self-supervised learning. An outline slide for the survey is provided.",0
"In the past decade, deep supervised learning has been highly successful, but its reliance on manual labels and susceptibility to attacks has prompted researchers to seek better alternatives. Self-supervised learning has emerged as a popular option due to its exceptional performance in representation learning across various downstream tasks. This survey delves into the latest self-supervised learning methods for representation in computer vision, natural language processing, and graph learning. The review covers empirical methods, which are categorized into generative, contrastive, and generative-contrastive (adversarial) based on their objectives. The survey also delves into related theoretical analyses to gain a deeper understanding of self-supervised learning. Finally, the survey briefly discusses the open problems and future directions for self-supervised learning, and an outline slide is provided.",1
"Concentration inequalities are widely used for analyzing machine learning algorithms. However, current concentration inequalities cannot be applied to some of the most popular deep neural networks, notably in natural language processing. This is mostly due to the non-causal nature of such involved data, in the sense that each data point depends on other neighbor data points. In this paper, a framework for modeling non-causal random fields is provided and a Hoeffding-type concentration inequality is obtained for this framework. The proof of this result relies on a local approximation of the non-causal random field by a function of a finite number of i.i.d. random variables.",0
"Machine learning algorithms often rely on concentration inequalities for analysis, but some of the most popular deep neural networks used in natural language processing cannot be analyzed using current concentration inequalities. This is due to the non-causal nature of the data, where each data point is dependent on neighboring data points. This paper presents a framework for modeling non-causal random fields and derives a Hoeffding-type concentration inequality for this framework. The proof is based on approximating the non-causal random field locally with a finite number of i.i.d. random variables.",1
"The attention mechanism has been widely used in deep neural networks as a model component. By now, it has become a critical building block in many state-of-the-art natural language models. Despite its great success established empirically, the working mechanism of attention has not been investigated at a sufficient theoretical depth to date. In this paper, we set up a simple text classification task and study the dynamics of training a simple attention-based classification model using gradient descent. In this setting, we show that, for the discriminative words that the model should attend to, a persisting identity exists relating its embedding and the inner product of its key and the query. This allows us to prove that training must converge to attending to the discriminative words when the attention output is classified by a linear classifier. Experiments are performed, which validate our theoretical analysis and provide further insights.",0
"The use of the attention mechanism in deep neural networks is widespread, serving as a crucial element in modern natural language models. Despite its practical success, the theoretical workings of attention have not been sufficiently explored. To address this, we present a straightforward text classification task to investigate the training dynamics of a simple attention-based model using gradient descent. Our findings reveal that a persistent identity exists between the embedding of discriminative words and the inner product of their key and query, allowing us to demonstrate that the model must converge towards these words when the attention output is categorized by a linear classifier. Our theoretical analysis is supported by experiments that further enrich our understanding of the attention mechanism.",1
"Language model pre-training (LMPT) has achieved remarkable results in natural language understanding. However, LMPT is much less successful in non-natural language domains like protein sequences, revealing a crucial discrepancy between the various sequential domains. Here, we posit that while LMPT can effectively model per-token relations, it fails at modeling per-sequence relations in non-natural language domains. To this end, we develop a framework that couples LMPT with deep structure-preserving metric learning to produce richer embeddings than can be obtained from LMPT alone. We examine new and existing pre-training models in this framework and theoretically analyze the framework overall. We also design experiments on a variety of synthetic datasets and new graph-augmented datasets of proteins and scientific abstracts. Our approach offers notable performance improvements on downstream tasks, including prediction of protein remote homology and classification of citation intent.",0
"Although Language Model Pre-Training (LMPT) has been highly successful in natural language understanding, its effectiveness in non-natural language domains, such as protein sequences, has been limited. This discrepancy highlights the importance of per-sequence relations in non-natural language domains, which LMPT doesn't model well. To address this issue, we propose a framework that combines LMPT with deep structure-preserving metric learning to produce more robust embeddings than those produced by LMPT alone. We evaluate this framework by analyzing new and existing pre-training models and conducting experiments on both synthetic and graph-augmented datasets of proteins and scientific abstracts. Our approach leads to significant improvements in downstream tasks, including the prediction of protein remote homology and the classification of citation intent.",1
"Multi-Task Learning (MTL) networks have emerged as a promising method for transferring learned knowledge across different tasks. However, MTL must deal with challenges such as: overfitting to low resource tasks, catastrophic forgetting, and negative task transfer, or learning interference. Often, in Natural Language Processing (NLP), a separate model per task is needed to obtain the best performance. However, many fine-tuning approaches are both parameter inefficient, i.e., potentially involving one new model per task, and highly susceptible to losing knowledge acquired during pretraining. We propose a novel Transformer architecture consisting of a new conditional attention mechanism as well as a set of task-conditioned modules that facilitate weight sharing. Through this construction, we achieve more efficient parameter sharing and mitigate forgetting by keeping half of the weights of a pretrained model fixed. We also use a new multi-task data sampling strategy to mitigate the negative effects of data imbalance across tasks. Using this approach, we are able to surpass single task fine-tuning methods while being parameter and data efficient (using around 66% of the data for weight updates). Compared to other BERT Large methods on GLUE, our 8-task model surpasses other Adapter methods by 2.8% and our 24-task model outperforms by 0.7-1.0% models that use MTL and single task fine-tuning. We show that a larger variant of our single multi-task model approach performs competitively across 26 NLP tasks and yields state-of-the-art results on a number of test and development sets. Our code is publicly available at https://github.com/CAMTL/CA-MTL.",0
"Multi-Task Learning (MTL) networks have shown promise in transferring learned knowledge across various tasks, but they face challenges such as overfitting, catastrophic forgetting, and negative task transfer. In Natural Language Processing (NLP), a separate model per task is often necessary for optimal performance, but this approach is parameter inefficient and susceptible to knowledge loss. To address these issues, we propose a novel Transformer architecture with a conditional attention mechanism and task-conditioned modules to facilitate weight sharing. This approach achieves more efficient parameter sharing and mitigates forgetting by keeping half of the weights of a pre-trained model fixed. We also use a new multi-task data sampling strategy to mitigate the negative effects of data imbalance across tasks. Our approach outperforms other methods on GLUE and achieves state-of-the-art results on 26 NLP tasks. Our code is available at https://github.com/CAMTL/CA-MTL.",1
"Machine Learning in general and Deep Learning in particular has gained much interest in the recent decade and has shown significant performance improvements for many Computer Vision or Natural Language Processing tasks. In order to deal with databases which have just a small amount of training samples or to deal with models which have large amount of parameters, the regularization is indispensable. In this paper, we enforce the manifold preservation (manifold learning) from the original data into latent presentation by using ""manifold attack"". The later is inspired in a fashion of adversarial learning : finding virtual points that distort mostly the manifold preservation then using these points as supplementary samples to train the model. We show that our approach of regularization provides improvements for the accuracy rate and for the robustness to adversarial examples.",0
"Over the past decade, there has been a growing interest in Machine Learning, especially Deep Learning, which has proven to enhance performance in several Computer Vision and Natural Language Processing tasks. Regularization is crucial to manage databases with few training samples or models with numerous parameters. This study incorporates ""manifold attack,"" which ensures that the manifold preservation from the original data is maintained in the latent presentation. This approach is inspired by adversarial learning, where virtual points are identified that significantly alter manifold preservation and are used as additional samples to train the model. The results demonstrate that this regularization approach enhances accuracy rates and improves resilience to adversarial examples.",1
"Data augmentation is an effective technique to improve the generalization of deep neural networks. However, previous data augmentation methods usually treat the augmented samples equally without considering their individual impacts on the model. To address this, for the augmented samples from the same training example, we propose to assign different weights to them. We construct the maximal expected loss which is the supremum over any reweighted loss on augmented samples. Inspired by adversarial training, we minimize this maximal expected loss (MMEL) and obtain a simple and interpretable closed-form solution: more attention should be paid to augmented samples with large loss values (i.e., harder examples). Minimizing this maximal expected loss enables the model to perform well under any reweighting strategy. The proposed method can generally be applied on top of any data augmentation methods. Experiments are conducted on both natural language understanding tasks with token-level data augmentation, and image classification tasks with commonly-used image augmentation techniques like random crop and horizontal flip. Empirical results show that the proposed method improves the generalization performance of the model.",0
"The generalization of deep neural networks can be enhanced by using data augmentation, but previous methods did not consider the impact of each augmented sample on the model. To rectify this, we suggest assigning varying weights to augmented samples from the same training example. We establish the maximal expected loss, which represents the highest possible loss for any reweighted loss on augmented samples. Our approach is based on adversarial training, and we minimize this maximal expected loss (MMEL) to obtain a straightforward and understandable solution. Augmented samples with higher loss values (i.e., harder examples) should receive more attention. Minimizing this maximal expected loss aids the model in performing well under various reweighting strategies. This method can be generally applied to any data augmentation techniques. We conducted experiments on natural language understanding tasks and image classification tasks, using token-level data augmentation and commonly-used image augmentation techniques such as random crop and horizontal flip. The empirical results show that our proposed method enhances the model's generalization performance.",1
"We aim to address the problem of Natural Language Video Localization (NLVL)-localizing the video segment corresponding to a natural language description in a long and untrimmed video. State-of-the-art NLVL methods are almost in one-stage fashion, which can be typically grouped into two categories: 1) anchor-based approach: it first pre-defines a series of video segment candidates (e.g., by sliding window), and then does classification for each candidate; 2) anchor-free approach: it directly predicts the probabilities for each video frame as a boundary or intermediate frame inside the positive segment. However, both kinds of one-stage approaches have inherent drawbacks: the anchor-based approach is susceptible to the heuristic rules, further limiting the capability of handling videos with variant length. While the anchor-free approach fails to exploit the segment-level interaction thus achieving inferior results. In this paper, we propose a novel Boundary Proposal Network (BPNet), a universal two-stage framework that gets rid of the issues mentioned above. Specifically, in the first stage, BPNet utilizes an anchor-free model to generate a group of high-quality candidate video segments with their boundaries. In the second stage, a visual-language fusion layer is proposed to jointly model the multi-modal interaction between the candidate and the language query, followed by a matching score rating layer that outputs the alignment score for each candidate. We evaluate our BPNet on three challenging NLVL benchmarks (i.e., Charades-STA, TACoS and ActivityNet-Captions). Extensive experiments and ablative studies on these datasets demonstrate that the BPNet outperforms the state-of-the-art methods.",0
"Our objective is to tackle the issue of Natural Language Video Localization (NLVL), which involves identifying the video segment corresponding to a natural language description in a long and untrimmed video. Current NLVL techniques are mostly one-stage methods and can be categorized into two groups: anchor-based and anchor-free approaches. The former involves predefining a set of video segment candidates (e.g., through sliding window) and then classifying each candidate, while the latter directly predicts the probabilities for each video frame as a boundary or intermediate frame within the positive segment. However, both these one-stage approaches have their limitations. The anchor-based approach is prone to heuristic rules and hence cannot handle videos with varying lengths effectively. On the other hand, the anchor-free approach fails to leverage the segment-level interaction, leading to inferior results. To address these issues, we propose a new two-stage framework called the Boundary Proposal Network (BPNet). In the first stage, BPNet uses an anchor-free model to generate a set of high-quality candidate video segments with their boundaries. In the second stage, we introduce a visual-language fusion layer to model the multi-modal interaction between the candidate and the language query, followed by a matching score rating layer that outputs the alignment score for each candidate. We evaluate the performance of BPNet on three challenging NLVL benchmarks (Charades-STA, TACoS, and ActivityNet-Captions) and conduct extensive experiments and ablative studies on these datasets. Our results demonstrate that BPNet outperforms existing state-of-the-art methods.",1
"Negotiation, as an essential and complicated aspect of online shopping, is still challenging for an intelligent agent. To that end, we propose the Price Negotiator, a modular deep neural network that addresses the unsolved problems in recent studies by (1) considering images of the items as a crucial, though neglected, source of information in a negotiation, (2) heuristically finding the most similar items from an external online source to predict the potential value and an acceptable agreement price, (3) predicting a general price-based action at each turn which is fed into the language generator to output the supporting natural language, and (4) adjusting the prices based on the predicted actions. Empirically, we show that our model, that is trained in both supervised and reinforcement learning setting, significantly improves negotiation on the CraigslistBargain dataset, in terms of the agreement price, price consistency, and dialogue quality.",0
"Online shopping involves a complex and crucial aspect known as negotiation, which still poses a challenge to intelligent agents. Therefore, we introduce the Price Negotiator, a modular deep neural network that addresses the unresolved issues in recent studies by (1) recognizing the significance of item images as an important source of information during negotiation, (2) using heuristics to identify the most similar items from an external online source to predict their potential value and an acceptable agreement price, (3) generating natural language that supports the price-based actions predicted at each turn, and (4) making adjustments to the prices based on the predicted actions. Our model, which is trained through supervised and reinforcement learning, demonstrates a significant improvement in negotiation on the CraigslistBargain dataset, specifically in terms of agreement price, price consistency, and dialogue quality.",1
"We present a locality preserving loss (LPL) that improves the alignment between vector space embeddings while separating uncorrelated representations. Given two pretrained embedding manifolds, LPL optimizes a model to project an embedding and maintain its local neighborhood while aligning one manifold to another. This reduces the overall size of the dataset required to align the two in tasks such as cross-lingual word alignment. We show that the LPL-based alignment between input vector spaces acts as a regularizer, leading to better and consistent accuracy than the baseline, especially when the size of the training set is small. We demonstrate the effectiveness of LPL optimized alignment on semantic text similarity (STS), natural language inference (SNLI), multi-genre language inference (MNLI) and cross-lingual word alignment(CLA) showing consistent improvements, finding up to 16% improvement over our baseline in lower resource settings.",0
"Introducing the Locality Preserving Loss (LPL), a technique that enhances the alignment between vector space embeddings while separating uncorrelated representations. LPL optimizes a model to maintain the local neighborhood of an embedding while aligning one manifold to another, given two pretrained embedding manifolds. This results in a reduction in the dataset size required to align the two manifolds, thereby facilitating cross-lingual word alignment. Our study demonstrates that the LPL-based alignment between input vector spaces acts as a regularizer, leading to consistent accuracy improvements over the baseline approach, particularly when training data is limited. We validate the effectiveness of LPL optimized alignment on various tasks, including semantic text similarity (STS), natural language inference (SNLI), multi-genre language inference (MNLI), and cross-lingual word alignment (CLA), exhibiting consistent enhancements of up to 16% over our baseline in low-resource settings.",1
"Integration is indispensable, not only in mathematics, but also in a wide range of other fields. A deep learning method has recently been developed and shown to be capable of integrating mathematical functions that could not previously be integrated on a computer. However, that method treats integration as equivalent to natural language translation and does not reflect mathematical information. In this study, we adjusted the learning model to take mathematical information into account and developed a wide range of learning models that learn the order of numerical operations more robustly. In this way, we achieved a 98.80% correct answer rate with symbolic integration, a higher rate than that of any existing method. We judged the correctness of the integration based on whether the derivative of the primitive function was consistent with the integrand. By building an integrated model based on this strategy, we achieved a 99.79% rate of correct answers with symbolic integration.",0
"Integration is a crucial component not just in mathematics, but also in various other fields. A new deep learning technique has been created which has the ability to integrate mathematical functions that were previously impossible to integrate by computer. Nonetheless, this method views integration as equivalent to natural language translation and does not consider mathematical data. In this research, we have modified the learning model to factor in mathematical information and have developed a wide range of learning models that learn the sequence of numerical operations more strongly. This has enabled us to attain a correct answer rate of 98.80% with symbolic integration, which is greater than any existing method. We evaluated the correctness of the integration based on the consistency of the derivative of the primitive function with the integrand. By creating an integrated model using this approach, we achieved a 99.79% rate of correct answers with symbolic integration.",1
"Transformer is a powerful tool for many natural language tasks which is based on self-attention, a mechanism that encodes the dependence of other tokens on each specific token, but the computation of self-attention is a bottleneck due to its quadratic time complexity. There are various approaches to reduce the time complexity and approximation of matrix is one such. In Nystr\""omformer, the authors used Nystr\""om based method for approximation of softmax. The Nystr\""om method generates a fast approximation to any large-scale symmetric positive semidefinite (SPSD) matrix using only a few columns of the SPSD matrix. However, since the Nystr\""om approximation is low-rank when the spectrum of the SPSD matrix decays slowly, the Nystr\""om approximation is of low accuracy. Here an alternative method is proposed for approximation which has a much stronger error bound than the Nystr\""om method. The time complexity of this same as Nystr\""omformer which is $O\left({n}\right)$.",0
"The Transformer is a potent tool for a variety of natural language tasks, utilizing the self-attention mechanism to encode the dependence of other tokens on each specific token. However, the computation of self-attention poses a bottleneck due to its quadratic time complexity. To address this issue, various approaches have been proposed, including matrix approximation. In the case of Nystr\""omformer, the authors utilize the Nystr\""om-based method to approximate softmax. This method generates a swift approximation of large-scale symmetric positive semidefinite (SPSD) matrices, utilizing only a few columns of the SPSD matrix. Nonetheless, when the spectrum of the SPSD matrix decays slowly, the Nystr\""om approximation is of low accuracy due to its low-rank nature. An alternative method is suggested here, which has a stronger error bound than the Nystr\""om method and a time complexity of $O\left({n}\right)$, similar to that of Nystr\""omformer.",1
"Due to the rapid emergence of short videos and the requirement for content understanding and creation, the video captioning task has received increasing attention in recent years. In this paper, we convert traditional video captioning task into a new paradigm, \ie, Open-book Video Captioning, which generates natural language under the prompts of video-content-relevant sentences, not limited to the video itself. To address the open-book video captioning problem, we propose a novel Retrieve-Copy-Generate network, where a pluggable video-to-text retriever is constructed to retrieve sentences as hints from the training corpus effectively, and a copy-mechanism generator is introduced to extract expressions from multi-retrieved sentences dynamically. The two modules can be trained end-to-end or separately, which is flexible and extensible. Our framework coordinates the conventional retrieval-based methods with orthodox encoder-decoder methods, which can not only draw on the diverse expressions in the retrieved sentences but also generate natural and accurate content of the video. Extensive experiments on several benchmark datasets show that our proposed approach surpasses the state-of-the-art performance, indicating the effectiveness and promising of the proposed paradigm in the task of video captioning.",0
"In recent years, the video captioning task has gained increasing attention due to the rise of short videos and the need for content understanding and creation. In this study, we introduce a new paradigm called Open-book Video Captioning, which aims to generate natural language under the prompts of video-content-relevant sentences, not limited to the video itself. To address this problem, we propose a novel Retrieve-Copy-Generate network that consists of a pluggable video-to-text retriever and a copy-mechanism generator. These modules can be trained together or separately, allowing for flexibility and extensibility. Our framework combines conventional retrieval-based methods with orthodox encoder-decoder methods, resulting in natural and accurate content generation. Extensive experiments on several benchmark datasets demonstrate that our proposed approach outperforms the state-of-the-art methods, highlighting the effectiveness and potential of the Open-book Video Captioning paradigm.",1
"In meta-learning, the knowledge learned from previous tasks is transferred to new ones, but this transfer only works if tasks are related. Sharing information between unrelated tasks might hurt performance, and it is unclear how to transfer knowledge across tasks with a hierarchical structure. Our research extends a model agnostic meta-learning model, MAML, by exploiting hierarchical task relationships. Our algorithm, TreeMAML, adapts the model to each task with a few gradient steps, but the adaptation follows the hierarchical tree structure: in each step, gradients are pooled across tasks clusters, and subsequent steps follow down the tree. We also implement a clustering algorithm that generates the tasks tree without previous knowledge of the task structure, allowing us to make use of implicit relationships between the tasks. We show that the new algorithm, which we term TreeMAML, performs better than MAML when the task structure is hierarchical for synthetic experiments. To study the performance of the method in real-world data, we apply this method to Natural Language Understanding, we use our algorithm to finetune Language Models taking advantage of the language phylogenetic tree. We show that TreeMAML improves the state of the art results for cross-lingual Natural Language Inference. This result is useful, since most languages in the world are under-resourced and the improvement on cross-lingual transfer allows the internationalization of NLP models. This results open the window to use this algorithm in other real-world hierarchical datasets.",0
"Meta-learning involves transferring knowledge from previous tasks to new ones, but this only works if the tasks are related. Sharing information between unrelated tasks can hinder performance and transferring knowledge across tasks with a hierarchical structure is unclear. Our research enhances the model agnostic meta-learning model, MAML, by leveraging hierarchical task relationships. Our algorithm, TreeMAML, modifies the model to fit each task with a few gradient steps, following the hierarchical tree structure. We also use a clustering algorithm to create the tasks tree without prior knowledge of the task structure, allowing us to use implicit relationships between tasks. Synthetic experiments show that TreeMAML outperforms MAML when the task structure is hierarchical. We apply this method to Natural Language Understanding, where we use our algorithm to fine-tune Language Models based on the language phylogenetic tree. Our algorithm improves the state of the art results for cross-lingual Natural Language Inference, which is beneficial for under-resourced languages and enables the internationalization of NLP models. This result suggests that our algorithm can be applied to other real-world hierarchical datasets.",1
"The classification accuracy of deep learning models depends not only on the size of their training sets, but also on the quality of their labels. In medical image classification, large-scale datasets are becoming abundant, but their labels will be noisy when they are automatically extracted from radiology reports using natural language processing tools. Given that deep learning models can easily overfit these noisy-label samples, it is important to study training approaches that can handle label noise. In this paper, we adapt a state-of-the-art (SOTA) noisy-label multi-class training approach to learn a multi-label classifier for the dataset Chest X-ray14, which is a large scale dataset known to contain label noise in the training set. Given that this dataset also has label noise in the testing set, we propose a new theoretically sound method to estimate the performance of the model on a hidden clean testing data, given the result on the noisy testing data. Using our clean data performance estimation, we notice that the majority of label noise on Chest X-ray14 is present in the class 'No Finding', which is intuitively correct because this is the most likely class to contain one or more of the 14 diseases due to labelling mistakes.",0
"Deep learning models' ability to classify accurately is influenced not only by the size of their training sets but also by the quality of their labels. Although medical image classification has abundant large-scale datasets, their labels are usually noisy when they are automatically extracted using natural language processing tools from radiology reports. This noise can cause deep learning models to overfit, making it vital to explore training approaches that can handle label noise. In this study, we adapted a state-of-the-art multi-class training approach for noisy labels to develop a multi-label classifier for the Chest X-ray14 dataset, which is known to have label noise in the training set. We also proposed a new method to estimate the model's performance on hidden clean testing data, given the result obtained on the noisy testing data, as this dataset also has label noise in the testing set. Our clean data performance estimation revealed that the majority of label noise in Chest X-ray14 occurred in the class 'No Finding,' which is understandable since this class is most likely to contain one or more of the 14 diseases due to labeling errors.",1
"The Traveling Salesman Problem (TSP) is the most popular and most studied combinatorial problem, starting with von Neumann in 1951. It has driven the discovery of several optimization techniques such as cutting planes, branch-and-bound, local search, Lagrangian relaxation, and simulated annealing. The last five years have seen the emergence of promising techniques where (graph) neural networks have been capable to learn new combinatorial algorithms. The main question is whether deep learning can learn better heuristics from data, i.e. replacing human-engineered heuristics? This is appealing because developing algorithms to tackle efficiently NP-hard problems may require years of research, and many industry problems are combinatorial by nature. In this work, we propose to adapt the recent successful Transformer architecture originally developed for natural language processing to the combinatorial TSP. Training is done by reinforcement learning, hence without TSP training solutions, and decoding uses beam search. We report improved performances over recent learned heuristics with an optimal gap of 0.004% for TSP50 and 0.39% for TSP100.",0
"The Traveling Salesman Problem (TSP) has been extensively researched since von Neumann introduced it in 1951, making it the most prominent combinatorial problem. Its difficulty has led to the development of various optimization techniques like cutting planes, branch-and-bound, local search, Lagrangian relaxation, and simulated annealing. Recently, graph neural networks have shown potential in learning new combinatorial algorithms. This approach aims to replace human-engineered heuristics with deep learning models that can learn better heuristics from data. This is particularly useful for solving NP-hard problems, which can take years of research to develop efficient algorithms for. In this study, we propose to apply the Transformer architecture, initially developed for natural language processing, to solve the TSP. We employ reinforcement learning for training without TSP training solutions, and beam search for decoding. Our results show improved performances over recent learned heuristics, with an optimal gap of 0.004% for TSP50 and 0.39% for TSP100.",1
"Visual Question Answering (VQA) is an extremely stimulating and challenging research area where Computer Vision (CV) and Natural Language Processig (NLP) have recently met. In image captioning and video summarization, the semantic information is completely contained in still images or video dynamics, and it has only to be mined and expressed in a human-consistent way. Differently from this, in VQA semantic information in the same media must be compared with the semantics implied by a question expressed in natural language, doubling the artificial intelligence-related effort. Some recent surveys about VQA approaches have focused on methods underlying either the image-related processing or the verbal-related one, or on the way to consistently fuse the conveyed information. Possible applications are only suggested, and, in fact, most cited works rely on general-purpose datasets that are used to assess the building blocks of a VQA system. This paper rather considers the proposals that focus on real-world applications, possibly using as benchmarks suitable data bound to the application domain. The paper also reports about some recent challenges in VQA research.",0
"The intersection of Computer Vision (CV) and Natural Language Processing (NLP) has resulted in a highly intriguing and challenging research area known as Visual Question Answering (VQA). While image captioning and video summarization involve mining and expressing semantic information in a human-consistent way, VQA doubles the effort required as it involves comparing the semantics of media with the implied semantics of a question expressed in natural language. Recent VQA surveys have focused on methods for image or verbal processing, as well as techniques for fusing information. Most cited works use general-purpose datasets to evaluate VQA systems, with only a few suggesting possible applications. However, this paper focuses on proposals that prioritize real-world applications, using benchmark data relevant to the application domain. Additionally, the paper highlights recent challenges in VQA research.",1
"We tackle the problem of producing compact models, maximizing their accuracy for a given model size. A standard solution is to train networks with Quantization Aware Training, where the weights are quantized during training and the gradients approximated with the Straight-Through Estimator. In this paper, we extend this approach to work beyond int8 fixed-point quantization with extreme compression methods where the approximations introduced by STE are severe, such as Product Quantization. Our proposal is to only quantize a different random subset of weights during each forward, allowing for unbiased gradients to flow through the other weights. Controlling the amount of noise and its form allows for extreme compression rates while maintaining the performance of the original model. As a result we establish new state-of-the-art compromises between accuracy and model size both in natural language processing and image classification. For example, applying our method to state-of-the-art Transformer and ConvNet architectures, we can achieve 82.5% accuracy on MNLI by compressing RoBERTa to 14MB and 80.0 top-1 accuracy on ImageNet by compressing an EfficientNet-B3 to 3.3MB.",0
"The issue of creating smaller models while maintaining their accuracy is addressed by our research. One common solution is to use Quantization Aware Training, where weights are quantized during training and gradients are estimated using the Straight-Through Estimator. However, our approach goes beyond this by using extreme compression techniques, such as Product Quantization, which can introduce severe approximations. To overcome this, we propose randomly quantizing different subsets of weights during each forward pass, allowing unbiased gradients to flow through the other weights. By controlling the amount and type of noise, we can achieve high compression rates while preserving the model's performance. Our method has resulted in new state-of-the-art trade-offs between accuracy and model size in natural language processing and image classification. For instance, we achieved 82.5% accuracy on MNLI by compressing RoBERTa to 14MB and 80.0 top-1 accuracy on ImageNet by compressing an EfficientNet-B3 to 3.3MB.",1
"State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",0
"Current computer vision systems are limited in their abilities as they are only trained to recognize predetermined object categories, requiring additional labeled data to identify any other visual concepts. An alternative approach is to learn directly from raw text about images, which provides a broader source of supervision. By pre-training on the simple task of matching captions with images using a dataset of 400 million (image, text) pairs collected from the internet, we have developed state-of-the-art image representations that can be referenced and transferred to downstream tasks using natural language. We have benchmarked our approach on over 30 computer vision datasets and found that our model often performs competitively with a fully supervised baseline without requiring dataset-specific training. For example, we achieved the same accuracy as the ResNet-50 model on ImageNet zero-shot without using any of its 1.28 million training examples. Our code and pre-trained model weights are available at https://github.com/OpenAI/CLIP.",1
"MLPerf Mobile is the first industry-standard open-source mobile benchmark developed by industry members and academic researchers to allow performance/accuracy evaluation of mobile devices with different AI chips and software stacks. The benchmark draws from the expertise of leading mobile-SoC vendors, ML-framework providers, and model producers. In this paper, we motivate the drive to demystify mobile-AI performance and present MLPerf Mobile's design considerations, architecture, and implementation. The benchmark comprises a suite of models that operate under standard models, data sets, quality metrics, and run rules. For the first iteration, we developed an app to provide an ""out-of-the-box"" inference-performance benchmark for computer vision and natural-language processing on mobile devices. MLPerf Mobile can serve as a framework for integrating future models, for customizing quality-target thresholds to evaluate system performance, for comparing software frameworks, and for assessing heterogeneous-hardware capabilities for machine learning, all fairly and faithfully with fully reproducible results.",0
"Industry members and academic researchers have created MLPerf Mobile, the first open-source mobile benchmark that enables the evaluation of performance and accuracy of mobile devices with different AI chips and software stacks. The benchmark has been developed by leading mobile-SoC vendors, ML-framework providers, and model producers. This paper presents MLPerf Mobile's design considerations, architecture, and implementation, and highlights the importance of demystifying mobile-AI performance. The benchmark includes a suite of models operating under standard models, data sets, quality metrics, and run rules. An app has been developed for the first iteration to provide an ""out-of-the-box"" inference-performance benchmark for computer vision and natural-language processing on mobile devices. MLPerf Mobile can be used as a framework for integrating future models, customizing quality-target thresholds, comparing software frameworks, and assessing heterogeneous-hardware capabilities for machine learning, all with fully reproducible results.",1
"Sign language is the primary language for people with a hearing loss. Sign language recognition (SLR) is the automatic recognition of sign language, which represents a challenging problem for computers, though some progress has been made recently using deep learning. Huge amounts of data are generally required to train deep learning models. However, corresponding datasets are missing for the majority of sign languages. Transfer learning is a technique to utilize a related task with an abundance of data available to help solve a target task lacking sufficient data. Transfer learning has been applied highly successfully in computer vision and natural language processing. However, much less research has been conducted in the field of SLR. This paper investigates how effectively transfer learning can be applied to isolated SLR using an inflated 3D convolutional neural network as the deep learning architecture. Transfer learning is implemented by pre-training a network on the American Sign Language dataset MS-ASL and subsequently fine-tuning it separately on three different sizes of the German Sign Language dataset SIGNUM. The results of the experiments give clear empirical evidence that transfer learning can be effectively applied to isolated SLR. The accuracy performances of the networks applying transfer learning increased substantially by up to 21% as compared to the baseline models that were not pre-trained on the MS-ASL dataset.",0
"Sign language is the main means of communication for individuals with hearing impairments. However, recognizing sign language through computers is a complex task known as sign language recognition (SLR). Deep learning has seen some progress in recent years, but it requires extensive data to train models. Unfortunately, datasets for most sign languages are not readily available. To address this issue, transfer learning has been proposed as a solution. This technique has been successful in computer vision and natural language processing. However, there has been limited research on its effectiveness in SLR. In this study, an inflated 3D convolutional neural network was used to investigate the effectiveness of transfer learning in isolated SLR. The network was pre-trained on the American Sign Language dataset MS-ASL and fine-tuned on three different sizes of the German Sign Language dataset SIGNUM. The results show that transfer learning can be effectively applied to isolated SLR. The accuracy of the networks increased by up to 21% compared to baseline models that were not pre-trained on the MS-ASL dataset.",1
"Large scale recommender models find most relevant items from huge catalogs, and they play a critical role in modern search and recommendation systems. To model the input space with large-vocab categorical features, a typical recommender model learns a joint embedding space through neural networks for both queries and items from user feedback data. However, with millions to billions of items in the corpus, users tend to provide feedback for a very small set of them, causing a power-law distribution. This makes the feedback data for long-tail items extremely sparse.   Inspired by the recent success in self-supervised representation learning research in both computer vision and natural language understanding, we propose a multi-task self-supervised learning (SSL) framework for large-scale item recommendations. The framework is designed to tackle the label sparsity problem by learning better latent relationship of item features. Specifically, SSL improves item representation learning as well as serving as additional regularization to improve generalization. Furthermore, we propose a novel data augmentation method that utilizes feature correlations within the proposed framework.   We evaluate our framework using two real-world datasets with 500M and 1B training examples respectively. Our results demonstrate the effectiveness of SSL regularization and show its superior performance over the state-of-the-art regularization techniques. We also have already launched the proposed techniques to a web-scale commercial app-to-app recommendation system, with significant improvements top-tier business metrics demonstrated in A/B experiments on live traffic. Our online results also verify our hypothesis that our framework indeed improves model performance even more on slices that lack supervision.",0
"Modern search and recommendation systems rely heavily on large scale recommender models that identify the most relevant items from extensive catalogs. To create a joint embedding space for both queries and items, these models use neural networks to model the input space with large-vocab categorical features, drawing on user feedback data. However, with millions to billions of items available, user feedback tends to be concentrated on a small subset of items, leading to a power-law distribution and sparse feedback data for long-tail items. To address this, we propose a multi-task self-supervised learning (SSL) framework that employs a novel data augmentation method to improve item representation learning and generalization. Our approach tackles the issue of label sparsity by learning better latent relationships among item features. We evaluate our framework on real-world datasets with 500M and 1B training examples, demonstrating its effectiveness and superiority over state-of-the-art regularization techniques. Our proposed techniques have already been implemented in a web-scale commercial app-to-app recommendation system, leading to significant improvements in top-tier business metrics as demonstrated in A/B experiments on live traffic. Our online results confirm that our framework improves model performance, particularly on slices that lack supervision.",1
"Product embeddings have been heavily investigated in the past few years, serving as the cornerstone for a broad range of machine learning applications in e-commerce. Despite the empirical success of product embeddings, little is known on how and why they work from the theoretical standpoint. Analogous results from the natural language processing (NLP) often rely on domain-specific properties that are not transferable to the e-commerce setting, and the downstream tasks often focus on different aspects of the embeddings. We take an e-commerce-oriented view of the product embeddings and reveal a complete theoretical view from both the representation learning and the learning theory perspective. We prove that product embeddings trained by the widely-adopted skip-gram negative sampling algorithm and its variants are sufficient dimension reduction regarding a critical product relatedness measure. The generalization performance in the downstream machine learning task is controlled by the alignment between the embeddings and the product relatedness measure. Following the theoretical discoveries, we conduct exploratory experiments that supports our theoretical insights for the product embeddings.",0
"In recent years, there has been extensive research on product embeddings, which have become a key component in various machine learning applications in e-commerce. While product embeddings have demonstrated empirical success, their theoretical foundations remain largely unknown. Unlike natural language processing (NLP), which relies on domain-specific properties, e-commerce presents unique challenges, and downstream tasks require different aspects of the embeddings. Therefore, we adopt an e-commerce-oriented approach to provide a comprehensive theoretical view of product embeddings from both the representation learning and learning theory perspectives. Our research demonstrates that product embeddings generated by the popular skip-gram negative sampling algorithm and its variants are sufficient for dimension reduction regarding a crucial product relatedness measure. Furthermore, the alignment between the embeddings and the product relatedness measure determines the generalization performance in downstream machine learning tasks. We conducted exploratory experiments that support our theoretical insights into product embeddings.",1
"Weaknesses in computer systems such as faults, bugs and errors in the architecture, design or implementation of software provide vulnerabilities that can be exploited by attackers to compromise the security of a system. Common Weakness Enumerations (CWE) are a hierarchically designed dictionary of software weaknesses that provide a means to understand software flaws, potential impact of their exploitation, and means to mitigate these flaws. Common Vulnerabilities and Exposures (CVE) are brief low-level descriptions that uniquely identify vulnerabilities in a specific product or protocol. Classifying or mapping of CVEs to CWEs provides a means to understand the impact and mitigate the vulnerabilities. Since manual mapping of CVEs is not a viable option, automated approaches are desirable but challenging.   We present a novel Transformer-based learning framework (V2W-BERT) in this paper. By using ideas from natural language processing, link prediction and transfer learning, our method outperforms previous approaches not only for CWE instances with abundant data to train, but also rare CWE classes with little or no data to train. Our approach also shows significant improvements in using historical data to predict links for future instances of CVEs, and therefore, provides a viable approach for practical applications. Using data from MITRE and National Vulnerability Database, we achieve up to 97% prediction accuracy for randomly partitioned data and up to 94% prediction accuracy in temporally partitioned data. We believe that our work will influence the design of better methods and training models, as well as applications to solve increasingly harder problems in cybersecurity.",0
"Computer systems can be vulnerable to attacks due to weaknesses such as flaws, bugs, and errors in software architecture, design, or implementation. Common Weakness Enumerations (CWE) and Common Vulnerabilities and Exposures (CVE) are used to identify and understand software flaws and vulnerabilities. Mapping CVEs to CWEs helps to mitigate these vulnerabilities, but manual mapping is impractical, so automated approaches are preferred. In this paper, we present a new learning framework called V2W-BERT, which uses natural language processing, link prediction, and transfer learning to outperform previous methods in predicting CWE instances with both abundant and little data. Our approach also improves prediction accuracy for future CVE instances, making it suitable for practical use. Our work is expected to inspire better methods and training models for cybersecurity applications.",1
"Semantic embeddings have advanced the state of the art for countless natural language processing tasks, and various extensions to multimodal domains, such as visual-semantic embeddings, have been proposed. While the power of visual-semantic embeddings comes from the distillation and enrichment of information through machine learning, their inner workings are poorly understood and there is a shortage of analysis tools. To address this problem, we generalize the notion of probing tasks to the visual-semantic case. To this end, we (i) discuss the formalization of probing tasks for embeddings of image-caption pairs, (ii) define three concrete probing tasks within our general framework, (iii) train classifiers to probe for those properties, and (iv) compare various state-of-the-art embeddings under the lens of the proposed probing tasks. Our experiments reveal an up to 12% increase in accuracy on visual-semantic embeddings compared to the corresponding unimodal embeddings, which suggest that the text and image dimensions represented in the former do complement each other.",0
"The use of semantic embeddings has greatly improved various natural language processing tasks, and there have been proposed extensions to multimodal domains, such as visual-semantic embeddings. However, the workings of visual-semantic embeddings are not well understood and there is a lack of analysis tools. To address this issue, we have expanded the concept of probing tasks to include visual-semantic embeddings. We have formalized probing tasks for image-caption pair embeddings, defined three specific probing tasks, trained classifiers to probe for these properties, and compared different state-of-the-art embeddings using our probing tasks. Our experiments have shown up to a 12% increase in accuracy for visual-semantic embeddings compared to unimodal embeddings, indicating that the text and image dimensions complement each other in these embeddings.",1
"Transformer is a ubiquitous model for natural language processing and has attracted wide attentions in computer vision. The attention maps are indispensable for a transformer model to encode the dependencies among input tokens. However, they are learned independently in each layer and sometimes fail to capture precise patterns. In this paper, we propose a novel and generic mechanism based on evolving attention to improve the performance of transformers. On one hand, the attention maps in different layers share common knowledge, thus the ones in preceding layers can instruct the attention in succeeding layers through residual connections. On the other hand, low-level and high-level attentions vary in the level of abstraction, so we adopt convolutional layers to model the evolutionary process of attention maps. The proposed evolving attention mechanism achieves significant performance improvement over various state-of-the-art models for multiple tasks, including image classification, natural language understanding and machine translation.",0
"The transformer model is widely used in natural language processing and has gained attention in computer vision. To encode input token dependencies, attention maps are essential, but they are learned separately in each layer and may not capture precise patterns. Our paper proposes a novel mechanism that utilizes evolving attention to enhance transformer performance. Attention maps in different layers share common knowledge, allowing preceding layers to guide succeeding layers via residual connections. We also use convolutional layers to model the evolutionary process of low-level and high-level attentions, which differ in abstraction. Our evolving attention mechanism significantly improves performance for various tasks, such as image classification, natural language understanding, and machine translation, surpassing state-of-the-art models.",1
"This paper attacks the challenging problem of video retrieval by text. In such a retrieval paradigm, an end user searches for unlabeled videos by ad-hoc queries described exclusively in the form of a natural-language sentence, with no visual example provided. Given videos as sequences of frames and queries as sequences of words, an effective sequence-to-sequence cross-modal matching is crucial. To that end, the two modalities need to be first encoded into real-valued vectors and then projected into a common space. In this paper we achieve this by proposing a dual deep encoding network that encodes videos and queries into powerful dense representations of their own. Our novelty is two-fold. First, different from prior art that resorts to a specific single-level encoder, the proposed network performs multi-level encoding that represents the rich content of both modalities in a coarse-to-fine fashion. Second, different from a conventional common space learning algorithm which is either concept based or latent space based, we introduce hybrid space learning which combines the high performance of the latent space and the good interpretability of the concept space. Dual encoding is conceptually simple, practically effective and end-to-end trained with hybrid space learning. Extensive experiments on four challenging video datasets show the viability of the new method.",0
"This article addresses the difficult issue of retrieving videos through text-based queries. The retrieval method involves a user searching for unlabeled videos by using natural-language sentences as queries, without providing any visual examples. As such, cross-modal matching between the video frames and the words in the query is essential. To accomplish this, the video frames and query words must first be encoded into real-valued vectors and then projected into a common space. The authors propose a dual deep encoding network that encodes both the videos and the queries into powerful and dense representations. Their approach differs from previous methods in two ways: first, by using a multi-level encoding approach that captures the rich content of both modalities in a coarse-to-fine fashion; second, by introducing hybrid space learning that combines the high performance of the latent space with the good interpretability of the concept space. The dual encoding approach is straightforward, effective, and trained end-to-end with hybrid space learning. The authors demonstrate the effectiveness of their approach through extensive experiments on four challenging video datasets.",1
"Statistical learning theory provides the foundation to applied machine learning, and its various successful applications in computer vision, natural language processing and other scientific domains. The theory, however, does not take into account the unique challenges of performing statistical learning in geospatial settings. For instance, it is well known that model errors cannot be assumed to be independent and identically distributed in geospatial (a.k.a. regionalized) variables due to spatial correlation; and trends caused by geophysical processes lead to covariate shifts between the domain where the model was trained and the domain where it will be applied, which in turn harm the use of classical learning methodologies that rely on random samples of the data. In this work, we introduce the geostatistical (transfer) learning problem, and illustrate the challenges of learning from geospatial data by assessing widely-used methods for estimating generalization error of learning models, under covariate shift and spatial correlation. Experiments with synthetic Gaussian process data as well as with real data from geophysical surveys in New Zealand indicate that none of the methods are adequate for model selection in a geospatial context. We provide general guidelines regarding the choice of these methods in practice while new methods are being actively researched.",0
"Applied machine learning is built upon the foundation of statistical learning theory, which has proven successful in a variety of scientific domains, including computer vision and natural language processing. However, this theory does not adequately address the unique challenges of performing statistical learning in geospatial settings. Geospatial variables are regionalized and have spatial correlation, meaning that model errors cannot be assumed to be independent and identically distributed. Additionally, geophysical processes cause covariate shifts between the domain where the model was trained and the domain where it will be applied, making classical learning methodologies that rely on random samples of data ineffective. In this study, we introduce the geostatistical (transfer) learning problem and evaluate widely-used methods for estimating generalization error of learning models in geospatial contexts. Our experiments with synthetic Gaussian process data and real data from geophysical surveys in New Zealand reveal that none of the methods are adequate for model selection. We provide general guidelines for the choice of these methods in practice, while new methods are actively being researched.",1
"We introduce QuerYD, a new large-scale dataset for retrieval and event localisation in video. A unique feature of our dataset is the availability of two audio tracks for each video: the original audio, and a high-quality spoken description of the visual content. The dataset is based on YouDescribe, a volunteer project that assists visually-impaired people by attaching voiced narrations to existing YouTube videos. This ever-growing collection of videos contains highly detailed, temporally aligned audio and text annotations. The content descriptions are more relevant than dialogue, and more detailed than previous description attempts, which can be observed to contain many superficial or uninformative descriptions. To demonstrate the utility of the QuerYD dataset, we show that it can be used to train and benchmark strong models for retrieval and event localisation. Data, code and models are made publicly available, and we hope that QuerYD inspires further research on video understanding with written and spoken natural language.",0
"Introducing QuerYD, a novel extensive dataset designed for video retrieval and event localisation. Our dataset offers a distinctive feature of having two audio tracks for each video, namely the original audio and a high-quality spoken description of the visual content. It is derived from YouDescribe, a volunteer initiative aimed at catering to the needs of visually-impaired individuals by adding voiced narrations to pre-existing YouTube videos. The rapidly expanding compilation boasts of remarkably detailed audio and text annotations that are temporally aligned. The content descriptions surpass dialogues and previous description attempts in terms of relevance and detail, thereby eliminating numerous superficial and uninformative descriptions. To showcase the practicality of the QuerYD dataset, we present evidence that it can be employed to train and evaluate robust models for retrieval and event localisation. Our data, code, and models are available publicly, and we aspire to stimulate further investigations into natural language-based video comprehension using QuerYD.",1
"Recent works have demonstrated reasonable success of representation learning in hypercomplex space. Specifically, ""fully-connected layers with Quaternions"" (4D hypercomplex numbers), which replace real-valued matrix multiplications in fully-connected layers with Hamilton products of Quaternions, both enjoy parameter savings with only 1/4 learnable parameters and achieve comparable performance in various applications. However, one key caveat is that hypercomplex space only exists at very few predefined dimensions (4D, 8D, and 16D). This restricts the flexibility of models that leverage hypercomplex multiplications. To this end, we propose parameterizing hypercomplex multiplications, allowing models to learn multiplication rules from data regardless of whether such rules are predefined. As a result, our method not only subsumes the Hamilton product, but also learns to operate on any arbitrary nD hypercomplex space, providing more architectural flexibility using arbitrarily $1/n$ learnable parameters compared with the fully-connected layer counterpart. Experiments of applications to the LSTM and Transformer models on natural language inference, machine translation, text style transfer, and subject verb agreement demonstrate architectural flexibility and effectiveness of the proposed approach.",0
"Representation learning in hypercomplex space has shown reasonable success in recent works. A notable approach is fully-connected layers with Quaternions, which replace real-valued matrix multiplications with Hamilton products of 4D hypercomplex numbers. This method achieves comparable performance in various applications and enjoys parameter savings with only 1/4 learnable parameters. However, the drawback is that hypercomplex space is limited to predefined dimensions of 4D, 8D, and 16D, which restricts the flexibility of models. To overcome this limitation, we suggest parameterizing hypercomplex multiplications, enabling models to learn multiplication rules from data, irrespective of whether such rules are predefined. Our approach not only subsumes the Hamilton product but also operates on any arbitrary nD hypercomplex space, offering more architectural flexibility using arbitrarily $1/n$ learnable parameters compared to the fully-connected layer counterpart. The proposed approach's effectiveness and flexibility are demonstrated in experiments on natural language inference, machine translation, text style transfer, and subject-verb agreement using LSTM and Transformer models.",1
"The success of deep learning in the computer vision and natural language processing communities can be attributed to training of very deep neural networks with millions or billions of parameters which can then be trained with massive amounts of data. However, similar trend has largely eluded training of deep reinforcement learning (RL) algorithms where larger networks do not lead to performance improvement. Previous work has shown that this is mostly due to instability during training of deep RL agents when using larger networks. In this paper, we make an attempt to understand and address training of larger networks for deep RL. We first show that naively increasing network capacity does not improve performance. Then, we propose a novel method that consists of 1) wider networks with DenseNet connection, 2) decoupling representation learning from training of RL, 3) a distributed training method to mitigate overfitting problems. Using this three-fold technique, we show that we can train very large networks that result in significant performance gains. We present several ablation studies to demonstrate the efficacy of the proposed method and some intuitive understanding of the reasons for performance gain. We show that our proposed method outperforms other baseline algorithms on several challenging locomotion tasks.",0
"The success of deep learning in computer vision and natural language processing is due to the training of very deep neural networks with millions or billions of parameters, using massive amounts of data. However, this trend has not been observed in the training of deep reinforcement learning (RL) algorithms, as larger networks do not lead to performance improvement. Previous studies have shown that this is mainly due to instability during the training of deep RL agents when using larger networks. This paper attempts to understand and address this issue by proposing a new method that involves wider networks with DenseNet connection, decoupling representation learning from RL training, and a distributed training method to mitigate overfitting problems. Through this approach, the authors demonstrate significant performance gains by training very large networks. Ablation studies are presented to demonstrate the effectiveness of the proposed method and provide an intuitive understanding of the reasons for the performance gain. The proposed method outperforms other baseline algorithms on several challenging locomotion tasks.",1
"Image Captioning is an arduous task of producing syntactically and semantically correct textual descriptions of an image in natural language with context related to the image. Existing notable pieces of research in Bengali Image Captioning (BIC) are based on encoder-decoder architecture. This paper presents an end-to-end image captioning system utilizing a multimodal architecture by combining a one-dimensional convolutional neural network (CNN) to encode sequence information with a pre-trained ResNet-50 model image encoder for extracting region-based visual features. We investigate our approach's performance on the BanglaLekhaImageCaptions dataset using the existing evaluation metrics and perform a human evaluation for qualitative analysis. Experiments show that our approach's language encoder captures the fine-grained information in the caption, and combined with the image features, it generates accurate and diversified caption. Our work outperforms all the existing BIC works and achieves a new state-of-the-art (SOTA) performance by scoring 0.651 on BLUE-1, 0.572 on CIDEr, 0.297 on METEOR, 0.434 on ROUGE, and 0.357 on SPICE.",0
"The challenging task of Image Captioning involves creating grammatically and semantically correct written descriptions of an image in everyday language that relates to the image. Previous research on Bengali Image Captioning (BIC) has centered on an encoder-decoder architecture. This study introduces a multimodal architecture that combines a one-dimensional convolutional neural network (CNN) to encode sequence information with a pre-trained ResNet-50 model image encoder to extract visual features based on regions. We evaluate our methodology's performance on the BanglaLekhaImageCaptions dataset using existing evaluation metrics and perform human evaluations for qualitative analysis. Our experiments demonstrate that our language encoder captures detailed information in the caption, and when combined with image features, it produces precise and varied captions. Our work surpasses all previous BIC research, attaining new state-of-the-art (SOTA) performance scores of 0.651 on BLUE-1, 0.572 on CIDEr, 0.297 on METEOR, 0.434 on ROUGE, and 0.357 on SPICE.",1
"Structured matrices, such as those derived from Kronecker products (KP), are effective at compressing neural networks, but can lead to unacceptable accuracy loss when applied to large models. In this paper, we propose the notion of doping -- addition of an extremely sparse matrix to a structured matrix. Doping facilitates additional degrees of freedom for a small number of parameters, allowing them to independently diverge from the fixed structure. To train LSTMs with doped structured matrices, we introduce the additional parameter matrix while slowly annealing its sparsity level. However, we find that performance degrades as we slowly sparsify the doping matrix, due to co-matrix adaptation (CMA) between the structured and the sparse matrices. We address this over dependence on the sparse matrix using a co-matrix dropout regularization (CMR) scheme. We provide empirical evidence to show that doping, CMA and CMR are concepts generally applicable to multiple structured matrices (Kronecker Product, LMF, Hybrid Matrix Decomposition). Additionally, results with doped kronecker product matrices demonstrate state-of-the-art accuracy at large compression factors (10 - 25x) across 4 natural language processing applications with minor loss in accuracy. Doped KP compression technique outperforms previous state-of-the art compression results by achieving 1.3 - 2.4x higher compression factor at a similar accuracy, while also beating strong alternatives like pruning and low-rank methods by a large margin (8% or more). Additionally, we show that doped KP can be deployed on commodity hardware using the current software stack and achieve 2.5 - 5.5x inference run-time speed-up over baseline.",0
"The use of structured matrices, specifically those from Kronecker products (KP), is effective for compressing neural networks. However, for larger models, this method can result in unacceptable accuracy loss. To address this issue, we propose ""doping"" - the addition of an extremely sparse matrix to a structured matrix. This allows for additional degrees of freedom for a small number of parameters to independently diverge from the fixed structure. For training LSTMs using doped structured matrices, we introduce an additional parameter matrix and gradually increase its sparsity level. However, our findings show that performance deteriorates as we increase the sparsity level due to co-matrix adaptation (CMA) between the structured and sparse matrices. To address this, we use a co-matrix dropout regularization (CMR) scheme to prevent over-dependence on the sparse matrix. Our empirical evidence shows that doping, CMA, and CMR are applicable to multiple structured matrices such as Kronecker Product, LMF, and Hybrid Matrix Decomposition. Our results demonstrate that doped KP compression achieves state-of-the-art accuracy at large compression factors (10-25x) across 4 natural language processing applications with minimal loss in accuracy. Furthermore, the doped KP compression technique outperforms previous state-of-the-art compression results by achieving a higher compression factor (1.3-2.4x) at similar accuracy and surpasses pruning and low-rank methods by a significant margin (8% or more). Lastly, we show that doped KP can be deployed on commodity hardware using the current software stack and achieve 2.5-5.5x inference run-time speed-up over baseline.",1
"Analog electronic and optical computing exhibit tremendous advantages over digital computing for accelerating deep learning when operations are executed at low precision. In this work, we derive a relationship between analog precision, which is limited by noise, and digital bit precision. We propose extending analog computing architectures to support varying levels of precision by repeating operations and averaging the result, decreasing the impact of noise. Such architectures enable programmable tradeoffs between precision and other desirable performance metrics such as energy efficiency or throughput. To utilize dynamic precision, we propose a method for learning the precision of each layer of a pre-trained model without retraining network weights. We evaluate this method on analog architectures subject to a variety of noise sources such as shot noise, thermal noise, and weight noise and find that employing dynamic precision reduces energy consumption by up to 89% for computer vision models such as Resnet50 and by 24% for natural language processing models such as BERT. In one example, we apply dynamic precision to a shot-noise limited homodyne optical neural network and simulate inference at an optical energy consumption of 2.7 aJ/MAC for Resnet50 and 1.6 aJ/MAC for BERT with <2% accuracy degradation.",0
"When operations are executed at low precision, analog electronic and optical computing offer significant advantages over digital computing for accelerating deep learning. This study establishes a connection between analog precision, which is limited by noise, and digital bit precision. To reduce the impact of noise, we advocate for the expansion of analog computing structures to include various levels of precision by repeating operations and averaging the result. These architectures allow for programmable tradeoffs between precision and other important performance metrics such as energy efficiency or throughput. To utilize dynamic precision, we suggest a method for learning the precision of each layer of a pre-trained model without retraining network weights. We assess this technique on analog architectures under various noise sources such as shot noise, thermal noise, and weight noise, and discover that using dynamic precision can reduce energy consumption by up to 89% for computer vision models and by 24% for natural language processing models. Finally, we apply dynamic precision to a shot-noise limited homodyne optical neural network and simulate inference at an optical energy consumption of 2.7 aJ/MAC for Resnet50 and 1.6 aJ/MAC for BERT with <2% accuracy degradation in one example.",1
"A pruning-based AutoML framework for run-time reconfigurability, namely RT3, is proposed in this work. This enables Transformer-based large Natural Language Processing (NLP) models to be efficiently executed on resource-constrained mobile devices and reconfigured (i.e., switching models for dynamic hardware conditions) at run-time. Such reconfigurability is the key to save energy for battery-powered mobile devices, which widely use dynamic voltage and frequency scaling (DVFS) technique for hardware reconfiguration to prolong battery life. In this work, we creatively explore a hybrid block-structured pruning (BP) and pattern pruning (PP) for Transformer-based models and first attempt to combine hardware and software reconfiguration to maximally save energy for battery-powered mobile devices. Specifically, RT3 integrates two-level optimizations: First, it utilizes an efficient BP as the first-step compression for resource-constrained mobile devices; then, RT3 heuristically generates a shrunken search space based on the first level optimization and searches multiple pattern sets with diverse sparsity for PP via reinforcement learning to support lightweight software reconfiguration, which corresponds to available frequency levels of DVFS (i.e., hardware reconfiguration). At run-time, RT3 can switch the lightweight pattern sets within 45ms to guarantee the required real-time constraint at different frequency levels. Results further show that RT3 can prolong battery life over 4x improvement with less than 1% accuracy loss for Transformer and 1.5% score decrease for DistilBERT.",0
"This work proposes an AutoML framework called RT3 that utilizes pruning to enable efficient execution of large NLP models on resource-constrained mobile devices. This framework also allows for run-time reconfigurability, which is necessary for optimizing energy consumption on battery-powered mobile devices that use dynamic voltage and frequency scaling (DVFS). The approach combines block-structured pruning and pattern pruning to generate a shrunken search space, which is then optimized using reinforcement learning. This allows for lightweight software reconfiguration, corresponding to available frequency levels of DVFS, to be combined with hardware reconfiguration. At run-time, RT3 can switch between lightweight pattern sets within 45ms to meet real-time constraints. Results show that RT3 can prolong battery life by over 4x and cause less than 1% accuracy loss for Transformer and 1.5% score decrease for DistilBERT.",1
"The knowledge that data lies close to a particular submanifold of the ambient Euclidean space may be useful in a number of ways. For instance, one may want to automatically mark any point far away from the submanifold as an outlier, or to use its geodesic distance to measure similarity between points. Classical problems for manifold learning are often posed in a very high dimension, e.g. for spaces of images or spaces of representations of words. Today, with deep representation learning on the rise in areas such as computer vision and natural language processing, many problems of this kind may be transformed into problems of moderately high dimension, typically of the order of hundreds. Motivated by this, we propose a manifold learning technique suitable for moderately high dimension and large datasets. The manifold is learned from the training data in the form of an intersection of quadric hypersurfaces -- simple but expressive objects. At test time, this manifold can be used to introduce an outlier score for arbitrary new points and to improve a given similarity metric by incorporating learned geometric structure into it.",0
"Understanding that data is located in close proximity to a particular submanifold within the larger Euclidean space has various potential applications. For instance, one could automatically identify any point that is distant from the submanifold as an outlier or use the geodesic distance to evaluate similarity between points. Traditional problems associated with manifold learning are frequently presented in high dimensions, such as in the case of image or word representation spaces. However, with the recent emergence of deep representation learning in fields like natural language processing and computer vision, many of these issues can now be reframed as moderately high dimensional problems, often in the hundreds. With this in mind, we suggest a manifold learning method that is suitable for large datasets and moderately high dimensions. We teach the manifold using an intersection of quadric hypersurfaces, which are simple yet expressive objects. During testing, this manifold can be utilized to establish an outlier score for any new points, as well as improve a given similarity metric by integrating learned geometric structure.",1
"Machine learning and deep learning have shown great promise in mobile sensing applications, including Human Activity Recognition. However, the performance of such models in real-world settings largely depends on the availability of large datasets that captures diverse behaviors. Recently, studies in computer vision and natural language processing have shown that leveraging massive amounts of unlabeled data enables performance on par with state-of-the-art supervised models.   In this work, we present SelfHAR, a semi-supervised model that effectively learns to leverage unlabeled mobile sensing datasets to complement small labeled datasets. Our approach combines teacher-student self-training, which distills the knowledge of unlabeled and labeled datasets while allowing for data augmentation, and multi-task self-supervision, which learns robust signal-level representations by predicting distorted versions of the input.   We evaluated SelfHAR on various HAR datasets and showed state-of-the-art performance over supervised and previous semi-supervised approaches, with up to 12% increase in F1 score using the same number of model parameters at inference. Furthermore, SelfHAR is data-efficient, reaching similar performance using up to 10 times less labeled data compared to supervised approaches. Our work not only achieves state-of-the-art performance in a diverse set of HAR datasets, but also sheds light on how pre-training tasks may affect downstream performance.",0
"Mobile sensing applications, such as Human Activity Recognition, have shown great potential with the use of machine learning and deep learning. However, the success of these models in real-world situations is heavily reliant on the availability of comprehensive datasets that capture a range of behaviors. Recent studies in natural language processing and computer vision have demonstrated that incorporating vast amounts of unlabeled data can lead to comparable performance to supervised models. This paper introduces SelfHAR, a semi-supervised model that effectively utilizes unlabeled mobile sensing datasets to supplement small labeled datasets. Our approach combines teacher-student self-training, which distills the knowledge of both labeled and unlabeled datasets while also incorporating data augmentation, and multi-task self-supervision, which learns robust signal-level representations by predicting distorted versions of the input. We evaluated SelfHAR on various HAR datasets and demonstrated superior performance compared to supervised and previous semi-supervised approaches, with up to a 12% increase in F1 score using the same number of model parameters during inference. Additionally, SelfHAR is efficient with data, achieving similar performance using up to ten times less labeled data than supervised approaches. Our findings not only achieve state-of-the-art results across a diverse range of HAR datasets, but also provide insight into how pre-training tasks may impact downstream performance.",1
"The count-min sketch (CMS) is a randomized data structure that provides estimates of tokens' frequencies in a large data stream using a compressed representation of the data by random hashing. In this paper, we rely on a recent Bayesian nonparametric (BNP) view on the CMS to develop a novel learning-augmented CMS under power-law data streams. We assume that tokens in the stream are drawn from an unknown discrete distribution, which is endowed with a normalized inverse Gaussian process (NIGP) prior. Then, using distributional properties of the NIGP, we compute the posterior distribution of a token's frequency in the stream, given the hashed data, and in turn corresponding BNP estimates. Applications to synthetic and real data show that our approach achieves a remarkable performance in the estimation of low-frequency tokens. This is known to be a desirable feature in the context of natural language processing, where it is indeed common in the context of the power-law behaviour of the data.",0
"The count-min sketch (CMS) is a compressed data structure that uses random hashing to estimate the frequency of tokens in a large data stream. Our paper introduces a new approach to the CMS by applying Bayesian nonparametric (BNP) principles to power-law data streams. Specifically, we assume that the tokens in the stream are drawn from an unknown discrete distribution with a normalized inverse Gaussian process (NIGP) prior. Using the distributional properties of the NIGP, we calculate the posterior distribution of a token's frequency in the stream based on the hashed data, resulting in BNP estimates. Our method demonstrated excellent performance in estimating low-frequency tokens, which is particularly useful in natural language processing, where power-law behavior is common. We tested our approach on both synthetic and real data, with promising results.",1
"Veracity is an essential key in research and development of innovative products. Live Emotion analysis and verification nullify deceit made to complainers on live chat, corroborate messages of both ends in messaging apps and promote an honest conversation between users. The main concept behind this emotion artificial intelligent verifier is to license or decline message accountability by comparing variegated emotions of chat app users recognized through facial expressions and text prediction. In this paper, a proposed emotion intelligent live detector acts as an honest arbiter who distributes facial emotions into labels namely, Happiness, Sadness, Surprise, and Hate. Further, it separately predicts a label of messages through text classification. Finally, it compares both labels and declares the message as a fraud or a bonafide. For emotion detection, we deployed Convolutional Neural Network (CNN) using a miniXception model and for text prediction, we selected Support Vector Machine (SVM) natural language processing probability classifier due to receiving the best accuracy on training dataset after applying Support Vector Machine (SVM), Random Forest Classifier, Naive Bayes Classifier, and Logistic regression.",0
"In the research and development of innovative products, it is crucial to prioritize veracity. The utilization of Live Emotion analysis and verification can help eradicate any deceitful behavior in live chats, validate messages in messaging apps, and encourage truthful conversations among users. The fundamental idea behind the emotion artificial intelligent verifier is to authenticate or reject message accountability by comparing various emotions of chat app users identified through facial expressions and text prediction. This paper proposes an emotion intelligent live detector that functions as an impartial mediator by categorizing facial emotions into labels, such as Happiness, Sadness, Surprise, and Hate. Additionally, it predicts message labels through text classification and compares both labels to determine the authenticity of the message. To achieve this, we utilized a Convolutional Neural Network (CNN) with a miniXception model for emotion detection and a Support Vector Machine (SVM) natural language processing probability classifier for text prediction, which demonstrated the best accuracy on the training dataset compared to other classifiers, including Support Vector Machine (SVM), Random Forest Classifier, Naive Bayes Classifier, and Logistic regression.",1
"Transformers have shown outstanding results for natural language understanding and, more recently, for image classification. We here extend this work and propose a transformer-based approach for image retrieval: we adopt vision transformers for generating image descriptors and train the resulting model with a metric learning objective, which combines a contrastive loss with a differential entropy regularizer. Our results show consistent and significant improvements of transformers over convolution-based approaches. In particular, our method outperforms the state of the art on several public benchmarks for category-level retrieval, namely Stanford Online Product, In-Shop and CUB-200. Furthermore, our experiments on ROxford and RParis also show that, in comparable settings, transformers are competitive for particular object retrieval, especially in the regime of short vector representations and low-resolution images.",0
"The use of transformers has demonstrated remarkable achievements in natural language understanding and, more recently, in image classification. Our study builds upon this progress and introduces a transformer-based technique for image retrieval. Specifically, we utilize vision transformers to create image descriptors and train the resultant model using a metric learning objective that couples a contrastive loss with a differential entropy regularizer. Our findings reveal that transformers consistently and considerably outperform convolution-based approaches. Notably, our method surpasses the current leading methods in category-level retrieval on various public benchmarks, including Stanford Online Product, In-Shop, and CUB-200. Additionally, our experiments on ROxford and RParis demonstrate that transformers are competitive in object retrieval, particularly for short vector representations and low-resolution images.",1
"We address the problem of learning on sets of features, motivated by the need of performing pooling operations in long biological sequences of varying sizes, with long-range dependencies, and possibly few labeled data. To address this challenging task, we introduce a parametrized representation of fixed size, which embeds and then aggregates elements from a given input set according to the optimal transport plan between the set and a trainable reference. Our approach scales to large datasets and allows end-to-end training of the reference, while also providing a simple unsupervised learning mechanism with small computational cost. Our aggregation technique admits two useful interpretations: it may be seen as a mechanism related to attention layers in neural networks, or it may be seen as a scalable surrogate of a classical optimal transport-based kernel. We experimentally demonstrate the effectiveness of our approach on biological sequences, achieving state-of-the-art results for protein fold recognition and detection of chromatin profiles tasks, and, as a proof of concept, we show promising results for processing natural language sequences. We provide an open-source implementation of our embedding that can be used alone or as a module in larger learning models at https://github.com/claying/OTK.",0
"Our focus is on learning from sets of features, which is necessary for performing pooling operations on long biological sequences that vary in size, have long-range dependencies, and may have limited labeled data. To tackle this difficult task, we propose a parametrized representation of fixed size that embeds and aggregates elements from an input set based on the optimal transport plan between the set and a trainable reference. Our approach can handle large datasets and enables end-to-end training of the reference, while also providing a straightforward unsupervised learning method that is computationally efficient. Our aggregation technique has two interpretations: it can be viewed as a mechanism related to attention layers in neural networks, or as a scalable substitute for a classical optimal transport-based kernel. We demonstrate the effectiveness of our approach through experiments on biological sequences, achieving state-of-the-art results for protein fold recognition and chromatin profile detection tasks. Additionally, we demonstrate promising results for natural language sequence processing. Our embedding is open-source and can be used independently or as a component in larger learning models. It is available at https://github.com/claying/OTK.",1
"Natural Language Video Description (NLVD) has recently received strong interest in the Computer Vision, Natural Language Processing (NLP), Multimedia, and Autonomous Robotics communities. The State-of-the-Art (SotA) approaches obtained remarkable results when tested on the benchmark datasets. However, those approaches poorly generalize to new datasets. In addition, none of the existing works focus on the processing of the input to the NLVD systems, which is both visual and textual. In this work, it is presented an extensive study dealing with the role of the visual input, evaluated with respect to the overall NLP performance. This is achieved performing data augmentation of the visual component, applying common transformations to model camera distortions, noise, lighting, and camera positioning, that are typical in real-world operative scenarios. A t-SNE based analysis is proposed to evaluate the effects of the considered transformations on the overall visual data distribution. For this study, it is considered the English subset of Microsoft Research Video Description (MSVD) dataset, which is used commonly for NLVD. It was observed that this dataset contains a relevant amount of syntactic and semantic errors. These errors have been amended manually, and the new version of the dataset (called MSVD-v2) is used in the experimentation. The MSVD-v2 dataset is released to help to gain insight into the NLVD problem.",0
"The Computer Vision, Natural Language Processing (NLP), Multimedia, and Autonomous Robotics communities have shown a keen interest in Natural Language Video Description (NLVD) recently. Although the State-of-the-Art (SotA) approaches have achieved remarkable results on benchmark datasets, they are not ideal for generalizing to new datasets. Furthermore, the existing works have not focused on processing the input to the NLVD systems, which comprises both visual and textual components. This research presents a comprehensive study that examines the impact of the visual input on the overall NLP performance. Data augmentation of the visual component is conducted by applying typical real-world transformations to model camera distortions, noise, lighting, and camera positioning. A t-SNE based analysis is performed to assess the effects of the transformations on the visual data distribution. The MSVD English subset, which is commonly used for NLVD, was employed for this study. The researchers discovered that this dataset has a significant number of syntactic and semantic errors, which they corrected manually. The new version of the dataset, called MSVD-v2, is now accessible to assist in addressing the NLVD problem.",1
"We consider the problem of referring segmentation in images and videos with natural language. Given an input image (or video) and a referring expression, the goal is to segment the entity referred by the expression in the image or video. In this paper, we propose a cross-modal self-attention (CMSA) module to utilize fine details of individual words and the input image or video, which effectively captures the long-range dependencies between linguistic and visual features. Our model can adaptively focus on informative words in the referring expression and important regions in the visual input. We further propose a gated multi-level fusion (GMLF) module to selectively integrate self-attentive cross-modal features corresponding to different levels of visual features. This module controls the feature fusion of information flow of features at different levels with high-level and low-level semantic information related to different attentive words. Besides, we introduce cross-frame self-attention (CFSA) module to effectively integrate temporal information in consecutive frames which extends our method in the case of referring segmentation in videos. Experiments on benchmark datasets of four referring image datasets and two actor and action video segmentation datasets consistently demonstrate that our proposed approach outperforms existing state-of-the-art methods.",0
"This paper focuses on the issue of referring to segmentation in images and videos using natural language. The objective is to segment the entity referred to by the expression present in the input image or video. To tackle this problem, the authors propose a CMSA module that captures the long-range dependencies between linguistic and visual features by utilizing fine details of individual words and the input image or video. This module enables the model to adaptively focus on informative words and important regions in the visual input. Additionally, a GMLF module is introduced to selectively integrate self-attentive cross-modal features corresponding to different levels of visual features, controlling the feature fusion of information flow of features at different levels. The authors also introduce a CFSA module to effectively integrate temporal information in consecutive frames, extending the proposed approach to the case of referring segmentation in videos. Experimental results on benchmark datasets of four referring image datasets and two actor and action video segmentation datasets demonstrate that the proposed approach outperforms existing state-of-the-art methods consistently.",1
"In the last decades, extreme classification has become an essential topic for deep learning. It has achieved great success in many areas, especially in computer vision and natural language processing (NLP). However, it is very challenging to train a deep model with millions of classes due to the memory and computation explosion in the last output layer. In this paper, we propose a large-scale training system to address these challenges. First, we build a hybrid parallel training framework to make the training process feasible. Second, we propose a novel softmax variation named KNN softmax, which reduces both the GPU memory consumption and computation costs and improves the throughput of training. Then, to eliminate the communication overhead, we propose a new overlapping pipeline and a gradient sparsification method. Furthermore, we design a fast continuous convergence strategy to reduce total training iterations by adaptively adjusting learning rate and updating model parameters. With the help of all the proposed methods, we gain 3.9$\times$ throughput of our training system and reduce almost 60\% of training iterations. The experimental results show that using an in-house 256 GPUs cluster, we could train a classifier of 100 million classes on Alibaba Retail Product Dataset in about five days while achieving a comparable accuracy with the naive softmax training process.",0
"Deep learning has seen extreme classification emerge as a vital area of research in recent years. This approach has proved successful in computer vision and natural language processing, but training a deep model with millions of classes presents significant challenges, particularly with memory and computation limitations. To address these issues, we propose a large-scale training system that employs a hybrid parallel training framework and a KNN softmax variation to reduce GPU memory consumption and computation costs. Additionally, we introduce a gradient sparsification method and overlapping pipeline to eliminate communication overhead and use a continuous convergence strategy to reduce training iterations. By implementing these methods, our training system achieves 3.9x throughput and 60% fewer training iterations, as demonstrated in our experiment training a 100 million class classifier on the Alibaba Retail Product Dataset in just five days with comparable accuracy to the naive softmax training process.",1
"Deep learning-based models have been very successful in achieving state-of-the-art results in many of the computer vision, speech recognition, and natural language processing tasks in the last few years. These models seem a natural fit for handling the ever-increasing scale of biometric recognition problems, from cellphone authentication to airport security systems. Deep learning-based models have increasingly been leveraged to improve the accuracy of different biometric recognition systems in recent years. In this work, we provide a comprehensive survey of more than 120 promising works on biometric recognition (including face, fingerprint, iris, palmprint, ear, voice, signature, and gait recognition), which deploy deep learning models, and show their strengths and potentials in different applications. For each biometric, we first introduce the available datasets that are widely used in the literature and their characteristics. We will then talk about several promising deep learning works developed for that biometric, and show their performance on popular public benchmarks. We will also discuss some of the main challenges while using these models for biometric recognition, and possible future directions to which research in this area is headed.",0
"Over the past few years, deep learning-based models have demonstrated remarkable success in achieving state-of-the-art results in computer vision, speech recognition, and natural language processing tasks. Given their impressive performance, these models are an ideal candidate for addressing the growing scale of biometric recognition challenges, ranging from cellphone authentication to airport security systems. As a result, deep learning approaches have been increasingly employed to enhance the accuracy of various biometric recognition systems. This paper presents a comprehensive survey of more than 120 works on biometric recognition, including face, fingerprint, iris, palmprint, ear, voice, signature, and gait recognition, that utilize deep learning models and showcases their strengths and potential applications. For each biometric, we provide an overview of the commonly used datasets and their properties, followed by a discussion of several promising deep learning works developed for that biometric and their performance on widely used public benchmarks. Moreover, we highlight some of the primary challenges associated with utilizing these models for biometric recognition and explore potential future research directions in this field.",1
"The count-min sketch (CMS) is a time and memory efficient randomized data structure that provides estimates of tokens' frequencies in a data stream, i.e. point queries, based on random hashed data. Learning-augmented CMSs improve the CMS by learning models that allow to better exploit data properties. In this paper, we focus on the learning-augmented CMS of Cai, Mitzenmacher and Adams (\textit{NeurIPS} 2018), which relies on Bayesian nonparametric (BNP) modeling of a data stream via Dirichlet process (DP) priors. This is referred to as the CMS-DP, and it leads to BNP estimates of a point query as posterior means of the point query given the hashed data. While BNPs is proved to be a powerful tool for developing robust learning-augmented CMSs, ideas and methods behind the CMS-DP are tailored to point queries under DP priors, and they can not be used for other priors or more general queries. In this paper, we present an alternative, and more flexible, derivation of the CMS-DP such that: i) it allows to make use of the Pitman-Yor process (PYP) prior, which is arguably the most popular generalization of the DP prior; ii) it can be readily applied to the more general problem of estimating range queries. This leads to develop a novel learning-augmented CMS under power-law data streams, referred to as the CMS-PYP, which relies on BNP modeling of the stream via PYP priors. Applications to synthetic and real data show that the CMS-PYP outperforms the CMS and the CMS-DP in the estimation of low-frequency tokens; this known to be a critical feature in natural language processing, where it is indeed common to encounter power-law data streams.",0
"The count-min sketch (CMS) is a data structure that uses random hashed data to estimate token frequencies in a data stream (i.e. point queries) in a time and memory efficient manner. Learning-augmented CMSs improve the accuracy of these estimates by incorporating models that leverage data properties. This paper focuses on the CMS-DP, developed by Cai, Mitzenmacher, and Adams (NeurIPS 2018), which uses Bayesian nonparametric (BNP) modeling of a data stream via Dirichlet process (DP) priors. The CMS-DP provides BNP estimates of point queries as posterior means given the hashed data. While BNPs are a powerful tool for developing robust learning-augmented CMSs, the CMS-DP is limited to point queries under DP priors and cannot be used for other priors or more general queries. This paper presents an alternative derivation of the CMS-DP that allows for the use of the Pitman-Yor process (PYP) prior, a popular generalization of the DP prior, and can be applied to estimating range queries. This results in a novel learning-augmented CMS, the CMS-PYP, which uses BNP modeling of the stream via PYP priors. The CMS-PYP outperforms the CMS and CMS-DP in estimating low-frequency tokens, a critical feature in natural language processing, where power-law data streams are common.",1
"Unsupervised representation learning techniques, such as learning word embeddings, have had a significant impact on the field of natural language processing. Similar representation learning techniques have not yet become commonplace in the context of 3D vision. This, despite the fact that the physical 3D spaces have a similar semantic structure to bodies of text: words are surrounded by words that are semantically related, just like objects are surrounded by other objects that are similar in concept and usage.   In this work, we exploit this structure in learning semantically meaningful low dimensional vector representations of objects. We learn these vector representations by mining a dataset of scanned 3D spaces using an unsupervised algorithm. We represent objects as point clouds, a flexible and general representation for 3D data, which we encode into a vector representation. We show that using our method to include context increases the ability of a clustering algorithm to distinguish different semantic classes from each other. Furthermore, we show that our algorithm produces continuous and meaningful object embeddings through interpolation experiments.",0
"The impact of unsupervised representation learning techniques, such as word embeddings, on natural language processing is significant. However, similar techniques have yet to be widely adopted in the context of 3D vision. Despite the fact that physical 3D spaces and bodies of text share a similar semantic structure, where semantically related elements are grouped together, the potential of unsupervised algorithms to learn meaningful low-dimensional vector representations of objects has not been fully explored. In this study, we take advantage of this structure to learn vector representations of objects by analyzing a dataset of scanned 3D spaces using an unsupervised algorithm. We use point clouds to represent objects, which is a flexible and general approach for 3D data. Our method enhances the ability of clustering algorithms to distinguish between different semantic classes by including context. In addition, our algorithm produces continuous and meaningful object embeddings through interpolation experiments.",1
"Graph neural networks are emerging as continuation of deep learning success w.r.t. graph data. Tens of different graph neural network variants have been proposed, most following a neighborhood aggregation scheme, where the node features are updated via aggregating features of its neighboring nodes from layer to layer. Though related research surges, the power of GNNs are still not on-par-with their counterpart CNNs in computer vision and RNNs in natural language processing. We rethink this problem from the perspective of information propagation, and propose to enhance information propagation among GNN layers by combining heterogeneous aggregations. We argue that as richer information are propagated from shallow to deep layers, the discriminative capability of features formulated by GNN can benefit from it. As our first attempt in this direction, a new generic GNN layer formulation and upon this a new GNN variant referred as HAG-Net is proposed. We empirically validate the effectiveness of HAG-Net on a number of graph classification benchmarks, and elaborate all the design options and criterions along with.",0
"Graph neural networks are a recent development in deep learning that focus on graph data. Many variations of graph neural networks have been proposed, most of which use a neighborhood aggregation scheme where node features are updated by aggregating features of neighboring nodes across layers. Despite a surge in related research, graph neural networks have not yet achieved the same level of performance as convolutional neural networks in computer vision or recurrent neural networks in natural language processing. To address this issue, we propose enhancing information propagation among graph neural network layers by combining heterogeneous aggregations. Our approach aims to increase the discriminative capability of features formulated by graph neural networks as richer information is propagated from shallow to deep layers. We introduce a new generic graph neural network layer formulation and a new variant called HAG-Net. Our empirical validation of HAG-Net on several graph classification benchmarks demonstrates its effectiveness, and we provide detailed explanations of all design options and criteria.",1
"Self-supervised learning has gained popularity because of its ability to avoid the cost of annotating large-scale datasets. It is capable of adopting self-defined pseudo labels as supervision and use the learned representations for several downstream tasks. Specifically, contrastive learning has recently become a dominant component in self-supervised learning methods for computer vision, natural language processing (NLP), and other domains. It aims at embedding augmented versions of the same sample close to each other while trying to push away embeddings from different samples. This paper provides an extensive review of self-supervised methods that follow the contrastive approach. The work explains commonly used pretext tasks in a contrastive learning setup, followed by different architectures that have been proposed so far. Next, we have a performance comparison of different methods for multiple downstream tasks such as image classification, object detection, and action recognition. Finally, we conclude with the limitations of the current methods and the need for further techniques and future directions to make substantial progress.",0
"Self-supervised learning has become popular due to its ability to avoid the costs of annotating large datasets. It can use self-defined pseudo labels as supervision and utilize learned representations for various downstream tasks. Contrastive learning has emerged as a leading component in self-supervised learning methods for various domains, including computer vision and natural language processing. The aim of contrastive learning is to embed augmented versions of the same sample close to each other while pushing away embeddings from different samples. This paper reviews extensively self-supervised methods that adopt the contrastive approach, describing the pretext tasks commonly used in a contrastive learning setup, different proposed architectures, and a performance comparison for multiple downstream tasks, such as image classification, object detection, and action recognition. Lastly, the limitations of current methods and the need for further techniques and future directions to make substantial progress are discussed.",1
"The parameters of a neural network are naturally organized in groups, some of which might not contribute to its overall performance. To prune out unimportant groups of parameters, we can include some non-differentiable penalty to the objective function, and minimize it using proximal gradient methods. In this paper, we derive the weighted proximal operator, which is a necessary component of these proximal methods, of two structured sparsity inducing penalties. Moreover, they can be approximated efficiently with a numerical solver, and despite this approximation, we prove that existing convergence guarantees are preserved when these operators are integrated as part of a generic adaptive proximal method. Finally, we show that this adaptive method, together with the weighted proximal operators derived here, is indeed capable of finding solutions with structure in their sparsity patterns, on representative examples from computer vision and natural language processing.",0
"Neural network parameters are grouped naturally, but some groups may not contribute to overall performance. To eliminate unimportant parameter groups, a non-differentiable penalty can be added to the objective function and minimized through proximal gradient methods. This paper presents the weighted proximal operator, a necessary component of these methods, for two structured sparsity inducing penalties. The operators can be efficiently approximated with a numerical solver, and existing convergence guarantees are maintained. The adaptive method, along with the derived operators, is capable of finding solutions with structured sparsity patterns in computer vision and natural language processing examples.",1
"Predicting motion of surrounding agents is critical to real-world applications of tactical path planning for autonomous driving. Due to the complex temporal dependencies and social interactions of agents, on-line trajectory prediction is a challenging task. With the development of attention mechanism in recent years, transformer model has been applied in natural language sequence processing first and then image processing. In this paper, we present a Spatial-Channel Transformer Network for trajectory prediction with attention functions. Instead of RNN models, we employ transformer model to capture the spatial-temporal features of agents. A channel-wise module is inserted to measure the social interaction between agents. We find that the Spatial-Channel Transformer Network achieves promising results on real-world trajectory prediction datasets on the traffic scenes.",0
"The ability to anticipate the movement of other entities in the vicinity is crucial for practical applications of autonomous driving's tactical path planning. The task of predicting online trajectories is challenging due to the intricate temporal dependencies and social interactions of these entities. Recently, the transformer model, which has been used in natural language and image sequence processing, has gained attention due to the development of attention mechanisms. In this study, we propose a Spatial-Channel Transformer Network that utilizes attention functions for trajectory prediction. Rather than relying on RNN models, we implement the transformer model to capture agents' spatial-temporal features. A channel-wise module is integrated to measure social interaction among agents. Our findings demonstrate that the Spatial-Channel Transformer Network produces promising results on real-world trajectory prediction datasets in traffic environments.",1
"Deep learning has shown its power in many applications, including object detection in images, natural-language understanding, and speech recognition. To make it more accessible to end users, many deep learning models are now embedded in mobile apps. Compared to offloading deep learning from smartphones to the cloud, performing machine learning on-device can help improve latency, connectivity, and power consumption. However, most deep learning models within Android apps can easily be obtained via mature reverse engineering, while the models' exposure may invite adversarial attacks. In this study, we propose a simple but effective approach to hacking deep learning models using adversarial attacks by identifying highly similar pre-trained models from TensorFlow Hub. All 10 real-world Android apps in the experiment are successfully attacked by our approach. Apart from the feasibility of the model attack, we also carry out an empirical study that investigates the characteristics of deep learning models used by hundreds of Android apps on Google Play. The results show that many of them are similar to each other and widely use fine-tuning techniques to pre-trained models on the Internet.",0
"The effectiveness of deep learning has been demonstrated in various domains, such as image recognition, natural language processing, and speech identification. To increase accessibility, numerous deep learning models have been integrated into mobile applications. Performing on-device machine learning can enhance latency, connectivity, and power consumption, unlike offloading deep learning from smartphones to the cloud. However, the exposure of deep learning models in Android apps to reverse engineering may lead to adversarial attacks. The study proposes a simple yet successful strategy for hacking deep learning models by identifying comparable pre-trained models from TensorFlow Hub. All ten Android apps in the experiment were successfully attacked using the approach. Additionally, an empirical study was conducted to analyze the features of deep learning models used in hundreds of Android apps on Google Play. The results indicate that many of these models are similar and employ fine-tuning techniques with pre-trained models on the Internet.",1
"In this paper we investigate the problem of automatically naming pieces of assembly code. Where by naming we mean assigning to an assembly function a string of words that would likely be assigned by a human reverse engineer. We formally and precisely define the framework in which our investigation takes place. That is we define the problem, we provide reasonable justifications for the choices that we made for the design of training and the tests. We performed an analysis on a large real-world corpora constituted by nearly 9 millions of functions taken from more than 22k softwares. In such framework we test baselines coming from the field of Natural Language Processing (e.g., Seq2Seq networks and Transformer). Interestingly, our evaluation shows promising results beating the state-of-the-art and reaching good performance. We investigate the applicability of tine-tuning (i.e., taking a model already trained on a large generic corpora and retraining it for a specific task). Such technique is popular and well-known in the NLP field. Our results confirm that fine-tuning is effective even when neural networks are applied to binaries. We show that a model, pre-trained on the aforementioned corpora, when fine-tuned has higher performances on specific domains (such as predicting names in system utilites, malware, etc).",0
"The focus of this paper is to explore the issue of automatically assigning names to segments of assembly code. The goal is to assign a string of words to an assembly function that would be expected from a human reverse engineer. We establish a formal and precise framework for our investigation, including the problem definition and the design of training and testing methods. To conduct our analysis, we examine a large real-world dataset of almost 9 million functions from over 22,000 software programs. We evaluate the effectiveness of baselines from Natural Language Processing, such as Seq2Seq networks and Transformer models, and find that our approach surpasses the current state-of-the-art in terms of performance. Additionally, we explore the potential use of fine-tuning, a technique commonly used in NLP, to improve the performance of pre-trained models on specific domains such as system utilities and malware. Our results show that fine-tuning can be effectively applied to neural networks for binary analysis.",1
"Summarization of long sequences into a concise statement is a core problem in natural language processing, requiring non-trivial understanding of the input. Based on the promising results of graph neural networks on highly structured data, we develop a framework to extend existing sequence encoders with a graph component that can reason about long-distance relationships in weakly structured data such as text. In an extensive evaluation, we show that the resulting hybrid sequence-graph models outperform both pure sequence models as well as pure graph models on a range of summarization tasks.",0
"A fundamental challenge in natural language processing is to summarize lengthy sequences in a brief statement, which necessitates significant comprehension of the input. Drawing on the favorable outcomes of graph neural networks for highly structured data, we establish a structure to enhance current sequence encoders with a graph element that can analyze distant associations in loosely organized data like text. Through comprehensive evaluation, we demonstrate that the resultant composite sequence-graph models surpass both pure sequence models and pure graph models in various summarization assignments.",1
"We describe a procedure for explaining neurons in deep representations by identifying compositional logical concepts that closely approximate neuron behavior. Compared to prior work that uses atomic labels as explanations, analyzing neurons compositionally allows us to more precisely and expressively characterize their behavior. We use this procedure to answer several questions on interpretability in models for vision and natural language processing. First, we examine the kinds of abstractions learned by neurons. In image classification, we find that many neurons learn highly abstract but semantically coherent visual concepts, while other polysemantic neurons detect multiple unrelated features; in natural language inference (NLI), neurons learn shallow lexical heuristics from dataset biases. Second, we see whether compositional explanations give us insight into model performance: vision neurons that detect human-interpretable concepts are positively correlated with task performance, while NLI neurons that fire for shallow heuristics are negatively correlated with task performance. Finally, we show how compositional explanations provide an accessible way for end users to produce simple ""copy-paste"" adversarial examples that change model behavior in predictable ways.",0
"Our procedure involves identifying compositional logical concepts that closely approximate neuron behavior in order to explain neurons in deep representations. By analyzing neurons compositionally, we can more precisely and expressively characterize their behavior compared to prior work that uses atomic labels as explanations. We apply this procedure to address interpretability questions in models for vision and natural language processing. Our first investigation examines the types of abstractions learned by neurons. We find that many neurons in image classification learn highly abstract but semantically coherent visual concepts, while others detect multiple unrelated features. In NLI, neurons learn shallow lexical heuristics from dataset biases. Secondly, we determine whether compositional explanations can provide insight into model performance. We observe that vision neurons detecting human-interpretable concepts are positively correlated with task performance, while NLI neurons firing for shallow heuristics are negatively correlated with task performance. Finally, we demonstrate how compositional explanations enable end users to produce simple ""copy-paste"" adversarial examples that can change model behavior in predictable ways.",1
"Existing deep learning models applied to reaction prediction in organic chemistry can reach high levels of accuracy (> 90% for Natural Language Processing-based ones). With no chemical knowledge embedded than the information learnt from reaction data, the quality of the data sets plays a crucial role in the performance of the prediction models. While human curation is prohibitively expensive, the need for unaided approaches to remove chemically incorrect entries from existing data sets is essential to improve artificial intelligence models' performance in synthetic chemistry tasks. Here we propose a machine learning-based, unassisted approach to remove chemically wrong entries from chemical reaction collections. We applied this method to the collection of chemical reactions Pistachio and to an open data set, both extracted from USPTO (United States Patent Office) patents. Our results show an improved prediction quality for models trained on the cleaned and balanced data sets. For the retrosynthetic models, the round-trip accuracy metric grows by 13 percentage points and the value of the cumulative Jensen Shannon divergence decreases by 30% compared to its original record. The coverage remains high with 97%, and the value of the class-diversity is not affected by the cleaning. The proposed strategy is the first unassisted rule-free technique to address automatic noise reduction in chemical data sets.",0
"Deep learning models for organic chemistry reaction prediction can achieve high accuracy levels (>90% for Natural Language Processing-based models). However, since these models rely solely on the information learned from reaction data, the quality of the data sets is crucial for their performance. Although human curation is too expensive, it is essential to remove chemically incorrect entries from data sets to improve synthetic chemistry tasks' artificial intelligence models. This paper proposes a machine learning-based, unassisted approach to remove chemically incorrect entries from chemical reaction collections. This approach was applied to the chemical reaction collections Pistachio and an open data set extracted from USPTO patents. The results show an improved prediction quality for models trained on the cleaned and balanced data sets. Specifically, the round-trip accuracy metric for retrosynthetic models increased by 13 percentage points, and the cumulative Jensen Shannon divergence decreased by 30% compared to the original record. The coverage remained high at 97%, and the class-diversity was not affected by the cleaning process. This unassisted, rule-free technique is the first to address automatic noise reduction in chemical data sets.",1
"In this paper, we present a novel approach for conformal prediction (CP), in which we aim to identify a set of promising prediction candidates -- in place of a single prediction. This set is guaranteed to contain a correct answer with high probability, and is well-suited for many open-ended classification tasks. In the standard CP paradigm, the predicted set can often be unusably large and also costly to obtain. This is particularly pervasive in settings where the correct answer is not unique, and the number of total possible answers is high. We first expand the CP correctness criterion to allow for additional, inferred ""admissible"" answers, which can substantially reduce the size of the predicted set while still providing valid performance guarantees. Second, we amortize costs by conformalizing prediction cascades, in which we aggressively prune implausible labels early on by using progressively stronger classifiers -- again, while still providing valid performance guarantees. We demonstrate the empirical effectiveness of our approach for multiple applications in natural language processing and computational chemistry for drug discovery.",0
"This paper introduces a new method for conformal prediction (CP) that identifies a set of potential predictions instead of a single one, which is more suitable for classification tasks that are open-ended. The traditional CP approach often leads to large and expensive predicted sets, especially in cases where there are multiple possible correct answers. To address this issue, we propose expanding the CP correctness criterion to include inferred ""admissible"" answers, which helps reduce the size of the predicted set while maintaining valid performance guarantees. In addition, we use prediction cascades to minimize costs by eliminating implausible labels early on with progressively stronger classifiers, while still ensuring valid performance guarantees. We provide empirical evidence of the effectiveness of our method in natural language processing and computational chemistry for drug discovery.",1
"Deep Neural Networks (DNNs) learn representation from data with an impressive capability, and brought important breakthroughs for processing images, time-series, natural language, audio, video, and many others. In the remote sensing field, surveys and literature revisions specifically involving DNNs algorithms' applications have been conducted in an attempt to summarize the amount of information produced in its subfields. Recently, Unmanned Aerial Vehicles (UAV) based applications have dominated aerial sensing research. However, a literature revision that combines both ""deep learning"" and ""UAV remote sensing"" thematics has not yet been conducted. The motivation for our work was to present a comprehensive review of the fundamentals of Deep Learning (DL) applied in UAV-based imagery. We focused mainly on describing classification and regression techniques used in recent applications with UAV-acquired data. For that, a total of 232 papers published in international scientific journal databases was examined. We gathered the published material and evaluated their characteristics regarding application, sensor, and technique used. We relate how DL presents promising results and has the potential for processing tasks associated with UAV-based image data. Lastly, we project future perspectives, commentating on prominent DL paths to be explored in the UAV remote sensing field. Our revision consists of a friendly-approach to introduce, commentate, and summarize the state-of-the-art in UAV-based image applications with DNNs algorithms in diverse subfields of remote sensing, grouping it in the environmental, urban, and agricultural contexts.",0
"DNNs have proved to be highly effective in learning representations from various types of data, including images, time-series, natural language, audio, and video. In the field of remote sensing, researchers have conducted surveys and literature reviews to assess the use of DNNs algorithms in different subfields. While UAV-based applications have become increasingly popular in aerial sensing research, there has not yet been a review that combines the themes of ""deep learning"" and ""UAV remote sensing."" Our motivation was to provide a comprehensive overview of the fundamentals of Deep Learning (DL) as applied to UAV-based imagery. We focused on recent applications using classification and regression techniques with UAV-acquired data. Our review examined 232 papers published in international scientific journal databases, evaluating their characteristics such as application, sensor, and technique used. We found that DL has shown promising results and has great potential for processing tasks related to UAV-based image data. We also discussed future directions for exploring DL paths in the UAV remote sensing field. Our review offers a friendly introduction, commentary, and summary of the state-of-the-art in UAV-based image applications with DNNs algorithms in various subfields of remote sensing, including environmental, urban, and agricultural contexts.",1
"We propose a new framework, Translation between Augmented Natural Languages (TANL), to solve many structured prediction language tasks including joint entity and relation extraction, nested named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. Instead of tackling the problem by training task-specific discriminative classifiers, we frame it as a translation task between augmented natural languages, from which the task-relevant information can be easily extracted. Our approach can match or outperform task-specific models on all tasks, and in particular, achieves new state-of-the-art results on joint entity and relation extraction (CoNLL04, ADE, NYT, and ACE2005 datasets), relation classification (FewRel and TACRED), and semantic role labeling (CoNLL-2005 and CoNLL-2012). We accomplish this while using the same architecture and hyperparameters for all tasks and even when training a single model to solve all tasks at the same time (multi-task learning). Finally, we show that our framework can also significantly improve the performance in a low-resource regime, thanks to better use of label semantics.",0
"A new framework called Translation between Augmented Natural Languages (TANL) is being presented as a solution to various structured prediction language tasks such as entity and relation extraction, named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. Instead of using task-specific classifiers, the problem is approached as a translation task between augmented natural languages, which makes it easier to extract task-relevant information. This approach performs as well as or better than task-specific models on all tasks, achieving new state-of-the-art results on joint entity and relation extraction, relation classification, and semantic role labeling. Furthermore, the same architecture and hyperparameters can be used for all tasks, even when training a single model. Additionally, the framework improves performance in low-resource situations by better utilizing label semantics.",1
"Cycle-consistent training is widely used for jointly learning a forward and inverse mapping between two domains of interest without the cumbersome requirement of collecting matched pairs within each domain. In this regard, the implicit assumption is that there exists (at least approximately) a ground-truth bijection such that a given input from either domain can be accurately reconstructed from successive application of the respective mappings. But in many applications no such bijection can be expected to exist and large reconstruction errors can compromise the success of cycle-consistent training. As one important instance of this limitation, we consider practically-relevant situations where there exists a many-to-one or surjective mapping between domains. To address this regime, we develop a conditional variational autoencoder (CVAE) approach that can be viewed as converting surjective mappings to implicit bijections whereby reconstruction errors in both directions can be minimized, and as a natural byproduct, realistic output diversity can be obtained in the one-to-many direction. As theoretical motivation, we analyze a simplified scenario whereby minima of the proposed CVAE-based energy function align with the recovery of ground-truth surjective mappings. On the empirical side, we consider a synthetic image dataset with known ground-truth, as well as a real-world application involving natural language generation from knowledge graphs and vice versa, a prototypical surjective case. For the latter, our CVAE pipeline can capture such many-to-one mappings during cycle training while promoting textural diversity for graph-to-text tasks. Our code is available at github.com/QipengGuo/CycleGT   *A condensed version of this paper has been accepted to AISTATS 2021. This version contains additional content and updates.",0
"Cycle-consistent training is a popular method for simultaneously learning a forward and inverse mapping between two domains, without requiring matched pairs within each domain. However, this approach assumes the existence of a ground-truth bijection, which may not always be the case. This can lead to large reconstruction errors and undermine the effectiveness of cycle-consistent training, particularly in situations where there is a many-to-one or surjective mapping between domains. To overcome this limitation, we propose a conditional variational autoencoder (CVAE) approach that can convert surjective mappings to implicit bijections, minimizing reconstruction errors in both directions and promoting output diversity in the one-to-many direction. We provide theoretical analysis and empirical evidence of the effectiveness of our approach, including a real-world application involving natural language generation from knowledge graphs. Our code is publicly available on GitHub.",1
"We propose a generalization of transformer neural network architecture for arbitrary graphs. The original transformer was designed for Natural Language Processing (NLP), which operates on fully connected graphs representing all connections between the words in a sequence. Such architecture does not leverage the graph connectivity inductive bias, and can perform poorly when the graph topology is important and has not been encoded into the node features. We introduce a graph transformer with four new properties compared to the standard model. First, the attention mechanism is a function of the neighborhood connectivity for each node in the graph. Second, the positional encoding is represented by the Laplacian eigenvectors, which naturally generalize the sinusoidal positional encodings often used in NLP. Third, the layer normalization is replaced by a batch normalization layer, which provides faster training and better generalization performance. Finally, the architecture is extended to edge feature representation, which can be critical to tasks s.a. chemistry (bond type) or link prediction (entity relationship in knowledge graphs). Numerical experiments on a graph benchmark demonstrate the performance of the proposed graph transformer architecture. This work closes the gap between the original transformer, which was designed for the limited case of line graphs, and graph neural networks, that can work with arbitrary graphs. As our architecture is simple and generic, we believe it can be used as a black box for future applications that wish to consider transformer and graphs.",0
"We present a modified transformer neural network architecture that can accommodate any graph. The original transformer architecture was developed for Natural Language Processing and is not optimized for graph connectivity. It performs poorly when the graph topology is critical but not encoded into the node features. We introduce a graph transformer that has four distinct features compared to the standard model. Firstly, the attention mechanism is a function of the node's neighborhood connectivity in the graph. Secondly, we represent the positional encoding using Laplacian eigenvectors, which are an extension of the sinusoidal positional encodings used in NLP. Thirdly, we replace layer normalization with a batch normalization layer, resulting in better generalization performance and faster training. Finally, we extend the architecture to include edge feature representation, which is essential in tasks such as chemistry or link prediction. Numerical experiments on a graph benchmark demonstrate the effectiveness of our proposed architecture, which bridges the gap between the original transformer and graph neural networks. We believe that our simple and generic architecture can be used as a black box for future applications that require transformer and graph integration.",1
"Many irregular domains such as social networks, financial transactions, neuron connections, and natural language constructs are represented using graph structures. In recent years, a variety of graph neural networks (GNNs) have been successfully applied for representation learning and prediction on such graphs. In many of the real-world applications, the underlying graph changes over time, however, most of the existing GNNs are inadequate for handling such dynamic graphs. In this paper we propose a novel technique for learning embeddings of dynamic graphs using a tensor algebra framework. Our method extends the popular graph convolutional network (GCN) for learning representations of dynamic graphs using the recently proposed tensor M-product technique. Theoretical results presented establish a connection between the proposed tensor approach and spectral convolution of tensors. The proposed method TM-GCN is consistent with the Message Passing Neural Network (MPNN) framework, accounting for both spatial and temporal message passing. Numerical experiments on real-world datasets demonstrate the performance of the proposed method for edge classification and link prediction tasks on dynamic graphs. We also consider an application related to the COVID-19 pandemic, and show how our method can be used for early detection of infected individuals from contact tracing data.",0
"Graph structures are commonly used to represent diverse domains like social networks, neuron connections, financial transactions, and natural language constructs. Several graph neural networks (GNNs) have proven effective for prediction and representation learning on such irregular domains. However, most existing GNNs are ill-suited to handle dynamic graphs, which are prevalent in real-world applications. Therefore, we propose a novel method for learning embeddings of dynamic graphs using a tensor algebra framework. Our approach extends the graph convolutional network (GCN) using the tensor M-product technique. We demonstrate the consistency of our proposed method, named TM-GCN, with the Message Passing Neural Network (MPNN) framework, which accounts for both spatial and temporal message passing. Our theoretical results establish a connection between the proposed tensor approach and spectral convolution of tensors. We perform numerical experiments on real-world datasets, showcasing the effectiveness of our proposed method in edge classification and link prediction tasks on dynamic graphs. Additionally, we apply our method to contact tracing data related to the COVID-19 pandemic, demonstrating its ability for early detection of infected individuals.",1
"Visual question answering requires a deep understanding of both images and natural language. However, most methods mainly focus on visual concept; such as the relationships between various objects. The limited use of object categories combined with their relationships or simple question embedding is insufficient for representing complex scenes and explaining decisions. To address this limitation, we propose the use of text expressions generated for images, because such expressions have few structural constraints and can provide richer descriptions of images. The generated expressions can be incorporated with visual features and question embedding to obtain the question-relevant answer. A joint-embedding multi-head attention network is also proposed to model three different information modalities with co-attention. We quantitatively and qualitatively evaluated the proposed method on the VQA v2 dataset and compared it with state-of-the-art methods in terms of answer prediction. The quality of the generated expressions was also evaluated on the RefCOCO, RefCOCO+, and RefCOCOg datasets. Experimental results demonstrate the effectiveness of the proposed method and reveal that it outperformed all of the competing methods in terms of both quantitative and qualitative results.",0
"Understanding both images and natural language is crucial for visual question answering. However, existing methods primarily focus on visual concepts, such as object relationships, which limits their ability to represent complex scenes and explain decisions. To overcome this limitation, we propose incorporating text expressions generated for images, which can provide richer descriptions and have fewer structural constraints. By combining these expressions with visual features and question embedding, we can obtain more relevant answers. Our approach employs a joint-embedding multi-head attention network that models three different information modalities with co-attention. We evaluated our method on the VQA v2 dataset and compared it with state-of-the-art methods, showing that it outperforms them in both quantitative and qualitative results. We also evaluated the quality of the generated expressions on RefCOCO, RefCOCO+, and RefCOCOg datasets, further demonstrating the effectiveness of our proposed method.",1
"A common vision from science fiction is that robots will one day inhabit our physical spaces, sense the world as we do, assist our physical labours, and communicate with us through natural language. Here we study how to design artificial agents that can interact naturally with humans using the simplification of a virtual environment. This setting nevertheless integrates a number of the central challenges of artificial intelligence (AI) research: complex visual perception and goal-directed physical control, grounded language comprehension and production, and multi-agent social interaction. To build agents that can robustly interact with humans, we would ideally train them while they interact with humans. However, this is presently impractical. Therefore, we approximate the role of the human with another learned agent, and use ideas from inverse reinforcement learning to reduce the disparities between human-human and agent-agent interactive behaviour. Rigorously evaluating our agents poses a great challenge, so we develop a variety of behavioural tests, including evaluation by humans who watch videos of agents or interact directly with them. These evaluations convincingly demonstrate that interactive training and auxiliary losses improve agent behaviour beyond what is achieved by supervised learning of actions alone. Further, we demonstrate that agent capabilities generalise beyond literal experiences in the dataset. Finally, we train evaluation models whose ratings of agents agree well with human judgement, thus permitting the evaluation of new agent models without additional effort. Taken together, our results in this virtual environment provide evidence that large-scale human behavioural imitation is a promising tool to create intelligent, interactive agents, and the challenge of reliably evaluating such agents is possible to surmount.",0
"The idea of robots coexisting with humans, helping us with physical tasks and communicating naturally, is a common theme in science fiction. In this study, we explore how to design artificial agents capable of interacting with humans in a natural way by using a simplified virtual environment. This environment presents several challenges faced by artificial intelligence research, including visual perception, physical control, language comprehension and production, and social interaction. Ideally, we would train agents to interact with humans directly, but this is not currently feasible, so we use another learned agent as a proxy and apply inverse reinforcement learning to reduce discrepancies between human-human and agent-agent interactions. We develop various tests, including evaluation by humans who observe videos of agents or interact with them directly, to rigorously assess agent performance. Our results show that interactive training and auxiliary losses improve agent behavior beyond what is achieved by supervised learning alone. We also demonstrate that agent capabilities generalize beyond specific experiences in the dataset. Finally, we train evaluation models that agree with human judgment, simplifying the evaluation of new agent models. Our findings suggest that large-scale human behavioral imitation can help create intelligent, interactive agents, and that it is possible to overcome the challenges of reliably evaluating such agents in a virtual environment.",1
"On-device Deep Neural Networks (DNNs) have recently gained more attention due to the increasing computing power of the mobile devices and the number of applications in Computer Vision (CV), Natural Language Processing (NLP), and Internet of Things (IoTs). Unfortunately, the existing efficient convolutional neural network (CNN) architectures designed for CV tasks are not directly applicable to NLP tasks and the tiny Recurrent Neural Network (RNN) architectures have been designed primarily for IoT applications. In NLP applications, although model compression has seen initial success in on-device text classification, there are at least three major challenges yet to be addressed: adversarial robustness, explainability, and personalization. Here we attempt to tackle these challenges by designing a new training scheme for model compression and adversarial robustness, including the optimization of an explainable feature mapping objective, a knowledge distillation objective, and an adversarially robustness objective. The resulting compressed model is personalized using on-device private training data via fine-tuning. We perform extensive experiments to compare our approach with both compact RNN (e.g., FastGRNN) and compressed RNN (e.g., PRADO) architectures in both natural and adversarial NLP test settings.",0
"The growing computing power of mobile devices and the rise in applications for Computer Vision (CV), Natural Language Processing (NLP), and Internet of Things (IoTs) has led to an increased focus on on-device Deep Neural Networks (DNNs). However, the efficient convolutional neural network (CNN) architectures used for CV tasks are not directly applicable to NLP tasks, and tiny Recurrent Neural Network (RNN) architectures have mainly been designed for IoT applications. While model compression has shown initial success in on-device text classification for NLP applications, there remain three significant challenges: adversarial robustness, explainability, and personalization. This paper aims to address these challenges by proposing a new training scheme for model compression and adversarial robustness, which includes an explainable feature mapping objective, a knowledge distillation objective, and an adversarially robustness objective. The resulting compressed model is then personalized through on-device private training data via fine-tuning. Extensive experiments were conducted to compare this approach with both compact RNN (e.g., FastGRNN) and compressed RNN (e.g., PRADO) architectures for natural and adversarial NLP test settings.",1
"In this paper we propose a new framework - MoViLan (Modular Vision and Language) for execution of visually grounded natural language instructions for day to day indoor household tasks. While several data-driven, end-to-end learning frameworks have been proposed for targeted navigation tasks based on the vision and language modalities, performance on recent benchmark data sets revealed the gap in developing comprehensive techniques for long horizon, compositional tasks (involving manipulation and navigation) with diverse object categories, realistic instructions and visual scenarios with non-reversible state changes. We propose a modular approach to deal with the combined navigation and object interaction problem without the need for strictly aligned vision and language training data (e.g., in the form of expert demonstrated trajectories). Such an approach is a significant departure from the traditional end-to-end techniques in this space and allows for a more tractable training process with separate vision and language data sets. Specifically, we propose a novel geometry-aware mapping technique for cluttered indoor environments, and a language understanding model generalized for household instruction following. We demonstrate a significant increase in success rates for long-horizon, compositional tasks over the baseline on the recently released benchmark data set-ALFRED.",0
"MoViLan (Modular Vision and Language) is a new framework suggested in this paper for carrying out natural language instructions related to daily indoor household tasks. While there have been various data-driven frameworks proposed for targeted navigation tasks based on vision and language modalities, recent benchmark data sets have exposed the lack of comprehensive techniques for long horizon, compositional tasks that involve manipulation and navigation with diverse object categories, realistic instructions, and visual scenarios with non-reversible state changes. A modular approach is proposed to tackle the joint navigation and object interaction problem, without the need for strictly aligned vision and language training data. This approach differs from traditional end-to-end techniques and facilitates a more manageable training process with separate vision and language data sets. The paper suggests a novel geometry-aware mapping technique for cluttered indoor environments and a language understanding model generalized for household instruction following. The authors also demonstrate a significant improvement in success rates for long-horizon, compositional tasks over the baseline on the recently released benchmark data set-ALFRED.",1
"Discrete latent spaces in variational autoencoders have been shown to effectively capture the data distribution for many real-world problems such as natural language understanding, human intent prediction, and visual scene representation. However, discrete latent spaces need to be sufficiently large to capture the complexities of real-world data, rendering downstream tasks computationally challenging. For instance, performing motion planning in a high-dimensional latent representation of the environment could be intractable. We consider the problem of sparsifying the discrete latent space of a trained conditional variational autoencoder, while preserving its learned multimodality. As a post hoc latent space reduction technique, we use evidential theory to identify the latent classes that receive direct evidence from a particular input condition and filter out those that do not. Experiments on diverse tasks, such as image generation and human behavior prediction, demonstrate the effectiveness of our proposed technique at reducing the discrete latent sample space size of a model while maintaining its learned multimodality.",0
"Variational autoencoders with discrete latent spaces have proven effective in capturing data distribution for various real-world applications, including natural language understanding, visual scene representation, and human intent prediction. However, the need for sufficiently large discrete latent spaces to accommodate the complexities of real-world data makes downstream tasks computationally challenging. For example, performing motion planning in a high-dimensional latent representation of the environment may prove infeasible. Our approach addresses this issue by using evidential theory as a post hoc latent space reduction technique to sparsify the discrete latent space of a trained conditional variational autoencoder, all while preserving its learned multimodality. We identify latent classes that receive direct evidence from a specific input condition using this technique and filter out those that do not. Our experiments on various tasks, including image generation and human behavior prediction, demonstrate the effectiveness of our proposed method in reducing the sample space size of a model's discrete latent space while retaining its learned multimodality.",1
"Policy specification is a process by which a human can initialize a robot's behaviour and, in turn, warm-start policy optimization via Reinforcement Learning (RL). While policy specification/design is inherently a collaborative process, modern methods based on Learning from Demonstration or Deep RL lack the model interpretability and accessibility to be classified as such. Current state-of-the-art methods for policy specification rely on black-box models, which are an insufficient means of collaboration for non-expert users: These models provide no means of inspecting policies learnt by the agent and are not focused on creating a usable modality for teaching robot behaviour. In this paper, we propose a novel machine learning framework that enables humans to 1) specify, through natural language, interpretable policies in the form of easy-to-understand decision trees, 2) leverage these policies to warm-start reinforcement learning and 3) outperform baselines that lack our natural language initialization mechanism. We train our approach by collecting a first-of-its-kind corpus mapping free-form natural language policy descriptions to decision tree-based policies. We show that our novel framework translates natural language to decision trees with a 96% and 97% accuracy on a held-out corpus across two domains, respectively. Finally, we validate that policies initialized with natural language commands are able to significantly outperform relevant baselines (p < 0.001) that do not benefit from our natural language-based warm-start technique.",0
"The process of policy specification involves a human initiating a robot's behavior and jumpstarting policy optimization through Reinforcement Learning (RL). Although policy design is a collaborative process, current methods such as Deep RL and Learning from Demonstration lack interpretability and accessibility to be considered as such. The use of black-box models for policy specification is not sufficient for non-expert users, as they do not allow for policy inspection or the creation of a usable teaching modality. In this study, we introduce a new machine learning framework that enables humans to specify policies in the form of easily understandable decision trees using natural language. Our framework outperforms baselines without the natural language initialization mechanism. We trained our approach by collecting a corpus of free-form natural language policy descriptions mapped to decision tree-based policies. We demonstrated that our framework has high accuracy in translating natural language to decision trees and that policies initialized with natural language commands outperform relevant baselines without the natural language-based warm-start technique.",1
"Self-attention architectures, which are rapidly pushing the frontier in natural language processing, demonstrate a surprising depth-inefficient behavior: previous works indicate that increasing the internal representation (network width) is just as useful as increasing the number of self-attention layers (network depth). We theoretically predict a width-dependent transition between depth-efficiency and depth-inefficiency in self-attention. We conduct systematic empirical ablations on networks of depths 6 to 48 that clearly reveal the theoretically predicted behaviors, and provide explicit quantitative suggestions regarding the optimal depth-to-width allocation for a given self-attention network size. The race towards beyond 1-Trillion parameter language models renders informed guidelines for increasing self-attention depth and width in tandem an essential ingredient. Our guidelines elucidate the depth-to-width trade-off in self-attention networks of sizes up to the scale of GPT3 (which we project to be too deep for its size), and beyond, marking an unprecedented width of 30K as optimal for a 1-Trillion parameter network.",0
"Self-attention architectures are advancing natural language processing, but they have an unexpected inefficiency in depth. Increasing the network width is just as effective as increasing the number of self-attention layers. We predict a transition from depth-efficiency to depth-inefficiency based on network width. Through empirical ablations, we confirm our predictions and provide recommendations for the optimal depth-to-width ratio in self-attention networks up to the size of GPT3. Our guidelines are crucial for increasing self-attention depth and width in large language models. For a 1-Trillion parameter network, an unprecedented width of 30K is optimal.",1
"The task of Video Question Answering (VideoQA) consists in answering natural language questions about a video and serves as a proxy to evaluate the performance of a model in scene sequence understanding. Most methods designed for VideoQA up-to-date are end-to-end deep learning architectures which struggle at complex temporal and causal reasoning and provide limited transparency in reasoning steps. We present the HySTER: a Hybrid Spatio-Temporal Event Reasoner to reason over physical events in videos. Our model leverages the strength of deep learning methods to extract information from video frames with the reasoning capabilities and explainability of symbolic artificial intelligence in an answer set programming framework. We define a method based on general temporal, causal and physics rules which can be transferred across tasks. We apply our model to the CLEVRER dataset and demonstrate state-of-the-art results in question answering accuracy. This work sets the foundations for the incorporation of inductive logic programming in the field of VideoQA.",0
"The objective of Video Question Answering (VideoQA) is to answer questions in natural language related to a video, serving as a means to assess the ability of a model in comprehending scene sequences. Most of the VideoQA techniques developed so far utilize end-to-end deep learning architectures, which face difficulties in complex temporal and causal reasoning and lack transparency in the reasoning process. We introduce HySTER, a Hybrid Spatio-Temporal Event Reasoner that reasons about physical events in videos. Our model capitalizes on the strengths of deep learning techniques to extract information from video frames while also incorporating the reasoning and explainability features of symbolic artificial intelligence in an answer set programming framework. We define a method based on general principles of temporal, causal, and physical rules that can be applied across various tasks. We demonstrate the effectiveness of our model on the CLEVRER dataset, achieving state-of-the-art accuracy in question answering. This work lays the groundwork for the integration of inductive logic programming in the field of VideoQA.",1
"Training robust deep learning models for down-stream tasks is a critical challenge. Research has shown that down-stream models can be easily fooled with adversarial inputs that look like the training data, but slightly perturbed, in a way imperceptible to humans. Understanding the behavior of natural language models under these attacks is crucial to better defend these models against such attacks. In the black-box attack setting, where no access to model parameters is available, the attacker can only query the output information from the targeted model to craft a successful attack. Current black-box state-of-the-art models are costly in both computational complexity and number of queries needed to craft successful adversarial examples. For real world scenarios, the number of queries is critical, where less queries are desired to avoid suspicion towards an attacking agent. In this paper, we propose Explain2Attack, a black-box adversarial attack on text classification task. Instead of searching for important words to be perturbed by querying the target model, Explain2Attack employs an interpretable substitute model from a similar domain to learn word importance scores. We show that our framework either achieves or out-performs attack rates of the state-of-the-art models, yet with lower queries cost and higher efficiency.",0
"Developing robust deep learning models for downstream tasks is a significant challenge due to the susceptibility of downstream models to adversarial inputs that resemble the training data but are slightly altered in a way that is undetectable to humans. It is essential to comprehend how natural language models behave under such attacks to better safeguard them from such assaults. In the case of a black-box attack, where the attacker lacks access to the model parameters, the only option is to obtain output information from the targeted model to create a successful attack. However, state-of-the-art black-box models are expensive in terms of computational complexity and the number of queries required to create successful adversarial examples. In real-world situations, the number of queries is crucial, and fewer queries are preferable to avoid suspicion of a malicious agent. This paper proposes Explain2Attack, a black-box adversarial attack on text classification tasks that employs an interpretable substitute model from a similar domain to learn word importance scores instead of querying the target model to identify important words to be perturbed. The results demonstrate that our framework outperforms state-of-the-art models in terms of attack rates while being more efficient with lower query costs.",1
"In this paper, we propose CI-VI an efficient and scalable solver for semi-implicit variational inference (SIVI). Our method, first, maps SIVI's evidence lower bound (ELBO) to a form involving a nonlinear functional nesting of expected values and then develops a rigorous optimiser capable of correctly handling bias inherent to nonlinear nested expectations using an extrapolation-smoothing mechanism coupled with gradient sketching. Our theoretical results demonstrate convergence to a stationary point of the ELBO in general non-convex settings typically arising when using deep network models and an order of $O(t^{-\frac{4}{5}})$ gradient-bias-vanishing rate. We believe these results generalise beyond the specific nesting arising from SIVI to other forms. Finally, in a set of experiments, we demonstrate the effectiveness of our algorithm in approximating complex posteriors on various data-sets including those from natural language processing.",0
"CI-VI is a solver proposed in this paper for semi-implicit variational inference (SIVI) that is both efficient and scalable. Our approach involves first mapping the evidence lower bound (ELBO) of SIVI to a form that includes a nonlinear functional nesting of expected values. We then develop an optimiser that can handle the inherent bias in nonlinear nested expectations by using an extrapolation-smoothing mechanism coupled with gradient sketching. The theoretical results show that the method converges to a stationary point of the ELBO in general non-convex settings, such as those found in deep network models, with an O(t^-4/5) gradient-bias-vanishing rate. We believe that these results can be applied to other forms of nesting beyond SIVI. Finally, we demonstrate the effectiveness of the algorithm in approximating complex posteriors on various data-sets, including those from natural language processing, through a series of experiments.",1
"Identifier names convey useful information about the intended semantics of code. Name-based program analyses use this information, e.g., to detect bugs, to predict types, and to improve the readability of code. At the core of name-based analyses are semantic representations of identifiers, e.g., in the form of learned embeddings. The high-level goal of such a representation is to encode whether two identifiers, e.g., len and size, are semantically similar. Unfortunately, it is currently unclear to what extent semantic representations match the semantic relatedness and similarity perceived by developers. This paper presents IdBench, the first benchmark for evaluating semantic representations against a ground truth created from thousands of ratings by 500 software developers. We use IdBench to study state-of-the-art embedding techniques proposed for natural language, an embedding technique specifically designed for source code, and lexical string distance functions. Our results show that the effectiveness of semantic representations varies significantly and that the best available embeddings successfully represent semantic relatedness. On the downside, no existing technique provides a satisfactory representation of semantic similarities, among other reasons because identifiers with opposing meanings are incorrectly considered to be similar, which may lead to fatal mistakes, e.g., in a refactoring tool. Studying the strengths and weaknesses of the different techniques shows that they complement each other. As a first step toward exploiting this complementarity, we present an ensemble model that combines existing techniques and that clearly outperforms the best available semantic representation.",0
"The names given to identifiers contain valuable information regarding the intended meaning of code. This information is utilized by name-based program analyses to detect bugs, forecast types, and enhance code readability. These analyses rely on semantic representations of identifiers, such as learned embeddings, which encode the semantic similarity between two identifiers, such as ""len"" and ""size"". However, it remains uncertain to what extent these representations match the perceived semantic relatedness by developers. To address this issue, the paper presents IdBench, the first benchmark that evaluates semantic representations against a ground truth based on ratings from 500 software developers. The study compares state-of-the-art embedding techniques for natural language, a technique specifically designed for source code, and lexical string distance functions. The results reveal that the effectiveness of semantic representations varies significantly, and that existing techniques fail to provide a satisfactory representation of semantic similarity. These shortcomings can lead to fatal mistakes in a refactoring tool, as opposing meanings are incorrectly considered as similar. However, combining existing techniques in an ensemble model improves performance and highlights their complementary strengths and weaknesses.",1
"Video captioning is a popular task that challenges models to describe events in videos using natural language. In this work, we investigate the ability of various visual feature representations derived from state-of-the-art convolutional neural networks to capture high-level semantic context. We introduce the Weighted Additive Fusion Transformer with Memory Augmented Encoders (WAFTM), a captioning model that incorporates memory in a transformer encoder and uses a novel method, to fuse features, that ensures due importance is given to more significant representations. We illustrate a gain in performance realized by applying Word-Piece Tokenization and a popular REINFORCE algorithm. Finally, we benchmark our model on two datasets and obtain a CIDEr of 92.4 on MSVD and a METEOR of 0.091 on the ActivityNet Captions Dataset.",0
"The task of captioning videos using natural language is widely popular and challenging for models. Our study explores the ability of diverse visual feature representations obtained from cutting-edge convolutional neural networks to grasp the high-level semantic context. We present the Weighted Additive Fusion Transformer with Memory Augmented Encoders (WAFTM), a captioning model that incorporates memory in a transformer encoder. It utilizes a new feature fusion approach that ensures more significant representations are given due importance. We also demonstrate an improvement in performance by implementing Word-Piece Tokenization and a well-known REINFORCE algorithm. Lastly, we evaluate our model on two datasets, achieving a CIDEr of 92.4 on MSVD and a METEOR of 0.091 on the ActivityNet Captions Dataset.",1
"In this paper, we teach machines to understand visuals and natural language by learning the mapping between sentences and noisy video snippets without explicit annotations. Firstly, we define a self-supervised learning framework that captures the cross-modal information. A novel adversarial learning module is then introduced to explicitly handle the noises in the natural videos, where the subtitle sentences are not guaranteed to be strongly corresponded to the video snippets. For training and evaluation, we contribute a new dataset `ApartmenTour' that contains a large number of online videos and subtitles. We carry out experiments on the bidirectional retrieval tasks between sentences and videos, and the results demonstrate that our proposed model achieves the state-of-the-art performance on both retrieval tasks and exceeds several strong baselines. The dataset can be downloaded at https://github.com/zyj-13/WAL.",0
"The focus of this paper is on teaching machines to comprehend both visuals and natural language. This is accomplished by developing a method that can learn the connection between sentences and video snippets in the absence of explicit annotations. Our approach involves creating a self-supervised learning framework that can capture cross-modal information. To address the noise present in natural videos, we introduce an innovative adversarial learning module. This module handles the instances where subtitle sentences and video snippets do not correspond strongly. To validate our method, we introduce a new dataset called `ApartmenTour' that includes a large number of online videos and subtitles. Our experiments demonstrate that our proposed model outperforms several strong baselines and achieves state-of-the-art performance on both retrieval tasks. Interested parties can download the dataset at https://github.com/zyj-13/WAL.",1
"Transformer has become the new standard method in natural language processing (NLP), and it also attracts research interests in computer vision area. In this paper we investigate the application of Transformer in Image Quality (TRIQ) assessment. Following the original Transformer encoder employed in Vision Transformer (ViT), we propose an architecture of using a shallow Transformer encoder on the top of a feature map extracted by convolution neural networks (CNN). Adaptive positional embedding is employed in the Transformer encoder to handle images with arbitrary resolutions. Different settings of Transformer architectures have been investigated on publicly available image quality databases. We have found that the proposed TRIQ architecture achieves outstanding performance. The implementation of TRIQ is published on Github (https://github.com/junyongyou/triq).",0
"The Transformer method has become widely used in natural language processing (NLP) and has also piqued interest in computer vision research. This study explores the application of Transformer in assessing Image Quality (TRIQ). The proposed architecture uses a shallow Transformer encoder on top of a feature map extracted by convolution neural networks (CNN), similar to the encoder used in Vision Transformer (ViT). Adaptive positional embedding is incorporated in the Transformer encoder to handle images of varying resolutions. Different Transformer architectures are tested on publicly available image quality databases, and the proposed TRIQ architecture is found to perform exceptionally well. The implementation of TRIQ is available on Github at https://github.com/junyongyou/triq.",1
"Uncertainty quantification (UQ) plays a pivotal role in reduction of uncertainties during both optimization and decision making processes. It can be applied to solve a variety of real-world applications in science and engineering. Bayesian approximation and ensemble learning techniques are two most widely-used UQ methods in the literature. In this regard, researchers have proposed different UQ methods and examined their performance in a variety of applications such as computer vision (e.g., self-driving cars and object detection), image processing (e.g., image restoration), medical image analysis (e.g., medical image classification and segmentation), natural language processing (e.g., text classification, social media texts and recidivism risk-scoring), bioinformatics, etc. This study reviews recent advances in UQ methods used in deep learning. Moreover, we also investigate the application of these methods in reinforcement learning (RL). Then, we outline a few important applications of UQ methods. Finally, we briefly highlight the fundamental research challenges faced by UQ methods and discuss the future research directions in this field.",0
"During optimization and decision-making processes, uncertainty quantification (UQ) is essential in reducing uncertainties. UQ has various applications in science and engineering, and the most widely-used methods in the literature are Bayesian approximation and ensemble learning techniques. Recent research has proposed different UQ methods and examined their performance in various applications such as computer vision, image processing, medical image analysis, natural language processing, and bioinformatics. This study reviews recent advances in UQ methods used in deep learning and their application in reinforcement learning (RL), as well as outlining important applications of UQ methods. Lastly, the fundamental research challenges faced by UQ methods are briefly highlighted, and future research directions in this field are discussed.",1
"When minimizing the empirical risk in binary classification, it is a common practice to replace the zero-one loss with a surrogate loss to make the learning objective feasible to optimize. Examples of well-known surrogate losses for binary classification include the logistic loss, hinge loss, and sigmoid loss. It is known that the choice of a surrogate loss can highly influence the performance of the trained classifier and therefore it should be carefully chosen. Recently, surrogate losses that satisfy a certain symmetric condition (aka., symmetric losses) have demonstrated their usefulness in learning from corrupted labels. In this article, we provide an overview of symmetric losses and their applications. First, we review how a symmetric loss can yield robust classification from corrupted labels in balanced error rate (BER) minimization and area under the receiver operating characteristic curve (AUC) maximization. Then, we demonstrate how the robust AUC maximization method can benefit natural language processing in the problem where we want to learn only from relevant keywords and unlabeled documents. Finally, we conclude this article by discussing future directions, including potential applications of symmetric losses for reliable machine learning and the design of non-symmetric losses that can benefit from the symmetric condition.",0
"In binary classification, it is common to use surrogate losses instead of the zero-one loss to minimize the empirical risk and make the learning objective easier to optimize. Examples of well-known surrogate losses include the logistic loss, hinge loss, and sigmoid loss. Careful consideration should be given to the choice of surrogate loss as it can significantly affect the performance of the trained classifier. Recently, symmetric losses have been found useful in learning from corrupted labels. This article provides an overview of symmetric losses and their applications. Firstly, we discuss how a symmetric loss can lead to robust classification in balanced error rate (BER) minimization and area under the receiver operating characteristic curve (AUC) maximization, even with corrupted labels. Secondly, we demonstrate how the robust AUC maximization method can be beneficial in natural language processing for learning from relevant keywords and unlabeled documents. Finally, we discuss future directions, including potential applications of symmetric losses for reliable machine learning and the design of non-symmetric losses that can benefit from the symmetric condition.",1
"Recently, a special kind of graph, i.e., supernet, which allows two nodes connected by multi-choice edges, has exhibited its power in neural architecture search (NAS) by searching for better architectures for computer vision (CV) and natural language processing (NLP) tasks. In this paper, we discover that the design of such discrete architectures also appears in many other important learning tasks, e.g., logical chain inference in knowledge graphs (KGs) and meta-path discovery in heterogeneous information networks (HINs). Thus, we are motivated to generalize the supernet search problem on a broader horizon. However, none of the existing works are effective since the supernet topology is highly task-dependent and diverse. To address this issue, we propose to tensorize the supernet, i.e., unify the subgraph search problems by a tensor formulation and encode the topology inside the supernet by a tensor network. We further propose an efficient algorithm that admits both stochastic and deterministic objectives to solve the search problem. Finally, we perform extensive experiments on diverse learning tasks, i.e., architecture design for CV, logic inference for KG, and meta-path discovery for HIN. Empirical results demonstrate that our method leads to better performance and architectures.",0
"A graph called a supernet has shown its effectiveness in neural architecture search for computer vision and natural language processing by allowing multi-choice edge connections between nodes. The authors of this paper have found that this type of architecture can also be applied to other learning tasks such as logical chain inference in knowledge graphs and meta-path discovery in heterogeneous information networks. However, existing supernet search methods are not effective due to the highly diverse and task-dependent nature of the supernet topology. To address this issue, the authors propose a method to tensorize the supernet, which unifies subgraph search problems through a tensor formulation that encodes topology within a tensor network. The authors also propose an algorithm that can accommodate both stochastic and deterministic objectives for solving the search problem. The proposed method is evaluated through experiments on various learning tasks, including architecture design for computer vision, logic inference for knowledge graphs, and meta-path discovery for heterogeneous information networks. The empirical results demonstrate that the proposed method leads to better performance and architectures.",1
"Machine Learning has been applied in a wide range of tasks throughout the last years, ranging from image classification to autonomous driving and natural language processing. Restricted Boltzmann Machine (RBM) has received recent attention and relies on an energy-based structure to model data probability distributions. Notwithstanding, such a technique is susceptible to adversarial manipulation, i.e., slightly or profoundly modified data. An alternative to overcome the adversarial problem lies in the Generative Adversarial Networks (GAN), capable of modeling data distributions and generating adversarial data that resemble the original ones. Therefore, this work proposes to artificially generate RBMs using Adversarial Learning, where pre-trained weight matrices serve as the GAN inputs. Furthermore, it proposes to sample copious amounts of matrices and combine them into ensembles, alleviating the burden of training new models'. Experimental results demonstrate the suitability of the proposed approach under image reconstruction and image classification tasks, and describe how artificial-based ensembles are alternatives to pre-training vast amounts of RBMs.",0
"Over the past few years, Machine Learning has been utilized in various tasks ranging from autonomous driving to natural language processing, and image classification. Recently, attention has been given to the Restricted Boltzmann Machine (RBM), which uses an energy-based structure to model data probability distributions. However, this technique is vulnerable to adversarial manipulation, where data can be slightly or significantly modified. To address this issue, Generative Adversarial Networks (GAN) can model data distributions and generate adversarial data that resemble the original data. This study proposes the use of Adversarial Learning to artificially generate RBMs, where pre-trained weight matrices are used as GAN inputs. Additionally, the study suggests sampling numerous matrices and combining them into ensembles, which reduces the need to train new models. Experimental results show that this approach is suitable for image classification and reconstruction tasks, and that artificial-based ensembles can be an alternative to pre-training large numbers of RBMs.",1
"Image captioning is a challenging computer vision task, which aims to generate a natural language description of an image. Most recent researches follow the encoder-decoder framework which depends heavily on the previous generated words for the current prediction. Such methods can not effectively take advantage of the future predicted information to learn complete semantics. In this paper, we propose Context-Aware Auxiliary Guidance (CAAG) mechanism that can guide the captioning model to perceive global contexts. Upon the captioning model, CAAG performs semantic attention that selectively concentrates on useful information of the global predictions to reproduce the current generation. To validate the adaptability of the method, we apply CAAG to three popular captioners and our proposal achieves competitive performance on the challenging Microsoft COCO image captioning benchmark, e.g. 132.2 CIDEr-D score on Karpathy split and 130.7 CIDEr-D (c40) score on official online evaluation server.",0
"Generating a natural language description of an image is a complex task in computer vision known as image captioning. The common approach is the encoder-decoder framework, which heavily relies on the previous generated words for current predictions. However, this method is limited in effectively utilizing the predicted information to learn complete semantics. To address this issue, we suggest a Context-Aware Auxiliary Guidance (CAAG) mechanism that guides the captioning model to perceive global contexts. CAAG applies semantic attention to the global predictions' useful information to reproduce the current generation. We apply CAAG to three popular captioners and achieve competitive performance on the challenging Microsoft COCO image captioning benchmark. Our proposal scores 132.2 CIDEr-D on Karpathy split and 130.7 CIDEr-D (c40) on the official online evaluation server, validating the method's adaptability.",1
"The number of videos being produced and consequently stored in databases for video streaming platforms has been increasing exponentially over time. This vast database should be easily index-able to find the requisite clip or video to match the given search specification, preferably in the form of a textual query. This work aims to provide an end-to-end pipeline to search a video database with a voice query from the end user. The pipeline makes use of Recurrent Neural Networks in combination with Convolutional Neural Networks to generate captions of the video clips present in the database.",0
"Over time, there has been an exponential increase in the production and storage of videos in databases for video streaming platforms. It is essential that this extensive database can be easily indexed to locate the relevant clip or video that matches the specific search criteria, preferably in the form of a textual query. This project's objective is to establish an end-to-end pipeline for users to search a video database using voice commands. The pipeline employs Recurrent Neural Networks and Convolutional Neural Networks to produce captions for video clips stored in the database.",1
"Deep generative models have achieved great success in areas such as image, speech, and natural language processing in the past few years. Thanks to the advances in graph-based deep learning, and in particular graph representation learning, deep graph generation methods have recently emerged with new applications ranging from discovering novel molecular structures to modeling social networks. This paper conducts a comprehensive survey on deep learning-based graph generation approaches and classifies them into five broad categories, namely, autoregressive, autoencoder-based, RL-based, adversarial, and flow-based graph generators, providing the readers a detailed description of the methods in each class. We also present publicly available source codes, commonly used datasets, and the most widely utilized evaluation metrics. Finally, we highlight the existing challenges and discuss future research directions.",0
"In recent years, deep generative models have achieved significant success in various fields including image, speech, and natural language processing. With the advancements in graph-based deep learning, particularly graph representation learning, new applications of deep graph generation methods have emerged, such as discovering novel molecular structures and modeling social networks. This paper presents a comprehensive survey of deep learning-based graph generation approaches and categorizes them into five main groups: autoregressive, autoencoder-based, RL-based, adversarial, and flow-based graph generators. Each group is described in detail, and publicly available source codes, commonly used datasets, and widely utilized evaluation metrics are presented. Additionally, existing challenges are highlighted, and future research directions are discussed.",1
"With the advent of state-of-the-art machine learning and deep learning technologies, several industries are moving towards the field. Applications of such technologies are highly diverse ranging from natural language processing to computer vision. Object recognition is one such area in the computer vision domain. Although proven to perform with high accuracy, there are still areas where such models can be improved. This is in-fact highly important in real-world use cases like autonomous driving or cancer detection, that are highly sensitive and expect such technologies to have almost no uncertainties. In this paper, we attempt to visualise the uncertainties in object recognition models and propose a correction process via user feedback. We further demonstrate our approach on the data provided by the VAST 2020 Mini-Challenge 2.",0
"Various industries are adopting cutting-edge machine learning and deep learning technologies, which offer diverse applications including natural language processing and computer vision. One specific area in computer vision is object recognition. Although this technology has demonstrated high accuracy, there is still room for improvement, particularly in real-world scenarios such as autonomous driving and cancer detection, which require minimal uncertainties. In this study, we aim to identify and address uncertainties in object recognition models through user feedback. Our approach is demonstrated using data from the VAST 2020 Mini-Challenge 2.",1
"One of the most challenging problems in the field of intrusion detection is anomaly detection for discrete event logs. While most earlier work focused on applying unsupervised learning upon engineered features, most recent work has started to resolve this challenge by applying deep learning methodology to abstraction of discrete event entries. Inspired by natural language processing, LSTM-based anomaly detection models were proposed. They try to predict upcoming events, and raise an anomaly alert when a prediction fails to meet a certain criterion. However, such a predict-next-event methodology has a fundamental limitation: event predictions may not be able to fully exploit the distinctive characteristics of sequences. This limitation leads to high false positives (FPs) and high false negatives (FNs). It is also critical to examine the structure of sequences and the bi-directional causality among individual events. To this end, we propose a new methodology: Recomposing event sequences as anomaly detection. We propose DabLog, a Deep Autoencoder-Based anomaly detection method for discrete event Logs. The fundamental difference is that, rather than predicting upcoming events, our approach determines whether a sequence is normal or abnormal by analyzing (encoding) and reconstructing (decoding) the given sequence. Our evaluation results show that our new methodology can significantly reduce the numbers of FPs and FNs, hence achieving a higher $F_1$ score.",0
"Anomaly detection for discrete event logs is a challenging problem in intrusion detection. Previous research focused on using unsupervised learning on engineered features, but recent work has employed deep learning techniques to abstract discrete event entries. Inspired by natural language processing, anomaly detection models based on LSTM predict upcoming events and raise alerts when predictions fail to meet certain criteria. However, this approach has limitations that lead to high false positives and false negatives. To address this issue, we propose a new methodology called Recomposing event sequences as anomaly detection. Our approach, DabLog, uses a Deep Autoencoder-Based method to analyze and reconstruct sequences to determine if they are normal or abnormal. This approach significantly reduces the number of false positives and false negatives, achieving a higher F1 score.",1
"Electrical energy is a vital part of modern life, and expectations for grid resilience to allow a continuous and reliable energy supply has tremendously increased even during adverse events (e.g., Ukraine cyber-attack, Hurricane Maria). The global pandemic COVID-19 has raised the electric energy reliability risk due to potential workforce disruptions, supply chain interruptions, and increased possible cybersecurity threats. The pandemic introduces a significant degree of uncertainly to the grid operation in the presence of other extreme events like natural disasters, unprecedented outages, aging power grids, high proliferation of distributed generation, and cyber-attacks. This situation increases the need for measures for the resiliency of power grids to mitigate the impacts of the pandemic as well as simultaneous extreme events. Solutions to manage such an adverse scenario will be multi-fold: a) emergency planning and organizational support, b) following safety protocol, c) utilizing enhanced automation and sensing for situational awareness, and d) integration of advanced technologies and data points for ML-driven enhanced decision support. Enhanced digitalization and automation resulted in better network visibility at various levels, including generation, transmission, and distribution. These data or information can be utilized to take advantage of advanced machine learning techniques for automation and increased power grid resilience. In this paper, a) we review the impact of COVID-19 on power grid operations and actions taken by operators/organizations to minimize the impact of COVID-19, and b) we have presented the recently developed tool and concepts using natural language processing (NLP) in the domain of machine learning and artificial intelligence that can be used for increasing resiliency of power systems in normal and in extreme scenarios such as COVID-19 pandemics.",0
"Modern life is heavily reliant on electrical energy, and the demand for a dependable grid that can withstand adverse circumstances has risen significantly. Recent events such as the Ukraine cyber-attack and Hurricane Maria have highlighted the importance of maintaining a continuous and reliable energy supply. However, the COVID-19 pandemic has increased the risk of electric energy reliability due to potential disruptions in the workforce, interruptions in the supply chain, and heightened cybersecurity threats. This uncertain situation has made it challenging to maintain power grid operations in the presence of other extreme events, such as natural disasters, outages, aging power grids, distributed generation, and cyber-attacks. To mitigate the impacts of the pandemic and other simultaneous extreme events, several measures must be taken, including emergency planning and support, following safety protocols, utilizing enhanced automation and sensing for situational awareness, and integrating advanced technologies and data points for machine learning-driven enhanced decision support. By leveraging the benefits of digitalization and automation, we can improve network visibility and take advantage of advanced machine learning techniques to increase power grid resilience. In this paper, we review the impact of COVID-19 on power grid operations, the actions taken by operators/organizations to minimize its impact, and present a tool and concepts using natural language processing in the domain of machine learning and artificial intelligence that can be used to enhance the resiliency of power systems in normal and extreme scenarios.",1
"We address the problem of retrieving a specific moment from an untrimmed video by a query sentence. This is a challenging problem because a target moment may take place in relations to other temporal moments in the untrimmed video. Existing methods cannot tackle this challenge well since they consider temporal moments individually and neglect the temporal dependencies. In this paper, we model the temporal relations between video moments by a two-dimensional map, where one dimension indicates the starting time of a moment and the other indicates the end time. This 2D temporal map can cover diverse video moments with different lengths, while representing their adjacent relations. Based on the 2D map, we propose a Temporal Adjacent Network (2D-TAN), a single-shot framework for moment localization. It is capable of encoding the adjacent temporal relation, while learning discriminative features for matching video moments with referring expressions. We evaluate the proposed 2D-TAN on three challenging benchmarks, i.e., Charades-STA, ActivityNet Captions, and TACoS, where our 2D-TAN outperforms the state-of-the-art.",0
"The retrieval of a specific moment from an untrimmed video using a query sentence is a difficult task, as the target moment may have temporal dependencies with other moments in the video. Existing methods fail to address this challenge, as they ignore the temporal dependencies and consider moments individually. In this study, we propose a solution by representing the temporal relations between moments in a two-dimensional map that covers diverse video moments of varying lengths while representing their adjacent relations. We introduce the Temporal Adjacent Network (2D-TAN), a single-shot framework for moment localization that encodes the adjacent temporal relation and learns discriminative features for matching video moments with referring expressions. We evaluate the performance of our proposed 2D-TAN on three challenging benchmarks, namely Charades-STA, ActivityNet Captions, and TACoS, and demonstrate superior results compared to the state-of-the-art techniques.",1
"Generative Adversarial Networks (GANs) have been extremely successful in various application domains such as computer vision, medicine, and natural language processing. Moreover, transforming an object or person to a desired shape become a well-studied research in the GANs. GANs are powerful models for learning complex distributions to synthesize semantically meaningful samples. However, there is a lack of comprehensive review in this field, especially lack of a collection of GANs loss-variant, evaluation metrics, remedies for diverse image generation, and stable training. Given the current fast GANs development, in this survey, we provide a comprehensive review of adversarial models for image synthesis. We summarize the synthetic image generation methods, and discuss the categories including image-to-image translation, fusion image generation, label-to-image mapping, and text-to-image translation. We organize the literature based on their base models, developed ideas related to architectures, constraints, loss functions, evaluation metrics, and training datasets. We present milestones of adversarial models, review an extensive selection of previous works in various categories, and present insights on the development route from the model-based to data-driven methods. Further, we highlight a range of potential future research directions. One of the unique features of this review is that all software implementations of these GAN methods and datasets have been collected and made available in one place at https://github.com/pshams55/GAN-Case-Study.",0
"GANs have proven to be highly effective in several fields, including computer vision, medicine, and natural language processing. Additionally, the transformation of individuals or objects into a desired shape has become a well-explored area of research within GANs. These models are adept at learning complex distributions to create meaningful samples. However, there is a dearth of comprehensive reviews in this area, particularly in terms of GANs loss-variants, assessment metrics, solutions for varied image generation, and stable training. With the rapid development of GANs, this study offers a thorough overview of adversarial models for image synthesis. It outlines methods for creating synthetic images and examines categories such as image-to-image translation, fusion image generation, label-to-image mapping, and text-to-image translation. The literature is organized based on base models, architectural concepts, constraints, loss functions, evaluation metrics, and training datasets. The study presents significant milestones in adversarial models, assesses a broad selection of previous works across various categories, and provides insights into the development path from model-based to data-driven methods. Additionally, it highlights several potential research directions. A unique aspect of this review is that all software implementations of the GAN methods and datasets are available in a single location at https://github.com/pshams55/GAN-Case-Study.",1
"We study reinforcement learning (RL) for text-based games, which are interactive simulations in the context of natural language. While different methods have been developed to represent the environment information and language actions, existing RL agents are not empowered with any reasoning capabilities to deal with textual games. In this work, we aim to conduct explicit reasoning with knowledge graphs for decision making, so that the actions of an agent are generated and supported by an interpretable inference procedure. We propose a stacked hierarchical attention mechanism to construct an explicit representation of the reasoning process by exploiting the structure of the knowledge graph. We extensively evaluate our method on a number of man-made benchmark games, and the experimental results demonstrate that our method performs better than existing text-based agents.",0
"Our research focuses on reinforcement learning (RL) applied to text-based games, which involve interactive simulations that use natural language. Although different approaches have been developed to represent the environment and language actions, current RL agents lack reasoning abilities to handle textual games. Our goal is to use explicit reasoning with knowledge graphs to enable decision making that is supported by an interpretable inference procedure. To achieve this, we propose a stacked hierarchical attention mechanism that uses the knowledge graph structure to create an explicit representation of the reasoning process. We thoroughly evaluate our approach on various man-made benchmark games, and our experimental results demonstrate that our method outperforms existing text-based agents.",1
"Vision-and-Language Navigation (VLN) requires an agent to navigate in a real-world environment following natural language instructions. From both the textual and visual perspectives, we find that the relationships among the scene, its objects,and directional clues are essential for the agent to interpret complex instructions and correctly perceive the environment. To capture and utilize the relationships, we propose a novel Language and Visual Entity Relationship Graph for modelling the inter-modal relationships between text and vision, and the intra-modal relationships among visual entities. We propose a message passing algorithm for propagating information between language elements and visual entities in the graph, which we then combine to determine the next action to take. Experiments show that by taking advantage of the relationships we are able to improve over state-of-the-art. On the Room-to-Room (R2R) benchmark, our method achieves the new best performance on the test unseen split with success rate weighted by path length (SPL) of 52%. On the Room-for-Room (R4R) dataset, our method significantly improves the previous best from 13% to 34% on the success weighted by normalized dynamic time warping (SDTW). Code is available at: https://github.com/YicongHong/Entity-Graph-VLN.",0
"The task of Vision-and-Language Navigation (VLN) involves directing an agent to navigate through a real-world environment based on natural language instructions. To successfully interpret complex instructions and accurately perceive the environment, it is crucial for the agent to understand the relationships between the scene, its objects, and directional cues from both the textual and visual perspectives. To model these inter-modal and intra-modal relationships, we propose a Language and Visual Entity Relationship Graph and a message passing algorithm to propagate information between language elements and visual entities. By leveraging these relationships, our method outperforms state-of-the-art approaches on the Room-to-Room (R2R) benchmark and the Room-for-Room (R4R) dataset. Our approach achieves a success rate weighted by path length (SPL) of 52% on the test unseen split of the R2R benchmark, and significantly improves the previous best on the success weighted by normalized dynamic time warping (SDTW) metric from 13% to 34% on the R4R dataset. The code for our approach is available at: https://github.com/YicongHong/Entity-Graph-VLN.",1
"Deep neural networks have been successfully deployed in various domains of artificial intelligence, including computer vision and natural language processing. We observe that the current standard procedure for training DNNs discards all the learned information in the past epochs except the current learned weights. An interesting question is: is this discarded information indeed useless? We argue that the discarded information can benefit the subsequent training. In this paper, we propose learning with retrospection (LWR) which makes use of the learned information in the past epochs to guide the subsequent training. LWR is a simple yet effective training framework to improve accuracies, calibration, and robustness of DNNs without introducing any additional network parameters or inference cost, only with a negligible training overhead. Extensive experiments on several benchmark datasets demonstrate the superiority of LWR for training DNNs.",0
"Various artificial intelligence domains, such as computer vision and natural language processing, have successfully utilized deep neural networks. However, the current training procedure for DNNs discards all previously learned information except the current weights. This raises the question: is this discarded information truly useless? Our argument is that it can actually benefit subsequent training. To address this, we propose learning with retrospection (LWR), a simple yet effective training framework that utilizes past learned information to guide future training. LWR improves DNN accuracies, calibration, and robustness without adding network parameters or inference cost, with only a minimal training overhead. Our extensive experiments on several benchmark datasets demonstrate LWR's superiority for training DNNs.",1
"The Visual Question Answering (VQA) task combines challenges for processing data with both Visual and Linguistic processing, to answer basic `common sense' questions about given images. Given an image and a question in natural language, the VQA system tries to find the correct answer to it using visual elements of the image and inference gathered from textual questions. In this survey, we cover and discuss the recent datasets released in the VQA domain dealing with various types of question-formats and robustness of the machine-learning models. Next, we discuss about new deep learning models that have shown promising results over the VQA datasets. At the end, we present and discuss some of the results computed by us over the vanilla VQA model, Stacked Attention Network and the VQA Challenge 2017 winner model. We also provide the detailed analysis along with the challenges and future research directions.",0
"The Visual Question Answering (VQA) task presents a complex challenge as it involves processing both visual and linguistic data to answer basic questions about images using common sense. To achieve this, the VQA system analyzes the visual elements of the image and infers information from the accompanying textual questions. This article covers recent datasets related to VQA, which encompass different question formats and machine-learning model robustness. Additionally, the article discusses new deep learning models that have demonstrated promising results on VQA datasets. Moreover, the article presents and analyzes the results obtained using the vanilla VQA model, Stacked Attention Network, and the VQA Challenge 2017 winner model. Lastly, the article outlines the challenges and future research directions in this field.",1
"There is very little notable research on generating descriptions of the Bengali language. About 243 million people speak in Bengali, and it is the 7th most spoken language on the planet. The purpose of this research is to propose a CNN and Bidirectional GRU based architecture model that generates natural language captions in the Bengali language from an image. Bengali people can use this research to break the language barrier and better understand each other's perspectives. It will also help many blind people with their everyday lives. This paper used an encoder-decoder approach to generate captions. We used a pre-trained Deep convolutional neural network (DCNN) called InceptonV3image embedding model as the encoder for analysis, classification, and annotation of the dataset's images Bidirectional Gated Recurrent unit (BGRU) layer as the decoder to generate captions. Argmax and Beam search is used to produce the highest possible quality of the captions. A new dataset called BNATURE is used, which comprises 8000 images with five captions per image. It is used for training and testing the proposed model. We obtained BLEU-1, BLEU-2, BLEU-3, BLEU-4 and Meteor is 42.6, 27.95, 23, 66, 16.41, 28.7 respectively.",0
"Research on generating descriptions in the Bengali language is limited despite its significant number of speakers, with 243 million people worldwide and ranking as the 7th most spoken language globally. To aid in breaking language barriers and promoting mutual understanding, this study proposes a CNN and Bidirectional GRU-based architecture model for generating natural language captions in Bengali from images. This approach also aims to assist visually impaired individuals in their daily lives. The encoder-decoder approach employs a pre-trained DCNN called InceptonV3 and BGRU layer to analyze and annotate the dataset's images, respectively. The model utilizes Argmax and Beam search to ensure high-quality captions. The proposed model is trained and tested on a new dataset called BNATURE, consisting of 8000 images with five captions per image. The obtained BLEU-1, BLEU-2, BLEU-3, BLEU-4, and Meteor scores are 42.6, 27.95, 23, 66, and 16.41, respectively.",1
"Mesh is a powerful data structure for 3D shapes. Representation learning for 3D meshes is important in many computer vision and graphics applications. The recent success of convolutional neural networks (CNNs) for structured data (e.g., images) suggests the value of adapting insight from CNN for 3D shapes. However, 3D shape data are irregular since each node's neighbors are unordered. Various graph neural networks for 3D shapes have been developed with isotropic filters or predefined local coordinate systems to overcome the node inconsistency on graphs. However, isotropic filters or predefined local coordinate systems limit the representation power. In this paper, we propose a local structure-aware anisotropic convolutional operation (LSA-Conv) that learns adaptive weighting matrices for each node according to the local neighboring structure and performs shared anisotropic filters. In fact, the learnable weighting matrix is similar to the attention matrix in the random synthesizer -- a new Transformer model for natural language processing (NLP). Comprehensive experiments demonstrate that our model produces significant improvement in 3D shape reconstruction compared to state-of-the-art methods.",0
"The utilization of a mesh is a potent method for organizing data related to 3D shapes. The acquisition of knowledge on how to represent 3D meshes is crucial for multiple computer vision and graphics applications. The triumph of convolutional neural networks (CNNs) in handling structured data (such as images) highlights the potential of incorporating CNN insight for 3D shapes. However, 3D shape data is irregular because the neighbors of each node are unordered. Various graph neural networks have been developed for 3D shapes with isotropic filters or predefined local coordinate systems to combat node inconsistency on graphs. Nonetheless, this approach limits the representation power. In this paper, we put forward a local structure-aware anisotropic convolutional operation (LSA-Conv) that learns adaptive weighting matrices for each node based on the local neighboring structure and executes shared anisotropic filters. The learnable weighting matrix is similar to the attention matrix in the random synthesizer, which is a new Transformer model for natural language processing (NLP). Our comprehensive experiments demonstrate that our model produces significant improvement in 3D shape reconstruction compared to state-of-the-art methods.",1
"In recent years, deep learning has made great progress in many fields such as image recognition, natural language processing, speech recognition and video super-resolution. In this survey, we comprehensively investigate 33 state-of-the-art video super-resolution (VSR) methods based on deep learning. It is well known that the leverage of information within video frames is important for video super-resolution. Thus we propose a taxonomy and classify the methods into six sub-categories according to the ways of utilizing inter-frame information. Moreover, the architectures and implementation details of all the methods are depicted in detail. Finally, we summarize and compare the performance of the representative VSR method on some benchmark datasets. We also discuss some challenges, which need to be further addressed by researchers in the community of VSR. To the best of our knowledge, this work is the first systematic review on VSR tasks, and it is expected to make a contribution to the development of recent studies in this area and potentially deepen our understanding to the VSR techniques based on deep learning.",0
"Deep learning has made significant advancements in various fields such as image recognition, speech recognition, natural language processing, and video super-resolution. This study delves into 33 state-of-the-art video super-resolution (VSR) approaches that utilize deep learning. It is crucial to take advantage of inter-frame information to achieve effective video super-resolution. To classify the methods, we propose a taxonomy that categorizes them into six sub-groups based on how they leverage inter-frame information. The methods' architectures and implementation details are thoroughly explained. We then compare the performance of the most representative VSR method on benchmark datasets and discuss the challenges that researchers in the VSR community need to address. This work is the first comprehensive review of VSR tasks and is expected to contribute to recent developments in this area while deepening our understanding of VSR techniques based on deep learning.",1
"Since the publication of the original Transformer architecture (Vaswani et al. 2017), Transformers revolutionized the field of Natural Language Processing. This, mainly due to their ability to understand timely dependencies better than competing RNN-based architectures. Surprisingly, this architecture change does not affect the field of Reinforcement Learning (RL), even though RNNs are quite popular in RL, and time dependencies are very common in RL. Recently, Parisotto et al. 2019) conducted the first promising research of Transformers in RL. To support the findings of this work, this paper seeks to provide an additional example of a Transformer-based RL method. Specifically, the goal is a simple Transformer-based Deep Q-Learning method that is stable over several environments. Due to the unstable nature of Transformers and RL, an extensive method search was conducted to arrive at a final method that leverages developments around Transformers as well as Q-learning. The proposed method can match the performance of classic Q-learning on control environments while showing potential on some selected Atari benchmarks. Furthermore, it was critically evaluated to give additional insights into the relation between Transformers and RL.",0
"Transformers have had a significant impact on Natural Language Processing since their introduction in Vaswani et al.'s 2017 paper. They excel at recognizing timely dependencies, surpassing RNN-based architectures. Despite this, the field of Reinforcement Learning has not been as influenced by their use, despite the prevalence of time dependencies in RL and the popularity of RNNs in the field. Recently, Parisotto et al. (2019) conducted promising research on the application of Transformers in RL. This paper aims to provide another example of a Transformer-based RL method, specifically a stable Deep Q-Learning method that can be applied across multiple environments. Due to the unstable nature of both Transformers and RL, an extensive method search was conducted to arrive at a final approach that combines Transformers and Q-learning developments. The proposed method matches classic Q-learning's performance on control environments while also showing potential on selected Atari benchmarks. Furthermore, the paper's evaluation provides further insight into the relationship between Transformers and RL.",1
"Deep neural networks (DNNs) have achieved outstanding performance in a wide range of applications, e.g., image classification, natural language processing, etc. Despite the good performance, the huge number of parameters in DNNs brings challenges to efficient training of DNNs and also their deployment in low-end devices with limited computing resources. In this paper, we explore the correlations in the weight matrices, and approximate the weight matrices with the low-rank block-term tensors. We name the new corresponding structure as block-term tensor layers (BT-layers), which can be easily adapted to neural network models, such as CNNs and RNNs. In particular, the inputs and the outputs in BT-layers are reshaped into low-dimensional high-order tensors with a similar or improved representation power. Sufficient experiments have demonstrated that BT-layers in CNNs and RNNs can achieve a very large compression ratio on the number of parameters while preserving or improving the representation power of the original DNNs.",0
"DNNs have excelled in various fields, including image classification and natural language processing. However, their numerous parameters pose a challenge to both effective training and deployment on devices with limited computing resources. To tackle this issue, we investigate the correlations in weight matrices and approximate them with low-rank block-term tensors. This leads to the creation of block-term tensor layers (BT-layers), which can be easily integrated into CNNs and RNNs. The inputs and outputs in BT-layers are transformed into low-dimensional high-order tensors with comparable or superior representation power. Numerous experiments have established that BT-layers in CNNs and RNNs provide a significant compression ratio in parameter numbers without compromising the representation power of the original DNNs.",1
"Biological data including gene expression data are generally high-dimensional and require efficient, generalizable, and scalable machine-learning methods to discover their complex nonlinear patterns. The recent advances in machine learning can be attributed to deep neural networks (DNNs), which excel in various tasks in terms of computer vision and natural language processing. However, standard DNNs are not appropriate for high-dimensional datasets generated in biology because they have many parameters, which in turn require many samples. In this paper, we propose a DNN-based, nonlinear feature selection method, called the feature selection network (FsNet), for high-dimensional and small number of sample data. Specifically, FsNet comprises a selection layer that selects features and a reconstruction layer that stabilizes the training. Because a large number of parameters in the selection and reconstruction layers can easily result in overfitting under a limited number of samples, we use two tiny networks to predict the large, virtual weight matrices of the selection and reconstruction layers. Experimental results on several real-world, high-dimensional biological datasets demonstrate the efficacy of the proposed method.",0
"Machine-learning methods that are efficient, generalizable, and scalable are necessary for analyzing high-dimensional biological data, such as gene expression data, and uncovering their complex nonlinear patterns. Deep neural networks (DNNs) have made significant contributions to machine learning, particularly in computer vision and natural language processing. However, standard DNNs are not suitable for high-dimensional biological datasets due to their many parameters, which require a large sample size. In this study, we introduce a nonlinear feature selection method called FsNet, which is based on DNNs and is designed for small sample sizes and high-dimensional datasets. The method comprises a selection layer for feature selection and a reconstruction layer for training stability. To avoid overfitting, we use two small networks to predict the virtual weight matrices of the selection and reconstruction layers. Our experimental results demonstrate the effectiveness of the proposed method on several real-world biological datasets with high dimensionality.",1
"Transformers have become the dominant model in natural language processing, owing to their ability to pretrain on massive amounts of data, then transfer to smaller, more specific tasks via fine-tuning. The Vision Transformer was the first major attempt to apply a pure transformer model directly to images as input, demonstrating that as compared to convolutional networks, transformer-based architectures can achieve competitive results on benchmark classification tasks. However, the computational complexity of the attention operator means that we are limited to low-resolution inputs. For more complex tasks such as detection or segmentation, maintaining a high input resolution is crucial to ensure that models can properly identify and reflect fine details in their output. This naturally raises the question of whether or not transformer-based architectures such as the Vision Transformer are capable of performing tasks other than classification. In this paper, we determine that Vision Transformers can be used as a backbone by a common detection task head to produce competitive COCO results. The model that we propose, ViT-FRCNN, demonstrates several known properties associated with transformers, including large pretraining capacity and fast fine-tuning performance. We also investigate improvements over a standard detection backbone, including superior performance on out-of-domain images, better performance on large objects, and a lessened reliance on non-maximum suppression. We view ViT-FRCNN as an important stepping stone toward a pure-transformer solution of complex vision tasks such as object detection.",0
"Due to their capacity to pretrain on vast amounts of data and fine-tune for smaller, more specific tasks, transformers have become the leading model in natural language processing. Although initially designed for images, the Vision Transformer proved to be a competitive alternative to convolutional networks in benchmark classification tasks. However, the attention operator's computational complexity limits the resolution of the inputs, which is crucial in more complex tasks like detection or segmentation. This leads to the question of whether transformers can perform tasks beyond classification. In this study, we propose ViT-FRCNN, a detection task head utilizing the Vision Transformer as a backbone, demonstrating its large pretraining capacity and fast fine-tuning performance. Our model outperforms a standard detection backbone, showing superior results on out-of-domain images, large objects, and less dependence on non-maximum suppression. We regard ViT-FRCNN as a key milestone towards a pure-transformer solution for complex vision tasks such as object detection.",1
"Modern Neural Networks are eminent in achieving state of the art performance on tasks under Computer Vision, Natural Language Processing and related verticals. However, they are notorious for their voracious memory and compute appetite which further obstructs their deployment on resource limited edge devices. In order to achieve edge deployment, researchers have developed pruning and quantization algorithms to compress such networks without compromising their efficacy. Such compression algorithms are broadly experimented on standalone CNN and RNN architectures while in this work, we present an unconventional end to end compression pipeline of a CNN-LSTM based Image Captioning model. The model is trained using VGG16 or ResNet50 as an encoder and an LSTM decoder on the flickr8k dataset. We then examine the effects of different compression architectures on the model and design a compression architecture that achieves a 73.1% reduction in model size, 71.3% reduction in inference time and a 7.7% increase in BLEU score as compared to its uncompressed counterpart.",0
"Neural Networks of today have made remarkable progress in achieving exceptional performance in areas like Computer Vision, Natural Language Processing, and other related fields. However, their extensive use of memory and computing power has made them challenging to deploy on resource-constrained edge devices. To tackle this issue, researchers have developed pruning and quantization algorithms to compress these networks while still maintaining their effectiveness. Although such compression techniques have been primarily tested on standalone CNN and RNN architectures, we present an unconventional end-to-end compression pipeline of a CNN-LSTM-based Image Captioning model. Our model is trained using VGG16 or ResNet50 as the encoder and an LSTM decoder on the flickr8k dataset. We examine the impact of various compression architectures on the model and design a compression architecture that achieves a 73.1% reduction in model size, 71.3% reduction in inference time, and a 7.7% increase in BLEU score compared to its uncompressed counterpart.",1
"Pre-trained language models have achieved state-of-the-art accuracies on various text classification tasks, e.g., sentiment analysis, natural language inference, and semantic textual similarity. However, the reliability of the fine-tuned text classifiers is an often underlooked performance criterion. For instance, one may desire a model that can detect out-of-distribution (OOD) samples (drawn far from training distribution) or be robust against domain shifts. We claim that one central obstacle to the reliability is the over-reliance of the model on a limited number of keywords, instead of looking at the whole context. In particular, we find that (a) OOD samples often contain in-distribution keywords, while (b) cross-domain samples may not always contain keywords; over-relying on the keywords can be problematic for both cases. In light of this observation, we propose a simple yet effective fine-tuning method, coined masked keyword regularization (MASKER), that facilitates context-based prediction. MASKER regularizes the model to reconstruct the keywords from the rest of the words and make low-confidence predictions without enough context. When applied to various pre-trained language models (e.g., BERT, RoBERTa, and ALBERT), we demonstrate that MASKER improves OOD detection and cross-domain generalization without degrading classification accuracy. Code is available at https://github.com/alinlab/MASKER.",0
"Various text classification tasks, such as sentiment analysis, natural language inference, and semantic textual similarity, have achieved state-of-the-art accuracies with pre-trained language models. However, the reliability of fine-tuned text classifiers is often overlooked. Models that can detect out-of-distribution (OOD) samples or be robust against domain shifts are desirable, but over-reliance on a limited number of keywords instead of the entire context is a central obstacle to reliability. OOD samples may contain in-distribution keywords, while cross-domain samples may not always contain keywords, making over-reliance on keywords problematic for both cases. To address this issue, we propose the masked keyword regularization (MASKER) fine-tuning method, which encourages context-based prediction by regularizing the model to reconstruct keywords from the remaining words and make low-confidence predictions without enough context. We show that applying MASKER to pre-trained language models such as BERT, RoBERTa, and ALBERT improves OOD detection and cross-domain generalization without reducing classification accuracy. The code is available at https://github.com/alinlab/MASKER.",1
"Image captioning transforms complex visual information into abstract natural language for representation, which can help computers understanding the world quickly. However, due to the complexity of the real environment, it needs to identify key objects and realize their connections, and further generate natural language. The whole process involves a visual understanding module and a language generation module, which brings more challenges to the design of deep neural networks than other tasks. Neural Architecture Search (NAS) has shown its important role in a variety of image recognition tasks. Besides, RNN plays an essential role in the image captioning task. We introduce a AutoCaption method to better design the decoder module of the image captioning where we use the NAS to design the decoder module called AutoRNN automatically. We use the reinforcement learning method based on shared parameters for automatic design the AutoRNN efficiently. The search space of the AutoCaption includes connections between the layers and the operations in layers both, and it can make AutoRNN express more architectures. In particular, RNN is equivalent to a subset of our search space. Experiments on the MSCOCO datasets show that our AutoCaption model can achieve better performance than traditional hand-design methods. Our AutoCaption obtains the best published CIDEr performance of 135.8% on COCO Karpathy test split. When further using ensemble technology, CIDEr is boosted up to 139.5%.",0
"The process of image captioning involves converting intricate visual information into natural language, which can aid computers in comprehending the world more efficiently. However, due to the intricacies of the physical environment, it is necessary to identify crucial objects and discern their interconnections in order to generate natural language. This process entails a visual understanding component as well as a language generation element, which presents more significant challenges to the design of deep neural networks than other tasks. Neural Architecture Search (NAS) has demonstrated its significance in a variety of image recognition tasks, while RNN is critical to the image captioning task. We introduce the AutoCaption method, which employs NAS to design the decoder module, referred to as AutoRNN, automatically, to enhance our decoder module's design. We use the reinforcement learning method based on shared parameters to design AutoRNN effectively. The AutoCaption search space includes both layer connections and layer operations, allowing AutoRNN to express more architectures. Notably, RNN is a subset of our search space. Experiments conducted on the MSCOCO datasets demonstrate that our AutoCaption model outperforms traditional hand-design methods. Our AutoCaption achieves the best published CIDEr performance of 135.8% on COCO Karpathy test split. When using ensemble technology, CIDEr is boosted up to 139.5%.",1
"Although Transformer models such as Google's BERT and OpenAI's GPT-3 are successful in many natural language processing tasks, training and deploying these models are costly and inefficient.Even if pre-trained models are used, deploying these models still remained a challenge due to their large size. Apart from deployment, these models take higher time during inference restricting user-friendliness. The main bottleneck is self-attention which uses quadratic time and space with respect to the sequence length. In order to reduce the quadratic time complexity of the self-attention mechanism, Linformer by Facebook's AI research team was introduced where they showed that the self-attention mechanism can be approximated by a low-rank matrix and exploiting this finding, a new method for self-attention with linear time and space complexity was proposed by them. In the Linformer, the time complexity depends on the projection mapping dimension which acts as a hyperparameter and affects the performance of the model, tuning this hyperparameter can be time-consuming. In this paper, I proposed an alternative method for self-attention with linear complexity in time and space and is independent of the projection mapping dimension. Since this method works for long sequences this can be used for images as well as audios.",0
"Despite the success of Transformer models like Google's BERT and OpenAI's GPT-3 in natural language processing tasks, the cost and inefficiency of training and deploying these models remains a challenge. Even when pre-trained models are utilized, deployment is hindered by their large size, and the high time required for inference limits user-friendliness. The bottleneck lies in self-attention, which uses quadratic time and space relative to the sequence length. To address this issue, Facebook's AI research team introduced Linformer, which approximates self-attention with a low-rank matrix to achieve linear time and space complexity. However, Linformer's time complexity is dependent on the projection mapping dimension, making tuning this hyperparameter time-consuming. In this paper, an alternative method for self-attention with linear complexity in both time and space is proposed, independent of the projection mapping dimension. This method is suitable for processing long sequences, including images and audio.",1
"Humans generally use natural language to communicate task requirements to each other. Ideally, natural language should also be usable for communicating goals to autonomous machines (e.g., robots) to minimize friction in task specification. However, understanding and mapping natural language goals to sequences of states and actions is challenging. Specifically, existing work along these lines has encountered difficulty in generalizing learned policies to new natural language goals and environments. In this paper, we propose a novel adversarial inverse reinforcement learning algorithm to learn a language-conditioned policy and reward function. To improve generalization of the learned policy and reward function, we use a variational goal generator to relabel trajectories and sample diverse goals during training. Our algorithm outperforms multiple baselines by a large margin on a vision-based natural language instruction following dataset (Room-2-Room), demonstrating a promising advance in enabling the use of natural language instructions in specifying agent goals.",0
"Communication of task requirements among humans is typically done using natural language. It is desirable to extend the use of natural language to communicate goals to autonomous machines, such as robots, to reduce difficulties in task specification. However, translating natural language goals into sequences of actions and states is a complex process, and current approaches have not been successful in generalizing learned policies to new environments and goals. This paper presents a novel algorithm using adversarial inverse reinforcement learning to learn a language-dependent policy and reward function. To enhance generalization, a variational goal generator is used to relabel trajectories and sample diverse goals during training. Our algorithm outperforms several baselines on a vision-based natural language instruction following dataset (Room-2-Room), demonstrating a promising advancement in enabling the use of natural language instructions to specify agent goals.",1
"Most deep neural networks are considered to be black boxes, meaning their output is hard to interpret. In contrast, logical expressions are considered to be more comprehensible since they use symbols that are semantically close to natural language instead of distributed representations. However, for high-dimensional input data such as images, the individual symbols, i.e. pixels, are not easily interpretable. We introduce the concept of first-order convolutional rules, which are logical rules that can be extracted using a convolutional neural network (CNN), and whose complexity depends on the size of the convolutional filter and not on the dimensionality of the input. Our approach is based on rule extraction from binary neural networks with stochastic local search. We show how to extract rules that are not necessarily short, but characteristic of the input, and easy to visualize. Our experiments show that the proposed approach is able to model the functionality of the neural network while at the same time producing interpretable logical rules.",0
"The majority of deep neural networks are thought to be opaque, which means that it is difficult to comprehend their output. On the other hand, logical expressions are deemed to be more comprehensible because they employ symbols that are semantically similar to natural language as opposed to distributed representations. However, when it comes to high-dimensional input data like images, the individual symbols, namely pixels, are challenging to interpret. We suggest first-order convolutional rules, which are logical rules that can be obtained by using a convolutional neural network (CNN), and their complexity is dependent on the convolutional filter size rather than the input dimensionality. Our strategy is based on rule extraction from binary neural networks through stochastic local search. We demonstrate how to obtain rules that are not necessarily brief but are representative of the input and simple to visualize. Our experiments reveal that our proposed approach can simulate the neural network's functionality while also producing logical rules that are easy to interpret.",1
"Deep neural networks have found widespread adoption in solving complex tasks ranging from image recognition to natural language processing. However, these networks make confident mispredictions when presented with data that does not belong to the training distribution, i.e. out-of-distribution (OoD) samples. In this paper we explore whether the property of Vicinal Risk Minimization (VRM) to smoothly interpolate between different class boundaries helps to train better OoD detectors. We apply VRM to existing OoD detection techniques and show their improved performance. We observe that existing OoD detectors have significant memory and compute overhead, hence we leverage VRM to develop an OoD detector with minimal overheard. Our detection method introduces an auxiliary class for classifying OoD samples. We utilize mixup in two ways to implement Vicinal Risk Minimization. First, we perform mixup within the same class and second, we perform mixup with Gaussian noise when training the auxiliary class. Our method achieves near competitive performance with significantly less compute and memory overhead when compared to existing OoD detection techniques. This facilitates the deployment of OoD detection on edge devices and expands our understanding of Vicinal Risk Minimization for use in training OoD detectors.",0
"The use of deep neural networks is widespread in solving complex tasks such as image recognition and natural language processing. However, these networks tend to make confident but inaccurate predictions when presented with data that does not belong to the training distribution, referred to as out-of-distribution (OoD) samples. In this study, we investigate whether Vicinal Risk Minimization (VRM), which allows for smooth interpolation between different class boundaries, can be used to improve OoD detectors. We apply VRM to existing OoD detection techniques and demonstrate their enhanced performance. Due to the significant memory and compute overhead of existing OoD detectors, we utilize VRM to develop an OoD detector with minimal overhead. Our detection method involves introducing an auxiliary class to classify OoD samples. We use mixup in two ways to implement Vicinal Risk Minimization: within the same class and with Gaussian noise when training the auxiliary class. Our method achieves competitive performance with significantly less compute and memory overhead, making it suitable for deployment on edge devices. This study expands our understanding of Vicinal Risk Minimization for training OoD detectors.",1
"We present *-CFQ (""star-CFQ""): a suite of large-scale datasets of varying scope based on the CFQ semantic parsing benchmark, designed for principled investigation of the scalability of machine learning systems in a realistic compositional task setting. Using this suite, we conduct a series of experiments investigating the ability of Transformers to benefit from increased training size under conditions of fixed computational cost. We show that compositional generalization remains a challenge at all training sizes, and we show that increasing the scope of natural language leads to consistently higher error rates, which are only partially offset by increased training data. We further show that while additional training data from a related domain improves the accuracy in data-starved situations, this improvement is limited and diminishes as the distance from the related domain to the target domain increases.",0
"Our study introduces *-CFQ (""star-CFQ""), a collection of large-scale datasets that vary in scope and are based on the CFQ semantic parsing benchmark. Our aim is to provide a rigorous investigation of the scalability of machine learning systems in a realistic compositional task setting. To achieve this, we use this collection to conduct a series of experiments that explore the ability of Transformers to benefit from increased training size while maintaining fixed computational costs. Our results demonstrate that compositional generalization remains a challenge at all training sizes, and that larger natural language scopes lead to consistently higher error rates, even with increased training data. Furthermore, we observed that additional training data from a related domain can improve accuracy in data-starved situations, but this improvement is limited and decreases as the distance between the related domain and the target domain increases.",1
"The development of Information Technology has been increasingly changing the means of information exchange leading to the need of digitizing print documents. In the present era, there is a lot of fraud that often occur. To avoid account fraud there was verification using ID card extraction using OCR and NLP. Optical Character Recognition (OCR) is technology that used to generate text from image. With OCR we can extract Indonesian ID card or kartu tanda penduduk (KTP) into text too. This is using to make easier service operator to do data entry. To improve the accuracy we made text correction using Natural language Processing (NLP) method to fixing the text. With 50 Indonesian ID card image we got 0.78 F-score, and we need 4510 milliseconds to extract per ID card.",0
"The advancement of Information Technology has significantly transformed the way information is shared, which has led to a growing need to digitize printed documents. Currently, fraudulent activities are rampant, necessitating the use of ID card extraction verification through Optical Character Recognition (OCR) and Natural Language Processing (NLP) to prevent account fraud. OCR is a technology that converts images into readable text, which includes Indonesian ID cards or kartu tanda penduduk (KTP). This facilitates data entry for service operators. To enhance accuracy, the NLP method can be used to correct any errors in the extracted text. After testing with 50 Indonesian ID card images, we achieved an F-score of 0.78, and it took us 4510 milliseconds to extract each ID card.",1
"Generating queries corresponding to natural language questions is a long standing problem. Traditional methods lack language flexibility, while newer sequence-to-sequence models require large amount of data. Schema-agnostic sequence-to-sequence models can be fine-tuned for a specific schema using a small dataset but these models have relatively low accuracy. We present a method that transforms the query generation problem into an intent classification and slot filling problem. This method can work using small datasets. For questions similar to the ones in the training dataset, it produces complex queries with high accuracy. For other questions, it can use a template-based approach or predict query pieces to construct the queries, still at a higher accuracy than sequence-to-sequence models. On a real-world dataset, a schema fine-tuned state-of-the-art generative model had 60\% exact match accuracy for the query generation task, while our method resulted in 92\% exact match accuracy.",0
"For a while, generating queries that match natural language questions has been a difficult task. Traditional techniques have a limited language adaptability, while newer sequence-to-sequence models require vast amounts of data. Sequence-to-sequence models that are schema-agnostic can be adjusted to a specific schema with the help of a small dataset, but these models are not very accurate. We have developed a technique that transforms the query generation issue into an intent classification and slot filling problem, which can work with small datasets. For questions akin to those in the training dataset, this technique delivers complex queries with high accuracy. For other questions, it can use a template-based approach or anticipate query components to create queries, with better accuracy than sequence-to-sequence models. In a real-world dataset, a schema fine-tuned state-of-the-art generative model had 60% exact match accuracy for the query generation task, while our technique achieved 92% exact match accuracy.",1
"Neural sequence-to-sequence models are finding increasing use in editing of documents, for example in correcting a text document or repairing source code. In this paper, we argue that common seq2seq models (with a facility to copy single tokens) are not a natural fit for such tasks, as they have to explicitly copy each unchanged token. We present an extension of seq2seq models capable of copying entire spans of the input to the output in one step, greatly reducing the number of decisions required during inference. This extension means that there are now many ways of generating the same output, which we handle by deriving a new objective for training and a variation of beam search for inference that explicitly handles this problem. In our experiments on a range of editing tasks of natural language and source code, we show that our new model consistently outperforms simpler baselines.",0
"The use of neural sequence-to-sequence models is increasing when it comes to document editing, such as correcting a text document or repairing source code. However, we argue that the typical seq2seq models that allow for the copying of single tokens are not well-suited for these tasks, as they require explicit copying of each unchanged token. To address this issue, we propose an extension of seq2seq models that can copy entire input spans to the output in a single step, reducing the number of decisions needed during inference. This extension results in multiple ways of generating the same output, which we handle by introducing a new objective for training and a variation of beam search for inference that explicitly deals with this problem. Our experiments on various editing tasks involving natural language and source code demonstrate that our new model consistently outperforms simpler baselines.",1
"Automatic generation of video descriptions in natural language, also called video captioning, aims to understand the visual content of the video and produce a natural language sentence depicting the objects and actions in the scene. This challenging integrated vision and language problem, however, has been predominantly addressed for English. The lack of data and the linguistic properties of other languages limit the success of existing approaches for such languages. In this paper we target Turkish, a morphologically rich and agglutinative language that has very different properties compared to English. To do so, we create the first large scale video captioning dataset for this language by carefully translating the English descriptions of the videos in the MSVD (Microsoft Research Video Description Corpus) dataset into Turkish. In addition to enabling research in video captioning in Turkish, the parallel English-Turkish descriptions also enables the study of the role of video context in (multimodal) machine translation. In our experiments, we build models for both video captioning and multimodal machine translation and investigate the effect of different word segmentation approaches and different neural architectures to better address the properties of Turkish. We hope that the MSVD-Turkish dataset and the results reported in this work will lead to better video captioning and multimodal machine translation models for Turkish and other morphology rich and agglutinative languages.",0
"The process of generating natural language video descriptions, known as video captioning, involves comprehending the visual elements of the video and producing a sentence that describes the objects and actions in the scene. While this has been primarily done in English, it poses a significant challenge for other languages due to a lack of data and linguistic differences. This study focuses on Turkish, a language with unique properties, by translating the English descriptions from the MSVD dataset into Turkish to create the first large-scale video captioning dataset for Turkish. This dataset not only enables research in video captioning for Turkish but also enables the study of video context in machine translation. The study experiments with different word segmentation approaches and neural architectures to improve the accuracy of models for video captioning and multimodal machine translation in Turkish and other morphology rich and agglutinative languages. The authors hope that their work will lead to better models for video captioning and machine translation in these languages.",1
"Despite being the workhorse of deep learning, the backpropagation algorithm is no panacea. It enforces sequential layer updates, thus preventing efficient parallelization of the training process. Furthermore, its biological plausibility is being challenged. Alternative schemes have been devised; yet, under the constraint of synaptic asymmetry, none have scaled to modern deep learning tasks and architectures. Here, we challenge this perspective, and study the applicability of Direct Feedback Alignment to neural view synthesis, recommender systems, geometric learning, and natural language processing. In contrast with previous studies limited to computer vision tasks, our findings show that it successfully trains a large range of state-of-the-art deep learning architectures, with performance close to fine-tuned backpropagation. At variance with common beliefs, our work supports that challenging tasks can be tackled in the absence of weight transport.",0
"Although the backpropagation algorithm is widely used in deep learning, it has limitations. Its sequential layer updates prevent efficient parallelization during training, while its biological plausibility is being questioned. While alternative methods have been created, none have been able to scale to modern deep learning tasks and architectures due to synaptic asymmetry. However, in this study, we examine the Direct Feedback Alignment method and its suitability for neural view synthesis, recommender systems, geometric learning, and natural language processing. Unlike previous research that focused on computer vision tasks, our results demonstrate that it can effectively train a variety of advanced deep learning architectures with performance comparable to fine-tuned backpropagation. Our work challenges the common belief that weight transport is necessary for tackling challenging tasks.",1
"Natural Language Search (NLS) extends the capabilities of search engines that perform keyword search allowing users to issue queries in a more ""natural"" language. The engine tries to understand the meaning of the queries and to map the query words to the symbols it supports like Persons, Organizations, Time Expressions etc.. It, then, retrieves the information that satisfies the user's need in different forms like an answer, a record or a list of records. We present an NLS system we implemented as part of the Search service of a major CRM platform. The system is currently in production serving thousands of customers. Our user studies showed that creating dynamic reports with NLS saved more than 50% of our user's time compared to achieving the same result with navigational search. We describe the architecture of the system, the particularities of the CRM domain as well as how they have influenced our design decisions. Among several submodules of the system we detail the role of a Deep Learning Named Entity Recognizer. The paper concludes with discussion over the lessons learned while developing this product.",0
"Natural Language Search (NLS) enhances the search capabilities of keyword-based search engines by allowing users to issue queries in a more natural language. The search engine endeavors to comprehend the meaning of the queries and map the query words to the symbols it supports, such as Persons, Organizations, and Time Expressions, among others. It then retrieves information that meets the user's requirements in various forms, such as an answer, a record, or a list of records. This article presents an NLS system that we developed as part of the Search service for a major CRM platform. The system is currently in production and serves thousands of customers. Our research has shown that creating dynamic reports with NLS reduces our users' time by over 50% compared to navigational search. We describe the system's architecture, the unique features of CRM, and how they influenced our design decisions. We also discuss the role of a Deep Learning Named Entity Recognizer among several submodules of the system. The paper concludes with a review of the lessons learned while developing this product.",1
"We propose Attention Grounder (AttnGrounder), a single-stage end-to-end trainable model for the task of visual grounding. Visual grounding aims to localize a specific object in an image based on a given natural language text query. Unlike previous methods that use the same text representation for every image region, we use a visual-text attention module that relates each word in the given query with every region in the corresponding image for constructing a region dependent text representation. Furthermore, for improving the localization ability of our model, we use our visual-text attention module to generate an attention mask around the referred object. The attention mask is trained as an auxiliary task using a rectangular mask generated with the provided ground-truth coordinates. We evaluate AttnGrounder on the Talk2Car dataset and show an improvement of 3.26% over the existing methods.",0
"Our proposal, known as AttnGrounder, is an end-to-end trainable model designed for the task of visual grounding. This involves identifying a specific object in an image based on a natural language text query. Unlike previous methods that use the same text representation for all image regions, our approach utilizes a visual-text attention module to establish a region-specific text representation by linking every word in the query to each region in the image. To enhance our model's localization ability, we also employ the visual-text attention module to produce an attention mask around the object in question. This mask is trained as an auxiliary task using a rectangular mask based on the provided ground-truth coordinates. We evaluate AttnGrounder on the Talk2Car dataset and demonstrate a 3.26% improvement over existing methods.",1
"Deep neural network models represent the state-of-the-art methodologies for natural language processing. Here we build on top of these methodologies to incorporate temporal information and model how to review data changes with time. Specifically, we use the dynamic representations of recurrent point process models, which encode the history of how business or service reviews are received in time, to generate instantaneous language models with improved prediction capabilities. Simultaneously, our methodologies enhance the predictive power of our point process models by incorporating summarized review content representations. We provide recurrent network and temporal convolution solutions for modeling the review content. We deploy our methodologies in the context of recommender systems, effectively characterizing the change in preference and taste of users as time evolves. Source code is available at [1].",0
"The cutting-edge techniques for natural language processing are represented by deep neural network models. We take these models a step further by integrating temporal information and demonstrating how to analyze changes in data over time. To achieve this, we employ the dynamic representations of recurrent point process models, which capture the history of how business or service reviews are received over time, resulting in language models that provide superior prediction capabilities. Additionally, our techniques improve the predictive strength of our point process models by including condensed review content representations. We present solutions for modeling review content using recurrent network and temporal convolution methods. Our approach is implemented in recommender systems, which effectively track changes in user preferences and tastes over time. To access the source code, please visit [1].",1
"Large-scale network embedding is to learn a latent representation for each node in an unsupervised manner, which captures inherent properties and structural information of the underlying graph. In this field, many popular approaches are influenced by the skip-gram model from natural language processing. Most of them use a contrastive objective to train an encoder which forces the embeddings of similar pairs to be close and embeddings of negative samples to be far. A key of success to such contrastive learning methods is how to draw positive and negative samples. While negative samples that are generated by straightforward random sampling are often satisfying, methods for drawing positive examples remains a hot topic.   In this paper, we propose SCE for unsupervised network embedding only using negative samples for training. Our method is based on a new contrastive objective inspired by the well-known sparsest cut problem. To solve the underlying optimization problem, we introduce a Laplacian smoothing trick, which uses graph convolutional operators as low-pass filters for smoothing node representations. The resulting model consists of a GCN-type structure as the encoder and a simple loss function. Notably, our model does not use positive samples but only negative samples for training, which not only makes the implementation and tuning much easier, but also reduces the training time significantly.   Finally, extensive experimental studies on real world data sets are conducted. The results clearly demonstrate the advantages of our new model in both accuracy and scalability compared to strong baselines such as GraphSAGE, G2G and DGI.",0
"The objective of large-scale network embedding is to learn latent representations for each node in an unsupervised manner that capture the inherent properties and structural information of the underlying graph. This field is heavily influenced by the skip-gram model from natural language processing, and many popular approaches use a contrastive objective to train an encoder that ensures similar embeddings are close and negative samples are far away. Generating negative samples through random sampling is usually satisfactory, whereas drawing positive examples remains a challenge. In this study, we present SCE, an unsupervised network embedding method that only uses negative samples for training. Our model relies on a new contrastive objective based on the sparsest cut problem and a Laplacian smoothing technique that leverages graph convolutional operators as low-pass filters. The resulting model comprises a GCN-type encoder and a simple loss function, and it does not require positive samples for training, making implementation and tuning simple and significantly reducing training time. We conducted extensive experiments on real-world datasets, and our results show that our model outperforms strong baselines such as GraphSAGE, G2G, and DGI in terms of accuracy and scalability.",1
"In order to build efficient deep recurrent neural architectures, it is essential to analyze the complexityof long distance dependencies (LDDs) of the dataset being modeled. In this paper, we presentdetailed analysis of the dependency decay curve exhibited by various datasets. The datasets sampledfrom a similar process (e.g. natural language, sequential MNIST, Strictlyk-Piecewise languages,etc) display variations in the properties of the dependency decay curve. Our analysis reveal thefactors resulting in these variations; such as (i) number of unique symbols in a dataset, (ii) size ofthe dataset, (iii) number of interacting symbols within a given LDD, and (iv) the distance betweenthe interacting symbols. We test these factors by generating synthesized datasets of the Strictlyk-Piecewise languages. Another advantage of these synthesized datasets is that they enable targetedtesting of deep recurrent neural architectures in terms of their ability to model LDDs with differentcharacteristics. We also demonstrate that analysing dependency decay curves can inform the selectionof optimal hyper-parameters for SOTA deep recurrent neural architectures. This analysis can directlycontribute to the development of more accurate and efficient sequential models.",0
"To construct effective deep recurrent neural structures, it is crucial to scrutinize the intricacy of long range dependencies (LDDs) in the data being modeled. This study presents a comprehensive examination of the dependency decay curve demonstrated by various datasets. Despite being derived from similar processes, such as natural language, sequential MNIST, and Strictlyk-Piecewise languages, these datasets exhibit variations in the features of their dependency decay curves. Our analysis uncovers the factors responsible for these variations, including the number of unique symbols, dataset size, the number of interacting symbols within a given LDD, and the distance between them. To test these factors, we created synthesized datasets of the Strictlyk-Piecewise languages. These synthesized datasets also enable the targeted evaluation of deep recurrent neural architectures in terms of their ability to model LDDs with distinct characteristics. Furthermore, we demonstrate that analyzing dependency decay curves can help select optimal hyper-parameters for state-of-the-art deep recurrent neural architectures. This analysis can significantly contribute to the development of more accurate and efficient sequential models.",1
"Millions of unsolicited medical inquiries are received by pharmaceutical companies every year. It has been hypothesized that these inquiries represent a treasure trove of information, potentially giving insight into matters regarding medicinal products and the associated medical treatments. However, due to the large volume and specialized nature of the inquiries, it is difficult to perform timely, recurrent, and comprehensive analyses. Here, we propose a machine learning approach based on natural language processing and unsupervised learning to automatically discover key topics in real-world medical inquiries from customers. This approach does not require ontologies nor annotations. The discovered topics are meaningful and medically relevant, as judged by medical information specialists, thus demonstrating that unsolicited medical inquiries are a source of valuable customer insights. Our work paves the way for the machine-learning-driven analysis of medical inquiries in the pharmaceutical industry, which ultimately aims at improving patient care.",0
"Pharmaceutical companies receive millions of unsolicited medical inquiries annually, which could provide valuable information on medicinal products and associated treatments. However, analyzing these inquiries is challenging due to their volume and specialized nature. To address this, we propose a machine learning approach that utilizes natural language processing and unsupervised learning to automatically identify key topics in real-world medical inquiries. Our method does not require ontologies or annotations and is capable of producing medically relevant and meaningful topics. This demonstrates that unsolicited medical inquiries are a valuable source of customer insights. Our work opens up opportunities for the pharmaceutical industry to use machine-learning-driven analysis to improve patient care.",1
"As deep learning models become tasked with more and more decisions that impact human lives, such as criminal recidivism, loan repayment, and face recognition for law enforcement, bias is becoming a growing concern. Debiasing algorithms are typically split into three paradigms: pre-processing, in-processing, and post-processing. However, in computer vision or natural language applications, it is common to start with a large generic model and then fine-tune to a specific use-case. Pre- or in-processing methods would require retraining the entire model from scratch, while post-processing methods only have black-box access to the model, so they do not leverage the weights of the trained model. Creating debiasing algorithms specifically for this fine-tuning use-case has largely been neglected.   In this work, we initiate the study of a new paradigm in debiasing research, intra-processing, which sits between in-processing and post-processing methods. Intra-processing methods are designed specifically to debias large models which have been trained on a generic dataset and fine-tuned on a more specific task. We show how to repurpose existing in-processing methods for this use-case, and we also propose three baseline algorithms: random perturbation, layerwise optimization, and adversarial fine-tuning. All of our techniques can be used for all popular group fairness measures such as equalized odds or statistical parity difference. We evaluate these methods across three popular datasets from the AIF360 toolkit, as well as on the CelebA faces dataset. Our code is available at https://github.com/abacusai/intraprocessing_debiasing.",0
"The increasing impact of deep learning models on human lives, such as in criminal recidivism, loan repayment, and law enforcement face recognition, brings about the issue of bias. Debiasing algorithms are typically categorized into three paradigms: pre-processing, in-processing, and post-processing. However, in certain applications like computer vision or natural language, a generic model is often used and fine-tuned for specific purposes. Pre- or in-processing methods require full model retraining, while post-processing methods only have limited access to the model's weights. Thus, debiasing algorithms for this fine-tuning use-case have been largely disregarded. This research introduces a new paradigm, intra-processing, which lies between in-processing and post-processing methods. Intra-processing methods are designed to debias large models trained on a generic dataset and fine-tuned for a specific task. Existing in-processing methods can be repurposed for this use-case, and three baseline algorithms: random perturbation, layerwise optimization, and adversarial fine-tuning are proposed. These methods are applicable to popular group fairness measures like equalized odds or statistical parity difference. Evaluation of the methods is conducted on three AIF360 toolkit datasets and the CelebA faces dataset. The code is available at https://github.com/abacusai/intraprocessing_debiasing.",1
"Referring expression comprehension (REC) aims to localize a target object in an image described by a referring expression phrased in natural language. Different from the object detection task that queried object labels have been pre-defined, the REC problem only can observe the queries during the test. It thus more challenging than a conventional computer vision problem. This task has attracted a lot of attention from both computer vision and natural language processing community, and several lines of work have been proposed, from CNN-RNN model, modular network to complex graph-based model. In this survey, we first examine the state of the art by comparing modern approaches to the problem. We classify methods by their mechanism to encode the visual and textual modalities. In particular, we examine the common approach of joint embedding images and expressions to a common feature space. We also discuss modular architectures and graph-based models that interface with structured graph representation. In the second part of this survey, we review the datasets available for training and evaluating REC systems. We then group results according to the datasets, backbone models, settings so that they can be fairly compared. Finally, we discuss promising future directions for the field, in particular the compositional referring expression comprehension that requires longer reasoning chain to address.",0
"The objective of Referring Expression Comprehension (REC) is to locate a specific object in an image based on a natural language description. This differs from object detection where object labels are predetermined, as in REC, queries are only observed during testing. Therefore, REC is a more challenging problem for computer vision. It has garnered significant attention from both computer vision and natural language processing communities, resulting in various approaches such as CNN-RNN models, modular networks, and graph-based models. This survey evaluates the current state-of-the-art techniques by categorizing methods according to their mechanism for encoding visual and textual modalities. The common approach of embedding images and expressions to a common feature space is discussed, as well as modular architectures and graph-based models that use structured graph representation. The second part of the survey reviews available datasets for training and evaluating REC systems, grouped according to settings, backbone models, and datasets to enable fair comparison. Finally, future directions for the field are discussed, with a focus on compositional referring expression comprehension that requires longer reasoning chains.",1
"Deep neural networks (DNNs) have been extremely successful in solving many challenging AI tasks in natural language processing, speech recognition, and computer vision nowadays. However, DNNs are typically computation intensive, memory demanding, and power hungry, which significantly limits their usage on platforms with constrained resources. Therefore, a variety of compression techniques (e.g. quantization, pruning, and knowledge distillation) have been proposed to reduce the size and power consumption of DNNs. Blockwise knowledge distillation is one of the compression techniques that can effectively reduce the size of a highly complex DNN. However, it is not widely adopted due to its long training time. In this paper, we propose a novel parallel blockwise distillation algorithm to accelerate the distillation process of sophisticated DNNs. Our algorithm leverages local information to conduct independent blockwise distillation, utilizes depthwise separable layers as the efficient replacement block architecture, and properly addresses limiting factors (e.g. dependency, synchronization, and load balancing) that affect parallelism. The experimental results running on an AMD server with four Geforce RTX 2080Ti GPUs show that our algorithm can achieve 3x speedup plus 19% energy savings on VGG distillation, and 3.5x speedup plus 29% energy savings on ResNet distillation, both with negligible accuracy loss. The speedup of ResNet distillation can be further improved to 3.87 when using four RTX6000 GPUs in a distributed cluster.",0
"Nowadays, Deep Neural Networks (DNNs) have proven to be highly effective in solving various challenging AI tasks in computer vision, speech recognition, and natural language processing. However, their high computation demands, memory requirements, and power consumption have limited their use in resource-constrained platforms. To address this limitation, many compression techniques, including quantization, pruning, and knowledge distillation, have been proposed to reduce the size and power consumption of DNNs. Blockwise knowledge distillation is one such technique that can effectively reduce the size of complex DNNs, but its long training time has hindered its widespread adoption. In this paper, we propose a new parallel blockwise distillation algorithm that leverages local information to perform independent blockwise distillation, replaces the block architecture with depthwise separable layers for improved efficiency, and addresses limiting factors that affect parallelism. Our experimental results show that our algorithm achieves a 3x speedup with 19% energy savings on VGG distillation and a 3.5x speedup with 29% energy savings on ResNet distillation, both with minimal accuracy loss when running on an AMD server with four Geforce RTX 2080Ti GPUs. Furthermore, our algorithm's ResNet distillation speedup can be further improved to 3.87 when using four RTX6000 GPUs in a distributed cluster.",1
"Visual captioning aims to generate textual descriptions given images or videos. Traditionally, image captioning models are trained on human annotated datasets such as Flickr30k and MS-COCO, which are limited in size and diversity. This limitation hinders the generalization capabilities of these models while also rendering them liable to making mistakes. Language models can, however, be trained on vast amounts of freely available unlabelled data and have recently emerged as successful language encoders and coherent text generators. Meanwhile, several unimodal and multimodal fusion techniques have been proven to work well for natural language generation and automatic speech recognition. Building on these recent developments, and with the aim of improving the quality of generated captions, the contribution of our work in this paper is two-fold: First, we propose a generic multimodal model fusion framework for caption generation as well as emendation where we utilize different fusion strategies to integrate a pretrained Auxiliary Language Model (AuxLM) within the traditional encoder-decoder visual captioning frameworks. Next, we employ the same fusion strategies to integrate a pretrained Masked Language Model (MLM), namely BERT, with a visual captioning model, viz. Show, Attend, and Tell, for emending both syntactic and semantic errors in captions. Our caption emendation experiments on three benchmark image captioning datasets, viz. Flickr8k, Flickr30k, and MSCOCO, show improvements over the baseline, indicating the usefulness of our proposed multimodal fusion strategies. Further, we perform a preliminary qualitative analysis on the emended captions and identify error categories based on the type of corrections.",0
"The primary objective of visual captioning is to generate textual descriptions for images and videos. However, conventional image captioning models are limited in size and diversity since they are trained on human annotated datasets like Flickr30k and MS-COCO. This limitation affects the generalization capabilities of the models and increases their likelihood of making errors. In contrast, language models can be trained on vast amounts of unlabelled data, making them successful language encoders and text generators. Recent advancements in unimodal and multimodal fusion techniques have also proven effective for natural language generation and speech recognition. In this paper, we propose a multimodal model fusion framework to enhance the quality of generated captions by integrating a pretrained Auxiliary Language Model (AuxLM) and a Masked Language Model (MLM) with traditional encoder-decoder visual captioning frameworks. Our experiments on three benchmark datasets show significant improvements in caption emendation, indicating the usefulness of our proposed multimodal fusion strategies. Additionally, we conduct a preliminary qualitative analysis of the emended captions to identify error categories based on the types of corrections made.",1
"We address the problem of retrieving a specific moment from an untrimmed video by natural language. It is a challenging problem because a target moment may take place in the context of other temporal moments in the untrimmed video. Existing methods cannot tackle this challenge well since they do not fully consider the temporal contexts between temporal moments. In this paper, we model the temporal context between video moments by a set of predefined two-dimensional maps under different temporal scales. For each map, one dimension indicates the starting time of a moment and the other indicates the duration. These 2D temporal maps can cover diverse video moments with different lengths, while representing their adjacent contexts at different temporal scales. Based on the 2D temporal maps, we propose a Multi-Scale Temporal Adjacent Network (MS-2D-TAN), a single-shot framework for moment localization. It is capable of encoding the adjacent temporal contexts at each scale, while learning discriminative features for matching video moments with referring expressions. We evaluate the proposed MS-2D-TAN on three challenging benchmarks, i.e., Charades-STA, ActivityNet Captions, and TACoS, where our MS-2D-TAN outperforms the state of the art.",0
"The challenge we address in this study is how to retrieve a specific moment from an untrimmed video using natural language, which is difficult because the target moment may occur in the context of other temporal moments in the video. Existing methods have not been successful in tackling this challenge because they do not fully consider the temporal contexts between moments. To overcome this limitation, we propose a novel approach that models the temporal context between video moments using a set of predefined two-dimensional maps under different temporal scales. Each map represents adjacent contexts for moments with different lengths, indicating their starting time and duration. Our proposed framework, the Multi-Scale Temporal Adjacent Network (MS-2D-TAN), utilizes these maps to encode the adjacent temporal contexts at each scale and learn discriminative features for matching video moments with natural language expressions. We evaluate MS-2D-TAN on three challenging benchmarks (Charades-STA, ActivityNet Captions, and TACoS) and demonstrate its superior performance compared to the state of the art.",1
"We introduce the task of dense captioning in 3D scans from commodity RGB-D sensors. As input, we assume a point cloud of a 3D scene; the expected output is the bounding boxes along with the descriptions for the underlying objects. To address the 3D object detection and description problems, we propose Scan2Cap, an end-to-end trained method, to detect objects in the input scene and describe them in natural language. We use an attention mechanism that generates descriptive tokens while referring to the related components in the local context. To reflect object relations (i.e. relative spatial relations) in the generated captions, we use a message passing graph module to facilitate learning object relation features. Our method can effectively localize and describe 3D objects in scenes from the ScanRefer dataset, outperforming 2D baseline methods by a significant margin (27.61% CiDEr@0.5IoUimprovement).",0
"The task of dense captioning in 3D scans from standard RGB-D sensors is presented. The input consists of a point cloud of a 3D scene and the expected output is the bounding boxes and descriptions of the objects within. To solve the problems of 3D object detection and description, an end-to-end trained method called Scan2Cap is proposed. This method employs an attention mechanism that generates descriptive tokens based on the local context of related components. In order to capture object relations, a message passing graph module is used to facilitate learning object relation features. The proposed method outperforms 2D baseline methods by a significant margin (27.61% CiDEr@0.5IoU improvement) in localizing and describing 3D objects in scenes from the ScanRefer dataset.",1
"Extreme classification tasks are multi-label tasks with an extremely large number of labels (tags). These tasks are hard because the label space is usually (i) very large, e.g. thousands or millions of labels, (ii) very sparse, i.e. very few labels apply to each input document, and (iii) highly correlated, meaning that the existence of one label changes the likelihood of predicting all other labels. In this work, we propose a self-attention based variational encoder-model to extract the label-label and label-feature dependencies jointly and to predict labels for a given input. In more detail, we propose a non-autoregressive latent variable model and compare it to a strong autoregressive baseline that predicts a label based on all previously generated labels. Our model can therefore be used to predict all labels in parallel while still including both label-label and label-feature dependencies through latent variables, and compares favourably to the autoregressive baseline. We apply our models to four standard extreme classification natural language data sets, and one news videos dataset for automated label detection from a lexicon of semantic concepts. Experimental results show that although the autoregressive models, where use a given order of the labels for chain-order label prediction, work great for the small scale labels or the prediction of the highly ranked label, but our non-autoregressive model surpasses them by around 2% to 6% when we need to predict more labels, or the dataset has a larger number of the labels.",0
"Tasks involving extreme classification refer to multi-label tasks that involve a vast number of labels. These tasks are challenging because the label space is typically (i) extensive, with thousands or even millions of labels, (ii) sparse, with very few labels applying to each input document, and (iii) highly correlated, meaning that the presence of one label affects the likelihood of predicting all other labels. This study introduces a self-attention based variational encoder-model that extracts label-label and label-feature dependencies together to predict labels for a given input. The proposed non-autoregressive latent variable model is compared to a robust autoregressive baseline that predicts a label based on all previously generated labels. Our model can predict all labels simultaneously while incorporating both label-label and label-feature dependencies through latent variables, and performs well compared to the autoregressive baseline. We evaluate our models on four standard extreme classification natural language data sets and a news video dataset for automated label detection from a lexicon of semantic concepts. Our experimental results demonstrate that while autoregressive models work well for small-scale or highly-ranked labels, our non-autoregressive model outperforms them by 2% to 6% when predicting more labels or when the dataset contains a larger number of labels.",1
"Recent advances in neural forecasting have produced major improvements in accuracy for probabilistic demand prediction. In this work, we propose novel improvements to the current state of the art by incorporating changes inspired by recent advances in Transformer architectures for Natural Language Processing. We develop a novel decoder-encoder attention for context-alignment, improving forecasting accuracy by allowing the network to study its own history based on the context for which it is producing a forecast. We also present a novel positional encoding that allows the neural network to learn context-dependent seasonality functions as well as arbitrary holiday distances. Finally we show that the current state of the art MQ-Forecaster (Wen et al., 2017) models display excess variability by failing to leverage previous errors in the forecast to improve accuracy. We propose a novel decoder-self attention scheme for forecasting that produces significant improvements in the excess variation of the forecast.",0
"Probabilistic demand prediction has seen significant improvements in accuracy due to recent advancements in neural forecasting. Our work introduces unique enhancements to the current state of the art by drawing inspiration from Transformer architectures used in Natural Language Processing. We introduce a novel decoder-encoder attention mechanism that aligns with context, which improves forecasting accuracy by enabling the network to analyze its own history based on the context of the forecast. Additionally, we present a new positional encoding approach that allows the neural network to learn context-specific seasonality functions and arbitrary holiday distances. Furthermore, our analysis shows that the current state-of-the-art MQ-Forecaster models display excess variability as they fail to utilize earlier forecast errors to improve accuracy. To address this, we propose a novel decoder-self attention scheme for forecasting that leads to a significant reduction in the excess variation of the forecast.",1
"Learning deep representations to solve complex machine learning tasks has become the prominent trend in the past few years. Indeed, Deep Neural Networks are now the golden standard in domains as various as computer vision, natural language processing or even playing combinatorial games. However, problematic limitations are hidden behind this surprising universal capability. Among other things, explainability of the decisions is a major concern, especially since deep neural networks are made up of a very large number of trainable parameters. Moreover, computational complexity can quickly become a problem, especially in contexts constrained by real time or limited resources. Therefore, understanding how information is stored and the impact this storage can have on the system remains a major and open issue. In this chapter, we introduce a method to transform deep neural network models into deep associative memories, with simpler, more explicable and less expensive operations. We show through experiments that these transformations can be done without penalty on predictive performance. The resulting deep associative memories are excellent candidates for artificial intelligence that is easier to theorize and manipulate.",0
"Over the past few years, the trend in solving complex machine learning tasks has been centered around learning deep representations. Deep Neural Networks have become the standard in various domains, including computer vision, natural language processing, and combinatorial games. However, there are limitations to this approach, such as the explainability of decisions, given the large number of trainable parameters in deep neural networks. Additionally, computational complexity can be a problem in real-time or resource-limited contexts. As a result, understanding how information is stored and its impact on the system remains an open issue. In this chapter, we present a method to transform deep neural network models into deep associative memories, which enable simpler, more explicable, and less expensive operations. Our experiments demonstrate that this transformation can be achieved without sacrificing predictive performance. These deep associative memories are excellent options for artificial intelligence that is easier to theorize and manipulate.",1
"With the advancement of deep learning, artificial intelligence (AI) has made many breakthroughs in recent years and achieved superhuman performance in various tasks such as object detection, reading comprehension, and video games. Generative Modeling, such as various Generative Adversarial Networks (GAN) models, has been applied to generate paintings and music. Research in Natural Language Processing (NLP) also had a leap forward in 2018 since the release of the pre-trained contextual neural language models such as BERT and recently released GPT3. Despite the exciting AI applications aforementioned, AI is still significantly lagging behind humans in creativity, which is often considered the ultimate moonshot for AI. Our work is inspired by Chinese calligraphy, which is a unique form of visual art where the character itself is an aesthetic painting. We also draw inspirations from paintings of the Abstract Expressionist movement in the 1940s and 1950s, such as the work by American painter Franz Kline. In this paper, we present a creative framework based on Conditional Generative Adversarial Networks and Contextual Neural Language Model to generate abstract artworks that have intrinsic meaning and aesthetic value, which is different from the existing work, such as image captioning and text-to-image generation, where the texts are the descriptions of the images. In addition, we have publicly released a Chinese calligraphy image dataset and demonstrate our framework using a prototype system and a user study.",0
"Recent years have seen significant progress in artificial intelligence (AI) thanks to the development of deep learning. AI has surpassed human performance in tasks such as object detection, reading comprehension and video games. Generative Modeling, including various Generative Adversarial Networks (GAN) models, has been used to create paintings and music. Natural Language Processing (NLP) research has also advanced with the release of pre-trained contextual neural language models like BERT and GPT3. However, AI still lags significantly behind humans in the realm of creativity, which is considered the ultimate challenge for AI. This paper introduces a creative framework that draws inspiration from Chinese calligraphy and the Abstract Expressionist movement. Our approach, based on Conditional Generative Adversarial Networks and Contextual Neural Language Models, generates abstract artworks with intrinsic meaning and aesthetic value. Unlike existing work, such as image captioning and text-to-image generation, our framework generates images without textual descriptions. We provide a publicly available Chinese calligraphy image dataset and showcase our framework using a prototype system and user study.",1
"Over the past few years, self-attention is shining in the field of deep learning, especially in the domain of natural language processing(NLP). Its impressive effectiveness, along with ubiquitous implementations, have aroused our interest in efficiently scheduling the data-flow of corresponding computations onto architectures with many computing units to realize parallel computing. In this paper, based on the theory of self-attention mechanism and state-of-the-art realization of self-attention in language models, we propose a general scheduling algorithm, which is derived from the optimum scheduling for small instances solved by a satisfiability checking(SAT) solver, to parallelize typical computations of self-attention. Strategies for further optimization on skipping redundant computations are put forward as well, with which reductions of almost 25% and 50% of the original computations are respectively achieved for two widely-adopted application schemes of self-attention. With the proposed optimization adopted, we have correspondingly come up with another two scheduling algorithms. The proposed algorithms are applicable regardless of problem sizes, as long as the number of input vectors is divisible to the number of computing units available in the architecture. Due to the complexity of proving the correctness of the algorithms mathematically for general cases, we have conducted experiments to reveal their validity, together with the superior quality of the solutions provided by which, by solving SAT problems for particular instances.",0
"Self-attention has become increasingly popular in deep learning, particularly in natural language processing. It is highly effective and widely implemented, prompting interest in scheduling data-flow for parallel computing. This paper proposes a scheduling algorithm for self-attention computations based on the self-attention mechanism theory and state-of-the-art language models. The algorithm is derived from an SAT solver's optimum scheduling for small instances and includes strategies for skipping redundant computations. The proposed algorithms result in a reduction of almost 25% and 50% of the original computations for two commonly used application schemes. Although the algorithms' mathematical correctness for general cases is complex to prove, experiments have shown their validity and superior solutions by solving SAT problems for specific instances. The algorithms are applicable to any problem size as long as the number of input vectors is divisible by the available computing units in the architecture.",1
"As one of the most well-known artificial feature sampler, the sliding window is widely used in scenarios where spatial and temporal information exists, such as computer vision, natural language process, data stream, and time series. Among which time series is common in many scenarios like credit card payment, user behavior, and sensors. General feature selection for features extracted by sliding window aggregate calls for time-consuming iteration to generate features, and then traditional feature selection methods are employed to rank them. The decision of key parameter, i.e. the period of sliding windows, depends on the domain knowledge and calls for trivial. Currently, there is no automatic method to handle the sliding window aggregate features selection. As the time consumption of feature generation with different periods and sliding windows is huge, it is very hard to enumerate them all and then select them.   In this paper, we propose a general framework using Markov Chain to solve this problem. This framework is very efficient and has high accuracy, such that it is able to perform feature selection on a variety of features and period options. We show the detail by 2 common sliding windows and 3 types of aggregation operators. And it is easy to extend more sliding windows and aggregation operators in this framework by employing existing theory about Markov Chain.",0
"The sliding window is a widely recognized artificial feature sampler that is utilized in scenarios that involve spatial and temporal information, including computer vision, natural language processing, data stream, and time series. Time series is prevalent in various scenarios, such as credit card payment, user behavior, and sensors. The process of general feature selection for features extracted through sliding window aggregate requires time-consuming iteration to generate features, followed by traditional feature selection methods to rank them. Determining the key parameter, i.e., the period of sliding windows, depends on domain knowledge and can be challenging. Unfortunately, there is no automatic method to handle sliding window aggregate feature selection, as the time consumption of feature generation with different periods and sliding windows is enormous, making it difficult to enumerate and select them all. In this paper, we propose a general framework that uses Markov Chain to address this problem efficiently and accurately, enabling feature selection on a variety of features and period options. We demonstrate this framework in detail using two common sliding windows and three types of aggregation operators, and it is easy to extend the framework by applying existing Markov Chain theory to include more sliding windows and aggregation operators.",1
"Machine learning has been developed dramatically and witnessed a lot of applications in various fields over the past few years. This boom originated in 2009, when a new model emerged, that is, the deep artificial neural network, which began to surpass other established mature models on some important benchmarks. Later, it was widely used in academia and industry. Ranging from image analysis to natural language processing, it fully exerted its magic and now become the state-of-the-art machine learning models. Deep neural networks have great potential in medical imaging technology, medical data analysis, medical diagnosis and other healthcare issues, and is promoted in both pre-clinical and even clinical stages. In this review, we performed an overview of some new developments and challenges in the application of machine learning to medical image analysis, with a special focus on deep learning in photoacoustic imaging. The aim of this review is threefold: (i) introducing deep learning with some important basics, (ii) reviewing recent works that apply deep learning in the entire ecological chain of photoacoustic imaging, from image reconstruction to disease diagnosis, (iii) providing some open source materials and other resources for researchers interested in applying deep learning to photoacoustic imaging.",0
"Over the past few years, machine learning has undergone significant advancements and found a multitude of applications in various industries. This growth began in 2009, with the emergence of the deep artificial neural network model, which surpassed other established models on crucial benchmarks and gained widespread use in academia and industry. It has proven to be particularly effective in tasks such as image analysis and natural language processing, making it a state-of-the-art machine learning model. The potential of deep neural networks in healthcare is significant, with applications in medical imaging, data analysis, and diagnosis. In this review, we provide an overview of recent developments and challenges in applying machine learning to medical image analysis, with a focus on deep learning in photoacoustic imaging. Our goals are to introduce deep learning basics, review recent works applying deep learning in photoacoustic imaging, and provide open-source resources for researchers interested in this field.",1
"Qualitative analysis of verbal data is of central importance in the learning sciences. It is labor-intensive and time-consuming, however, which limits the amount of data researchers can include in studies. This work is a step towards building a statistical machine learning (ML) method for achieving an automated support for qualitative analyses of students' writing, here specifically in score laboratory reports in introductory biology for sophistication of argumentation and reasoning. We start with a set of lab reports from an undergraduate biology course, scored by a four-level scheme that considers the complexity of argument structure, the scope of evidence, and the care and nuance of conclusions. Using this set of labeled data, we show that a popular natural language modeling processing pipeline, namely vector representation of words, a.k.a word embeddings, followed by Long Short Term Memory (LSTM) model for capturing language generation as a state-space model, is able to quantitatively capture the scoring, with a high Quadratic Weighted Kappa (QWK) prediction score, when trained in via a novel contrastive learning set-up. We show that the ML algorithm approached the inter-rater reliability of human analysis. Ultimately, we conclude, that machine learning (ML) for natural language processing (NLP) holds promise for assisting learning sciences researchers in conducting qualitative studies at much larger scales than is currently possible.",0
"Verbal data analysis is crucial in the learning sciences, but it can be time-consuming. To overcome this issue, we developed a statistical machine learning (ML) method that automatically supports qualitative analysis of students' writing, specifically in introductory biology lab reports. We used a set of lab reports from an undergraduate biology course, scored according to a four-level scheme that considers argument structure, evidence scope, and conclusion nuance. By employing word embeddings and Long Short Term Memory (LSTM) models, we achieved high Quadratic Weighted Kappa (QWK) prediction scores that approached human inter-rater reliability. Our results suggest that ML for natural language processing (NLP) has great potential to assist learning sciences researchers in conducting qualitative studies on a much larger scale.",1
"The prosperity of computer vision (CV) and natural language procession (NLP) in recent years has spurred the development of deep learning in many other domains. The advancement in machine learning provides us with an alternative option besides the computationally expensive density functional theories (DFT). Kernel method and graph neural networks have been widely studied as two mainstream methods for property prediction. The promising graph neural networks have achieved comparable accuracy to the DFT method for specific objects in the recent study. However, most of the graph neural networks with high precision so far require fully connected graphs with pairwise distance distribution as edge information. In this work, we shed light on the Directed Graph Attention Neural Network (DGANN), which only takes chemical bonds as edges and operates on bonds and atoms of molecules. DGANN distinguishes from previous models with those features: (1) It learns the local chemical environment encoding by graph attention mechanism on chemical bonds. Every initial edge message only flows into every message passing trajectory once. (2) The transformer blocks aggregate the global molecular representation from the local atomic encoding. (3) The position vectors and coordinates are used as inputs instead of distances. Our model has matched or outperformed most baseline graph neural networks on QM9 datasets even without thorough hyper-parameters searching. Moreover, this work suggests that models directly utilizing 3D coordinates can still reach high accuracies for molecule representation even without rotational and translational invariance incorporated.",0
"The recent success of computer vision (CV) and natural language processing (NLP) has spurred the development of deep learning in various domains. Machine learning has become an alternative option to computationally expensive density functional theories (DFT) for property prediction, with kernel method and graph neural networks being the two mainstream methods. While graph neural networks have achieved comparable accuracy to DFT for specific objects, most require fully connected graphs with pairwise distance distribution as edge information. However, the Directed Graph Attention Neural Network (DGANN) only takes chemical bonds as edges and operates on bonds and atoms of molecules. DGANN distinguishes from previous models by learning the local chemical environment encoding through a graph attention mechanism and using position vectors and coordinates as inputs instead of distances. Our DGANN model has matched or outperformed most baseline graph neural networks on QM9 datasets, even without thorough hyper-parameters searching. This work suggests that models directly utilizing 3D coordinates can still reach high accuracies for molecule representation without rotational and translational invariance incorporated.",1
"The attention mechanism can refine the extracted feature maps and boost the classification performance of the deep network, which has become an essential technique in computer vision and natural language processing. However, the memory and computational costs of the dot-product attention mechanism increase quadratically with the spatio-temporal size of the input. Such growth hinders the usage of attention mechanisms considerably in application scenarios with large-scale inputs. In this Letter, we propose a Linear Attention Mechanism (LAM) to address this issue, which is approximately equivalent to dot-product attention with computational efficiency. Such a design makes the incorporation between attention mechanisms and deep networks much more flexible and versatile. Based on the proposed LAM, we re-factor the skip connections in the raw U-Net and design a Multi-stage Attention ResU-Net (MAResU-Net) for semantic segmentation from fine-resolution remote sensing images. Experiments conducted on the Vaihingen dataset demonstrated the effectiveness and efficiency of our MAResU-Net. Open-source code is available at https://github.com/lironui/Multistage-Attention-ResU-Net.",0
"The utilization of attention mechanisms has become a crucial technique in computer vision and natural language processing as it can refine the feature maps and enhance the classification performance of deep networks. However, the dot-product attention mechanism's memory and computational costs surge quadratically with the spatio-temporal input size, which restricts its usage in application scenarios with large-scale inputs. To tackle this issue, we introduce the Linear Attention Mechanism (LAM), which is almost similar to the dot-product attention mechanism in computational efficiency. This design provides more flexibility and versatility in incorporating attention mechanisms with deep networks. Using the proposed LAM, we revamp the skip connections in the raw U-Net and develop a Multi-stage Attention ResU-Net (MAResU-Net) for semantic segmentation of fine-resolution remote sensing images. Our experiments on the Vaihingen dataset demonstrate the efficiency and effectiveness of MAResU-Net. The open-source code is available at https://github.com/lironui/Multistage-Attention-ResU-Net.",1
"For protein sequence datasets, unlabeled data has greatly outpaced labeled data due to the high cost of wet-lab characterization. Recent deep-learning approaches to protein prediction have shown that pre-training on unlabeled data can yield useful representations for downstream tasks. However, the optimal pre-training strategy remains an open question. Instead of strictly borrowing from natural language processing (NLP) in the form of masked or autoregressive language modeling, we introduce a new pre-training task: directly predicting protein profiles derived from multiple sequence alignments. Using a set of five, standardized downstream tasks for protein models, we demonstrate that our pre-training task along with a multi-task objective outperforms masked language modeling alone on all five tasks. Our results suggest that protein sequence models may benefit from leveraging biologically-inspired inductive biases that go beyond existing language modeling techniques in NLP.",0
"Due to the high cost of wet-lab characterization, unlabeled data has surpassed labeled data in protein sequence datasets. Recent utilization of deep-learning approaches for protein prediction has revealed that pre-training on unlabeled data can generate valuable representations for downstream tasks. However, the optimal pre-training strategy remains uncertain. Rather than solely relying on masked or autoregressive language modeling from natural language processing (NLP), we propose a new pre-training task that involves directly predicting protein profiles obtained from multiple sequence alignments. Our pre-training task, combined with a multi-task objective, outperforms masked language modeling alone on all five standardized downstream tasks for protein models. These findings suggest that incorporating biologically-inspired inductive biases beyond existing language modeling techniques in NLP can enhance protein sequence models.",1
"Video description involves the generation of the natural language description of actions, events, and objects in the video. There are various applications of video description by filling the gap between languages and vision for visually impaired people, generating automatic title suggestion based on content, browsing of the video based on the content and video-guided machine translation [86] etc.In the past decade, several works had been done in this field in terms of approaches/methods for video description, evaluation metrics,and datasets. For analyzing the progress in the video description task, a comprehensive survey is needed that covers all the phases of video description approaches with a special focus on recent deep learning approaches. In this work, we report a comprehensive survey on the phases of video description approaches, the dataset for video description, evaluation metrics, open competitions for motivating the research on the video description, open challenges in this field, and future research directions. In this survey, we cover the state-of-the-art approaches proposed for each and every dataset with their pros and cons. For the growth of this research domain,the availability of numerous benchmark dataset is a basic need. Further, we categorize all the dataset into two classes: open domain dataset and domain-specific dataset. From our survey, we observe that the work in this field is in fast-paced development since the task of video description falls in the intersection of computer vision and natural language processing. But still, the work in the video description is far from saturation stage due to various challenges like the redundancy due to similar frames which affect the quality of visual features, the availability of dataset containing more diverse content and availability of an effective evaluation metric.",0
"Generating a natural language description of actions, events, and objects in a video is known as video description. This technique has numerous applications, such as aiding visually impaired individuals, suggesting automatic titles based on content, and browsing videos based on their content. Over the past decade, various approaches and methods have been developed for video description, along with evaluation metrics and datasets. In order to analyze the progress of this task, a comprehensive survey is necessary, with a special focus on recent deep learning approaches. Our work presents such a survey, covering the phases of video description approaches, available datasets, evaluation metrics, open competitions, challenges, and future research directions. We categorize datasets into open domain and domain-specific, and report on the pros and cons of state-of-the-art approaches for each dataset. Despite the fast-paced development of this field, challenges remain, such as redundancy caused by similar frames and the need for more diverse datasets and effective evaluation metrics.",1
"While improvements in deep learning architectures have played a crucial role in improving the state of supervised and unsupervised learning in computer vision and natural language processing, neural network architecture choices for reinforcement learning remain relatively under-explored. We take inspiration from successful architectural choices in computer vision and generative modelling, and investigate the use of deeper networks and dense connections for reinforcement learning on a variety of simulated robotic learning benchmark environments. Our findings reveal that current methods benefit significantly from dense connections and deeper networks, across a suite of manipulation and locomotion tasks, for both proprioceptive and image-based observations. We hope that our results can serve as a strong baseline and further motivate future research into neural network architectures for reinforcement learning. The project website with code is at this link https://sites.google.com/view/d2rl/home.",0
"The progress made in deep learning designs has greatly contributed to the advancement of supervised and unsupervised learning in computer vision and natural language processing. However, the exploration of neural network architecture options for reinforcement learning has been relatively limited. Our study was inspired by successful architectural choices in computer vision and generative modeling, and we examined the use of deeper networks and dense connections for reinforcement learning in various simulated robotic learning environments. Our research uncovered that the current methods greatly benefit from dense connections and deeper networks, regardless of whether the observations were proprioceptive or image-based, for both manipulation and locomotion tasks. We hope that our findings will provide a strong foundation and further encourage future research into neural network architectures for reinforcement learning. To access our code, please visit the project website at https://sites.google.com/view/d2rl/home.",1
"Conversational interfaces for the detail-oriented retail fashion domain are more natural, expressive, and user friendly than classical keyword-based search interfaces. In this paper, we introduce the Fashion IQ dataset to support and advance research on interactive fashion image retrieval. Fashion IQ is the first fashion dataset to provide human-generated captions that distinguish similar pairs of garment images together with side-information consisting of real-world product descriptions and derived visual attribute labels for these images. We provide a detailed analysis of the characteristics of the Fashion IQ data, and present a transformer-based user simulator and interactive image retriever that can seamlessly integrate visual attributes with image features, user feedback, and dialog history, leading to improved performance over the state of the art in dialog-based image retrieval. We believe that our dataset will encourage further work on developing more natural and real-world applicable conversational shopping assistants.",0
"The use of conversational interfaces in the retail fashion industry is more effective than traditional keyword-based search interfaces. To further research on interactive fashion image retrieval, we have created the Fashion IQ dataset. This dataset includes human-generated captions that differentiate similar garment images, along with real-world product descriptions and visual attribute labels. Our analysis of the Fashion IQ data has resulted in the development of a transformer-based user simulator and interactive image retriever, which can improve the performance of dialog-based image retrieval. Our hope is that this dataset will inspire the development of more natural and practical conversational shopping assistants.",1
"ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.",0
"The behavior of ML models in real-world situations can often be worse than expected. This is often due to underspecification, which means that an ML pipeline can produce multiple predictors that perform equally well in the training domain. This is a common issue in modern ML pipelines, such as deep learning. While these predictors may seem equivalent based on their performance in the training domain, they can behave very differently when deployed in real-world domains, leading to instability and poor model behavior. This issue is distinct from other problems that arise from differences between training and deployment domains. Our study explores this problem in various practical ML pipelines, including computer vision, medical imaging, natural language processing, clinical risk prediction, and medical genomics. Our findings highlight the importance of explicitly addressing underspecification in modeling pipelines intended for real-world deployment.",1
"Classically, visual object tracking involves following a target object throughout a given video, and it provides us the motion trajectory of the object. However, for many practical applications, this output is often insufficient since additional semantic information is required to act on the video material. Example applications of this are surveillance and target-specific video summarization, where the target needs to be monitored with respect to certain predefined constraints, e.g., 'when standing near a yellow car'. This paper explores, tracking visual objects subjected to additional lingual constraints. Differently from Li et al., we impose additional lingual constraints upon tracking, which enables new applications of tracking. Whereas in their work the goal is to improve and extend upon tracking itself. To perform benchmarks and experiments, we contribute two datasets: c-MOT16 and c-LaSOT, curated through appending additional constraints to the frames of the original LaSOT and MOT16 datasets. We also experiment with two deep models SiamCT-DFG and SiamCT-CA, obtained through extending a recent state-of-the-art Siamese tracking method and adding modules inspired from the fields of natural language processing and visual question answering. Through experimental results, we show that the proposed model SiamCT-CA can significantly outperform its counterparts. Furthermore, our method enables the selective compression of videos, based on the validity of the constraint.",0
"Traditionally, visual object tracking involves tracing an object's movements in a video, producing a motion path. However, this output is often insufficient for practical applications like surveillance and video summarization, which require semantic understanding of the video's content. This paper explores tracking visual objects while considering additional linguistic constraints. Unlike previous work, which aimed to improve and extend tracking itself, our approach imposes additional linguistic constraints to enable new applications. We present two datasets (c-MOT16 and c-LaSOT) curated with additional constraints and experiment with two deep models (SiamCT-DFG and SiamCT-CA) that extend a state-of-the-art Siamese tracking method with modules inspired by natural language processing and visual question answering. Our experimental results show that SiamCT-CA outperforms its counterparts and enables selective video compression based on constraint validity.",1
"Federated learning (FL) has attracted increasing attention in recent years. As a privacy-preserving collaborative learning paradigm, it enables a broader range of applications, especially for computer vision and natural language processing tasks. However, to date, there is limited research of federated learning on relational data, namely Knowledge Graph (KG). In this work, we present a modified version of the graph neural network algorithm that performs federated modeling over KGs across different participants. Specifically, to tackle the inherent data heterogeneity issue and inefficiency in algorithm convergence, we propose a novel optimization algorithm, named FedAlign, with 1) optimal transportation (OT) for on-client personalization and 2) weight constraint to speed up the convergence. Extensive experiments have been conducted on several widely used datasets. Empirical results show that our proposed method outperforms the state-of-the-art FL methods, such as FedAVG and FedProx, with better convergence.",0
"Federated learning (FL) has become increasingly popular in recent years due to its ability to preserve privacy while facilitating collaborative learning. FL has a wide range of applications, particularly in computer vision and natural language processing. Nonetheless, there has been limited research on the use of FL with relational data, specifically Knowledge Graph (KG). In this study, we present a modified version of the graph neural network algorithm that performs federated modeling over KGs across different participants. To address the issues of data heterogeneity and algorithmic inefficiency, we propose a novel optimization algorithm called FedAlign. FedAlign utilizes optimal transportation (OT) for on-client personalization and weight constraint to speed up convergence. Our proposed method outperforms the state-of-the-art FL methods, including FedAVG and FedProx, with improved convergence, as demonstrated by extensive experiments conducted on several widely used datasets.",1
"Most existing text-to-image generation methods adopt a multi-stage modular architecture which has three significant problems: 1) Training multiple networks increases the run time and affects the convergence and stability of the generative model; 2) These approaches ignore the quality of early-stage generator images; 3) Many discriminators need to be trained. To this end, we propose the Dual Attention Generative Adversarial Network (DTGAN) which can synthesize high-quality and semantically consistent images only employing a single generator/discriminator pair. The proposed model introduces channel-aware and pixel-aware attention modules that can guide the generator to focus on text-relevant channels and pixels based on the global sentence vector and to fine-tune original feature maps using attention weights. Also, Conditional Adaptive Instance-Layer Normalization (CAdaILN) is presented to help our attention modules flexibly control the amount of change in shape and texture by the input natural-language description. Furthermore, a new type of visual loss is utilized to enhance the image resolution by ensuring vivid shape and perceptually uniform color distributions of generated images. Experimental results on benchmark datasets demonstrate the superiority of our proposed method compared to the state-of-the-art models with a multi-stage framework. Visualization of the attention maps shows that the channel-aware attention module is able to localize the discriminative regions, while the pixel-aware attention module has the ability to capture the globally visual contents for the generation of an image.",0
"The majority of current text-to-image generation methods use a multi-stage modular architecture that presents three main issues. Firstly, training multiple networks increases the runtime and affects the convergence and stability of the generative model. Secondly, these approaches disregard the quality of early-stage generator images. Lastly, many discriminators need to be trained. To address these problems, our proposed solution is the Dual Attention Generative Adversarial Network (DTGAN), which can synthesize high-quality and semantically consistent images using only a single generator/discriminator pair. Our model introduces channel-aware and pixel-aware attention modules that guide the generator to focus on text-relevant channels and pixels based on the global sentence vector and fine-tune original feature maps using attention weights. Additionally, we present Conditional Adaptive Instance-Layer Normalization (CAdaILN) to enable our attention modules to flexibly control the amount of change in shape and texture based on the input natural-language description. Furthermore, we use a new form of visual loss to improve the image resolution by ensuring vivid shape and perceptually uniform color distributions of generated images. Our experimental results on benchmark datasets demonstrate the superiority of our method over state-of-the-art models with a multi-stage framework. The visualization of the attention maps shows that the channel-aware attention module can identify the discriminative regions, while the pixel-aware attention module can capture the global visual contents for image generation.",1
"Reinforcement learning provides a general framework for flexible decision making and control, but requires extensive data collection for each new task that an agent needs to learn. In other machine learning fields, such as natural language processing or computer vision, pre-training on large, previously collected datasets to bootstrap learning for new tasks has emerged as a powerful paradigm to reduce data requirements when learning a new task. In this paper, we ask the following question: how can we enable similarly useful pre-training for RL agents? We propose a method for pre-training behavioral priors that can capture complex input-output relationships observed in successful trials from a wide range of previously seen tasks, and we show how this learned prior can be used for rapidly learning new tasks without impeding the RL agent's ability to try out novel behaviors. We demonstrate the effectiveness of our approach in challenging robotic manipulation domains involving image observations and sparse reward functions, where our method outperforms prior works by a substantial margin.",0
"The concept of reinforcement learning offers a versatile approach for decision making and control, although it necessitates meticulous data gathering for every new task an agent encounters. Conversely, other machine learning domains like natural language processing and computer vision utilize pre-training on extensive, pre-existing datasets to facilitate learning for fresh tasks, which has proven to be a highly effective technique for reducing data requirements. In this study, we explore the possibility of implementing similar pre-training for RL agents. Our proposal entails pre-training behavioral priors that can comprehend intricate input-output correlations observed in successful trials across a broad range of past tasks. We demonstrate how this prior knowledge can expedite learning new tasks without undermining the RL agent's capacity to experiment with innovative behaviors. Our approach was tested on challenging robotic manipulation scenarios involving image-based observations and sparse reward functions, where it outperformed prior methods by a significant margin.",1
"Reinforcement learning (RL), particularly in sparse reward settings, often requires prohibitively large numbers of interactions with the environment, thereby limiting its applicability to complex problems. To address this, several prior approaches have used natural language to guide the agent's exploration. However, these approaches typically operate on structured representations of the environment, and/or assume some structure in the natural language commands. In this work, we propose a model that directly maps pixels to rewards, given a free-form natural language description of the task, which can then be used for policy learning. Our experiments on the Meta-World robot manipulation domain show that language-based rewards significantly improves the sample efficiency of policy learning, both in sparse and dense reward settings.",0
"Sparse reward settings in reinforcement learning (RL) often require too many interactions with the environment, making it difficult to apply to complex problems. Previous attempts to address this used natural language to guide exploration but relied on structured representations or assumed structure in the language commands. Our proposed model directly maps pixels to rewards based on free-form natural language descriptions of the task, improving policy learning sample efficiency. Experiments on the Meta-World robot manipulation domain demonstrate that language-based rewards greatly enhance policy learning efficiency in both sparse and dense reward settings.",1
"Machine Learning (ML) techniques for image classification routinely require many labelled images for training the model and while testing, we ought to use images belonging to the same domain as those used for training. In this paper, we overcome the two main hurdles of ML, i.e. scarcity of data and constrained prediction of the classification model. We do this by introducing a visual classifier which uses a concept of transfer learning, namely Zero-Shot Learning (ZSL), and standard Natural Language Processing techniques. We train a classifier by mapping labelled images to their textual description instead of training it for specific classes. Transfer learning involves transferring knowledge across domains that are similar. ZSL intelligently applies the knowledge learned while training for future recognition tasks. ZSL differentiates classes as two types: seen and unseen classes. Seen classes are the classes upon which we have trained our model and unseen classes are the classes upon which we test our model. The examples from unseen classes have not been encountered in the training phase. Earlier research in this domain focused on developing a binary classifier but, in this paper, we present a multi-class classifier with a Zero-Shot Learning approach.",0
"For image classification using Machine Learning (ML) techniques, a large number of labelled images are typically required for training the model. Additionally, during testing, it is necessary to use images from the same domain as those used for training. However, in this paper, we address the two significant challenges of ML, namely the scarcity of data and the limited prediction capabilities of the classification model. To achieve this, we introduce a visual classifier that utilizes Zero-Shot Learning (ZSL), a form of transfer learning, along with standard Natural Language Processing techniques. Instead of training the classifier for specific classes, we train it by mapping labelled images to their textual description. Transfer learning allows us to transfer knowledge across similar domains, and ZSL applies the knowledge learned during training to future recognition tasks. ZSL differentiates between two types of classes: seen and unseen. Seen classes are the classes on which the model is trained, while unseen classes are the ones used for testing but not encountered during training. Previous research focused on developing a binary classifier, but our paper presents a multi-class classifier using a ZSL approach.",1
"Understanding images and text together is an important aspect of cognition and building advanced Artificial Intelligence (AI) systems. As a community, we have achieved good benchmarks over language and vision domains separately, however joint reasoning is still a challenge for state-of-the-art computer vision and natural language processing (NLP) systems. We propose a novel task to derive joint inference about a given image-text modality and compile the Visuo-Linguistic Question Answering (VLQA) challenge corpus in a question answering setting. Each dataset item consists of an image and a reading passage, where questions are designed to combine both visual and textual information i.e., ignoring either modality would make the question unanswerable. We first explore the best existing vision-language architectures to solve VLQA subsets and show that they are unable to reason well. We then develop a modular method with slightly better baseline performance, but it is still far behind human performance. We believe that VLQA will be a good benchmark for reasoning over a visuo-linguistic context. The dataset, code and leaderboard is available at https://shailaja183.github.io/vlqa/.",0
"Cognition and advanced Artificial Intelligence (AI) systems require a comprehensive understanding of images and text. Despite achieving good benchmarks over language and vision domains separately, the challenge remains for state-of-the-art computer vision and natural language processing (NLP) systems to reason jointly. To address this, we propose a new task to derive joint inference about image-text modalities and compile the Visuo-Linguistic Question Answering (VLQA) challenge corpus in a question answering setting. Each dataset item comprises an image and a reading passage, with questions designed to combine visual and textual information. Ignoring either modality would render the question unanswerable. We tested the best existing vision-language architectures to solve VLQA subsets and found that they were unable to reason well. We then developed a modular approach with slightly better baseline performance, but still lacking in comparison to human performance. We believe that VLQA will serve as a useful benchmark for reasoning over a visuo-linguistic context. The dataset, code, and leaderboard are available at https://shailaja183.github.io/vlqa/.",1
"Personality image captioning (PIC) aims to describe an image with a natural language caption given a personality trait. In this work, we introduce a novel formulation for PIC based on a communication game between a speaker and a listener. The speaker attempts to generate natural language captions while the listener encourages the generated captions to contain discriminative information about the input images and personality traits. In this way, we expect that the generated captions can be improved to naturally represent the images and express the traits. In addition, we propose to adapt the language model GPT2 to perform caption generation for PIC. This enables the speaker and listener to benefit from the language encoding capacity of GPT2. Our experiments show that the proposed model achieves the state-of-the-art performance for PIC.",0
"The objective of Personality Image Captioning (PIC) is to provide a natural language caption for an image based on a specific personality trait. To achieve this goal, we present a fresh approach to PIC that employs a communication game between a speaker and a listener. The speaker generates the caption in natural language, while the listener encourages the speaker to include distinctive information about the image and the personality trait. This approach enhances the caption's accuracy in expressing the image and trait. Additionally, we suggest using GPT2, a language model, to produce captions for PIC. This allows the speaker and listener to profit from GPT2's language encoding capabilities. Our experiments confirm that the proposed model is the most effective for PIC.",1
"Kronecker Products (KP) have been used to compress IoT RNN Applications by 15-38x compression factors, achieving better results than traditional compression methods. However when KP is applied to large Natural Language Processing tasks, it leads to significant accuracy loss (approx 26%). This paper proposes a way to recover accuracy otherwise lost when applying KP to large NLP tasks, by allowing additional degrees of freedom in the KP matrix. More formally, we propose doping, a process of adding an extremely sparse overlay matrix on top of the pre-defined KP structure. We call this compression method doped kronecker product compression. To train these models, we present a new solution to the phenomenon of co-matrix adaption (CMA), which uses a new regularization scheme called co matrix dropout regularization (CMR). We present experimental results that demonstrate compression of a large language model with LSTM layers of size 25 MB by 25x with 1.4% loss in perplexity score. At 25x compression, an equivalent pruned network leads to 7.9% loss in perplexity score, while HMD and LMF lead to 15% and 27% loss in perplexity score respectively.",0
"The use of Kronecker Products (KP) has successfully compressed IoT RNN Applications by 15-38x compression factors, outperforming traditional compression methods. However, applying KP to large Natural Language Processing tasks results in a significant accuracy loss of approximately 26%. This study proposes a method to recover the lost accuracy when using KP for large NLP tasks by introducing additional degrees of freedom in the KP matrix. The proposed approach, called doped Kronecker product compression, involves the addition of an extremely sparse overlay matrix to the pre-defined KP structure. To train these models, the study introduces co-matrix dropout regularization (CMR) to solve the issue of co-matrix adaption (CMA). Experimental results demonstrate that a large language model with LSTM layers of size 25 MB can be compressed by 25x with only a 1.4% loss in perplexity score using the proposed method. In comparison, an equivalent pruned network leads to a 7.9% loss in perplexity score at 25x compression, while HMD and LMF result in 15% and 27% loss in perplexity score, respectively.",1
"The aim of image captioning is to generate textual description of a given image. Though seemingly an easy task for humans, it is challenging for machines as it requires the ability to comprehend the image (computer vision) and consequently generate a human-like description for the image (natural language understanding). In recent times, encoder-decoder based architectures have achieved state-of-the-art results for image captioning. Here, we present a heuristic of beam search on top of the encoder-decoder based architecture that gives better quality captions on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.",0
"The objective of image captioning is to produce written explanations of a specified image. Although it may appear effortless for humans, it poses a difficulty for machines because they must be able to comprehend the image (computer vision) and subsequently produce a description that mimics human language (natural language understanding). Currently, encoder-decoder based designs have accomplished exceptional outcomes for image captioning. In this study, we introduce a heuristic of beam search onto the encoder-decoder based architecture, which enhances the quality of the captions on three benchmark datasets: Flickr8k, Flickr30k, and MS COCO.",1
"We introduce the task of 3D object localization in RGB-D scans using natural language descriptions. As input, we assume a point cloud of a scanned 3D scene along with a free-form description of a specified target object. To address this task, we propose ScanRefer, learning a fused descriptor from 3D object proposals and encoded sentence embeddings. This fused descriptor correlates language expressions with geometric features, enabling regression of the 3D bounding box of a target object. We also introduce the ScanRefer dataset, containing 51,583 descriptions of 11,046 objects from 800 ScanNet scenes. ScanRefer is the first large-scale effort to perform object localization via natural language expression directly in 3D.",0
"Our research presents a novel task of using natural language descriptions for 3D object localization in RGB-D scans. Our approach involves using a point cloud of a scanned 3D scene and a free-form description of a specific target object as input. To tackle this task, we propose ScanRefer, which learns a fused descriptor from 3D object proposals and encoded sentence embeddings. This fused descriptor enables the correlation of language expressions with geometric features and allows for the regression of the 3D bounding box of a target object. Additionally, we introduce the ScanRefer dataset, which comprises 51,583 descriptions of 11,046 objects from 800 ScanNet scenes. Our work is the first large-scale effort in performing object localization directly in 3D using natural language expressions.",1
"Dot-product attention has wide applications in computer vision and natural language processing. However, its memory and computational costs grow quadratically with the input size. Such growth prohibits its application on high-resolution inputs. To remedy this drawback, this paper proposes a novel efficient attention mechanism equivalent to dot-product attention but with substantially less memory and computational costs. Its resource efficiency allows more widespread and flexible integration of attention modules into a network, which leads to better accuracies. Empirical evaluations demonstrated the effectiveness of its advantages. Efficient attention modules brought significant performance boosts to object detectors and instance segmenters on MS-COCO 2017. Further, the resource efficiency democratizes attention to complex models, where high costs prohibit the use of dot-product attention. As an exemplar, a model with efficient attention achieved state-of-the-art accuracies for stereo depth estimation on the Scene Flow dataset. Code is available at https://github.com/cmsflash/efficient-attention.",0
"Dot-product attention is commonly used in computer vision and natural language processing, but its memory and computational costs increase exponentially as input size grows, preventing its use on high-resolution inputs. To address this issue, a new efficient attention mechanism that is comparable to dot-product attention but has significantly lower memory and computational costs has been proposed in this paper. This resource-efficient approach facilitates the integration of attention modules into a network and leads to improved accuracies. Empirical evaluations have confirmed its effectiveness, with efficient attention modules contributing to significant performance improvements in object detectors and instance segmenters on MS-COCO 2017. The resource efficiency also makes attention accessible to complex models, where the use of dot-product attention is prohibitive due to high costs. As an example, a model that used efficient attention achieved state-of-the-art accuracies for stereo depth estimation on the Scene Flow dataset. The code for this approach is available at https://github.com/cmsflash/efficient-attention.",1
"There are many applications of Generative Adversarial Networks (GANs) in fields like computer vision, natural language processing, speech synthesis, and more. Undoubtedly the most notable results have been in the area of image synthesis and in particular in the generation of deepfake videos. While deepfakes have received much negative media coverage, they can be a useful technology in applications like entertainment, customer relations, or even assistive care. One problem with generating deepfakes is the requirement for a lot of image training data of the subject which is not an issue if the subject is a celebrity for whom many images already exist. If there are only a small number of training images then the quality of the deepfake will be poor. Some media reports have indicated that a good deepfake can be produced with as few as 500 images but in practice, quality deepfakes require many thousands of images, one of the reasons why deepfakes of celebrities and politicians have become so popular. In this study, we exploit the property of a GAN to produce images of an individual with variable facial expressions which we then use to generate a deepfake. We observe that with such variability in facial expressions of synthetic GAN-generated training images and a reduced quantity of them, we can produce a near-realistic deepfake videos.",0
"Generative Adversarial Networks (GANs) have numerous applications across various fields such as computer vision, natural language processing, and speech synthesis. Among these applications, image synthesis stands out as the most remarkable achievement, with deepfake videos being the most notable result. Although deepfakes have been criticized by the media, they can be useful in areas such as customer relations, entertainment, and assistive care. However, generating high-quality deepfakes requires a large number of training images, which can be challenging to obtain if the subject is not a celebrity. The media has reported that a good deepfake can be created with as few as 500 images, but in reality, thousands of images are required, which explains why deepfakes of celebrities and politicians are more common. In this study, we leverage the ability of GANs to produce synthetic images with varying facial expressions to generate deepfakes. By using GAN-generated training images with facial expression variability and a reduced quantity of them, we can create near-realistic deepfake videos.",1
"Variational autoencoders (VAEs) hold great potential for modelling text, as they could in theory separate high-level semantic and syntactic properties from local regularities of natural language. Practically, however, VAEs with autoregressive decoders often suffer from posterior collapse, a phenomenon where the model learns to ignore the latent variables, causing the sequence VAE to degenerate into a language model. In this paper, we argue that posterior collapse is in part caused by the lack of dispersion in encoder features. We provide empirical evidence to verify this hypothesis, and propose a straightforward fix using pooling. This simple technique effectively prevents posterior collapse, allowing model to achieve significantly better data log-likelihood than standard sequence VAEs. Comparing to existing work, our proposed method is able to achieve comparable or superior performances while being more computationally efficient.",0
"The potential of Variational autoencoders (VAEs) in text modelling lies in their ability to separate high-level semantic and syntactic properties from local regularities of natural language. However, VAEs with autoregressive decoders often suffer from a phenomenon known as posterior collapse, where the model ignores the latent variables, leading to the sequence VAE becoming a language model. This paper proposes that the lack of dispersion in encoder features is a contributor to posterior collapse. By using pooling, a simple technique, we prevent posterior collapse and achieve better data log-likelihood compared to standard sequence VAEs. Our proposed method is computationally efficient and comparable or superior to existing work. We provide empirical evidence to support our hypothesis.",1
"Sequence labeling is a fundamental problem in machine learning, natural language processing and many other fields. A classic approach to sequence labeling is linear chain conditional random fields (CRFs). When combined with neural network encoders, they achieve very good performance in many sequence labeling tasks. One limitation of linear chain CRFs is their inability to model long-range dependencies between labels. High order CRFs extend linear chain CRFs by modeling dependencies no longer than their order, but the computational complexity grows exponentially in the order. In this paper, we propose the Neural Latent Dependency Model (NLDM) that models dependencies of arbitrary length between labels with a latent tree structure. We develop an end-to-end training algorithm and a polynomial-time inference algorithm of our model. We evaluate our model on both synthetic and real datasets and show that our model outperforms strong baselines.",0
"The problem of sequence labeling is crucial in various fields such as natural language processing and machine learning. One conventional technique for sequence labeling is using linear chain conditional random fields (CRFs). These CRFs, when combined with neural network encoders, exhibit exceptional performance in several sequence labeling tasks. However, linear chain CRFs have a limitation in modeling long-range dependencies between labels. High order CRFs are an extension of linear chain CRFs that can model dependencies of a length equal to or shorter than their order. Nonetheless, the computational complexity increases exponentially with the order. This paper introduces the Neural Latent Dependency Model (NLDM), which uses a latent tree structure to model dependencies of any length between labels. We present an end-to-end training algorithm and a polynomial-time inference algorithm for our model. Our model is evaluated on both real and synthetic datasets, and we demonstrate its superiority over robust baselines.",1
"The last few years have seen an increased interest in deep learning (DL) due to its success in applications such as computer vision, natural language processing (NLP), and self-driving cars. Inspired by this success, this paper applied DL to predict flight demand and delays, which have been a concern for airlines and the other stakeholders in the National Airspace System (NAS). Demand and delay prediction can be formulated as a supervised learning problem, where, given an understanding of past historical demand and delays, a deep learning network can examine sequences of historic data to predict current and future sequences. With that in mind, we applied a well-known DL method, sequence to sequence (seq2seq), to solve the problem. Our results show that the seq2seq method can reduce demand prediction mean squared error (MSE) by 50%, compared to two classical baseline algorithms.",0
"In recent years, deep learning (DL) has gained popularity for its success in various applications, including computer vision, natural language processing (NLP), and self-driving cars. Drawing inspiration from this success, the authors of this paper employed DL to predict flight demand and delays, which have been a growing concern for airlines and other stakeholders in the National Airspace System (NAS). The problem of demand and delay prediction can be viewed as a supervised learning problem, where a deep learning network can analyze sequences of historical data to forecast current and future sequences, given an understanding of past data. To address this issue, the authors used a well-known DL method called sequence to sequence (seq2seq). The results showed that the seq2seq method reduced demand prediction mean squared error (MSE) by 50% in comparison to two classical baseline algorithms.",1
"word2vec due to Mikolov \textit{et al.} (2013) is a word embedding method that is widely used in natural language processing. Despite its great success and frequent use, theoretical justification is still lacking. The main contribution of our paper is to propose a rigorous analysis of the highly nonlinear functional of word2vec. Our results suggest that word2vec may be primarily driven by an underlying spectral method. This insight may open the door to obtaining provable guarantees for word2vec. We support these findings by numerical simulations. One fascinating open question is whether the nonlinear properties of word2vec that are not captured by the spectral method are beneficial and, if so, by what mechanism.",0
"Mikolov \textit{et al.} (2013) introduced word2vec, a widely used word embedding method in natural language processing. Despite its success, there is still a lack of theoretical justification for its effectiveness. Our paper proposes a comprehensive analysis of the highly nonlinear function of word2vec, which reveals that it may be mainly influenced by an underlying spectral method. This discovery could potentially provide provable guarantees for word2vec. We conducted numerical simulations to support our findings. However, an intriguing question remains: how do the nonlinear properties of word2vec that are not captured by the spectral method contribute to its success, if at all?",1
"We study the challenging problem of releasing a robot in a previously unseen environment, and having it follow unconstrained natural language navigation instructions. Recent work on the task of Vision-and-Language Navigation (VLN) has achieved significant progress in simulation. To assess the implications of this work for robotics, we transfer a VLN agent trained in simulation to a physical robot. To bridge the gap between the high-level discrete action space learned by the VLN agent, and the robot's low-level continuous action space, we propose a subgoal model to identify nearby waypoints, and use domain randomization to mitigate visual domain differences. For accurate sim and real comparisons in parallel environments, we annotate a 325m2 office space with 1.3km of navigation instructions, and create a digitized replica in simulation. We find that sim-to-real transfer to an environment not seen in training is successful if an occupancy map and navigation graph can be collected and annotated in advance (success rate of 46.8% vs. 55.9% in sim), but much more challenging in the hardest setting with no prior mapping at all (success rate of 22.5%).",0
"Our research focuses on the complex issue of enabling a robot to navigate through an unfamiliar environment by following natural language instructions. While significant progress has been made in the simulation of Vision-and-Language Navigation (VLN), our objective is to examine the implications of this work for robotics. To achieve this, we transfer a VLN agent trained in simulation to a physical robot. To address the disparity between the high-level actions learned by the VLN agent and the robot's low-level continuous actions, we propose a subgoal model for identifying nearby waypoints and utilize domain randomization to minimize visual differences. In order to compare simulation and real-world environments, we annotate a 325m2 office space with 1.3km of navigation instructions and create a digitized replica in simulation. Our findings indicate that sim-to-real transfer is achievable when an occupancy map and navigation graph are available in advance, resulting in a success rate of 46.8% (compared to 55.9% in sim). However, the most challenging task is navigating an entirely unmapped environment, which resulted in a success rate of 22.5%.",1
"The ever-growing advances of deep learning in many areas including vision, recommendation systems, natural language processing, etc., have led to the adoption of Deep Neural Networks (DNNs) in production systems. The availability of large datasets and high computational power are the main contributors to these advances. The datasets are usually crowdsourced and may contain sensitive information. This poses serious privacy concerns as this data can be misused or leaked through various vulnerabilities. Even if the cloud provider and the communication link is trusted, there are still threats of inference attacks where an attacker could speculate properties of the data used for training, or find the underlying model architecture and parameters. In this survey, we review the privacy concerns brought by deep learning, and the mitigating techniques introduced to tackle these issues. We also show that there is a gap in the literature regarding test-time inference privacy, and propose possible future research directions.",0
"The increasing developments in deep learning across multiple domains, such as vision, natural language processing, and recommendation systems, have resulted in the implementation of Deep Neural Networks (DNNs) in actual production systems. These advancements are largely attributed to the availability of massive datasets and powerful computational resources. However, the datasets utilized are often gathered from crowdsourcing and may contain sensitive information, thereby creating significant privacy concerns. The information can be misused or leaked through various vulnerabilities, even if the cloud provider and communication links are trustworthy. Moreover, there is a possibility of inference attacks, where attackers can make inferences about the data's properties, the underlying model architecture, and parameters. To address these privacy concerns, this survey examines the mitigation techniques introduced in deep learning and highlights the gap in the literature concerning test-time inference privacy, proposing possible future research directions.",1
"Computational research on mental health disorders from written texts covers an interdisciplinary area between natural language processing and psychology. A crucial aspect of this problem is prevention and early diagnosis, as suicide resulted from depression being the second leading cause of death for young adults. In this work, we focus on methods for detecting the early onset of depression from social media texts, in particular from Reddit. To that end, we explore the eRisk 2018 dataset and achieve good results with regard to the state of the art by leveraging topic analysis and learned confidence scores to guide the decision process.",0
"The study of mental health disorders through written texts involves a collaborative field combining natural language processing and psychology. It is essential to address prevention and early detection as depression is the second leading cause of death among young adults due to suicide. The present research aims to identify techniques for detecting the initial stages of depression via social media texts, specifically on Reddit. To achieve this goal, we examine the eRisk 2018 dataset and successfully employ topic analysis and learned confidence scores to steer the decision-making process, surpassing existing standards.",1
"Continual learning systems will interact with humans, with each other, and with the physical world through time -- and continue to learn and adapt as they do. An important open problem for continual learning is a large-scale benchmark that enables realistic evaluation of algorithms. In this paper, we study a natural setting for continual learning on a massive scale. We introduce the problem of personalized online language learning (POLL), which involves fitting personalized language models to a population of users that evolves over time. To facilitate research on POLL, we collect massive datasets of Twitter posts. These datasets, Firehose10M and Firehose100M, comprise 100 million tweets, posted by one million users over six years. Enabled by the Firehose datasets, we present a rigorous evaluation of continual learning algorithms on an unprecedented scale. Based on this analysis, we develop a simple algorithm for continual gradient descent (ConGraD) that outperforms prior continual learning methods on the Firehose datasets as well as earlier benchmarks. Collectively, the POLL problem setting, the Firehose datasets, and the ConGraD algorithm enable a complete benchmark for reproducible research on web-scale continual learning.",0
"Systems for continual learning will engage with humans, other systems, and the physical world over time, constantly learning and adapting. A significant challenge in the field of continual learning is the absence of a comprehensive benchmark to realistically evaluate algorithms. This study focuses on a natural setting for continual learning on a massive scale, specifically the problem of personalized online language learning (POLL), which involves creating personalized language models for a population of users that evolves over time. To aid research on POLL, this study collects extensive datasets of Twitter posts, Firehose10M and Firehose100M, which contain 100 million tweets from one million users over six years. Using these datasets, this study conducts a thorough evaluation of continual learning algorithms, culminating in the development of a simple algorithm for continual gradient descent (ConGraD) that performs better than previous methods on both the Firehose datasets and earlier benchmarks. The POLL problem, Firehose datasets, and ConGraD algorithm collectively provide a comprehensive benchmark for reproducible research on web-scale continual learning.",1
"The neural attention mechanism plays an important role in many natural language processing applications. In particular, the use of multi-head attention extends single-head attention by allowing a model to jointly attend information from different perspectives. Without explicit constraining, however, multi-head attention may suffer from attention collapse, an issue that makes different heads extract similar attentive features, thus limiting the model's representation power. In this paper, for the first time, we provide a novel understanding of multi-head attention from a Bayesian perspective. Based on the recently developed particle-optimization sampling techniques, we propose a non-parametric approach that explicitly improves the repulsiveness in multi-head attention and consequently strengthens model's expressiveness. Remarkably, our Bayesian interpretation provides theoretical inspirations on the not-well-understood questions: why and how one uses multi-head attention. Extensive experiments on various attention models and applications demonstrate that the proposed repulsive attention can improve the learned feature diversity, leading to more informative representations with consistent performance improvement on various tasks.",0
"The neural attention mechanism is vital in natural language processing applications. Multi-head attention enhances single-head attention by enabling a model to attend to information from different perspectives. However, without constraints, multi-head attention may suffer from attention collapse, which limits the model's representation power by making different heads extract similar features. This paper presents a new understanding of multi-head attention from a Bayesian perspective. Using particle-optimization sampling techniques, we propose a non-parametric approach that explicitly improves repulsiveness in multi-head attention, thereby strengthening the model's expressiveness. Our Bayesian interpretation provides theoretical insights into the use of multi-head attention. Extensive experiments on various attention models and applications demonstrate that our proposed repulsive attention improves feature diversity, leading to more informative representations and consistent performance improvement across different tasks.",1
"Generating schema labels automatically for column values of data tables has many data science applications such as schema matching, and data discovery and linking. For example, automatically extracted tables with missing headers can be filled by the predicted schema labels which significantly minimizes human effort. Furthermore, the predicted labels can reduce the impact of inconsistent names across multiple data tables. Understanding the connection between column values and contextual information is an important yet neglected aspect as previously proposed methods treat each column independently. In this paper, we propose a context-aware semantic labeling method using both the column values and context. Our new method is based on a new setting for semantic labeling, where we sequentially predict labels for an input table with missing headers. We incorporate both the values and context of each data column using the pre-trained contextualized language model, BERT, that has achieved significant improvements in multiple natural language processing tasks. To our knowledge, we are the first to successfully apply BERT to solve the semantic labeling task. We evaluate our approach using two real-world datasets from different domains, and we demonstrate substantial improvements in terms of evaluation metrics over state-of-the-art feature-based methods.",0
"There are various data science applications for automatically generating schema labels for column values in data tables, including schema matching, data discovery, and linking. For instance, schema labels predicted through automated means can be used to fill in missing headers in extracted tables, which reduces the need for human intervention. Additionally, predicted labels can help alleviate the impact of inconsistent names across several data tables. Despite the importance of understanding the relationship between contextual information and column values, previous methods have treated each column independently. This paper introduces a new method for context-aware semantic labeling that uses both column values and context. Our approach involves sequentially predicting labels for input tables with missing headers, incorporating values and context for each data column through a pre-trained contextualized language model called BERT. This is the first successful application of BERT to solve the semantic labeling task. We evaluate our method using two real-world datasets from various domains and demonstrate significant improvements in evaluation metrics compared to state-of-the-art feature-based methods.",1
"Despite the widespread application of recurrent neural networks (RNNs) across a variety of tasks, a unified understanding of how RNNs solve these tasks remains elusive. In particular, it is unclear what dynamical patterns arise in trained RNNs, and how those patterns depend on the training dataset or task. This work addresses these questions in the context of a specific natural language processing task: text classification. Using tools from dynamical systems analysis, we study recurrent networks trained on a battery of both natural and synthetic text classification tasks. We find the dynamics of these trained RNNs to be both interpretable and low-dimensional. Specifically, across architectures and datasets, RNNs accumulate evidence for each class as they process the text, using a low-dimensional attractor manifold as the underlying mechanism. Moreover, the dimensionality and geometry of the attractor manifold are determined by the structure of the training dataset; in particular, we describe how simple word-count statistics computed on the training dataset can be used to predict these properties. Our observations span multiple architectures and datasets, reflecting a common mechanism RNNs employ to perform text classification. To the degree that integration of evidence towards a decision is a common computational primitive, this work lays the foundation for using dynamical systems techniques to study the inner workings of RNNs.",0
"Despite the widespread use of recurrent neural networks (RNNs) for various tasks, there is still no unified understanding of how RNNs solve these tasks. The way trained RNNs develop dynamical patterns and how these patterns relate to the training dataset or task is unclear. This study aims to answer these questions in the context of text classification, a natural language processing task. By using dynamical systems analysis, the researchers studied RNNs trained on both natural and synthetic text classification tasks. They found that the dynamics of trained RNNs are low-dimensional and easy to interpret. These networks accumulate evidence for each class while processing the text, using a low-dimensional attractor manifold as the underlying mechanism. Additionally, the training dataset determines the dimensionality and geometry of the attractor manifold, and simple word-count statistics computed on the dataset can predict these properties. These findings apply to different architectures and datasets and provide insight into how RNNs perform text classification. The study sets the groundwork for using dynamical systems techniques to explore the inner workings of RNNs, especially in integrating evidence towards a decision.",1
"Matching information across image and text modalities is a fundamental challenge for many applications that involve both vision and natural language processing. The objective is to find efficient similarity metrics to compare the similarity between visual and textual information. Existing approaches mainly match the local visual objects and the sentence words in a shared space with attention mechanisms. The matching performance is still limited because the similarity computation is based on simple comparisons of the matching features, ignoring the characteristics of their distribution in the data. In this paper, we address this limitation with an efficient learning objective that considers the discriminative feature distributions between the visual objects and sentence words. Specifically, we propose a novel Adversarial Discriminative Domain Regularization (ADDR) learning framework, beyond the paradigm metric learning objective, to construct a set of discriminative data domains within each image-text pairs. Our approach can generally improve the learning efficiency and the performance of existing metrics learning frameworks by regulating the distribution of the hidden space between the matching pairs. The experimental results show that this new approach significantly improves the overall performance of several popular cross-modal matching techniques (SCAN, VSRN, BFAN) on the MS-COCO and Flickr30K benchmarks.",0
"Many applications that involve vision and natural language processing face the challenge of matching information across image and text modalities. The aim is to efficiently compare the similarity between visual and textual information using appropriate similarity metrics. Existing approaches use attention mechanisms to match local visual objects and sentence words in a shared space, but their performance is limited due to the simplistic nature of similarity computation. In this study, we propose a new learning objective called Adversarial Discriminative Domain Regularization (ADDR) that considers the discriminative feature distributions between visual objects and sentence words to overcome this limitation. Our approach constructs a set of discriminative data domains within each image-text pair to improve the learning efficiency and performance of existing metrics learning frameworks. Our experimental results show that ADDR significantly enhances the overall performance of several popular cross-modal matching techniques (SCAN, VSRN, BFAN) on the MS-COCO and Flickr30K benchmarks.",1
"Contrastive Learning has recently received interest due to its success in self-supervised representation learning in the computer vision domain. However, the origins of Contrastive Learning date as far back as the 1990s and its development has spanned across many fields and domains including Metric Learning and natural language processing. In this paper we provide a comprehensive literature review and we propose a general Contrastive Representation Learning framework that simplifies and unifies many different contrastive learning methods. We also provide a taxonomy for each of the components of contrastive learning in order to summarise it and distinguish it from other forms of machine learning. We then discuss the inductive biases which are present in any contrastive learning system and we analyse our framework under different views from various sub-fields of Machine Learning. Examples of how contrastive learning has been applied in computer vision, natural language processing, audio processing, and others, as well as in Reinforcement Learning are also presented. Finally, we discuss the challenges and some of the most promising future research directions ahead.",0
"The success of Contrastive Learning in self-supervised representation learning in the field of computer vision has sparked interest. However, it has been around since the 1990s and has been used in various domains, including Metric Learning and natural language processing. This paper presents a literature review and proposes a general Contrastive Representation Learning framework that simplifies and unifies different contrastive learning methods. A taxonomy for each component of contrastive learning is provided to summarize and differentiate it from other machine learning forms. The paper also discusses the inductive biases present in contrastive learning systems and analyzes the framework from various sub-fields of Machine Learning. Furthermore, it presents examples of how contrastive learning has been applied in various fields, including computer vision, natural language processing, audio processing, and Reinforcement Learning. The paper concludes by discussing the challenges and promising future research directions.",1
"Transformer models have advanced the state of the art in many Natural Language Processing (NLP) tasks. In this paper, we present a new Transformer architecture, Extended Transformer Construction (ETC), that addresses two key challenges of standard Transformer architectures, namely scaling input length and encoding structured inputs. To scale attention to longer inputs, we introduce a novel global-local attention mechanism between global tokens and regular input tokens. We also show that combining global-local attention with relative position encodings and a Contrastive Predictive Coding (CPC) pre-training objective allows ETC to encode structured inputs. We achieve state-of-the-art results on four natural language datasets requiring long and/or structured inputs.",0
"The state of the art in various Natural Language Processing (NLP) tasks has been improved by Transformer models. This paper introduces a novel Transformer architecture called Extended Transformer Construction (ETC), which deals with two crucial challenges facing standard Transformer architectures - scaling input length and encoding structured inputs. Our ETC model addresses the scaling of attention to longer inputs by introducing a unique global-local attention mechanism between global tokens and regular input tokens. We also demonstrate that the combination of global-local attention, relative position encodings, and a Contrastive Predictive Coding (CPC) pre-training objective enables ETC to encode structured inputs. Our ETC model achieves state-of-the-art results on four natural language datasets that require long and/or structured inputs.",1
"We present VisualHints, a novel environment for multimodal reinforcement learning (RL) involving text-based interactions along with visual hints (obtained from the environment). Real-life problems often demand that agents interact with the environment using both natural language information and visual perception towards solving a goal. However, most traditional RL environments either solve pure vision-based tasks like Atari games or video-based robotic manipulation; or entirely use natural language as a mode of interaction, like Text-based games and dialog systems. In this work, we aim to bridge this gap and unify these two approaches in a single environment for multimodal RL. We introduce an extension of the TextWorld cooking environment with the addition of visual clues interspersed throughout the environment. The goal is to force an RL agent to use both text and visual features to predict natural language action commands for solving the final task of cooking a meal. We enable variations and difficulties in our environment to emulate various interactive real-world scenarios. We present a baseline multimodal agent for solving such problems using CNN-based feature extraction from visual hints and LSTMs for textual feature extraction. We believe that our proposed visual-lingual environment will facilitate novel problem settings for the RL community.",0
"VisualHints is an innovative environment for multimodal reinforcement learning that combines text-based interactions with visual cues from the environment. Many real-world challenges require agents to interact with the environment using both natural language information and visual perception to achieve a goal. However, traditional RL environments typically focus on either pure vision-based tasks or entirely text-based interactions. Our aim is to create a unified environment that bridges this gap, enabling agents to use both text and visual features to predict natural language action commands. To achieve this, we have extended the TextWorld cooking environment with visual clues that are scattered throughout the environment. Our goal is to challenge RL agents to use both text and visual features to solve the final task of cooking a meal. Our environment offers variations and difficulties that simulate real-world scenarios. We have developed a baseline multimodal agent that uses CNN-based feature extraction from visual hints and LSTMs for textual feature extraction. We are confident that our proposed visual-lingual environment will provide new opportunities for the RL community to tackle novel problem settings.",1
"Language modeling is a keystone task in natural language processing. When training a language model on sensitive information, differential privacy (DP) allows us to quantify the degree to which our private data is protected. However, training algorithms which enforce differential privacy often lead to degradation in model quality. We study the feasibility of learning a language model which is simultaneously high-quality and privacy preserving by tuning a public base model on a private corpus. We find that DP fine-tuning boosts the performance of language models in the private domain, making the training of such models possible.",0
"In the field of natural language processing, language modeling is a crucial task. In cases where we train a language model using sensitive data, differential privacy (DP) enables us to measure the level of protection provided to the private information. However, enforcing differential privacy through training algorithms can compromise the quality of the model. Our research explores the possibility of developing a language model that is both high-quality and privacy-preserving by adjusting a public base model with a private corpus. We discovered that DP fine-tuning enhances the performance of language models in the private domain, making it feasible to train such models.",1
"This article reviews meta-learning also known as learning-to-learn which seeks rapid and accurate model adaptation to unseen tasks with applications in highly automated AI, few-shot learning, natural language processing and robotics. Unlike deep learning, meta-learning can be applied to few-shot high-dimensional datasets and considers further improving model generalization to unseen tasks. Deep learning is focused upon in-sample prediction and meta-learning concerns model adaptation for out-of-sample prediction. Meta-learning can continually perform self-improvement to achieve highly autonomous AI. Meta-learning may serve as an additional generalization block complementary for original deep learning model. Meta-learning seeks adaptation of machine learning models to unseen tasks which are vastly different from trained tasks. Meta-learning with coevolution between agent and environment provides solutions for complex tasks unsolvable by training from scratch. Meta-learning methodology covers a wide range of great minds and thoughts. We briefly introduce meta-learning methodologies in the following categories: black-box meta-learning, metric-based meta-learning, layered meta-learning and Bayesian meta-learning framework. Recent applications concentrate upon the integration of meta-learning with other machine learning framework to provide feasible integrated problem solutions. We briefly present recent meta-learning advances and discuss potential future research directions.",0
"This article provides an overview of meta-learning, also known as learning-to-learn. Meta-learning aims to quickly and accurately adapt models to new tasks, with applications in areas such as highly automated AI, few-shot learning, natural language processing, and robotics. Unlike deep learning, meta-learning can handle few-shot high-dimensional datasets and focuses on improving model generalization for out-of-sample prediction. By continually improving itself, meta-learning can achieve highly autonomous AI and complement original deep learning models. Meta-learning seeks to adapt machine learning models to vastly different, unseen tasks, and coevolution between agent and environment can solve complex tasks that cannot be trained from scratch. The article introduces four meta-learning methodologies: black-box, metric-based, layered, and Bayesian frameworks. Recent applications integrate meta-learning with other machine learning frameworks for feasible integrated problem solutions. The article concludes by discussing potential future research directions in meta-learning.",1
"A class of recent semi-supervised learning (SSL) methods heavily rely on domain-specific data augmentations. In contrast, generative SSL methods involve unsupervised learning based on generative models by either joint-training or pre-training, and are more appealing from the perspective of being domain-agnostic, since they do not inherently require data augmentations. Joint-training estimates the joint distribution of observations and labels, while pre-training is taken over observations only. Recently, energy-based models (EBMs) have achieved promising results for generative modeling. Joint-training via EBMs for SSL has been explored with encouraging results across different data modalities. In this paper, we make two contributions. First, we explore pre-training via EBMs for SSL and compare it to joint-training. Second, a suite of experiments are conducted over domains of image classification and natural language labeling to give a realistic whole picture of the performances of EBM based SSL methods. It is found that joint-training EBMs outperform pre-training EBMs marginally but nearly consistently.",0
"There are two types of semi-supervised learning (SSL) methods: those that use domain-specific data augmentations and those that rely on generative models. The latter, known as generative SSL methods, do not require data augmentations and are therefore considered more domain-agnostic. Generative SSL methods can be achieved through either joint-training or pre-training, with joint-training estimating the joint distribution of observations and labels and pre-training focusing on observations only. Energy-based models (EBMs) have been successful in generative modeling and joint-training with EBMs for SSL has produced positive results across different data modalities. In this paper, we explore pre-training with EBMs for SSL and compare it to joint-training. We also conduct experiments in image classification and natural language labeling to provide a comprehensive understanding of EBM-based SSL methods. Our findings show that joint-training EBMs marginally outperform pre-training EBMs, but this result is consistent across multiple domains.",1
"Sign language is a gesture based symbolic communication medium among speech and hearing impaired people. It also serves as a communication bridge between non-impaired population and impaired population. Unfortunately, in most situations a non-impaired person is not well conversant in such symbolic languages which restricts natural information flow between these two categories of population. Therefore, an automated translation mechanism can be greatly useful that can seamlessly translate sign language into natural language. In this paper, we attempt to perform recognition on 30 basic Indian sign gestures. Gestures are represented as temporal sequences of 3D depth maps each consisting of 3D coordinates of 20 body joints. A recurrent neural network (RNN) is employed as classifier. To improve performance of the classifier, we use geometric transformation for alignment correction of depth frames. In our experiments the model achieves 84.81% accuracy.",0
"Gesture-based symbolic communication known as sign language is commonly used by people who have speech and hearing impairments. It also serves as a communication bridge between those who have impairments and those who do not. Unfortunately, most people who are not impaired are not fluent in this type of communication, which can hinder information flow between the two groups. To address this issue, an automated translation mechanism that can seamlessly translate sign language into natural language would be highly beneficial. This paper aims to recognize 30 basic Indian sign gestures by representing them as temporal sequences of 3D depth maps, each of which contains 3D coordinates of 20 body joints. To classify these gestures, a recurrent neural network (RNN) is utilized. Geometric transformation is used to correct alignment issues in depth frames and improve the performance of the classifier. The model achieves an accuracy of 84.81% in our experiments.",1
"The transformer has been extensively used in research domains such as computer vision, image processing, and natural language processing. The transformer, however, has not been actively used in graph neural networks. To this end, we introduce a transformer-based advanced GNN model, named UGformer, to learn graph representations. In particular, given an input graph, we present two UGformer variants. The first variant is to leverage the transformer on a set of sampled neighbors for each node, while the second is to leverage the transformer directly on the input graph. Experimental results demonstrate that our UGformer achieves state-of-the-art accuracies on well-known benchmark datasets for graph classification and inductive text classification. The code is available on Github: \url{https://github.com/daiquocnguyen/Graph-Transformer}.",0
"In various fields of study, such as computer vision, image processing, and natural language processing, the transformer has been widely utilized. However, the transformer has not been frequently utilized in graph neural networks. Therefore, we have introduced a transformer-based advanced GNN model, the UGformer, which aims to learn graph representations. We present two versions of the UGformer for an input graph. The first version utilizes the transformer on a set of selected neighbors for each node, while the second version utilizes the transformer directly on the input graph. Our experimental results show that the UGformer achieves outstanding accuracy on benchmark datasets for graph classification and inductive text classification. The code is available on Github at \url{https://github.com/daiquocnguyen/Graph-Transformer}.",1
"We propose a novel lightweight generative adversarial network for efficient image manipulation using natural language descriptions. To achieve this, a new word-level discriminator is proposed, which provides the generator with fine-grained training feedback at word-level, to facilitate training a lightweight generator that has a small number of parameters, but can still correctly focus on specific visual attributes of an image, and then edit them without affecting other contents that are not described in the text. Furthermore, thanks to the explicit training signal related to each word, the discriminator can also be simplified to have a lightweight structure. Compared with the state of the art, our method has a much smaller number of parameters, but still achieves a competitive manipulation performance. Extensive experimental results demonstrate that our method can better disentangle different visual attributes, then correctly map them to corresponding semantic words, and thus achieve a more accurate image modification using natural language descriptions.",0
"A new method for efficiently manipulating images using natural language descriptions is proposed. Our approach utilizes a lightweight generative adversarial network that incorporates a novel word-level discriminator. The discriminator provides the generator with detailed training feedback at the word-level, allowing for the creation of a lightweight generator with a small number of parameters. This generator can still accurately focus on specific visual attributes of an image and edit them without affecting other contents not described in the text. Additionally, the lightweight structure of the discriminator is simplified due to the explicit training signal related to each word. Despite having fewer parameters than current methods, our approach achieves competitive manipulation performance. Extensive experimentation demonstrates that our method successfully disentangles different visual attributes and accurately maps them to corresponding semantic words for precise image modification using natural language descriptions.",1
"Latent Dirichlet Allocation (LDA) is a topic model widely used in natural language processing and machine learning. Most approaches to training the model rely on iterative algorithms, which makes it difficult to run LDA on big corpora that are best analyzed in parallel and distributed computational environments. Indeed, current approaches to parallel inference either don't converge to the correct posterior or require storage of large dense matrices in memory. We present a novel sampler that overcomes both problems, and we show that this sampler is faster, both empirically and theoretically, than previous Gibbs samplers for LDA. We do so by employing a novel P\'olya-urn-based approximation in the sparse partially collapsed sampler for LDA. We prove that the approximation error vanishes with data size, making our algorithm asymptotically exact, a property of importance for large-scale topic models. In addition, we show, via an explicit example, that - contrary to popular belief in the topic modeling literature - partially collapsed samplers can be more efficient than fully collapsed samplers. We conclude by comparing the performance of our algorithm with that of other approaches on well-known corpora.",0
"LDA, a topic model utilized in natural language processing and machine learning, faces obstacles when applied to voluminous corpora that require analysis in parallel and distributed computational environments due to its reliance on iterative algorithms. Existing parallel inference techniques either fail to converge to the correct posterior or necessitate storage of large dense matrices in memory. To address these issues, we have introduced a new sampler that overcomes both problems and is faster than previous Gibbs samplers for LDA, both empirically and theoretically. Our algorithm employs a novel P\'olya-urn-based approximation in the sparse partially collapsed sampler for LDA, which we prove to be asymptotically exact as the data size increases. Moreover, we demonstrate that partially collapsed samplers can be more efficient than fully collapsed samplers, contrary to popular belief in the topic modeling literature. Finally, we compare the performance of our approach with other methods on well-known corpora.",1
"Most recent successes on forecasting the people motion are based on LSTM models and all most recent progress has been achieved by modelling the social interaction among people and the people interaction with the scene. We question the use of the LSTM models and propose the novel use of Transformer Networks for trajectory forecasting. This is a fundamental switch from the sequential step-by-step processing of LSTMs to the only-attention-based memory mechanisms of Transformers. In particular, we consider both the original Transformer Network (TF) and the larger Bidirectional Transformer (BERT), state-of-the-art on all natural language processing tasks. Our proposed Transformers predict the trajectories of the individual people in the scene. These are ""simple"" model because each person is modelled separately without any complex human-human nor scene interaction terms. In particular, the TF model without bells and whistles yields the best score on the largest and most challenging trajectory forecasting benchmark of TrajNet. Additionally, its extension which predicts multiple plausible future trajectories performs on par with more engineered techniques on the 5 datasets of ETH + UCY. Finally, we show that Transformers may deal with missing observations, as it may be the case with real sensor data. Code is available at https://github.com/FGiuliari/Trajectory-Transformer.",0
"Recent advancements in predicting human movement have been centered around LSTM models, which model social interactions among people and their interactions with their surroundings. However, we challenge the use of LSTM models and propose the use of Transformer Networks for trajectory forecasting. This is a significant departure from the sequential processing of LSTMs to the memory mechanisms of Transformers. We examine both the original Transformer Network and the larger Bidirectional Transformer, which is the state-of-the-art in natural language processing. Our Transformer models predict individual trajectories without complex human-human or scene interaction terms. The TF model performs the best on the largest trajectory forecasting benchmark of TrajNet, and its extension that predicts multiple plausible future trajectories matches the performance of more engineered methods on the 5 ETH + UCY datasets. Furthermore, we demonstrate that Transformers can handle missing observations in real sensor data. Our code is available at https://github.com/FGiuliari/Trajectory-Transformer.",1
"We are interested in understanding how well Transformer language models (TLMs) can perform reasoning tasks when trained on knowledge encoded in the form of natural language. We investigate their systematic generalization abilities on a logical reasoning task in natural language, which involves reasoning over relationships between entities grounded in first-order logical proofs. Specifically, we perform soft theorem-proving by leveraging TLMs to generate natural language proofs. We test the generated proofs for logical consistency, along with the accuracy of the final inference. We observe length-generalization issues when evaluated on longer-than-trained sequences. However, we observe TLMs improve their generalization performance after being exposed to longer, exhaustive proofs. In addition, we discover that TLMs are able to generalize better using backward-chaining proofs compared to their forward-chaining counterparts, while they find it easier to generate forward chaining proofs. We observe that models that are not trained to generate proofs are better at generalizing to problems based on longer proofs. This suggests that Transformers have efficient internal reasoning strategies that are harder to interpret. These results highlight the systematic generalization behavior of TLMs in the context of logical reasoning, and we believe this work motivates deeper inspection of their underlying reasoning strategies.",0
"Our focus is to comprehend how well Transformer language models (TLMs) can perform reasoning tasks by utilizing knowledge encoded in natural language. We explore their ability to systematically generalize on a natural language logical reasoning task that involves reasoning over relationships between entities grounded in first-order logical proofs. Our approach involves soft theorem-proving, where TLMs generate natural language proofs, which are then evaluated for logical consistency and the accuracy of the final inference. We have identified length-generalization issues when evaluated on longer-than-trained sequences; however, we have also observed that TLMs improve their generalization performance after being exposed to longer, exhaustive proofs. Furthermore, we have observed that TLMs can generalize better using backward-chaining proofs than their forward-chaining counterparts, although they find it easier to generate forward chaining proofs. Interestingly, we have found that models not trained to generate proofs are better at generalizing to problems based on longer proofs, indicating that Transformers possess efficient internal reasoning strategies that are challenging to interpret. These findings underscore the systematic generalization behavior of TLMs in the context of logical reasoning and suggest the need for a deeper understanding of their underlying reasoning strategies.",1
"Every year physicians face an increasing demand of image-based diagnosis from patients, a problem that can be addressed with recent artificial intelligence methods. In this context, we survey works in the area of automatic report generation from medical images, with emphasis on methods using deep neural networks, with respect to: (1) Datasets, (2) Architecture Design, (3) Explainability and (4) Evaluation Metrics. Our survey identifies interesting developments, but also remaining challenges. Among them, the current evaluation of generated reports is especially weak, since it mostly relies on traditional Natural Language Processing (NLP) metrics, which do not accurately capture medical correctness.",0
"Physicians are increasingly facing a higher demand for image-based diagnosis from patients, which can be solved using recent artificial intelligence techniques. Our research focuses on automatic report generation from medical images, specifically exploring the use of deep neural networks in terms of datasets, architecture design, explainability, and evaluation metrics. While we have identified some promising advancements, there are still challenges to be addressed, particularly in the evaluation of generated reports, as current metrics based on traditional Natural Language Processing (NLP) do not completely capture medical accuracy.",1
"First-order stochastic optimization methods are currently the most widely used class of methods for training deep neural networks. However, the choice of the optimizer has become an ad-hoc rule that can significantly affect the performance. For instance, SGD with momentum (SGD+M) is typically used in computer vision (CV) and Adam is used for training transformer models for Natural Language Processing (NLP). Using the wrong method can lead to significant performance degradation. Inspired by the dual averaging algorithm, we propose Modernized Dual Averaging (MDA), an optimizer that is able to perform as well as SGD+M in CV and as Adam in NLP. Our method is not adaptive and is significantly simpler than Adam. We show that MDA induces a decaying uncentered $L_2$-regularization compared to vanilla SGD+M and hypothesize that this may explain why it works on NLP problems where SGD+M fails.",0
"Currently, first-order stochastic optimization methods are extensively utilized for training deep neural networks. Nevertheless, the selection of optimizer has become arbitrary and can greatly influence the performance. For instance, computer vision (CV) typically employs SGD with momentum (SGD+M), while Adam is used for training transformer models in Natural Language Processing (NLP). The use of an inappropriate method can result in significant performance deterioration. Drawing inspiration from the dual averaging algorithm, we introduce Modernized Dual Averaging (MDA), an optimizer that can match the performance of SGD+M in CV and Adam in NLP. Our method is uncomplicated and non-adaptive, unlike Adam. We demonstrate that MDA induces a decaying uncentered $L_2$-regularization in contrast to vanilla SGD+M and postulate that this may clarify why it is effective in NLP problems where SGD+M fails.",1
"Many optimizers have been proposed for training deep neural networks, and they often have multiple hyperparameters, which make it tricky to benchmark their performance. In this work, we propose a new benchmarking protocol to evaluate both end-to-end efficiency (training a model from scratch without knowing the best hyperparameter) and data-addition training efficiency (the previously selected hyperparameters are used for periodically re-training the model with newly collected data). For end-to-end efficiency, unlike previous work that assumes random hyperparameter tuning, which over-emphasizes the tuning time, we propose to evaluate with a bandit hyperparameter tuning strategy. A human study is conducted to show that our evaluation protocol matches human tuning behavior better than the random search. For data-addition training, we propose a new protocol for assessing the hyperparameter sensitivity to data shift. We then apply the proposed benchmarking framework to 7 optimizers and various tasks, including computer vision, natural language processing, reinforcement learning, and graph mining. Our results show that there is no clear winner across all the tasks.",0
"Several optimizers have been suggested for deep neural network training, but they often have numerous hyperparameters, making it difficult to compare their efficiency. In this study, we suggest a new benchmarking method that assesses both end-to-end and data-addition training efficiency. For end-to-end efficiency, we propose a bandit hyperparameter tuning strategy instead of random tuning, which overemphasizes the tuning time. Our evaluation protocol is demonstrated to correspond better with human tuning behavior than random search. For data-addition training, we propose a new protocol for evaluating hyperparameter sensitivity to data shift. We apply this benchmarking framework to seven optimizers and various tasks, including computer vision, natural language processing, reinforcement learning, and graph mining, and find no clear winner across all tasks.",1
"Pooling operations have shown to be effective on computer vision and natural language processing tasks. One challenge of performing pooling operations on graph data is the lack of locality that is not well-defined on graphs. Previous studies used global ranking methods to sample some of the important nodes, but most of them are not able to incorporate graph topology. In this work, we propose the topology-aware pooling (TAP) layer that explicitly considers graph topology. Our TAP layer is a two-stage voting process that selects more important nodes in a graph. It first performs local voting to generate scores for each node by attending each node to its neighboring nodes. The scores are generated locally such that topology information is explicitly considered. In addition, graph topology is incorporated in global voting to compute the importance score of each node globally in the entire graph. Altogether, the final ranking score for each node is computed by combining its local and global voting scores. To encourage better graph connectivity in the sampled graph, we propose to add a graph connectivity term to the computation of ranking scores. Results on graph classification tasks demonstrate that our methods achieve consistently better performance than previous methods.",0
"Pooling operations have been proven effective for computer vision and natural language processing tasks, but they pose a challenge for graph data due to the undefined lack of locality. To address this issue, previous studies utilized global ranking methods to sample important nodes, but failed to include graph topology. In this study, we introduce the topology-aware pooling (TAP) layer, which explicitly considers graph topology. Our TAP layer consists of a two-stage voting process that selects the most important nodes in a graph. Firstly, local voting generates scores for each node by considering its neighboring nodes, taking into account topology information. Secondly, global voting incorporates graph topology to compute the importance score of each node globally in the entire graph. The final ranking score for each node is computed by combining its local and global voting scores. To enhance graph connectivity, we propose adding a graph connectivity term to the ranking score computation. Our results on graph classification tasks demonstrate that our proposed method consistently outperforms previous methods.",1
"In the computational prediction of chemical compound properties, molecular descriptors and fingerprints encoded to low dimensional vectors are used. The selection of proper molecular descriptors and fingerprints is both important and challenging as the performance of such models is highly dependent on descriptors. To overcome this challenge, natural language processing models that utilize simplified molecular input line-entry system as input were studied, and several transformer-variant models achieved superior results when compared with conventional methods. In this study, we explored the structural differences of the transformer-variant model and proposed a new self-attention based model. The representation learning performance of the self-attention module was evaluated in a multi-task learning environment using imbalanced chemical datasets. The experiment results showed that our model achieved competitive outcomes on several benchmark datasets. The source code of our experiment is available at https://github.com/arwhirang/sa-mtl and the dataset is available from the same URL.",0
"To predict properties of chemical compounds, low dimensional vectors are created using molecular descriptors and fingerprints. Choosing the right molecular descriptors and fingerprints is crucial and difficult, as the accuracy of the models heavily relies on them. To address this challenge, researchers have explored natural language processing models that use simplified molecular input line-entry system, and found that transformer-variant models outperform conventional methods. In this study, we investigated the structural differences of transformer-variant models and introduced a new self-attention based model. We evaluated the performance of the self-attention module in a multi-task learning environment using imbalanced chemical datasets and found that our model achieved competitive results on several benchmark datasets. The source code and dataset for our experiment are available at https://github.com/arwhirang/sa-mtl.",1
"In natural language processing (NLP), enormous pre-trained models like BERT have become the standard starting point for training on a range of downstream tasks, and similar trends are emerging in other areas of deep learning. In parallel, work on the lottery ticket hypothesis has shown that models for NLP and computer vision contain smaller matching subnetworks capable of training in isolation to full accuracy and transferring to other tasks. In this work, we combine these observations to assess whether such trainable, transferrable subnetworks exist in pre-trained BERT models. For a range of downstream tasks, we indeed find matching subnetworks at 40% to 90% sparsity. We find these subnetworks at (pre-trained) initialization, a deviation from prior NLP research where they emerge only after some amount of training. Subnetworks found on the masked language modeling task (the same task used to pre-train the model) transfer universally; those found on other tasks transfer in a limited fashion if at all. As large-scale pre-training becomes an increasingly central paradigm in deep learning, our results demonstrate that the main lottery ticket observations remain relevant in this context. Codes available at https://github.com/VITA-Group/BERT-Tickets.",0
"Pre-trained models like BERT have become the norm for training in various downstream tasks in natural language processing and other areas of deep learning. Recent research on the lottery ticket hypothesis has uncovered smaller subnetworks within NLP and computer vision models that can be trained independently and transferred to other tasks. This study combines these findings to examine whether such trainable subnetworks exist in pre-trained BERT models. The study shows that matching subnetworks can be found at a sparsity of 40% to 90% at initialization, unlike previous NLP research where they were only found after some training. Subnetworks found on the masked language modeling task transfer universally, while those found on other tasks transfer in a limited manner, if at all. These results highlight the continued relevance of the lottery ticket hypothesis in the context of large-scale pre-training. The codes used in the study are available at https://github.com/VITA-Group/BERT-Tickets.",1
"In recent years, text-guided image manipulation has gained increasing attention in the image generation research field. Recent works have proposed to deal with a simplified setting where the input image only has a single object and the text modification is acquired by swapping image captions or labels. In this paper, we study a setting that allows users to edit an image with multiple objects using complex text instructions. In this image generation task, the inputs are a reference image and an instruction in natural language that describes desired modifications to the input image. We propose a GAN-based method to tackle this problem. The key idea is to treat text as neural operators to locally modify the image feature. We show that the proposed model performs favorably against recent baselines on three public datasets.",0
"There has been an increased interest in text-guided image manipulation in the image generation research field. Previous works have focused on a simpler scenario where the input image has only one object and the text modification is obtained by swapping image captions or labels. However, this paper addresses a more complex situation where multiple objects are present in the input image, and users can edit it using complex text instructions. To solve this problem, we propose a GAN-based method that treats text as neural operators to locally modify the image feature. Our approach outperforms recent baselines on three public datasets, as demonstrated in our experiments.",1
"Vision-and-Language Navigation (VLN) is a natural language grounding task where an agent learns to follow language instructions and navigate to specified destinations in real-world environments. A key challenge is to recognize and stop at the correct location, especially for complicated outdoor environments. Existing methods treat the STOP action equally as other actions, which results in undesirable behaviors that the agent often fails to stop at the destination even though it might be on the right path. Therefore, we propose Learning to Stop (L2Stop), a simple yet effective policy module that differentiates STOP and other actions. Our approach achieves the new state of the art on a challenging urban VLN dataset Touchdown, outperforming the baseline by 6.89% (absolute improvement) on Success weighted by Edit Distance (SED).",0
"The task of Vision-and-Language Navigation (VLN) involves an agent learning to navigate real-world environments based on natural language instructions. A critical challenge is for the agent to recognize and stop at the correct location, particularly in complex outdoor settings. Current methods treat the STOP action equally to other actions, resulting in undesirable outcomes where the agent fails to stop at the destination even when on the correct path. In response, we introduce Learning to Stop (L2Stop), a straightforward yet effective policy module that distinguishes STOP from other actions. Our method achieves state-of-the-art performance on the challenging urban VLN dataset Touchdown, surpassing the baseline by 6.89% (absolute improvement) on the Success weighted by Edit Distance (SED) metric.",1
"Neural Networks and Deep Learning have seen an upsurge of research in the past decade due to the improved results. Generates text from the given image is a crucial task that requires the combination of both sectors which are computer vision and natural language processing in order to understand an image and represent it using a natural language. However existing works have all been done on a particular lingual domain and on the same set of data. This leads to the systems being developed to perform poorly on images that belong to specific locales' geographical context. TextMage is a system that is capable of understanding visual scenes that belong to the Bangladeshi geographical context and use its knowledge to represent what it understands in Bengali. Hence, we have trained a model on our previously developed and published dataset named BanglaLekhaImageCaptions. This dataset contains 9,154 images along with two annotations for each image. In order to access performance, the proposed model has been implemented and evaluated.",0
"Over the past decade, there has been a surge in research on Neural Networks and Deep Learning due to their improved results. A crucial task in this field is generating text from images, which requires a combination of computer vision and natural language processing to understand and represent an image using natural language. However, current works have focused on a specific lingual domain and data set, resulting in poor performance when applied to images from specific geographic locations. To address this, TextMage was developed to understand visual scenes specific to the Bangladeshi context and represent them in Bengali. The model was trained on a previously published dataset, BanglaLekhaImageCaptions, which includes 9,154 images and two annotations for each image. The proposed model was implemented and evaluated to assess its performance.",1
"We study learning of indexed families from positive data where a learner can freely choose a hypothesis space (with uniformly decidable membership) comprising at least the languages to be learned. This abstracts a very universal learning task which can be found in many areas, for example learning of (subsets of) regular languages or learning of natural languages. We are interested in various restrictions on learning, such as consistency, conservativeness or set-drivenness, exemplifying various natural learning restrictions.   Building on previous results from the literature, we provide several maps (depictions of all pairwise relations) of various groups of learning criteria, including a map for monotonicity restrictions and similar criteria and a map for restrictions on data presentation. Furthermore, we consider, for various learning criteria, whether learners can be assumed consistent.",0
"Our focus is on the study of learning indexed families through positive data, allowing learners to select a hypothesis space that includes the languages to be learned and has uniformly decidable membership. This type of learning is applicable in various fields, such as the learning of regular language subsets or natural languages. We aim to explore different learning restrictions, including consistency, conservativeness, and set-drivenness, which are natural constraints on learning. By expanding on previous literature, we provide several maps that illustrate the pairwise relationships between different groups of learning criteria. For instance, we have a map for monotonicity restrictions and similar criteria, as well as a map for constraints on data presentation. Additionally, we examine whether learners can be assumed to be consistent when considering different learning criteria.",1
"The quality of datasets is one of the key factors that affect the accuracy of aerodynamic data models. For example, in the uniformly sampled Burgers' dataset, the insufficient high-speed data is overwhelmed by massive low-speed data. Predicting high-speed data is more difficult than predicting low-speed data, owing to that the number of high-speed data is limited, i.e. the quality of the Burgers' dataset is not satisfactory. To improve the quality of datasets, traditional methods usually employ the data resampling technology to produce enough data for the insufficient parts in the original datasets before modeling, which increases computational costs. Recently, the mixtures of experts have been used in natural language processing to deal with different parts of sentences, which provides a solution for eliminating the need for data resampling in aerodynamic data modeling. Motivated by this, we propose the multi-task learning (MTL), a datasets quality-adaptive learning scheme, which combines task allocation and aerodynamic characteristics learning together to disperse the pressure of the entire learning task. The task allocation divides a whole learning task into several independent subtasks, while the aerodynamic characteristics learning learns these subtasks simultaneously to achieve better precision. Two experiments with poor quality datasets are conducted to verify the data quality-adaptivity of the MTL to datasets. The results show than the MTL is more accurate than FCNs and GANs in poor quality datasets.",0
"The accuracy of aerodynamic data models is heavily influenced by the quality of datasets. For instance, the uniformly sampled Burgers' dataset contains an overwhelming amount of low-speed data, making it difficult to predict high-speed data accurately. This is because high-speed data is scarce, thereby reducing the quality of the dataset. Traditional methods attempt to improve dataset quality by resampling data, which increases computational costs. However, recent developments in natural language processing have shown that mixtures of experts can be used to process different parts of sentences, eliminating the need for data resampling in aerodynamic data modeling. Inspired by this, we propose a multi-task learning (MTL) approach that combines task allocation and aerodynamic characteristics learning to improve dataset quality. MTL divides a learning task into independent subtasks and simultaneously learns them to achieve better precision. We conducted two experiments with low-quality datasets to verify the MTL's data quality-adaptivity, and the results show that MTL outperforms FCNs and GANs in such datasets.",1
"Despite the widely reported success of embedding-based machine learning methods on natural language processing tasks, the use of more easily interpreted engineered features remains common in fields such as cognitive impairment (CI) detection. Manually engineering features from noisy text is time and resource consuming, and can potentially result in features that do not enhance model performance. To combat this, we describe a new approach to feature engineering that leverages sequential machine learning models and domain knowledge to predict which features help enhance performance. We provide a concrete example of this method on a standard data set of CI speech and demonstrate that CI classification accuracy improves by 2.3% over a strong baseline when using features produced by this method. This demonstration provides an ex-ample of how this method can be used to assist classification in fields where interpretability is important, such as health care.",0
"Although embedding-based machine learning methods have been widely successful in natural language processing tasks, industries such as cognitive impairment detection continue to rely on easily interpretable engineered features. However, manually engineering these features from noisy text can be time-consuming and may not necessarily improve model performance. To address this issue, we present a novel approach to feature engineering that utilizes sequential machine learning models and domain knowledge to predict which features are beneficial. We provide a specific instance of this technique on a standard dataset of CI speech and show that the accuracy of CI classification increases by 2.3% when using features generated by our method, compared to a strong baseline. This example illustrates how our approach can aid classification in areas that prioritize interpretability, such as healthcare.",1
"This paper studies the task of temporal moment localization in a long untrimmed video using natural language query. Given a query sentence, the goal is to determine the start and end of the relevant segment within the video. Our key innovation is to learn a video feature embedding through a language-conditioned message-passing algorithm suitable for temporal moment localization which captures the relationships between humans, objects and activities in the video. These relationships are obtained by a spatial sub-graph that contextualizes the scene representation using detected objects and human features conditioned in the language query. Moreover, a temporal sub-graph captures the activities within the video through time. Our method is evaluated on three standard benchmark datasets, and we also introduce YouCookII as a new benchmark for this task. Experiments show our method outperforms state-of-the-art methods on these datasets, confirming the effectiveness of our approach.",0
"The focus of this research is on the process of locating temporal moments in a lengthy video that has not been edited, using a natural language query. The objective is to identify the start and end points of the relevant section within the video when given a query sentence. The main innovation of our study is the utilization of a language-conditioned message-passing algorithm to create a video feature embedding that is ideal for temporal moment localization. This algorithm captures the connections between humans, objects, and activities in the video using a spatial sub-graph that contextualizes the scene representation based on detected objects and human characteristics that are dependent on the language query. Additionally, a temporal sub-graph monitors the activities in the video over time. Our approach was tested on three widely used benchmark datasets, and we also introduced a new benchmark called YouCookII for this purpose. Our method outperformed existing state-of-the-art techniques on these datasets, providing evidence of its effectiveness.",1
"Most existing work that grounds natural language phrases in images starts with the assumption that the phrase in question is relevant to the image. In this paper we address a more realistic version of the natural language grounding task where we must both identify whether the phrase is relevant to an image and localize the phrase. This can also be viewed as a generalization of object detection to an open-ended vocabulary, introducing elements of few- and zero-shot detection. We propose an approach for this task that extends Faster R-CNN to relate image regions and phrases. By carefully initializing the classification layers of our network using canonical correlation analysis (CCA), we encourage a solution that is more discerning when reasoning between similar phrases, resulting in over double the performance compared to a naive adaptation on three popular phrase grounding datasets, Flickr30K Entities, ReferIt Game, and Visual Genome, with test-time phrase vocabulary sizes of 5K, 32K, and 159K, respectively.",0
"The majority of current research on relating natural language phrases to images assumes that the phrase is relevant to the image. However, this paper tackles a more practical version of this task, where we must determine if the phrase is relevant to the image and locate it within the image. This is a broader form of object detection that includes few- and zero-shot detection. To accomplish this, we propose an extension of Faster R-CNN that connects image regions and phrases. We use canonical correlation analysis (CCA) to initialize the network's classification layers, which results in a more precise solution when distinguishing between similar phrases. Our approach significantly outperforms a basic adaptation on three well-known datasets (Flickr30K Entities, ReferIt Game, and Visual Genome) with phrase vocabularies of 5K, 32K, and 159K, respectively.",1
"Sign language translation (SLT) aims to interpret sign video sequences into text-based natural language sentences. Sign videos consist of continuous sequences of sign gestures with no clear boundaries in between. Existing SLT models usually represent sign visual features in a frame-wise manner so as to avoid needing to explicitly segmenting the videos into isolated signs. However, these methods neglect the temporal information of signs and lead to substantial ambiguity in translation. In this paper, we explore the temporal semantic structures of signvideos to learn more discriminative features. To this end, we first present a novel sign video segment representation which takes into account multiple temporal granularities, thus alleviating the need for accurate video segmentation. Taking advantage of the proposed segment representation, we develop a novel hierarchical sign video feature learning method via a temporal semantic pyramid network, called TSPNet. Specifically, TSPNet introduces an inter-scale attention to evaluate and enhance local semantic consistency of sign segments and an intra-scale attention to resolve semantic ambiguity by using non-local video context. Experiments show that our TSPNet outperforms the state-of-the-art with significant improvements on the BLEU score (from 9.58 to 13.41) and ROUGE score (from 31.80 to 34.96)on the largest commonly-used SLT dataset. Our implementation is available at https://github.com/verashira/TSPNet.",0
"The objective of sign language translation (SLT) is to convert sign video sequences into natural language sentences in text format. Sign videos are continuous streams of sign gestures without clear boundaries between them. Existing SLT models typically represent sign features in a frame-by-frame manner to avoid the need for explicit video segmentation. However, this approach overlooks the temporal information of signs, leading to significant ambiguity in translation. This study investigates the temporal semantic structures of sign videos to learn more distinctive features. A new sign video segment representation is proposed, which considers various temporal granularities, reducing the requirement for accurate video segmentation. A new hierarchical sign video feature learning method called TSPNet is then developed using a temporal semantic pyramid network. TSPNet includes inter-scale attention to evaluate and enhance local semantic consistency of sign segments and intra-scale attention to resolve semantic uncertainty using non-local video context. Experiments reveal that TSPNet outperforms the state-of-the-art, with significant improvements on the BLEU score (from 9.58 to 13.41) and ROUGE score (from 31.80 to 34.96) on the most commonly-used SLT dataset. The implementation of TSPNet is available at https://github.com/verashira/TSPNet.",1
"Models based on self-attention mechanisms have been successful in analyzing temporal data and have been widely used in the natural language domain. We propose a new model architecture for video face representation and recognition based on a self-attention mechanism. Our approach could be used for video with single and multiple identities. To the best of our knowledge, no one has explored the aggregation approaches that consider the video with multiple identities. The proposed approach utilizes existing models to get the face representation for each video frame, e.g., ArcFace and MobileFaceNet, and the aggregation module produces the aggregated face representation vector for video by taking into consideration the order of frames and their quality scores. We demonstrate empirical results on a public dataset for video face recognition called IJB-C to indicate that the self-attention aggregation network (SAAN) outperforms naive average pooling. Moreover, we introduce a new multi-identity video dataset based on the publicly available UMDFaces dataset and collected GIFs from Giphy. We show that SAAN is capable of producing a compact face representation for both single and multiple identities in a video. The dataset and source code will be publicly available.",0
"Self-attention mechanisms have proven successful in analyzing temporal data and have been widely utilized in natural language applications. Our proposal introduces a novel model architecture for video face representation and recognition that is based on a self-attention mechanism. This approach is suitable for videos with one or more identities. To our knowledge, no previous research has explored aggregation techniques for videos with multiple identities. Our method employs existing face representation models such as ArcFace and MobileFaceNet, and the aggregation module produces a vector of aggregated face representations for each video frame, incorporating the order of frames and their quality scores. Empirical results using the IJB-C dataset demonstrate that the self-attention aggregation network (SAAN) outperforms naive average pooling. Additionally, we introduce a new multi-identity video dataset based on the publicly available UMDFaces dataset and Giphy GIFs. Our results indicate that SAAN can generate a compact face representation for both single and multiple identities in a video. The dataset and source code will be publicly accessible.",1
"We propose a novel type of balanced clustering algorithm to approximate attention. Attention complexity is reduced from $O(N^2)$ to $O(N \log N)$, where $N$ is the sequence length. Our algorithm, SMYRF, uses Locality Sensitive Hashing (LSH) in a novel way by defining new Asymmetric transformations and an adaptive scheme that produces balanced clusters. The biggest advantage of SMYRF is that it can be used as a drop-in replacement for dense attention layers without any retraining. On the contrary, prior fast attention methods impose constraints (e.g. queries and keys share the same vector representations) and require re-training from scratch. We apply our method to pre-trained state-of-the-art Natural Language Processing and Computer Vision models and we report significant memory and speed benefits. Notably, SMYRF-BERT outperforms (slightly) BERT on GLUE, while using $50\%$ less memory. We also show that SMYRF can be used interchangeably with dense attention before and after training. Finally, we use SMYRF to train GANs with attention in high resolutions. Using a single TPU, we were able to scale attention to 128x128=16k and 256x256=65k tokens on BigGAN on CelebA-HQ.",0
"We present a new type of clustering algorithm that approximates attention in a balanced manner. Our algorithm, SMYRF, reduces attention complexity from $O(N^2)$ to $O(N \log N)$, where $N$ represents sequence length. SMYRF employs Locality Sensitive Hashing (LSH) in a unique way by defining new Asymmetric transformations and an adaptive scheme that produces balanced clusters. The main advantage of SMYRF is that it can replace dense attention layers without any retraining, unlike prior fast attention methods that have constraints and require re-training from scratch. We applied SMYRF to state-of-the-art Natural Language Processing and Computer Vision models, resulting in significant memory and speed benefits. Notably, SMYRF-BERT outperforms BERT on GLUE while using only $50\%$ of the memory. We demonstrate that SMYRF can be used interchangeably with dense attention before and after training. Additionally, we used SMYRF to train GANs with attention in high resolutions, scaling attention to 128x128=16k and 256x256=65k tokens on BigGAN on CelebA-HQ using a single TPU.",1
"Deep neural networks have demonstrated their superior performance in almost every Natural Language Processing task, however, their increasing complexity raises concerns. In particular, these networks require high expenses on computational hardware, and training budget is a concern for many. Even for a trained network, the inference phase can be too demanding for resource-constrained devices, thus limiting its applicability. The state-of-the-art transformer models are a vivid example. Simplifying the computations performed by a network is one way of relaxing the complexity requirements. In this paper, we propose an end to end binarized neural network architecture for the intent classification task. In order to fully utilize the potential of end to end binarization, both input representations (vector embeddings of tokens statistics) and the classifier are binarized. We demonstrate the efficiency of such architecture on the intent classification of short texts over three datasets and for text classification with a larger dataset. The proposed architecture achieves comparable to the state-of-the-art results on standard intent classification datasets while utilizing ~ 20-40% lesser memory and training time. Furthermore, the individual components of the architecture, such as binarized vector embeddings of documents or binarized classifiers, can be used separately with not necessarily fully binary architectures.",0
"Although deep neural networks have proven highly effective in various Natural Language Processing tasks, their increasing complexity has raised concerns regarding their practicality. These networks often require costly computational hardware and training budgets, making them inaccessible to many. Additionally, even trained networks may be too demanding for resource-limited devices, limiting their application. The state-of-the-art transformer models serve as a clear example of this issue. To address these concerns, we propose an end to end binarized neural network architecture for intent classification. Our approach involves binarizing both the input representations (vector embeddings of token statistics) and the classifier to simplify computations. We demonstrate the effectiveness of our architecture on three intent classification datasets for short texts and a larger text classification dataset. Our proposed architecture achieves results comparable to state-of-the-art methods while utilizing 20-40% less memory and training time. Moreover, individual components of our architecture may be used separately with non-binary architectures.",1
"A visual relationship denotes a relationship between two objects in an image, which can be represented as a triplet of (subject; predicate; object). Visual relationship detection is crucial for scene understanding in images. Existing visual relationship detection datasets only contain true relationships that correctly describe the content in an image. However, distinguishing false visual relationships from true ones is also crucial for image understanding and grounded natural language processing. In this paper, we construct a visual relationship authenticity dataset, where both true and false relationships among all objects appeared in the captions in the Flickr30k entities image caption dataset are annotated. The dataset is available at https://github.com/codecreator2053/VR_ClassifiedDataset. We hope that this dataset can promote the study on both vision and language understanding.",0
"The concept of visual relationship refers to the connection between two objects in an image, which can be expressed as a set of (subject; predicate; object) triplets. Detecting visual relationships is essential for comprehending the scene depicted in an image. Although current visual relationship detection datasets only feature accurate relationships that correctly depict the image's content, it is equally important to differentiate between false and true visual relationships for image understanding and grounded natural language processing. To address this issue, we have developed a visual relationship authenticity dataset that contains both true and false relationships among all objects mentioned in the captions of the Flickr30k entities image caption dataset. The dataset is available at https://github.com/codecreator2053/VR_ClassifiedDataset, and we hope it will support research in vision and language understanding.",1
"Measuring similarity between IP addresses is an important task in the daily operations of any enterprise network. Applications that depend on an IP similarity measure include measuring correlation between security alerts, building baselines for behavioral modelling, debugging network failures and tracking persistent attacks. However, IPs do not have a natural similarity measure by definition. Deep Learning architectures are a promising solution here since they are able to learn numerical representations for IPs directly from data, allowing various distance measures to be applied on the calculated representations. Current works have utilized Natural Language Processing (NLP) techniques for learning IP embeddings. However, these approaches have no proper way to handle out-of-vocabulary (OOV) IPs not seen during training. In this paper, we propose a novel approach for IP embedding using an adapted graph neural network (GNN) architecture. This approach has the advantages of working on the raw data, scalability and, most importantly, induction, i.e. the ability to measure similarity between previously unseen IPs. Using data from an enterprise network, our approach is able to identify similarities between local DNS servers and root DNS servers even though some of these machines are never encountered during the training phase.",0
"An essential task in the day-to-day activities of enterprise networks is measuring the similarity between IP addresses. Applications that rely on IP similarity measures include building behavioral models, measuring the correlation between security alerts, debugging network failures, and tracking persistent attacks. Despite this, IP addresses do not possess a natural similarity measure by definition. To address this issue, Deep Learning architectures show promise by learning numerical representations for IPs directly from data, enabling various distance measures to be applied. However, current works utilizing Natural Language Processing (NLP) techniques lack a proper way to handle out-of-vocabulary (OOV) IPs that were not encountered during training. To address this limitation, this paper presents a novel approach utilizing an adapted graph neural network (GNN) architecture to embed IP addresses. The proposed approach provides numerous benefits such as working on raw data, scalability, and induction, which allows for measuring similarity between previously unseen IPs. Our approach, utilizing data from an enterprise network, successfully identified similarities between local and root DNS servers, even for machines that were not present during the training phase.",1
"Recent technological advancements in the Internet and Social media usage have resulted in the evolution of faster and efficient platforms of communication. These platforms include visual, textual and speech mediums and have brought a unique social phenomenon called Internet memes. Internet memes are in the form of images with witty, catchy, or sarcastic text descriptions. In this paper, we present a multi-modal sentiment analysis system using deep neural networks combining Computer Vision and Natural Language Processing. Our aim is different than the normal sentiment analysis goal of predicting whether a text expresses positive or negative sentiment; instead, we aim to classify the Internet meme as a positive, negative, or neutral, identify the type of humor expressed and quantify the extent to which a particular effect is being expressed. Our system has been developed using CNN and LSTM and outperformed the baseline score.",0
"The advancement of technology in the Internet and Social media has led to the emergence of faster and more efficient communication platforms. These platforms encompass various modes of communication, such as visual, textual, and speech mediums, and have given rise to a unique social phenomenon known as Internet memes. Internet memes typically consist of images accompanied by humorous, catchy, or sarcastic text descriptions. This study proposes a multi-modal sentiment analysis system that utilizes deep neural networks to combine Computer Vision and Natural Language Processing. Unlike traditional sentiment analysis systems that predict the polarity of a text, our system aims to categorize Internet memes as positive, negative, or neutral, identify the type of humor conveyed, and quantify the degree of expression. The proposed system employs CNN and LSTM and outperforms the baseline score.",1
"Natural language descriptions of user interface (UI) elements such as alternative text are crucial for accessibility and language-based interaction in general. Yet, these descriptions are constantly missing in mobile UIs. We propose widget captioning, a novel task for automatically generating language descriptions for UI elements from multimodal input including both the image and the structural representations of user interfaces. We collected a large-scale dataset for widget captioning with crowdsourcing. Our dataset contains 162,859 language phrases created by human workers for annotating 61,285 UI elements across 21,750 unique UI screens. We thoroughly analyze the dataset, and train and evaluate a set of deep model configurations to investigate how each feature modality as well as the choice of learning strategies impact the quality of predicted captions. The task formulation and the dataset as well as our benchmark models contribute a solid basis for this novel multimodal captioning task that connects language and user interfaces.",0
"In order to make user interface (UI) elements more accessible and improve language-based interaction, natural language descriptions such as alternative text are essential. However, these descriptions are often missing in mobile UIs. To address this issue, we introduce widget captioning, a new technique for automatically generating language descriptions for UI elements using both image and structural representations of interfaces. To build a dataset for widget captioning, we utilized crowdsourcing and collected 162,859 language phrases created by human workers to annotate 61,285 UI elements across 21,750 unique UI screens. We meticulously examine the dataset and use a range of deep model configurations to explore how different feature modalities and learning strategies affect the quality of predicted captions. Our task formulation, dataset, and benchmark models provide a solid foundation for this innovative multimodal captioning task that links language and user interfaces.",1
"Bolukbasi et al. (2016) presents one of the first gender bias mitigation techniques for word embeddings. Their method takes pre-trained word embeddings as input and attempts to isolate a linear subspace that captures most of the gender bias in the embeddings. As judged by an analogical evaluation task, their method virtually eliminates gender bias in the embeddings. However, an implicit and untested assumption of their method is that the bias sub-space is actually linear. In this work, we generalize their method to a kernelized, non-linear version. We take inspiration from kernel principal component analysis and derive a non-linear bias isolation technique. We discuss and overcome some of the practical drawbacks of our method for non-linear gender bias mitigation in word embeddings and analyze empirically whether the bias subspace is actually linear. Our analysis shows that gender bias is in fact well captured by a linear subspace, justifying the assumption of Bolukbasi et al. (2016).",0
"The method proposed by Bolukbasi et al. (2016) is among the earliest approaches to address gender bias in word embeddings. The technique involves using pre-trained word embeddings as input and identifying a linear subspace that encompasses the majority of the gender bias in the embeddings. Through an analogical evaluation task, the effectiveness of their method in eliminating gender bias is observed. However, their approach assumes that the bias subspace is linear, which has yet to be tested. In this study, we extend their method to a non-linear version using kernel principal component analysis as inspiration. We address the practical limitations of our approach for non-linear gender bias mitigation in word embeddings and analyze whether the bias subspace is truly linear. Our findings demonstrate that gender bias is indeed well-captured by a linear subspace, which supports the assumption made by Bolukbasi et al. (2016).",1
"Product descriptions in e-commerce platforms contain detailed and valuable information about retailers assortment. In particular, coding promotions within digital leaflets are of great interest in e-commerce as they capture the attention of consumers by showing regular promotions for different products. However, this information is embedded into images, making it difficult to extract and process for downstream tasks. In this paper, we present an end-to-end approach that classifies promotions within digital leaflets into their corresponding product categories using both visual and textual information. Our approach can be divided into three key components: 1) region detection, 2) text recognition and 3) text classification. In many cases, a single promotion refers to multiple product categories, so we introduce a multi-label objective in the classification head. We demonstrate the effectiveness of our approach for two separated tasks: 1) image-based detection of the descriptions for each individual promotion and 2) multi-label classification of the product categories using the text from the product descriptions. We train and evaluate our models using a private dataset composed of images from digital leaflets obtained by Nielsen. Results show that we consistently outperform the proposed baseline by a large margin in all the experiments.",0
"E-commerce platforms provide comprehensive and valuable information about the products they offer. One significant aspect of this information is the promotion codes embedded in digital leaflets, which attract consumers by offering regular discounts on different products. Unfortunately, this information is presented in image form, making it challenging to extract and process for further use. To address this issue, we propose an end-to-end approach that utilizes both visual and textual information to classify promotions by product category. Our approach consists of three primary components: region detection, text recognition, and text classification, with a multi-label objective to account for promotions that apply to multiple categories. We evaluate our approach using a private dataset of digital leaflet images obtained from Nielsen and demonstrate its effectiveness for both individual promotion detection and multi-label product category classification. Our results consistently outperform the baseline in all experiments.",1
"The query-based moment retrieval is a problem of localising a specific clip from an untrimmed video according a query sentence. This is a challenging task that requires interpretation of both the natural language query and the video content. Like in many other areas in computer vision and machine learning, the progress in query-based moment retrieval is heavily driven by the benchmark datasets and, therefore, their quality has significant impact on the field. In this paper, we present a series of experiments assessing how well the benchmark results reflect the true progress in solving the moment retrieval task. Our results indicate substantial biases in the popular datasets and unexpected behaviour of the state-of-the-art models. Moreover, we present new sanity check experiments and approaches for visualising the results. Finally, we suggest possible directions to improve the temporal sentence grounding in the future. Our code for this paper is available at https://mayu-ot.github.io/hidden-challenges-MR .",0
"The task of query-based moment retrieval involves identifying a specific clip from an untrimmed video based on a given query sentence. This task is complex and requires understanding both the video content and natural language query. As with other areas in computer vision and machine learning, the quality of benchmark datasets greatly influences progress in the field. This study presents a series of experiments that assess the accuracy of benchmark results in reflecting progress in solving the moment retrieval task. The results reveal biases in popular datasets and unexpected behavior in state-of-the-art models. Additionally, the study introduces new sanity check experiments and visualization approaches. Finally, the authors suggest possible directions for improving temporal sentence grounding in the future. The code for this study is available at https://mayu-ot.github.io/hidden-challenges-MR.",1
"Simultaneously modeling source code and natural language has many exciting applications in automated software development and understanding. Pursuant to achieving such technology, we introduce PyMT5, the Python method text-to-text transfer transformer, which is trained to translate between all pairs of Python method feature combinations: a single model that can both predict whole methods from natural language documentation strings (docstrings) and summarize code into docstrings of any common style. We present an analysis and modeling effort of a large-scale parallel corpus of 26 million Python methods and 7.7 million method-docstring pairs, demonstrating that for docstring and method generation, PyMT5 outperforms similarly-sized auto-regressive language models (GPT2) which were English pre-trained or randomly initialized. On the CodeSearchNet test set, our best model predicts 92.1% syntactically correct method bodies, achieved a BLEU score of 8.59 for method generation and 16.3 for docstring generation (summarization), and achieved a ROUGE-L F-score of 24.8 for method generation and 36.7 for docstring generation.",0
"The simultaneous modeling of source code and natural language has numerous potential uses in automated software development and comprehension. To achieve this technology, we introduce PyMT5, a Python method text-to-text transfer transformer that is trained to translate between all pairs of Python method feature combinations. This single model can predict entire methods from natural language documentation strings and summarize code into docstrings of any common style. We conducted an analysis and modeling effort using a large-scale parallel corpus of 26 million Python methods and 7.7 million method-docstring pairs. Our results demonstrate that PyMT5 outperforms similarly-sized auto-regressive language models, including English pre-trained or randomly initialized GPT2 models, for both docstring and method generation. Our best model achieved a high level of accuracy, with a 92.1% prediction rate for syntactically correct method bodies, BLEU scores of 8.59 for method generation and 16.3 for docstring generation (summarization), and ROUGE-L F-scores of 24.8 for method generation and 36.7 for docstring generation on the CodeSearchNet test set.",1
"Knowledge graph reasoning is a critical task in natural language processing. The task becomes more challenging on temporal knowledge graphs, where each fact is associated with a timestamp. Most existing methods focus on reasoning at past timestamps and they are not able to predict facts happening in the future. This paper proposes Recurrent Event Network (RE-NET), a novel autoregressive architecture for predicting future interactions. The occurrence of a fact (event) is modeled as a probability distribution conditioned on temporal sequences of past knowledge graphs. Specifically, our RE-NET employs a recurrent event encoder to encode past facts and uses a neighborhood aggregator to model the connection of facts at the same timestamp. Future facts can then be inferred in a sequential manner based on the two modules. We evaluate our proposed method via link prediction at future times on five public datasets. Through extensive experiments, we demonstrate the strength of RENET, especially on multi-step inference over future timestamps, and achieve state-of-the-art performance on all five datasets. Code and data can be found at https://github.com/INK-USC/RE-Net.",0
"The task of reasoning with knowledge graphs is essential in the field of natural language processing. When dealing with temporal knowledge graphs, which include timestamps for each fact, the task becomes even more difficult. Many existing methods for reasoning on temporal knowledge graphs focus only on past timestamps and cannot predict future facts. A new autoregressive architecture called Recurrent Event Network (RE-NET) is introduced in this paper to address this issue. RE-NET models the occurrence of a fact as a probability distribution based on past knowledge graphs. It includes a recurrent event encoder to encode past facts and a neighborhood aggregator to model the connection of facts at the same timestamp. Using these two modules, future facts can be inferred sequentially. The proposed method is evaluated on five public datasets, and the results show that RE-NET performs better than existing methods, particularly on multi-step inference over future timestamps. Code and data for RE-NET can be found at https://github.com/INK-USC/RE-Net.",1
"Zero-shot learning aims to recognize instances of unseen classes, for which no visual instance is available during training, by learning multimodal relations between samples from seen classes and corresponding class semantic representations. These class representations usually consist of either attributes, which do not scale well to large datasets, or word embeddings, which lead to poorer performance. A good trade-off could be to employ short sentences in natural language as class descriptions. We explore different solutions to use such short descriptions in a ZSL setting and show that while simple methods cannot achieve very good results with sentences alone, a combination of usual word embeddings and sentences can significantly outperform current state-of-the-art.",0
"The goal of zero-shot learning is to identify new classes that were not present during training, by establishing connections between samples from known classes and their corresponding semantic representations. These representations are typically attributes or word embeddings, but these methods have limitations. Alternatively, using brief natural language sentences as class descriptions may offer a better balance. We investigate ways to incorporate short descriptions in a ZSL context and find that while simple techniques do not perform well with sentences alone, combining traditional word embeddings and sentences can produce better results than current state-of-the-art methods.",1
"To scale non-parametric extensions of probabilistic topic models such as Latent Dirichlet allocation to larger data sets, practitioners rely increasingly on parallel and distributed systems. In this work, we study data-parallel training for the hierarchical Dirichlet process (HDP) topic model. Based upon a representation of certain conditional distributions within an HDP, we propose a doubly sparse data-parallel sampler for the HDP topic model. This sampler utilizes all available sources of sparsity found in natural language - an important way to make computation efficient. We benchmark our method on a well-known corpus (PubMed) with 8m documents and 768m tokens, using a single multi-core machine in under four days.",0
"To handle bigger data sets, professionals have started relying on parallel and distributed systems to enlarge non-parametric extensions of probabilistic topic models like Latent Dirichlet allocation. The focus of this research is on data-parallel training for the hierarchical Dirichlet process (HDP) topic model. By using a representation of certain conditional distributions within an HDP, the authors suggest a doubly sparse data-parallel sampler for the HDP topic model. This sampler uses all possible sources of sparsity found in natural language to improve computation efficiency. The team tests their approach on a well-known corpus (PubMed) with 8m documents and 768m tokens, using a single multi-core machine in under four days.",1
"Discovering concepts (or temporal abstractions) in an unsupervised manner from demonstration data in the absence of an environment is an important problem. Organizing these discovered concepts hierarchically at different levels of abstraction is useful in discovering patterns, building ontologies, and generating tutorials from demonstration data. However, recent work to discover such concepts without access to any environment does not discover relationships (or a hierarchy) between these discovered concepts. In this paper, we present a Transformer-based concept abstraction architecture UNHCLE (pronounced uncle) that extracts a hierarchy of concepts in an unsupervised way from demonstration data. We empirically demonstrate how UNHCLE discovers meaningful hierarchies using datasets from Chess and Cooking domains. Finally, we show how UNHCLE learns meaningful language labels for concepts by using demonstration data augmented with natural language for cooking and chess. All of our code is available at https://github.com/UNHCLE/UNHCLE",0
"The problem of discovering temporal abstractions or concepts in the absence of an environment from demonstration data is significant. It is helpful to organize these concepts hierarchically at different levels of abstraction for pattern recognition, ontology building, and generating tutorials. However, previous attempts to discover such concepts without environment access failed to identify relationships or hierarchies between them. This study introduces UNHCLE, a Transformer-based concept abstraction architecture that extracts a hierarchy of concepts from demonstration data in an unsupervised manner. The study demonstrates UNHCLE's ability to discover meaningful hierarchies using datasets from the Cooking and Chess domains. Additionally, the study showcases UNHCLE's capability to learn meaningful language labels for concepts by using natural language augmented cooking and chess demonstration data. The code for UNHCLE is available at https://github.com/UNHCLE/UNHCLE.",1
"In Natural Language Processing (NLP), pretrained language models (LMs) that are transferred to downstream tasks have been recently shown to achieve state-of-the-art results. However, standard fine-tuning can degrade the general-domain representations captured during pretraining. To address this issue, we introduce a new regularization technique, AFTER; domain Adversarial Fine-Tuning as an Effective Regularizer. Specifically, we complement the task-specific loss used during fine-tuning with an adversarial objective. This additional loss term is related to an adversarial classifier, that aims to discriminate between in-domain and out-of-domain text representations. In-domain refers to the labeled dataset of the task at hand while out-of-domain refers to unlabeled data from a different domain. Intuitively, the adversarial classifier acts as a regularizer which prevents the model from overfitting to the task-specific domain. Empirical results on various natural language understanding tasks show that AFTER leads to improved performance compared to standard fine-tuning.",0
"Recent studies in Natural Language Processing (NLP) have demonstrated that pretrained language models (LMs) transferred to downstream tasks are capable of achieving state-of-the-art outcomes. However, typical fine-tuning may cause a decline in the general-domain representations that were captured during pretraining. To tackle this issue, we propose a novel regularization technique called AFTER (domain Adversarial Fine-Tuning as an Effective Regularizer). We incorporate an adversarial objective in addition to the task-specific loss used during fine-tuning. This complementary loss term is linked to an adversarial classifier that differentiates in-domain and out-of-domain text representations. In-domain refers to the task-specific labeled dataset, while out-of-domain refers to unlabeled data from a different domain. The adversarial classifier serves as a regularizer, preventing the model from overfitting to the task-specific domain. Empirical evidence from various natural language understanding tasks demonstrates that AFTER enhances performance compared to standard fine-tuning.",1
"Neural autoregressive sequence models are used to generate sequences in a variety of natural language processing (NLP) tasks, where they are evaluated according to sequence-level task losses. These models are typically trained with maximum likelihood estimation, which ignores the task loss, yet empirically performs well as a surrogate objective. Typical approaches to directly optimizing the task loss such as policy gradient and minimum risk training are based around sampling in the sequence space to obtain candidate update directions that are scored based on the loss of a single sequence. In this paper, we develop an alternative method based on random search in the parameter space that leverages access to the maximum likelihood gradient. We propose maximum likelihood guided parameter search (MGS), which samples from a distribution over update directions that is a mixture of random search around the current parameters and around the maximum likelihood gradient, with each direction weighted by its improvement in the task loss. MGS shifts sampling to the parameter space, and scores candidates using losses that are pooled from multiple sequences. Our experiments show that MGS is capable of optimizing sequence-level losses, with substantial reductions in repetition and non-termination in sequence completion, and similar improvements to those of minimum risk training in machine translation.",0
"Neural autoregressive sequence models play a significant role in generating sequences in various natural language processing (NLP) tasks, and their performance is measured according to sequence-level task losses. Although maximum likelihood estimation is the standard method for training these models, it disregards the task loss, but is still effective in practice as a substitute objective. To optimize the task loss directly, methods such as policy gradient and minimum risk training are commonly used, which involve sampling in the sequence space to find update directions that are evaluated based on the loss of a single sequence. In this study, we introduce an alternative approach that employs random search in the parameter space and utilizes the maximum likelihood gradient. Our method, called maximum likelihood guided parameter search (MGS), selects update directions by sampling from a distribution that involves a mixture of random search around the current parameters and around the maximum likelihood gradient, with each direction weighted by its ability to improve the task loss. MGS shifts sampling to the parameter space and evaluates candidates using losses pooled from multiple sequences. Our experiments demonstrate that MGS effectively optimizes sequence-level losses, leading to substantial reductions in repetition and non-termination in sequence completion, and similar enhancements to those achieved by minimum risk training in machine translation.",1
"Referring image segmentation aims to predict the foreground mask of the object referred by a natural language sentence. Multimodal context of the sentence is crucial to distinguish the referent from the background. Existing methods either insufficiently or redundantly model the multimodal context. To tackle this problem, we propose a ""gather-propagate-distribute"" scheme to model multimodal context by cross-modal interaction and implement this scheme as a novel Linguistic Structure guided Context Modeling (LSCM) module. Our LSCM module builds a Dependency Parsing Tree suppressed Word Graph (DPT-WG) which guides all the words to include valid multimodal context of the sentence while excluding disturbing ones through three steps over the multimodal feature, i.e., gathering, constrained propagation and distributing. Extensive experiments on four benchmarks demonstrate that our method outperforms all the previous state-of-the-arts.",0
"The aim of referring image segmentation is to predict the foreground mask of an object mentioned in a natural language sentence. The ability to distinguish the referent from the background is dependent on the multimodal context of the sentence. However, current methods either fail to model the multimodal context adequately or do so redundantly. To address this issue, we propose a new approach called the ""gather-propagate-distribute"" scheme, which involves cross-modal interaction to model the multimodal context. We implement this scheme as a Linguistic Structure guided Context Modeling (LSCM) module. The LSCM module creates a Dependency Parsing Tree suppressed Word Graph (DPT-WG) that guides the inclusion of valid multimodal context while excluding irrelevant ones through three steps - gathering, constrained propagation, and distributing. Our experiments on four benchmarks demonstrate that our method outperforms all previous state-of-the-art methods.",1
"Vision-and-language navigation requires an agent to navigate through a real 3D environment following natural language instructions. Despite significant advances, few previous works are able to fully utilize the strong correspondence between the visual and textual sequences. Meanwhile, due to the lack of intermediate supervision, the agent's performance at following each part of the instruction cannot be assessed during navigation. In this work, we focus on the granularity of the visual and language sequences as well as the traceability of agents through the completion of an instruction. We provide agents with fine-grained annotations during training and find that they are able to follow the instruction better and have a higher chance of reaching the target at test time. We enrich the benchmark dataset Room-to-Room (R2R) with sub-instructions and their corresponding paths. To make use of this data, we propose effective sub-instruction attention and shifting modules that select and attend to a single sub-instruction at each time-step. We implement our sub-instruction modules in four state-of-the-art agents, compare with their baseline models, and show that our proposed method improves the performance of all four agents.   We release the Fine-Grained R2R dataset (FGR2R) and the code at https://github.com/YicongHong/Fine-Grained-R2R.",0
"The task of vision-and-language navigation involves guiding an agent through a physical 3D environment using natural language instructions. Despite significant progress, previous approaches have not fully exploited the correlation between visual and textual sequences, and the lack of intermediate supervision has made it difficult to evaluate the agent's performance at each step of the instruction. This study focuses on the granularity of visual and language sequences and the traceability of agents during instruction completion. Fine-grained annotations are provided during training, resulting in improved instruction-following ability and better target-reaching success rates during testing. The Room-to-Room (R2R) benchmark dataset is enriched with sub-instructions and corresponding paths, and effective sub-instruction attention and shifting modules are proposed to select and attend to a single sub-instruction at each time-step. The proposed sub-instruction modules are implemented in four state-of-the-art agents, and their performance is compared with baseline models, showing improvement across the board. The Fine-Grained R2R dataset (FGR2R) and code are available at https://github.com/YicongHong/Fine-Grained-R2R.",1
"Conservation laws are considered to be fundamental laws of nature. It has broad applications in many fields, including physics, chemistry, biology, geology, and engineering. Solving the differential equations associated with conservation laws is a major branch in computational mathematics. The recent success of machine learning, especially deep learning in areas such as computer vision and natural language processing, has attracted a lot of attention from the community of computational mathematics and inspired many intriguing works in combining machine learning with traditional methods. In this paper, we are the first to view numerical PDE solvers as an MDP and to use (deep) RL to learn new solvers. As proof of concept, we focus on 1-dimensional scalar conservation laws. We deploy the machinery of deep reinforcement learning to train a policy network that can decide on how the numerical solutions should be approximated in a sequential and spatial-temporal adaptive manner. We will show that the problem of solving conservation laws can be naturally viewed as a sequential decision-making process, and the numerical schemes learned in such a way can easily enforce long-term accuracy. Furthermore, the learned policy network is carefully designed to determine a good local discrete approximation based on the current state of the solution, which essentially makes the proposed method a meta-learning approach. In other words, the proposed method is capable of learning how to discretize for a given situation mimicking human experts. Finally, we will provide details on how the policy network is trained, how well it performs compared with some state-of-the-art numerical solvers such as WENO schemes, and supervised learning based approach L3D and PINN, and how well it generalizes.",0
"The conservation laws are fundamental laws that have widespread applications in various fields, including physics, chemistry, biology, geology, and engineering. Computational mathematics has a major branch dedicated to solving differential equations associated with these laws. With the recent success of machine learning, particularly deep learning, in computer vision and natural language processing, there has been a growing interest in combining traditional methods with machine learning. This paper presents a new approach that views numerical PDE solvers as an MDP and uses (deep) RL to learn new solvers. The focus is on 1-dimensional scalar conservation laws, and the proposed method trains a policy network to determine how to approximate numerical solutions in a sequential and spatial-temporal adaptive manner. The approach is a meta-learning approach that learns how to discretize a given situation, similar to how human experts would approach the problem. The paper provides details on how the policy network is trained, how well it performs compared to state-of-the-art numerical solvers such as WENO schemes, L3D, and PINN, and how well it generalizes. Overall, the proposed method offers a natural sequential decision-making process to solve conservation laws and can enforce long-term accuracy.",1
"As Machine Learning technologies become increasingly used in contexts that affect citizens, companies as well as researchers need to be confident that their application of these methods will not have unexpected social implications, such as bias towards gender, ethnicity, and/or people with disabilities. There is significant literature on approaches to mitigate bias and promote fairness, yet the area is complex and hard to penetrate for newcomers to the domain. This article seeks to provide an overview of the different schools of thought and approaches to mitigating (social) biases and increase fairness in the Machine Learning literature. It organises approaches into the widely accepted framework of pre-processing, in-processing, and post-processing methods, subcategorizing into a further 11 method areas. Although much of the literature emphasizes binary classification, a discussion of fairness in regression, recommender systems, unsupervised learning, and natural language processing is also provided along with a selection of currently available open source libraries. The article concludes by summarising open challenges articulated as four dilemmas for fairness research.",0
"The increasing use of Machine Learning technologies in areas that impact citizens requires companies and researchers to ensure that their implementation of these methods does not result in unexpected social repercussions, such as bias towards gender, ethnicity, and/or individuals with disabilities. Despite significant literature on methods to mitigate bias and promote fairness, the field remains complex and challenging for those unfamiliar with it. This article aims to provide an overview of various approaches to mitigating social biases and enhancing fairness in Machine Learning literature. The article categorizes these approaches into pre-processing, in-processing, and post-processing methods, further subdivided into 11 method areas. While most literature focuses on binary classification, this article also discusses fairness in regression, recommender systems, unsupervised learning, and natural language processing, along with available open source libraries. The article ends by outlining four dilemmas for fairness research that require further investigation.",1
"In deep neural nets, lower level embedding layers account for a large portion of the total number of parameters. Tikhonov regularization, graph-based regularization, and hard parameter sharing are approaches that introduce explicit biases into training in a hope to reduce statistical complexity. Alternatively, we propose stochastically shared embeddings (SSE), a data-driven approach to regularizing embedding layers, which stochastically transitions between embeddings during stochastic gradient descent (SGD). Because SSE integrates seamlessly with existing SGD algorithms, it can be used with only minor modifications when training large scale neural networks. We develop two versions of SSE: SSE-Graph using knowledge graphs of embeddings; SSE-SE using no prior information. We provide theoretical guarantees for our method and show its empirical effectiveness on 6 distinct tasks, from simple neural networks with one hidden layer in recommender systems, to the transformer and BERT in natural languages. We find that when used along with widely-used regularization methods such as weight decay and dropout, our proposed SSE can further reduce overfitting, which often leads to more favorable generalization results.",0
"A significant portion of the total number of parameters in deep neural nets is accounted for by lower level embedding layers. To reduce statistical complexity, explicit biases are introduced into training using approaches such as Tikhonov regularization, graph-based regularization, and hard parameter sharing. However, we suggest an alternative approach called stochastically shared embeddings (SSE) that uses a data-driven method to regularize embedding layers. During stochastic gradient descent (SGD), SSE stochastically transitions between embeddings, seamlessly integrating with existing SGD algorithms and requiring only minor modifications when training large scale neural networks. We have developed two versions of SSE: SSE-Graph using knowledge graphs of embeddings and SSE-SE using no prior information. Our method has theoretical guarantees and has shown empirical effectiveness in six distinct tasks, ranging from simple neural networks with one hidden layer in recommender systems to the transformer and BERT in natural languages. We have found that when used in conjunction with widely-used regularization methods such as weight decay and dropout, our proposed SSE further reduces overfitting, leading to more favorable generalization results.",1
"Recently, Deep Learning (DL) methods have shown an excellent performance in image captioning and visual question answering. However, despite their performance, DL methods do not learn the semantics of the words that are being used to describe a scene, making it difficult to spot incorrect words used in captions or to interchange words that have similar meanings. This work proposes a combination of DL methods for object detection and natural language processing to validate image's captions. We test our method in the FOIL-COCO data set, since it provides correct and incorrect captions for various images using only objects represented in the MS-COCO image data set. Results show that our method has a good overall performance, in some cases similar to the human performance.",0
"In recent times, image captioning and visual question answering have been dominated by Deep Learning (DL) methods, which have exhibited outstanding performance. However, DL methods have not been able to grasp the meaning of words employed to describe a particular scene, which poses a challenge in identifying incorrect words in captions or substituting words with similar meanings. This study proposes a blend of DL techniques for natural language processing and object detection to authenticate captions of images. The FOIL-COCO data set was used to test our approach, as it contains correct and incorrect captions for diverse images utilizing only objects present in the MS-COCO image data set. Our findings reveal that our technique performs well overall and is comparable to human performance in some cases.",1
"Referring image segmentation aims at segmenting the foreground masks of the entities that can well match the description given in the natural language expression. Previous approaches tackle this problem using implicit feature interaction and fusion between visual and linguistic modalities, but usually fail to explore informative words of the expression to well align features from the two modalities for accurately identifying the referred entity. In this paper, we propose a Cross-Modal Progressive Comprehension (CMPC) module and a Text-Guided Feature Exchange (TGFE) module to effectively address the challenging task. Concretely, the CMPC module first employs entity and attribute words to perceive all the related entities that might be considered by the expression. Then, the relational words are adopted to highlight the correct entity as well as suppress other irrelevant ones by multimodal graph reasoning. In addition to the CMPC module, we further leverage a simple yet effective TGFE module to integrate the reasoned multimodal features from different levels with the guidance of textual information. In this way, features from multi-levels could communicate with each other and be refined based on the textual context. We conduct extensive experiments on four popular referring segmentation benchmarks and achieve new state-of-the-art performances.",0
"The goal of referring image segmentation is to identify foreground masks of objects that match the natural language description given. Past methods have relied on implicit feature interaction and fusion between visual and linguistic modalities, but have not utilized informative words from the expression to accurately align features from both modalities. To address this issue, we introduce the Cross-Modal Progressive Comprehension (CMPC) module and Text-Guided Feature Exchange (TGFE) module in this paper. The CMPC module uses entity and attribute words to identify potential entities related to the expression, and relational words to filter out irrelevant entities through multimodal graph reasoning. Furthermore, the TGFE module integrates multimodal features from different levels with the aid of textual context to refine them. Our experiments on four popular referring segmentation benchmarks demonstrate new state-of-the-art results.",1
"Text is the most widely used means of communication today. This data is abundant but nevertheless complex to exploit within algorithms. For years, scientists have been trying to implement different techniques that enable computers to replicate some mechanisms of human reading. During the past five years, research disrupted the capacity of the algorithms to unleash the value of text data. It brings today, many opportunities for the insurance industry.Understanding those methods and, above all, knowing how to apply them is a major challenge and key to unleash the value of text data that have been stored for many years. Processing language with computer brings many new opportunities especially in the insurance sector where reports are central in the information used by insurers. SCOR's Data Analytics team has been working on the implementation of innovative tools or products that enable the use of the latest research on text analysis. Understanding text mining techniques in insurance enhances the monitoring of the underwritten risks and many processes that finally benefit policyholders.This article proposes to explain opportunities that Natural Language Processing (NLP) are providing to insurance. It details different methods used today in practice traces back the story of them. We also illustrate the implementation of certain methods using open source libraries and python codes that we have developed to facilitate the use of these techniques.After giving a general overview on the evolution of text mining during the past few years,we share about how to conduct a full study with text mining and share some examples to serve those models into insurance products or services. Finally, we explained in more details every step that composes a Natural Language Processing study to ensure the reader can have a deep understanding on the implementation.",0
"Communication today is largely dominated by text, but utilizing this data in algorithms can be complex. Scientists have been searching for ways to replicate human reading mechanisms in computers for years and recent research has disrupted the algorithms' capacity to extract value from text data. The insurance industry can benefit greatly from understanding and applying these methods. SCOR's Data Analytics team has been developing innovative tools and products to utilize the latest research on text analysis. This article explores the opportunities that Natural Language Processing (NLP) provides to the insurance industry, delving into the methods used today and their history. It also demonstrates the implementation of certain methods using open source libraries and Python codes. A full study on NLP is detailed, including examples of how it can be incorporated into insurance products and services. The article concludes by breaking down each step of the NLP study for a comprehensive understanding of the implementation process.",1
"Change Captioning is a task that aims to describe the difference between images with natural language. Most existing methods treat this problem as a difference judgment without the existence of distractors, such as viewpoint changes. However, in practice, viewpoint changes happen often and can overwhelm the semantic difference to be described. In this paper, we propose a novel visual encoder to explicitly distinguish viewpoint changes from semantic changes in the change captioning task. Moreover, we further simulate the attention preference of humans and propose a novel reinforcement learning process to fine-tune the attention directly with language evaluation rewards. Extensive experimental results show that our method outperforms the state-of-the-art approaches by a large margin in both Spot-the-Diff and CLEVR-Change datasets.",0
"The aim of Change Captioning is to describe the distinctions between images using natural language. However, current methods only consider differences without any distractors, such as changes in viewpoint, which can often overshadow the semantic differences that need to be conveyed. This paper introduces a new visual encoder that can distinguish between viewpoint changes and semantic changes in the change captioning task. Additionally, the study simulates human attention preferences and proposes a reinforcement learning process to optimize attention with language evaluation rewards. The experimental results demonstrate that this approach outperforms existing methods significantly on Spot-the-Diff and CLEVR-Change datasets.",1
"Anomaly detection in videos is a problem that has been studied for more than a decade. This area has piqued the interest of researchers due to its wide applicability. Because of this, there has been a wide array of approaches that have been proposed throughout the years and these approaches range from statistical-based approaches to machine learning-based approaches. Numerous surveys have already been conducted on this area but this paper focuses on providing an overview on the recent advances in the field of anomaly detection using Deep Learning. Deep Learning has been applied successfully in many fields of artificial intelligence such as computer vision, natural language processing and more. This survey, however, focuses on how Deep Learning has improved and provided more insights to the area of video anomaly detection. This paper provides a categorization of the different Deep Learning approaches with respect to their objectives. Additionally, it also discusses the commonly used datasets along with the common evaluation metrics. Afterwards, a discussion synthesizing all of the recent approaches is made to provide direction and possible areas for future research.",0
"The problem of detecting anomalies in videos has been a subject of research for over a decade. This area has attracted the attention of researchers due to its broad applicability, resulting in a variety of proposed approaches ranging from statistical-based to machine learning-based methods. While numerous surveys have been conducted on this topic, this paper focuses specifically on recent advances in anomaly detection using Deep Learning. Deep Learning has proven successful in various fields of artificial intelligence, including computer vision and natural language processing. This survey provides an overview of how Deep Learning has improved and contributed to the area of video anomaly detection, categorizing the different approaches based on their objectives and discussing commonly used datasets and evaluation metrics. Finally, the paper synthesizes recent approaches to provide direction and identify potential areas for future research.",1
"The internal workings of modern deep learning models stay often unclear to an external observer, although spatial attention mechanisms are involved. The idea of this work is to translate these spatial attentions into natural language to provide a simpler access to the model's function. Thus, I took a neural image captioning model and measured the reactions to external modification in its spatial attention for three different interface methods: a fixation over the whole generation process, a fixation for the first time-steps and an addition to the generator's attention. The experimental results for bounding box based spatial attention vectors have shown that the captioning model reacts to method dependent changes in up to 52.65% and includes in 9.00% of the cases object categories, which were otherwise unmentioned. Afterwards, I established such a link to a hierarchical co-attention network for visual question answering by extraction of its word, phrase and question level spatial attentions. Here, generated captions for the word level included details of the question-answer pairs in up to 55.20% of the cases. This work indicates that spatial attention seen as an external interface for image caption generators is an useful method to access visual functions in natural language.",0
"It is often difficult for an external observer to understand the inner workings of modern deep learning models, even though they involve spatial attention mechanisms. The objective of this study is to make these spatial attentions easier to comprehend by translating them into natural language. To achieve this, a neural image captioning model was used to measure the reactions to external modifications in its spatial attention for three different interface methods. These methods included fixation over the entire generation process, fixation for the first few time-steps, and an addition to the generator's attention. The experimental results showed that changes in the method can cause the captioning model to react by up to 52.65%, including object categories that were previously unmentioned in 9.00% of cases. The results also showed a link to a hierarchical co-attention network for visual question answering, where the extraction of word, phrase, and question level spatial attentions revealed that generated captions for the word level included details of the question-answer pairs in up to 55.20% of cases. The study highlights the usefulness of spatial attention as an external interface for accessing visual functions in natural language.",1
"Recognition of Arabic characters is essential for natural language processing and computer vision fields. The need to recognize and classify the handwritten Arabic letters and characters are essentially required. In this paper, we present an algorithm for recognizing Arabic letters and characters based on using deep convolution neural networks (DCNN) and support vector machine (SVM). This paper addresses the problem of recognizing the Arabic handwritten characters by determining the similarity between the input templates and the pre-stored templates using both fully connected DCNN and dropout SVM. Furthermore, this paper determines the correct classification rate (CRR) depends on the accuracy of the corrected classified templates, of the recognized handwritten Arabic characters. Moreover, we determine the error classification rate (ECR). The experimental results of this work indicate the ability of the proposed algorithm to recognize, identify, and verify the input handwritten Arabic characters. Furthermore, the proposed system determines similar Arabic characters using a clustering algorithm based on the K-means clustering approach to handle the problem of multi-stroke in Arabic characters. The comparative evaluation is stated and the system accuracy reached 95.07% CRR with 4.93% ECR compared with the state of the art.",0
"It is crucial to recognize Arabic characters in natural language processing and computer vision fields. The recognition and classification of handwritten Arabic letters and characters are necessary. This study introduces an algorithm that utilizes deep convolution neural networks (DCNN) and support vector machine (SVM) for recognizing Arabic letters and characters. The algorithm determines the similarity between input and pre-stored templates using fully connected DCNN and dropout SVM to address the problem of recognizing handwritten Arabic characters. The correct classification rate (CRR) and error classification rate (ECR) are determined for the recognized handwritten Arabic characters. The proposed algorithm can recognize, identify, and verify input handwritten Arabic characters. Additionally, a clustering algorithm based on the K-means clustering approach is used to handle the problem of multi-stroke in Arabic characters. The system accuracy is evaluated and compared to the state of the art, reaching a 95.07% CRR with 4.93% ECR.",1
"Machine learning models have been successfully applied to a wide range of applications including computer vision, natural language processing, and speech recognition. A successful implementation of these models however, usually relies on deep neural networks (DNNs) which are treated as opaque black-box systems due to their incomprehensible complexity and intricate internal mechanism. In this work, we present a novel algorithm for explaining the predictions of a DNN using adversarial machine learning. Our approach identifies the relative importance of input features in relation to the predictions based on the behavior of an adversarial attack on the DNN. Our algorithm has the advantage of being fast, consistent, and easy to implement and interpret. We present our detailed analysis that demonstrates how the behavior of an adversarial attack, given a DNN and a task, stays consistent for any input test data point proving the generality of our approach. Our analysis enables us to produce consistent and efficient explanations. We illustrate the effectiveness of our approach by conducting experiments using a variety of DNNs, tasks, and datasets. Finally, we compare our work with other well-known techniques in the current literature.",0
"A broad range of applications such as computer vision, natural language processing, and speech recognition have successfully utilized machine learning models. However, the effectiveness of these models often relies on complex deep neural networks (DNNs) that are considered black-box systems due to their intricate internal workings. This study introduces a new algorithm for explaining DNN predictions through adversarial machine learning. By examining the behavior of an adversarial attack on the DNN, the algorithm identifies the importance of input features in relation to predictions. The algorithm is fast, consistent, easy to implement, and interpret. Our analysis demonstrates that the behavior of an adversarial attack remains consistent for any input test data point, proving the generality of our approach. This consistency allows for efficient and consistent explanations. We validate our approach through experiments using various DNNs, tasks, and datasets and compare it with other well-known techniques in the current literature.",1
"Finding a path free from obstacles that poses minimal risk is critical for safe navigation. People who are sighted and people who are visually impaired require navigation safety while walking on a sidewalk. In this research we developed an assistive navigation on a sidewalk by integrating sensory inputs using reinforcement learning. We trained a Sidewalk Obstacle Avoidance Agent (SOAA) through reinforcement learning in a simulated robotic environment. A Sidewalk Obstacle Conversational Agent (SOCA) is built by training a natural language conversation agent with real conversation data. The SOAA along with SOCA was integrated in a prototype device called augmented guide (AG). Empirical analysis showed that this prototype improved the obstacle avoidance experience about 5% from a base case of 81.29%",0
"It is crucial to find a safe path that is free from obstructions and poses little danger in order to navigate safely. Both sighted individuals and those with visual impairments require safe navigation while walking on sidewalks. This study aimed to create an assistive navigation system on sidewalks by incorporating sensory inputs through reinforcement learning. To achieve this, we trained a Sidewalk Obstacle Avoidance Agent (SOAA) in a simulated robotic environment. Additionally, we built a Sidewalk Obstacle Conversational Agent (SOCA) by training a natural language conversation agent with real conversation data. These agents were then integrated into a prototype device called augmented guide (AG). The prototype was tested empirically and showed a 5% improvement in obstacle avoidance over a base case of 81.29%.",1
"Deep learning methods have shown state of the art performance in a range of tasks from computer vision to natural language processing. However, it is well known that such systems are vulnerable to attackers who craft inputs in order to cause misclassification. The level of perturbation an attacker needs to introduce in order to cause such a misclassification can be extremely small, and often imperceptible. This is of significant security concern, particularly where misclassification can cause harm to humans.   We thus propose Deep Latent Defence, an architecture which seeks to combine adversarial training with a detection system. At its core Deep Latent Defence has a adversarially trained neural network. A series of encoders take the intermediate layer representation of data as it passes though the network and project it to a latent space which we use for detecting adversarial samples via a $k$-nn classifier. We present results using both grey and white box attackers, as well as an adaptive $L_{\infty}$ bounded attack which was constructed specifically to try and evade our defence. We find that even under the strongest attacker model that we have investigated our defence is able to offer significant defensive benefits.",0
"State-of-the-art performance has been demonstrated by deep learning methods in various tasks ranging from computer vision to natural language processing. However, these systems are susceptible to attackers who create inputs to cause misclassification. The level of perturbation required to cause such misclassification can be extremely small, often imperceptible, posing a significant security concern, particularly where misclassification can result in harm to humans. Therefore, we propose Deep Latent Defence, a architecture that combines adversarial training with a detection system. Deep Latent Defence is primarily composed of an adversarially trained neural network. Encoders project the intermediate layer representation of data through the network to a latent space used to detect adversarial samples via a $k$-nn classifier. Our results demonstrate the effectiveness of our defence against grey and white box attackers, as well as an adaptive $L_{\infty}$ bounded attack designed to evade our defence. Even under the strongest attacker model investigated, our defence provides significant defensive benefits.",1
"Attention-based models have demonstrated remarkable success in various natural language understanding tasks. However, efficient execution remains a challenge for these models which are memory-bound due to their massive number of parameters. We present GOBO, a model quantization technique that compresses the vast majority (typically 99.9%) of the 32-bit floating-point parameters of state-of-the-art BERT models and their variants to 3 bits while maintaining their accuracy. Unlike other quantization methods, GOBO does not require fine-tuning nor retraining to compensate for the quantization error. We present two practical hardware applications of GOBO. In the first GOBO reduces memory storage and traffic and as a result inference latency and energy consumption. This GOBO memory compression mechanism is plug-in compatible with many architectures; we demonstrate it with the TPU, Eyeriss, and an architecture using Tensor Cores-like units. Second, we present a co-designed hardware architecture that also reduces computation. Uniquely, the GOBO architecture maintains most of the weights in 3b even during computation, a property that: (1) makes the processing elements area efficient, allowing us to pack more compute power per unit area, (2) replaces most multiply-accumulations with additions, and (3) reduces the off-chip traffic by amplifying on-chip memory capacity.",0
"Various natural language understanding tasks have seen remarkable success with attention-based models. However, the models face challenges in efficient execution as they are memory-bound due to their numerous parameters. We introduce GOBO, a model quantization technique that compresses the majority (typically 99.9%) of the 32-bit floating-point parameters of state-of-the-art BERT models and their variations to 3 bits while maintaining their accuracy. Unlike other quantization methods, GOBO does not mandate fine-tuning or retraining to compensate for quantization errors. GOBO has two practical hardware applications. Firstly, it reduces memory storage and traffic, leading to reduced inference latency and energy consumption. Secondly, a co-designed hardware architecture reduces computation. The GOBO architecture maintains most of the weights in 3b even during computation, making processing elements area-efficient and amplifying on-chip memory capacity, which reduces off-chip traffic.",1
"Deep neural networks have achieved great success both in computer vision and natural language processing tasks. However, mostly state-of-art methods highly rely on external training or computing to improve the performance. To alleviate the external reliance, we proposed a gradient enhancement approach, conducted by the short circuit neural connections, to improve the gradient learning of deep neural networks. The proposed short circuit is a unidirectional connection that single back propagates the sensitive from the deep layer to the shallows. Moreover, the short circuit formulates to be a gradient truncation of its crossing layers which can plug into the backbone deep neural networks without introducing external training parameters. Extensive experiments demonstrate deep neural networks with our short circuit gain a large margin over the baselines on both computer vision and natural language processing tasks.",0
"Although deep neural networks have been highly successful in tasks related to computer vision and natural language processing, their performance is often dependent on external training or computing methods. In order to reduce this reliance, we have proposed a gradient enhancement approach that involves short-circuiting neural connections to improve the gradient learning of deep neural networks. These short circuits are unidirectional connections that back propagate information from deep layers to shallower ones, acting as a gradient truncation of crossing layers. As a result, they can be integrated into pre-existing deep neural networks without introducing any additional training parameters. Our extensive experiments demonstrate that deep neural networks with short circuits outperform baselines in both computer vision and natural language processing tasks.",1
"Alzheimer's Dementia (AD) is an incurable, debilitating, and progressive neurodegenerative condition that affects cognitive function. Early diagnosis is important as therapeutics can delay progression and give those diagnosed vital time. Developing models that analyse spontaneous speech could eventually provide an efficient diagnostic modality for earlier diagnosis of AD. The Alzheimer's Dementia Recognition through Spontaneous Speech task offers acoustically pre-processed and balanced datasets for the classification and prediction of AD and associated phenotypes through the modelling of spontaneous speech. We exclusively analyse the supplied textual transcripts of the spontaneous speech dataset, building and comparing performance across numerous models for the classification of AD vs controls and the prediction of Mental Mini State Exam scores. We rigorously train and evaluate Support Vector Machines (SVMs), Gradient Boosting Decision Trees (GBDT), and Conditional Random Fields (CRFs) alongside deep learning Transformer based models. We find our top performing models to be a simple Term Frequency-Inverse Document Frequency (TF-IDF) vectoriser as input into a SVM model and a pre-trained Transformer based model `DistilBERT' when used as an embedding layer into simple linear models. We demonstrate test set scores of 0.81-0.82 across classification metrics and a RMSE of 4.58.",0
"Alzheimer's Dementia (AD) is a progressive and incurable condition that affects cognitive function. Early diagnosis is crucial to delay progression and provide patients with more time. Analyzing spontaneous speech models may lead to an efficient diagnostic method for diagnosing AD early. The Alzheimer's Dementia Recognition through Spontaneous Speech task provides processed datasets for the classification and prediction of AD and associated phenotypes through the modeling of spontaneous speech. We exclusively analyze the provided textual transcripts of the spontaneous speech dataset and compare the performance of numerous models for the classification of AD vs controls and the prediction of Mental Mini State Exam scores. We rigorously train and evaluate various models, including Support Vector Machines, Gradient Boosting Decision Trees, Conditional Random Fields, and deep learning Transformer-based models. Our results show that the top-performing models are a simple Term Frequency-Inverse Document Frequency vectorizer as input into an SVM model and a pre-trained Transformer-based model 'DistilBERT' used as an embedding layer into simple linear models. We achieve test set scores of 0.81-0.82 across classification metrics and an RMSE of 4.58.",1
"We investigated the effect of different training scenarios on predicting the (retro)synthesis of chemical compounds using a text-like representation of chemical reactions (SMILES) and Natural Language Processing neural network Transformer architecture. We showed that data augmentation, which is a powerful method used in image processing, eliminated the effect of data memorization by neural networks, and improved their performance for the prediction of new sequences. This effect was observed when augmentation was used simultaneously for input and the target data simultaneously. The top-5 accuracy was 84.8% for the prediction of the largest fragment (thus identifying principal transformation for classical retro-synthesis) for the USPTO-50k test dataset and was achieved by a combination of SMILES augmentation and a beam search algorithm. The same approach provided significantly better results for the prediction of direct reactions from the single-step USPTO-MIT test set. Our model achieved 90.6% top-1 and 96.1% top-5 accuracy for its challenging mixed set and 97% top-5 accuracy for the USPTO-MIT separated set. It also significantly improved results for USPTO-full set single-step retrosynthesis for both top-1 and top-10 accuracies. The appearance frequency of the most abundantly generated SMILES was well correlated with the prediction outcome and can be used as a measure of the quality of reaction prediction.",0
"Our study examined the impact of various training scenarios on the prediction of (retro)synthesis of chemical compounds using a text-based representation of chemical reactions (SMILES) and a Natural Language Processing neural network Transformer architecture. Through our research, we discovered that data augmentation, a powerful technique commonly used in image processing, effectively eliminated the neural network's tendency to memorize data, ultimately improving performance and enhancing prediction accuracy for new sequences. This effect was most noticeable when augmentation was used simultaneously for input and target data. Our model achieved an impressive top-5 accuracy of 84.8% for the prediction of the largest fragment, identifying the primary transformation for classical retro-synthesis, for the USPTO-50k test dataset. This was accomplished with a combination of SMILES augmentation and a beam search algorithm. The same approach yielded even better results for the prediction of direct reactions from the single-step USPTO-MIT test set, with our model achieving 90.6% top-1 and 96.1% top-5 accuracy for the challenging mixed set, and 97% top-5 accuracy for the USPTO-MIT separated set. Furthermore, our model significantly improved results for the USPTO-full set single-step retrosynthesis for both top-1 and top-10 accuracies. Lastly, we found that the frequency of the most commonly generated SMILES was directly correlated with the prediction outcome and could be used as a measure of the reaction prediction's quality.",1
"Mass utilization of body-worn cameras has led to a huge corpus of available egocentric video. Existing video summarization algorithms can accelerate browsing such videos by selecting (visually) interesting shots from them. Nonetheless, since the system user still has to watch the summary videos, browsing large video databases remain a challenge. Hence, in this work, we propose to generate a textual synopsis, consisting of a few sentences describing the most important events in a long egocentric videos. Users can read the short text to gain insight about the video, and more importantly, efficiently search through the content of a large video database using text queries. Since egocentric videos are long and contain many activities and events, using video-to-text algorithms results in thousands of descriptions, many of which are incorrect. Therefore, we propose a multi-task learning scheme to simultaneously generate descriptions for video segments and summarize the resulting descriptions in an end-to-end fashion. We Input a set of video shots and the network generates a text description for each shot. Next, visual-language content matching unit that is trained with a weakly supervised objective, identifies the correct descriptions. Finally, the last component of our network, called purport network, evaluates the descriptions all together to select the ones containing crucial information. Out of thousands of descriptions generated for the video, a few informative sentences are returned to the user. We validate our framework on the challenging UT Egocentric video dataset, where each video is between 3 to 5 hours long, associated with over 3000 textual descriptions on average. The generated textual summaries, including only 5 percent (or less) of the generated descriptions, are compared to groundtruth summaries in text domain using well-established metrics in natural language processing.",0
"The widespread use of body-worn cameras has resulted in a vast amount of egocentric video footage. Although current video summarization techniques can speed up the process of browsing through this footage by selecting visually appealing shots, it is still time-consuming for the user to watch the summary videos. To tackle this issue, we suggest creating a brief written summary that highlights the most significant events in long egocentric videos. This method allows users to read a short description of the video, making it easier to search through a large video database using text queries. However, generating a written summary for egocentric videos is challenging as they contain numerous activities and events, leading to thousands of often incorrect descriptions. To address this, we propose a multi-task learning approach that generates text descriptions for video segments and summarizes them in an end-to-end manner. Our network inputs a set of video shots and generates a text description for each shot, which is then evaluated by a visual-language content matching unit and a purport network. Only a handful of informative sentences are returned to the user from thousands of descriptions generated for the video. To validate our framework, we used the UT Egocentric video dataset, which consists of videos that are three to five hours long and have over 3000 textual descriptions on average. We compared the generated textual summaries using established metrics in natural language processing to the ground truth summaries.",1
"Recommendation has been a long-standing problem in many areas ranging from e-commerce to social websites. Most current studies focus only on traditional approaches such as content-based or collaborative filtering while there are relatively fewer studies in hybrid recommender systems. Due to the latest advances of deep learning achieved in different fields including computer vision and natural language processing, deep learning has also gained much attention in Recommendation Systems. There are several studies that utilize ID embeddings of users and items to implement collaborative filtering with deep neural networks. However, such studies do not take advantage of other categorical or continuous features of inputs. In this paper, we propose a new deep neural network architecture which consists of not only ID embeddings but also auxiliary information such as features of job postings and candidates for job recommendation system which is a reciprocal recommendation system. Experimental results on the dataset from a job-site show that the proposed method improves recommendation results over deep learning models utilizing ID embeddings.",0
"Recommendation has been a persistent issue in a variety of domains, spanning from e-commerce to social media platforms. The majority of existing research has focused solely on conventional methods, such as content-based or collaborative filtering, with a relatively smaller number of studies investigating hybrid recommender systems. With the recent advancements in deep learning, which have been realized in various fields, including computer vision and natural language processing, deep learning has gained considerable attention in Recommendation Systems. Prior studies have utilized ID embeddings of users and items to implement collaborative filtering with deep neural networks, but these studies have not taken advantage of other categorical or continuous features of inputs. In this article, we propose a novel deep neural network architecture that incorporates not only ID embeddings but also auxiliary information, such as job posting and candidate features, for job recommendation systems, which are reciprocal recommendation systems. Experimental results using a job-site dataset demonstrate that the proposed approach outperforms deep learning models that utilize only ID embeddings for recommendation.",1
"Adversarial machine learning has exposed several security hazards of neural models and has become an important research topic in recent times. Thus far, the concept of an ""adversarial perturbation"" has exclusively been used with reference to the input space referring to a small, imperceptible change which can cause a ML model to err. In this work we extend the idea of ""adversarial perturbations"" to the space of model weights, specifically to inject backdoors in trained DNNs, which exposes a security risk of using publicly available trained models. Here, injecting a backdoor refers to obtaining a desired outcome from the model when a trigger pattern is added to the input, while retaining the original model predictions on a non-triggered input. From the perspective of an adversary, we characterize these adversarial perturbations to be constrained within an $\ell_{\infty}$ norm around the original model weights. We introduce adversarial perturbations in the model weights using a composite loss on the predictions of the original model and the desired trigger through projected gradient descent. We empirically show that these adversarial weight perturbations exist universally across several computer vision and natural language processing tasks. Our results show that backdoors can be successfully injected with a very small average relative change in model weight values for several applications.",0
"In recent times, the topic of adversarial machine learning has gained importance due to the security hazards it has exposed in neural models. The term ""adversarial perturbation"" has so far only been used in reference to the input space, where a small imperceptible change can cause a ML model to err. However, in this work, we expand on the concept of ""adversarial perturbations"" to include the space of model weights, specifically for injecting backdoors in trained DNNs. This poses a security risk when using publicly available trained models. Injecting a backdoor involves obtaining a desired outcome from the model when a trigger pattern is added to the input, while retaining the original model predictions on a non-triggered input. These adversarial perturbations are characterized as being constrained within an $\ell_{\infty}$ norm around the original model weights. Adversarial perturbations in the model weights are introduced using a composite loss on the predictions of the original model and the desired trigger through projected gradient descent. Empirical evidence shows that these adversarial weight perturbations exist universally across several computer vision and natural language processing tasks. The results indicate that backdoors can be successfully injected with a very small average relative change in model weight values for several applications.",1
"We present a novel generalized zero-shot algorithm to recognize perceived emotions from gestures. Our task is to map gestures to novel emotion categories not encountered in training. We introduce an adversarial, autoencoder-based representation learning that correlates 3D motion-captured gesture sequence with the vectorized representation of the natural-language perceived emotion terms using word2vec embeddings. The language-semantic embedding provides a representation of the emotion label space, and we leverage this underlying distribution to map the gesture-sequences to the appropriate categorical emotion labels. We train our method using a combination of gestures annotated with known emotion terms and gestures not annotated with any emotions. We evaluate our method on the MPI Emotional Body Expressions Database (EBEDB) and obtain an accuracy of $58.43\%$. This improves the performance of current state-of-the-art algorithms for generalized zero-shot learning by $25$--$27\%$ on the absolute.",0
"Our study introduces a new approach for detecting emotions from gestures using a generalized zero-shot algorithm. The objective is to recognize emotions that were not part of the training dataset. We propose an adversarial, autoencoder-based representation learning that correlates 3D motion-captured gesture sequence with the vectorized representation of natural-language perceived emotion terms using word2vec embeddings. By utilizing the language-semantic embedding, we can map gesture-sequences to the corresponding categorical emotion labels. The method is trained using a combination of annotated and unannotated gestures. Our approach is evaluated on the MPI Emotional Body Expressions Database (EBEDB), achieving an accuracy of $58.43\%$. The performance of our approach improves current state-of-the-art algorithms for generalized zero-shot learning by $25$--$27\%$ on the absolute.",1
"The task of visual grounding requires locating the most relevant region or object in an image, given a natural language query. So far, progress on this task was mostly measured on curated datasets, which are not always representative of human spoken language. In this work, we deviate from recent, popular task settings and consider the problem under an autonomous vehicle scenario. In particular, we consider a situation where passengers can give free-form natural language commands to a vehicle which can be associated with an object in the street scene. To stimulate research on this topic, we have organized the \emph{Commands for Autonomous Vehicles} (C4AV) challenge based on the recent \emph{Talk2Car} dataset (URL: https://www.aicrowd.com/challenges/eccv-2020-commands-4-autonomous-vehicles). This paper presents the results of the challenge. First, we compare the used benchmark against existing datasets for visual grounding. Second, we identify the aspects that render top-performing models successful, and relate them to existing state-of-the-art models for visual grounding, in addition to detecting potential failure cases by evaluating on carefully selected subsets. Finally, we discuss several possibilities for future work.",0
"The task of visual grounding involves finding the most relevant object or region in an image based on a natural language query. However, progress on this task has mainly been evaluated on curated datasets that may not accurately represent human language. In this study, we depart from recent task settings and examine the problem in an autonomous vehicle context. Specifically, we consider a scenario where passengers can give free-form natural language commands to a vehicle that can associate them with objects in the street scene. To encourage research in this area, we created the Commands for Autonomous Vehicles (C4AV) challenge based on the Talk2Car dataset. This paper presents the challenge results and compares the benchmark to existing datasets for visual grounding. We also identify the factors that make top-performing models successful and examine their relation to current state-of-the-art models for visual grounding. In addition, we detect potential failure cases by evaluating on carefully selected subsets and explore future research possibilities.",1
"BERT has recently attracted a lot of attention in natural language understanding (NLU) and achieved state-of-the-art results in various NLU tasks. However, its success requires large deep neural networks and huge amount of data, which result in long training time and impede development progress. Using stochastic gradient methods with large mini-batch has been advocated as an efficient tool to reduce the training time. Along this line of research, LAMB is a prominent example that reduces the training time of BERT from 3 days to 76 minutes on a TPUv3 Pod. In this paper, we propose an accelerated gradient method called LANS to improve the efficiency of using large mini-batches for training. As the learning rate is theoretically upper bounded by the inverse of the Lipschitz constant of the function, one cannot always reduce the number of optimization iterations by selecting a larger learning rate. In order to use larger mini-batch size without accuracy loss, we develop a new learning rate scheduler that overcomes the difficulty of using large learning rate. Using the proposed LANS method and the learning rate scheme, we scaled up the mini-batch sizes to 96K and 33K in phases 1 and 2 of BERT pretraining, respectively. It takes 54 minutes on 192 AWS EC2 P3dn.24xlarge instances to achieve a target F1 score of 90.5 or higher on SQuAD v1.1, achieving the fastest BERT training time in the cloud.",0
"In the field of natural language understanding, BERT has gained significant attention due to its high performance in various tasks. However, its success comes at the cost of lengthy training times and a requirement for massive amounts of data and complex neural networks. To address this issue, researchers have suggested using stochastic gradient methods with large mini-batches to reduce training time. LAMB is an example of this approach, which has been successful in reducing BERT's training time from 3 days to 76 minutes. In this paper, we introduce LANS, a new accelerated gradient method that further improves the efficiency of using large mini-batches for training. Our approach includes a learning rate scheduler that allows for larger mini-batch sizes without sacrificing accuracy. Using LANS, we increased the mini-batch sizes to 96K and 33K in different phases of BERT pretraining, achieving a target F1 score of 90.5 or higher on SQuAD v1.1 in just 54 minutes on 192 AWS EC2 P3dn.24xlarge instances, making it the fastest BERT training time in the cloud.",1
"Text-to-Face (TTF) synthesis is a challenging task with great potential for diverse computer vision applications. Compared to Text-to-Image (TTI) synthesis tasks, the textual description of faces can be much more complicated and detailed due to the variety of facial attributes and the parsing of high dimensional abstract natural language. In this paper, we propose a Text-to-Face model that not only produces images in high resolution (1024x1024) with text-to-image consistency, but also outputs multiple diverse faces to cover a wide range of unspecified facial features in a natural way. By fine-tuning the multi-label classifier and image encoder, our model obtains the vectors and image embeddings which are used to transform the input noise vector sampled from the normal distribution. Afterwards, the transformed noise vector is fed into a pre-trained high-resolution image generator to produce a set of faces with the desired facial attributes. We refer to our model as TTF-HD. Experimental results show that TTF-HD generates high-quality faces with state-of-the-art performance.",0
"The synthesis of Text-to-Face (TTF) is a task that poses a challenge but has immense potential for various computer vision applications. Unlike Text-to-Image (TTI) synthesis tasks, the creation of text-based descriptions for faces can be more complex and intricate due to the diverse facial attributes and parsing of high dimensional natural language. This article proposes a Text-to-Face model that not only generates high-resolution images (1024x1024) with text-to-image consistency but also produces multiple diverse faces that naturally cover a broad spectrum of unspecified facial features. By refining the multi-label classifier and image encoder, the model obtains vectors and image embeddings that transform the input noise vector sampled from a normal distribution. The transformed noise vector is then fed into a pre-trained high-resolution image generator to produce a set of faces with the desired facial attributes. This model is referred to as TTF-HD, and experimental results demonstrate that it produces high-quality faces with state-of-the-art performance.",1
"Temporal grounding of natural language in untrimmed videos is a fundamental yet challenging multimedia task facilitating cross-media visual content retrieval. We focus on the weakly supervised setting of this task that merely accesses to coarse video-level language description annotation without temporal boundary, which is more consistent with reality as such weak labels are more readily available in practice. In this paper, we propose a \emph{Boundary Adaptive Refinement} (BAR) framework that resorts to reinforcement learning (RL) to guide the process of progressively refining the temporal boundary. To the best of our knowledge, we offer the first attempt to extend RL to temporal localization task with weak supervision. As it is non-trivial to obtain a straightforward reward function in the absence of pairwise granular boundary-query annotations, a cross-modal alignment evaluator is crafted to measure the alignment degree of segment-query pair to provide tailor-designed rewards. This refinement scheme completely abandons traditional sliding window based solution pattern and contributes to acquiring more efficient, boundary-flexible and content-aware grounding results. Extensive experiments on two public benchmarks Charades-STA and ActivityNet demonstrate that BAR outperforms the state-of-the-art weakly-supervised method and even beats some competitive fully-supervised ones.",0
"Grounding natural language in untrimmed videos over time is a complex multimedia task that is crucial for cross-media visual content retrieval. Our focus is on the weakly supervised aspect of this task, where only coarse video-level language description annotation is available without temporal boundaries. This is more realistic as weak labels are more readily available in practice. This paper proposes the \emph{Boundary Adaptive Refinement} (BAR) framework that uses reinforcement learning (RL) to guide the process of progressively refining the temporal boundary. We believe this is the first attempt to extend RL to temporal localization tasks with weak supervision. As obtaining a straightforward reward function is difficult without pairwise granular boundary-query annotations, a cross-modal alignment evaluator is used to measure the alignment degree of segment-query pairs and provide tailor-designed rewards. This refinement scheme is more efficient, boundary-flexible and content-aware than traditional sliding window based solutions. We conducted extensive experiments on two public benchmarks, Charades-STA and ActivityNet, and BAR outperformed the state-of-the-art weakly-supervised method and even some competitive fully-supervised ones.",1
"Memes are graphics and text overlapped so that together they present concepts that become dubious if one of them is absent. It is spread mostly on social media platforms, in the form of jokes, sarcasm, motivating, etc. After the success of BERT in Natural Language Processing (NLP), researchers inclined to Visual-Linguistic (VL) multimodal problems like memes classification, image captioning, Visual Question Answering (VQA), and many more. Unfortunately, many memes get uploaded each day on social media platforms that need automatic censoring to curb misinformation and hate. Recently, this issue has attracted the attention of researchers and practitioners. State-of-the-art methods that performed significantly on other VL dataset, tends to fail on memes classification. In this context, this work aims to conduct a comprehensive study on memes classification, generally on the VL multimodal problems and cutting edge solutions. We propose a generalized framework for VL problems. We cover the early and next-generation works on VL problems. Finally, we identify and articulate several open research issues and challenges. This is the first study that presents the generalized view of the advanced classification techniques concerning memes classification to the best of our knowledge. We believe this study presents a clear road-map for the Machine Learning (ML) research community to implement and enhance memes classification techniques.",0
"Memes are combinations of images and text that convey concepts that may lose meaning if either component is missing. Memes are widely circulated on social media platforms as jokes, sarcastic remarks, motivational messages, and more. Following the success of BERT in Natural Language Processing (NLP), researchers have turned their attention to Visual-Linguistic (VL) multimodal problems, such as memes classification, image captioning, Visual Question Answering (VQA), and others. However, the sheer number of memes uploaded daily on social media platforms requires automatic censoring to prevent the spread of misinformation and hate. As a result, researchers and practitioners are focusing their efforts on this issue. Although many state-of-the-art techniques have been developed for other VL datasets, they tend to fail in memes classification. In this study, we aim to conduct a comprehensive investigation into memes classification and other VL multimodal problems, as well as cutting-edge solutions. We present a generalized framework for VL problems, covering both early and next-generation works. Finally, we identify several open research issues and challenges. To the best of our knowledge, this is the first study that provides a comprehensive view of advanced classification techniques for memes classification. We believe that our study provides a clear roadmap for the Machine Learning (ML) research community to implement and improve memes classification techniques.",1
"It is well observed that in deep learning and computer vision literature, visual data are always represented in a manually designed coding scheme (eg., RGB images are represented as integers ranging from 0 to 255 for each channel) when they are input to an end-to-end deep neural network (DNN) for any learning task. We boldly question whether the manually designed inputs are good for DNN training for different tasks and study whether the input to a DNN can be optimally learned end-to-end together with learning the weights of the DNN. In this paper, we propose the paradigm of {\em deep collective learning} which aims to learn the weights of DNNs and the inputs to DNNs simultaneously for given tasks. We note that collective learning has been implicitly but widely used in natural language processing while it has almost never been studied in computer vision. Consequently, we propose the lookup vision networks (Lookup-VNets) as a solution to deep collective learning in computer vision. This is achieved by associating each color in each channel with a vector in lookup tables. As learning inputs in computer vision has almost never been studied in the existing literature, we explore several aspects of this question through varieties of experiments on image classification tasks. Experimental results on four benchmark datasets, i.e., CIFAR-10, CIFAR-100, Tiny ImageNet, and ImageNet (ILSVRC2012) have shown several surprising characteristics of Lookup-VNets and have demonstrated the advantages and promise of Lookup-VNets and deep collective learning.",0
"It is commonly observed that visual data is represented through a manually designed coding scheme, such as RGB images being represented as integers ranging from 0 to 255 for each channel, when fed into an end-to-end deep neural network (DNN) for learning tasks in deep learning and computer vision literature. We challenge the effectiveness of these manually designed inputs in DNN training for different tasks and investigate the possibility of learning the input to a DNN end-to-end along with the weights of the DNN. Our proposed paradigm of ""deep collective learning"" aims to achieve this by simultaneously learning the weights of DNNs and their inputs for given tasks. While collective learning has been widely used in natural language processing, it has not been thoroughly studied in computer vision. Therefore, we propose Lookup Vision Networks (Lookup-VNets) as a solution to deep collective learning in computer vision. This is achieved by associating each color in each channel with a vector in lookup tables. As learning inputs in computer vision has not been extensively studied, we conduct various experiments on image classification tasks to explore different aspects of this question. Our experimental results on four benchmark datasets, namely CIFAR-10, CIFAR-100, Tiny ImageNet, and ImageNet (ILSVRC2012), reveal several surprising characteristics of Lookup-VNets and demonstrate the advantages and potential of Lookup-VNets and deep collective learning.",1
"The spectacular expansion of the Internet has led to the development of a new research problem in the field of natural language processing: automatic toxic comment detection, since many countries prohibit hate speech in public media. There is no clear and formal definition of hate, offensive, toxic and abusive speeches. In this article, we put all these terms under the umbrella of ""toxic"" speech. The contribution of this paper is the design of binary classification and regression-based approaches aiming to predict whether a comment is toxic or not. We compare different unsupervised word representations and different DNN based classifiers. Moreover, we study the robustness of the proposed approaches to adversarial attacks by adding one (healthy or toxic) word. We evaluate the proposed methodology on the English Wikipedia Detox corpus. Our experiments show that using BERT fine-tuning outperforms feature-based BERT, Mikolov's and fastText representations with different DNN classifiers.",0
"The Internet's remarkable growth has given rise to a new area of research in natural language processing - the detection of toxic comments - due to the prohibition of hate speech in public media in many countries. Although there is no clear definition of hate, offense, toxic and abusive speech, we classify all these terms as ""toxic"" speech in this article. Our paper presents binary classification and regression-based methods to predict whether a comment is toxic or not. We compare unsupervised word representations and DNN-based classifiers and examine the robustness of our approaches to adversarial attacks. By adding one word, either healthy or toxic, we analyze the proposed methodology on the English Wikipedia Detox corpus. Our experiments reveal that BERT fine-tuning outperforms feature-based BERT, Mikolov's and fastText representations using various DNN classifiers.",1
"Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of ""X-former"" models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this paper characterizes a large and thoughtful selection of recent efficiency-flavored ""X-former"" models, providing an organized and comprehensive overview of existing work and models across multiple domains.",0
"The effectiveness of Transformer model architectures has caused a surge of interest across various domains including language, vision, and reinforcement learning. In natural language processing, Transformers have become a crucial part of modern deep learning. Numerous ""X-former"" models have been introduced, such as Reformer, Linformer, Performer, and Longformer, which aim to enhance the original Transformer architecture by improving computational and memory efficiency. This paper aims to assist researchers in navigating this wide array of models by presenting a thorough and organized overview of recent efficiency-focused ""X-former"" models across multiple domains.",1
"Few sample learning (FSL) is significant and challenging in the field of machine learning. The capability of learning and generalizing from very few samples successfully is a noticeable demarcation separating artificial intelligence and human intelligence since humans can readily establish their cognition to novelty from just a single or a handful of examples whereas machine learning algorithms typically entail hundreds or thousands of supervised samples to guarantee generalization ability. Despite the long history dated back to the early 2000s and the widespread attention in recent years with booming deep learning technologies, little surveys or reviews for FSL are available until now. In this context, we extensively review 300+ papers of FSL spanning from the 2000s to 2019 and provide a timely and comprehensive survey for FSL. In this survey, we review the evolution history as well as the current progress on FSL, categorize FSL approaches into the generative model based and discriminative model based kinds in principle, and emphasize particularly on the meta learning based FSL approaches. We also summarize several recently emerging extensional topics of FSL and review the latest advances on these topics. Furthermore, we highlight the important FSL applications covering many research hotspots in computer vision, natural language processing, audio and speech, reinforcement learning and robotic, data analysis, etc. Finally, we conclude the survey with a discussion on promising trends in the hope of providing guidance and insights to follow-up researches.",0
"Learning from few samples (FSL) is a challenging and significant area in machine learning. The ability to learn and generalize from only a handful of examples is a notable difference between human and artificial intelligence. While humans can easily adapt to new situations from just one or a few examples, machine learning algorithms typically require hundreds or thousands of supervised samples to ensure generalization. Despite its long history and recent popularity with deep learning technologies, there are few surveys or reviews available for FSL. To fill this gap, we extensively review over 300 FSL papers from 2000 to 2019 and categorize FSL approaches into generative and discriminative models, emphasizing meta-learning approaches. We also cover emerging topics and applications of FSL in computer vision, natural language processing, audio and speech, reinforcement learning, robotic data analysis, and more. Finally, we discuss promising trends and provide guidance for future research.",1
"The interest in Artificial Intelligence (AI) and its applications has seen unprecedented growth in the last few years. This success can be partly attributed to the advancements made in the sub-fields of AI such as Machine Learning (ML), Computer Vision (CV), and Natural Language Processing (NLP). The largest of the growths in these fields has been made possible with deep learning, a sub-area of machine learning, which uses the principles of artificial neural networks. This has created significant interest in the integration of vision and language. The tasks are designed such that they perfectly embrace the ideas of deep learning. In this survey, we focus on ten prominent tasks that integrate language and vision by discussing their problem formulations, methods, existing datasets, evaluation measures, and compare the results obtained with corresponding state-of-the-art methods. Our efforts go beyond earlier surveys which are either task-specific or concentrate only on one type of visual content, i.e., image or video. Furthermore, we also provide some potential future directions in this field of research with an anticipation that this survey brings in innovative thoughts and ideas to address the existing challenges and build new applications.",0
"In recent years, there has been an unprecedented surge in interest in Artificial Intelligence (AI) and its various applications. This can be partly attributed to the progress made in sub-fields such as Machine Learning (ML), Computer Vision (CV), and Natural Language Processing (NLP). The most significant advancements in these areas have been made possible thanks to deep learning, a sub-area of ML that utilizes artificial neural networks. Consequently, there has been a growing interest in integrating vision and language, with tasks designed to embrace the principles of deep learning. This survey focuses on ten prominent tasks that integrate language and vision, discussing their problem formulations, methods, existing datasets, evaluation measures, and results obtained compared to state-of-the-art methods. Unlike previous surveys that were task-specific or focused solely on image or video content, this survey provides a comprehensive review of various types of visual content. Additionally, the survey presents potential future directions for research in this field, with the hope of inspiring innovative ideas to address existing challenges and create new applications.",1
"Despite great recent advances in visual tracking, its further development, including both algorithm design and evaluation, is limited due to lack of dedicated large-scale benchmarks. To address this problem, we present LaSOT, a high-quality Large-scale Single Object Tracking benchmark. LaSOT contains a diverse selection of 85 object classes, and offers 1,550 totaling more than 3.87 million frames. Each video frame is carefully and manually annotated with a bounding box. This makes LaSOT, to our knowledge, the largest densely annotated tracking benchmark. Our goal in releasing LaSOT is to provide a dedicated high quality platform for both training and evaluation of trackers. The average video length of LaSOT is around 2,500 frames, where each video contains various challenge factors that exist in real world video footage,such as the targets disappearing and re-appearing. These longer video lengths allow for the assessment of long-term trackers. To take advantage of the close connection between visual appearance and natural language, we provide language specification for each video in LaSOT. We believe such additions will allow for future research to use linguistic features to improve tracking. Two protocols, full-overlap and one-shot, are designated for flexible assessment of trackers. We extensively evaluate 48 baseline trackers on LaSOT with in-depth analysis, and results reveal that there still exists significant room for improvement. The complete benchmark, tracking results as well as analysis are available at http://vision.cs.stonybrook.edu/~lasot/.",0
"The development of visual tracking has been limited by the lack of large-scale benchmarks, despite recent advances in algorithm design and evaluation. To address this problem, we have introduced LaSOT, a high-quality benchmark for single object tracking. LaSOT includes 85 object classes, and offers 1,550 videos with over 3.87 million frames. Each frame has been manually annotated with a bounding box, making LaSOT the largest densely annotated tracking benchmark to our knowledge. Our aim is to provide a dedicated platform for training and evaluating trackers. The average video length is around 2,500 frames, with various challenges that exist in real-world footage. We have also provided language specifications for each video, allowing for future research to use linguistic features to improve tracking. Two protocols, full-overlap and one-shot, are designated for flexible assessment of trackers. We have evaluated 48 baseline trackers on LaSOT with in-depth analysis, revealing significant room for improvement. The complete benchmark, tracking results, and analysis can be found at http://vision.cs.stonybrook.edu/~lasot/.",1
"Historical Document Processing is the process of digitizing written material from the past for future use by historians and other scholars. It incorporates algorithms and software tools from various subfields of computer science, including computer vision, document analysis and recognition, natural language processing, and machine learning, to convert images of ancient manuscripts, letters, diaries, and early printed texts automatically into a digital format usable in data mining and information retrieval systems. Within the past twenty years, as libraries, museums, and other cultural heritage institutions have scanned an increasing volume of their historical document archives, the need to transcribe the full text from these collections has become acute. Since Historical Document Processing encompasses multiple sub-domains of computer science, knowledge relevant to its purpose is scattered across numerous journals and conference proceedings. This paper surveys the major phases of, standard algorithms, tools, and datasets in the field of Historical Document Processing, discusses the results of a literature review, and finally suggests directions for further research.",0
"The process of converting written material from the past into a digitized format for future use by scholars and historians is known as Historical Document Processing. This involves utilizing computer science subfields such as document analysis and recognition, computer vision, natural language processing, and machine learning to automatically convert images of ancient manuscripts, letters, diaries, and early printed texts into a digital format. With an increasing volume of historical document archives being scanned by libraries, museums, and other cultural heritage institutions in the past two decades, the need to transcribe the full text from these collections has become crucial. However, as Historical Document Processing encompasses multiple sub-domains of computer science, relevant knowledge is scattered across various journals and conference proceedings. This paper examines the major phases, standard algorithms, tools, and datasets in the field of Historical Document Processing, presents the results of a literature review, and proposes directions for further research.",1
"In this paper, we propose a novel method for video moment retrieval (VMR) that achieves state of the arts (SOTA) performance on R@1 metrics and surpassing the SOTA on the high IoU metric (R@1, IoU=0.7).   First, we propose to use a multi-head self-attention mechanism, and further a cross-attention scheme to capture video/query interaction and long-range query dependencies from video context. The attention-based methods can develop frame-to-query interaction and query-to-frame interaction at arbitrary positions and the multi-head setting ensures the sufficient understanding of complicated dependencies. Our model has a simple architecture, which enables faster training and inference while maintaining .   Second, We also propose to use multiple task training objective consists of moment segmentation task, start/end distribution prediction and start/end location regression task. We have verified that start/end prediction are noisy due to annotator disagreement and joint training with moment segmentation task can provide richer information since frames inside the target clip are also utilized as positive training examples.   Third, we propose to use an early fusion approach, which achieves better performance at the cost of inference time. However, the inference time will not be a problem for our model since our model has a simple architecture which enables efficient training and inference.",0
"This paper presents a new approach to video moment retrieval (VMR) that outperforms previous state-of-the-art (SOTA) methods on R@1 metrics and surpasses the current SOTA on the high IoU metric (R@1, IoU=0.7). Our method utilizes a multi-head self-attention mechanism and a cross-attention scheme to capture video/query interaction, allowing for frame-to-query and query-to-frame interaction at any position. The multi-head setting ensures that complex dependencies are fully understood. Our model has a simple architecture, enabling faster training and inference without sacrificing performance. We also propose a multiple task training objective that includes moment segmentation, start/end distribution prediction, and start/end location regression. We have found that joint training with moment segmentation can provide richer information, as there is disagreement among annotators regarding start/end predictions. Finally, we use an early fusion approach, which improves performance at the expense of inference time. However, our simple architecture ensures efficient training and inference, making inference time a non-issue.",1
"We address the challenging task of cross-modal moment retrieval, which aims to localize a temporal segment from an untrimmed video described by a natural language query. It poses great challenges over the proper semantic alignment between vision and linguistic domains. Existing methods independently extract the features of videos and sentences and purely utilize the sentence embedding in the multi-modal fusion stage, which do not make full use of the potential of language. In this paper, we present Language Guided Networks (LGN), a new framework that leverages the sentence embedding to guide the whole process of moment retrieval. In the first feature extraction stage, we propose to jointly learn visual and language features to capture the powerful visual information which can cover the complex semantics in the sentence query. Specifically, the early modulation unit is designed to modulate the visual feature extractor's feature maps by a linguistic embedding. Then we adopt a multi-modal fusion module in the second fusion stage. Finally, to get a precise localizer, the sentence information is utilized to guide the process of predicting temporal positions. Specifically, the late guidance module is developed to linearly transform the output of localization networks via the channel attention mechanism. The experimental results on two popular datasets demonstrate the superior performance of our proposed method on moment retrieval (improving by 5.8\% in terms of Rank1@IoU0.5 on Charades-STA and 5.2\% on TACoS). The source code for the complete system will be publicly available.",0
"Our focus is on the challenging task of cross-modal moment retrieval, which involves identifying a specific section of an untrimmed video based on a natural language query. This task poses significant difficulties in aligning the semantic meanings between visual and linguistic domains. Previous methods have only utilized sentence embeddings during the multi-modal fusion stage, failing to take full advantage of the rich language information available. To address this issue, we introduce a novel framework called Language Guided Networks (LGN), which employs sentence embeddings to guide the moment retrieval process. Our approach involves jointly learning visual and language features in the feature extraction stage, allowing for more robust and accurate semantic representation. We also incorporate a multi-modal fusion module, followed by a late guidance module that uses the sentence information to guide the prediction of temporal positions. Our experiments on two popular datasets demonstrate that our method outperforms existing approaches, achieving an improvement of 5.8\% in terms of Rank1@IoU0.5 on Charades-STA and 5.2\% on TACoS. The complete system's source code will be made publicly available.",1
"Software developers routinely search for code using general-purpose search engines. However, these search engines cannot find code semantically unless it has an accompanying description. We propose a technique for semantic code search: A Convolutional Neural Network approach to code retrieval (CoNCRA). Our technique aims to find the code snippet that most closely matches the developer's intent, expressed in natural language. We evaluated our approach's efficacy on a dataset composed of questions and code snippets collected from Stack Overflow. Our preliminary results showed that our technique, which prioritizes local interactions (words nearby), improved the state-of-the-art (SOTA) by 5% on average, retrieving the most relevant code snippets in the top 3 (three) positions by almost 80% of the time. Therefore, our technique is promising and can improve the efficacy of semantic code retrieval.",0
"Code search using general-purpose search engines is common among software developers, but these engines are unable to search for code semantically without an accompanying description. To address this issue, we propose a semantic code search technique called CoNCRA, which utilizes a Convolutional Neural Network approach to retrieve the code snippet that aligns with the developer's expressed natural language intent. Our technique was evaluated on a dataset of questions and code snippets from Stack Overflow, and preliminary results indicate that it outperforms the state-of-the-art by 5% on average, with 80% of the most relevant code snippets appearing in the top 3 positions due to the technique's emphasis on local interactions. This promising technique has the potential to enhance the efficacy of semantic code retrieval.",1
"The ability to efficiently search for images over an indexed database is the cornerstone for several user experiences. Incorporating user feedback, through multi-modal inputs provide flexible and interaction to serve fine-grained specificity in requirements. We specifically focus on text feedback, through descriptive natural language queries. Given a reference image and textual user feedback, our goal is to retrieve images that satisfy constraints specified by both of these input modalities. The task is challenging as it requires understanding the textual semantics from the text feedback and then applying these changes to the visual representation. To address these challenges, we propose a novel architecture TRACE which contains a hierarchical feature aggregation module to learn the composite visio-linguistic representations. TRACE achieves the SOTA performance on 3 benchmark datasets: FashionIQ, Shoes, and Birds-to-Words, with an average improvement of at least ~5.7%, ~3%, and ~5% respectively in R@K metric. Our extensive experiments and ablation studies show that TRACE consistently outperforms the existing techniques by significant margins both quantitatively and qualitatively.",0
"Efficiently searching for images in an indexed database is crucial for various user experiences. To ensure flexibility and interaction in meeting specific requirements, we incorporate user feedback through multi-modal inputs, with a particular focus on descriptive natural language queries. Our objective is to retrieve images that meet the constraints specified by both reference image and textual feedback. However, this task is challenging as it involves understanding the semantics of textual feedback and applying it to visual representations. To overcome these challenges, we propose a novel architecture called TRACE, which includes a hierarchical feature aggregation module to learn composite visio-linguistic representations. Our approach achieves state-of-the-art performance on three benchmark datasets, namely FashionIQ, Shoes, and Birds-to-Words, with at least an average improvement of 5.7%, 3%, and 5% in R@K metric, respectively. Our extensive experiments and ablation studies demonstrate that TRACE consistently outperforms existing techniques, both quantitatively and qualitatively.",1
"Intensive care clinicians need reliable clinical practice tools to preempt unexpected critical events that might harm their patients in intensive care units (ICU), to pre-plan timely interventions, and to keep the patient's family well informed. The conventional statistical models are built by curating only a limited number of key variables, which means a vast unknown amount of potentially precious data remains unused. Deep learning models (DLMs) can be leveraged to learn from large complex datasets and construct predictive clinical tools. This retrospective study was performed using 42,818 hospital admissions involving 35,348 patients, which is a subset of the MIMIC-III dataset. Natural language processing (NLP) techniques were applied to build DLMs to predict in-hospital mortality (IHM) and length of stay >=7 days (LOS). Over 75 million events across multiple data sources were processed, resulting in over 355 million tokens. DLMs for predicting IHM using data from all sources (AS) and chart data (CS) achieved an AUC-ROC of 0.9178 and 0.9029, respectively, and PR-AUC of 0.6251 and 0.5701, respectively. DLMs for predicting LOS using AS and CS achieved an AUC-ROC of 0.8806 and 0.8642, respectively, and PR-AUC of 0.6821 and 0.6575, respectively. The observed AUC-ROC difference between models was found to be significant for both IHM and LOS at p=0.05. The observed PR-AUC difference between the models was found to be significant for IHM and statistically insignificant for LOS at p=0.05. In this study, deep learning models were constructed using data combined from a variety of sources in Electronic Health Records (EHRs) such as chart data, input and output events, laboratory values, microbiology events, procedures, notes, and prescriptions. It is possible to predict in-hospital mortality with much better confidence and higher reliability from models built using all sources of data.",0
"Clinical practitioners in intensive care units require dependable tools to anticipate unforeseen critical events that could potentially harm their patients, to plan interventions in a timely manner, and to keep the patient's family well-informed. Traditional statistical models only incorporate a limited number of essential variables, leaving a vast amount of potentially valuable data untapped. Deep learning models (DLMs) can learn from complex and extensive datasets to create predictive clinical tools. This study utilized natural language processing (NLP) techniques on a subset of the MIMIC-III dataset consisting of 42,818 hospital admissions involving 35,348 patients. DLMs were constructed to predict in-hospital mortality (IHM) and length of stay (LOS) of at least 7 days. DLMs incorporating data from various sources such as chart data, laboratory values, procedures, notes, and prescriptions were found to be more effective in predicting IHM with a higher degree of reliability (AUC-ROC of 0.9178) compared to traditional models. The observed differences in AUC-ROC and PR-AUC between the DLMs and traditional models were statistically significant for both IHM and LOS.",1
"Deep neural networks for natural language processing tasks are vulnerable to adversarial input perturbations. In this paper, we present a versatile language for programmatically specifying string transformations -- e.g., insertions, deletions, substitutions, swaps, etc. -- that are relevant to the task at hand. We then present an approach to adversarially training models that are robust to such user-defined string transformations. Our approach combines the advantages of search-based techniques for adversarial training with abstraction-based techniques. Specifically, we show how to decompose a set of user-defined string transformations into two component specifications, one that benefits from search and another from abstraction. We use our technique to train models on the AG and SST2 datasets and show that the resulting models are robust to combinations of user-defined transformations mimicking spelling mistakes and other meaning-preserving transformations.",0
"Adversarial input perturbations can compromise the effectiveness of deep neural networks in natural language processing tasks. This paper introduces a flexible language that allows for the programmatic specification of string transformations, including insertions, deletions, substitutions, and swaps, that are pertinent to the task. Furthermore, we propose a method for adversarial training of models that can withstand user-defined string transformations, which combines search-based and abstraction-based techniques. We demonstrate how a set of user-defined string transformations can be broken down into two component specifications, one benefiting from search and the other from abstraction. Our approach is applied to training models on the AG and SST2 datasets, and the results indicate that the models are able to resist combinations of user-defined transformations that simulate spelling errors and other transformations that preserve meaning.",1
"Machine learning methods based on statistical principles have proven highly successful in dealing with a wide variety of data analysis and analytics tasks. Traditional data models are mostly concerned with independent identically distributed data. The recent success of end-to-end modelling scheme using deep neural networks equipped with effective structures such as convolutional layers or skip connections allows the extension to more sophisticated and structured practical data, such as natural language, images, videos, etc. On the application side, vector fields are an extremely useful type of data in empirical sciences, as well as signal processing, e.g. non-parametric transformations of 3D point clouds using 3D vector fields, the modelling of the fluid flow in earth science, and the modelling of physical fields.   This review article is dedicated to recent computational tools of vector fields, including vector data representations, predictive model of spatial data, as well as applications in computer vision, signal processing, and empirical sciences.",0
"Statistical-based machine learning techniques have proven to be highly effective in handling a diverse range of data analysis and analytics tasks. Traditional data models primarily focus on independent and identically distributed data. However, the recent triumph of end-to-end modeling schemes utilizing deep neural networks equipped with advanced structures like convolutional layers or skip connections permits the extension to more complex and organized practical data, such as natural language, images, videos, and so on. In empirical sciences and signal processing, vector fields are a highly valuable data type, enabling non-parametric transformations of 3D point clouds and the modeling of fluid flow in earth science and physical fields. This article is a comprehensive review of recent computational tools for vector fields, encompassing vector data representations, predictive models of spatial data, and applications in computer vision, signal processing, and empirical sciences.",1
"We propose a cross-modality manifold alignment procedure that leverages triplet loss to jointly learn consistent, multi-modal embeddings of language-based concepts of real-world items. Our approach learns these embeddings by sampling triples of anchor, positive, and negative data points from RGB-depth images and their natural language descriptions. We show that our approach can benefit from, but does not require, post-processing steps such as Procrustes analysis, in contrast to some of our baselines which require it for reasonable performance. We demonstrate the effectiveness of our approach on two datasets commonly used to develop robotic-based grounded language learning systems, where our approach outperforms four baselines, including a state-of-the-art approach, across five evaluation metrics.",0
"Our proposed method employs triplet loss to achieve cross-modality manifold alignment, allowing for the simultaneous learning of consistent, multi-modal embeddings of language-based concepts for real-world objects. The embeddings are learned through the selection of anchor, positive, and negative data points from RGB-depth images and their corresponding natural language descriptions. Unlike some of our baselines, our approach does not necessitate post-processing steps like Procrustes analysis for effective performance. We evaluate our approach on two datasets commonly used in the development of grounded language learning systems for robotics, and it outperforms four baselines, including a state-of-the-art approach, across five evaluation metrics.",1
"An infrastructure for multisite, geographically-distributed creation and collection of diverse, high-quality, curated and labeled radiology image data is crucial for the successful automated development, deployment, monitoring and continuous improvement of Artificial Intelligence (AI)/Machine Learning (ML) solutions in the real world. An interactive radiology reporting approach that integrates image viewing, dictation, natural language processing (NLP) and creation of hyperlinks between image findings and the report, provides localized labels during routine interpretation. These images and labels can be captured and centralized in a cloud-based system. This method provides a practical and efficient mechanism with which to monitor algorithm performance. It also supplies feedback for iterative development and quality improvement of new and existing algorithmic models. Both feedback and monitoring are achieved without burdening the radiologist. The method addresses proposed regulatory requirements for post-marketing surveillance and external data. Comprehensive multi-site data collection assists in reducing bias. Resource requirements are greatly reduced compared to dedicated retrospective expert labeling.",0
"To successfully develop, deploy, and improve Artificial Intelligence/Machine Learning solutions for radiology, it is essential to establish a multisite infrastructure that enables the creation and collection of high-quality, curated, and labeled radiology image data. An interactive radiology reporting system that integrates image viewing, dictation, natural language processing, and the creation of hyperlinks between image findings and reports can provide localized labels during routine interpretation. This system can capture and centralize images and labels in a cloud-based system, providing an efficient mechanism for monitoring algorithm performance and providing feedback for iterative development and quality improvement without burdening radiologists. This method also addresses proposed regulatory requirements for post-marketing surveillance and external data, and comprehensive multi-site data collection helps reduce bias. Compared to dedicated retrospective expert labeling, resource requirements are greatly reduced.",1
"Large scale analysis of source code, and in particular scientific source code, holds the promise of better understanding the data science process, identifying analytical best practices, and providing insights to the builders of scientific toolkits. However, large corpora have remained unanalyzed in depth, as descriptive labels are absent and require expert domain knowledge to generate. We propose a novel weakly supervised transformer-based architecture for computing joint representations of code from both abstract syntax trees and surrounding natural language comments. We then evaluate the model on a new classification task for labeling computational notebook cells as stages in the data analysis process from data import to wrangling, exploration, modeling, and evaluation. We show that our model, leveraging only easily-available weak supervision, achieves a 38% increase in accuracy over expert-supplied heuristics and outperforms a suite of baselines. Our model enables us to examine a set of 118,000 Jupyter Notebooks to uncover common data analysis patterns. Focusing on notebooks with relationships to academic articles, we conduct the largest ever study of scientific code and find that notebook composition correlates with the citation count of corresponding papers.",0
"An extensive analysis of source code, specifically scientific source code, has the potential to enhance comprehension of the data science process, recognize the best analytical practices, and provide helpful insights for those who develop scientific toolkits. However, due to the lack of descriptive labels, large collections of code have not been thoroughly analyzed and necessitate expert domain knowledge to generate such labels. To address this issue, we propose a unique transformer-based architecture that uses weak supervision to compute joint representations of code from both abstract syntax trees and natural language comments in the surrounding context. Our model is evaluated on a novel classification task that involves labeling computational notebook cells as stages in the data analysis process, ranging from data import to wrangling, exploration, modeling, and evaluation. The results demonstrate that our model, which only utilizes easily-available weak supervision, surpasses a suite of baselines and achieves a 38% increase in accuracy compared to expert-supplied heuristics. Our model allows us to investigate a set of 118,000 Jupyter Notebooks to identify common data analysis patterns. By focusing on notebooks that are related to academic articles, we conduct the largest-ever study of scientific code and discover that notebook composition is correlated with the citation count of corresponding papers.",1
"This paper is a note on new directions and methodologies for validation and explanation of Machine Learning (ML) models employed for retail credit scoring in finance. Our proposed framework draws motivation from the field of Artificial Intelligence (AI) security and adversarial ML where the need for certifying the performance of the ML algorithms in the face of their overwhelming complexity poses a need for rethinking the traditional notions of model architecture selection, sensitivity analysis and stress testing. Our point of view is that the phenomenon of adversarial perturbations when detached from the AI security domain, has purely algorithmic roots and fall within the scope of model risk assessment. We propose a model criticism and explanation framework based on adversarially generated counterfactual examples for tabular data. A counterfactual example to a given instance in this context is defined as a synthetically generated data point sampled from the estimated data distribution which is treated differently by a model. The counterfactual examples can be used to provide a black-box instance-level explanation of the model behaviour as well as studying the regions in the input space where the model performance deteriorates. Adversarial example generating algorithms are extensively studied in the image and natural language processing (NLP) domains. However, most financial data come in tabular format and naive application of the existing techniques on this class of datasets generates unrealistic samples. In this paper, we propose a counterfactual example generation method capable of handling tabular data including discrete and categorical variables. Our proposed algorithm uses a gradient-free optimization based on genetic algorithms and therefore is applicable to any classification model.",0
"This paper discusses innovative approaches and techniques for validating and explaining Machine Learning (ML) models used for retail credit scoring in finance. Our suggested framework is inspired by Artificial Intelligence (AI) security and adversarial ML, which emphasizes the importance of certifying the performance of ML algorithms in the face of their complex nature. We believe that the issue of adversarial perturbations, which is often associated with AI security, is rooted in algorithmic principles and falls under the category of model risk assessment. To address this issue, we propose a model criticism and explanation framework that utilizes adversarially generated counterfactual examples for tabular data. These examples are synthetic data points sampled from the estimated data distribution that are treated differently by the model and can be used to explain the model's behavior and identify areas where its performance deteriorates. While adversarial example generating algorithms are well-studied in the image and NLP domains, they are not suitable for most financial data, which comes in tabular format. Therefore, we propose a counterfactual example generation method that can handle tabular data, including discrete and categorical variables. Our algorithm uses a gradient-free optimization based on genetic algorithms, making it applicable to any classification model.",1
"Online display advertising is growing rapidly in recent years thanks to the automation of the ad buying process. Real-time bidding (RTB) allows the automated trading of ad impressions between advertisers and publishers through real-time auctions. In order to increase the effectiveness of their campaigns, advertisers should deliver ads to the users who are highly likely to be converted (i.e., purchase, registration, website visit, etc.) in the near future. In this study, we introduce and examine different models for estimating the probability of a user converting, given their history of visited URLs. Inspired by natural language processing, we introduce three URL embedding models to compute semantically meaningful URL representations. To demonstrate the effectiveness of the different proposed representation and conversion prediction models, we have conducted experiments on real logged events collected from an advertising platform.",0
"The automation of the ad buying process has led to a rapid growth of online display advertising in recent years. Real-time bidding (RTB) enables advertisers and publishers to trade ad impressions through real-time auctions. To achieve a more effective campaign, advertisers should target users who are highly likely to convert (i.e., purchase, register, visit a website, etc.) in the near future. This study explores various models for estimating a user's probability of converting based on their history of visited URLs. Three URL embedding models, inspired by natural language processing, are introduced to compute semantically meaningful URL representations. Real logged events from an advertising platform are used to demonstrate the effectiveness of the different proposed representation and conversion prediction models through experiments.",1
"In the era of big data, a large number of text data generated by the Internet has given birth to a variety of text representation methods. In natural language processing (NLP), text representation transforms text into vectors that can be processed by computer without losing the original semantic information. However, these methods are difficult to effectively extract the semantic features among words and distinguish polysemy in language. Therefore, a text feature representation model based on convolutional neural network (CNN) and variational autoencoder (VAE) is proposed to extract the text features and apply the obtained text feature representation on the text classification tasks. CNN is used to extract the features of text vector to get the semantics among words and VAE is introduced to make the text feature space more consistent with Gaussian distribution. In addition, the output of the improved word2vec model is employed as the input of the proposed model to distinguish different meanings of the same word in different contexts. The experimental results show that the proposed model outperforms in k-nearest neighbor (KNN), random forest (RF) and support vector machine (SVM) classification algorithms.",0
"The Internet's production of vast amounts of text data has led to the development of various methods for representing text in the age of big data. Text representation is crucial in natural language processing (NLP) as it transforms text into vectors for computer processing without losing semantic information. However, current methods face challenges in effectively extracting semantic features and distinguishing polysemy in language. To address these issues, a text feature representation model that combines convolutional neural network (CNN) and variational autoencoder (VAE) is proposed. This model extracts text features and applies them to text classification tasks. CNN is utilized to extract semantic features from the text vector, while VAE ensures consistency with Gaussian distribution in the text feature space. Additionally, the model incorporates an improved word2vec model to differentiate the meanings of the same word in different contexts. Results show that the proposed model outperforms KNN, RF, and SVM classification algorithms.",1
"Dialog State Tracking (DST) is one of the most crucial modules for goal-oriented dialogue systems. In this paper, we introduce FastSGT (Fast Schema Guided Tracker), a fast and robust BERT-based model for state tracking in goal-oriented dialogue systems. The proposed model is designed for the Schema-Guided Dialogue (SGD) dataset which contains natural language descriptions for all the entities including user intents, services, and slots. The model incorporates two carry-over procedures for handling the extraction of the values not explicitly mentioned in the current user utterance. It also uses multi-head attention projections in some of the decoders to have a better modelling of the encoder outputs. In the conducted experiments we compared FastSGT to the baseline model for the SGD dataset. Our model keeps the efficiency in terms of computational and memory consumption while improving the accuracy significantly. Additionally, we present ablation studies measuring the impact of different parts of the model on its performance. We also show the effectiveness of data augmentation for improving the accuracy without increasing the amount of computational resources.",0
"The tracking of Dialog State (DST) is a vital component for dialogue systems with specific objectives. This study proposes the use of FastSGT (Fast Schema Guided Tracker), a BERT-based model, that is both rapid and robust for state tracking in goal-oriented dialogue systems. The study focuses on the Schema-Guided Dialogue (SGD) dataset, which describes natural language entities such as user intents, services, and slots. The model incorporates two carry-over procedures to handle the extraction of values that are not explicitly mentioned in the current user utterance. It also employs multi-head attention projections in some of the decoders to better model the encoder outputs. The study compares FastSGT to the baseline model for the SGD dataset and demonstrates that the proposed model improves accuracy significantly while maintaining computational and memory efficiency. Furthermore, the study conducts ablation studies to evaluate the impact of different aspects of the model on its performance while demonstrating the effectiveness of data augmentation in enhancing accuracy without increasing computational resources.",1
"We introduce the task of Image-Set Visual Question Answering (ISVQA), which generalizes the commonly studied single-image VQA problem to multi-image settings. Taking a natural language question and a set of images as input, it aims to answer the question based on the content of the images. The questions can be about objects and relationships in one or more images or about the entire scene depicted by the image set. To enable research in this new topic, we introduce two ISVQA datasets - indoor and outdoor scenes. They simulate the real-world scenarios of indoor image collections and multiple car-mounted cameras, respectively. The indoor-scene dataset contains 91,479 human annotated questions for 48,138 image sets, and the outdoor-scene dataset has 49,617 questions for 12,746 image sets. We analyze the properties of the two datasets, including question-and-answer distributions, types of questions, biases in dataset, and question-image dependencies. We also build new baseline models to investigate new research challenges in ISVQA.",0
"Our focus is on Image-Set Visual Question Answering (ISVQA), which extends the single-image VQA problem to encompass multiple images. The task involves answering a natural language question with reference to a set of images. The questions may relate to objects and relationships within one or more images or the entire scene depicted by the set. To facilitate research in this area, we present two ISVQA datasets, one for indoor scenes and one for outdoor scenes. These datasets simulate real-world scenarios of indoor image collections and multiple car-mounted cameras, respectively. The indoor-scene dataset comprises 91,479 questions, which are human-annotated for 48,138 image sets. The outdoor-scene dataset includes 49,617 questions for 12,746 image sets. We examine various properties of these datasets, including question-and-answer distributions, question types, dataset biases, and question-image dependencies. Additionally, we develop new baseline models to investigate emerging research challenges in ISVQA.",1
"Video Moment Retrieval (VMR) is a task to localize the temporal moment in untrimmed video specified by natural language query. For VMR, several methods that require full supervision for training have been proposed. Unfortunately, acquiring a large number of training videos with labeled temporal boundaries for each query is a labor-intensive process. This paper explores methods for performing VMR in a weakly-supervised manner (wVMR): training is performed without temporal moment labels but only with the text query that describes a segment of the video. Existing methods on wVMR generate multi-scale proposals and apply query-guided attention mechanisms to highlight the most relevant proposal. To leverage the weak supervision, contrastive learning is used which predicts higher scores for the correct video-query pairs than for the incorrect pairs. It has been observed that a large number of candidate proposals, coarse query representation, and one-way attention mechanism lead to blurry attention maps which limit the localization performance. To handle this issue, Video-Language Alignment Network (VLANet) is proposed that learns sharper attention by pruning out spurious candidate proposals and applying a multi-directional attention mechanism with fine-grained query representation. The Surrogate Proposal Selection module selects a proposal based on the proximity to the query in the joint embedding space, and thus substantially reduces candidate proposals which leads to lower computation load and sharper attention. Next, the Cascaded Cross-modal Attention module considers dense feature interactions and multi-directional attention flow to learn the multi-modal alignment. VLANet is trained end-to-end using contrastive loss which enforces semantically similar videos and queries to gather. The experiments show that the method achieves state-of-the-art performance on Charades-STA and DiDeMo datasets.",0
"The task of Video Moment Retrieval (VMR) involves identifying a specific moment in an untrimmed video based on a natural language query. Various methods for VMR that require complete supervision during training have been proposed, but acquiring a large number of training videos with labeled temporal boundaries for each query is a time-consuming process. This study explores methods for performing VMR in a weakly-supervised manner (wVMR), which involves training without temporal moment labels but only with the text query that describes a segment of the video. Existing wVMR approaches generate multi-scale proposals and use query-guided attention mechanisms to highlight the most relevant proposal. To improve weak supervision, contrastive learning is used to predict higher scores for correct video-query pairs than for incorrect ones. However, a large number of candidate proposals, coarse query representation, and one-way attention mechanism lead to blurry attention maps, which limit localization performance. To address this issue, the proposed Video-Language Alignment Network (VLANet) learns sharper attention by pruning out spurious candidate proposals and applying a multi-directional attention mechanism with fine-grained query representation. The Surrogate Proposal Selection module selects a proposal based on the proximity to the query in the joint embedding space, which substantially reduces candidate proposals and leads to lower computation load and sharper attention. The Cascaded Cross-modal Attention module considers dense feature interactions and multi-directional attention flow to learn the multi-modal alignment. VLANet is trained end-to-end using contrastive loss, which ensures that semantically similar videos and queries are grouped together. The experiments demonstrate that the method achieves state-of-the-art performance on Charades-STA and DiDeMo datasets.",1
"With the arising concerns for the AI systems provided with direct access to abundant sensitive information, researchers seek to develop more reliable AI with implicit information sources. To this end, in this paper, we introduce a new task called video description via two multi-modal cooperative dialog agents, whose ultimate goal is for one conversational agent to describe an unseen video based on the dialog and two static frames. Specifically, one of the intelligent agents - Q-BOT - is given two static frames from the beginning and the end of the video, as well as a finite number of opportunities to ask relevant natural language questions before describing the unseen video. A-BOT, the other agent who has already seen the entire video, assists Q-BOT to accomplish the goal by providing answers to those questions. We propose a QA-Cooperative Network with a dynamic dialog history update learning mechanism to transfer knowledge from A-BOT to Q-BOT, thus helping Q-BOT to better describe the video. Extensive experiments demonstrate that Q-BOT can effectively learn to describe an unseen video by the proposed model and the cooperative learning method, achieving the promising performance where Q-BOT is given the full ground truth history dialog.",0
"Researchers are concerned about AI systems having unrestricted access to sensitive information. As a result, they are striving to develop more trustworthy AI that employs implicit information sources. This paper introduces a new task, video description via two multi-modal cooperative dialog agents. The objective is for one agent to describe an unseen video based on dialog and two static frames. Q-BOT, one of the agents, is given two static frames at the beginning and end of the video. It can ask relevant natural language questions before describing the unseen video. A-BOT, the other agent, has already viewed the entire video and assists Q-BOT by providing answers to its questions. The authors propose a QA-Cooperative Network with a dynamic dialog history update learning mechanism to transfer knowledge from A-BOT to Q-BOT. This helps Q-BOT describe the video more effectively. Extensive experiments demonstrate the efficacy of the proposed model and cooperative learning method, achieving promising performance when Q-BOT is provided with the full ground truth history dialog.",1
"Significant advances have been made in recent years on Natural Language Processing with machines surpassing human performance in many tasks, including but not limited to Question Answering. The majority of deep learning methods for Question Answering targets domains with large datasets and highly matured literature. The area of Nuclear and Atomic energy has largely remained unexplored in exploiting non-annotated data for driving industry viable applications. Due to lack of dataset, a new dataset was created from the 7000 research papers on nuclear domain. This paper contributes to research in understanding nuclear domain knowledge which is then evaluated on Nuclear Question Answering Dataset (NQuAD) created by nuclear domain experts as part of this research. NQuAD contains 612 questions developed on 181 paragraphs randomly selected from the IGCAR research paper corpus. In this paper, the Nuclear Bidirectional Encoder Representational Transformers (NukeBERT) is proposed, which incorporates a novel technique for building BERT vocabulary to make it suitable for tasks with less training data. The experiments evaluated on NQuAD revealed that NukeBERT was able to outperform BERT significantly, thus validating the adopted methodology. Training NukeBERT is computationally expensive and hence we will be open-sourcing the NukeBERT pretrained weights and NQuAD for fostering further research work in the nuclear domain.",0
"Recent years have witnessed significant progress in Natural Language Processing, with machines exceeding human performance in many tasks, including Question Answering. Most deep learning methods for Question Answering concentrate on domains with plentiful data and developed literature. However, the Nuclear and Atomic energy field has not been fully explored for driving industry viable applications due to the lack of datasets. Consequently, a dataset was created using 7000 research papers on nuclear domain. This paper aims to contribute to research on nuclear domain knowledge, evaluated on Nuclear Question Answering Dataset (NQuAD) created by nuclear domain experts as part of this research. NQuAD contains 612 questions developed from 181 randomly selected paragraphs of the IGCAR research paper corpus. The Nuclear Bidirectional Encoder Representational Transformers (NukeBERT) is proposed, which incorporates a novel technique to build BERT vocabulary for tasks with less training data. Experiments evaluated on NQuAD revealed NukeBERT significantly outperformed BERT, validating the adopted methodology. NukeBERT training is computationally costly, thus the NukeBERT pretrained weights and NQuAD will be open-sourced to encourage further research in the nuclear domain.",1
"Today, the dominant paradigm for training neural networks involves minimizing task loss on a large dataset. Using world knowledge to inform a model, and yet retain the ability to perform end-to-end training remains an open question. In this paper, we present a novel framework for introducing declarative knowledge to neural network architectures in order to guide training and prediction. Our framework systematically compiles logical statements into computation graphs that augment a neural network without extra learnable parameters or manual redesign. We evaluate our modeling strategy on three tasks: machine comprehension, natural language inference, and text chunking. Our experiments show that knowledge-augmented networks can strongly improve over baselines, especially in low-data regimes.",0
"Presently, the prevailing approach to training neural networks involves minimizing task loss on a vast dataset. It is unclear how to incorporate world knowledge into a model while maintaining the capacity for end-to-end training. Our study introduces an innovative framework for integrating declarative knowledge into neural network architectures to guide training and prediction. Our framework assimilates logical statements into computation graphs that enhance a neural network without requiring extra learnable parameters or manual redesign. We assess our modeling technique using three tasks: machine comprehension, natural language inference, and text chunking. Our experiments demonstrate that knowledge-augmented networks can significantly outperform baselines, particularly in settings with limited data.",1
"Many recent advances in neural information retrieval models, which predict top-K items given a query, learn directly from a large training set of (query, item) pairs. However, they are often insufficient when there are many previously unseen (query, item) combinations, often referred to as the cold start problem. Furthermore, the search system can be biased towards items that are frequently shown to a query previously, also known as the 'rich get richer' (a.k.a. feedback loop) problem. In light of these problems, we observed that most online content platforms have both a search and a recommender system that, while having heterogeneous input spaces, can be connected through their common output item space and a shared semantic representation. In this paper, we propose a new Zero-Shot Heterogeneous Transfer Learning framework that transfers learned knowledge from the recommender system component to improve the search component of a content platform. First, it learns representations of items and their natural-language features by predicting (item, item) correlation graphs derived from the recommender system as an auxiliary task. Then, the learned representations are transferred to solve the target search retrieval task, performing query-to-item prediction without having seen any (query, item) pairs in training. We conduct online and offline experiments on one of the world's largest search and recommender systems from Google, and present the results and lessons learned. We demonstrate that the proposed approach can achieve high performance on offline search retrieval tasks, and more importantly, achieved significant improvements on relevance and user interactions over the highly-optimized production system in online experiments.",0
"Neural information retrieval models have made significant progress in predicting top-K items from a query using a large training set of (query, item) pairs. However, these models may not be sufficient when faced with many unfamiliar (query, item) combinations, which is known as the cold start problem. Additionally, the search system may be biased toward items that have been shown frequently to a query, which is also called the 'rich get richer' problem. To address these issues, many online content platforms have both a search and a recommender system that can be connected through their common output item space and a shared semantic representation. This paper proposes a Zero-Shot Heterogeneous Transfer Learning framework that transfers knowledge from the recommender system to improve the search component of a content platform. The framework learns item representations and natural-language features by predicting (item, item) correlation graphs derived from the recommender system. These learned representations are then transferred to solve the target search retrieval task without having seen any (query, item) pairs in training. The proposed approach achieves high performance on offline search retrieval tasks and significant improvements on relevance and user interactions in online experiments on one of the world's largest search and recommender systems from Google.",1
"Given a natural language query, teaching machines to ask clarifying questions is of immense utility in practical natural language processing systems. Such interactions could help in filling information gaps for better machine comprehension of the query. For the task of ranking clarification questions, we hypothesize that determining whether a clarification question pertains to a missing entry in a given post (on QA forums such as StackExchange) could be considered as a special case of Natural Language Inference (NLI), where both the post and the most relevant clarification question point to a shared latent piece of information or context. We validate this hypothesis by incorporating representations from a Siamese BERT model fine-tuned on NLI and Multi-NLI datasets into our models and demonstrate that our best performing model obtains a relative performance improvement of 40 percent and 60 percent respectively (on the key metric of Precision@1), over the state-of-the-art baseline(s) on the two evaluation sets of the StackExchange dataset, thereby, significantly surpassing the state-of-the-art.",0
"In practical natural language processing systems, it would be highly beneficial to teach machines how to ask clarifying questions when presented with a natural language query. This could help bridge any gaps in information and improve the machine's understanding of the query. We propose that ranking clarification questions could be seen as a special case of Natural Language Inference (NLI), where the missing information is a shared context between the post and the relevant clarification question on QA forums like StackExchange. To test this hypothesis, we integrated representations from a Siamese BERT model fine-tuned on NLI and Multi-NLI datasets into our models. Our best performing model showed a relative performance improvement of 40 percent and 60 percent respectively (on the key metric of Precision@1) compared to the state-of-the-art baseline(s) on the two evaluation sets of the StackExchange dataset, demonstrating significant progress.",1
"Transformer-based models have achieved stateof-the-art results in many tasks in natural language processing. However, such models are usually slow at inference time, making deployment difficult. In this paper, we develop an efficient algorithm to search for fast models while maintaining model quality. We describe a novel approach to decompose the Transformer architecture into smaller components, and propose a sampling-based one-shot architecture search method to find an optimal model for inference. The model search process is more efficient than alternatives, adding only a small overhead to training time. By applying our methods to BERT-base architectures, we achieve 10% to 30% speedup for pre-trained BERT and 70% speedup on top of a previous state-of-the-art distilled BERT model on Cloud TPU-v2 with a generally acceptable drop in performance.",0
"Numerous tasks in natural language processing have been dominated by Transformer-based models, however, these models tend to be slow during inference which can make deployment challenging. This study presents an innovative algorithm to discover fast models without compromising on model quality. Our approach involves breaking down the Transformer architecture into smaller components and using a sampling-based one-shot architecture search method to determine the best model for inference. Compared to other alternatives, our model search process is more efficient and only slightly increases training time. Our method was tested on BERT-base architectures resulting in a 10% to 30% speed boost for pre-trained BERT and a 70% improvement over a previous state-of-the-art distilled BERT model on Cloud TPU-v2, with minimal performance loss.",1
"Cross-domain alignment between image objects and text sequences is key to many visual-language tasks, and it poses a fundamental challenge to both computer vision and natural language processing. This paper investigates a novel approach for the identification and optimization of fine-grained semantic similarities between image and text entities, under a weakly-supervised setup, improving performance over state-of-the-art solutions. Our method builds upon recent advances in optimal transport (OT) to resolve the cross-domain matching problem in a principled manner. Formulated as a drop-in regularizer, the proposed OT solution can be efficiently computed and used in combination with other existing approaches. We present empirical evidence to demonstrate the effectiveness of our approach, showing how it enables simpler model architectures to outperform or be comparable with more sophisticated designs on a range of vision-language tasks.",0
"The alignment of image objects and text sequences across different domains is crucial for various visual-language tasks, and it poses a significant challenge for both computer vision and natural language processing. This study explores a new method for identifying and optimizing fine-grained semantic similarities between image and text entities, using weakly-supervised techniques to improve performance beyond existing solutions. Our approach leverages recent advances in optimal transport (OT) to tackle the cross-domain matching problem with a principled approach. The proposed OT solution, which can be easily computed and combined with other methods, is formulated as a drop-in regularizer. Empirical evidence supports the effectiveness of our method, demonstrating how it enables simpler model architectures to outperform or perform comparably to more complex designs across various vision-language tasks.",1
"Symbolic regression is a powerful technique that can discover analytical equations that describe data, which can lead to explainable models and generalizability outside of the training data set. In contrast, neural networks have achieved amazing levels of accuracy on image recognition and natural language processing tasks, but are often seen as black-box models that are difficult to interpret and typically extrapolate poorly. Here we use a neural network-based architecture for symbolic regression called the Equation Learner (EQL) network and integrate it with other deep learning architectures such that the whole system can be trained end-to-end through backpropagation. To demonstrate the power of such systems, we study their performance on several substantially different tasks. First, we show that the neural network can perform symbolic regression and learn the form of several functions. Next, we present an MNIST arithmetic task where a separate part of the neural network extracts the digits. Finally, we demonstrate prediction of dynamical systems where an unknown parameter is extracted through an encoder. We find that the EQL-based architecture can extrapolate quite well outside of the training data set compared to a standard neural network-based architecture, paving the way for deep learning to be applied in scientific exploration and discovery.",0
"Symbolic regression is a potent method that can detect analytical equations for data, enabling the development of models that are easily explicable and generalizable beyond the training data. In contrast, although neural networks have achieved remarkable accuracy in image recognition and natural language processing, they are frequently regarded as black-box models that are challenging to interpret and have limited potential for extrapolation. We have utilized an Equation Learner (EQL) network, which is a neural network-based architecture for symbolic regression, and combined it with other deep learning architectures for end-to-end training through backpropagation. To illustrate the efficacy of these systems, we have assessed their performance on several distinct tasks, including symbolic regression, an MNIST arithmetic task, and prediction of dynamical systems. Our findings indicate that the EQL-based architecture has superior extrapolation capabilities outside of the training data set when compared to a standard neural network-based architecture, which opens up exciting possibilities for the application of deep learning in scientific exploration and discovery.",1
"Identifying common patterns among events is a key ability in human and machine perception, as it underlies intelligent decision making. We propose an approach for learning semantic relational set abstractions on videos, inspired by human learning. We combine visual features with natural language supervision to generate high-level representations of similarities across a set of videos. This allows our model to perform cognitive tasks such as set abstraction (which general concept is in common among a set of videos?), set completion (which new video goes well with the set?), and odd one out detection (which video does not belong to the set?). Experiments on two video benchmarks, Kinetics and Multi-Moments in Time, show that robust and versatile representations emerge when learning to recognize commonalities among sets. We compare our model to several baseline algorithms and show that significant improvements result from explicitly learning relational abstractions with semantic supervision.",0
"The ability to identify patterns in events is crucial for intelligent decision making in both humans and machines. Drawing inspiration from human learning, we propose a method for acquiring semantic relational set abstractions from videos. By combining visual features with natural language guidance, we obtain high-level representations that capture similarities across a collection of videos. This approach enables our model to perform various cognitive tasks, such as identifying the common concept shared by a set of videos, predicting which new video would fit well with the set, and detecting the odd one out. Our experiments on Kinetics and Multi-Moments in Time video datasets demonstrate the emergence of robust and adaptable representations when learning to recognize commonalities among sets. Furthermore, we compare our model to various baseline algorithms and show that explicit learning of relational abstractions with semantic supervision yields significant improvements.",1
"Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning. Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the deep learning is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide ""obviously"" interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that (1) clinicians and practitioners can subsequently approach these methods with caution, (2) insights into interpretability will be born with more considerations for medical practices, and (3) initiatives to push forward data-based, mathematically- and technically-grounded medical education are encouraged.",0
"In recent times, artificial intelligence and machine learning have proven to be highly effective in various tasks, such as natural language processing and image processing, particularly with the introduction of deep learning. These advancements have enabled their integration into various fields and disciplines, including those that demand accountability and transparency, such as the medical sector. To ensure reliability, there is a need for explanations on machine decisions and predictions, which requires greater interpretability. However, the opaque nature of deep learning remains unresolved, and many machine decisions are still not well understood. This review categorizes different interpretability approaches suggested by various research works, ranging from those that provide clear and straightforward information to those that study complex patterns. By utilizing the same categorization in medical research, it is hoped that clinicians and practitioners will approach these methods with caution, new insights into interpretability will emerge, and there will be a push towards data-based, mathematically- and technically-grounded medical education.",1
"Manipulating visual attributes of images through human-written text is a very challenging task. On the one hand, models have to learn the manipulation without the ground truth of the desired output. On the other hand, models have to deal with the inherent ambiguity of natural language. Previous research usually requires either the user to describe all the characteristics of the desired image or to use richly-annotated image captioning datasets. In this work, we propose a novel unsupervised approach, based on image-to-image translation, that alters the attributes of a given image through a command-like sentence such as ""change the hair color to black"". Contrarily to state-of-the-art approaches, our model does not require a human-annotated dataset nor a textual description of all the attributes of the desired image, but only those that have to be modified. Our proposed model disentangles the image content from the visual attributes, and it learns to modify the latter using the textual description, before generating a new image from the content and the modified attribute representation. Because text might be inherently ambiguous (blond hair may refer to different shadows of blond, e.g. golden, icy, sandy), our method generates multiple stochastic versions of the same translation. Experiments show that the proposed model achieves promising performances on two large-scale public datasets: CelebA and CUB. We believe our approach will pave the way to new avenues of research combining textual and speech commands with visual attributes.",0
"Altering visual attributes of images using human-written text is a difficult task, as models face the challenge of learning manipulation without knowledge of the desired output and the inherent ambiguity of natural language. Previous research has typically required users to describe all desired image characteristics or use richly-annotated image captioning datasets. Our work proposes a novel unsupervised approach based on image-to-image translation that modifies image attributes via a command-like sentence, without requiring human-annotated data or description of all attributes. Our model disentangles image content from visual attributes and learns to modify the latter using textual description before generating a new image with the modified attribute representation. Due to text ambiguity, our method generates multiple stochastic versions of each translation. Experiments show promising performance on CelebA and CUB datasets, opening new avenues for research combining textual and speech commands with visual attributes.",1
"We addressed the challenging task of video question answering, which requires machines to answer questions about videos in a natural language form. Previous state-of-the-art methods attempt to apply spatio-temporal attention mechanism on video frame features without explicitly modeling the location and relations among object interaction occurred in videos. However, the relations between object interaction and their location information are very critical for both action recognition and question reasoning. In this work, we propose to represent the contents in the video as a location-aware graph by incorporating the location information of an object into the graph construction. Here, each node is associated with an object represented by its appearance and location features. Based on the constructed graph, we propose to use graph convolution to infer both the category and temporal locations of an action. As the graph is built on objects, our method is able to focus on the foreground action contents for better video question answering. Lastly, we leverage an attention mechanism to combine the output of graph convolution and encoded question features for final answer reasoning. Extensive experiments demonstrate the effectiveness of the proposed methods. Specifically, our method significantly outperforms state-of-the-art methods on TGIF-QA, Youtube2Text-QA, and MSVD-QA datasets. Code and pre-trained models are publicly available at: https://github.com/SunDoge/L-GCN",0
"Our focus was on video question answering, which involves machines answering questions about videos using natural language. Existing methods use spatio-temporal attention mechanisms on video frame features but do not consider the location and relationships among objects in the video. Object interaction and their location are crucial for action recognition and question reasoning. To address this, we propose a location-aware graph that incorporates object location information during construction. Each node represents an object with appearance and location features. We use graph convolution to infer category and temporal locations of actions. This allows our method to focus on foreground action contents for better video question answering. Finally, we use an attention mechanism to combine the graph convolution output and encoded question features for final answer reasoning. Our method significantly outperforms state-of-the-art on TGIF-QA, Youtube2Text-QA, and MSVD-QA datasets. Code and pre-trained models are available at: https://github.com/SunDoge/L-GCN",1
"Zero-shot learning (ZSL) is commonly used to address the very pervasive problem of predicting unseen classes in fine-grained image classification and other tasks. One family of solutions is to learn synthesised unseen visual samples produced by generative models from auxiliary semantic information, such as natural language descriptions. However, for most of these models, performance suffers from noise in the form of irrelevant image backgrounds. Further, most methods do not allocate a calculated weight to each semantic patch. Yet, in the real world, the discriminative power of features can be quantified and directly leveraged to improve accuracy and reduce computational complexity. To address these issues, we propose a novel framework called multi-patch generative adversarial nets (MPGAN) that synthesises local patch features and labels unseen classes with a novel weighted voting strategy. The process begins by generating discriminative visual features from noisy text descriptions for a set of predefined local patches using multiple specialist generative models. The features synthesised from each patch for unseen classes are then used to construct an ensemble of diverse supervised classifiers, each corresponding to one local patch. A voting strategy averages the probability distributions output from the classifiers and, given that some patches are more discriminative than others, a discrimination-based attention mechanism helps to weight each patch accordingly. Extensive experiments show that MPGAN has significantly greater accuracy than state-of-the-art methods.",0
"To tackle the challenge of predicting unseen classes in fine-grained image classification and other tasks, Zero-shot learning (ZSL) is frequently employed. One approach involves generating synthetic visual samples for unobserved classes using generative models and auxiliary semantic information like natural language descriptions. However, the models often suffer from irrelevant image backgrounds, leading to poor performance. Moreover, most methods do not assign weights to each semantic patch, although quantifying the discriminative power of features can enhance accuracy and reduce computational complexity. Our proposed solution, multi-patch generative adversarial nets (MPGAN), addresses these issues by synthesizing local patch features and using a novel weighted voting strategy to label the unseen classes. We first generate discriminative visual features for a set of predefined local patches using multiple specialist generative models and noisy text descriptions. Then, we construct an ensemble of diverse supervised classifiers, each corresponding to a local patch, and use the synthesized features from each patch to label the unseen classes. A discrimination-based attention mechanism is used to weight each patch according to its discriminative power. Extensive experiments demonstrate that MPGAN outperforms state-of-the-art methods significantly.",1
"Most existing text-to-image synthesis tasks are static single-turn generation, based on pre-defined textual descriptions of images. To explore more practical and interactive real-life applications, we introduce a new task - Interactive Image Editing, where users can guide an agent to edit images via multi-turn textual commands on-the-fly. In each session, the agent takes a natural language description from the user as the input and modifies the image generated in the previous turn to a new design, following the user description. The main challenges in this sequential and interactive image generation task are two-fold: 1) contextual consistency between a generated image and the provided textual description; 2) step-by-step region-level modification to maintain visual consistency across the generated image sequence in each session. To address these challenges, we propose a novel Sequential Attention Generative Adversarial Net-work (SeqAttnGAN), which applies a neural state tracker to encode the previous image and the textual description in each turn of the sequence, and uses a GAN framework to generate a modified version of the image that is consistent with the preceding images and coherent with the description. To achieve better region-specific refinement, we also introduce a sequential attention mechanism into the model. To benchmark on the new task, we introduce two new datasets, Zap-Seq and DeepFashion-Seq, which contain multi-turn sessions with image-description sequences in the fashion domain. Experiments on both datasets show that the proposed SeqAttnGANmodel outperforms state-of-the-art approaches on the interactive image editing task across all evaluation metrics including visual quality, image sequence coherence, and text-image consistency.",0
"Currently, most text-to-image synthesis tasks involve generating static images based on pre-defined textual descriptions. However, to explore more practical and interactive real-life applications, we have introduced a new task called Interactive Image Editing. This involves users guiding an agent to edit images via multi-turn textual commands on-the-fly. In each session, the agent takes a natural language description from the user and modifies the image generated in the previous turn to a new design, following the user's description. The main challenges of this task are ensuring contextual consistency between the generated image and the provided textual description, as well as step-by-step region-level modification to maintain visual consistency across the generated image sequence in each session. To address these challenges, we have proposed a Sequential Attention Generative Adversarial Network (SeqAttnGAN). Our model applies a neural state tracker to encode the previous image and textual description in each turn of the sequence, and uses a GAN framework to generate a modified version of the image that is consistent with the preceding images and coherent with the description. We have also introduced a sequential attention mechanism for better region-specific refinement. To benchmark on this new task, we have created two new datasets, Zap-Seq and DeepFashion-Seq, which contain multi-turn sessions with image-description sequences in the fashion domain. Our experiments on both datasets have shown that the proposed SeqAttnGAN model outperforms state-of-the-art approaches on the interactive image editing task across all evaluation metrics, including visual quality, image sequence coherence, and text-image consistency.",1
"We consider the problem of segmenting image regions given a natural language phrase, and study it on a novel dataset of 77,262 images and 345,486 phrase-region pairs. Our dataset is collected on top of the Visual Genome dataset and uses the existing annotations to generate a challenging set of referring phrases for which the corresponding regions are manually annotated. Phrases in our dataset correspond to multiple regions and describe a large number of object and stuff categories as well as their attributes such as color, shape, parts, and relationships with other entities in the image. Our experiments show that the scale and diversity of concepts in our dataset poses significant challenges to the existing state-of-the-art. We systematically handle the long-tail nature of these concepts and present a modular approach to combine category, attribute, and relationship cues that outperforms existing approaches.",0
"We examine the issue of segmenting image regions based on natural language phrases, using a new dataset of 77,262 images and 345,486 phrase-region pairs. Our dataset is built upon the Visual Genome dataset and generates a difficult set of referring phrases that are manually annotated using existing annotations. The phrases in our dataset correspond to multiple regions and encompass a wide range of object and stuff categories, as well as their attributes such as color, shape, parts, and relationships with other entities in the image. Our experiments demonstrate that the vast array of concepts in our dataset presents significant challenges to current state-of-the-art techniques. We address the long-tail nature of these concepts through a modular approach that combines category, attribute, and relationship cues, resulting in superior performance compared to existing methods.",1
"Textures in natural images can be characterized by color, shape, periodicity of elements within them, and other attributes that can be described using natural language. In this paper, we study the problem of describing visual attributes of texture on a novel dataset containing rich descriptions of textures, and conduct a systematic study of current generative and discriminative models for grounding language to images on this dataset. We find that while these models capture some properties of texture, they fail to capture several compositional properties, such as the colors of dots. We provide critical analysis of existing models by generating synthetic but realistic textures with different descriptions. Our dataset also allows us to train interpretable models and generate language-based explanations of what discriminative features are learned by deep networks for fine-grained categorization where texture plays a key role. We present visualizations of several fine-grained domains and show that texture attributes learned on our dataset offer improvements over expert-designed attributes on the Caltech-UCSD Birds dataset.",0
"The textures present in natural images can be delineated with the help of color, shape, periodicity of elements, and other qualities that can be elucidated through natural language. This research paper explores the issue of describing the visual attributes of texture on a new dataset that encompasses comprehensive descriptions of textures. Additionally, a methodical examination of the currently prevalent generative and discriminative models for grounding language to images on this dataset is conducted. The results indicate that these models are successful in capturing some properties of texture but fail to capture a few essential compositional properties like the colors of dots. A critical analysis of these models is provided by producing synthetic textures that are realistic and possess different descriptions. The dataset further enables the training of interpretable models that generate language-based explanations of the discriminative features learned by deep networks for fine-grained categorization where texture has a crucial role. Multiple fine-grained domains are visualized, illustrating that the texture attributes learned by the dataset offer improvements over expert-designed attributes on the Caltech-UCSD Birds dataset.",1
"We present a new video understanding pentathlon challenge, an open competition held in conjunction with the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2020. The objective of the challenge was to explore and evaluate new methods for text-to-video retrieval-the task of searching for content within a corpus of videos using natural language queries. This report summarizes the results of the first edition of the challenge together with the findings of the participants.",0
"Our report covers the inaugural edition of the video understanding pentathlon challenge, which was an open competition held in collaboration with the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2020. The primary aim of this challenge was to investigate and assess novel approaches for text-to-video retrieval. This refers to the task of using natural language queries to search for content within a collection of videos. The report provides a summary of the outcomes of the challenge, as well as the insights gained by the participants.",1
"Overparameterized networks trained to convergence have shown impressive performance in domains such as computer vision and natural language processing. Pushing state of the art on salient tasks within these domains corresponds to these models becoming larger and more difficult for machine learning practitioners to use given the increasing memory and storage requirements, not to mention the larger carbon footprint. Thus, in recent years there has been a resurgence in model compression techniques, particularly for deep convolutional neural networks and self-attention based networks such as the Transformer.   Hence, this paper provides a timely overview of both old and current compression techniques for deep neural networks, including pruning, quantization, tensor decomposition, knowledge distillation and combinations thereof.   We assume a basic familiarity with deep learning architectures\footnote{For an introduction to deep learning, see ~\citet{goodfellow2016deep}}, namely, Recurrent Neural Networks~\citep[(RNNs)][]{rumelhart1985learning,hochreiter1997long}, Convolutional Neural Networks~\citep{fukushima1980neocognitron}~\footnote{For an up to date overview see~\citet{khan2019survey}} and Self-Attention based networks~\citep{vaswani2017attention}\footnote{For a general overview of self-attention networks, see ~\citet{chaudhari2019attentive}.},\footnote{For more detail and their use in natural language processing, see~\citet{hu2019introductory}}. Most of the papers discussed are proposed in the context of at least one of these DNN architectures.",0
"In domains such as computer vision and natural language processing, overparameterized networks trained to convergence have demonstrated impressive performance. However, as these models become larger and more complex, machine learning practitioners face challenges with storage and memory requirements, as well as carbon footprint. Consequently, there has been a renewed interest in model compression techniques, particularly for deep convolutional neural networks and self-attention based networks like the Transformer. This paper provides an overview of old and current compression techniques for deep neural networks, including pruning, quantization, tensor decomposition, knowledge distillation and their combinations. Basic knowledge of deep learning architectures, such as Recurrent Neural Networks (RNNs), Convolutional Neural Networks, and Self-Attention based networks, is assumed. Most of the discussed papers propose techniques in the context of at least one of these DNN architectures, with a focus on natural language processing.",1
"Recently, large scale Transformer-based language models such as BERT, GPT-2, and XLNet have brought about exciting leaps in state-of-the-art results for many Natural Language Processing (NLP) tasks. One of the common trends in these recent models is a significant increase in model complexity, which introduces both more weights and computation. Moreover, with the advent of large-scale unsupervised datasets, training time is further extended due to the increased amount of data samples within a single training epoch. As a result, to train these models within a reasonable time, machine learning (ML) programmers often require advanced hardware setups such as the premium GPU-enabled NVIDIA DGX workstations or specialized accelerators such as Google's TPU Pods. Our work addresses this limitation and demonstrates that the BERT pre-trained model can be trained within 2 weeks on an academic-size cluster of widely available GPUs through careful algorithmic and software optimizations. In this paper, we present these optimizations on how to improve single device training throughput, distribute the training workload over multiple nodes and GPUs, and overcome the communication bottleneck introduced by the large data exchanges over the network. We show that we are able to perform pre-training on BERT within a reasonable time budget (12 days) in an academic setting, but with a much less expensive and less aggressive hardware resource requirement than in previously demonstrated industrial settings based on NVIDIA DGX machines or Google's TPU Pods.",0
"In recent times, Transformer-based language models such as BERT, GPT-2, and XLNet have achieved significant improvements in state-of-the-art results for various Natural Language Processing (NLP) tasks. However, these models are complex and require a significant amount of computation due to their large size and the increasing unsupervised datasets. As a result, advanced hardware setups such as NVIDIA DGX workstations or Google's TPU Pods are often used to train these models within a reasonable timeframe. Our study addresses this challenge by demonstrating that BERT can be trained on widely available GPUs within two weeks through algorithmic and software optimizations. This paper describes the optimizations used to improve single device training throughput, distribute the workload over multiple nodes and GPUs, and overcome the communication bottleneck caused by large data exchanges. Our approach achieves pre-training on BERT within 12 days in an academic setting, using less expensive and less aggressive hardware resources than previously demonstrated industrial settings.",1
"It has been found that software, like natural language texts, exhibits ""naturalness"", which can be captured by statistical language models. In recent years, neural language models have been proposed to represent the naturalness of software through deep learning. In this paper, we conduct an experimental evaluation of state-of-the-art neural language models for source code, including RNN-based models and Transformer-XL based models. Through experiments on a large-scale Python code corpus, we find that the Transformer-XL model outperforms RNN-based models (including LSTM and GRU models) in capturing the naturalness of software, with far less computational cost.",0
"Statistical language models can capture the ""naturalness"" of software, similar to how they capture it in natural language texts. Recently, deep learning has introduced neural language models to represent software naturalness. This research evaluates state-of-the-art neural language models for source code, such as RNN-based models and Transformer-XL based models, through experiments on a large Python code corpus. The results show that the Transformer-XL model is better at capturing software naturalness than RNN-based models (including LSTM and GRU models) and is computationally more efficient.",1
"Detecting clinically relevant objects in medical images is a challenge despite large datasets due to the lack of detailed labels. To address the label issue, we utilize the scene-level labels with a detection architecture that incorporates natural language information. We present a challenging new set of radiologist paired bounding box and natural language annotations on the publicly available MIMIC-CXR dataset especially focussed on pneumonia and pneumothorax. Along with the dataset, we present a joint vision language weakly supervised transformer layer-selected one-stage dual head detection architecture (LITERATI) alongside strong baseline comparisons with class activation mapping (CAM), gradient CAM, and relevant implementations on the NIH ChestXray-14 and MIMIC-CXR dataset. Borrowing from advances in vision language architectures, the LITERATI method demonstrates joint image and referring expression (objects localized in the image using natural language) input for detection that scales in a purely weakly supervised fashion. The architectural modifications address three obstacles -- implementing a supervised vision and language detection method in a weakly supervised fashion, incorporating clinical referring expression natural language information, and generating high fidelity detections with map probabilities. Nevertheless, the challenging clinical nature of the radiologist annotations including subtle references, multi-instance specifications, and relatively verbose underlying medical reports, ensures the vision language detection task at scale remains stimulating for future investigation.",0
"Detecting objects of clinical relevance in medical images is a difficult task due to the lack of detailed labels, despite the availability of vast datasets. To address this issue, we have used scene-level labels and a detection architecture that incorporates natural language information. We have created a new dataset with radiologist paired bounding box and natural language annotations, specifically focused on pneumonia and pneumothorax, which is publicly available on MIMIC-CXR. Along with the dataset, we present a weakly supervised transformer layer-selected one-stage dual head detection architecture called LITERATI, which is a joint vision language method that uses natural language expressions to localize objects in the image. We have modified the architecture to overcome three obstacles, namely implementing a supervised vision and language detection method in a weakly supervised fashion, incorporating clinical referring expression natural language information, and generating high-fidelity detections with map probabilities. The radiologist annotations are challenging due to subtle references, multi-instance specifications, and verbose medical reports, which makes this vision language detection task exciting for future research. We have also compared our method with other strong baseline implementations on the NIH ChestXray-14 and MIMIC-CXR dataset using class activation mapping (CAM) and gradient CAM.",1
"Person search by natural language aims at retrieving a specific person in a large-scale image pool that matches the given textual descriptions. While most of the current methods treat the task as a holistic visual and textual feature matching one, we approach it from an attribute-aligning perspective that allows grounding specific attribute phrases to the corresponding visual regions. We achieve success as well as the performance boosting by a robust feature learning that the referred identity can be accurately bundled by multiple attribute visual cues. To be concrete, our Visual-Textual Attribute Alignment model (dubbed as ViTAA) learns to disentangle the feature space of a person into subspaces corresponding to attributes using a light auxiliary attribute segmentation computing branch. It then aligns these visual features with the textual attributes parsed from the sentences by using a novel contrastive learning loss. Upon that, we validate our ViTAA framework through extensive experiments on tasks of person search by natural language and by attribute-phrase queries, on which our system achieves state-of-the-art performances. Code will be publicly available upon publication.",0
"The objective of natural language person search is to locate a specific person in a vast collection of images that match the given textual descriptions. Most existing methods approach this task by matching the visual and textual features as a whole, but our approach focuses on aligning specific attribute phrases with corresponding visual regions. We achieve this by learning robust features that accurately bundle multiple attribute visual cues, which leads to improved performance. Our Visual-Textual Attribute Alignment model (ViTAA) uses an auxiliary attribute segmentation computing branch to disentangle a person's feature space into subspaces corresponding to attributes. We then align these visual features with textual attributes parsed from sentences using a novel contrastive learning loss. Our ViTAA framework has been extensively tested on person search by natural language and attribute-phrase queries, and it has achieved state-of-the-art performance. We will make the code publicly available upon publication.",1
"There has been considerable and growing interest in applying machine learning for cyber defenses. One promising approach has been to apply natural language processing techniques to analyze logs data for suspicious behavior. A natural question arises to how robust these systems are to adversarial attacks. Defense against sophisticated attack is of particular concern for cyber defenses. In this paper, we develop a testing framework to evaluate adversarial robustness of machine learning cyber defenses, particularly those focused on log data. Our framework uses techniques from deep reinforcement learning and adversarial natural language processing. We validate our framework using a publicly available dataset and demonstrate that our adversarial attack does succeed against the target systems, revealing a potential vulnerability. We apply our framework to analyze the influence of different levels of dropout regularization and find that higher dropout levels increases robustness. Moreover 90% dropout probability exhibited the highest level of robustness by a significant margin, which suggests unusually high dropout may be necessary to properly protect against adversarial attacks.",0
"The use of machine learning in cyber defenses has garnered significant attention and interest. One promising strategy involves utilizing natural language processing techniques to scrutinize log data for suspicious activity. However, the effectiveness of these systems against adversarial attacks has been called into question, particularly given the concern surrounding sophisticated attacks in the realm of cyber defenses. This article introduces a testing framework that aims to assess the adversarial robustness of machine learning cyber defenses, with a specific focus on log data. Drawing on deep reinforcement learning and adversarial natural language processing techniques, our framework was tested using a publicly available dataset, and we were able to successfully conduct an adversarial attack on the target systems, highlighting a potential vulnerability. Our analysis further revealed that higher levels of dropout regularization led to increased robustness, with a dropout probability of 90% exhibiting the greatest level of strength against adversarial attacks, indicating that exceptionally high dropout rates may be necessary for adequate protection.",1
"Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",0
"The practice of transfer learning has become a valuable tool in natural language processing (NLP), involving pre-training a model on a data-rich task before fine-tuning it for a downstream task. This technique has led to the development of various methods, practices, and approaches. This study aims to investigate transfer learning techniques for NLP, utilizing a unified framework that converts all text-based language problems into a text-to-text format. By conducting a systematic analysis of pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors across multiple language understanding tasks, we achieve state-of-the-art results on various benchmarks, such as summarization, question answering, and text classification. To support further research in transfer learning for NLP, we provide access to our data set, pre-trained models, and code.",1
"Given an image, generating its natural language description (i.e., caption) is a well studied problem. Approaches proposed to address this problem usually rely on image features that are difficult to interpret. Particularly, these image features are subdivided into global and local features, where global features are extracted from the global representation of the image, while local features are extracted from the objects detected locally in an image. Although, local features extract rich visual information from the image, existing models generate captions in a blackbox manner and humans have difficulty interpreting which local objects the caption is aimed to represent. Hence in this paper, we propose a novel framework for the image captioning with an explicit object (e.g., knowledge graph entity) selection process while still maintaining its end-to-end training ability. The model first explicitly selects which local entities to include in the caption according to a human-interpretable mask, then generate proper captions by attending to selected entities. Experiments conducted on the MSCOCO dataset demonstrate that our method achieves good performance in terms of the caption quality and diversity with a more interpretable generating process than previous counterparts.",0
"The problem of generating a natural language description (caption) for an image has been extensively studied. Previous approaches have relied on image features that are challenging to interpret, specifically global and local features. Global features are extracted from the overall representation of the image, while local features are extracted from objects identified within the image. Although local features provide rich visual information, existing models generate captions in a manner that is difficult for humans to interpret which objects are being referred to. This paper presents a new framework for image captioning that includes an explicit object selection process, such as a knowledge graph entity, while still allowing for end-to-end training. The model selects local entities to include in the caption based on a human-interpretable mask, and then generates captions by attending to the selected entities. Experiments on the MSCOCO dataset show that our method produces high-quality and diverse captions with a more interpretable generating process than previous approaches.",1
"The outbreak of the novel coronavirus disease 2019 (COVID-19), caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has been continuously affecting human lives and communities around the world in many ways, from cities under lockdown to new social experiences. Although in most cases COVID-19 results in mild illness, it has drawn global attention due to the extremely contagious nature of SARS-CoV-2. Governments and healthcare professionals, along with people and society as a whole, have taken any measures to break the chain of transition and flatten the epidemic curve. In this study, we used multiple data sources, i.e., PubMed and ArXiv, and built several machine learning models to characterize the landscape of current COVID-19 research by identifying the latent topics and analyzing the temporal evolution of the extracted research themes, publications similarity, and sentiments, within the time-frame of January- May 2020. Our findings confirm the types of research available in PubMed and ArXiv differ significantly, with the former exhibiting greater diversity in terms of COVID-19 related issues and the latter focusing more on intelligent systems/tools to predict/diagnose COVID-19. The special attention of the research community to the high-risk groups and people with complications was also confirmed.",0
"The COVID-19 pandemic caused by the SARS-CoV-2 virus has had a profound impact on people and communities worldwide, resulting in lockdowns and unique social experiences. Although COVID-19 typically causes mild illness, its highly contagious nature has garnered global attention, leading to measures to break the chain of transmission and flatten the epidemic curve. This study utilized various data sources and machine learning models to analyze the landscape of current COVID-19 research, identifying research themes and sentiments from January to May 2020. Findings suggest that PubMed and ArXiv differ significantly, with the former exhibiting a wider range of COVID-19 related issues and the latter focusing more on intelligent systems/tools for predicting and diagnosing COVID-19. The research community has also shown particular attention to high-risk groups and people with complications.",1
"Adversarial attacks of deep neural networks have been intensively studied on image, audio, natural language, patch, and pixel classification tasks. Nevertheless, as a typical, while important real-world application, the adversarial attacks of online video object tracking that traces an object's moving trajectory instead of its category are rarely explored. In this paper, we identify a new task for the adversarial attack to visual tracking: online generating imperceptible perturbations that mislead trackers along an incorrect (Untargeted Attack, UA) or specified trajectory (Targeted Attack, TA). To this end, we first propose a \textit{spatial-aware} basic attack by adapting existing attack methods, i.e., FGSM, BIM, and C&W, and comprehensively analyze the attacking performance. We identify that online object tracking poses two new challenges: 1) it is difficult to generate imperceptible perturbations that can transfer across frames, and 2) real-time trackers require the attack to satisfy a certain level of efficiency. To address these challenges, we further propose the spatial-aware online incremental attack (a.k.a. SPARK) that performs spatial-temporal sparse incremental perturbations online and makes the adversarial attack less perceptible. In addition, as an optimization-based method, SPARK quickly converges to very small losses within several iterations by considering historical incremental perturbations, making it much more efficient than basic attacks. The in-depth evaluation on state-of-the-art trackers (i.e., SiamRPN++ with AlexNet, MobileNetv2, and ResNet-50, and SiamDW) on OTB100, VOT2018, UAV123, and LaSOT demonstrates the effectiveness and transferability of SPARK in misleading the trackers under both UA and TA with minor perturbations.",0
"The research on adversarial attacks in deep neural networks has primarily focused on image, audio, natural language, patch, and pixel classification tasks. However, there has been limited exploration of these attacks in online video object tracking, which involves tracing an object's moving trajectory instead of its category. In this study, we introduce a new task for adversarial attacks in visual tracking, which involves online generation of imperceptible perturbations to mislead trackers along an incorrect (Untargeted Attack, UA) or specified trajectory (Targeted Attack, TA). Our initial proposal is a spatial-aware basic attack that adapts existing attack methods, such as FGSM, BIM, and C&W, and we analyze its performance comprehensively. We identify two challenges in online object tracking: generating imperceptible perturbations that can transfer across frames and satisfying real-time trackers' efficiency requirements. To overcome these challenges, we propose the spatial-aware online incremental attack (SPARK), which performs spatial-temporal sparse incremental perturbations and reduces the perceptibility of adversarial attacks. As an optimization-based method, SPARK quickly converges to very small losses within several iterations and considers historical incremental perturbations, making it much more efficient than basic attacks. Our evaluation on state-of-the-art trackers on OTB100, VOT2018, UAV123, and LaSOT demonstrates the effectiveness and transferability of SPARK in misleading the trackers under both UA and TA with minor perturbations.",1
"Vision-and-Language Navigation (VLN) requires an agent to find a specified spot in an unseen environment by following natural language instructions. Dominant methods based on supervised learning clone expert's behaviours and thus perform better on seen environments, while showing restricted performance on unseen ones. Reinforcement Learning (RL) based models show better generalisation ability but have issues as well, requiring large amount of manual reward engineering is one of which. In this paper, we introduce a Soft Expert Reward Learning (SERL) model to overcome the reward engineering designing and generalisation problems of the VLN task. Our proposed method consists of two complementary components: Soft Expert Distillation (SED) module encourages agents to behave like an expert as much as possible, but in a soft fashion; Self Perceiving (SP) module targets at pushing the agent towards the final destination as fast as possible. Empirically, we evaluate our model on the VLN seen, unseen and test splits and the model outperforms the state-of-the-art methods on most of the evaluation metrics.",0
"The task of Vision-and-Language Navigation (VLN) involves following natural language instructions to locate a particular spot in an unfamiliar environment. Existing approaches based on supervised learning tend to imitate the behavior of experts and perform better in familiar settings, but struggle with novel environments. Reinforcement Learning (RL) approaches offer better generalization capabilities, but require significant manual effort in designing rewards. This paper proposes a Soft Expert Reward Learning (SERL) model to address these issues in VLN. The model comprises two parts: Soft Expert Distillation (SED) encourages expert-like behavior in a gentle manner, while Self Perceiving (SP) aims to guide the agent to the final destination as quickly as possible. The proposed model is evaluated on VLN seen, unseen, and test splits, and outperforms state-of-the-art methods across most evaluation criteria.",1
"The task of retrieving video content relevant to natural language queries plays a critical role in effectively handling internet-scale datasets. Most of the existing methods for this caption-to-video retrieval problem do not fully exploit cross-modal cues present in video. Furthermore, they aggregate per-frame visual features with limited or no temporal information. In this paper, we present a multi-modal transformer to jointly encode the different modalities in video, which allows each of them to attend to the others. The transformer architecture is also leveraged to encode and model the temporal information. On the natural language side, we investigate the best practices to jointly optimize the language embedding together with the multi-modal transformer. This novel framework allows us to establish state-of-the-art results for video retrieval on three datasets. More details are available at http://thoth.inrialpes.fr/research/MMT.",0
"Effectively managing large-scale datasets on the internet requires retrieving video content relevant to natural language queries. However, most existing methods for caption-to-video retrieval overlook cross-modal cues present in video and only aggregate per-frame visual features with limited or no temporal information. To address this, we introduce a multi-modal transformer that jointly encodes the different modalities in video, allowing each to attend to the others while also encoding and modeling temporal information. On the natural language side, we explore optimal approaches for jointly optimizing language embedding with the multi-modal transformer. Our framework achieves state-of-the-art results for video retrieval on three datasets. Further information can be found at http://thoth.inrialpes.fr/research/MMT.",1
"Though deep neural networks (DNNs) have shown superiority over other techniques in major fields like computer vision, natural language processing, robotics, recently, it has been proven that they are vulnerable to adversarial attacks. The addition of a simple, small and almost invisible perturbation to the original input image can be used to fool DNNs into making wrong decisions. With more attack algorithms being designed, a need for defending the neural networks from such attacks arises. Retraining the network with adversarial images is one of the simplest techniques. In this paper, we evaluate the effectiveness of such a retraining strategy in defending against adversarial attacks. We also show how simple algorithms like KNN can be used to determine the labels of the adversarial images needed for retraining. We present the results on two standard datasets namely, CIFAR-10 and TinyImageNet.",0
"Although deep neural networks (DNNs) have demonstrated their superiority over other technologies in significant fields such as computer vision, natural language processing, and robotics, recent research has shown that they are susceptible to adversarial attacks. By applying a small, almost invisible perturbation to the original input image, DNNs can be easily tricked into making incorrect decisions. As more attack algorithms are developed, there is a growing need to protect neural networks from these types of attacks. One of the simplest techniques for defending against adversarial attacks is to retrain the network with adversarial images. In this study, we assess the effectiveness of this retraining approach and demonstrate how basic algorithms such as KNN can be used to classify the labels of the adversarial images needed for retraining. We present our findings on two widely used datasets, CIFAR-10 and TinyImageNet.",1
"With online calendar services gaining popularity worldwide, calendar data has become one of the richest context sources for understanding human behavior. However, event scheduling is still time-consuming even with the development of online calendars. Although machine learning based event scheduling models have automated scheduling processes to some extent, they often fail to understand subtle user preferences and complex calendar contexts with event titles written in natural language. In this paper, we propose Neural Event Scheduling Assistant (NESA) which learns user preferences and understands calendar contexts, directly from raw online calendars for fully automated and highly effective event scheduling. We leverage over 593K calendar events for NESA to learn scheduling personal events, and we further utilize NESA for multi-attendee event scheduling. NESA successfully incorporates deep neural networks such as Bidirectional Long Short-Term Memory, Convolutional Neural Network, and Highway Network for learning the preferences of each user and understanding calendar context based on natural languages. The experimental results show that NESA significantly outperforms previous baseline models in terms of various evaluation metrics on both personal and multi-attendee event scheduling tasks. Our qualitative analysis demonstrates the effectiveness of each layer in NESA and learned user preferences.",0
"As online calendar services continue to gain popularity worldwide, calendar data has become an invaluable source for understanding human behavior. Despite the development of machine learning-based event scheduling models, event scheduling remains a time-consuming process due to the models' inability to comprehend subtle user preferences and complex calendar contexts. In this paper, we introduce the Neural Event Scheduling Assistant (NESA), which leverages over 593K calendar events to learn user preferences and understand natural language-based calendar contexts for fully automated and highly effective event scheduling. NESA incorporates deep neural networks such as Bidirectional Long Short-Term Memory, Convolutional Neural Network, and Highway Network to improve its performance in both personal and multi-attendee event scheduling tasks. Our experimental results demonstrate that NESA significantly outperforms previous baseline models in terms of various evaluation metrics, and our qualitative analysis highlights the effectiveness of each layer in NESA and the learned user preferences.",1
"Multi-stage training and knowledge transfer, from a large-scale pretraining task to various finetuning tasks, have revolutionized natural language processing and computer vision resulting in state-of-the-art performance improvements. In this paper, we develop a multi-stage influence function score to track predictions from a finetuned model all the way back to the pretraining data. With this score, we can identify the pretraining examples in the pretraining task that contribute most to a prediction in the finetuning task. The proposed multi-stage influence function generalizes the original influence function for a single model in (Koh & Liang, 2017), thereby enabling influence computation through both pretrained and finetuned models. We study two different scenarios with the pretrained embeddings fixed or updated in the finetuning tasks. We test our proposed method in various experiments to show its effectiveness and potential applications.",0
"Natural language processing and computer vision have undergone a significant transformation due to multi-stage training and knowledge transfer, where a large-scale pretraining task is followed by several finetuning tasks, leading to state-of-the-art performance improvements. This research paper introduces a multi-stage influence function score that can trace predictions of a finetuned model back to the pretraining data. The proposed score helps identify the pretraining examples that contribute the most to a prediction in the finetuning task. This multi-stage influence function extends the original influence function for a single model, enabling influence computation through both finetuned and pretrained models. The study explores two scenarios: with pretrained embeddings fixed or updated in finetuning tasks. The proposed method is tested in various experiments to demonstrate its effectiveness and potential applications.",1
"Generating natural language descriptions for videos, i.e., video captioning, essentially requires step-by-step reasoning along the generation process. For example, to generate the sentence ""a man is shooting a basketball"", we need to first locate and describe the subject ""man"", next reason out the man is ""shooting"", then describe the object ""basketball"" of shooting. However, existing visual reasoning methods designed for visual question answering are not appropriate to video captioning, for it requires more complex visual reasoning on videos over both space and time, and dynamic module composition along the generation process. In this paper, we propose a novel visual reasoning approach for video captioning, named Reasoning Module Networks (RMN), to equip the existing encoder-decoder framework with the above reasoning capacity. Specifically, our RMN employs 1) three sophisticated spatio-temporal reasoning modules, and 2) a dynamic and discrete module selector trained by a linguistic loss with a Gumbel approximation. Extensive experiments on MSVD and MSR-VTT datasets demonstrate the proposed RMN outperforms the state-of-the-art methods while providing an explicit and explainable generation process. Our code is available at https://github.com/tgc1997/RMN.",0
"The process of generating natural language descriptions for videos, also known as video captioning, requires reasoning step-by-step throughout the generation process. To create a sentence like ""a man is shooting a basketball,"" one must first identify and describe the subject ""man,"" deduce that he is ""shooting,"" and then describe the object ""basketball."" However, existing visual reasoning methods developed for visual question answering are not suitable for video captioning because it requires more complex visual reasoning over both space and time, and dynamic module composition throughout the generation process. This paper proposes a new visual reasoning approach called Reasoning Module Networks (RMN) for video captioning, which enhances the existing encoder-decoder framework with the necessary reasoning capability. The RMN employs three advanced spatio-temporal reasoning modules and a dynamic and discrete module selector that is trained using a linguistic loss with a Gumbel approximation. Extensive experiments conducted on MSVD and MSR-VTT datasets demonstrate that the proposed RMN outperforms state-of-the-art methods while providing an explicit and explainable generation process. The code for RMN is available at https://github.com/tgc1997/RMN.",1
"Vision-and-Language Navigation (VLN) is a task where agents must decide how to move through a 3D environment to reach a goal by grounding natural language instructions to the visual surroundings. One of the problems of the VLN task is data scarcity since it is difficult to collect enough navigation paths with human-annotated instructions for interactive environments. In this paper, we explore the use of counterfactual thinking as a human-inspired data augmentation method that results in robust models. Counterfactual thinking is a concept that describes the human propensity to create possible alternatives to life events that have already occurred. We propose an adversarial-driven counterfactual reasoning model that can consider effective conditions instead of low-quality augmented data. In particular, we present a model-agnostic adversarial path sampler (APS) that learns to sample challenging paths that force the navigator to improve based on the navigation performance. APS also serves to do pre-exploration of unseen environments to strengthen the model's ability to generalize. We evaluate the influence of APS on the performance of different VLN baseline models using the room-to-room dataset (R2R). The results show that the adversarial training process with our proposed APS benefits VLN models under both seen and unseen environments. And the pre-exploration process can further gain additional improvements under unseen environments.",0
"The task of Vision-and-Language Navigation (VLN) involves agents using natural language instructions to navigate a 3D environment in order to reach a goal. However, VLN poses a challenge due to a lack of available data as it is difficult to collect enough navigation paths with human-annotated instructions for interactive environments. To address this issue, we investigate the use of counterfactual thinking as a human-inspired data augmentation approach that leads to robust models. Counterfactual thinking refers to the human tendency to create hypothetical alternatives to past events. We propose an adversarial-driven counterfactual reasoning model that can effectively consider conditions instead of low-quality augmented data. Specifically, we introduce a model-agnostic adversarial path sampler (APS) that can learn to generate difficult paths that prompt navigators to improve their performance. APS can also be used for pre-exploration of unseen environments to enhance the model's ability to generalize. We evaluate the impact of APS on the performance of various VLN baseline models using the room-to-room dataset (R2R). Our results demonstrate that the adversarial training process with the proposed APS improves VLN models in both seen and unseen environments. Furthermore, the pre-exploration process can provide additional benefits in unseen environments.",1
"Recent advances in facial expression synthesis have shown promising results using diverse expression representations including facial action units. Facial action units for an elaborate facial expression synthesis need to be intuitively represented for human comprehension, not a numeric categorization of facial action units. To address this issue, we utilize human-friendly approach: use of natural language where language helps human grasp conceptual contexts. In this paper, therefore, we propose a new facial expression synthesis model from language-based facial expression description. Our method can synthesize the facial image with detailed expressions. In addition, effectively embedding language features on facial features, our method can control individual word to handle each part of facial movement. Extensive qualitative and quantitative evaluations were conducted to verify the effectiveness of the natural language.",0
"Recent research has made progress in generating facial expressions using various representations, such as facial action units. However, a problem arises when attempting to represent facial action units in a way that humans can easily understand, rather than just categorizing them numerically. To solve this issue, we propose a human-friendly approach that uses natural language to convey conceptual contexts. Specifically, we introduce a new model that generates facial expressions based on language-based descriptions. Our approach produces detailed facial expressions and allows for fine-grained control over each component of facial movement by embedding language features onto facial features. We conducted extensive qualitative and quantitative evaluations to demonstrate the effectiveness of our natural language approach.",1
"Logical connectives and their implications on the meaning of a natural language sentence are a fundamental aspect of understanding. In this paper, we investigate whether visual question answering (VQA) systems trained to answer a question about an image, are able to answer the logical composition of multiple such questions. When put under this \textit{Lens of Logic}, state-of-the-art VQA models have difficulty in correctly answering these logically composed questions. We construct an augmentation of the VQA dataset as a benchmark, with questions containing logical compositions and linguistic transformations (negation, disjunction, conjunction, and antonyms). We propose our {Lens of Logic (LOL)} model which uses question-attention and logic-attention to understand logical connectives in the question, and a novel Fr\'echet-Compatibility Loss, which ensures that the answers of the component questions and the composed question are consistent with the inferred logical operation. Our model shows substantial improvement in learning logical compositions while retaining performance on VQA. We suggest this work as a move towards robustness by embedding logical connectives in visual understanding.",0
"Understanding logical connectives and their impact on the meaning of natural language sentences is a crucial aspect of comprehension. This study examines whether visual question answering (VQA) systems, which are trained to answer questions about images, can accurately respond to logically composed questions. State-of-the-art VQA models struggle with this task when viewed through the ""Lens of Logic."" To address this issue, we introduce an augmented VQA dataset with questions that feature logical compositions and linguistic transformations (such as negation, disjunction, conjunction, and antonyms). Our proposed ""Lens of Logic (LOL)"" model utilizes question-attention and logic-attention to comprehend logical connectives in the question. Additionally, our novel Fréchet-Compatibility Loss ensures that the answers to component questions and the composed question align with the inferred logical operation. Our model demonstrates significant improvements in learning logical compositions while maintaining strong performance in VQA. We believe this work represents a step towards embedding logical connectives in visual understanding for greater robustness.",1
"Embodied AI has been recently gaining attention as it aims to foster the development of autonomous and intelligent agents. In this paper, we devise a novel embodied setting in which an agent needs to explore a previously unknown environment while recounting what it sees during the path. In this context, the agent needs to navigate the environment driven by an exploration goal, select proper moments for description, and output natural language descriptions of relevant objects and scenes. Our model integrates a novel self-supervised exploration module with penalty, and a fully-attentive captioning model for explanation. Also, we investigate different policies for selecting proper moments for explanation, driven by information coming from both the environment and the navigation. Experiments are conducted on photorealistic environments from the Matterport3D dataset and investigate the navigation and explanation capabilities of the agent as well as the role of their interactions.",0
"The concept of Embodied AI has recently garnered attention for its potential to cultivate autonomous and intelligent agents. This study introduces a unique approach to Embodied AI by creating a setting in which an agent must explore an unfamiliar environment while simultaneously describing its observations in natural language. The agent must navigate the environment with a specific exploration goal in mind, selecting appropriate moments to provide descriptive explanations of relevant objects and scenes. Our model incorporates a self-supervised exploration module with a penalty, and a fully-attentive captioning model for explanation. We also explore different policies for selecting the optimal moments for explanation, based on information from both the environment and navigation. We conduct experiments using photorealistic environments from the Matterport3D dataset, evaluating the agent's navigation and explanation skills, and examining the impact of their interactions.",1
"In the past decade, deep learning (DL) has achieved unprecedented success in numerous fields including computer vision, natural language processing, and healthcare. In particular, DL is experiencing an increasing development in applications for advanced medical image analysis in terms of analysis, segmentation, classification, and furthermore. On the one hand, tremendous needs that leverage the power of DL for medical image analysis are arising from the research community of a medical, clinical, and informatics background to jointly share their expertise, knowledge, skills, and experience. On the other hand, barriers between disciplines are on the road for them often hampering a full and efficient collaboration. To this end, we propose our novel open-source platform, i.e., MeDaS -- the MeDical open-source platform as Service. To the best of our knowledge, MeDaS is the first open-source platform proving a collaborative and interactive service for researchers from a medical background easily using DL related toolkits, and at the same time for scientists or engineers from information sciences to understand the medical knowledge side. Based on a series of toolkits and utilities from the idea of RINV (Rapid Implementation aNd Verification), our proposed MeDaS platform can implement pre-processing, post-processing, augmentation, visualization, and other phases needed in medical image analysis. Five tasks including the subjects of lung, liver, brain, chest, and pathology, are validated and demonstrated to be efficiently realisable by using MeDaS.",0
"Deep learning (DL) has achieved unprecedented success in various fields, such as computer vision, natural language processing, and healthcare, over the past decade. In particular, DL is increasingly being utilized for advanced medical image analysis, including analysis, segmentation, classification, and more. However, there is a need for collaboration between the research community of medical, clinical, and informatics backgrounds to leverage the power of DL for medical image analysis. Unfortunately, the barriers between disciplines often hamper full and efficient collaboration. To address this issue, we propose our open-source platform, MeDaS (the MeDical open-source platform as Service). MeDaS is a novel platform that provides a collaborative and interactive service, allowing researchers from a medical background to easily use DL-related toolkits and scientists or engineers from information sciences to understand the medical knowledge side. MeDaS is based on a series of toolkits and utilities from the RINV (Rapid Implementation aNd Verification) concept and can perform pre-processing, post-processing, augmentation, visualization, and other necessary phases in medical image analysis. We validated and demonstrated the efficiency of MeDaS in five tasks, including lung, liver, brain, chest, and pathology.",1
"The joint understanding of vision and language has been recently gaining a lot of attention in both the Computer Vision and Natural Language Processing communities, with the emergence of tasks such as image captioning, image-text matching, and visual question answering. As both images and text can be encoded as sets or sequences of elements -- like regions and words -- proper reduction functions are needed to transform a set of encoded elements into a single response, like a classification or similarity score. In this paper, we propose a novel fully-attentive reduction method for vision and language. Specifically, our approach computes a set of scores for each element of each modality employing a novel variant of cross-attention, and performs a learnable and cross-modal reduction, which can be used for both classification and ranking. We test our approach on image-text matching and visual question answering, building fair comparisons with other reduction choices, on both COCO and VQA 2.0 datasets. Experimentally, we demonstrate that our approach leads to a performance increase on both tasks. Further, we conduct ablation studies to validate the role of each component of the approach.",0
"Recently, there has been a growing interest in the correlation between vision and language within the Computer Vision and Natural Language Processing communities. This is due to the emergence of tasks such as image captioning, image-text matching, and visual question answering, where both images and text can be represented as sets or sequences of elements. To transform these sets of encoded elements into a single response, such as a classification or similarity score, appropriate reduction functions are necessary. In this study, we propose a new fully-attentive reduction method for vision and language. Our method utilizes a novel variant of cross-attention to compute a set of scores for each element of each modality and performs a learnable and cross-modal reduction that can be used for both classification and ranking. We evaluate our method on image-text matching and visual question answering tasks on COCO and VQA 2.0 datasets, and show that our approach outperforms other reduction choices. Additionally, we conduct ablation studies to validate the effectiveness of each component of our approach.",1
"The ability to perform effective planning is crucial for building an instruction-following agent. When navigating through a new environment, an agent is challenged with (1) connecting the natural language instructions with its progressively growing knowledge of the world; and (2) performing long-range planning and decision making in the form of effective exploration and error correction. Current methods are still limited on both fronts despite extensive efforts. In this paper, we introduce the Evolving Graphical Planner (EGP), a model that performs global planning for navigation based on raw sensory input. The model dynamically constructs a graphical representation, generalizes the action space to allow for more flexible decision making, and performs efficient planning on a proxy graph representation. We evaluate our model on a challenging Vision-and-Language Navigation (VLN) task with photorealistic images and achieve superior performance compared to previous navigation architectures. For instance, we achieve a 53% success rate on the test split of the Room-to-Room navigation task through pure imitation learning, outperforming previous navigation architectures by up to 5%.",0
"Effective planning is crucial for developing an instruction-following agent. When exploring a new environment, the agent faces the challenge of linking natural language instructions with its growing knowledge of the world, while also engaging in long-range planning and decision-making to navigate effectively and rectify errors. Despite extensive efforts, current methods are still limited in these areas. This paper introduces the Evolving Graphical Planner (EGP), a model that performs global planning for navigation using raw sensory input. The EGP constructs a graphical representation dynamically, broadens the action space to facilitate flexible decision-making, and conducts efficient planning on a proxy graph representation. We evaluate the EGP on a challenging Vision-and-Language Navigation (VLN) task with photorealistic images, and our model outperforms previous navigation architectures. For example, through pure imitation learning, we attain a 53% success rate on the test split of the Room-to-Room navigation task, surpassing previous navigation architectures by up to 5%.",1
"In image captioning where fluency is an important factor in evaluation, e.g., $n$-gram metrics, sequential models are commonly used; however, sequential models generally result in overgeneralized expressions that lack the details that may be present in an input image. Inspired by the idea of the compositional neural module networks in the visual question answering task, we introduce a hierarchical framework for image captioning that explores both compositionality and sequentiality of natural language. Our algorithm learns to compose a detail-rich sentence by selectively attending to different modules corresponding to unique aspects of each object detected in an input image to include specific descriptions such as counts and color. In a set of experiments on the MSCOCO dataset, the proposed model outperforms a state-of-the art model across multiple evaluation metrics, more importantly, presenting visually interpretable results. Furthermore, the breakdown of subcategories $f$-scores of the SPICE metric and human evaluation on Amazon Mechanical Turk show that our compositional module networks effectively generate accurate and detailed captions.",0
"When evaluating image captioning, fluency is important and sequential models are commonly used despite their tendency to produce overgeneralized expressions lacking details. To address this, we introduce a hierarchical framework for image captioning that combines compositionality and sequentiality. Our algorithm selectively attends to different modules corresponding to unique aspects of each object in the image, allowing it to compose detail-rich sentences that include specific descriptions. We conducted experiments on the MSCOCO dataset and found that our model outperformed a state-of-the-art model across multiple evaluation metrics, while also producing visually interpretable results. Subcategory breakdowns of the SPICE metric and human evaluation on Amazon Mechanical Turk demonstrated that our compositional module networks generate accurate and detailed captions.",1
"We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is $x\Phi(x)$, where $\Phi(x)$ the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs ($x\mathbf{1}_{x>0}$). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.",0
"Our proposal is called the Gaussian Error Linear Unit (GELU) activation function, which has shown to be highly effective in neural networks. GELU is computed as $x\Phi(x)$, where $\Phi(x)$ is the standard Gaussian cumulative distribution function. Unlike ReLUs ($x\mathbf{1}_{x>0}$) that gate inputs by their sign, GELU weights inputs by their value. We conducted an empirical evaluation of GELU against ELU and ReLU activations and observed improved performance across various computer vision, natural language processing, and speech tasks.",1
"With great advances in vision and natural language processing, the generation of image captions becomes a need. In a recent paper, Mathews, Xie and He [1], extended a new model to generate styled captions by separating semantics and style. In continuation of this work, here a new captioning model is developed including an image encoder to extract the features, a mixture of recurrent networks to embed the set of extracted features to a set of words, and a sentence generator that combines the obtained words as a stylized sentence. The resulted system that entitled as Mixture of Recurrent Experts (MoRE), uses a new training algorithm that derives singular value decomposition (SVD) from weighting matrices of Recurrent Neural Networks (RNNs) to increase the diversity of captions. Each decomposition step depends on a distinctive factor based on the number of RNNs in MoRE. Since the used sentence generator gives a stylized language corpus without paired images, our captioning model can do the same. Besides, the styled and diverse captions are extracted without training on a densely labeled or styled dataset. To validate this captioning model, we use Microsoft COCO which is a standard factual image caption dataset. We show that the proposed captioning model can generate a diverse and stylized image captions without the necessity of extra-labeling. The results also show better descriptions in terms of content accuracy.",0
"Due to significant advancements in vision and natural language processing, the need to generate image captions has arisen. In a recent study, Mathews, Xie, and He created a new model that separates semantics and style to generate styled captions. Building upon their work, a new captioning model has been developed. This model includes an image encoder to extract features, a mixture of recurrent networks to embed these features into words, and a sentence generator to produce a stylized sentence. This system, called Mixture of Recurrent Experts (MoRE), uses a unique training algorithm that employs singular value decomposition (SVD) from weighting matrices of Recurrent Neural Networks (RNNs) to increase caption diversity. Each decomposition step is based on a distinctive factor that corresponds to the number of RNNs in MoRE. The sentence generator produces stylized language without paired images, and the model extracts diverse and styled captions without the need for training on densely labeled or styled datasets. To validate this captioning model, the Microsoft COCO dataset, a standard factual image caption dataset, was used. The results demonstrate that the proposed captioning model can generate diverse and stylized image captions without requiring extra-labeling. Furthermore, the descriptions are more accurate in terms of content.",1
"E-commerce customers in developing nations like India tend to follow no fixed format while entering shipping addresses. Parsing such addresses is challenging because of a lack of inherent structure or hierarchy. It is imperative to understand the language of addresses, so that shipments can be routed without delays. In this paper, we propose a novel approach towards understanding customer addresses by deriving motivation from recent advances in Natural Language Processing (NLP). We also formulate different pre-processing steps for addresses using a combination of edit distance and phonetic algorithms. Then we approach the task of creating vector representations for addresses using Word2Vec with TF-IDF, Bi-LSTM and BERT based approaches. We compare these approaches with respect to sub-region classification task for North and South Indian cities. Through experiments, we demonstrate the effectiveness of generalized RoBERTa model, pre-trained over a large address corpus for language modelling task. Our proposed RoBERTa model achieves a classification accuracy of around 90% with minimal text preprocessing for sub-region classification task outperforming all other approaches. Once pre-trained, the RoBERTa model can be fine-tuned for various downstream tasks in supply chain like pincode suggestion and geo-coding. The model generalizes well for such tasks even with limited labelled data. To the best of our knowledge, this is the first of its kind research proposing a novel approach of understanding customer addresses in e-commerce domain by pre-training language models and fine-tuning them for different purposes.",0
"The process of parsing shipping addresses in e-commerce can be difficult in developing countries like India due to the lack of structure or hierarchy. To avoid delays in routing shipments, it is crucial to understand the language used in addresses. This paper proposes a new method for comprehending customer addresses by drawing inspiration from recent advancements in Natural Language Processing. The approach involves pre-processing addresses with a combination of edit distance and phonetic algorithms, creating vector representations with Word2Vec and TF-IDF, Bi-LSTM and BERT methods, and comparing them for sub-region classification in North and South Indian cities. The results show that the RoBERTa model, pre-trained on a large address corpus for language modelling, achieves a classification accuracy of around 90% with minimal text preprocessing for sub-region classification task, surpassing other methods. Once pre-trained, the RoBERTa model can be fine-tuned for various downstream tasks in supply chain like pincode suggestion and geo-coding. This research is the first of its kind to propose a novel approach to understanding customer addresses in e-commerce by pre-training language models and fine-tuning them for different purposes.",1
"Advertising and feed ranking are essential to many Internet companies such as Facebook. Among many real-world advertising and feed ranking systems, click through rate (CTR) prediction plays a central role. In recent years, many neural network based CTR models have been proposed and achieved success such as Factorization-Machine Supported Neural Networks, DeepFM and xDeepFM. Many of them contain two commonly used components: embedding layer and MLP hidden layers. On the other side, gating mechanism is also widely applied in many research fields such as computer vision(CV) and natural language processing(NLP). Some research has proved that gating mechanism improves the trainability of non-convex deep neural networks. Inspired by these observations, we propose a novel model named GateNet which introduces either the feature embedding gate or the hidden gate to the embedding layer or hidden layers of DNN CTR models, respectively. The feature embedding gate provides a learnable feature gating module to select salient latent information from the feature-level. The hidden gate helps the model to implicitly capture the high-order interaction more effectively. Extensive experiments conducted on three real-world datasets demonstrate its effectiveness to boost the performance of various state-of-the-art models such as FM, DeepFM and xDeepFM on all datasets.",0
"Many Internet companies, including Facebook, rely on advertising and feed ranking. One crucial aspect of real-world advertising and feed ranking systems is predicting click through rate (CTR). Neural network-based CTR models, such as Factorization-Machine Supported Neural Networks, DeepFM, and xDeepFM, have been successful and often include embedding and MLP hidden layers. Gating mechanisms, commonly used in computer vision and natural language processing, have been shown to improve the trainability of deep neural networks. Building on this research, we introduce GateNet, a novel model that incorporates either a feature embedding gate or a hidden gate to enhance the saliency of latent information and capture high-order interactions more effectively. Our experiments demonstrate that GateNet improves the performance of state-of-the-art models like FM, DeepFM, and xDeepFM across all datasets.",1
"Visual question answering is concerned with answering free-form questions about an image. Since it requires a deep linguistic understanding of the question and the ability to associate it with various objects that are present in the image, it is an ambitious task and requires techniques from both computer vision and natural language processing. We propose a novel method that approaches the task by performing context-driven, sequential reasoning based on the objects and their semantic and spatial relationships present in the scene. As a first step, we derive a scene graph which describes the objects in the image, as well as their attributes and their mutual relationships. A reinforcement agent then learns to autonomously navigate over the extracted scene graph to generate paths, which are then the basis for deriving answers. We conduct a first experimental study on the challenging GQA dataset with manually curated scene graphs, where our method almost reaches the level of human performance.",0
"The objective of visual question answering is to answer open-ended queries related to an image. To accomplish this, a comprehensive comprehension of the question and the ability to link it with different objects present in the image is required, making it a challenging task that necessitates techniques from both computer vision and natural language processing. Our innovative approach to this task involves carrying out context-driven, sequential reasoning based on the objects and their semantic and spatial relationships in the scene. We begin by creating a scene graph that details the objects in the image, along with their attributes and relationships. Subsequently, a reinforcement agent learns to navigate through the scene graph to create paths that serve as the basis for generating answers. Using the GQA dataset, we conducted an initial experimental study using manually curated scene graphs, and our method nearly matched human-level performance.",1
"Graph representation learning has emerged as a powerful technique for addressing real-world problems. Various downstream graph learning tasks have benefited from its recent developments, such as node classification, similarity search, and graph classification. However, prior arts on graph representation learning focus on domain specific problems and train a dedicated model for each graph dataset, which is usually non-transferable to out-of-domain data. Inspired by the recent advances in pre-training from natural language processing and computer vision, we design Graph Contrastive Coding (GCC) -- a self-supervised graph neural network pre-training framework -- to capture the universal network topological properties across multiple networks. We design GCC's pre-training task as subgraph instance discrimination in and across networks and leverage contrastive learning to empower graph neural networks to learn the intrinsic and transferable structural representations. We conduct extensive experiments on three graph learning tasks and ten graph datasets. The results show that GCC pre-trained on a collection of diverse datasets can achieve competitive or better performance to its task-specific and trained-from-scratch counterparts. This suggests that the pre-training and fine-tuning paradigm presents great potential for graph representation learning.",0
"The use of graph representation learning has become a powerful tool for solving real-world issues. Several downstream graph learning tasks, such as node classification, similarity search, and graph classification, have benefited from its recent advancements. However, previous studies on graph representation learning have focused on domain-specific problems and have trained dedicated models for each graph dataset, which are not transferable to out-of-domain data. Drawing inspiration from recent progress in pre-training from natural language processing and computer vision, we have created Graph Contrastive Coding (GCC), a self-supervised graph neural network pre-training framework. GCC aims to capture universal network topological properties across multiple networks. We have designed GCC's pre-training task as subgraph instance discrimination in and across networks and have utilized contrastive learning to enable graph neural networks to learn intrinsic and transferable structural representations. We have conducted extensive experiments on ten graph datasets and three graph learning tasks. Our results indicate that GCC pre-trained on a diverse set of datasets can achieve competitive or better performance than its task-specific and trained-from-scratch counterparts. These findings suggest that the pre-training and fine-tuning approach holds great potential for graph representation learning.",1
"Machine learning models tend to over-rely on statistical shortcuts. These spurious correlations between parts of the input and the output labels does not hold in real-world settings. We target this issue on the recent open-ended visual counting task which is well suited to study statistical shortcuts. We aim to develop models that learn a proper mechanism of counting regardless of the output label. First, we propose the Modifying Count Distribution (MCD) protocol, which penalizes models that over-rely on statistical shortcuts. It is based on pairs of training and testing sets that do not follow the same count label distribution such as the odd-even sets. Intuitively, models that have learned a proper mechanism of counting on odd numbers should perform well on even numbers. Secondly, we introduce the Spatial Counting Network (SCN), which is dedicated to visual analysis and counting based on natural language questions. Our model selects relevant image regions, scores them with fusion and self-attention mechanisms, and provides a final counting score. We apply our protocol on the recent dataset, TallyQA, and show superior performances compared to state-of-the-art models. We also demonstrate the ability of our model to select the correct instances to count in the image. Code and datasets are available: https://github.com/cdancette/spatial-counting-network",0
"The issue with machine learning models is that they rely too heavily on statistical shortcuts, leading to spurious correlations between input and output labels that don't hold up in real-world settings. To address this problem, we are focusing on the open-ended visual counting task as a means of studying these shortcuts. Our goal is to develop models that can count properly, regardless of the output label. To accomplish this, we have introduced the Modifying Count Distribution (MCD) protocol, which penalizes models that rely too much on statistical shortcuts by training them on sets with different count label distributions. We have also introduced the Spatial Counting Network (SCN), which is designed for visual analysis and counting using natural language questions. Our model selects relevant image regions, scores them with self-attention mechanisms, and provides a final counting score. We have applied our protocol to the TallyQA dataset and have shown that our model performs significantly better than state-of-the-art models. We have also demonstrated our model's ability to select the correct instances to count in an image. Our code and datasets are available on GitHub at https://github.com/cdancette/spatial-counting-network.",1
"Image retrieval with natural language feedback offers the promise of catalog search based on fine-grained visual features that go beyond objects and binary attributes, facilitating real-world applications such as e-commerce. Our Modality-Agnostic Attention Fusion (MAAF) model combines image and text features and outperforms existing approaches on two visual search with modifying phrase datasets, Fashion IQ and CSS, and performs competitively on a dataset with only single-word modifications, Fashion200k. We also introduce two new challenging benchmarks adapted from Birds-to-Words and Spot-the-Diff, which provide new settings with rich language inputs, and we show that our approach without modification outperforms strong baselines. To better understand our model, we conduct detailed ablations on Fashion IQ and provide visualizations of the surprising phenomenon of words avoiding ""attending"" to the image region they refer to.",0
"The potential of natural language feedback in image retrieval is to enhance catalog searches by utilizing detailed visual features that go beyond just objects and binary attributes. This advancement has real-world applications, especially in e-commerce. Our Modality-Agnostic Attention Fusion (MAAF) model combines both image and text features, surpassing existing methods on two datasets, Fashion IQ and CSS, that require modifying phrases for visual search. Our model also performs competitively on a dataset with only single-word modifications, Fashion200k. Additionally, we introduce two new challenging benchmarks adapted from Birds-to-Words and Spot-the-Diff, which provide complex language inputs, and prove that our approach outperforms strong baselines without modification. To gain a better understanding of our model, we conduct detailed analyses on Fashion IQ and provide visualizations of the peculiar phenomenon of words avoiding attention to the image region they reference.",1
"Emergent communication in artificial agents has been studied to understand language evolution, as well as to develop artificial systems that learn to communicate with humans. We show that agents performing a cooperative navigation task in various gridworld environments learn an interpretable communication protocol that enables them to efficiently, and in many cases, optimally, solve the task. An analysis of the agents' policies reveals that emergent signals spatially cluster the state space, with signals referring to specific locations and spatial directions such as ""left"", ""up"", or ""upper left room"". Using populations of agents, we show that the emergent protocol has basic compositional structure, thus exhibiting a core property of natural language.",0
"The study of emergent communication in artificial agents has two main purposes: understanding language evolution and creating artificial systems that can communicate effectively with humans. In our research, we found that agents working together to navigate various gridworld environments learn a communication protocol that is easy to understand and helps them efficiently and often optimally complete the task. Our analysis of the agents' policies shows that the emergent signals are clustered in specific areas of the state space and refer to certain locations and spatial directions, such as ""left,"" ""up,"" or ""upper left room."" By using groups of agents, we demonstrate that the emergent protocol has a basic compositional structure, which is a fundamental property of natural language.",1
"The Transformer is widely used in natural language processing tasks. To train a Transformer however, one usually needs a carefully designed learning rate warm-up stage, which is shown to be crucial to the final performance but will slow down the optimization and bring more hyper-parameter tunings. In this paper, we first study theoretically why the learning rate warm-up stage is essential and show that the location of layer normalization matters. Specifically, we prove with mean field theory that at initialization, for the original-designed Post-LN Transformer, which places the layer normalization between the residual blocks, the expected gradients of the parameters near the output layer are large. Therefore, using a large learning rate on those gradients makes the training unstable. The warm-up stage is practically helpful for avoiding this problem. On the other hand, our theory also shows that if the layer normalization is put inside the residual blocks (recently proposed as Pre-LN Transformer), the gradients are well-behaved at initialization. This motivates us to remove the warm-up stage for the training of Pre-LN Transformers. We show in our experiments that Pre-LN Transformers without the warm-up stage can reach comparable results with baselines while requiring significantly less training time and hyper-parameter tuning on a wide range of applications.",0
"The Transformer is commonly utilized in natural language processing tasks, but its training typically requires a meticulously designed learning rate warm-up stage. This stage is critical for achieving optimal performance, but it can slow down optimization and necessitate more hyper-parameter adjustments. In this study, we investigate the importance of the learning rate warm-up phase through a theoretical analysis and demonstrate that the placement of layer normalization is a crucial factor. Specifically, our mean field theory indicates that the expected gradients of the parameters near the output layer are large for the original Post-LN Transformer, which has layer normalization between the residual blocks, making training unstable with a large learning rate. The warm-up stage is thus beneficial to avoid this issue. However, we also show that if layer normalization is placed inside the residual blocks, as in the recently proposed Pre-LN Transformer, the gradients are well-behaved at initialization. This finding motivates us to remove the warm-up stage for training Pre-LN Transformers, which we demonstrate in our experiments yields comparable results to baselines while requiring less time and hyper-parameter tuning across a broad range of applications.",1
"Video captioning works on the two fundamental concepts, feature detection and feature composition. While modern day transformers are beneficial in composing features, they lack the fundamental problems of selecting and understanding of the contents. As the feature length increases, it becomes increasingly important to include provisions for improved capturing of the pertinent contents. In this work, we have introduced a new concept of Self-Aware Composition Transformer (SACT) that is capable of generating Multinomial Attention (MultAtt) which is a way of generating distributions of various combinations of frames. Also, multi-head attention transformer works on the principle of combining all possible contents for attention, which is good for natural language classification, but has limitations for video captioning. Video contents have repetitions and require parsing of important contents for better content composition. In this work, we have introduced SACT for more selective attention and combined them for different attention heads for better capturing of the usable contents for any applications. To address the problem of diversification and encourage selective utilization, we propose the Self-Aware Composition Transformer model for dense video captioning and apply the technique on two benchmark datasets like ActivityNet and YouCookII.",0
"The core concepts of video captioning involve feature detection and feature composition. While modern transformers are useful in feature composition, they struggle with recognizing and comprehending the contents. As the length of the features increases, it becomes crucial to capture the relevant contents accurately. This research presents the Self-Aware Composition Transformer (SACT), which creates Multinomial Attention (MultAtt) to generate distributions of various frame combinations. The multi-head attention transformer is insufficient for video captioning due to the repetition of contents and the need for parsing essential information for better composition. SACT provides more selective attention and combines the results from different attention heads to capture the most useful contents for any application. To address diversification and encourage selective utilization, the Self-Aware Composition Transformer model is proposed for dense video captioning. The technique is applied and evaluated on two benchmark datasets, ActivityNet and YouCookII.",1
"State-of-the-art machine learning methods exhibit limited compositional generalization. At the same time, there is a lack of realistic benchmarks that comprehensively measure this ability, which makes it challenging to find and evaluate improvements. We introduce a novel method to systematically construct such benchmarks by maximizing compound divergence while guaranteeing a small atom divergence between train and test sets, and we quantitatively compare this method to other approaches for creating compositional generalization benchmarks. We present a large and realistic natural language question answering dataset that is constructed according to this method, and we use it to analyze the compositional generalization ability of three machine learning architectures. We find that they fail to generalize compositionally and that there is a surprisingly strong negative correlation between compound divergence and accuracy. We also demonstrate how our method can be used to create new compositionality benchmarks on top of the existing SCAN dataset, which confirms these findings.",0
"The ability of advanced machine learning techniques to generalize in a compositional manner is limited. However, the absence of practical benchmarks that can thoroughly assess this capability makes it difficult to identify and assess any improvements. To address this issue, we propose a new technique to systematically create benchmarks by maximizing compound divergence while minimizing atom divergence between training and testing sets. We compare this method quantitatively to other methods for developing benchmarks that assess compositional generalization. We introduce a large natural language question answering dataset that is created using this method. We use this dataset to evaluate the compositional generalization of three machine learning architectures and find that they are unable to generalize in a compositional manner. We also discover a strong negative correlation between compound divergence and accuracy. Our method can also be used to introduce new compositionality benchmarks based on the existing SCAN dataset, which supports our findings.",1
"We propose Pixel-BERT to align image pixels with text by deep multi-modal transformers that jointly learn visual and language embedding in a unified end-to-end framework. We aim to build a more accurate and thorough connection between image pixels and language semantics directly from image and sentence pairs instead of using region-based image features as the most recent vision and language tasks. Our Pixel-BERT which aligns semantic connection in pixel and text level solves the limitation of task-specific visual representation for vision and language tasks. It also relieves the cost of bounding box annotations and overcomes the unbalance between semantic labels in visual task and language semantic. To provide a better representation for down-stream tasks, we pre-train a universal end-to-end model with image and sentence pairs from Visual Genome dataset and MS-COCO dataset. We propose to use a random pixel sampling mechanism to enhance the robustness of visual representation and to apply the Masked Language Model and Image-Text Matching as pre-training tasks. Extensive experiments on downstream tasks with our pre-trained model show that our approach makes the most state-of-the-arts in downstream tasks, including Visual Question Answering (VQA), image-text retrieval, Natural Language for Visual Reasoning for Real (NLVR). Particularly, we boost the performance of a single model in VQA task by 2.17 points compared with SOTA under fair comparison.",0
"Our proposal, Pixel-BERT, employs deep multi-modal transformers that jointly learn visual and language embeddings to align image pixels with text in a unified end-to-end framework. Instead of relying on region-based image features, we directly establish a more precise and comprehensive connection between image pixels and language semantics using image and sentence pairs. By doing so, Pixel-BERT overcomes the limitations of task-specific visual representations for vision and language tasks, eliminates the need for bounding box annotations, and addresses the imbalance between semantic labels in visual and language tasks. To improve the representation for downstream tasks, we pre-train a universal model with image and sentence pairs from the Visual Genome and MS-COCO datasets. We use a random pixel sampling mechanism to enhance visual representation and apply the Masked Language Model and Image-Text Matching as pre-training tasks. Our approach outperforms the state-of-the-art in downstream tasks such as Visual Question Answering, image-text retrieval, and Natural Language for Visual Reasoning for Real. Notably, we enhance the performance of a single model in VQA task by 2.17 points under fair comparison.",1
"Neural Networks (NNs) have provided state-of-the-art results for many challenging machine learning tasks such as detection, regression and classification across the domains of computer vision, speech recognition and natural language processing. Despite their success, they are often implemented in a frequentist scheme, meaning they are unable to reason about uncertainty in their predictions. This article introduces Bayesian Neural Networks (BNNs) and the seminal research regarding their implementation. Different approximate inference methods are compared, and used to highlight where future research can improve on current methods.",0
"The use of Neural Networks (NNs) has yielded remarkable outcomes in various challenging machine learning tasks, including detection, regression, and classification in computer vision, speech recognition, and natural language processing. Although they have proved to be successful, their typical implementation is frequentist, which means they cannot deliberate uncertainties in their predictions. This article presents Bayesian Neural Networks (BNNs) and the groundbreaking studies on their implementation. It compares various approximate inference methods and identifies areas where future research can enhance current methods.",1
"Image captioning is a multimodal problem that has drawn extensive attention in both the natural language processing and computer vision community. In this paper, we present a novel image captioning architecture to better explore semantics available in captions and leverage that to enhance both image representation and caption generation. Our models first construct caption-guided visual relationship graphs that introduce beneficial inductive bias using weakly supervised multi-instance learning. The representation is then enhanced with neighbouring and contextual nodes with their textual and visual features. During generation, the model further incorporates visual relationships using multi-task learning for jointly predicting word and object/predicate tag sequences. We perform extensive experiments on the MSCOCO dataset, showing that the proposed framework significantly outperforms the baselines, resulting in the state-of-the-art performance under a wide range of evaluation metrics.",0
"The problem of image captioning has captured the interest of both the natural language processing and computer vision communities. This paper introduces a new architecture for image captioning that takes advantage of semantic information in captions to improve image representation and caption generation. To achieve this, our model creates visual relationship graphs guided by captions, employing weakly supervised multi-instance learning to introduce inductive bias. We also incorporate neighbouring and contextual nodes with their visual and textual characteristics to enhance the representation. During generation, the model employs multi-task learning to predict word and object/predicate tag sequences jointly, using visual relationships. We evaluate our framework on the MSCOCO dataset, demonstrating significant improvement over baselines and achieving state-of-the-art performance across multiple metrics.",1
"Self-attention has emerged as a vital component of state-of-the-art sequence-to-sequence models for natural language processing in recent years, brought to the forefront by pre-trained bi-directional Transformer models. Its effectiveness is partly due to its non-sequential architecture, which promotes scalability and parallelism but limits the model to inputs of a bounded length. In particular, such architectures perform poorly on algorithmic tasks, where the model must learn a procedure which generalizes to input lengths unseen in training, a capability we refer to as inductive generalization. Identifying the computational limits of existing self-attention mechanisms, we propose I-BERT, a bi-directional Transformer that replaces positional encodings with a recurrent layer. The model inductively generalizes on a variety of algorithmic tasks where state-of-the-art Transformer models fail to do so. We also test our method on masked language modeling tasks where training and validation sets are partitioned to verify inductive generalization. Out of three algorithmic and two natural language inductive generalization tasks, I-BERT achieves state-of-the-art results on four tasks.",0
"In recent times, self-attention has become a critical element in advanced sequence-to-sequence models utilized in natural language processing. This has been primarily due to the pre-trained bi-directional Transformer models. The non-sequential design of self-attention promotes scalability and parallelism, but it has limitations in terms of input length. Consequently, such architectures perform poorly in algorithmic tasks, where the model must learn a procedure that can be applied to input lengths that it has not seen during training. This ability is called inductive generalization. To address the computational limits of existing self-attention mechanisms, we introduce I-BERT, a bi-directional Transformer that utilizes a recurrent layer instead of positional encodings. Our model demonstrates inductive generalization on various algorithmic tasks, where state-of-the-art Transformer models fail to do so. In addition, we evaluate our approach on masked language modeling tasks, where we partition the training and validation sets to verify inductive generalization. I-BERT outperforms other models on four out of three algorithmic and two natural language inductive generalization tasks.",1
"Pretraining has become a standard technique in computer vision and natural language processing, which usually helps to improve performance substantially. Previously, the most dominant pretraining method is transfer learning (TL), which uses labeled data to learn a good representation network. Recently, a new pretraining approach -- self-supervised learning (SSL) -- has demonstrated promising results on a wide range of applications. SSL does not require annotated labels. It is purely conducted on input data by solving auxiliary tasks defined on the input data examples. The current reported results show that in certain applications, SSL outperforms TL and the other way around in other applications. There has not been a clear understanding on what properties of data and tasks render one approach outperforms the other. Without an informed guideline, ML researchers have to try both methods to find out which one is better empirically. It is usually time-consuming to do so. In this work, we aim to address this problem. We perform a comprehensive comparative study between SSL and TL regarding which one works better under different properties of data and tasks, including domain difference between source and target tasks, the amount of pretraining data, class imbalance in source data, and usage of target data for additional pretraining, etc. The insights distilled from our comparative studies can help ML researchers decide which method to use based on the properties of their applications.",0
"Pretraining is a widely used technique in computer vision and natural language processing to improve performance. Transfer learning (TL) has been the most popular pretraining method, which involves using labeled data to learn a representation network. However, self-supervised learning (SSL) has emerged as a promising new approach that does not require annotated labels and involves solving auxiliary tasks on input data examples. The effectiveness of SSL and TL varies across different applications, and there is no clear understanding of which approach is better for specific data and tasks. This leads to a time-consuming trial and error process for ML researchers. This study aims to address this problem by conducting a comprehensive comparative analysis of SSL and TL under different properties of data and tasks. The insights gained from this study can help ML researchers make informed decisions about which method to use for their specific applications.",1
"Differentially private stochastic gradient descent (DPSGD) is a variation of stochastic gradient descent based on the Differential Privacy (DP) paradigm which can mitigate privacy threats arising from the presence of sensitive information in training data. One major drawback of training deep neural networks with DPSGD is a reduction in the model's accuracy. In this paper, we propose an alternative method for preserving data privacy based on introducing noise through learnable probability distributions, which leads to a significant improvement in the utility of the resulting private models. We also demonstrate that normalization layers have a large beneficial impact on the performance of deep neural networks with noisy parameters. In particular, we show that contrary to general belief, a large amount of random noise can be added to the weights of neural networks without harming the performance, once the networks are augmented with normalization layers. We hypothesize that this robustness is a consequence of the scale invariance property of normalization operators. Building on these observations, we propose a new algorithmic technique for training deep neural networks under very low privacy budgets by sampling weights from Gaussian distributions and utilizing batch or layer normalization techniques to prevent performance degradation. Our method outperforms previous approaches, including DPSGD, by a substantial margin on a comprehensive set of experiments on Computer Vision and Natural Language Processing tasks. In particular, we obtain a 20 percent accuracy improvement over DPSGD on the MNIST and CIFAR10 datasets with DP-privacy budgets of $\varepsilon = 0.05$ and $\varepsilon = 2.0$, respectively. Our code is available online: https://github.com/uds-lsv/SIDP.",0
"Differential Privacy (DP) is a paradigm that addresses potential privacy risks caused by sensitive information in training data. One variation of stochastic gradient descent based on DP is Differentially Private Stochastic Gradient Descent (DPSGD). However, this method may reduce the accuracy of deep neural networks. In this paper, we suggest a new approach to protect data privacy by using learnable probability distributions to introduce noise. This method leads to a notable improvement in the utility of private models. We also show that normalization layers have a significant positive impact on the performance of deep neural networks with noisy parameters. This is contrary to the belief that adding a large amount of random noise to neural network weights can harm performance. We propose a new algorithmic technique that involves sampling weights from Gaussian distributions and utilizing batch or layer normalization techniques to prevent performance degradation. Our method outperforms previous approaches, including DPSGD, by a significant margin in Computer Vision and Natural Language Processing tasks. For instance, on the MNIST and CIFAR10 datasets, we achieve a 20 percent accuracy improvement over DPSGD with DP-privacy budgets of $\varepsilon = 0.05$ and $\varepsilon = 2.0$, respectively. We have made our code available online at https://github.com/uds-lsv/SIDP.",1
"Recent work has shown that topological enhancements to recurrent neural networks (RNNs) can increase their expressiveness and representational capacity. Two popular enhancements are stacked RNNs, which increases the capacity for learning non-linear functions, and bidirectional processing, which exploits acausal information in a sequence. In this work, we explore the delayed-RNN, which is a single-layer RNN that has a delay between the input and output. We prove that a weight-constrained version of the delayed-RNN is equivalent to a stacked-RNN. We also show that the delay gives rise to partial acausality, much like bidirectional networks. Synthetic experiments confirm that the delayed-RNN can mimic bidirectional networks, solving some acausal tasks similarly, and outperforming them in others. Moreover, we show similar performance to bidirectional networks in a real-world natural language processing task. These results suggest that delayed-RNNs can approximate topologies including stacked RNNs, bidirectional RNNs, and stacked bidirectional RNNs - but with equivalent or faster runtimes for the delayed-RNNs.",0
"Recent research has indicated that adding topological features to recurrent neural networks (RNNs) can enhance their representational capacity and expressiveness. Stacked RNNs and bidirectional processing are two commonly used enhancements that expand the capacity for non-linear function learning and exploit acausal information within sequences. This study explores the delayed-RNN, a single-layer RNN that has a time delay between input and output. By demonstrating that a constrained version of the delayed-RNN is equivalent to a stacked-RNN, we establish that the delay creates partial acausality, akin to bidirectional networks. Synthetic experiments confirm that the delayed-RNN can replicate bidirectional networks, resolving some acausal tasks comparably while surpassing them in others. Furthermore, we achieve comparable performance to bidirectional networks in a natural language processing task. These findings suggest that delayed-RNNs are capable of approximating topologies such as stacked RNNs, bidirectional RNNs, and stacked bidirectional RNNs, while having similar or faster runtimes than the delayed-RNNs.",1
"We present a new large-scale multilingual video description dataset, VATEX, which contains over 41,250 videos and 825,000 captions in both English and Chinese. Among the captions, there are over 206,000 English-Chinese parallel translation pairs. Compared to the widely-used MSR-VTT dataset, VATEX is multilingual, larger, linguistically complex, and more diverse in terms of both video and natural language descriptions. We also introduce two tasks for video-and-language research based on VATEX: (1) Multilingual Video Captioning, aimed at describing a video in various languages with a compact unified captioning model, and (2) Video-guided Machine Translation, to translate a source language description into the target language using the video information as additional spatiotemporal context. Extensive experiments on the VATEX dataset show that, first, the unified multilingual model can not only produce both English and Chinese descriptions for a video more efficiently, but also offer improved performance over the monolingual models. Furthermore, we demonstrate that the spatiotemporal video context can be effectively utilized to align source and target languages and thus assist machine translation. In the end, we discuss the potentials of using VATEX for other video-and-language research.",0
"We have introduced a novel dataset called VATEX, which comprises more than 41,250 videos and 825,000 captions in English and Chinese. Within this dataset, there are over 206,000 parallel translation pairs between the two languages. Compared to the widely-used MSR-VTT dataset, VATEX is more linguistically complex, diverse in both natural language and video descriptions, and larger. We have also proposed two video-and-language research tasks based on VATEX, namely Multilingual Video Captioning and Video-guided Machine Translation. Our experiments on the VATEX dataset reveal that the unified multilingual model is more efficient in generating both English and Chinese descriptions and performs better than monolingual models. We have also demonstrated that the video context can be effectively utilized to align source and target languages to assist machine translation. Finally, we have discussed the potential uses of VATEX for other video-and-language research.",1
"Large transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses $O(n^2)$ time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from $O(n^2)$ to $O(n)$ in both time and space. The resulting linear transformer, the \textit{Linformer}, performs on par with standard Transformer models, while being much more memory- and time-efficient.",0
"Numerous natural language processing applications have benefited greatly from the use of large transformer models. However, the cost associated with training and deploying these models can be prohibitively high for lengthy sequences due to the standard self-attention mechanism of the Transformer utilizing $O(n^2)$ time and space in relation to sequence length. This study demonstrates that the self-attention mechanism can be approximated by a low-rank matrix and utilizes this finding to propose a new self-attention mechanism, which decreases the overall self-attention complexity to $O(n)$ in both time and space. The resulting linear transformer, referred to as the \textit{Linformer}, performs comparably to standard Transformer models, while being far more efficient in terms of memory and time.",1
"The multinomial and related distributions have long been used to model categorical, count-based data in fields ranging from bioinformatics to natural language processing. Commonly utilized variants include the standard multinomial and the Dirichlet multinomial distributions due to their computational efficiency and straightforward parameter estimation process. However, these distributions make strict assumptions about the mean, variance, and covariance between the categorical features being modeled. If these assumptions are not met by the data, it may result in poor parameter estimates and loss in accuracy for downstream applications like classification. Here, we explore efficient parameter estimation and supervised classification methods using an alternative distribution, called the Beta-Liouville multinomial, which relaxes some of the multinomial assumptions. We show that the Beta-Liouville multinomial is comparable in efficiency to the Dirichlet multinomial for Newton-Raphson maximum likelihood estimation, and that its performance on simulated data matches or exceeds that of the multinomial and Dirichlet multinomial distributions. Finally, we demonstrate that the Beta-Liouville multinomial outperforms the multinomial and Dirichlet multinomial on two out of four gold standard datasets, supporting its use in modeling data with low to medium class overlap in a supervised classification context.",0
"For years, the multinomial and its variants have been employed to model count-based data in various fields, including bioinformatics and natural language processing. The standard multinomial and Dirichlet multinomial distributions are often used due to their computational efficiency and straightforward parameter estimation process. However, these distributions make strict assumptions about the mean, variance, and covariance of the categorical features, which can result in poor parameter estimates and accuracy loss for classification if the data does not meet these assumptions. To address this issue, we investigate an alternative distribution, the Beta-Liouville multinomial, which relaxes some of the multinomial assumptions. We demonstrate that the Beta-Liouville multinomial is as efficient as the Dirichlet multinomial for Newton-Raphson maximum likelihood estimation and performs as well as, or even better than, the standard multinomial and Dirichlet multinomial on simulated data. Additionally, we show that the Beta-Liouville multinomial outperforms the standard multinomial and Dirichlet multinomial on two out of four gold standard datasets, indicating its suitability for modeling low to medium class overlap data in a supervised classification context.",1
"Neural Architecture Search (NAS) is a promising and rapidly evolving research area. Training a large number of neural networks requires an exceptional amount of computational power, which makes NAS unreachable for those researchers who have limited or no access to high-performance clusters and supercomputers. A few benchmarks with precomputed neural architectures performances have been recently introduced to overcome this problem and ensure more reproducible experiments. However, these benchmarks are only for the computer vision domain and, thus, are built from the image datasets and convolution-derived architectures. In this work, we step outside the computer vision domain by leveraging the language modeling task, which is the core of natural language processing (NLP). Our main contribution is as follows: we have provided search space of recurrent neural networks on the text datasets and trained 14k architectures within it; we have conducted both intrinsic and extrinsic evaluation of the trained models using datasets for semantic relatedness and language understanding evaluation; finally, we have tested several NAS algorithms to demonstrate how the precomputed results can be utilized. We believe that our results have high potential of usage for both NAS and NLP communities.",0
"The research area of Neural Architecture Search (NAS) is rapidly developing and holds great promise. However, training numerous neural networks requires significant computational power, making it difficult for researchers without access to high-performance clusters and supercomputers to engage in NAS. Recently, some benchmarks with precomputed neural architectures performances were introduced to address this issue and ensure more reproducible experiments. These benchmarks are only applicable to the computer vision domain, which limits their usefulness. To expand the scope, we focused on the language modeling task, which is fundamental to natural language processing (NLP). Our primary contribution is twofold: we created a search space of recurrent neural networks on text datasets and trained 14k architectures within it. We evaluated the trained models intrinsically and extrinsically using datasets for semantic relatedness and language understanding evaluation. Finally, we tested several NAS algorithms to demonstrate the benefits of precomputed results. We believe that our findings will benefit both NAS and NLP communities.",1
"In the real world, linguistic agents are also embodied agents: they perceive and act in the physical world. The notion of Language Grounding questions the interactions between language and embodiment: how do learning agents connect or ground linguistic representations to the physical world ? This question has recently been approached by the Reinforcement Learning community under the framework of instruction-following agents. In these agents, behavioral policies or reward functions are conditioned on the embedding of an instruction expressed in natural language. This paper proposes another approach: using language to condition goal generators. Given any goal-conditioned policy, one could train a language-conditioned goal generator to generate language-agnostic goals for the agent. This method allows to decouple sensorimotor learning from language acquisition and enable agents to demonstrate a diversity of behaviors for any given instruction. We propose a particular instantiation of this approach and demonstrate its benefits.",0
"Linguistic agents in the real world are embodied and interact with the physical environment. The concept of Language Grounding explores the connection between language and embodiment, specifically how learning agents link linguistic representations to the physical world. The Reinforcement Learning community has studied this question through instruction-following agents, where policies and reward functions are based on natural language instructions. This paper suggests a different approach: conditioning goal generators with language. This method separates language acquisition from sensorimotor learning, allowing agents to exhibit various behaviors for the same instruction. We present a specific implementation of this approach and showcase its advantages.",1
"A challenging open question in deep learning is how to handle tabular data. Unlike domains such as image and natural language processing, where deep architectures prevail, there is still no widely accepted neural architecture that dominates tabular data. As a step toward bridging this gap, we present DNF-Net a novel generic architecture whose inductive bias elicits models whose structure corresponds to logical Boolean formulas in disjunctive normal form (DNF) over affine soft-threshold decision terms. In addition, DNF-Net promotes localized decisions that are taken over small subsets of the features. We present an extensive empirical study showing that DNF-Nets significantly and consistently outperform FCNs over tabular data. With relatively few hyperparameters, DNF-Nets open the door to practical end-to-end handling of tabular data using neural networks. We present ablation studies, which justify the design choices of DNF-Net including the three inductive bias elements, namely, Boolean formulation, locality, and feature selection.",0
"Tabular data poses a difficult challenge for deep learning, as there is no established neural architecture that excels in this domain. In contrast to image and natural language processing, where deep architectures are prevalent, tabular data requires a novel approach. To address this gap, we propose DNF-Net, a new generic architecture that leverages logical Boolean formulas in disjunctive normal form (DNF) over affine soft-threshold decision terms. DNF-Net is designed to promote localized decisions taken over small feature subsets, and we conducted extensive empirical studies to demonstrate its superiority over FCNs in handling tabular data. DNF-Nets require few hyperparameters and are capable of practical end-to-end processing of tabular data with neural networks. Ablation studies support the three inductive bias elements of DNF-Net, namely, Boolean formulation, locality, and feature selection.",1
"This paper introduces RTEx, a novel methodology for a) ranking radiography exams based on their probability to contain an abnormality, b) generating abnormality tags for abnormal exams, and c) providing a diagnostic explanation in natural language for each abnormal exam. The task of ranking radiography exams is an important first step for practitioners who want to identify and prioritize those radiography exams that are more likely to contain abnormalities, for example, to avoid mistakes due to tiredness or to manage heavy workload (e.g., during a pandemic). We used two publicly available datasets to assess our methodology and demonstrate that for the task of ranking it outperforms its competitors in terms of NDCG@k. For each abnormal radiography exam RTEx generates a set of abnormality tags alongside an explanatory diagnostic text to explain the tags and guide the medical expert. Our tagging component outperforms two strong competitor methods in terms of F1. Moreover, the diagnostic captioning component of RTEx, which exploits the already extracted tags to constrain the captioning process, outperforms all competitors with respect to clinical precision and recall.",0
"In this paper, a new methodology called RTEx is introduced. Its main objectives are to rank radiography exams based on their probability of containing an abnormality, provide abnormality tags for abnormal exams, and offer a diagnostic explanation in natural language for each abnormal exam. Ranking radiography exams is crucial for practitioners who wish to prioritize the exams that are more likely to contain abnormalities to avoid mistakes due to fatigue or manage heavy workload during a pandemic. To evaluate the effectiveness of RTEx, two publicly available datasets were used. The results show that RTEx outperforms its competitors in terms of NDCG@k for the task of ranking. In addition, RTEx generates a set of abnormality tags and an explanatory diagnostic text for every abnormal radiography exam. The tagging component of RTEx outperforms two strong competitor methods in terms of F1. Furthermore, the diagnostic captioning component of RTEx, which utilizes the already extracted tags to guide the captioning process, outperforms all competitors in terms of clinical precision and recall.",1
"Although stochastic optimization is central to modern machine learning, the precise mechanisms underlying its success, and in particular, the precise role of the stochasticity, still remain unclear. Modelling stochastic optimization algorithms as discrete random recurrence relations, we show that multiplicative noise, as it commonly arises due to variance in local rates of convergence, results in heavy-tailed stationary behaviour in the parameters. A detailed analysis is conducted for SGD applied to a simple linear regression problem, followed by theoretical results for a much larger class of models (including non-linear and non-convex) and optimizers (including momentum, Adam, and stochastic Newton), demonstrating that our qualitative results hold much more generally. In each case, we describe dependence on key factors, including step size, batch size, and data variability, all of which exhibit similar qualitative behavior to recent empirical results on state-of-the-art neural network models from computer vision and natural language processing. Furthermore, we empirically demonstrate how multiplicative noise and heavy-tailed structure improve capacity for basin hopping and exploration of non-convex loss surfaces, over commonly-considered stochastic dynamics with only additive noise and light-tailed structure.",0
"Stochastic optimization plays a central role in modern machine learning, but there is still uncertainty surrounding the specific reasons for its success and the role of stochasticity. Using discrete random recurrence relations, we model stochastic optimization algorithms and find that multiplicative noise, resulting from variance in local rates of convergence, leads to heavy-tailed stationary behavior in parameters. We examine SGD applied to a simple linear regression problem and provide theoretical results for a broader class of models and optimizers, including non-linear and non-convex functions and momentum, Adam, and stochastic Newton optimizers. Our analysis considers key factors including step size, batch size, and data variability, which exhibit similar behavior to empirical results on state-of-the-art neural network models. Additionally, we demonstrate how multiplicative noise and heavy-tailed structure enhance basin hopping and exploration of non-convex loss surfaces, outperforming stochastic dynamics with only additive noise and light-tailed structure.",1
"Graphs are the natural data structure to represent relational and structural information in many domains. To cover the broad range of graph-data applications including graph classification as well as graph generation, it is desirable to have a general and flexible model consisting of an encoder and a decoder that can handle graph data. Although the representative encoder-decoder model, Transformer, shows superior performance in various tasks especially of natural language processing, it is not immediately available for graphs due to their non-sequential characteristics. To tackle this incompatibility, we propose GRaph-Aware Transformer (GRAT), the first Transformer-based model which can encode and decode whole graphs in end-to-end fashion. GRAT is featured with a self-attention mechanism adaptive to the edge information and an auto-regressive decoding mechanism based on the two-path approach consisting of sub-graph encoding path and node-and-edge generation path for each decoding step. We empirically evaluated GRAT on multiple setups including encoder-based tasks such as molecule property predictions on QM9 datasets and encoder-decoder-based tasks such as molecule graph generation in the organic molecule synthesis domain. GRAT has shown very promising results including state-of-the-art performance on 4 regression tasks in QM9 benchmark.",0
"In many fields, graphs are a natural way to display relational and structural information. To handle a wide range of graph-related tasks, such as classification and generation, it is necessary to have a versatile and all-encompassing model that includes both an encoder and a decoder. Although the Transformer model is well-suited for natural language processing, it is not immediately applicable to graphs due to their non-sequential nature. To address this issue, we have developed the GRaph-Aware Transformer (GRAT), the first Transformer-based model capable of encoding and decoding entire graphs in a single step. GRAT features a self-attention mechanism that adapts to edge data and an auto-regressive decoding mechanism that employs a two-path approach consisting of sub-graph encoding and node-and-edge generation. We have empirically tested GRAT in a variety of settings, including molecule property prediction and graph generation in organic molecule synthesis. Our results demonstrate that GRAT performs exceptionally well, achieving state-of-the-art performance in four QM9 benchmark regression tasks.",1
"By describing the features and abstractions of our world, language is a crucial tool for human learning and a promising source of supervision for machine learning models. We use language to improve few-shot visual classification in the underexplored scenario where natural language task descriptions are available during training, but unavailable for novel tasks at test time. Existing models for this setting sample new descriptions at test time and use those to classify images. Instead, we propose language-shaped learning (LSL), an end-to-end model that regularizes visual representations to predict language. LSL is conceptually simpler, more data efficient, and outperforms baselines in two challenging few-shot domains.",0
"Language is a vital instrument for human learning as it elucidates the characteristics and concepts of our environment. Furthermore, it possesses the potential to serve as a supervisory resource for machine learning models. In the situation where natural language tasks are accessible during training but not during novel tasks at test time, language is used to enhance few-shot visual classification. Current models for this scenario generate new descriptions during testing and utilize them to categorize images. However, we introduce an alternative approach called language-shaped learning (LSL), which is an end-to-end model that regulates visual representations to anticipate language. LSL is more straightforward in concept, more efficient in data usage, and outperforms established methods in two challenging few-shot domains.",1
"Semantic code search is the task of retrieving relevant code given a natural language query. While related to other information retrieval tasks, it requires bridging the gap between the language used in code (often abbreviated and highly technical) and natural language more suitable to describe vague concepts and ideas.   To enable evaluation of progress on code search, we are releasing the CodeSearchNet Corpus and are presenting the CodeSearchNet Challenge, which consists of 99 natural language queries with about 4k expert relevance annotations of likely results from CodeSearchNet Corpus. The corpus contains about 6 million functions from open-source code spanning six programming languages (Go, Java, JavaScript, PHP, Python, and Ruby). The CodeSearchNet Corpus also contains automatically generated query-like natural language for 2 million functions, obtained from mechanically scraping and preprocessing associated function documentation. In this article, we describe the methodology used to obtain the corpus and expert labels, as well as a number of simple baseline solutions for the task.   We hope that CodeSearchNet Challenge encourages researchers and practitioners to study this interesting task further and will host a competition and leaderboard to track the progress on the challenge. We are also keen on extending CodeSearchNet Challenge to more queries and programming languages in the future.",0
"The task of semantic code search involves finding relevant code based on natural language queries. Although it is similar to other information retrieval tasks, it requires bridging the gap between technical code language and natural language that is more appropriate for describing abstract concepts. To facilitate progress assessment in code search, the CodeSearchNet Corpus is being released, along with the CodeSearchNet Challenge. This challenge includes 99 natural language queries with around 4k expert relevance annotations, drawn from the CodeSearchNet Corpus. The corpus contains about 6 million functions from six programming languages (Go, Java, JavaScript, PHP, Python, and Ruby), as well as 2 million functions with query-like natural language obtained from associated function documentation. This article details the methodology used to create the corpus and expert labels, as well as some simple baseline solutions for the task. The CodeSearchNet Challenge is intended to encourage further research and practical application of this task, with a competition and leaderboard to monitor progress. There are plans to extend the challenge to more queries and programming languages in the future.",1
"We propose a multi-head attention mechanism as a blending layer in a neural network model that translates natural language to a high level behavioral language for indoor robot navigation. We follow the framework established by (Zang et al., 2018a) that proposes the use of a navigation graph as a knowledge base for the task. Our results show significant performance gains when translating instructions on previously unseen environments, therefore, improving the generalization capabilities of the model.",0
"Our suggestion is to incorporate a multi-head attention mechanism into a neural network architecture to facilitate the translation of natural language into high-level behavioral language for indoor robot navigation. We adopt the approach presented by (Zang et al., 2018a) that advocates for utilizing a navigation graph as a basis of knowledge for the task. Our findings indicate that our model achieves notable advancements in performance when processing instructions in unfamiliar settings, which enhances its overall ability to generalize.",1
"Learning disentangled representations of natural language is essential for many NLP tasks, e.g., conditional text generation, style transfer, personalized dialogue systems, etc. Similar problems have been studied extensively for other forms of data, such as images and videos. However, the discrete nature of natural language makes the disentangling of textual representations more challenging (e.g., the manipulation over the data space cannot be easily achieved). Inspired by information theory, we propose a novel method that effectively manifests disentangled representations of text, without any supervision on semantics. A new mutual information upper bound is derived and leveraged to measure dependence between style and content. By minimizing this upper bound, the proposed method induces style and content embeddings into two independent low-dimensional spaces. Experiments on both conditional text generation and text-style transfer demonstrate the high quality of our disentangled representation in terms of content and style preservation.",0
"Developing disentangled representations for natural language is crucial for various NLP tasks like personalized dialogue systems, style transfer, and conditional text generation. While similar problems have been extensively studied for images and videos, the discrete nature of natural language poses a greater challenge in disentangling textual representations. This is because manipulating the data space is not as straightforward. Drawing inspiration from information theory, we propose a new method that effectively produces disentangled representations of text, without requiring any supervision on semantics. We use a novel mutual information upper bound to measure the relationship between style and content. By minimizing this upper bound, our method induces independent low-dimensional spaces for style and content embeddings. Through experiments on text-style transfer and conditional text generation, we demonstrate the high quality of our disentangled representation in terms of preserving both style and content.",1
"Grounding free-form textual queries necessitates an understanding of these textual phrases and its relation to the visual cues to reliably reason about the described locations. Spatial attention networks are known to learn this relationship and focus its gaze on salient objects in the image. Thus, we propose to utilize spatial attention networks for image-level visual-textual fusion preserving local (word) and global (phrase) information to refine region proposals with an in-network Region Proposal Network (RPN) and detect single or multiple regions for a phrase query. We focus only on the phrase query - ground truth pair (referring expression) for a model independent of the constraints of the datasets i.e. additional attributes, context etc. For such referring expression dataset ReferIt game, our Multi-region Attention-assisted Grounding network (MAGNet) achieves over 12\% improvement over the state-of-the-art. Without the context from image captions and attribute information in Flickr30k Entities, we still achieve competitive results compared to the state-of-the-art.",0
"To effectively understand free-form textual queries and their relation to visual cues in order to reason about described locations, it is necessary to ground them. Spatial attention networks are capable of learning this relationship and focusing on important objects in an image. Therefore, we suggest the use of spatial attention networks for image-level visual-textual fusion that preserves both local and global information to enhance region proposals with an in-network Region Proposal Network. This allows for the detection of single or multiple regions for a phrase query, while focusing solely on the phrase query-ground truth pair, independent of any dataset constraints such as additional attributes or context. Our Multi-region Attention-assisted Grounding network (MAGNet) achieves over 12% improvement on the ReferIt game dataset, without relying on context from image captions or attribute information in Flickr30k Entities, and still performs competitively compared to the state-of-the-art.",1
"Beyond the common difficulties faced in the natural image captioning, medical report generation specifically requires the model to describe a medical image with a fine-grained and semantic-coherence paragraph that should satisfy both medical commonsense and logic. Previous works generally extract the global image features and attempt to generate a paragraph that is similar to referenced reports; however, this approach has two limitations. Firstly, the regions of primary interest to radiologists are usually located in a small area of the global image, meaning that the remainder parts of the image could be considered as irrelevant noise in the training procedure. Secondly, there are many similar sentences used in each medical report to describe the normal regions of the image, which causes serious data bias. This deviation is likely to teach models to generate these inessential sentences on a regular basis. To address these problems, we propose an Auxiliary Signal-Guided Knowledge Encoder-Decoder (ASGK) to mimic radiologists' working patterns. In more detail, ASGK integrates internal visual feature fusion and external medical linguistic information to guide medical knowledge transfer and learning. The core structure of ASGK consists of a medical graph encoder and a natural language decoder, inspired by advanced Generative Pre-Training (GPT). Experiments on the CX-CHR dataset and our COVID-19 CT Report dataset demonstrate that our proposed ASGK is able to generate a robust and accurate report, and moreover outperforms state-of-the-art methods on both medical terminology classification and paragraph generation metrics.",0
"Medical report generation presents unique challenges in natural image captioning as the model must create a detailed and coherent paragraph that adheres to medical logic and common sense when describing medical images. Previous methods have focused on generating paragraphs that resemble referenced reports by extracting global image features. However, this method has limitations as radiologists are primarily interested in small areas of the image, leaving the rest of the image irrelevant noise. Additionally, many similar sentences are used to describe normal regions of the image, leading to data bias and the generation of unnecessary sentences. To overcome these issues, we propose the Auxiliary Signal-Guided Knowledge Encoder-Decoder (ASGK) that mimics radiologists' working patterns. ASGK integrates internal visual feature fusion and external medical linguistic information to facilitate medical knowledge transfer and learning. The core structure of ASGK comprises a medical graph encoder and a natural language decoder, inspired by advanced Generative Pre-Training (GPT). Our experiments on the CX-CHR dataset and COVID-19 CT Report dataset show that ASGK generates a robust and accurate report, outperforming state-of-the-art methods in medical terminology classification and paragraph generation metrics.",1
"It has witnessed a growing demand for efficient representation learning on point clouds in many 3D computer vision applications. Behind the success story of convolutional neural networks (CNNs) is that the data (e.g., images) are Euclidean structured. However, point clouds are irregular and unordered. Various point neural networks have been developed with isotropic filters or using weighting matrices to overcome the structure inconsistency on point clouds. However, isotropic filters or weighting matrices limit the representation power. In this paper, we propose a permutable anisotropic convolutional operation (PAI-Conv) that calculates soft-permutation matrices for each point using dot-product attention according to a set of evenly distributed kernel points on a sphere's surface and performs shared anisotropic filters. In fact, dot product with kernel points is by analogy with the dot-product with keys in Transformer as widely used in natural language processing (NLP). From this perspective, PAI-Conv can be regarded as the transformer for point clouds, which is physically meaningful and is robust to cooperate with the efficient random point sampling method. Comprehensive experiments on point clouds demonstrate that PAI-Conv produces competitive results in classification and semantic segmentation tasks compared to state-of-the-art methods.",0
"There has been an increasing need for effective representation learning of point clouds in various 3D computer vision applications. Convolutional neural networks (CNNs) have been successful due to the structured nature of the data, such as images, which are Euclidean. However, point clouds are unordered and irregular, making it challenging to develop point neural networks that use isotropic filters or weighting matrices to address the structure inconsistency. Unfortunately, these methods limit the representation power. This paper proposes a solution: the permutable anisotropic convolutional operation (PAI-Conv), which utilizes dot-product attention to calculate soft-permutation matrices for each point relative to a set of evenly distributed kernel points on a sphere's surface and performs shared anisotropic filters. In essence, this is similar to how keys are used in the Transformer model for natural language processing (NLP). PAI-Conv can be considered the transformer for point clouds, which is both physically meaningful and compatible with the efficient random point sampling method. Extensive experiments on point clouds demonstrate that PAI-Conv yields competitive results in classification and semantic segmentation tasks compared to state-of-the-art methods.",1
"Transformer-based models have become ubiquitous in natural language processing thanks to their large capacity, innate parallelism and high performance. The contextualizing component of a Transformer block is the $\textit{pairwise dot-product}$ attention that has a large $\Omega(L^2)$ memory requirement for length $L$ sequences, limiting its ability to process long documents. This has been the subject of substantial interest recently, where multiple approximations were proposed to reduce the quadratic memory requirement using sparse attention matrices. In this work, we propose to augment sparse Transformer blocks with a dense attention-based $\textit{global memory}$ of length $M$ ($\ll L$) which provides an aggregate global view of the entire input sequence to each position. Our augmentation has a manageable $O(M\cdot(L+M))$ memory overhead, and can be seamlessly integrated with prior sparse solutions. Moreover, global memory can also be used for sequence compression, by representing a long input sequence with the memory representations only. We empirically show that our method leads to substantial improvement on a range of tasks, including (a) synthetic tasks that require global reasoning, (b) masked language modeling, and (c) reading comprehension.",0
"Due to their immense capacity, innate parallelism, and superior performance, Transformer-based models have become prevalent in natural language processing. However, the contextualizing component of a Transformer block, namely the pairwise dot-product attention, requires a significant amount of memory (Omega L^2) for sequences of length L, restricting its capability to process extended documents. This has recently piqued widespread interest, and numerous approximations have been suggested to reduce the quadratic memory requirement using sparse attention matrices. In this paper, we propose enhancing sparse Transformer blocks with a dense attention-based global memory of length M (much smaller than L), which provides a comprehensive global perspective of the entire input sequence to every position. Our augmentation has a manageable memory overhead of O(M*(L+M)), and it can be seamlessly integrated with previous sparse solutions. Additionally, global memory can also be employed for sequence compression by exclusively representing a long input sequence with the memory representations. We demonstrate empirically that our approach substantially improves various tasks, such as synthetic tasks necessitating global reasoning, masked language modeling, and reading comprehension.",1
"Detection and description of keypoints from an image is a well-studied problem in Computer Vision. Some methods like SIFT, SURF or ORB are computationally really efficient. This paper proposes a solution for a particular case study on object recognition of industrial parts based on hierarchical classification. Reducing the number of instances leads to better performance, indeed, that is what the use of the hierarchical classification is looking for. We demonstrate that this method performs better than using just one method like ORB, SIFT or FREAK, despite being fairly slower.",0
"The problem of identifying and explaining keypoints in an image has been thoroughly explored in the field of Computer Vision. Certain techniques, such as SIFT, SURF, and ORB, are highly efficient in terms of computation. This study introduces an approach for recognizing industrial components using hierarchical classification. Improved performance is achieved by reducing the number of occurrences, which is the goal of hierarchical classification. Although it is somewhat slower, this method outperforms single techniques like ORB, SIFT, or FREAK.",1
"Text based Visual Question Answering (TextVQA) is a recently raised challenge that requires a machine to read text in images and answer natural language questions by jointly reasoning over the question, Optical Character Recognition (OCR) tokens and visual content. Most of the state-of-the-art (SoTA) VQA methods fail to answer these questions because of i) poor text reading ability; ii) lacking of text-visual reasoning capacity; and iii) adopting a discriminative answering mechanism instead of a generative one which is hard to cover both OCR tokens and general text tokens in the final answer. In this paper, we propose a structured multimodal attention (SMA) neural network to solve the above issues. Our SMA first uses a structural graph representation to encode the object-object, object-text and text-text relationships appearing in the image, and then design a multimodal graph attention network to reason over it. Finally, the outputs from the above module are processed by a global-local attentional answering module to produce an answer that covers tokens from both OCR and general text iteratively. Our proposed model outperforms the SoTA models on TextVQA dataset and all three tasks of ST-VQA dataset. To provide an upper bound for our method and a fair testing base for further works, we also provide human-annotated ground-truth OCR annotations for the TextVQA dataset, which were not given in the original release.",0
"The TextVQA challenge requires machines to read text in images and answer natural language questions by jointly reasoning over the question, OCR tokens and visual content. However, most state-of-the-art VQA methods struggle with this task due to poor text reading ability, a lack of text-visual reasoning capacity, and an adoption of a discriminative answering mechanism that cannot cover both OCR tokens and general text tokens in the final answer. To address these issues, we propose a structured multimodal attention neural network (SMA) that encodes object-object, object-text, and text-text relationships using a structural graph representation and a multimodal graph attention network. The output is then processed by a global-local attentional answering module to produce an answer that covers both OCR and general text tokens iteratively. Our proposed model outperforms existing models on both TextVQA and ST-VQA datasets. To facilitate future research, we also provide human-annotated ground-truth OCR annotations for the TextVQA dataset.",1
"To successfully build a deep learning model, it will need a large amount of labeled data. However, labeled data are hard to collect in many use cases. To tackle this problem, a bunch of data augmentation methods have been introduced recently and have demonstrated successful results in computer vision, natural language and so on. For financial trading data, to our best knowledge, successful data augmentation framework has rarely been studied. Here we propose a Modified Local Search Attack Sampling method to augment the candlestick data, which is a very important tool for professional trader. Our results show that the proposed method can generate high-quality data which are hard to distinguish by human and will open a new way for finance community to employ existing machine learning techniques even if the dataset is small.",0
"Collecting a substantial amount of labeled data is imperative to develop an effective deep learning model, but this is challenging in numerous use cases. However, various data augmentation methods have been recently introduced and have proved to be successful in areas such as computer vision and natural language processing. Nevertheless, there has been minimal research on data augmentation frameworks for financial trading data. To address this issue, we suggest utilizing the Modified Local Search Attack Sampling method to augment candlestick data, an essential tool for professional traders. Our findings reveal that this method can generate high-quality data that is indistinguishable from human-generated data. This approach can provide the finance community with the opportunity to utilize existing machine learning techniques, even when datasets are limited.",1
"Machine Learning and Inference methods have become ubiquitous in our attempt to induce more abstract representations of natural language text, visual scenes, and other messy, naturally occurring data, and support decisions that depend on it. However, learning models for these tasks is difficult partly because generating the necessary supervision signals for it is costly and does not scale. This paper describes several learning paradigms that are designed to alleviate the supervision bottleneck. It will illustrate their benefit in the context of multiple problems, all pertaining to inducing various levels of semantic representations from text.",0
"The use of Machine Learning and Inference techniques has become widespread in our efforts to derive more conceptual interpretations of natural language text, visual scenes, and other complex, real-world data, and to inform decisions based on these interpretations. Nonetheless, developing learning models for these tasks is challenging due to the high cost and limited scalability of generating the required supervision signals. This article presents several learning approaches intended to address this supervision bottleneck, demonstrating their advantages in the context of multiple problems related to generating different types of semantic representations from text.",1
"Automatic search of neural architectures for various vision and natural language tasks is becoming a prominent tool as it allows to discover high-performing structures on any dataset of interest. Nevertheless, on more difficult domains, such as dense per-pixel classification, current automatic approaches are limited in their scope - due to their strong reliance on existing image classifiers they tend to search only for a handful of additional layers with discovered architectures still containing a large number of parameters. In contrast, in this work we propose a novel solution able to find light-weight and accurate segmentation architectures starting from only few blocks of a pre-trained classification network. To this end, we progressively build up a methodology that relies on templates of sets of operations, predicts which template and how many times should be applied at each step, while also generating the connectivity structure and downsampling factors. All these decisions are being made by a recurrent neural network that is rewarded based on the score of the emitted architecture on the holdout set and trained using reinforcement learning. One discovered architecture achieves 63.2% mean IoU on CamVid and 67.8% on CityScapes having only 270K parameters. Pre-trained models and the search code are available at https://github.com/DrSleep/nas-segm-pytorch.",0
"The use of automatic search for neural architectures in various vision and natural language tasks has become a valuable tool for discovering high-performing structures on any dataset of interest. However, current automatic approaches are limited in scope when it comes to more difficult domains, such as dense per-pixel classification. This is due to their heavy reliance on existing image classifiers, which restricts their search to only a few additional layers and results in architectures with a large number of parameters. In contrast, this work introduces a novel solution for finding accurate and lightweight segmentation architectures, using only a few blocks of a pre-trained classification network. The methodology involves using templates of operation sets, predicting the appropriate template and number of applications at each step, and generating the connectivity structure and downsampling factors. A recurrent neural network makes all of these decisions and is rewarded based on the score of the architecture on the holdout set, trained using reinforcement learning. The proposed method discovers an architecture that achieves 63.2% mean IoU on CamVid and 67.8% on CityScapes, with only 270K parameters. The pre-trained models and search code are available at https://github.com/DrSleep/nas-segm-pytorch.",1
"Multi-hop knowledge based question answering (KBQA) is a complex task for natural language understanding. Many KBQA approaches have been proposed in recent years, and most of them are trained based on labeled reasoning path. This hinders the system's performance as many correct reasoning paths are not labeled as ground truth, and thus they cannot be learned. In this paper, we introduce an end-to-end KBQA system which can leverage multiple reasoning paths' information and only requires labeled answer as supervision. We conduct experiments on several benchmark datasets containing both single-hop simple questions as well as muti-hop complex questions, including WebQuestionSP (WQSP), ComplexWebQuestion-1.1 (CWQ), and PathQuestion-Large (PQL), and demonstrate strong performance.",0
"Understanding natural language and answering multi-hop knowledge based questions (KBQA) is a challenging task. In recent years, numerous KBQA approaches have been proposed, but most of them rely on labeled reasoning paths. This approach limits the system's performance as many correct reasoning paths are not labeled and cannot be learned. This paper presents an end-to-end KBQA system that can utilize information from multiple reasoning paths and only needs a labeled answer for supervision. We evaluate our system on several benchmark datasets, including both single-hop simple questions and multi-hop complex questions such as WQSP, CWQ, and PQL, and demonstrate its strong performance.",1
"The ability to take into account the characteristics - also called features - of observations is essential in Natural Language Processing (NLP) problems. Hidden Markov Chain (HMC) model associated with classic Forward-Backward probabilities cannot handle arbitrary features like prefixes or suffixes of any size, except with an independence condition. For twenty years, this default has encouraged the development of other sequential models, starting with the Maximum Entropy Markov Model (MEMM), which elegantly integrates arbitrary features. More generally, it led to neglect HMC for NLP. In this paper, we show that the problem is not due to HMC itself, but to the way its restoration algorithms are computed. We present a new way of computing HMC based restorations using original Entropic Forward and Entropic Backward (EFB) probabilities. Our method allows taking into account features in the HMC framework in the same way as in the MEMM framework. We illustrate the efficiency of HMC using EFB in Part-Of-Speech Tagging, showing its superiority over MEMM based restoration. We also specify, as a perspective, how HMCs with EFB might appear as an alternative to Recurrent Neural Networks to treat sequential data with a deep architecture.",0
"In Natural Language Processing (NLP), it is crucial to consider the features of observations. The Hidden Markov Chain (HMC) model, which is associated with Forward-Backward probabilities, cannot handle arbitrary features such as prefixes or suffixes of any size without an independence condition. As a result, other sequential models, such as the Maximum Entropy Markov Model (MEMM), have been developed instead of HMC for NLP. However, this paper argues that the issue lies not with HMC itself, but with the way its restoration algorithms are computed. The paper proposes a new method of computing HMC-based restorations using Entropic Forward and Entropic Backward (EFB) probabilities, allowing features to be incorporated into HMC in the same way as in the MEMM framework. The paper demonstrates the effectiveness of HMC with EFB in Part-Of-Speech Tagging and suggests that it may be a viable alternative to Recurrent Neural Networks for processing sequential data with a deep architecture.",1
"In recent years, there has been an exponential growth in the number of complex documents and texts that require a deeper understanding of machine learning methods to be able to accurately classify texts in many applications. Many machine learning approaches have achieved surpassing results in natural language processing. The success of these learning algorithms relies on their capacity to understand complex models and non-linear relationships within data. However, finding suitable structures, architectures, and techniques for text classification is a challenge for researchers. In this paper, a brief overview of text classification algorithms is discussed. This overview covers different text feature extractions, dimensionality reduction methods, existing algorithms and techniques, and evaluations methods. Finally, the limitations of each technique and their application in the real-world problem are discussed.",0
"Over the past few years, the amount of intricate documents and texts has significantly increased, necessitating a deeper comprehension of machine learning methods for precise text classification in various applications. Machine learning techniques have proven to be highly successful in natural language processing by understanding intricate models and non-linear connections within data. Nonetheless, researchers face difficulties in discovering appropriate structures, architectures, and techniques for text classification. This essay presents a brief summary of text classification algorithms, including diverse text feature extractions, dimensionality reduction methods, current algorithms and techniques, and evaluation methods. In conclusion, the paper examines the limitations of each technique and their practical application in real-world situations.",1
"Time series modeling techniques based on deep learning have seen many advancements in recent years, especially in data-abundant settings and with the central aim of learning global models that can extract patterns across multiple time series. While the crucial importance of appropriate data pre-processing and scaling has often been noted in prior work, most studies focus on improving model architectures. In this paper we empirically investigate the effect of data input and output transformations on the predictive performance of several neural forecasting architectures. In particular, we investigate the effectiveness of several forms of data binning, i.e. converting real-valued time series into categorical ones, when combined with feed-forward, recurrent neural networks, and convolution-based sequence models. In many non-forecasting applications where these models have been very successful, the model inputs and outputs are categorical (e.g. words from a fixed vocabulary in natural language processing applications or quantized pixel color intensities in computer vision). For forecasting applications, where the time series are typically real-valued, various ad-hoc data transformations have been proposed, but have not been systematically compared. To remedy this, we evaluate the forecasting accuracy of instances of the aforementioned model classes when combined with different types of data scaling and binning. We find that binning almost always improves performance (compared to using normalized real-valued inputs), but that the particular type of binning chosen is of lesser importance.",0
"Recent years have witnessed significant advancements in time series modeling techniques that employ deep learning, particularly in data-rich environments. The primary objective is to develop global models capable of identifying patterns across multiple time series. While previous research has acknowledged the significance of appropriate data pre-processing and scaling, most studies have focused on enhancing model architectures. This paper aims to assess the impact of data input and output transformations on the predictive accuracy of various neural forecasting architectures. Specifically, our investigation examines the effectiveness of different forms of data binning, which involve transforming real-valued time series into categorical ones, when combined with feed-forward, recurrent neural networks, and convolution-based sequence models. Though these models have been successful in non-forecasting applications, where inputs and outputs are categorical, there has been no systematic comparison of ad-hoc data transformations for forecasting applications. We address this gap by evaluating the forecasting accuracy of these models when combined with different types of data scaling and binning. Our findings suggest that binning consistently enhances performance, compared to using normalized real-valued inputs, and that the specific type of binning employed is of relatively minor significance.",1
"We study the problem of jointly reasoning about language and vision through a navigation and spatial reasoning task. We introduce the Touchdown task and dataset, where an agent must first follow navigation instructions in a real-life visual urban environment, and then identify a location described in natural language to find a hidden object at the goal position. The data contains 9,326 examples of English instructions and spatial descriptions paired with demonstrations. Empirical analysis shows the data presents an open challenge to existing methods, and qualitative linguistic analysis shows that the data displays richer use of spatial reasoning compared to related resources.",0
"Our focus is on exploring the intersection of language and vision by means of a task that involves navigation and spatial reasoning. To this end, we propose the Touchdown dataset and task, which entails guiding an agent through a real-world urban environment based on natural language instructions, and subsequently locating a concealed object at a specified destination. The dataset comprises 9,326 instances of demonstrations coupled with English instructions and spatial descriptions. We demonstrate that the data presents a demanding test for current methods and that the linguistic analysis reveals a more intricate application of spatial reasoning than other comparable resources.",1
"This paper introduces the task of visual question answering for remote sensing data (RSVQA). Remote sensing images contain a wealth of information which can be useful for a wide range of tasks including land cover classification, object counting or detection. However, most of the available methodologies are task-specific, thus inhibiting generic and easy access to the information contained in remote sensing data. As a consequence, accurate remote sensing product generation still requires expert knowledge. With RSVQA, we propose a system to extract information from remote sensing data that is accessible to every user: we use questions formulated in natural language and use them to interact with the images. With the system, images can be queried to obtain high level information specific to the image content or relational dependencies between objects visible in the images. Using an automatic method introduced in this article, we built two datasets (using low and high resolution data) of image/question/answer triplets. The information required to build the questions and answers is queried from OpenStreetMap (OSM). The datasets can be used to train (when using supervised methods) and evaluate models to solve the RSVQA task. We report the results obtained by applying a model based on Convolutional Neural Networks (CNNs) for the visual part and on a Recurrent Neural Network (RNN) for the natural language part to this task. The model is trained on the two datasets, yielding promising results in both cases.",0
"In this article, we present the concept of Remote Sensing Visual Question Answering (RSVQA), which aims to extract useful information from remote sensing data. Currently, most methodologies are specific to certain tasks, making it difficult for non-experts to access the data. Our proposed system uses natural language questions to interact with remote sensing images and retrieve high-level information about object relations and image content. To create datasets for training and evaluation, we developed an automatic method to generate image/question/answer triplets using OpenStreetMap (OSM) data. We trained a model based on Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) using both low and high resolution datasets. Our results show promising performance in solving the RSVQA task.",1
"We present the task of Spatio-Temporal Video Question Answering, which requires intelligent systems to simultaneously retrieve relevant moments and detect referenced visual concepts (people and objects) to answer natural language questions about videos. We first augment the TVQA dataset with 310.8K bounding boxes, linking depicted objects to visual concepts in questions and answers. We name this augmented version as TVQA+. We then propose Spatio-Temporal Answerer with Grounded Evidence (STAGE), a unified framework that grounds evidence in both spatial and temporal domains to answer questions about videos. Comprehensive experiments and analyses demonstrate the effectiveness of our framework and how the rich annotations in our TVQA+ dataset can contribute to the question answering task. Moreover, by performing this joint task, our model is able to produce insightful and interpretable spatio-temporal attention visualizations. Dataset and code are publicly available at: http: //tvqa.cs.unc.edu, https://github.com/jayleicn/TVQAplus",0
"Our presentation focuses on the task of answering questions related to videos through Spatio-Temporal Video Question Answering. This task requires intelligent systems to retrieve relevant moments and detect referenced visual concepts simultaneously, such as people and objects, to provide answers to natural language queries. To enhance the TVQA dataset, we have added 310.8K bounding boxes that link visual concepts in questions and answers to depicted objects. This augmented version is known as TVQA+. We have also introduced Spatio-Temporal Answerer with Grounded Evidence (STAGE), a unified framework that incorporates evidence from spatial and temporal domains to answer video-related questions. Through extensive experiments and analyses, we have demonstrated the effectiveness of our framework and the benefits of using the rich annotations provided in our TVQA+ dataset for answering questions. Furthermore, our model produces meaningful spatio-temporal attention visualizations, making it interpretable. Our dataset and code are available to the public at http://tvqa.cs.unc.edu and https://github.com/jayleicn/TVQAplus, respectively.",1
"In the last few years, many different methods have been focusing on using deep recurrent neural networks for natural language generation. The most widely used sequence-to-sequence neural methods are word-based: as such, they need a pre-processing step called delexicalization (conversely, relexicalization) to deal with uncommon or unknown words. These forms of processing, however, give rise to models that depend on the vocabulary used and are not completely neural.   In this work, we present an end-to-end sequence-to-sequence model with attention mechanism which reads and generates at a character level, no longer requiring delexicalization, tokenization, nor even lowercasing. Moreover, since characters constitute the common ""building blocks"" of every text, it also allows a more general approach to text generation, enabling the possibility to exploit transfer learning for training. These skills are obtained thanks to two major features: (i) the possibility to alternate between the standard generation mechanism and a copy one, which allows to directly copy input facts to produce outputs, and (ii) the use of an original training pipeline that further improves the quality of the generated texts.   We also introduce a new dataset called E2E+, designed to highlight the copying capabilities of character-based models, that is a modified version of the well-known E2E dataset used in the E2E Challenge. We tested our model according to five broadly accepted metrics (including the widely used BLEU), showing that it yields competitive performance with respect to both character-based and word-based approaches.",0
"Recent years have seen a surge in the use of deep recurrent neural networks for natural language generation, with many methods focusing on sequence-to-sequence neural models that are word-based. However, these models require a preprocessing step called delexicalization (or relexicalization) to handle uncommon or unknown words, which creates a dependency on vocabulary and limits the purely neural nature of the models. In this study, we present an end-to-end sequence-to-sequence model with an attention mechanism that operates at the character level, eliminating the need for delexicalization, tokenization, and lowercasing. This approach allows for a more general method of text generation using transfer learning and is made possible by two key features: the ability to alternate between standard generation and copying input facts directly to produce outputs, and an innovative training pipeline that enhances text quality. We also introduce a new dataset called E2E+ that showcases the copying capabilities of character-based models. Our model yielded competitive performance on five widely accepted metrics, including BLEU, compared to both character-based and word-based approaches.",1
"Deep Convolutional Neural Network (CNN) is a special type of Neural Networks, which has shown exemplary performance on several competitions related to Computer Vision and Image Processing. Some of the exciting application areas of CNN include Image Classification and Segmentation, Object Detection, Video Processing, Natural Language Processing, and Speech Recognition. The powerful learning ability of deep CNN is primarily due to the use of multiple feature extraction stages that can automatically learn representations from the data. The availability of a large amount of data and improvement in the hardware technology has accelerated the research in CNNs, and recently interesting deep CNN architectures have been reported. Several inspiring ideas to bring advancements in CNNs have been explored, such as the use of different activation and loss functions, parameter optimization, regularization, and architectural innovations. However, the significant improvement in the representational capacity of the deep CNN is achieved through architectural innovations. Notably, the ideas of exploiting spatial and channel information, depth and width of architecture, and multi-path information processing have gained substantial attention. Similarly, the idea of using a block of layers as a structural unit is also gaining popularity. This survey thus focuses on the intrinsic taxonomy present in the recently reported deep CNN architectures and, consequently, classifies the recent innovations in CNN architectures into seven different categories. These seven categories are based on spatial exploitation, depth, multi-path, width, feature-map exploitation, channel boosting, and attention. Additionally, the elementary understanding of CNN components, current challenges, and applications of CNN are also provided.",0
"The Deep Convolutional Neural Network (CNN) is a specialized Neural Network that has demonstrated outstanding performance in various Computer Vision and Image Processing competitions. CNN has several exciting applications, including but not limited to Image Classification and Segmentation, Object Detection, Video Processing, Natural Language Processing, and Speech Recognition. The CNN's ability to learn effectively is mainly due to the multiple feature extraction stages that can automatically learn representations from data. The research in CNNs has been accelerated by the availability of a large amount of data and improvement in hardware technology, resulting in the discovery of interesting deep CNN architectures. To date, several inspiring ideas, such as the use of different activation and loss functions, parameter optimization, regularization, and architectural innovations, have been explored. However, the most significant improvement in the deep CNN's representational capacity is through architectural innovations, including exploiting spatial and channel information, depth and width of architecture, multi-path information processing, and using a block of layers as a structural unit. This survey classifies recent innovations in CNN architectures into seven categories based on spatial exploitation, depth, multi-path, width, feature-map exploitation, channel boosting, and attention. Additionally, the article provides a basic understanding of CNN components, current challenges, and applications of CNN.",1
"This paper focuses on the problem of unsupervised alignment of hierarchical data such as ontologies or lexical databases. This is a problem that appears across areas, from natural language processing to bioinformatics, and is typically solved by appeal to outside knowledge bases and label-textual similarity. In contrast, we approach the problem from a purely geometric perspective: given only a vector-space representation of the items in the two hierarchies, we seek to infer correspondences across them. Our work derives from and interweaves hyperbolic-space representations for hierarchical data, on one hand, and unsupervised word-alignment methods, on the other. We first provide a set of negative results showing how and why Euclidean methods fail in this hyperbolic setting. We then propose a novel approach based on optimal transport over hyperbolic spaces, and show that it outperforms standard embedding alignment techniques in various experiments on cross-lingual WordNet alignment and ontology matching tasks.",0
"The objective of this paper is to address the issue of unsupervised alignment of hierarchical data like ontologies or lexical databases, which is a common problem in various fields such as bioinformatics and natural language processing. Typically, outside knowledge bases and label-textual similarity are used to solve this problem. However, this paper takes a different approach by focusing solely on the geometric perspective. The aim is to establish correspondences between items in two hierarchies using only their vector-space representation. The study employs hyperbolic-space representations for hierarchical data and unsupervised word-alignment methods. The paper initially presents negative outcomes to demonstrate why Euclidean methods fail in this hyperbolic setting. The study then introduces a novel approach that employs optimal transport over hyperbolic spaces, which outperforms standard embedding alignment techniques in experiments on cross-lingual WordNet alignment and ontology matching tasks.",1
"Existing attention mechanisms are trained to attend to individual items in a collection (the memory) with a predefined, fixed granularity, e.g., a word token or an image grid. We propose area attention: a way to attend to areas in the memory, where each area contains a group of items that are structurally adjacent, e.g., spatially for a 2D memory such as images, or temporally for a 1D memory such as natural language sentences. Importantly, the shape and the size of an area are dynamically determined via learning, which enables a model to attend to information with varying granularity. Area attention can easily work with existing model architectures such as multi-head attention for simultaneously attending to multiple areas in the memory. We evaluate area attention on two tasks: neural machine translation (both character and token-level) and image captioning, and improve upon strong (state-of-the-art) baselines in all the cases. These improvements are obtainable with a basic form of area attention that is parameter free.",0
"The current attention mechanisms are designed to focus on individual elements within a set (known as memory) based on a fixed level of detail, such as a word or image grid. Our proposal is to introduce area attention, which allows for the targeting of specific areas within the memory. Each area comprises of a group of adjacent elements, either in a spatial or temporal sense. The size and shape of these areas are determined through the learning process, which allows for greater flexibility in the level of detail captured. Area attention can be easily integrated with existing models, such as multi-head attention, to enable simultaneous targeting of multiple areas within the memory. We tested area attention on two tasks: neural machine translation (character and token-level) and image captioning, and achieved better results compared to state-of-the-art approaches. Furthermore, we achieved these improvements using a basic version of area attention that requires no additional parameters.",1
"Deep Learning (DL) is one of the hottest trends in machine learning as DL approaches produced results superior to the state-of-the-art in problematic areas such as image processing and natural language processing (NLP). To foster the growth of DL, several open source frameworks appeared providing implementations of the most common DL algorithms. These frameworks vary in the algorithms they support and in the quality of their implementations. The purpose of this work is to provide a qualitative and quantitative comparison among three of the most popular and most comprehensive DL frameworks (namely Google's TensorFlow, University of Montreal's Theano and Microsoft's CNTK). The ultimate goal of this work is to help end users make an informed decision about the best DL framework that suits their needs and resources. To ensure that our study is as comprehensive as possible, we conduct several experiments using multiple benchmark datasets from different fields (image processing, NLP, etc.) and measure the performance of the frameworks' implementations of different DL algorithms. For most of our experiments, we find out that CNTK's implementations are superior to the other ones under consideration.",0
"DL has become a highly popular trend in machine learning due to its superior results in problematic areas like image processing and NLP. As a result, various open source frameworks have emerged to support the growth of DL. However, these frameworks differ in terms of algorithm support and implementation quality. The aim of this research is to conduct a qualitative and quantitative comparison of three of the most extensive and popular DL frameworks, namely TensorFlow, Theano, and CNTK. The ultimate goal is to assist users in making an informed decision about which framework is best suited to their needs and resources. To ensure comprehensive results, the study employs multiple benchmark datasets from various fields and measures the frameworks' implementation of various DL algorithms. The experiments reveal that CNTK's implementations outperform the other frameworks in most cases.",1
"The recent developments and growing interest in neural-symbolic models has shown that hybrid approaches can offer richer models for Artificial Intelligence. The integration of effective relational learning and reasoning methods is one of the key challenges in this direction, as neural learning and symbolic reasoning offer complementary characteristics that can benefit the development of AI systems. Relational labelling or link prediction on knowledge graphs has become one of the main problems in deep learning-based natural language processing research. Moreover, other fields which make use of neural-symbolic techniques may also benefit from such research endeavours. There have been several efforts towards the identification of missing facts from existing ones in knowledge graphs. Two lines of research try and predict knowledge relations between two entities by considering all known facts connecting them or several paths of facts connecting them. We propose a neural-symbolic graph neural network which applies learning over all the paths by feeding the model with the embedding of the minimal subset of the knowledge graph containing such paths. By learning to produce representations for entities and facts corresponding to word embeddings, we show how the model can be trained end-to-end to decode these representations and infer relations between entities in a multitask approach. Our contribution is two-fold: a neural-symbolic methodology leverages the resolution of relational inference in large graphs, and we also demonstrate that such neural-symbolic model is shown more effective than path-based approaches",0
"Hybrid approaches involving neural-symbolic models have gained interest in Artificial Intelligence due to their ability to provide richer models. Integrating effective relational learning and reasoning methods poses a significant challenge, but the complementary characteristics of neural learning and symbolic reasoning can benefit AI development. Deep learning-based natural language processing research has focused on relational labelling or link prediction on knowledge graphs. Research in this area can also benefit other fields that utilize neural-symbolic techniques. Previous research has attempted to identify missing facts from existing knowledge graphs through two lines of investigation: predicting knowledge relations between entities using known facts or multiple paths of facts. We propose a neural-symbolic graph neural network that learns over all paths by feeding the model with the embedding of the minimal subset of the knowledge graph containing such paths. Our model learns to produce entity and fact representations corresponding to word embeddings and can infer relations between entities in a multitask approach. Our contribution lies in the demonstration of the effectiveness of a neural-symbolic methodology in resolving relational inference in large graphs, which outperforms path-based approaches.",1
"Although adaptive optimization algorithms such as Adam show fast convergence in many machine learning tasks, this paper identifies a problem of Adam by analyzing its performance in a simple non-convex synthetic problem, showing that Adam's fast convergence would possibly lead the algorithm to local minimums. To address this problem, we improve Adam by proposing a novel adaptive gradient descent algorithm named AdaX. Unlike Adam that ignores the past gradients, AdaX exponentially accumulates the long-term gradient information in the past during training, to adaptively tune the learning rate. We thoroughly prove the convergence of AdaX in both the convex and non-convex settings. Extensive experiments show that AdaX outperforms Adam in various tasks of computer vision and natural language processing and can catch up with Stochastic Gradient Descent.",0
"This paper examines Adam, an adaptive optimization algorithm that is known for its fast convergence in many machine learning tasks. However, the paper identifies a potential issue with Adam's performance when dealing with non-convex synthetic problems. Specifically, Adam's fast convergence could lead to local minimums, which is problematic. To address this issue, the paper proposes a new adaptive gradient descent algorithm called AdaX. Unlike Adam, AdaX takes into account past gradients and uses this information to adjust the learning rate during training. The paper provides a rigorous proof of AdaX's convergence in both convex and non-convex settings. Furthermore, extensive experiments demonstrate that AdaX outperforms Adam in various computer vision and natural language processing tasks and can even match the performance of Stochastic Gradient Descent.",1
"This paper presents several strategies that can improve neural network-based predictive methods for MOOC student course trajectory modeling, applying multiple ideas previously applied to tackle NLP (Natural Language Processing) tasks. In particular, this paper investigates LSTM networks enhanced with two forms of regularization, along with the more recently introduced Transformer architecture.",0
"The present study outlines multiple techniques to enhance neural network-based predictive models for MOOC student course trajectory modeling. It draws inspiration from various approaches utilized for Natural Language Processing (NLP) tasks. The study delves deeper into the efficacy of LSTM networks fortified by two distinct forms of regularization, in addition to the recently introduced Transformer architecture.",1
"Recurrent Neural Networks (RNNs) have been shown to be valuable for constructing Intrusion Detection Systems (IDSs) for network data. They allow determining if a flow is malicious or not already before it is over, making it possible to take action immediately. However, considering the large number of packets that has to be inspected, for example in cloud/fog and edge computing, the question of computational efficiency arises. We show that by using a novel Reinforcement Learning (RL)-based approach called SparseIDS, we can reduce the number of consumed packets by more than three fourths while keeping classification accuracy high. To minimize the computational expenses of the RL-based sampling we show that a shared neural network can be used for both the classifier and the RL logic. Thus, no additional resources are consumed by the sampling in deployment. Comparing to various other sampling techniques, SparseIDS consistently achieves higher classification accuracy by learning to sample only relevant packets. A major novelty of our RL-based approach is that it can not only skip up to a predefined maximum number of samples like other approaches proposed in the domain of Natural Language Processing but can even skip arbitrarily many packets in one step. This enables saving even more computational resources for long sequences. Inspecting SparseIDS's behavior of choosing packets shows that it adopts different sampling strategies for different attack types and network flows. Finally we build an automatic steering mechanism that can guide SparseIDS in deployment to achieve a desired level of sparsity.",0
"Intrusion Detection Systems (IDSs) used for network data can benefit from Recurrent Neural Networks (RNNs) which are capable of identifying malicious flows before they complete, enabling swift action. However, in cloud/fog and edge computing scenarios, the high volume of packets that need to be inspected raises concerns about computational efficiency. SparseIDS is a new Reinforcement Learning (RL)-based approach that reduces the number of packets required for classification by over 75% while maintaining high accuracy. By employing a shared neural network for both the classifier and RL logic, there is no additional resource consumption during deployment. SparseIDS achieves higher accuracy than other sampling techniques by learning to sample only relevant packets. The RL-based approach can skip an arbitrary number of packets in one step, offering increased computational savings. SparseIDS adopts different sampling strategies for various attack types and network flows. An automatic steering mechanism can guide SparseIDS during deployment to achieve a desired level of sparsity.",1
"Voice Assistants aim to fulfill user requests by choosing the best intent from multiple options generated by its Automated Speech Recognition and Natural Language Understanding sub-systems. However, voice assistants do not always produce the expected results. This can happen because voice assistants choose from ambiguous intents - user-specific or domain-specific contextual information reduces the ambiguity of the user request. Additionally the user information-state can be leveraged to understand how relevant/executable a specific intent is for a user request. In this work, we propose a novel Energy-based model for the intent ranking task, where we learn an affinity metric and model the trade-off between extracted meaning from speech utterances and relevance/executability aspects of the intent. Furthermore we present a Multisource Denoising Autoencoder based pretraining that is capable of learning fused representations of data from multiple sources. We empirically show our approach outperforms existing state of the art methods by reducing the error-rate by 3.8%, which in turn reduces ambiguity and eliminates undesired dead-ends leading to better user experience. Finally, we evaluate the robustness of our algorithm on the intent ranking task and show our algorithm improves the robustness by 33.3%.",0
"The ultimate goal of Voice Assistants is to satisfy user requests by selecting the most appropriate intent from a range of options generated by its Automated Speech Recognition and Natural Language Understanding subsystems. However, there are times when voice assistants fail to deliver the expected outcomes. This can be attributed to the selection of ambiguous intents, which can be resolved by incorporating user-specific or domain-specific contextual information. Furthermore, understanding the relevance and feasibility of a particular intent can be enhanced by leveraging user information-state. To address these issues, we introduce an innovative Energy-based model for the intent ranking task, where we establish an affinity metric and balance the trade-off between the extracted meaning from speech utterances and the relevance/executability aspects of the intent. Additionally, we present a Multisource Denoising Autoencoder-based pretraining technique that can learn consolidated representations of data from multiple sources. Empirical results demonstrate that our approach outperforms existing state-of-the-art methods by reducing the error-rate by 3.8%, thus minimizing ambiguity and eliminating unwanted dead-ends, resulting in a better user experience. Finally, we evaluate the robustness of our algorithm in the intent ranking task and demonstrate a 33.3% improvement in robustness.",1
"Despite the great success of Convolutional Neural Networks (CNNs) in Computer Vision and Natural Language Processing, the working mechanism behind CNNs is still under extensive discussions and research. Driven by a strong demand for the theoretical explanation of neural networks, some researchers utilize information theory to provide insight into the black box model. However, to the best of our knowledge, employing information theory to quantitatively analyze and qualitatively visualize neural networks has not been extensively studied in the visualization community. In this paper, we combine information entropies and visualization techniques to shed light on how CNN works. Specifically, we first introduce a data model to organize the data that can be extracted from CNN models. Then we propose two ways to calculate entropy under different circumstances. To provide a fundamental understanding of the basic building blocks of CNNs (e.g., convolutional layers, pooling layers, normalization layers) from an information-theoretic perspective, we develop a visual analysis system, CNNSlicer. CNNSlicer allows users to interactively explore the amount of information changes inside the model. With case studies on the widely used benchmark datasets (MNIST and CIFAR-10), we demonstrate the effectiveness of our system in opening the blackbox of CNNs.",0
"Although Convolutional Neural Networks (CNNs) have achieved significant success in Computer Vision and Natural Language Processing, the operational process of CNNs remains a subject of ongoing discussion and research. Some researchers have applied information theory to gain insight into the black box model and meet the demand for theoretical explanations of neural networks. However, the application of information theory to quantitatively analyze and qualitatively visualize neural networks has not been extensively explored in the visualization community. This paper presents a combination of information entropies and visualization techniques to uncover the workings of CNNs. We first introduce a data model to organize data extracted from CNN models and then propose two methods for calculating entropy under different circumstances. Additionally, we develop a visual analysis system, CNNSlicer, to provide a fundamental understanding of the basic building blocks of CNNs from an information-theoretic perspective. CNNSlicer enables users to interactively explore the information changes inside the model. We show the effectiveness of our system in opening the blackbox of CNNs through case studies on widely used benchmark datasets (MNIST and CIFAR-10).",1
"We develop a language-guided navigation task set in a continuous 3D environment where agents must execute low-level actions to follow natural language navigation directions. By being situated in continuous environments, this setting lifts a number of assumptions implicit in prior work that represents environments as a sparse graph of panoramas with edges corresponding to navigability. Specifically, our setting drops the presumptions of known environment topologies, short-range oracle navigation, and perfect agent localization. To contextualize this new task, we develop models that mirror many of the advances made in prior settings as well as single-modality baselines. While some of these techniques transfer, we find significantly lower absolute performance in the continuous setting -- suggesting that performance in prior `navigation-graph' settings may be inflated by the strong implicit assumptions.",0
"A language-guided navigation task has been created in a continuous 3D environment. In this environment, agents must execute low-level actions to follow natural language navigation directions. This new setting differs from prior work, which represented environments as a sparse graph of panoramas with edges corresponding to navigability. The new setting eliminates the assumptions of known environment topologies, short-range oracle navigation, and perfect agent localization. To better understand this new task, models have been developed that reflect advances made in prior settings, as well as single-modality baselines. However, the techniques used in prior settings do not fully transfer to the new continuous setting, resulting in significantly lower absolute performance. This suggests that performance in prior navigation-graph settings may have been inflated by the strong implicit assumptions present in those settings.",1
"One of deep learning's attractive benefits is the ability to automatically extract relevant features for a target problem from largely raw data, instead of utilizing human engineered and error prone handcrafted features. While deep learning has shown success in fields such as image classification and natural language processing, its application for feature extraction on raw network packet data for intrusion detection is largely unexplored. In this paper we modify a Word2Vec approach, used for text processing, and apply it to packet data for automatic feature extraction. We call this approach Packet2Vec. For the classification task of benign versus malicious traffic on a 2009 DARPA network data set, we obtain an area under the curve (AUC) of the receiver operating characteristic (ROC) between 0.988-0.996 and an AUC of the Precision/Recall curve between 0.604-0.667.",0
"Deep learning has the ability to extract relevant features for a target problem automatically from raw data, avoiding the use of human-engineered and error-prone handcrafted features. Although deep learning has proven successful in fields such as image classification and natural language processing, its application for intrusion detection on raw network packet data, in particular, remains largely unexplored. In this paper, we introduce an approach called Packet2Vec, which modifies the Word2Vec technique used in text processing for automatic feature extraction on packet data. Our experiments on a 2009 DARPA network data set for the classification task of benign versus malicious traffic show an area under the curve (AUC) of the receiver operating characteristic (ROC) between 0.988-0.996 and an AUC of the Precision/Recall curve between 0.604-0.667.",1
"Multi-task learning (mtl) provides state-of-the-art results in many applications of computer vision and natural language processing. In contrast to single-task learning (stl), mtl allows for leveraging knowledge between related tasks improving prediction results on the main task (in contrast to an auxiliary task) or all tasks. However, there is a limited number of comparative studies on applying mtl architectures for regression and time series problems taking recent advances of mtl into account. An interesting, non-linear problem is the forecast of the expected power generation for renewable power plants. Therefore, this article provides a comparative study of the following recent and important mtl architectures: Hard parameter sharing, cross-stitch network, sluice network (sn). They are compared to a multi-layer perceptron model of similar size in an stl setting. Additionally, we provide a simple, yet effective approach to model task specific information through an embedding layer in an multi-layer perceptron, referred to as task embedding. Further, we introduce a new mtl architecture named emerging relation network (ern), which can be considered as an extension of the sluice network. For a solar power dataset, the task embedding achieves the best mean improvement with 14.9%. The mean improvement of the ern and the sn on the solar dataset is of similar magnitude with 14.7% and 14.8%. On a wind power dataset, only the ern achieves a significant improvement of up to 7.7%. Results suggest that the ern is beneficial when tasks are only loosely related and the prediction problem is more non-linear. Contrary, the proposed task embedding is advantageous when tasks are strongly correlated. Further, the task embedding provides an effective approach with reduced computational effort compared to other mtl architectures.",0
"Multi-task learning (MTL) is a highly effective approach in computer vision and natural language processing applications, surpassing single-task learning (STL) by enabling the use of shared knowledge between related tasks, which improves prediction results for the main task. However, there are limited comparative studies on using MTL architectures for regression and time series problems, despite recent advances. One such problem is predicting the expected power generation for renewable power plants. This article presents a comparative study of several MTL architectures, including Hard Parameter Sharing, Cross-Stitch Network, and Sluice Network, as well as a Multi-Layer Perceptron model in an STL setting. Additionally, a task embedding approach is introduced to model task-specific information, and a new MTL architecture called Emerging Relation Network is proposed. The study shows that the task embedding approach is effective when tasks are strongly correlated, whereas the Emerging Relation Network is beneficial when tasks are loosely related and the prediction problem is more nonlinear. The results also suggest that the proposed task embedding approach is computationally efficient compared to other MTL architectures.",1
"While the success of deep neural networks (DNNs) is well-established across a variety of domains, our ability to explain and interpret these methods is limited. Unlike previously proposed local methods which try to explain particular classification decisions, we focus on global interpretability and ask a universally applicable question: given a trained model, which features are the most important? In the context of neural networks, a feature is rarely important on its own, so our strategy is specifically designed to leverage partial covariance structures and incorporate variable dependence into feature ranking. Our methodological contributions in this paper are two-fold. First, we propose an effect size analogue for DNNs that is appropriate for applications with highly collinear predictors (ubiquitous in computer vision). Second, we extend the recently proposed ""RelATive cEntrality"" (RATE) measure (Crawford et al., 2019) to the Bayesian deep learning setting. RATE applies an information theoretic criterion to the posterior distribution of effect sizes to assess feature significance. We apply our framework to three broad application areas: computer vision, natural language processing, and social science.",0
"Although deep neural networks (DNNs) have been successful in various domains, understanding and interpreting the methods remain challenging. In contrast to local methods that explain specific classification decisions, we aim for global interpretability by identifying the most significant features in a trained model. As features in neural networks are often interdependent, we propose a strategy that considers partial covariance structures and variable dependence in feature ranking. Our contributions in this paper are twofold: we introduce an effect size analogue suitable for highly collinear predictors commonly found in computer vision, and we extend the ""RelATive cEntrality"" (RATE) measure to the Bayesian deep learning setting. RATE uses an information theoretic criterion to determine feature significance based on the posterior distribution of effect sizes. We demonstrate the effectiveness of our approach across three broad application areas: computer vision, natural language processing, and social science.",1
"In recent years, the emerging topics of recommender systems that take advantage of natural language processing techniques have attracted much attention, and one of their applications is the Conversational Recommender System (CRS). Unlike traditional recommender systems with content-based and collaborative filtering approaches, CRS learns and models user's preferences through interactive dialogue conversations. In this work, we provide a summarization of the recent evolution of CRS, where deep learning approaches are applied to CRS and have produced fruitful results. We first analyze the research problems and present key challenges in the development of Deep Conversational Recommender Systems (DCRS), then present the current state of the field taken from the most recent researches, including the most common deep learning models that benefit DCRS. Finally, we discuss future directions for this vibrant area.",0
"Recommender systems utilizing natural language processing techniques have become increasingly popular in recent years, with the Conversational Recommender System (CRS) being a notable application. Unlike traditional content-based and collaborative filtering systems, CRS incorporates interactive dialogue conversations to learn and model user preferences. This article provides a summary of the evolution of CRS, focusing on the successful integration of deep learning approaches. The challenges and research problems in developing Deep Conversational Recommender Systems (DCRS) are analyzed, and the current state of the field is presented, including the commonly used deep learning models. The article concludes by discussing potential future directions in this dynamic field.",1
"This paper presents a framework to recognize temporal compositions of atomic actions in videos. Specifically, we propose to express temporal compositions of actions as semantic regular expressions and derive an inference framework using probabilistic automata to recognize complex actions as satisfying these expressions on the input video features. Our approach is different from existing works that either predict long-range complex activities as unordered sets of atomic actions, or retrieve videos using natural language sentences. Instead, the proposed approach allows recognizing complex fine-grained activities using only pretrained action classifiers, without requiring any additional data, annotations or neural network training. To evaluate the potential of our approach, we provide experiments on synthetic datasets and challenging real action recognition datasets, such as MultiTHUMOS and Charades. We conclude that the proposed approach can extend state-of-the-art primitive action classifiers to vastly more complex activities without large performance degradation.",0
"In this paper, we introduce a framework for identifying temporal compositions of atomic actions in videos. Our approach involves expressing these compositions as semantic regular expressions and using probabilistic automata to infer whether complex actions satisfy these expressions based on video features. Unlike other methods, such as predicting long-range complex activities as unordered sets of atomic actions or using natural language sentences to retrieve videos, our approach can recognize complex and detailed activities using pretrained action classifiers without the need for extra data, annotations, or neural network training. We evaluate the effectiveness of our approach through experiments on both synthetic and real action recognition datasets, including MultiTHUMOS and Charades. Our results show that our approach can significantly improve the recognition of complex activities while maintaining high performance on primitive action classifiers.",1
"Referring expression comprehension aims to localize the object instance described by a natural language expression. Current referring expression methods have achieved good performance. However, none of them is able to achieve real-time inference without accuracy drop. The reason for the relatively slow inference speed is that these methods artificially split the referring expression comprehension into two sequential stages including proposal generation and proposal ranking. It does not exactly conform to the habit of human cognition. To this end, we propose a novel Realtime Cross-modality Correlation Filtering method (RCCF). RCCF reformulates the referring expression comprehension as a correlation filtering process. The expression is first mapped from the language domain to the visual domain and then treated as a template (kernel) to perform correlation filtering on the image feature map. The peak value in the correlation heatmap indicates the center points of the target box. In addition, RCCF also regresses a 2-D object size and 2-D offset. The center point coordinates, object size and center point offset together to form the target bounding box. Our method runs at 40 FPS while achieving leading performance in RefClef, RefCOCO, RefCOCO+ and RefCOCOg benchmarks. In the challenging RefClef dataset, our methods almost double the state-of-the-art performance (34.70% increased to 63.79%). We hope this work can arouse more attention and studies to the new cross-modality correlation filtering framework as well as the one-stage framework for referring expression comprehension.",0
"The objective of referring expression comprehension is to identify the object instance being described by a natural language expression. Although current methods for referring expression have achieved satisfactory results, they cannot perform real-time inference without a decrease in accuracy. This is because these methods divide the referring expression comprehension into two stages, which are proposal generation and proposal ranking, leading to slow inference speed. This process does not align with human cognition. To address this issue, we propose a new method called Realtime Cross-modality Correlation Filtering (RCCF). RCCF reformulates referring expression comprehension as a correlation filtering process. The expression is initially mapped from the language to the visual domain and then used as a template to perform correlation filtering on the image feature map. The highest point in the correlation heatmap shows the center points of the target. RCCF also regresses a 2-D object size and 2-D offset. The center point coordinates, object size, and center point offset together form the target bounding box. Our method operates at 40 FPS and achieves leading performance in RefClef, RefCOCO, RefCOCO+, and RefCOCOg benchmarks. In RefClef, our method almost doubles the state-of-the-art performance (34.70% increased to 63.79%). We aim to draw more attention and research to the novel cross-modality correlation filtering and one-stage framework for referring expression comprehension.",1
"Deep learning continues to revolutionize an ever-growing number of critical application areas including healthcare, transportation, finance, and basic sciences. Despite their increased predictive power, model transparency and human explainability remain a significant challenge due to the ""black box"" nature of modern deep learning models. In many cases the desired balance between interpretability and performance is predominately task specific. Human-centric domains such as healthcare necessitate a renewed focus on understanding how and why these frameworks are arriving at critical and potentially life-or-death decisions. Given the quantity of research and empirical successes of deep learning for computer vision, most of the existing interpretability research has focused on image processing techniques. Comparatively, less attention has been paid to interpreting deep learning frameworks using sequential data. Given recent deep learning advancements in highly sequential domains such as natural language processing and physiological signal processing, the need for deep sequential explanations is at an all-time high. In this paper, we review current techniques for interpreting deep learning techniques involving sequential data, identify similarities to non-sequential methods, and discuss current limitations and future avenues of sequential interpretability research.",0
"The use of deep learning is having a significant impact on various fields such as healthcare, finance, transportation, and basic sciences. However, these models have a ""black box"" nature which makes it challenging to provide model transparency and human explainability, despite their superior predictive power. In some cases, the balance between performance and interpretability is task-specific. In healthcare, understanding the reasoning behind critical and potentially life-saving decisions is crucial. While interpretability research in deep learning has mainly focused on image processing, there has been less attention given to interpreting these models using sequential data. With recent advancements in natural language and physiological signal processing, there is a growing need for deep sequential explanations. This paper explores current techniques for interpreting deep learning models with sequential data, highlights similarities with non-sequential methods, and discusses the limitations and future directions of sequential interpretability research.",1
"Convolutional Neural Networks (CNNs) have become common in many fields including computer vision, speech recognition, and natural language processing. Although CNN hardware accelerators are already included as part of many SoC architectures, the task of achieving high accuracy on resource-restricted devices is still considered challenging, mainly due to the vast number of design parameters that need to be balanced to achieve an efficient solution. Quantization techniques, when applied to the network parameters, lead to a reduction of power and area and may also change the ratio between communication and computation. As a result, some algorithmic solutions may suffer from lack of memory bandwidth or computational resources and fail to achieve the expected performance due to hardware constraints. Thus, the system designer and the micro-architect need to understand at early development stages the impact of their high-level decisions (e.g., the architecture of the CNN and the amount of bits used to represent its parameters) on the final product (e.g., the expected power saving, area, and accuracy). Unfortunately, existing tools fall short of supporting such decisions.   This paper introduces a hardware-aware complexity metric that aims to assist the system designer of the neural network architectures, through the entire project lifetime (especially at its early stages) by predicting the impact of architectural and micro-architectural decisions on the final product. We demonstrate how the proposed metric can help evaluate different design alternatives of neural network models on resource-restricted devices such as real-time embedded systems, and to avoid making design mistakes at early stages.",0
"Convolutional Neural Networks (CNNs) are widely used in various fields, including computer vision, speech recognition, and natural language processing. Despite the inclusion of CNN hardware accelerators in many SoC architectures, achieving high accuracy on resource-limited devices remains a challenge due to the complexity of balancing multiple design parameters to develop an efficient solution. Applying quantization techniques to network parameters can reduce power and area usage, but this may also affect the communication-computation ratio, potentially leading to insufficient memory bandwidth or computational resources that limit performance. Therefore, early understanding of the impact of high-level decisions on the final product, including CNN architecture and parameter representation, is crucial for system designers and micro-architects. Unfortunately, existing tools do not adequately support these decisions. This paper presents a hardware-aware complexity metric that assists system designers throughout the project lifecycle, especially in the early stages, by predicting the impact of architectural and micro-architectural decisions on the final product. The proposed metric helps evaluate different neural network models' design alternatives on resource-limited devices, such as real-time embedded systems, and avoid design errors early in the process.",1
"Image classification has been studied extensively, but there has been limited work in using unconventional, external guidance other than traditional image-label pairs for training. We present a set of methods for leveraging information about the semantic hierarchy embedded in class labels. We first inject label-hierarchy knowledge into an arbitrary CNN-based classifier and empirically show that availability of such external semantic information in conjunction with the visual semantics from images boosts overall performance. Taking a step further in this direction, we model more explicitly the label-label and label-image interactions using order-preserving embeddings governed by both Euclidean and hyperbolic geometries, prevalent in natural language, and tailor them to hierarchical image classification and representation learning. We empirically validate all the models on the hierarchical ETHEC dataset.",0
"Extensive research has been conducted on image classification, but there has been a limited focus on utilizing unconventional external guidance, beyond traditional image-label pairs, for training. Our proposed methods involve leveraging the semantic hierarchy embedded in class labels to improve performance. We demonstrate the effectiveness of integrating label-hierarchy knowledge into a CNN-based classifier, which leads to a significant boost in performance when combined with visual semantics from images. Furthermore, we take a step further by explicitly modeling label-label and label-image interactions using order-preserving embeddings based on Euclidean and hyperbolic geometries commonly found in natural language. These models are tailored to hierarchical image classification and representation learning, and we validate their efficacy on the ETHEC dataset.",1
"Neural network models are widely used in solving many challenging problems, such as computer vision, personalized recommendation, and natural language processing. Those models are very computationally intensive and reach the hardware limit of the existing server and IoT devices. Thus, finding better model architectures with much less amount of computation while maximally preserving the accuracy is a popular research topic. Among various mechanisms that aim to reduce the computation complexity, identifying the zero values in the model weights and in the activations to avoid computing them is a promising direction.   In this paper, we summarize the current status of the research on the computation of sparse neural networks, from the perspective of the sparse algorithms, the software frameworks, and the hardware accelerations. We observe that the search for the sparse structure can be a general methodology for high-quality model explorations, in addition to a strategy for high-efficiency model execution. We discuss the model accuracy influenced by the number of weight parameters and the structure of the model. The corresponding models are called to be located in the weight dominated and structure dominated regions, respectively. We show that for practically complicated problems, it is more beneficial to search large and sparse models in the weight dominated region. In order to achieve the goal, new approaches are required to search for proper sparse structures, and new sparse training hardware needs to be developed to facilitate fast iterations of sparse models.",0
"The use of neural network models is widespread in tackling difficult problems, including computer vision, personalized recommendation, and natural language processing. However, these models require a significant amount of computation and can push existing servers and IoT devices to their limits. Therefore, researchers are focusing on developing more efficient model architectures that can maintain accuracy while reducing computation. One promising approach is identifying zero values in model weights and activations to avoid unnecessary computations. This paper provides an overview of the current research on sparse neural networks, including sparse algorithms, software frameworks, and hardware accelerations. The search for sparse structures can serve as a general methodology for exploring high-quality models and achieving high-efficiency model execution. The paper also discusses the impact of weight parameters and model structure on accuracy, distinguishing between weight-dominated and structure-dominated regions. For complex problems, it is more advantageous to search for large and sparse models in the weight-dominated region. To achieve this, new approaches and hardware are needed to efficiently search for sparse structures and train sparse models.",1
"The extraction of main content from web pages is an important task for numerous applications, ranging from usability aspects, like reader views for news articles in web browsers, to information retrieval or natural language processing. Existing approaches are lacking as they rely on large amounts of hand-crafted features for classification. This results in models that are tailored to a specific distribution of web pages, e.g. from a certain time frame, but lack in generalization power. We propose a neural sequence labeling model that does not rely on any hand-crafted features but takes only the HTML tags and words that appear in a web page as input. This allows us to present a browser extension which highlights the content of arbitrary web pages directly within the browser using our model. In addition, we create a new, more current dataset to show that our model is able to adapt to changes in the structure of web pages and outperform the state-of-the-art model.",0
"The process of extracting primary content from web pages is a significant task for various applications, including usability features such as reader views for news articles, information retrieval, and natural language processing. The current approaches are deficient as they heavily rely on manually created features for classification, resulting in models that are tailored to specific web page distributions, such as those from a specific time period, but lack in generalization capability. Our solution is a neural sequence labeling model that relies solely on the HTML tags and words present in a web page for input, thus enabling us to introduce a browser extension that directly highlights the content of any web page using our model. Furthermore, we have developed a new dataset that is more up-to-date and demonstrates that our model can adapt to changes in web page structure and outperform the current state-of-the-art model.",1
"Neural networks are surprisingly good at interpolating and perform remarkably well when the training set examples resemble those in the test set. However, they are often unable to extrapolate patterns beyond the seen data, even when the abstractions required for such patterns are simple. In this paper, we first review the notion of extrapolation, why it is important and how one could hope to tackle it. We then focus on a specific type of extrapolation which is especially useful for natural language processing: generalization to sequences that are longer than the training ones. We hypothesize that models with a separate content- and location-based attention are more likely to extrapolate than those with common attention mechanisms. We empirically support our claim for recurrent seq2seq models with our proposed attention on variants of the Lookup Table task. This sheds light on some striking failures of neural models for sequences and on possible methods to approaching such issues.",0
"Although neural networks are capable of interpolating and performing well when the training set examples are similar to the test set, they often struggle to extrapolate patterns beyond the observed data, even when the required abstractions for such patterns are straightforward. This paper reviews the concept of extrapolation, its significance, and potential approaches to address it. Specifically, the paper focuses on generalizing to longer sequences in natural language processing and proposes that models with separate content- and location-based attention are more likely to extrapolate than those with common attention mechanisms. The authors provide empirical evidence to support their hypothesis using recurrent seq2seq models with the proposed attention on variants of the Lookup Table task. This sheds light on the limitations of neural models for sequences and suggests possible methods to overcome such issues.",1
"Joint feature selection and classification in an online setting is essential for time-sensitive decision making. However, most existing methods treat this coupled problem independently. Specifically, online feature selection methods can handle either streaming features or data instances offline to produce a fixed set of features for classification, while online classification methods classify incoming instances using full knowledge about the feature space. Nevertheless, all existing methods utilize a set of features, common for all data instances, for classification. Instead, we propose a framework to perform joint feature selection and classification on-the-fly, so as to minimize the number of features evaluated for every data instance and maximize classification accuracy. We derive the optimum solution of the associated optimization problem and analyze its structure. Two algorithms are proposed, ETANA and F-ETANA, which are based on the optimum solution and its properties. We evaluate the performance of the proposed algorithms on several public datasets, demonstrating (i) the dominance of the proposed algorithms over the state-of-the-art, and (ii) its applicability to broad range of application domains including clinical research and natural language processing.",0
"Performing joint feature selection and classification in an online setting is crucial for making time-sensitive decisions. However, current methods treat these two tasks independently. Online feature selection methods can handle streaming features or data instances offline to generate a fixed set of features for classification, whereas online classification methods classify incoming instances using complete knowledge of the feature space. However, all current methods utilize a set of features that is the same for all data instances during classification. Instead, we present a framework for performing joint feature selection and classification on-the-fly, which minimizes the number of features evaluated for each data instance and maximizes classification accuracy. We determine the optimal solution to the associated optimization problem and analyze its structure. We propose two algorithms, ETANA and F-ETANA, based on the optimal solution and its properties. We evaluate the performance of these algorithms on various public datasets, demonstrating their superiority over state-of-the-art methods and their applicability to a wide range of application domains, including clinical research and natural language processing.",1
"One of the primary challenges limiting the applicability of deep learning is its susceptibility to learning spurious correlations rather than the underlying mechanisms of the task of interest. The resulting failure to generalise cannot be addressed by simply using more data from the same distribution. We propose an auxiliary training objective that improves the generalization capabilities of neural networks by leveraging an overlooked supervisory signal found in existing datasets. We use pairs of minimally-different examples with different labels, a.k.a counterfactual or contrasting examples, which provide a signal indicative of the underlying causal structure of the task. We show that such pairs can be identified in a number of existing datasets in computer vision (visual question answering, multi-label image classification) and natural language processing (sentiment analysis, natural language inference). The new training objective orients the gradient of a model's decision function with pairs of counterfactual examples. Models trained with this technique demonstrate improved performance on out-of-distribution test sets.",0
"Deep learning faces a significant obstacle in its tendency to learn misleading correlations instead of the fundamental mechanisms of the target task. This issue results in an inability to generalize, which cannot be remedied by merely increasing the amount of data from the same source. To address this, we suggest a supplementary training objective that enhances the generalization capabilities of neural networks by utilizing an overlooked supervisory signal that already exists in current datasets. These signals are provided by contrasting or counterfactual examples, which consist of pairs of slightly different examples with distinct labels that indicate the underlying causal structure of the task. We demonstrate that these pairs can be located in a range of existing datasets in computer vision (e.g. multi-label image classification, visual question answering) and natural language processing (e.g. natural language inference, sentiment analysis). By guiding the gradient of a model's decision function with these counterfactual example pairs, we introduce a new training objective. Models trained using this method exhibit superior performance on out-of-distribution test sets.",1
"In recent years, topological data analysis has been utilized for a wide range of problems to deal with high dimensional noisy data. While text representations are often high dimensional and noisy, there are only a few work on the application of topological data analysis in natural language processing. In this paper, we introduce a novel algorithm to extract topological features from word embedding representation of text that can be used for text classification. Working on word embeddings, topological data analysis can interpret the embedding high-dimensional space and discover the relations among different embedding dimensions. We will use persistent homology, the most commonly tool from topological data analysis, for our experiment. Examining our topological algorithm on long textual documents, we will show our defined topological features may outperform conventional text mining features.",0
"Recently, topological data analysis has been employed to tackle high dimensional noisy data in various fields. Although text representations are also typically noisy and high dimensional, few studies have applied topological data analysis in natural language processing. This paper presents a novel algorithm that extracts topological features from word embedding representations of text for text classification. With the use of word embeddings, topological data analysis can interpret the high-dimensional space of embedding and identify the relationships among different embedding dimensions. We will employ persistent homology, the most commonly used tool in topological data analysis, for our experiment. Through the examination of our topological algorithm on lengthy textual documents, we will demonstrate that our defined topological features could potentially outperform traditional text mining features.",1
"Grounding referring expressions in images aims to locate the object instance in an image described by a referring expression. It involves a joint understanding of natural language and image content, and is essential for a range of visual tasks related to human-computer interaction. As a language-to-vision matching task, the core of this problem is to not only extract all the necessary information (i.e., objects and the relationships among them) in both the image and referring expression, but also make full use of context information to align cross-modal semantic concepts in the extracted information. Unfortunately, existing work on grounding referring expressions fails to accurately extract multi-order relationships from the referring expression and associate them with the objects and their related contexts in the image. In this paper, we propose a Cross-Modal Relationship Extractor (CMRE) to adaptively highlight objects and relationships (spatial and semantic relations) related to the given expression with a cross-modal attention mechanism, and represent the extracted information as a language-guided visual relation graph. In addition, we propose a Gated Graph Convolutional Network (GGCN) to compute multimodal semantic contexts by fusing information from different modes and propagating multimodal information in the structured relation graph. Experimental results on three common benchmark datasets show that our Cross-Modal Relationship Inference Network, which consists of CMRE and GGCN, significantly surpasses all existing state-of-the-art methods. Code is available at https://github.com/sibeiyang/sgmn/tree/master/lib/cmrin_models",0
"The process of grounding referring expressions in images involves identifying the object being described by a phrase in a given image. This task requires a comprehensive understanding of both natural language and the visual content of the image, and is crucial for various visual tasks related to human-computer interaction. The main challenge lies in extracting all the necessary information, including objects and their relationships, from both the image and the language used to describe it, while also taking into account contextual information to align semantic concepts across different modalities. However, current approaches to grounding referring expressions do not accurately extract multi-order relationships from the language used, nor do they associate them with objects and their respective contexts in the image. To address this issue, we propose a Cross-Modal Relationship Extractor (CMRE) that uses a cross-modal attention mechanism to highlight relevant objects and relationships, and represent them as a visual relation graph guided by language. We also introduce a Gated Graph Convolutional Network (GGCN) to compute multimodal semantic contexts by fusing information from different sources and propagating it through the structured relation graph. Our experimental results on three benchmark datasets demonstrate that our approach outperforms all existing state-of-the-art methods, and the code is available at https://github.com/sibeiyang/sgmn/tree/master/lib/cmrin_models.",1
"Grounding referring expressions aims to locate in an image an object referred to by a natural language expression. The linguistic structure of a referring expression provides a layout of reasoning over the visual contents, and it is often crucial to align and jointly understand the image and the referring expression. In this paper, we propose a scene graph guided modular network (SGMN), which performs reasoning over a semantic graph and a scene graph with neural modules under the guidance of the linguistic structure of the expression. In particular, we model the image as a structured semantic graph, and parse the expression into a language scene graph. The language scene graph not only decodes the linguistic structure of the expression, but also has a consistent representation with the image semantic graph. In addition to exploring structured solutions to grounding referring expressions, we also propose Ref-Reasoning, a large-scale real-world dataset for structured referring expression reasoning. We automatically generate referring expressions over the scene graphs of images using diverse expression templates and functional programs. This dataset is equipped with real-world visual contents as well as semantically rich expressions with different reasoning layouts. Experimental results show that our SGMN not only significantly outperforms existing state-of-the-art algorithms on the new Ref-Reasoning dataset, but also surpasses state-of-the-art structured methods on commonly used benchmark datasets. It can also provide interpretable visual evidences of reasoning. Data and code are available at https://github.com/sibeiyang/sgmn",0
"The goal of grounding referring expressions is to locate an object in an image using natural language. The structure of the language used in the expression is important for understanding the visual content, so it is necessary to align the image and the language. To achieve this, we propose a modular network called SGMN, which uses neural modules to reason over a semantic graph and a scene graph guided by the language structure. The image is modeled as a semantic graph, while the expression is parsed into a language scene graph that is consistent with the image graph. We also introduce Ref-Reasoning, a dataset for structured referring expression reasoning that includes real-world images and diverse expression templates. Our SGMN outperforms state-of-the-art algorithms on the dataset and provides interpretable visual evidence of reasoning. The code and data are available at https://github.com/sibeiyang/sgmn.",1
"Supervised machine learning (ML) algorithms are aimed at maximizing classification performance under available energy and storage constraints. They try to map the training data to the corresponding labels while ensuring generalizability to unseen data. However, they do not integrate meaning-based relationships among labels in the decision process. On the other hand, natural language processing (NLP) algorithms emphasize the importance of semantic information. In this paper, we synthesize the complementary advantages of supervised ML and NLP algorithms into one method that we refer to as SECRET (Semantically Enhanced Classification of REal-world Tasks). SECRET performs classifications by fusing the semantic information of the labels with the available data: it combines the feature space of the supervised algorithms with the semantic space of the NLP algorithms and predicts labels based on this joint space. Experimental results indicate that, compared to traditional supervised learning, SECRET achieves up to 14.0% accuracy and 13.1% F1 score improvements. Moreover, compared to ensemble methods, SECRET achieves up to 12.7% accuracy and 13.3% F1 score improvements. This points to a new research direction for supervised classification based on incorporation of semantic information.",0
"Supervised machine learning algorithms are designed to optimize classification performance while adhering to energy and storage constraints. These algorithms map training data to their respective labels and strive for generalizability to new data. However, they do not consider meaning-based relationships between labels when making decisions. Conversely, natural language processing algorithms prioritize semantic information. This paper presents a new method called Semantically Enhanced Classification of Real-world Tasks (SECRET) that combines the advantages of supervised ML and NLP algorithms. SECRET fuses the semantic information of labels with available data by integrating the feature space of supervised algorithms with the semantic space of NLP algorithms. Experimental results demonstrate that SECRET outperforms traditional supervised learning and ensemble methods in terms of accuracy and F1 score improvements by up to 14.0% and 13.1%, respectively. This research direction highlights the importance of incorporating semantic information into supervised classification.",1
"Recently, the bidirectional encoder representations from transformers (BERT) model has attracted much attention in the field of natural language processing, owing to its high performance in language understanding-related tasks. The BERT model learns language representation that can be adapted to various tasks via pre-training using a large corpus in an unsupervised manner. This study proposes the language and action learning using multimodal BERT (lamBERT) model that enables the learning of language and actions by 1) extending the BERT model to multimodal representation and 2) integrating it with reinforcement learning. To verify the proposed model, an experiment is conducted in a grid environment that requires language understanding for the agent to act properly. As a result, the lamBERT model obtained higher rewards in multitask settings and transfer settings when compared to other models, such as the convolutional neural network-based model and the lamBERT model without pre-training.",0
"The BERT model, which is a bidirectional encoder representation from transformers, has been gaining considerable attention in the realm of natural language processing due to its exceptional performance in tasks related to language understanding. By training on a large corpus in an unsupervised manner, the BERT model acquires adaptable language representations for various tasks. This research introduces the lamBERT model, which is a multimodal extension of the BERT model that incorporates reinforcement learning to enable language and action learning. A grid environment experiment was conducted to evaluate the proposed model's effectiveness in language comprehension for agent action. The results show that the lamBERT model outperformed other models, including the convolutional neural network-based model and the lamBERT model without pre-training, in multitask and transfer settings, receiving higher rewards.",1
"Benefiting from advances in machine vision and natural language processing techniques, current image captioning systems are able to generate detailed visual descriptions. For the most part, these descriptions represent an objective characterisation of the image, although some models do incorporate subjective aspects related to the observer's view of the image, such as sentiment; current models, however, usually do not consider the emotional content of images during the caption generation process. This paper addresses this issue by proposing novel image captioning models which use facial expression features to generate image captions. The models generate image captions using long short-term memory networks applying facial features in addition to other visual features at different time steps. We compare a comprehensive collection of image captioning models with and without facial features using all standard evaluation metrics. The evaluation metrics indicate that applying facial features with an attention mechanism achieves the best performance, showing more expressive and more correlated image captions, on an image caption dataset extracted from the standard Flickr 30K dataset, consisting of around 11K images containing faces. An analysis of the generated captions finds that, perhaps unexpectedly, the improvement in caption quality appears to come not from the addition of adjectives linked to emotional aspects of the images, but from more variety in the actions described in the captions.",0
"Thanks to advancements in techniques for machine vision and natural language processing, current systems for image captioning can produce highly detailed visual descriptions. Generally, these descriptions offer an objective depiction of the image, although some models take into account more subjective elements such as the observer's emotional response. However, most models currently in use do not consider the emotional content of an image during the captioning process. This study addresses this gap by introducing new image captioning models that incorporate facial expression features to generate image captions. By employing long short-term memory networks and facial features at different time steps, these models produce captions that are more expressive and correlated with the image. We compared a range of image captioning models, with and without facial features, using standard evaluation metrics. Our findings demonstrate that the application of facial features combined with an attention mechanism yields the best results. Surprisingly, the improvement in caption quality stems not from the addition of adjectives related to the emotions evoked by the images, but from greater variety in the actions described in the captions. Our data set, which we extracted from the standard Flickr 30K dataset, includes around 11,000 images featuring faces.",1
"The goal of the YouMakeup VQA Challenge 2020 is to provide a common benchmark for fine-grained action understanding in domain-specific videos e.g. makeup instructional videos. We propose two novel question-answering tasks to evaluate models' fine-grained action understanding abilities. The first task is \textbf{Facial Image Ordering}, which aims to understand visual effects of different actions expressed in natural language to the facial object. The second task is \textbf{Step Ordering}, which aims to measure cross-modal semantic alignments between untrimmed videos and multi-sentence texts. In this paper, we present the challenge guidelines, the dataset used, and performances of baseline models on the two proposed tasks. The baseline codes and models are released at \url{https://github.com/AIM3-RUC/YouMakeup_Baseline}.",0
"The main objective of the YouMakeup VQA Challenge 2020 is to establish a standard benchmark for the precise comprehension of actions in domain-specific videos, specifically makeup instructional videos. Two distinct question-answering tasks are proposed to assess the fine-grained action understanding abilities of models. The first task, called ""Facial Image Ordering,"" intends to discern the visual effects of diverse actions expressed in natural language on the facial object. The second task, termed ""Step Ordering,"" seeks to evaluate the semantic alignment between untrimmed videos and multi-sentence texts. This paper presents the challenge guidelines, dataset, and baseline model performances on the two tasks. The baseline models and codes are available at \url{https://github.com/AIM3-RUC/YouMakeup_Baseline}.",1
"Image classification has been studied extensively but there has been limited work in the direction of using non-conventional, external guidance other than traditional image-label pairs to train such models. In this thesis we present a set of methods to leverage information about the semantic hierarchy induced by class labels. In the first part of the thesis, we inject label-hierarchy knowledge to an arbitrary classifier and empirically show that availability of such external semantic information in conjunction with the visual semantics from images boosts overall performance. Taking a step further in this direction, we model more explicitly the label-label and label-image interactions by using order-preserving embedding-based models, prevalent in natural language, and tailor them to the domain of computer vision to perform image classification. Although, contrasting in nature, both the CNN-classifiers injected with hierarchical information, and the embedding-based models outperform a hierarchy-agnostic model on the newly presented, real-world ETH Entomological Collection image dataset https://www.research-collection.ethz.ch/handle/20.500.11850/365379.",0
"Extensive research has been conducted on image classification, but the use of unconventional external guidance beyond traditional image-label pairs to train such models has been limited. This thesis introduces a range of techniques that utilize information about the semantic hierarchy induced by class labels. The first part of the thesis incorporates label-hierarchy knowledge into an arbitrary classifier and demonstrates through empirical evidence that the availability of external semantic information in conjunction with visual semantics from images improves overall performance. In the second part of the thesis, label-label and label-image interactions are explicitly modeled using order-preserving embedding-based models that are prevalent in natural language and customized for computer vision to conduct image classification. Despite their contrasting nature, both the CNN-classifiers injected with hierarchical information and the embedding-based models outperform a hierarchy-agnostic model on the newly presented, real-world ETH Entomological Collection image dataset https://www.research-collection.ethz.ch/handle/20.500.11850/365379.",1
"Recent developments in image classification and natural language processing, coupled with the rapid growth in social media usage, have enabled fundamental advances in detecting breaking events around the world in real-time. Emergency response is one such area that stands to gain from these advances. By processing billions of texts and images a minute, events can be automatically detected to enable emergency response workers to better assess rapidly evolving situations and deploy resources accordingly. To date, most event detection techniques in this area have focused on image-only or text-only approaches, limiting detection performance and impacting the quality of information delivered to crisis response teams. In this paper, we present a new multimodal fusion method that leverages both images and texts as input. In particular, we introduce a cross-attention module that can filter uninformative and misleading components from weak modalities on a sample by sample basis. In addition, we employ a multimodal graph-based approach to stochastically transition between embeddings of different multimodal pairs during training to better regularize the learning process as well as dealing with limited training data by constructing new matched pairs from different samples. We show that our method outperforms the unimodal approaches and strong multimodal baselines by a large margin on three crisis-related tasks.",0
"The advancement in image classification, natural language processing, and the widespread use of social media have paved the way for detecting breaking events in real-time. Such progress can significantly benefit emergency response workers who can better assess rapidly changing situations and allocate resources accordingly. However, most existing event detection techniques focus exclusively on images or texts, which limits performance and reduces the quality of information delivered to crisis response teams. In this paper, we introduce a new approach that combines both images and texts as input, using a cross-attention module to filter out irrelevant components and a multimodal graph-based approach to improve the learning process and deal with limited training data. Our method outperforms unimodal approaches and strong multimodal baselines by a significant margin on three crisis-related tasks.",1
"This paper presents a framework for the analysis of changes in visual streams: ordered sequences of images, possibly separated by significant time gaps. We propose a new approach to incorporating unlabeled data into training to generate natural language descriptions of change. We also develop a framework for estimating the time of change in visual stream. We use learned representations for change evidence and consistency of perceived change, and combine these in a regularized graph cut based change detector. Experimental evaluation on visual stream datasets, which we release as part of our contribution, shows that representation learning driven by natural language descriptions significantly improves change detection accuracy, compared to methods that do not rely on language.",0
"In this paper, a framework is presented to analyze changes in visual streams, which are sequences of images with possible time gaps. The authors propose a novel method to include unlabeled data in the training process to generate natural language descriptions of change. Furthermore, they develop a framework to estimate the timing of change in visual streams by utilizing learned representations for change evidence and consistency of perceived change, and combining them in a regularized graph cut-based change detector. The authors release visual stream datasets as part of their contribution, and experimental evaluation shows that utilizing natural language descriptions in representation learning significantly improves change detection accuracy compared to methods that do not rely on language.",1
"We address the problem of video grounding from natural language queries. The key challenge in this task is that one training video might only contain a few annotated starting/ending frames that can be used as positive examples for model training. Most conventional approaches directly train a binary classifier using such imbalance data, thus achieving inferior results. The key idea of this paper is to use the distances between the frame within the ground truth and the starting (ending) frame as dense supervisions to improve the video grounding accuracy. Specifically, we design a novel dense regression network (DRN) to regress the distances from each frame to the starting (ending) frame of the video segment described by the query. We also propose a simple but effective IoU regression head module to explicitly consider the localization quality of the grounding results (i.e., the IoU between the predicted location and the ground truth). Experimental results show that our approach significantly outperforms state-of-the-arts on three datasets (i.e., Charades-STA, ActivityNet-Captions, and TACoS).",0
"The issue of video grounding from natural language inquiries is tackled in this paper. The main difficulty in this task is that a single training video may have only a few annotated starting/ending frames that can be utilized as positive examples for model training. Traditional methods directly train a binary classifier using this imbalanced data, resulting in subpar outcomes. To enhance video grounding accuracy, this paper proposes using the distances between the frame within the ground truth and the starting (ending) frame as dense supervisions. To achieve this, a novel dense regression network (DRN) is created to regress the distances from each frame to the starting (ending) frame of the video segment described by the query. Additionally, a simple but effective IoU regression head module is proposed to explicitly consider the localization quality of the grounding results. Experimental results reveal that this approach outperforms state-of-the-art methods on three datasets (Charades-STA, ActivityNet-Captions, and TACoS).",1
"In this paper we present an approach and a benchmark for visual reasoning in robotics applications, in particular small object grasping and manipulation. The approach and benchmark are focused on inferring object properties from visual and text data. It concerns small household objects with their properties, functionality, natural language descriptions as well as question-answer pairs for visual reasoning queries along with their corresponding scene semantic representations. We also present a method for generating synthetic data which allows to extend the benchmark to other objects or scenes and propose an evaluation protocol that is more challenging than in the existing datasets. We propose a reasoning system based on symbolic program execution. A disentangled representation of the visual and textual inputs is obtained and used to execute symbolic programs that represent a 'reasoning process' of the algorithm. We perform a set of experiments on the proposed benchmark and compare to results for the state of the art methods. These results expose the shortcomings of the existing benchmarks that may lead to misleading conclusions on the actual performance of the visual reasoning systems.",0
"The main focus of this paper is to introduce a new approach and benchmark for visual reasoning specifically for small object grasping and manipulation in robotics applications. The approach and benchmark aim to deduce object properties through visual and text data, which includes small household objects, their properties, functions, as well as natural language descriptions and question-answer pairs for visual reasoning queries. The corresponding scene semantic representations are also included. To extend the benchmark to other objects or scenes, we propose a method for generating synthetic data and an evaluation protocol that is more challenging than in existing datasets. Our reasoning system is based on symbolic program execution, where a disentangled visual and textual input representation is used to execute symbolic programs that represent a 'reasoning process' of the algorithm. We conducted experiments on the proposed benchmark and compared the results to state-of-the-art methods. Our findings highlight the shortcomings of existing benchmarks, which can lead to misleading conclusions about the actual performance of visual reasoning systems.",1
"Generative adversarial networks (GANs) are increasingly attracting attention in the computer vision, natural language processing, speech synthesis and similar domains. However, evaluating the performance of GANs is still an open and challenging problem. Existing evaluation metrics primarily measure the dissimilarity between real and generated images using automated statistical methods. They often require large sample sizes for evaluation and do not directly reflect human perception of image quality. In this work, we introduce an evaluation metric called Neuroscore, for evaluating the performance of GANs, that more directly reflects psychoperceptual image quality through the utilization of brain signals. Our results show that Neuroscore has superior performance to the current evaluation metrics in that: (1) It is more consistent with human judgment; (2) The evaluation process needs much smaller numbers of samples; and (3) It is able to rank the quality of images on a per GAN basis. A convolutional neural network (CNN) based neuro-AI interface is proposed to predict Neuroscore from GAN-generated images directly without the need for neural responses. Importantly, we show that including neural responses during the training phase of the network can significantly improve the prediction capability of the proposed model. Codes and data can be referred at this link: https://github.com/villawang/Neuro-AI-Interface.",0
"GANs are gaining popularity in various fields such as computer vision, natural language processing, and speech synthesis. However, evaluating their performance is still a challenging issue. The current evaluation metrics measure the difference between real and generated images using statistical methods. They require a large sample size for evaluation and do not reflect human perception accurately. To address this problem, we propose a new evaluation metric called Neuroscore, which utilizes brain signals to directly reflect the psychoperceptual image quality. Our study shows that Neuroscore outperforms the existing evaluation metrics in terms of consistency with human judgment, requiring fewer samples, and ranking the image quality per GAN basis. We also introduce a neuro-AI interface based on CNNs to predict Neuroscore from GAN-generated images without the need for neural responses. Furthermore, incorporating neural responses during the training phase significantly improves the prediction capability of our proposed model. The codes and data are available at https://github.com/villawang/Neuro-AI-Interface.",1
"Learning to navigate in a visual environment following natural-language instructions is a challenging task, because the multimodal inputs to the agent are highly variable, and the training data on a new task is often limited. In this paper, we present the first pre-training and fine-tuning paradigm for vision-and-language navigation (VLN) tasks. By training on a large amount of image-text-action triplets in a self-supervised learning manner, the pre-trained model provides generic representations of visual environments and language instructions. It can be easily used as a drop-in for existing VLN frameworks, leading to the proposed agent called Prevalent. It learns more effectively in new tasks and generalizes better in a previously unseen environment. The performance is validated on three VLN tasks. On the Room-to-Room benchmark, our model improves the state-of-the-art from 47% to 51% on success rate weighted by path length. Further, the learned representation is transferable to other VLN tasks. On two recent tasks, vision-and-dialog navigation and ""Help, Anna!"" the proposed Prevalent leads to significant improvement over existing methods, achieving a new state of the art.",0
"The task of navigating in a visual environment based on natural-language instructions is difficult due to the varied multimodal inputs and limited training data. To address this, we introduce a new pre-training and fine-tuning approach for vision-and-language navigation (VLN) tasks in this paper. Our model is pre-trained on a large dataset of image-text-action triplets using self-supervised learning, which generates generic representations of visual environments and language instructions. This pre-trained model, named Prevalent, can be easily integrated into existing VLN frameworks and performs better in new tasks and unseen environments. We evaluate the performance of Prevalent on three VLN tasks and demonstrate its state-of-the-art performance on the Room-to-Room benchmark, improving the success rate weighted by path length from 47% to 51%. Additionally, the learned representation is transferable to other VLN tasks, leading to significant improvements on two recent tasks, vision-and-dialog navigation and ""Help, Anna!"", and achieving a new state of the art.",1
"In many scenarios, humans prefer a text-based representation of quantitative data over numerical, tabular, or graphical representations. The attractiveness of textual summaries for complex data has inspired research on data-to-text systems. While there are several data-to-text tools for time series, few of them try to mimic how humans summarize for time series. In this paper, we propose a model to create human-like text descriptions for time series. Our system finds patterns in time series data and ranks these patterns based on empirical observations of human behavior using utility estimation. Our proposed utility estimation model is a Bayesian network capturing interdependencies between different patterns. We describe the learning steps for this network and introduce baselines along with their performance for each step. The output of our system is a natural language description of time series that attempts to match a human's summary of the same data.",0
"Text-based representations of quantitative data are often preferred by humans over numerical, tabular, or graphical formats in various scenarios. Consequently, research on data-to-text systems has been motivated by the appeal of textual summaries for complex data. However, while several data-to-text tools exist for time series, few of them attempt to replicate the way humans summarize such data. This paper proposes a model for generating human-like text descriptions of time series. The proposed system identifies patterns in time series data and ranks them based on observations of human behavior, using a utility estimation approach. This approach involves a Bayesian network that captures the interdependencies between different patterns. The paper outlines the learning process for this network and presents baselines and their performance for each step. The resulting output is a natural language summary of time series data that aims to replicate a human's summary of the same data.",1
"A sum-product network (SPN) is a probabilistic model, based on a rooted acyclic directed graph, in which terminal nodes represent univariate probability distributions and non-terminal nodes represent convex combinations (weighted sums) and products of probability functions. They are closely related to probabilistic graphical models, in particular to Bayesian networks with multiple context-specific independencies. Their main advantage is the possibility of building tractable models from data, i.e., models that can perform several inference tasks in time proportional to the number of links in the graph. They are somewhat similar to neural networks and can address the same kinds of problems, such as image processing and natural language understanding. This paper offers a survey of SPNs, including their definition, the main algorithms for inference and learning from data, the main applications, a brief review of software libraries, and a comparison with related models",0
"The sum-product network (SPN) is a type of probabilistic model that employs a directed acyclic graph. The graph consists of non-terminal nodes, representing convex combinations and products of probability functions, and terminal nodes, representing univariate probability distributions. SPNs are similar to Bayesian networks, with multiple context-specific independencies, and are advantageous in building models that are tractable and can perform various inference tasks in a time proportional to the number of links in the graph. These models can address similar problems as neural networks, such as natural language understanding and image processing. The paper provides a comprehensive overview of SPNs, including their definition, main algorithms for inference and learning from data, applications, software libraries, and a comparison with related models.",1
"Overparameterized neural networks can be highly accurate on average on an i.i.d. test set yet consistently fail on atypical groups of the data (e.g., by learning spurious correlations that hold on average but not in such groups). Distributionally robust optimization (DRO) allows us to learn models that instead minimize the worst-case training loss over a set of pre-defined groups. However, we find that naively applying group DRO to overparameterized neural networks fails: these models can perfectly fit the training data, and any model with vanishing average training loss also already has vanishing worst-case training loss. Instead, the poor worst-case performance arises from poor generalization on some groups. By coupling group DRO models with increased regularization---a stronger-than-typical L2 penalty or early stopping---we achieve substantially higher worst-group accuracies, with 10-40 percentage point improvements on a natural language inference task and two image tasks, while maintaining high average accuracies. Our results suggest that regularization is important for worst-group generalization in the overparameterized regime, even if it is not needed for average generalization. Finally, we introduce a stochastic optimization algorithm, with convergence guarantees, to efficiently train group DRO models.",0
"Although overparameterized neural networks may be highly accurate on an i.i.d. test set, they can consistently fail when presented with atypical groups of data due to learning spurious correlations that only hold on average. Distributionally robust optimization (DRO) allows us to minimize the worst-case training loss over a set of predetermined groups, but simply applying group DRO to overparameterized neural networks fails. This is because these models can perfectly fit the training data, and any model with vanishing average training loss already has vanishing worst-case training loss. Instead, poor worst-case performance arises from poor generalization on some groups. However, coupling group DRO models with increased regularization, such as a stronger-than-typical L2 penalty or early stopping, can substantially improve worst-group accuracies by 10-40 percentage points on natural language inference and two image tasks, while maintaining high average accuracies. Our findings suggest that regularization is important for worst-group generalization in the overparameterized regime, even if it is not necessary for average generalization. Finally, we introduce a stochastic optimization algorithm that guarantees convergence to efficiently train group DRO models.",1
"Convolutional Neural Network (CNN) is one of the most significant networks in the deep learning field. Since CNN made impressive achievements in many areas, including but not limited to computer vision and natural language processing, it attracted much attention both of industry and academia in the past few years. The existing reviews mainly focus on the applications of CNN in different scenarios without considering CNN from a general perspective, and some novel ideas proposed recently are not covered. In this review, we aim to provide novel ideas and prospects in this fast-growing field as much as possible. Besides, not only two-dimensional convolution but also one-dimensional and multi-dimensional ones are involved. First, this review starts with a brief introduction to the history of CNN. Second, we provide an overview of CNN. Third, classic and advanced CNN models are introduced, especially those key points making them reach state-of-the-art results. Fourth, through experimental analysis, we draw some conclusions and provide several rules of thumb for function selection. Fifth, the applications of one-dimensional, two-dimensional, and multi-dimensional convolution are covered. Finally, some open issues and promising directions for CNN are discussed to serve as guidelines for future work.",0
"The Convolutional Neural Network (CNN) has emerged as a significant network in the field of deep learning. Its remarkable accomplishments in various domains, such as computer vision and natural language processing, have captured the attention of both industry and academia in recent years. However, previous reviews have only focused on its applications in different scenarios without providing a comprehensive analysis of CNN. This review aims to offer fresh ideas and prospects in this rapidly advancing field. It covers not only two-dimensional convolution but also one-dimensional and multi-dimensional ones. The paper begins with a brief history of CNN, followed by an overview of the network. Classic and advanced CNN models are discussed, highlighting the critical factors that have helped them achieve state-of-the-art results. We draw conclusions and provide some rules of thumb for function selection based on experimental analysis. Additionally, we explore the applications of one-dimensional, two-dimensional, and multi-dimensional convolution. Lastly, we discuss some open issues and promising directions for CNN, which can serve as guidelines for future research.",1
"Vision-Language Navigation (VLN) is a task where agents learn to navigate following natural language instructions. The key to this task is to perceive both the visual scene and natural language sequentially. Conventional approaches exploit the vision and language features in cross-modal grounding. However, the VLN task remains challenging, since previous works have neglected the rich semantic information contained in the environment (such as implicit navigation graphs or sub-trajectory semantics). In this paper, we introduce Auxiliary Reasoning Navigation (AuxRN), a framework with four self-supervised auxiliary reasoning tasks to take advantage of the additional training signals derived from the semantic information. The auxiliary tasks have four reasoning objectives: explaining the previous actions, estimating the navigation progress, predicting the next orientation, and evaluating the trajectory consistency. As a result, these additional training signals help the agent to acquire knowledge of semantic representations in order to reason about its activity and build a thorough perception of the environment. Our experiments indicate that auxiliary reasoning tasks improve both the performance of the main task and the model generalizability by a large margin. Empirically, we demonstrate that an agent trained with self-supervised auxiliary reasoning tasks substantially outperforms the previous state-of-the-art method, being the best existing approach on the standard benchmark.",0
"The VLN task involves agents learning to navigate based on natural language instructions while perceiving both visual scenes and language sequentially. Traditional methods utilize cross-modal grounding to exploit vision and language features, but they neglect the semantic information present in the environment, such as implicit navigation graphs or sub-trajectory semantics. To address this limitation, we propose the AuxRN framework, which includes four self-supervised auxiliary reasoning tasks that provide additional training signals derived from semantic information. These tasks aim to explain previous actions, estimate navigation progress, predict the next orientation, and evaluate trajectory consistency. By incorporating these signals, the agent gains knowledge of semantic representations, enabling it to reason about its activity and develop a comprehensive perception of the environment. Our experiments demonstrate that the auxiliary reasoning tasks improve both task performance and model generalizability significantly. In fact, our agent outperforms the previous state-of-the-art method and is the best approach on the standard benchmark.",1
"Source code summarizing is a task of writing short, natural language descriptions of source code behavior during run time. Such summaries are extremely useful for software development and maintenance but are expensive to manually author,hence it is done for small fraction of the code that is produced and is often ignored. Automatic code documentation can possibly solve this at a low cost. This is thus an emerging research field with further applications to program comprehension, and software maintenance. Traditional methods often relied on cognitive models that were built in the form of templates and by heuristics and had varying degree of adoption by the developer community. But with recent advancements, end to end data-driven approaches based on neural techniques have largely overtaken the traditional techniques. Much of the current landscape employs neural translation based architectures with recurrence and attention which is resource and time intensive training procedure. In this paper, we employ neural techniques to solve the task of source code summarizing and specifically compare NMT based techniques to more simplified and appealing Transformer architecture on a dataset of Java methods and comments. We bring forth an argument to dispense the need of recurrence in the training procedure. To the best of our knowledge, transformer based models have not been used for the task before. With supervised samples of more than 2.1m comments and code, we reduce the training time by more than 50% and achieve the BLEU score of 17.99 for the test set of examples.",0
"Summarizing source code involves creating concise, natural language descriptions of the code's behavior during runtime. While these summaries are valuable for software development and maintenance, they can be costly to generate manually, leading to only a small fraction of produced code being summarized. Automatic code documentation can address this issue at a lower cost and has become a burgeoning research field with implications for program comprehension and software maintenance. Traditional methods for summarizing code relied on cognitive models built from templates and heuristics, with varying degrees of adoption among developers. However, recent advancements have led to data-driven approaches using neural techniques that largely surpass traditional methods. Many current techniques use neural translation with recurrent and attention-based architectures, which require extensive time and resources for training. In this paper, we apply neural techniques to the task of summarizing Java methods and comments, specifically comparing NMT-based techniques to a simplified Transformer architecture. We argue against the need for recurrence in the training process and show that transformer-based models have not been used for this task before. With over 2.1 million supervised samples, we reduce training time by over 50% and achieve a BLEU score of 17.99 for the test set of examples.",1
"In the Vision-and-Language Navigation (VLN) task, an agent with egocentric vision navigates to a destination given natural language instructions. The act of manually annotating these instructions is timely and expensive, such that many existing approaches automatically generate additional samples to improve agent performance. However, these approaches still have difficulty generalizing their performance to new environments. In this work, we investigate the popular Room-to-Room (R2R) VLN benchmark and discover that what is important is not only the amount of data you synthesize, but also how you do it. We find that shortest path sampling, which is used by both the R2R benchmark and existing augmentation methods, encode biases in the action space of the agent which we dub as action priors. We then show that these action priors offer one explanation toward the poor generalization of existing works. To mitigate such priors, we propose a path sampling method based on random walks to augment the data. By training with this augmentation strategy, our agent is able to generalize better to unknown environments compared to the baseline, significantly improving model performance in the process.",0
"The VLN task involves an agent with egocentric vision using natural language instructions to navigate to a destination. However, manually annotating these instructions is costly and time-consuming, leading to the use of automatic generation methods to improve agent performance. These methods still struggle to perform well in new environments. Our research focuses on the R2R VLN benchmark and shows that the quality of synthesized data is crucial, not just the quantity. Shortest path sampling, used by R2R and existing methods, introduces biases in the agent's action space, negatively impacting generalization. We propose a path sampling method based on random walks to mitigate these biases and improve generalization. Our agent trained with this method outperforms the baseline, significantly improving model performance.",1
"We present ALFRED (Action Learning From Realistic Environments and Directives), a benchmark for learning a mapping from natural language instructions and egocentric vision to sequences of actions for household tasks. ALFRED includes long, compositional tasks with non-reversible state changes to shrink the gap between research benchmarks and real-world applications. ALFRED consists of expert demonstrations in interactive visual environments for 25k natural language directives. These directives contain both high-level goals like ""Rinse off a mug and place it in the coffee maker."" and low-level language instructions like ""Walk to the coffee maker on the right."" ALFRED tasks are more complex in terms of sequence length, action space, and language than existing vision-and-language task datasets. We show that a baseline model based on recent embodied vision-and-language tasks performs poorly on ALFRED, suggesting that there is significant room for developing innovative grounded visual language understanding models with this benchmark.",0
"Introducing ALFRED (Action Learning From Realistic Environments and Directives), a benchmark designed to teach mapping from natural language instructions and egocentric vision to sequences of actions for household tasks. This benchmark focuses on long, compositional tasks with non-reversible state changes to narrow the gap between research benchmarks and real-world applications. In interactive visual environments, ALFRED includes expert demonstrations for 25k natural language directives, containing both high-level goals and low-level language instructions. Compared to existing vision-and-language task datasets, ALFRED tasks are more complex in terms of sequence length, action space, and language. Our study shows that the baseline model, based on recent embodied vision-and-language tasks, performs poorly on ALFRED, indicating the potential for innovative grounded visual language understanding models.",1
"The underlying structure of natural language is hierarchical; words combine into phrases, which in turn form clauses. An awareness of this hierarchical structure can aid machine learning models in performing many linguistic tasks. However, most such models just process text sequentially and there is no bias towards learning hierarchical structure encoded into their architecture. In this paper, we extend the recent transformer model (Vaswani et al., 2017) by enabling it to learn hierarchical representations. To achieve this, we adapt the ordering mechanism introduced in Shen et al., 2018, to the self-attention module of the transformer architecture. We train our new model on language modelling and then apply it to the task of unsupervised parsing. We achieve reasonable results on the freely available subset of the WSJ10 dataset with an F1-score of about 50%.",0
"The structure of natural language is hierarchical, where words combine to form phrases and clauses. This hierarchy can be useful for machine learning models in various linguistic tasks. However, most models currently process text sequentially without any emphasis on hierarchical learning. To address this, we enhance the transformer model by incorporating a mechanism from Shen et al., 2018, to enable it to learn hierarchical representations. We train the model on language modelling and test it on unsupervised parsing, achieving a reasonable F1-score of around 50% on the freely available subset of the WSJ10 dataset.",1
"Artificial Neural Networks (ANNs) replaced conventional software systems in various domains such as machine translation, natural language processing, and image processing. So, why do we need an repository for artificial neural networks? Those systems are developed with labeled data and we have strong dependencies between the data that is used for training and testing our network. Another challenge is the data quality as well as reuse-ability. There we are trying to apply concepts from classic software engineering that is not limited to the model, while data and code haven't been dealt with mostly in other projects. The first question that comes to mind might be, why don't we use GitHub, a well known widely spread tool for reuse, for our issue. And the reason why is that GitHub, although very good in its class is not developed for machine learning appliances and focuses more on software reuse. In addition to that GitHub does not allow to execute the code directly on the platform which would be very convenient for collaborative work on one project.",0
"Various domains, including machine translation, natural language processing, and image processing, have replaced traditional software systems with Artificial Neural Networks (ANNs). However, the need for a repository for artificial neural networks arises due to the strong dependence between the labeled data used for training and testing the network. Additionally, issues such as data quality and reuse-ability require the application of classic software engineering concepts, which are not limited to the model. Although GitHub is a well-known and widely-used tool for software reuse, it is not developed for machine learning appliances and does not allow for direct code execution on the platform, making it unsuitable for collaborative work on a single project.",1
"The goal of weakly-supervised video moment retrieval is to localize the video segment most relevant to the given natural language query without access to temporal annotations during training. Prior strongly- and weakly-supervised approaches often leverage co-attention mechanisms to learn visual-semantic representations for localization. However, while such approaches tend to focus on identifying relationships between elements of the video and language modalities, there is less emphasis on modeling relational context between video frames given the semantic context of the query. Consequently, the above-mentioned visual-semantic representations, built upon local frame features, do not contain much contextual information. To address this limitation, we propose a Latent Graph Co-Attention Network (LoGAN) that exploits fine-grained frame-by-word interactions to reason about correspondences between all possible pairs of frames, given the semantic context of the query. Comprehensive experiments across two datasets, DiDeMo and Charades-Sta, demonstrate the effectiveness of our proposed latent co-attention model where it outperforms current state-of-the-art (SOTA) weakly-supervised approaches by a significant margin. Notably, it even achieves a 11% improvement to Recall@1 accuracy over strongly-supervised SOTA methods on DiDeMo.",0
"The objective of weakly-supervised video moment retrieval is to locate the most relevant video segment based on a natural language query without temporal annotations during training. Previous techniques have utilized co-attention mechanisms to develop visual-semantic representations for localization. However, these approaches have primarily focused on identifying relationships between elements of the video and language, with less emphasis on modeling relational context between video frames given the semantic context of the query. As a result, the visual-semantic representations constructed from local frame features lack contextual information. To overcome this shortcoming, we introduce a Latent Graph Co-Attention Network (LoGAN) that employs fine-grained frame-by-word interactions to reason about correspondences between all possible frame pairs, taking into account the semantic context of the query. Our comprehensive evaluations on DiDeMo and Charades-Sta datasets demonstrate the superior performance of our proposed latent co-attention model, with a significant margin over current state-of-the-art (SOTA) weakly-supervised methods. Notably, LoGAN achieves a 11% improvement in Recall@1 accuracy over strongly-supervised SOTA methods on DiDeMo.",1
"Recent research advances in Computer Vision and Natural Language Processing have introduced novel tasks that are paving the way for solving AI-complete problems. One of those tasks is called Visual Question Answering (VQA). A VQA system must take an image and a free-form, open-ended natural language question about the image, and produce a natural language answer as the output. Such a task has drawn great attention from the scientific community, which generated a plethora of approaches that aim to improve the VQA predictive accuracy. Most of them comprise three major components: (i) independent representation learning of images and questions; (ii) feature fusion so the model can use information from both sources to answer visual questions; and (iii) the generation of the correct answer in natural language. With so many approaches being recently introduced, it became unclear the real contribution of each component for the ultimate performance of the model. The main goal of this paper is to provide a comprehensive analysis regarding the impact of each component in VQA models. Our extensive set of experiments cover both visual and textual elements, as well as the combination of these representations in form of fusion and attention mechanisms. Our major contribution is to identify core components for training VQA models so as to maximize their predictive performance.",0
"In recent times, the field of Computer Vision and Natural Language Processing has made significant progress. This has resulted in the emergence of new tasks that can help in solving AI-complete problems. Visual Question Answering (VQA) is one such task, where the system is required to provide a natural language answer to an open-ended question about an image. This task has gained immense popularity in the scientific community, leading to the development of numerous approaches that aim to improve the predictive accuracy of VQA. Most of these approaches consist of three key components - independent representation learning for images and questions, feature fusion, and generation of natural language answers. However, the contribution of each component towards the overall performance of the model has become unclear due to the introduction of several new approaches. Therefore, this paper aims to comprehensively analyze the impact of each component on VQA models. The experiments conducted in this study cover both visual and textual elements, as well as the combination of these representations through fusion and attention mechanisms. The major contribution of this study is the identification of core components that can effectively train VQA models and maximize their predictive performance.",1
"We introduce a new task, Video-and-Language Inference, for joint multimodal understanding of video and text. Given a video clip with aligned subtitles as premise, paired with a natural language hypothesis based on the video content, a model needs to infer whether the hypothesis is entailed or contradicted by the given video clip. A new large-scale dataset, named Violin (VIdeO-and-Language INference), is introduced for this task, which consists of 95,322 video-hypothesis pairs from 15,887 video clips, spanning over 582 hours of video. These video clips contain rich content with diverse temporal dynamics, event shifts, and people interactions, collected from two sources: (i) popular TV shows, and (ii) movie clips from YouTube channels. In order to address our new multimodal inference task, a model is required to possess sophisticated reasoning skills, from surface-level grounding (e.g., identifying objects and characters in the video) to in-depth commonsense reasoning (e.g., inferring causal relations of events in the video). We present a detailed analysis of the dataset and an extensive evaluation over many strong baselines, providing valuable insights on the challenges of this new task.",0
"We present Video-and-Language Inference as a novel task to achieve joint multimodal comprehension of video and text. The task involves a model inferring whether a natural language hypothesis based on video content is entailed or contradicted by a given video clip with aligned subtitles as premise. To enable this task, we introduce a new dataset called Violin (VIdeO-and-Language INference), which includes 95,322 video-hypothesis pairs from 15,887 video clips, spanning over 582 hours of video. The dataset contains diverse content with various temporal dynamics, event shifts, and people interactions collected from popular TV shows and movie clips from YouTube channels. Sophisticated reasoning skills are required for a model to succeed in this task, ranging from surface-level grounding to in-depth commonsense reasoning. We conduct a detailed analysis of the dataset and evaluate many strong baselines, providing valuable insights into the challenges of this task.",1
"We explore the task of Video Object Grounding (VOG), which grounds objects in videos referred to in natural language descriptions. Previous methods apply image grounding based algorithms to address VOG, fail to explore the object relation information and suffer from limited generalization. Here, we investigate the role of object relations in VOG and propose a novel framework VOGNet to encode multi-modal object relations via self-attention with relative position encoding. To evaluate VOGNet, we propose novel contrasting sampling methods to generate more challenging grounding input samples, and construct a new dataset called ActivityNet-SRL (ASRL) based on existing caption and grounding datasets. Experiments on ASRL validate the need of encoding object relations in VOG, and our VOGNet outperforms competitive baselines by a significant margin.",0
"The task we delve into is Video Object Grounding (VOG), which refers to grounding objects in videos through natural language descriptions. Existing approaches use algorithms based on image grounding to tackle VOG, but they fall short in exploring object relation information and suffer from limited generalization. Our study focuses on the significance of object relations in VOG and proposes a new VOGNet framework that employs self-attention with relative position encoding to encode multi-modal object relations. To assess VOGNet's effectiveness, we introduce novel contrasting sampling methods that generate more challenging grounding input samples and create a new dataset called ActivityNet-SRL (ASRL) based on existing caption and grounding datasets. Our experiments on ASRL demonstrate the importance of encoding object relations in VOG, and VOGNet outperforms competitive baselines by a significant margin.",1
"In this paper, our focus is the connection and influence of language technologies on the research in neurolinguistics. We present a review of brain imaging-based neurolinguistic studies with a focus on the natural language representations, such as word embeddings and pre-trained language models. Mutual enrichment of neurolinguistics and language technologies leads to development of brain-aware natural language representations. The importance of this research area is emphasized by medical applications.",0
"Our paper delves into the relationship between language technologies and neurolinguistics, highlighting their impact on each other. Specifically, we provide an analysis of neurolinguistic research that utilizes brain imaging techniques, with a particular emphasis on natural language representations like pre-trained language models and word embeddings. By bridging the gap between these two fields, we can create brain-aware natural language representations that have far-reaching implications in the medical field and beyond. This research area is of paramount importance and deserves significant attention.",1
"Technology and the fruition of cultural heritage are becoming increasingly more entwined, especially with the advent of smart audio guides, virtual and augmented reality, and interactive installations. Machine learning and computer vision are important components of this ongoing integration, enabling new interaction modalities between user and museum. Nonetheless, the most frequent way of interacting with paintings and statues still remains taking pictures. Yet images alone can only convey the aesthetics of the artwork, lacking is information which is often required to fully understand and appreciate it. Usually this additional knowledge comes both from the artwork itself (and therefore the image depicting it) and from an external source of knowledge, such as an information sheet. While the former can be inferred by computer vision algorithms, the latter needs more structured data to pair visual content with relevant information. Regardless of its source, this information still must be be effectively transmitted to the user. A popular emerging trend in computer vision is Visual Question Answering (VQA), in which users can interact with a neural network by posing questions in natural language and receiving answers about the visual content. We believe that this will be the evolution of smart audio guides for museum visits and simple image browsing on personal smartphones. This will turn the classic audio guide into a smart personal instructor with which the visitor can interact by asking for explanations focused on specific interests. The advantages are twofold: on the one hand the cognitive burden of the visitor will decrease, limiting the flow of information to what the user actually wants to hear; and on the other hand it proposes the most natural way of interacting with a guide, favoring engagement.",0
"The integration of technology and cultural heritage is becoming increasingly intertwined, particularly with the emergence of smart audio guides, virtual and augmented reality, and interactive installations. This integration relies heavily on machine learning and computer vision to enable new forms of user-museum interactions. While taking pictures of paintings and statues remains the most common way to interact with them, images alone are insufficient to provide a complete understanding and appreciation of the artwork. Additional knowledge, often obtained from external sources, is necessary to fully comprehend the artwork. Visual Question Answering (VQA) is a popular trend in computer vision that allows users to interact with a neural network by asking questions about the visual content in natural language. This technology is believed to be the future of smart audio guides and will transform the classic audio guide into a smart personal instructor that users can interact with by asking for explanations focused on their specific interests. This approach reduces the cognitive burden on visitors and promotes engagement, making it the most natural way to interact with a guide.",1
"Discrete structure rules for validating molecular structures are usually limited to fulfillment of the octet rule or similar simple deterministic heuristics. We propose a model, inspired by language modeling from natural language processing, with the ability to learn from a collection of undirected molecular graphs, enabling fitting of any underlying structure rule present in the collection. We introduce an adaption to the popular Transformer model, which can learn relationships between atoms and bonds. To our knowledge, the Transformer adaption is the first model that is trained to solve the unsupervised task of recovering partially observed molecules. In this work, we assess how different degrees of information impact performance w.r.t. to fitting the QM9 dataset, which conforms to the octet rule, and to fitting the ZINC dataset, which contains hypervalent molecules and ions requiring the model to learn a more complex structure rule. More specifically, we test a full discrete graph with bond order information, a full discrete graph with only connectivity, a bag-of-neighbors, a bag-of-atoms, and a count-based unigram statistics. These results provide encouraging evidence that neural networks, even when only connectivity is available, can learn arbitrary molecular structure rules specific to a dataset, as the Transformer adaption surpasses a strong octet rule baseline on the ZINC dataset.",0
"The typical guidelines for validating molecular structures are limited to following simple rules such as the octet rule. In this study, we propose a model inspired by language modeling in natural language processing that can learn from a collection of undirected molecular graphs and adapt to fit any underlying structure rule present in the collection. Our adaptation to the Transformer model can establish relationships between atoms and bonds, and is the first to solve the unsupervised task of recovering partially observed molecules. We investigate how various levels of information affect the model's performance when fitting the QM9 dataset, which complies with the octet rule, and the ZINC dataset, which includes hypervalent molecules and ions that require the model to learn a more complex structure rule. Our tests include a full discrete graph with bond order information, a full discrete graph with only connectivity, a bag-of-neighbors, a bag-of-atoms, and a count-based unigram statistics. Our results reveal that neural networks can learn arbitrary molecular structure rules specific to a dataset, even with limited connectivity information. Notably, our Transformer adaptation outperforms a strong octet rule baseline on the ZINC dataset, providing promising evidence for future research.",1
"This paper investigates the idea of encoding object-centered representations in the design of the reward function and policy architectures of a language-guided reinforcement learning agent. This is done using a combination of object-wise permutation invariant networks inspired from Deep Sets and gated-attention mechanisms. In a 2D procedurally-generated world where agents targeting goals in natural language navigate and interact with objects, we show that these architectures demonstrate strong generalization capacities to out-of-distribution goals. We study the generalization to varying numbers of objects at test time and further extend the object-centered architectures to goals involving relational reasoning.",0
"The focus of this paper is to explore the concept of incorporating object-centered representations into the reward function and policy architectures of a reinforcement learning agent that is guided by language. To achieve this, a combination of object-wise permutation invariant networks, inspired by Deep Sets, and gated-attention mechanisms are employed. Using a 2D world that is procedurally generated, where agents navigate and interact with objects to achieve goals described in natural language, we demonstrate the efficacy of these architectures in their ability to generalize to out-of-distribution goals. We also investigate the ability of these architectures to generalize to varying numbers of objects during testing, and expand the object-centered architectures to include goals that involve relational reasoning.",1
"Referring expression comprehension (REC) aims to localize a text-related region in a given image by a referring expression in natural language. Existing methods focus on how to build convincing visual and language representations independently, which may significantly isolate visual and language information. In this paper, we argue that for REC the referring expression and the target region are semantically correlated and subject, location and relationship consistency exist between vision and language.On top of this, we propose a novel approach called MutAtt to construct mutual guidance between vision and language, which treat vision and language equally thus yield compact information matching. Specifically, for each module of subject, location and relationship, MutAtt builds two kinds of attention-based mutual guidance strategies. One strategy is to generate vision-guided language embedding for the sake of matching relevant visual feature. The other reversely generates language-guided visual feature to match relevant language embedding. This mutual guidance strategy can effectively guarantees the vision-language consistency in three modules. Experiments on three popular REC datasets demonstrate that the proposed approach outperforms the current state-of-the-art methods.",0
"The objective of Referring Expression Comprehension (REC) is to identify a region in an image that is related to a given text by using natural language. Current methods focus on developing strong visual and language representations separately, which can separate visual and language information. However, we propose in this paper that the referring expression and target region are semantically linked, and that consistency exists between vision and language in terms of subject, location, and relationship. To achieve this, we introduce MutAtt, a novel approach that creates mutual guidance between vision and language, treating them equally and resulting in compact information matching. MutAtt generates two types of attention-based mutual guidance strategies for each module of subject, location, and relationship. One strategy produces vision-guided language embedding to match relevant visual features, while the other generates language-guided visual features to match relevant language embedding. This mutual guidance approach ensures consistency between vision and language in all three modules. Our experiments on three popular REC datasets demonstrate that our proposed approach outperforms current state-of-the-art methods.",1
"Referring expression comprehension (REC) and segmentation (RES) are two highly-related tasks, which both aim at identifying the referent according to a natural language expression. In this paper, we propose a novel Multi-task Collaborative Network (MCN) to achieve a joint learning of REC and RES for the first time. In MCN, RES can help REC to achieve better language-vision alignment, while REC can help RES to better locate the referent. In addition, we address a key challenge in this multi-task setup, i.e., the prediction conflict, with two innovative designs namely, Consistency Energy Maximization (CEM) and Adaptive Soft Non-Located Suppression (ASNLS). Specifically, CEM enables REC and RES to focus on similar visual regions by maximizing the consistency energy between two tasks. ASNLS supresses the response of unrelated regions in RES based on the prediction of REC. To validate our model, we conduct extensive experiments on three benchmark datasets of REC and RES, i.e., RefCOCO, RefCOCO+ and RefCOCOg. The experimental results report the significant performance gains of MCN over all existing methods, i.e., up to +7.13% for REC and +11.50% for RES over SOTA, which well confirm the validity of our model for joint REC and RES learning.",0
"The tasks of Referring Expression Comprehension (REC) and Segmentation (RES) are closely linked as they both seek to identify a referent using natural language. Our paper proposes a new approach, the Multi-task Collaborative Network (MCN), which enables joint learning of REC and RES for the first time. REC and RES are mutually beneficial in MCN as RES helps REC to align language and vision, while REC assists RES in locating the referent. We also tackle the challenge of prediction conflict in this multi-task setting by introducing two innovative designs: Consistency Energy Maximization (CEM) and Adaptive Soft Non-Located Suppression (ASNLS). CEM maximizes the consistency energy between tasks to enable REC and RES to focus on similar visual regions, while ASNLS suppresses the response of irrelevant RES regions based on REC predictions. To evaluate MCN, we conduct extensive experiments on three benchmark datasets of REC and RES: RefCOCO, RefCOCO+, and RefCOCOg. Our results show significant performance improvements of MCN over existing methods, with up to +7.13% for REC and +11.50% for RES over SOTA, demonstrating the effectiveness of our model for joint REC and RES learning.",1
"Vision-dialog navigation posed as a new holy-grail task in vision-language disciplinary targets at learning an agent endowed with the capability of constant conversation for help with natural language and navigating according to human responses. Besides the common challenges faced in visual language navigation, vision-dialog navigation also requires to handle well with the language intentions of a series of questions about the temporal context from dialogue history and co-reasoning both dialogs and visual scenes. In this paper, we propose the Cross-modal Memory Network (CMN) for remembering and understanding the rich information relevant to historical navigation actions. Our CMN consists of two memory modules, the language memory module (L-mem) and the visual memory module (V-mem). Specifically, L-mem learns latent relationships between the current language interaction and a dialog history by employing a multi-head attention mechanism. V-mem learns to associate the current visual views and the cross-modal memory about the previous navigation actions. The cross-modal memory is generated via a vision-to-language attention and a language-to-vision attention. Benefiting from the collaborative learning of the L-mem and the V-mem, our CMN is able to explore the memory about the decision making of historical navigation actions which is for the current step. Experiments on the CVDN dataset show that our CMN outperforms the previous state-of-the-art model by a significant margin on both seen and unseen environments.",0
"The objective of vision-dialog navigation in the field of vision-language study is to develop an agent that can communicate naturally with humans and navigate accordingly. This task presents challenges in understanding language intentions and co-reasoning both dialogs and visual scenes. To address this, we propose the Cross-modal Memory Network (CMN), which utilizes two memory modules: the language memory module (L-mem) and the visual memory module (V-mem). The L-mem employs a multi-head attention mechanism to learn latent relationships between the current language interaction and a dialog history, while the V-mem associates the current visual views with cross-modal memory generated via a vision-to-language attention and a language-to-vision attention. The collaborative learning of the L-mem and V-mem allows our CMN to explore the memory of historical navigation actions for the current step. Our experiments on the CVDN dataset demonstrate that our CMN outperforms the previous state-of-the-art model on both seen and unseen environments.",1
"Deep learning has been shown to be successful in a number of domains, ranging from acoustics, images, to natural language processing. However, applying deep learning to the ubiquitous graph data is non-trivial because of the unique characteristics of graphs. Recently, substantial research efforts have been devoted to applying deep learning methods to graphs, resulting in beneficial advances in graph analysis techniques. In this survey, we comprehensively review the different types of deep learning methods on graphs. We divide the existing methods into five categories based on their model architectures and training strategies: graph recurrent neural networks, graph convolutional networks, graph autoencoders, graph reinforcement learning, and graph adversarial methods. We then provide a comprehensive overview of these methods in a systematic manner mainly by following their development history. We also analyze the differences and compositions of different methods. Finally, we briefly outline the applications in which they have been used and discuss potential future research directions.",0
"Numerous fields, including acoustics, images, and natural language processing, have demonstrated the success of deep learning. However, applying this technique to graph data can be challenging due to the distinct characteristics of graphs. Recently, significant research efforts have been focused on applying deep learning to graphs, leading to valuable advancements in graph analysis techniques. This review comprehensively examines the various deep learning methods used on graphs, categorized into five groups based on their model architectures and training strategies: graph recurrent neural networks, graph convolutional networks, graph autoencoders, graph reinforcement learning, and graph adversarial methods. We provide a systematic overview of these methods, including their development history, differences, and compositions. Additionally, we discuss their applications and potential future research directions.",1
"This paper studies the problem of temporal moment localization in a long untrimmed video using natural language as the query. Given an untrimmed video and a sentence as the query, the goal is to determine the starting, and the ending, of the relevant visual moment in the video, that corresponds to the query sentence. While previous works have tackled this task by a propose-and-rank approach, we introduce a more efficient, end-to-end trainable, and {\em proposal-free approach} that relies on three key components: a dynamic filter to transfer language information to the visual domain, a new loss function to guide our model to attend the most relevant parts of the video, and soft labels to model annotation uncertainty. We evaluate our method on two benchmark datasets, Charades-STA and ActivityNet-Captions. Experimental results show that our approach outperforms state-of-the-art methods on both datasets.",0
"The aim of this study is to address the issue of temporal moment identification in a lengthy unedited video using natural language as a search query. The objective is to identify the precise beginning and end of the pertinent visual moment in the video that corresponds to the sentence query. Prior research has employed a propose-and-rank technique to tackle this task. However, we propose a more effective and proposal-free approach that is end-to-end trainable and relies on three crucial components: a dynamic filter to transfer linguistic information to the visual domain, a novel loss function to direct our model to focus on the most relevant parts of the video, and soft labels to account for annotation uncertainty. Our approach is assessed on two widely used datasets, namely Charades-STA and ActivityNet-Captions, and the results indicate that our method outperforms state-of-the-art approaches on both datasets.",1
"Visual question answering by using information from multiple modalities has attracted more and more attention in recent years. However, it is a very challenging task, as the visual content and natural language have quite different statistical properties. In this work, we present a method called Adversarial Multimodal Network (AMN) to better understand video stories for question answering. In AMN, as inspired by generative adversarial networks, we propose to learn multimodal feature representations by finding a more coherent subspace for video clips and the corresponding texts (e.g., subtitles and questions). Moreover, we introduce a self-attention mechanism to enforce the so-called consistency constraints in order to preserve the self-correlation of visual cues of the original video clips in the learned multimodal representations. Extensive experiments on the MovieQA dataset show the effectiveness of our proposed AMN over other published state-of-the-art methods.",0
"The integration of multiple modalities for visual question answering has become increasingly popular in recent years. However, it is a complex task due to the distinct statistical properties of visual content and natural language. This study introduces the Adversarial Multimodal Network (AMN) as a method to enhance comprehension of video stories for question answering. The AMN utilizes generative adversarial networks to learn multimodal feature representations. By identifying a more cohesive subspace for video clips and corresponding texts, such as subtitles and questions, the AMN improves performance. Additionally, a self-attention mechanism enforces consistency constraints to preserve the self-correlation of visual cues in the learned multimodal representations. Results of experiments on the MovieQA dataset demonstrate the superiority of the proposed AMN compared to other state-of-the-art methods.",1
"The ability to generate natural language explanations conditioned on the visual perception is a crucial step towards autonomous agents which can explain themselves and communicate with humans. While the research efforts in image and video captioning are giving promising results, this is often done at the expense of the computational requirements of the approaches, limiting their applicability to real contexts. In this paper, we propose a fully-attentive captioning algorithm which can provide state-of-the-art performances on language generation while restricting its computational demands. Our model is inspired by the Transformer model and employs only two Transformer layers in the encoding and decoding stages. Further, it incorporates a novel memory-aware encoding of image regions. Experiments demonstrate that our approach achieves competitive results in terms of caption quality while featuring reduced computational demands. Further, to evaluate its applicability on autonomous agents, we conduct experiments on simulated scenes taken from the perspective of domestic robots.",0
"Generating natural language explanations based on visual perception is a critical step towards the development of autonomous agents capable of communicating with humans and explaining themselves. Although current research in image and video captioning has shown promising results, the computational requirements of these approaches often limit their real-world applicability. Our approach proposes a fully-attentive captioning algorithm that achieves state-of-the-art language generation while maintaining reduced computational demands. Inspired by the Transformer model, our algorithm utilizes just two Transformer layers in the encoding and decoding stages and incorporates a novel memory-aware encoding of image regions. Our experiments demonstrate that our approach produces competitive results in caption quality while reducing computational demands. We also evaluate the applicability of our method on autonomous agents by conducting experiments on simulated scenes from the perspective of domestic robots.",1
"Most image captioning frameworks generate captions directly from images, learning a mapping from visual features to natural language. However, editing existing captions can be easier than generating new ones from scratch. Intuitively, when editing captions, a model is not required to learn information that is already present in the caption (i.e. sentence structure), enabling it to focus on fixing details (e.g. replacing repetitive words). This paper proposes a novel approach to image captioning based on iterative adaptive refinement of an existing caption. Specifically, our caption-editing model consisting of two sub-modules: (1) EditNet, a language module with an adaptive copy mechanism (Copy-LSTM) and a Selective Copy Memory Attention mechanism (SCMA), and (2) DCNet, an LSTM-based denoising auto-encoder. These components enable our model to directly copy from and modify existing captions. Experiments demonstrate that our new approach achieves state-of-art performance on the MS COCO dataset both with and without sequence-level training.",0
"Generating captions from images is the main approach used by most image captioning frameworks. However, modifying existing captions seems to be an easier task than creating new ones from scratch. This is because when editing captions, models do not need to learn information that is already present in the caption (such as sentence structure), meaning that they can focus on making specific improvements (like replacing repetitive words). In this paper, we present a novel image captioning approach that involves iteratively refining existing captions. Our caption-editing model has two sub-modules: EditNet, a language module with Copy-LSTM and SCMA, and DCNet, an LSTM-based denoising auto-encoder. These components allow our model to copy from and modify existing captions directly. Our experiments show that our approach achieves state-of-the-art performance on the MS COCO dataset, with or without sequence-level training.",1
"Important information that relates to a specific topic in a document is often organized in tabular format to assist readers with information retrieval and comparison, which may be difficult to provide in natural language. However, tabular data in unstructured digital documents, e.g., Portable Document Format (PDF) and images, are difficult to parse into structured machine-readable format, due to complexity and diversity in their structure and style. To facilitate image-based table recognition with deep learning, we develop the largest publicly available table recognition dataset PubTabNet (https://github.com/ibm-aur-nlp/PubTabNet), containing 568k table images with corresponding structured HTML representation. PubTabNet is automatically generated by matching the XML and PDF representations of the scientific articles in PubMed Central Open Access Subset (PMCOA). We also propose a novel attention-based encoder-dual-decoder (EDD) architecture that converts images of tables into HTML code. The model has a structure decoder which reconstructs the table structure and helps the cell decoder to recognize cell content. In addition, we propose a new Tree-Edit-Distance-based Similarity (TEDS) metric for table recognition, which more appropriately captures multi-hop cell misalignment and OCR errors than the pre-established metric. The experiments demonstrate that the EDD model can accurately recognize complex tables solely relying on the image representation, outperforming the state-of-the-art by 9.7% absolute TEDS score.",0
"Tabular information in documents is often presented in a structured format to aid in information retrieval and comparison. However, when dealing with unstructured digital documents such as PDFs and images, parsing tabular data into a structured machine-readable format can be challenging due to the complex and diverse structure and style of these types of documents. To address this issue, we have developed PubTabNet, the largest table recognition dataset publicly available, containing 568k table images with corresponding structured HTML representation. This dataset was generated by matching the XML and PDF representations of scientific articles in the PubMed Central Open Access Subset. We also propose a novel attention-based encoder-dual-decoder architecture that can accurately recognize complex tables solely relying on image representation, outperforming the state-of-the-art by 9.7% absolute TEDS score. Additionally, we introduce a new Tree-Edit-Distance-based Similarity metric for table recognition that more appropriately captures multi-hop cell misalignment and OCR errors.",1
"Deep learning natural language processing models often use vector word embeddings, such as word2vec or GloVe, to represent words. A discrete sequence of words can be much more easily integrated with downstream neural layers if it is represented as a sequence of continuous vectors. Also, semantic relationships between words, learned from a text corpus, can be encoded in the relative configurations of the embedding vectors. However, storing and accessing embedding vectors for all words in a dictionary requires large amount of space, and may stain systems with limited GPU memory. Here, we used approaches inspired by quantum computing to propose two related methods, {\em word2ket} and {\em word2ketXS}, for storing word embedding matrix during training and inference in a highly efficient way. Our approach achieves a hundred-fold or more reduction in the space required to store the embeddings with almost no relative drop in accuracy in practical natural language processing tasks.",0
"To represent words in deep learning natural language processing models, vector word embeddings such as word2vec or GloVe are often utilized. Representing a sequence of discrete words as a sequence of continuous vectors makes integration with downstream neural layers much simpler. Additionally, embedding vectors can encode semantic relationships between words learned from a text corpus. However, storing and accessing embedding vectors for all words in a dictionary can be challenging for systems with limited GPU memory due to the large amount of space required. To address this, we have proposed two related methods, word2ket and word2ketXS, which use quantum computing-inspired approaches to efficiently store word embedding matrices during training and inference. Our approach achieves a significant reduction in storage space while maintaining high accuracy in natural language processing tasks.",1
"Video description is the automatic generation of natural language sentences that describe the contents of a given video. It has applications in human-robot interaction, helping the visually impaired and video subtitling. The past few years have seen a surge of research in this area due to the unprecedented success of deep learning in computer vision and natural language processing. Numerous methods, datasets and evaluation metrics have been proposed in the literature, calling the need for a comprehensive survey to focus research efforts in this flourishing new direction. This paper fills the gap by surveying the state of the art approaches with a focus on deep learning models; comparing benchmark datasets in terms of their domains, number of classes, and repository size; and identifying the pros and cons of various evaluation metrics like SPICE, CIDEr, ROUGE, BLEU, METEOR, and WMD. Classical video description approaches combined subject, object and verb detection with template based language models to generate sentences. However, the release of large datasets revealed that these methods can not cope with the diversity in unconstrained open domain videos. Classical approaches were followed by a very short era of statistical methods which were soon replaced with deep learning, the current state of the art in video description. Our survey shows that despite the fast-paced developments, video description research is still in its infancy due to the following reasons. Analysis of video description models is challenging because it is difficult to ascertain the contributions, towards accuracy or errors, of the visual features and the adopted language model in the final description. Existing datasets neither contain adequate visual diversity nor complexity of linguistic structures. Finally, current evaluation metrics ...",0
"Video description refers to the generation of natural language sentences that describe the contents of a video, with applications in fields such as human-robot interaction, aiding visually impaired individuals, and video subtitling. Recent years have seen a surge of research in this area, largely due to the success of deep learning in computer vision and natural language processing. However, despite the development of various methods, datasets, and evaluation metrics, there remains a need for a comprehensive survey to focus research efforts. This paper aims to fill that gap by reviewing state-of-the-art approaches with a focus on deep learning models, comparing benchmark datasets in terms of their domains, class numbers, and repository sizes, and analyzing the pros and cons of various evaluation metrics such as SPICE, CIDEr, ROUGE, BLEU, METEOR, and WMD. Classical video description methods involved subject, object, and verb detection combined with template-based language models to generate sentences. However, these methods could not handle the diversity in unconstrained open domain videos, leading to statistical methods and later to deep learning as the current state of the art. Despite the rapid advancements, video description research is still in its infancy due to several challenges, including difficulty in analyzing the contributions of visual features and language models, inadequate visual diversity in existing datasets, and shortcomings in current evaluation metrics.",1
"Referring expression comprehension (REF) aims at identifying a particular object in a scene by a natural language expression. It requires joint reasoning over the textual and visual domains to solve the problem. Some popular referring expression datasets, however, fail to provide an ideal test bed for evaluating the reasoning ability of the models, mainly because 1) their expressions typically describe only some simple distinctive properties of the object and 2) their images contain limited distracting information. To bridge the gap, we propose a new dataset for visual reasoning in context of referring expression comprehension with two main features. First, we design a novel expression engine rendering various reasoning logics that can be flexibly combined with rich visual properties to generate expressions with varying compositionality. Second, to better exploit the full reasoning chain embodied in an expression, we propose a new test setting by adding additional distracting images containing objects sharing similar properties with the referent, thus minimising the success rate of reasoning-free cross-domain alignment. We evaluate several state-of-the-art REF models, but find none of them can achieve promising performance. A proposed modular hard mining strategy performs the best but still leaves substantial room for improvement. We hope this new dataset and task can serve as a benchmark for deeper visual reasoning analysis and foster the research on referring expression comprehension.",0
"Referring expression comprehension (REF) involves identifying an object in a scene using natural language. It requires reasoning across both text and visual domains. However, some popular REF datasets do not provide an adequate testing ground for evaluating model reasoning ability, due to simple expressions and limited distracting information. To address this gap, we propose a new dataset with two key features. Firstly, a novel expression engine generates expressions with varying compositionality using rich visual properties and flexible reasoning logics. Secondly, a new test setting includes additional distracting images with objects sharing similar properties, minimizing reasoning-free cross-domain alignment. We evaluate several state-of-the-art REF models, but none achieve promising performance. A proposed modular hard mining strategy performs the best, but there is still substantial room for improvement. Our hope is that this new dataset and task can serve as a benchmark for deeper visual reasoning analysis and foster research on referring expression comprehension.",1
"Home design is a complex task that normally requires architects to finish with their professional skills and tools. It will be fascinating that if one can produce a house plan intuitively without knowing much knowledge about home design and experience of using complex designing tools, for example, via natural language. In this paper, we formulate it as a language conditioned visual content generation problem that is further divided into a floor plan generation and an interior texture (such as floor and wall) synthesis task. The only control signal of the generation process is the linguistic expression given by users that describe the house details. To this end, we propose a House Plan Generative Model (HPGM) that first translates the language input to a structural graph representation and then predicts the layout of rooms with a Graph Conditioned Layout Prediction Network (GC LPN) and generates the interior texture with a Language Conditioned Texture GAN (LCT-GAN). With some post-processing, the final product of this task is a 3D house model. To train and evaluate our model, we build the first Text-to-3D House Model dataset.",0
"Architects typically rely on their professional skills and tools to successfully complete the complex task of home design. However, it would be intriguing if individuals could intuitively produce a house plan without extensive knowledge or experience using complex design tools, such as through natural language. This study presents a language-conditioned visual content generation problem, consisting of floor plan generation and interior texture synthesis tasks. The sole control signal for this generation process is the linguistic expression provided by users to describe the house details. To address this challenge, the House Plan Generative Model (HPGM) is proposed. HPGM first translates the language input to a structural graph representation and then predicts the room layout using a Graph Conditioned Layout Prediction Network (GC LPN) and generates interior texture with a Language Conditioned Texture GAN (LCT-GAN). The final output of this process is a 3D house model, which is achieved through post-processing. To train and evaluate the model, the first Text-to-3D House Model dataset is developed.",1
"In this dissertation, we cover some recent advances in collaborative filtering and ranking. In chapter 1, we give a brief introduction of the history and the current landscape of collaborative filtering and ranking; chapter 2 we first talk about pointwise collaborative filtering problem with graph information, and how our proposed new method can encode very deep graph information which helps four existing graph collaborative filtering algorithms; chapter 3 is on the pairwise approach for collaborative ranking and how we speed up the algorithm to near-linear time complexity; chapter 4 is on the new listwise approach for collaborative ranking and how the listwise approach is a better choice of loss for both explicit and implicit feedback over pointwise and pairwise loss; chapter 5 is about the new regularization technique Stochastic Shared Embeddings (SSE) we proposed for embedding layers and how it is both theoretically sound and empirically effectively for 6 different tasks across recommendation and natural language processing; chapter 6 is how we introduce personalization for the state-of-the-art sequential recommendation model with the help of SSE, which plays an important role in preventing our personalized model from overfitting to the training data; chapter 7, we summarize what we have achieved so far and predict what the future directions can be; chapter 8 is the appendix to all the chapters.",0
"This dissertation covers recent advancements in collaborative filtering and ranking. Chapter 1 provides a brief overview of the history and current state of collaborative filtering and ranking. In Chapter 2, we discuss the pointwise collaborative filtering problem with graph information and introduce our proposed method that encodes deep graph information to enhance four existing graph collaborative filtering algorithms. Chapter 3 focuses on the pairwise approach for collaborative ranking, and we demonstrate how we improved the algorithm's speed to near-linear time complexity. Chapter 4 explores the new listwise approach for collaborative ranking and how it outperforms both pointwise and pairwise loss for both explicit and implicit feedback. In Chapter 5, we propose a new regularization technique called Stochastic Shared Embeddings (SSE) for embedding layers. SSE is both theoretically sound and empirically effective for six different tasks across recommendation and natural language processing. Chapter 6 details how we introduce personalization for the state-of-the-art sequential recommendation model with SSE, which prevents overfitting to training data. Chapter 7 summarizes our achievements and predicts future directions. Finally, Chapter 8 is the appendix to all chapters.",1
"We present PrecisionBatching, a quantized inference algorithm for speeding up neural network execution on traditional hardware platforms at low bitwidths without the need for retraining or recalibration. PrecisionBatching decomposes a neural network into individual bitlayers and accumulates them using fast 1-bit operations while maintaining activations in full precision. PrecisionBatching not only facilitates quantized inference at low bitwidths (< 8 bits) without the need for retraining/recalibration, but also 1) enables traditional hardware platforms the ability to realize inference speedups at a finer granularity of quantization (e.g: 1-16 bit execution) and 2) allows accuracy and speedup tradeoffs at runtime by exposing the number of bitlayers to accumulate as a tunable parameter. Across a variety of applications (MNIST, language modeling, natural language inference) and neural network architectures (fully connected, RNN, LSTM), PrecisionBatching yields end-to-end speedups of over 8x on a GPU within a < 1% error margin of the full precision baseline, outperforming traditional 8-bit quantized inference by over 1.5x-2x at the same error tolerance.",0
"The article introduces PrecisionBatching, an algorithm that speeds up neural network execution on regular hardware platforms with low bitwidths without requiring retraining or recalibration. PrecisionBatching breaks down a neural network into individual bitlayers and combines them using fast 1-bit operations while maintaining full precision activations. The algorithm allows for quantized inference at low bitwidths, ranging from 1-16 bit execution, and enables accuracy and speedup tradeoffs at runtime by exposing the number of bitlayers as a tunable parameter. Across various applications and neural network architectures, PrecisionBatching achieves end-to-end speedups of over 8x on a GPU while maintaining an error margin of less than 1% compared to full precision. Furthermore, it outperforms traditional 8-bit quantized inference by 1.5x-2x at the same error tolerance.",1
"We propose Sparse Sinkhorn Attention, a new efficient and sparse method for learning to attend. Our method is based on differentiable sorting of internal representations. Concretely, we introduce a meta sorting network that learns to generate latent permutations over sequences. Given sorted sequences, we are then able to compute quasi-global attention with only local windows, improving the memory efficiency of the attention module. To this end, we propose new algorithmic innovations such as Causal Sinkhorn Balancing and SortCut, a dynamic sequence truncation method for tailoring Sinkhorn Attention for encoding and/or decoding purposes. Via extensive experiments on algorithmic seq2seq sorting, language modeling, pixel-wise image generation, document classification and natural language inference, we demonstrate that our memory efficient Sinkhorn Attention method is competitive with vanilla attention and consistently outperforms recently proposed efficient Transformer models such as Sparse Transformers.",0
"Our novel approach to learning to attend is called Sparse Sinkhorn Attention, which is both efficient and sparse. By utilizing a differentiable sorting mechanism for internal representations, we introduce a meta sorting network. This network is capable of generating latent permutations over sequences, which allows us to compute quasi-global attention with only local windows. This results in a significant improvement in memory efficiency for the attention module. To further enhance our method, we introduce new algorithmic techniques such as Causal Sinkhorn Balancing and SortCut, which are dynamic sequence truncation methods tailored for encoding and/or decoding purposes. Through extensive experiments on various tasks such as algorithmic seq2seq sorting, language modeling, pixel-wise image generation, document classification, and natural language inference, we demonstrate that our memory efficient Sinkhorn Attention method is highly competitive with vanilla attention and consistently outperforms other efficient Transformer models, such as Sparse Transformers.",1
"We propose a federated learning framework to handle heterogeneous client devices which do not conform to the population data distribution. The approach hinges upon a parameterized superquantile-based objective, where the parameter ranges over levels of conformity. We present an optimization algorithm and establish its convergence to a stationary point. We show how to practically implement it using secure aggregation by interleaving iterations of the usual federated averaging method with device filtering. We conclude with numerical experiments on neural networks as well as linear models on tasks from computer vision and natural language processing.",0
"Our proposal is a federated learning structure that can manage varied client devices that deviate from the population data distribution. The framework relies on a parameterized objective based on superquantiles, with the parameter covering different conformity levels. We introduce an optimization algorithm and confirm its ability to converge to a stationary point. To put it into practice, we utilize secure aggregation by intermingling iterations of the standard federated averaging method with device filtering. We conclude by presenting numerical experiments on neural networks and linear models and their performance on tasks related to computer vision and natural language processing.",1
"The lottery ticket hypothesis proposes that over-parameterization of deep neural networks (DNNs) aids training by increasing the probability of a ""lucky"" sub-network initialization being present rather than by helping the optimization process (Frankle & Carbin, 2019). Intriguingly, this phenomenon suggests that initialization strategies for DNNs can be improved substantially, but the lottery ticket hypothesis has only previously been tested in the context of supervised learning for natural image tasks. Here, we evaluate whether ""winning ticket"" initializations exist in two different domains: natural language processing (NLP) and reinforcement learning (RL).For NLP, we examined both recurrent LSTM models and large-scale Transformer models (Vaswani et al., 2017). For RL, we analyzed a number of discrete-action space tasks, including both classic control and pixel control. Consistent with workin supervised image classification, we confirm that winning ticket initializations generally outperform parameter-matched random initializations, even at extreme pruning rates for both NLP and RL. Notably, we are able to find winning ticket initializations for Transformers which enable models one-third the size to achieve nearly equivalent performance. Together, these results suggest that the lottery ticket hypothesis is not restricted to supervised learning of natural images, but rather represents a broader phenomenon in DNNs.",0
"The lottery ticket hypothesis suggests that deep neural networks (DNNs) are aided in training through over-parameterization, which increases the likelihood of a fortuitous sub-network initialization rather than optimizing the process (Frankle & Carbin, 2019). This intriguing idea implies that initialization strategies for DNNs could be significantly enhanced, but the hypothesis has only been tested in supervised image classification. In this study, we assess whether ""winning ticket"" initializations are present in natural language processing (NLP) and reinforcement learning (RL). We analyzed recurrent LSTM models and large-scale Transformer models (Vaswani et al., 2017) for NLP and various discrete-action space tasks, including classic control and pixel control, for RL. Our results confirm that winning ticket initializations consistently outperform parameter-matched random initializations, even at extreme pruning rates, for both NLP and RL, similar to supervised image classification. Notably, we discovered winning ticket initializations for Transformers that allow models one-third the size to achieve nearly equivalent performance. These findings suggest that the lottery ticket hypothesis applies to a broad range of DNNs beyond supervised image classification.",1
"This paper significantly improves on, and finishes to validate, an approach proposed in previous research in which safety outcomes were predicted from attributes with machine learning. Like in the original study, we use Natural Language Processing (NLP) to extract fundamental attributes from raw incident reports and machine learning models are trained to predict safety outcomes. The outcomes predicted here are injury severity, injury type, body part impacted, and incident type. However, unlike in the original study, safety outcomes were not extracted via NLP but were provided by independent human annotations, eliminating any potential source of artificial correlation between predictors and predictands. Results show that attributes are still highly predictive, confirming the validity of the original approach. Other improvements brought by the current study include the use of (1) a much larger dataset featuring more than 90,000 reports, (2) two new models, XGBoost and linear SVM (Support Vector Machines), (3) model stacking, (4) a more straightforward experimental setup with more appropriate performance metrics, and (5) an analysis of per-category attribute importance scores. Finally, the injury severity outcome is well predicted, which was not the case in the original study. This is a significant advancement.",0
"This paper enhances and validates an approach proposed in previous research, which involved predicting safety outcomes using machine learning based on extracted attributes via Natural Language Processing (NLP) from raw incident reports. However, unlike the original study, independent human annotations were used to provide safety outcomes, eliminating any potential artificial correlation between predictors and predictands. The study confirms the effectiveness of the original approach as attributes remain highly predictive. Additionally, the current study features a larger dataset of over 90,000 reports, two new models (XGBoost and linear SVM), model stacking, a more straightforward experimental setup with appropriate performance metrics, and an analysis of per-category attribute importance scores. Furthermore, the paper demonstrates significant progress as the injury severity outcome is now accurately predicted.",1
"Many applications, including natural language processing, sensor networks, collaborative filtering, and federated learning, call for estimating discrete distributions from data collected in batches, some of which may be untrustworthy, erroneous, faulty, or even adversarial.   Previous estimators for this setting ran in exponential time, and for some regimes required a suboptimal number of batches. We provide the first polynomial-time estimator that is optimal in the number of batches and achieves essentially the best possible estimation accuracy.",0
"A variety of applications, such as natural language processing, sensor networks, collaborative filtering, and federated learning, require the estimation of discrete distributions from batch-collected data. However, some batches may be unreliable, inaccurate, defective, or even hostile. Previous estimators for this scenario were time-consuming and, in certain situations, necessitated an inadequate number of batches. We introduce the initial estimator that operates in polynomial time, performs optimally in terms of the number of batches, and attains near-optimal estimation precision.",1
"Extracting context from visual representations is of utmost importance in the advancement of Computer Science. Representation of such a format in Natural Language has a huge variety of applications such as helping the visually impaired etc. Such an approach is a combination of Computer Vision and Natural Language techniques which is a hard problem to solve. This project aims to compare different approaches for solving the image captioning problem. In specific, the focus was on comparing two different types of models: Encoder-Decoder approach and a Multi-model approach. In the encoder-decoder approach, inject and merge architectures were compared against a multi-modal image captioning approach based primarily on object detection. These approaches have been compared on the basis on state of the art sentence comparison metrics such as BLEU, GLEU, Meteor, and Rouge on a subset of the Google Conceptual captions dataset which contains 100k images. On the basis of this comparison, we observed that the best model was the Inception injected encoder model. This best approach has been deployed as a web-based system. On uploading an image, such a system will output the best caption associated with the image.",0
"The extraction of context from visual representations is crucial for the progress of Computer Science, with Natural Language representation having numerous applications, including assisting the visually impaired. Combining Computer Vision and Natural Language techniques presents a challenging problem. This project seeks to compare two approaches to solve the image captioning problem: the Encoder-Decoder approach and the Multi-model approach. The study compared inject and merge architectures against a multi-modal image captioning approach based on object detection. The comparison was based on established sentence comparison metrics, such as BLEU, GLEU, Meteor, and Rouge, on a subset of the Google Conceptual captions dataset, which includes 100k images. The results revealed that the best model was the Inception injected encoder model, which was then deployed as a web-based system. This system can provide the best caption associated with an uploaded image.",1
"Classical person re-identification approaches assume that a person of interest has appeared across different cameras and can be queried by one of the existing images. However, in real-world surveillance scenarios, frequently no visual information will be available about the queried person. In such scenarios, a natural language description of the person by a witness will provide the only source of information for retrieval. In this work, person re-identification using both vision and language information is addressed under all possible gallery and query scenarios. A two stream deep convolutional neural network framework supervised by cross entropy loss is presented. The weights connecting the second last layer to the last layer with class probabilities, i.e., logits of softmax layer are shared in both networks. Canonical Correlation Analysis is performed to enhance the correlation between the two modalities in a joint latent embedding space. To investigate the benefits of the proposed approach, a new testing protocol under a multi modal ReID setting is proposed for the test split of the CUHK-PEDES and CUHK-SYSU benchmarks. The experimental results verify the merits of the proposed system. The learnt visual representations are more robust and perform 22\% better during retrieval as compared to a single modality system. The retrieval with a multi modal query greatly enhances the re-identification capability of the system quantitatively as well as qualitatively.",0
"Classical methods for person re-identification assume that a person can be identified through existing images from different cameras. However, in real-life surveillance situations, visual information may not be available, and a witness's description may be the only source of information for retrieval. This study presents a framework for person re-identification that uses both vision and language information, addressing all possible gallery and query scenarios. A two-stream deep convolutional neural network is used with shared weights to enhance correlation between the two modalities. Canonical Correlation Analysis is performed to create a joint latent embedding space. A new testing protocol is proposed for the test split of the CUHK-PEDES and CUHK-SYSU benchmarks to investigate the benefits of the proposed approach. The experimental results show that the proposed system has merits, with more robust visual representations that perform 22% better during retrieval compared to a single modality system. The system's re-identification capability is greatly enhanced with a multi-modal query, both quantitatively and qualitatively.",1
"Text-based games -- in which an agent interacts with the world through textual natural language -- present us with the problem of combinatorially-sized action-spaces. Most current reinforcement learning algorithms are not capable of effectively handling such a large number of possible actions per turn. Poor sample efficiency, consequently, results in agents that are unable to pass bottleneck states, where they are unable to proceed because they do not see the right action sequence to pass the bottleneck enough times to be sufficiently reinforced. Building on prior work using knowledge graphs in reinforcement learning, we introduce two new game state exploration strategies. We compare our exploration strategies against strong baselines on the classic text-adventure game, Zork1, where prior agent have been unable to get past a bottleneck where the agent is eaten by a Grue.",0
"The issue with text-based games is the vast number of possible actions an agent can take, which presents a challenge for reinforcement learning algorithms. The existing algorithms struggle to handle the large action-space and are inefficient in generating accurate results. This inefficiency leads to agents getting stuck in bottleneck states, where they cannot proceed as they do not have the correct action sequence to pass through the bottleneck. To address this problem, we build on prior research that utilizes knowledge graphs in reinforcement learning and propose two new exploration strategies. We evaluate the effectiveness of our strategies against strong baselines in the popular text-adventure game, Zork1, where previous agents have failed to progress past a bottleneck where the agent is killed by a Grue.",1
"Incorporating hierarchical structures like constituency trees has been shown to be effective for various natural language processing (NLP) tasks. However, it is evident that state-of-the-art (SOTA) sequence-based models like the Transformer struggle to encode such structures inherently. On the other hand, dedicated models like the Tree-LSTM, while explicitly modeling hierarchical structures, do not perform as efficiently as the Transformer. In this paper, we attempt to bridge this gap with ""Hierarchical Accumulation"" to encode parse tree structures into self-attention at constant time complexity. Our approach outperforms SOTA methods in four IWSLT translation tasks and the WMT'14 English-German translation task. It also yields improvements over Transformer and Tree-LSTM on three text classification tasks. We further demonstrate that using hierarchical priors can compensate for data shortage, and that our model prefers phrase-level attentions over token-level attentions.",0
"The effectiveness of incorporating hierarchical structures like constituency trees for various natural language processing tasks has been demonstrated. However, state-of-the-art sequence-based models such as the Transformer struggle to encode such structures inherently, while dedicated models like the Tree-LSTM, which explicitly model hierarchical structures, are not as efficient as the Transformer. This paper proposes a solution to bridge this gap by introducing ""Hierarchical Accumulation"" to encode parse tree structures into self-attention at constant time complexity. Our approach outperforms state-of-the-art methods in four IWSLT translation tasks and the WMT'14 English-German translation task and shows improvements over Transformer and Tree-LSTM on three text classification tasks. Additionally, we demonstrate that using hierarchical priors can compensate for data shortage and that our model prefers phrase-level attentions over token-level attentions.",1
"Transformer based Very Large Language Models (VLLMs) like BERT, XLNet and RoBERTa, have recently shown tremendous performance on a large variety of Natural Language Understanding (NLU) tasks. However, due to their size, these VLLMs are extremely resource intensive and cumbersome to deploy at production time. Several recent publications have looked into various ways to distil knowledge from a transformer based VLLM (most commonly BERT-Base) into a smaller model which can run much faster at inference time. Here, we propose a novel set of techniques which together produce a task-specific hybrid convolutional and transformer model, WaLDORf, that achieves state-of-the-art inference speed while still being more accurate than previous distilled models.",0
"Recently, Transformer-based VLLMs such as BERT, XLNet, and RoBERTa have exhibited remarkable performance on diverse NLU tasks. However, their large size makes them highly resource-intensive and cumbersome to deploy in production. To address this, various studies have explored methods to distill knowledge from a BERT-Base VLLM into a smaller model that can run faster during inference. In this study, we introduce a new approach that combines convolutional and Transformer models to create a task-specific hybrid model called WaLDORf. This model achieves both state-of-the-art inference speed and higher accuracy than previous distilled models.",1
"Attention based Transformer architecture has enabled significant advances in the field of natural language processing. In addition to new pre-training techniques, recent improvements crucially rely on working with a relatively larger embedding dimension for tokens. Unfortunately, this leads to models that are prohibitively large to be employed in the downstream tasks. In this paper we identify one of the important factors contributing to the large embedding size requirement. In particular, our analysis highlights that the scaling between the number of heads and the size of each head in the current architecture gives rise to a low-rank bottleneck in attention heads, causing this limitation. We further validate this in our experiments. As a solution we propose to set the head size of an attention unit to input sequence length, and independent of the number of heads, resulting in multi-head attention layers with provably more expressive power. We empirically show that this allows us to train models with a relatively smaller embedding dimension and with better performance scaling.",0
"The Attention based Transformer architecture has significantly advanced natural language processing. Recent improvements have relied on larger token embedding dimensions, but this results in models that are too large for downstream tasks. This paper identifies the cause of this requirement as the low-rank bottleneck in attention heads due to the scaling between the number of heads and the size of each head. This is validated in experiments. To solve this issue, the paper proposes setting the head size of an attention unit to the input sequence length, resulting in multi-head attention layers with more expressive power. The paper shows that this approach enables the training of models with smaller embedding dimensions and better performance scaling.",1
"Transfer learning has become the de facto standard in computer vision and natural language processing, especially where labeled data is scarce. Accuracy can be significantly improved by using pre-trained models and subsequent fine-tuning. In visual reasoning tasks, such as image question answering, transfer learning is more complex. In addition to transferring the capability to recognize visual features, we also expect to transfer the system's ability to reason. Moreover, for video data, temporal reasoning adds another dimension. In this work, we formalize these unique aspects of transfer learning and propose a theoretical framework for visual reasoning, exemplified by the well-established CLEVR and COG datasets. Furthermore, we introduce a new, end-to-end differentiable recurrent model (SAMNet), which shows state-of-the-art accuracy and better performance in transfer learning on both datasets. The improved performance of SAMNet stems from its capability to decouple the abstract multi-step reasoning from the length of the sequence and its selective attention enabling to store only the question-relevant objects in the external memory.",0
"The standard practice in computer vision and natural language processing is to use transfer learning, particularly when labeled data is limited. Utilizing pre-trained models and fine-tuning can notably enhance accuracy. However, when it comes to visual reasoning tasks like image question answering, transfer learning becomes more intricate. Besides recognizing visual features, we also aim to transfer the system's reasoning ability, which becomes more complex when dealing with video data and temporal reasoning. This study presents a theoretical framework for transfer learning in visual reasoning, focusing on CLEVR and COG datasets. Additionally, we introduce a new, end-to-end differentiable recurrent model (SAMNet) that displays improved accuracy and better performance in transfer learning on both datasets. SAMNet's superior performance results from its capacity to separate abstract multi-step reasoning from sequence length and selective attention that enables storing only question-relevant objects in external memory.",1
"Video captioning is an advanced multi-modal task which aims to describe a video clip using a natural language sentence. The encoder-decoder framework is the most popular paradigm for this task in recent years. However, there exist some problems in the decoder of a video captioning model. We make a thorough investigation into the decoder and adopt three techniques to improve the performance of the model. First of all, a combination of variational dropout and layer normalization is embedded into a recurrent unit to alleviate the problem of overfitting. Secondly, a new online method is proposed to evaluate the performance of a model on a validation set so as to select the best checkpoint for testing. Finally, a new training strategy called professional learning is proposed which uses the strengths of a captioning model and bypasses its weaknesses. It is demonstrated in the experiments on Microsoft Research Video Description Corpus (MSVD) and MSR-Video to Text (MSR-VTT) datasets that our model has achieved the best results evaluated by BLEU, CIDEr, METEOR and ROUGE-L metrics with significant gains of up to 18% on MSVD and 3.5% on MSR-VTT compared with the previous state-of-the-art models.",0
"The task of video captioning involves describing a video clip using natural language, and the encoder-decoder framework has been the most popular method for this task in recent years. However, the decoder component of video captioning models has encountered some issues. Our study delves into these issues and proposes three techniques to enhance the model's performance. Firstly, we integrate variational dropout and layer normalization into a recurrent unit to counter overfitting. Secondly, we introduce a novel online method to assess the model's performance on a validation set for optimal checkpoint selection during testing. Lastly, we propose a new training strategy called professional learning that leverages the strengths of the captioning model while addressing its weaknesses. Our experiments on the MSVD and MSR-VTT datasets demonstrate that our model achieves the best results in terms of BLEU, CIDEr, METEOR, and ROUGE-L metrics, with significant improvements of up to 18% on MSVD and 3.5% on MSR-VTT compared to existing state-of-the-art models.",1
"The rapid growth of video on the internet has made searching for video content using natural language queries a significant challenge. Human-generated queries for video datasets `in the wild' vary a lot in terms of degree of specificity, with some queries describing specific details such as the names of famous identities, content from speech, or text available on the screen. Our goal is to condense the multi-modal, extremely high dimensional information from videos into a single, compact video representation for the task of video retrieval using free-form text queries, where the degree of specificity is open-ended.   For this we exploit existing knowledge in the form of pre-trained semantic embeddings which include 'general' features such as motion, appearance, and scene features from visual content. We also explore the use of more 'specific' cues from ASR and OCR which are intermittently available for videos and find that these signals remain challenging to use effectively for retrieval. We propose a collaborative experts model to aggregate information from these different pre-trained experts and assess our approach empirically on five retrieval benchmarks: MSR-VTT, LSMDC, MSVD, DiDeMo, and ActivityNet. Code and data can be found at www.robots.ox.ac.uk/~vgg/research/collaborative-experts/. This paper contains a correction to results reported in the previous version.",0
"The exponential increase of online video has posed a significant challenge in searching for video content using natural language queries. Queries generated by humans for video datasets found ""in the wild"" vary greatly in terms of specificity, with some queries providing detailed information such as the names of famous individuals, speech content, or on-screen text. Our objective is to streamline the multi-modal, highly-dimensional information from videos into a single, concise video representation that can be used for video retrieval through open-ended text queries. To achieve this, we utilize pre-existing knowledge in the form of semantic embeddings that contain ""general"" motion, appearance, and scene features from visual content. Additionally, we explore using more specific cues from Automatic Speech Recognition (ASR) and Optical Character Recognition (OCR) but have found these signals to be challenging to use effectively for retrieval. We propose a collaborative experts model that aggregates information from these pre-trained experts and evaluate our approach on five retrieval benchmarks: MSR-VTT, LSMDC, MSVD, DiDeMo, and ActivityNet. Code and data are available at www.robots.ox.ac.uk/~vgg/research/collaborative-experts/. This paper also includes a correction to results previously reported.",1
"Biomedical research papers use significantly different language and jargon when compared to typical English text, which reduces the utility of pre-trained NLP models in this domain. Meanwhile Medline, a database of biomedical abstracts, introduces nearly a million new documents per-year. Applications that could benefit from understanding this wealth of publicly available information, such as scientific writing assistants, chat-bots, or descriptive hypothesis generation systems, require new domain-centered approaches. A conditional language model, one that learns the probability of words given some a priori criteria, is a fundamental building block in many such applications. We propose a transformer-based conditional language model with a shallow encoder ""condition"" stack, and a deep ""language model"" stack of multi-headed attention blocks. The condition stack encodes metadata used to alter the output probability distribution of the language model stack. We sample this distribution in order to generate biomedical abstracts given only a proposed title, an intended publication year, and a set of keywords. Using typical natural language generation metrics, we demonstrate that this proposed approach is more capable of producing non-trivial relevant entities within the abstract body than the 1.5B parameter GPT-2 language model.",0
"The language and jargon used in biomedical research papers differs significantly from that of typical English text, leading to a reduced usefulness of pre-trained NLP models in this field. Medline, a database of biomedical abstracts, adds almost a million new documents annually. However, applications such as scientific writing assistants, chat-bots, or descriptive hypothesis generation systems that could benefit from this publicly available information require new domain-focused approaches. A conditional language model, which learns the probability of words based on predetermined criteria, is a crucial component of such applications. Our proposal is a transformer-based conditional language model comprising a shallow encoder ""condition"" stack and a deep ""language model"" stack of multi-headed attention blocks. The condition stack encodes metadata that modifies the output probability distribution of the language model stack. We use this distribution to generate biomedical abstracts with only a proposed title, intended publication year, and a set of keywords. Our proposed approach outperforms the 1.5B parameter GPT-2 language model in producing non-trivial relevant entities within the abstract body, as demonstrated by typical natural language generation metrics.",1
"We present GluonCV and GluonNLP, the deep learning toolkits for computer vision and natural language processing based on Apache MXNet (incubating). These toolkits provide state-of-the-art pre-trained models, training scripts, and training logs, to facilitate rapid prototyping and promote reproducible research. We also provide modular APIs with flexible building blocks to enable efficient customization. Leveraging the MXNet ecosystem, the deep learning models in GluonCV and GluonNLP can be deployed onto a variety of platforms with different programming languages. The Apache 2.0 license has been adopted by GluonCV and GluonNLP to allow for software distribution, modification, and usage.",0
"GluonCV and GluonNLP are deep learning toolkits for computer vision and natural language processing that are built on Apache MXNet (incubating). These toolkits offer advanced pre-trained models, training scripts, and training logs which make it easy to create prototypes and conduct reproducible research. Additionally, the APIs are modular and have flexible building blocks that allow for efficient customization. With the help of the MXNet ecosystem, the deep learning models in GluonCV and GluonNLP can be deployed on various platforms with different programming languages. GluonCV and GluonNLP have adopted the Apache 2.0 license which permits software distribution, modification, and usage.",1
"The goal of this paper is to embed controllable factors, i.e., natural language descriptions, into image-to-image translation with generative adversarial networks, which allows text descriptions to determine the visual attributes of synthetic images. We propose four key components: (1) the implementation of part-of-speech tagging to filter out non-semantic words in the given description, (2) the adoption of an affine combination module to effectively fuse different modality text and image features, (3) a novel refined multi-stage architecture to strengthen the differential ability of discriminators and the rectification ability of generators, and (4) a new structure loss to further improve discriminators to better distinguish real and synthetic images. Extensive experiments on the COCO dataset demonstrate that our method has a superior performance on both visual realism and semantic consistency with given descriptions.",0
"This paper aims to incorporate controllable factors, such as natural language explanations, into image-to-image translation using generative adversarial networks. This allows text descriptions to determine the visual characteristics of synthetic images. We suggest four key components: (1) utilizing part-of-speech tagging to remove irrelevant words in the description, (2) employing an affine combination module to effectively merge text and image features from different modalities, (3) a new refined multi-stage architecture to enhance the differential ability of discriminators and the rectification ability of generators, and (4) a new structure loss to improve the ability of discriminators to distinguish between genuine and synthetic images. Comprehensive experiments on the COCO dataset show that our approach outperforms others in terms of both visual realism and semantic consistency with given descriptions.",1
"We extend the idea of word pieces in natural language models to machine learning tasks on opaque ids. This is achieved by applying hash functions to map each id to multiple hash tokens in a much smaller space, similarly to a Bloom filter. We show that by applying a multi-layer Transformer to these Bloom filter digests, we are able to obtain models with high accuracy. They outperform models of a similar size without hashing and, to a large degree, models of a much larger size trained using sampled softmax with the same computational budget. Our key observation is that it is important to use a multi-layer Transformer for Bloom filter digests to remove ambiguity in the hashed input. We believe this provides an alternative method to solving problems with large vocabulary size.",0
"We expand the concept of word pieces utilized in natural language models to machine learning tasks that involve hidden identifiers. This is accomplished by implementing hash functions to convert each identifier into multiple hash tokens within a much smaller space, similar to a Bloom filter. Our research demonstrates that by employing a multi-layer Transformer to these Bloom filter digests, we can create models with exceptional accuracy. These models outperform similarly-sized models without hashing and, to a significant extent, models with much larger sizes that are trained using sampled softmax with the same computational resources. Our key discovery is the importance of utilizing a multi-layer Transformer for Bloom filter digests to eliminate confusion in the hashed input. We believe that this approach presents an alternative method for resolving issues with a large vocabulary size.",1
"Boolean matrix has been used to represent digital information in many fields, including bank transaction, crime records, natural language processing, protein-protein interaction, etc. Boolean matrix factorization (BMF) aims to find an approximation of a binary matrix as the Boolean product of two low rank Boolean matrices, which could generate vast amount of information for the patterns of relationships between the features and samples. Inspired by binary matrix permutation theories and geometric segmentation, we developed a fast and efficient BMF approach called MEBF (Median Expansion for Boolean Factorization). Overall, MEBF adopted a heuristic approach to locate binary patterns presented as submatrices that are dense in 1's. At each iteration, MEBF permutates the rows and columns such that the permutated matrix is approximately Upper Triangular-Like (UTL) with so-called Simultaneous Consecutive-ones Property (SC1P). The largest submatrix dense in 1 would lies on the upper triangular area of the permutated matrix, and its location was determined based on a geometric segmentation of a triangular. We compared MEBF with other state of the art approaches on data scenarios with different sparsity and noise levels. MEBF demonstrated superior performances in lower reconstruction error, and higher computational efficiency, as well as more accurate sparse patterns than popular methods such as ASSO, PANDA and MP. We demonstrated the application of MEBF on both binary and non-binary data sets, and revealed its further potential in knowledge retrieving and data denoising.",0
"A Boolean matrix is widely used to represent digital information in various fields, such as bank transactions, crime records, natural language processing, and protein-protein interaction. Boolean matrix factorization (BMF) is a technique that aims to approximate a binary matrix as the Boolean product of two low rank Boolean matrices. This method can generate a vast amount of information regarding the relationships between features and samples. We developed a new, fast and efficient BMF approach called MEBF (Median Expansion for Boolean Factorization), which was inspired by binary matrix permutation theories and geometric segmentation. MEBF locates binary patterns that are presented as submatrices dense in 1's by adopting a heuristic approach. At each iteration, MEBF permutes the rows and columns of the matrix to make it approximately Upper Triangular-Like (UTL) with Simultaneous Consecutive-ones Property (SC1P). The largest submatrix dense in 1's is located on the upper triangular area of the permutated matrix using a geometric segmentation of a triangular. We compared MEBF with other state-of-the-art approaches on data scenarios with different sparsity and noise levels. MEBF outperformed popular methods such as ASSO, PANDA, and MP in terms of lower reconstruction error, higher computational efficiency, and more accurate sparse patterns. We also demonstrated the application of MEBF on both binary and non-binary data sets and revealed its potential for knowledge retrieving and data denoising.",1
"Early detection of psychological distress is key to effective treatment. Automatic detection of distress, such as depression, is an active area of research. Current approaches utilise vocal, facial, and bodily modalities. Of these, the bodily modality is the least investigated, partially due to the difficulty in extracting bodily representations from videos, and partially due to the lack of viable datasets. Existing body modality approaches use automatic categorization of expressions to represent body language as a series of specific expressions, much like words within natural language. In this dissertation I present a new type of feature, within the body modality, that represents meta information of gestures, such as speed, and use it to predict a non-clinical depression label. This differs to existing work by representing overall behaviour as a small set of aggregated meta features derived from a person's movement. In my method I extract pose estimation from videos, detect gestures within body parts, extract meta information from individual gestures, and finally aggregate these features to generate a small feature vector for use in prediction tasks. I introduce a new dataset of 65 video recordings of interviews with self-evaluated distress, personality, and demographic labels. This dataset enables the development of features utilising the whole body in distress detection tasks. I evaluate my newly introduced meta-features for predicting depression, anxiety, perceived stress, somatic stress, five standard personality measures, and gender. A linear regression based classifier using these features achieves a 82.70% F1 score for predicting depression within my novel dataset.",0
"The early identification of psychological distress is crucial for effective treatment, with current research focusing on automatic detection methods for conditions like depression. While vocal and facial modalities are commonly used, the bodily modality is less explored due to the difficulty of extracting bodily representations from videos and the lack of suitable datasets. Existing approaches use categorization of expressions to represent body language, but this dissertation presents a novel approach that uses meta information of gestures, such as speed, to predict non-clinical depression. The method extracts pose estimation and meta information from gestures in videos and aggregates them into a small feature vector for prediction tasks. A new dataset of 65 videos with self-evaluated distress, personality, and demographic labels was created to develop features for whole-body distress detection tasks. The newly introduced meta-features were evaluated for predicting depression, anxiety, perceived stress, somatic stress, personality measures, and gender, achieving an 82.70% F1 score for predicting depression using a linear regression-based classifier.",1
"Implementing enterprise process automation often requires significant technical expertise and engineering effort. It would be beneficial for non-technical users to be able to describe a business process in natural language and have an intelligent system generate the workflow that can be automatically executed. A building block of process automations are If-Then programs. In the consumer space, sites like IFTTT and Zapier allow users to create automations by defining If-Then programs using a graphical interface. We explore the efficacy of modeling If-Then programs as a sequence learning task. We find Seq2Seq approaches have high potential (performing strongly on the Zapier recipes) and can serve as a promising approach to more complex program synthesis challenges.",0
The implementation of enterprise process automation typically requires significant technical expertise and engineering effort. It would be advantageous for non-technical users to be able to express a business process using natural language and have an intelligent system create the workflow that can be executed automatically. If-Then programs are a fundamental component of process automations. Consumer-oriented websites such as IFTTT and Zapier enable users to generate automations by defining If-Then programs using a graphical interface. We investigate the effectiveness of modeling If-Then programs as a sequence learning task. Our results indicate that Seq2Seq techniques have considerable potential (demonstrating robust performance on the Zapier recipes) and could be a promising solution for more complex program synthesis challenges.,1
"Undirected neural sequence models such as BERT (Devlin et al., 2019) have received renewed interest due to their success on discriminative natural language understanding tasks such as question-answering and natural language inference. The problem of generating sequences directly from these models has received relatively little attention, in part because generating from undirected models departs significantly from conventional monotonic generation in directed sequence models. We investigate this problem by proposing a generalized model of sequence generation that unifies decoding in directed and undirected models. The proposed framework models the process of generation rather than the resulting sequence, and under this framework, we derive various neural sequence models as special cases, such as autoregressive, semi-autoregressive, and refinement-based non-autoregressive models. This unification enables us to adapt decoding algorithms originally developed for directed sequence models to undirected sequence models. We demonstrate this by evaluating various handcrafted and learned decoding strategies on a BERT-like machine translation model (Lample & Conneau, 2019). The proposed approach achieves constant-time translation results on par with linear-time translation results from the same undirected sequence model, while both are competitive with the state-of-the-art on WMT'14 English-German translation.",0
"There has been a recent resurgence in interest in undirected neural sequence models like BERT (Devlin et al., 2019) because of their success in natural language understanding tasks like question-answering and natural language inference. However, the problem of generating sequences directly from these models has not received much attention because it differs significantly from the conventional monotonic generation in directed sequence models. In this study, we propose a generalized model of sequence generation that unifies decoding in both directed and undirected models. Our framework models the generation process instead of the resulting sequence, and we derive various neural sequence models as special cases. This approach allows us to adapt decoding algorithms developed for directed sequence models to undirected sequence models. We demonstrate the effectiveness of our approach by evaluating various decoding strategies on a BERT-like machine translation model (Lample & Conneau, 2019). Our proposed method achieves constant-time translation results that are on par with linear-time translation results from the same undirected sequence model, and both perform competitively with state-of-the-art results on WMT'14 English-German translation.",1
"Distributed word embeddings have yielded state-of-the-art performance in many NLP tasks, mainly due to their success in capturing useful semantic information. These representations assign only a single vector to each word whereas a large number of words are polysemous (i.e., have multiple meanings). In this work, we approach this critical problem in lexical semantics, namely that of representing various senses of polysemous words in vector spaces. We propose a topic modeling based skip-gram approach for learning multi-prototype word embeddings. We also introduce a method to prune the embeddings determined by the probabilistic representation of the word in each topic. We use our embeddings to show that they can capture the context and word similarity strongly and outperform various state-of-the-art implementations.",0
"The success of distributed word embeddings in several NLP tasks is due to their ability to capture significant semantic information. However, since many words have multiple meanings, assigning only one vector to each word may not be sufficient. This presents a crucial problem in lexical semantics. To address this issue, we propose a topic modeling-based skip-gram approach that learns multi-prototype word embeddings. Additionally, we suggest a method to eliminate embeddings based on the word's probabilistic representation in each topic. Our embeddings demonstrate a robust ability to capture context and word similarity, outperforming other state-of-the-art implementations.",1
"Generative adversarial networks (GANs) are increasingly attracting attention in the computer vision, natural language processing, speech synthesis and similar domains. Arguably the most striking results have been in the area of image synthesis. However, evaluating the performance of GANs is still an open and challenging problem. Existing evaluation metrics primarily measure the dissimilarity between real and generated images using automated statistical methods. They often require large sample sizes for evaluation and do not directly reflect human perception of image quality. In this work, we describe an evaluation metric we call Neuroscore, for evaluating the performance of GANs, that more directly reflects psychoperceptual image quality through the utilization of brain signals. Our results show that Neuroscore has superior performance to the current evaluation metrics in that: (1) It is more consistent with human judgment; (2) The evaluation process needs much smaller numbers of samples; and (3) It is able to rank the quality of images on a per GAN basis. A convolutional neural network (CNN) based neuro-AI interface is proposed to predict Neuroscore from GAN-generated images directly without the need for neural responses. Importantly, we show that including neural responses during the training phase of the network can significantly improve the prediction capability of the proposed model. Materials related to this work are provided at https://github.com/villawang/Neuro-AI-Interface.",0
"The use of Generative adversarial networks (GANs) has gained popularity in domains such as computer vision, speech synthesis, and natural language processing. Among these, the most noteworthy results have been in image synthesis. Nevertheless, the evaluation of GANs remains a challenging and unresolved issue. Currently, existing evaluation metrics rely on automated statistical methods to measure the difference between real and generated images, often requiring large sample sizes and failing to mirror human perception of image quality. To address this, we have developed an evaluation metric called Neuroscore, which uses brain signals to more accurately measure psychoperceptual image quality. Our research shows that Neuroscore outperforms current evaluation metrics, as it aligns more closely with human judgment, requires fewer samples for evaluation, and can rank image quality per GAN basis. Furthermore, we propose a convolutional neural network (CNN) based neuro-AI interface that can directly predict Neuroscore from GAN-generated images without the need for neural responses. Importantly, our work demonstrates that incorporating neural responses during the training phase of the network can significantly enhance the prediction capability of our proposed model. Related materials for this research can be found at https://github.com/villawang/Neuro-AI-Interface.",1
"We consider referring image segmentation. It is a problem at the intersection of computer vision and natural language understanding. Given an input image and a referring expression in the form of a natural language sentence, the goal is to segment the object of interest in the image referred by the linguistic query. To this end, we propose a dual convolutional LSTM (ConvLSTM) network to tackle this problem. Our model consists of an encoder network and a decoder network, where ConvLSTM is used in both encoder and decoder networks to capture spatial and sequential information. The encoder network extracts visual and linguistic features for each word in the expression sentence, and adopts an attention mechanism to focus on words that are more informative in the multimodal interaction. The decoder network integrates the features generated by the encoder network at multiple levels as its input and produces the final precise segmentation mask. Experimental results on four challenging datasets demonstrate that the proposed network achieves superior segmentation performance compared with other state-of-the-art methods.",0
"We are examining the concept of image segmentation, which involves a problem that arises at the intersection of natural language understanding and computer vision. The objective is to segment the object of interest in an input image based on a referring expression in the form of a natural language sentence. In order to address this challenge, we present a dual convolutional LSTM (ConvLSTM) network. Our model comprises an encoder network and a decoder network, both of which employ ConvLSTM to capture spatial and sequential information. The encoder network extracts visual and linguistic features for each word in the expression sentence and utilizes an attention mechanism to focus on words that are most informative during the multimodal interaction. The decoder network integrates the encoder network's generated features at multiple levels and produces the final precise segmentation mask. Our proposed network outperforms other state-of-the-art methods according to experimental results obtained from four challenging datasets.",1
"Understanding intrinsic patterns and predicting spatiotemporal characteristics of cities require a comprehensive representation of urban neighborhoods. Existing works relied on either inter- or intra-region connectivities to generate neighborhood representations but failed to fully utilize the informative yet heterogeneous data within neighborhoods. In this work, we propose Urban2Vec, an unsupervised multi-modal framework which incorporates both street view imagery and point-of-interest (POI) data to learn neighborhood embeddings. Specifically, we use a convolutional neural network to extract visual features from street view images while preserving geospatial similarity. Furthermore, we model each POI as a bag-of-words containing its category, rating, and review information. Analog to document embedding in natural language processing, we establish the semantic similarity between neighborhood (""document"") and the words from its surrounding POIs in the vector space. By jointly encoding visual, textual, and geospatial information into the neighborhood representation, Urban2Vec can achieve performances better than baseline models and comparable to fully-supervised methods in downstream prediction tasks. Extensive experiments on three U.S. metropolitan areas also demonstrate the model interpretability, generalization capability, and its value in neighborhood similarity analysis.",0
"To comprehend the intrinsic patterns and foretell the spatiotemporal characteristics of cities, it is essential to have a comprehensive understanding of urban neighborhoods. Previous research relied on inter- or intra-region connections to create neighborhood representations but did not fully utilize the heterogeneous data within the neighborhoods. In this study, we introduce Urban2Vec, an unsupervised multi-modal framework that integrates street view images and point-of-interest (POI) data to learn neighborhood embeddings. Our approach uses a convolutional neural network to extract visual features from street view images while maintaining geospatial similarity. Additionally, we represent each POI as a bag-of-words that contains its category, rating, and review information. Like document embedding in natural language processing, we establish the semantic similarity between a neighborhood (""document"") and the words from its surrounding POIs in the vector space. By merging visual, textual, and geospatial information to form the neighborhood representation, Urban2Vec outperforms baseline models and is on par with fully-supervised methods in downstream prediction tasks. We conducted extensive experiments on three U.S. metropolitan areas to demonstrate the model's interpretability, generalization ability, and value in neighborhood similarity analysis.",1
"Recent deep generative models are able to provide photo-realistic images as well as visual or textual content embeddings useful to address various tasks of computer vision and natural language processing. Their usefulness is nevertheless often limited by the lack of control over the generative process or the poor understanding of the learned representation. To overcome these major issues, very recent work has shown the interest of studying the semantics of the latent space of generative models. In this paper, we propose to advance on the interpretability of the latent space of generative models by introducing a new method to find meaningful directions in the latent space of any generative model along which we can move to control precisely specific properties of the generated image like the position or scale of the object in the image. Our method does not require human annotations and is particularly well suited for the search of directions encoding simple transformations of the generated image, such as translation, zoom or color variations. We demonstrate the effectiveness of our method qualitatively and quantitatively, both for GANs and variational auto-encoders.",0
"Recent advancements in deep generative models have enabled the production of photo-realistic images and provided visual or textual content embeddings, which have proven useful in addressing various tasks related to computer vision and natural language processing. However, their effectiveness is often hindered by the inability to control the generative process or comprehend the learned representation. Recent studies have shown that it is useful to examine the semantics of the latent space of generative models to overcome these challenges. In this study, we present a new method to enhance the interpretability of the latent space of generative models. Our method allows for the discovery of meaningful directions in the latent space, which can be used to control specific properties of the generated image, such as object position or scale. It does not require human annotations and is ideal for identifying directions that encode simple transformations, like translation, zoom, or color variations. Our approach is effective for both GANs and variational auto-encoders, as demonstrated through qualitative and quantitative analysis.",1
"Variance reduction methods such as SVRG and SpiderBoost use a mixture of large and small batch gradients to reduce the variance of stochastic gradients. Compared to SGD, these methods require at least double the number of operations per update to model parameters. To reduce the computational cost of these methods, we introduce a new sparsity operator: The random-top-k operator. Our operator reduces computational complexity by estimating gradient sparsity exhibited in a variety of applications by combining the top-k operator and the randomized coordinate descent operator. With this operator, large batch gradients offer an extra benefit beyond variance reduction: A reliable estimate of gradient sparsity. Theoretically, our algorithm is at least as good as the best algorithm (SpiderBoost), and further excels in performance whenever the random-top-k operator captures gradient sparsity. Empirically, our algorithm consistently outperforms SpiderBoost using various models on various tasks including image classification, natural language processing, and sparse matrix factorization. We also provide empirical evidence to support the intuition behind our algorithm via a simple gradient entropy computation, which serves to quantify gradient sparsity at every iteration.",0
"Methods like SVRG and SpiderBoost use a combination of large and small batch gradients to decrease the variance of stochastic gradients. However, these methods require twice the number of operations per update to model parameters compared to SGD. We propose a new sparsity operator, the random-top-k operator, to decrease the computational cost of these methods. This operator estimates gradient sparsity by combining the top-k operator with the randomized coordinate descent operator. Large batch gradients not only reduce variance but also provide a reliable estimate of gradient sparsity. Our algorithm is theoretically as good as SpiderBoost and performs even better when the random-top-k operator captures gradient sparsity. Empirically, our algorithm consistently outperforms SpiderBoost on various models and tasks, such as image classification, natural language processing, and sparse matrix factorization. Additionally, we use gradient entropy computation to support the intuition behind our algorithm, which quantifies gradient sparsity at each iteration.",1
"Interactive Fiction games are text-based simulations in which an agent interacts with the world purely through natural language. They are ideal environments for studying how to extend reinforcement learning agents to meet the challenges of natural language understanding, partial observability, and action generation in combinatorially-large text-based action spaces. We present KG-A2C, an agent that builds a dynamic knowledge graph while exploring and generates actions using a template-based action space. We contend that the dual uses of the knowledge graph to reason about game state and to constrain natural language generation are the keys to scalable exploration of combinatorially large natural language actions. Results across a wide variety of IF games show that KG-A2C outperforms current IF agents despite the exponential increase in action space size.",0
"Text-based simulations called Interactive Fiction games involve an agent that interacts with the virtual environment solely through natural language. These games offer an excellent platform for investigating how reinforcement learning agents can be enhanced to tackle the challenges of natural language comprehension, partial observability, and action generation within vast text-based action spaces. Our paper introduces KG-A2C, an agent that constructs a dynamic knowledge graph while exploring and produces actions using an action space based on templates. We argue that the knowledge graph's two-fold function in reasoning about the game's status and restricting natural language production is critical to scaling up exploration of vast natural language actions. Our findings indicate that, compared to existing Interactive Fiction agents, KG-A2C performs better across a broad range of IF games, despite the exponential growth in the action space size.",1
"In natural language processing, it has been observed recently that generalization could be greatly improved by finetuning a large-scale language model pretrained on a large unlabeled corpus. Despite its recent success and wide adoption, finetuning a large pretrained language model on a downstream task is prone to degenerate performance when there are only a small number of training instances available. In this paper, we introduce a new regularization technique, to which we refer as ""mixout"", motivated by dropout. Mixout stochastically mixes the parameters of two models. We show that our mixout technique regularizes learning to minimize the deviation from one of the two models and that the strength of regularization adapts along the optimization trajectory. We empirically evaluate the proposed mixout and its variants on finetuning a pretrained language model on downstream tasks. More specifically, we demonstrate that the stability of finetuning and the average accuracy greatly increase when we use the proposed approach to regularize finetuning of BERT on downstream tasks in GLUE.",0
"Recently, natural language processing has witnessed an improvement in generalization through the finetuning of a large-scale language model that was pretrained on an extensive unlabeled corpus. However, the performance of finetuning a large pretrained language model on a downstream task is susceptible to degeneration when there is a limited number of training instances. This paper introduces a new regularization technique called ""mixout,"" inspired by dropout, which stochastically mixes the parameters of two models. The mixout technique minimizes the deviation from one of the two models, and the strength of regularization adapts during the optimization trajectory. Empirical evaluation of mixout and its variants on finetuning a pretrained language model on downstream tasks shows a significant increase in stability and average accuracy. This approach proves effective when regularizing the finetuning of BERT on downstream tasks in GLUE.",1
"Learning from one or few visual examples is one of the key capabilities of humans since early infancy, but is still a significant challenge for modern AI systems. While considerable progress has been achieved in few-shot learning from a few image examples, much less attention has been given to the verbal descriptions that are usually provided to infants when they are presented with a new object. In this paper, we focus on the role of additional semantics that can significantly facilitate few-shot visual learning. Building upon recent advances in few-shot learning with additional semantic information, we demonstrate that further improvements are possible by combining multiple and richer semantics (category labels, attributes, and natural language descriptions). Using these ideas, we offer the community new results on the popular miniImageNet and CUB few-shot benchmarks, comparing favorably to the previous state-of-the-art results for both visual only and visual plus semantics-based approaches. We also performed an ablation study investigating the components and design choices of our approach.",0
"Humans are able to learn from a small number of visual examples, but this is still a challenge for AI systems. While progress has been made in few-shot learning from images, less attention has been given to verbal descriptions. This paper focuses on the role of additional semantics in facilitating few-shot visual learning. By combining multiple and richer semantics, such as category labels, attributes, and natural language descriptions, we were able to achieve improved results on the miniImageNet and CUB few-shot benchmarks compared to previous approaches. We also conducted an ablation study to investigate the components and design choices of our approach.",1
"Generative adversarial networks (GANs) are a hot research topic recently. GANs have been widely studied since 2014, and a large number of algorithms have been proposed. However, there is few comprehensive study explaining the connections among different GANs variants, and how they have evolved. In this paper, we attempt to provide a review on various GANs methods from the perspectives of algorithms, theory, and applications. Firstly, the motivations, mathematical representations, and structure of most GANs algorithms are introduced in details. Furthermore, GANs have been combined with other machine learning algorithms for specific applications, such as semi-supervised learning, transfer learning, and reinforcement learning. This paper compares the commonalities and differences of these GANs methods. Secondly, theoretical issues related to GANs are investigated. Thirdly, typical applications of GANs in image processing and computer vision, natural language processing, music, speech and audio, medical field, and data science are illustrated. Finally, the future open research problems for GANs are pointed out.",0
"Recently, there has been a surge of interest in Generative Adversarial Networks (GANs). While numerous algorithms have been proposed since they were first widely studied in 2014, there is a lack of comprehensive research on the evolution of different GAN variants and their connections. In this paper, we aim to provide a detailed review of various GAN methods, covering algorithms, theory, and applications. We begin by introducing the motivations, mathematical representations, and structures of most GANs. We then explore how GANs have been combined with other machine learning algorithms for specific purposes, including semi-supervised learning, transfer learning, and reinforcement learning. Our paper compares the similarities and differences among these GAN methods. Next, we examine theoretical issues related to GANs. Finally, we illustrate typical applications of GANs in a range of fields, including image processing, computer vision, natural language processing, music, speech and audio, medical research, and data science. We also identify open research problems for future investigation in the field of GANs.",1
"One of the key challenges in designing machine learning systems is to determine the right balance amongst several objectives, which also oftentimes are incommensurable and conflicting. For example, when designing deep neural networks (DNNs), one often has to trade-off between multiple objectives, such as accuracy, energy consumption, and inference time. Typically, there is no single configuration that performs equally well for all objectives. Consequently, one is interested in identifying Pareto-optimal designs. Although different multi-objective optimization algorithms have been developed to identify Pareto-optimal configurations, state-of-the-art multi-objective optimization methods do not consider the different evaluation costs attending the objectives under consideration. This is particularly important for optimizing DNNs: the cost arising on account of assessing the accuracy of DNNs is orders of magnitude higher than that of measuring the energy consumption of pre-trained DNNs. We propose FlexiBO, a flexible Bayesian optimization method, to address this issue. We formulate a new acquisition function based on the improvement of the Pareto hyper-volume weighted by the measurement cost of each objective. Our acquisition function selects the next sample and objective that provides maximum information gain per unit of cost. We evaluated FlexiBO on 7 state-of-the-art DNNs for object detection, natural language processing, and speech recognition. Our results indicate that, when compared to other state-of-the-art methods across the 7 architectures we tested, the Pareto front obtained using FlexiBO has, on average, a 28.44% higher contribution to the true Pareto front and achieves 25.64% better diversity.",0
"Designing machine learning systems presents a significant challenge as there are multiple objectives to balance, which are often incompatible. This is particularly evident in deep neural networks (DNNs), where accuracy, energy consumption, and inference time must be traded off against each other. Identifying Pareto-optimal designs is crucial, as there is no single configuration that performs well for all objectives. However, current multi-objective optimization algorithms do not factor in the different evaluation costs associated with each objective. To address this issue, we propose FlexiBO, a flexible Bayesian optimization method. Our acquisition function is based on Pareto hyper-volume improvement weighted by the measurement cost of each objective. This ensures that the next sample and objective selected provide maximum information gain per unit of cost. We tested FlexiBO on seven DNNs for object detection, natural language processing, and speech recognition. Our results show that compared to other state-of-the-art methods, FlexiBO's Pareto front has 28.44% higher contribution to the true Pareto front and achieves 25.64% better diversity across the seven architectures.",1
"Video moment retrieval is to search the moment that is most relevant to the given natural language query. Existing methods are mostly trained in a fully-supervised setting, which requires the full annotations of temporal boundary for each query. However, manually labeling the annotations is actually time-consuming and expensive. In this paper, we propose a novel weakly-supervised moment retrieval framework requiring only coarse video-level annotations for training. Specifically, we devise a proposal generation module that aggregates the context information to generate and score all candidate proposals in one single pass. We then devise an algorithm that considers both exploitation and exploration to select top-K proposals. Next, we build a semantic completion module to measure the semantic similarity between the selected proposals and query, compute reward and provide feedbacks to the proposal generation module for scoring refinement. Experiments on the ActivityCaptions and Charades-STA demonstrate the effectiveness of our proposed method.",0
"The objective of video moment retrieval is to identify the most relevant moment in response to a natural language query. However, current techniques rely heavily on a fully-supervised training approach, which necessitates complete temporal boundary annotations for each query. This annotation process is both time-consuming and expensive. To address this issue, our paper introduces a novel weakly-supervised moment retrieval framework that only requires rough video-level annotations for training. We have developed a proposal generation module that employs context information to produce and evaluate all potential proposals in one go. We have also created an algorithm that balances exploitation and exploration to select the top-K proposals. Additionally, we have constructed a semantic completion module to assess the semantic similarity between the chosen proposals and the query, calculate rewards, and provide feedback for refinement of the proposal generation module's scoring. Our proposed technique has been tested on ActivityCaptions and Charades-STA, and the results demonstrate its efficacy.",1
"The recent success of deep neural networks (DNNs) for function approximation in reinforcement learning has triggered the development of Deep Reinforcement Learning (DRL) algorithms in various fields, such as robotics, computer games, natural language processing, computer vision, sensing systems, and wireless networking. Unfortunately, DNNs suffer from high computational cost and memory consumption, which limits the use of DRL algorithms in systems with limited hardware resources. In recent years, pruning algorithms have demonstrated considerable success in reducing the redundancy of DNNs in classification tasks. However, existing algorithms suffer from a significant performance reduction in the DRL domain. In this paper, we develop the first effective solution to the performance reduction problem of pruning in the DRL domain, and establish a working algorithm, named Policy Pruning and Shrinking (PoPS), to train DRL models with strong performance while achieving a compact representation of the DNN. The framework is based on a novel iterative policy pruning and shrinking method that leverages the power of transfer learning when training the DRL model. We present an extensive experimental study that demonstrates the strong performance of PoPS using the popular Cartpole, Lunar Lander, Pong, and Pacman environments. Finally, we develop an open source software for the benefit of researchers and developers in related fields.",0
"The success of deep neural networks (DNNs) in function approximation for reinforcement learning has led to the development of Deep Reinforcement Learning (DRL) algorithms in various fields. These include robotics, computer games, natural language processing, computer vision, sensing systems, and wireless networking. However, DNNs consume a lot of computational resources and memory, making it difficult to use DRL algorithms in systems with limited hardware resources. While pruning algorithms have been successful in reducing DNN redundancy in classification tasks, they have not performed well in the DRL domain. This paper introduces Policy Pruning and Shrinking (PoPS), the first effective solution to the performance reduction problem of pruning in the DRL domain. PoPS is a framework based on a novel iterative policy pruning and shrinking method that uses transfer learning when training the DRL model. Extensive experiments demonstrate the strong performance of PoPS in popular environments such as Cartpole, Lunar Lander, Pong, and Pacman. Additionally, an open source software is developed to benefit researchers and developers in related fields.",1
"The success stories from deep learning models increase every day spanning different tasks from image classification to natural language understanding. With the increasing popularity of these models, scientists spend more and more time finding the optimal parameters and best model architectures for their tasks. In this paper, we focus on the ingredient that feeds these machines: the data. We hypothesize that the data ordering affects how well a model performs. To that end, we conduct experiments on an image classification task using ImageNet dataset and show that some data orderings are better than others in terms of obtaining higher classification accuracies. Experimental results show that independent of model architecture, learning rate and batch size, ordering of the data significantly affects the outcome. We show these findings using different metrics: NDCG, accuracy @ 1 and accuracy @ 5. Our goal here is to show that not only parameters and model architectures but also the data ordering has a say in obtaining better results.",0
"Every day, there are more success stories about deep learning models that cover various tasks including image classification and natural language understanding. As these models continue to gain popularity, researchers are spending more time trying to find the best model architectures and optimal parameters for their specific tasks. This paper focuses on the crucial aspect that fuels these machines, which is the data. Our hypothesis is that the order in which the data is presented can have a significant impact on the model's performance. We conducted experiments on the ImageNet dataset for an image classification task and found that some data orderings produce higher classification accuracies than others. Regardless of the model architecture, learning rate, or batch size, the ordering of the data has a considerable effect on the outcome, as shown by our experimental results using various metrics such as NDCG, accuracy @ 1, and accuracy @ 5. Our aim is to demonstrate that achieving better results is not only dependent on the parameters and model architectures but also on the ordering of the data.",1
"One of the long-term challenges of robotics is to enable robots to interact with humans in the visual world via natural language, as humans are visual animals that communicate through language. Overcoming this challenge requires the ability to perform a wide variety of complex tasks in response to multifarious instructions from humans. In the hope that it might drive progress towards more flexible and powerful human interactions with robots, we propose a dataset of varied and complex robot tasks, described in natural language, in terms of objects visible in a large set of real images. Given an instruction, success requires navigating through a previously-unseen environment to identify an object. This represents a practical challenge, but one that closely reflects one of the core visual problems in robotics. Several state-of-the-art vision-and-language navigation, and referring-expression models are tested to verify the difficulty of this new task, but none of them show promising results because there are many fundamental differences between our task and previous ones. A novel Interactive Navigator-Pointer model is also proposed that provides a strong baseline on the task. The proposed model especially achieves the best performance on the unseen test split, but still leaves substantial room for improvement compared to the human performance.",0
"The challenge for robotics in the long run is to enable robots to communicate with humans through natural language in the visual world. Since humans are visual creatures that communicate through language, this challenge requires robots to be able to execute various complex tasks based on diverse instructions from humans. To make progress towards more effective human-robot interactions, we suggest a dataset of diverse and intricate robot tasks described in natural language in terms of visible objects in a large collection of real images. To succeed in this, the robot must navigate through an unseen environment to identify an object based on the instruction given. This is a practical challenge that reflects one of the core visual problems in robotics. Although several state-of-the-art vision-and-language navigation and referring-expression models were tested, none of them produced promising results due to fundamental differences between our task and previous ones. We propose a new Interactive Navigator-Pointer model that provides a robust baseline for the task, achieving the best performance on the unseen test split. However, there is still substantial room for improvement compared to human performance.",1
"For joint inference over multiple variables, a variety of structured prediction techniques have been developed to model correlations among variables and thereby improve predictions. However, many classical approaches suffer from one of two primary drawbacks: they either lack the ability to model high-order correlations among variables while maintaining computationally tractable inference, or they do not allow to explicitly model known correlations. To address this shortcoming, we introduce `Graph Structured Prediction Energy Networks,' for which we develop inference techniques that allow to both model explicit local and implicit higher-order correlations while maintaining tractability of inference. We apply the proposed method to tasks from the natural language processing and computer vision domain and demonstrate its general utility.",0
"Several techniques have been developed to enhance predictions by modeling correlations among multiple variables for joint inference. However, classical approaches have two major downsides. Firstly, they fail to model high-order correlations among variables while ensuring computationally feasible inference. Secondly, they don't enable the explicit modeling of known correlations. To overcome these limitations, we present 'Graph Structured Prediction Energy Networks'. We have developed inference techniques that facilitate the modeling of both explicit local and implicit higher-order correlations while maintaining the feasibility of inference. We have validated our approach by applying it to natural language processing and computer vision problems, demonstrating its versatility.",1
"We present a robust aggregation approach to make federated learning robust to settings when a fraction of the devices may be sending corrupted updates to the server. The proposed approach relies on a robust secure aggregation oracle based on the geometric median, which returns a robust aggregate using a constant number of calls to a regular non-robust secure average oracle. The robust aggregation oracle is privacy-preserving, similar to the secure average oracle it builds upon. We provide experimental results of the proposed approach with linear models and deep networks for two tasks in computer vision and natural language processing. The robust aggregation approach is agnostic to the level of corruption; it outperforms the classical aggregation approach in terms of robustness when the level of corruption is high, while being competitive in the regime of low corruption.",0
"In order to ensure that federated learning remains effective even when some devices send corrupted updates to the server, we have developed a robust aggregation approach. Our approach makes use of a secure aggregation oracle based on the geometric median, which is capable of returning a robust aggregate with just a few calls to a regular non-robust secure average oracle. The privacy-preserving nature of the robust aggregation oracle is similar to the secure average oracle it is built upon. We have conducted experiments with linear models and deep networks in the fields of computer vision and natural language processing, and have found that our approach is agnostic to the level of corruption. In fact, it outperforms the classical aggregation approach in terms of robustness when the level of corruption is high, while still remaining competitive in situations with low corruption.",1
"Nowadays, the use of machine learning models is becoming a utility in many applications. Companies deliver pre-trained models encapsulated as application programming interfaces (APIs) that developers combine with third party components and their own models and data to create complex data products to solve specific problems. The complexity of such products and the lack of control and knowledge of the internals of each component used cause unavoidable effects, such as lack of transparency, difficulty in auditability, and emergence of potential uncontrolled risks. They are effectively black-boxes. Accountability of such solutions is a challenge for the auditors and the machine learning community. In this work, we propose a wrapper that given a black-box model enriches its output prediction with a measure of uncertainty. By using this wrapper, we make the black-box auditable for the accuracy risk (risk derived from low quality or uncertain decisions) and at the same time we provide an actionable mechanism to mitigate that risk in the form of decision rejection; we can choose not to issue a prediction when the risk or uncertainty in that decision is significant. Based on the resulting uncertainty measure, we advocate for a rejection system that selects the more confident predictions, discarding those more uncertain, leading to an improvement in the trustability of the resulting system. We showcase the proposed technique and methodology in a practical scenario where a simulated sentiment analysis API based on natural language processing is applied to different domains. Results demonstrate the effectiveness of the uncertainty computed by the wrapper and its high correlation to bad quality predictions and misclassifications.",0
"In modern times, machine learning models are being widely utilized in various applications. Companies provide pre-trained models in the form of APIs, which developers combine with third-party components and their own data to create complex data products to solve specific problems. However, the complexity of these products and the lack of control and knowledge of the components cause issues such as lack of transparency, difficulty in auditability, and potential uncontrolled risks. This results in black-box solutions, making it challenging for auditors and the machine learning community to hold them accountable. To address this, we suggest using a wrapper to enrich the output prediction of a black-box model with a measure of uncertainty. This allows for audibility of accuracy risk and provides a mechanism for decision rejection when significant risk or uncertainty is present. Our proposed rejection system selects more confident predictions and discards uncertain ones, leading to an improvement in the trustability of the system. We demonstrate the effectiveness of this technique in a practical scenario involving simulated sentiment analysis API based on natural language processing. Results show a strong correlation between uncertainty and bad quality predictions or misclassifications.",1
"Significant advances have been made in artificial systems by using biological systems as a guide. However, there is often little interaction between computational models for emergent communication and biological models of the emergence of language. Many researchers in language origins and emergent communication take compositionality as their primary target for explaining how simple communication systems can become more like natural language. However, there is reason to think that compositionality is the wrong target on the biological side, and so too the wrong target on the machine-learning side. As such, the purpose of this paper is to explore this claim. This has theoretical implications for language origins research more generally, but the focus here will be the implications for research on emergent communication in computer science and machine learning---specifically regarding the types of programmes that might be expected to work and those which will not. I further suggest an alternative approach for future research which focuses on reflexivity, rather than compositionality, as a target for explaining how simple communication systems may become more like natural language. I end by providing some reference to the language origins literature that may be of some use to researchers in machine learning.",0
"Using biological systems as a guide has led to significant advances in artificial systems. However, there is often minimal interaction between computational models for emergent communication and biological models of language emergence. Many researchers focus on compositionality to explain how simple communication systems can resemble natural language. However, this may be the wrong target on both the biological and machine-learning sides. This paper aims to explore this claim, specifically in the context of emergent communication in computer science and machine learning. The theoretical implications of this research apply to language origins research more broadly. I suggest an alternative approach that prioritizes reflexivity over compositionality to explain how simple communication systems may become more like natural language. Finally, I provide references to language origins literature that may benefit machine learning researchers.",1
"Vision and language are two fundamental capabilities of human intelligence. Humans routinely perform tasks through the interactions between vision and language, supporting the uniquely human capacity to talk about what they see or hallucinate a picture on a natural-language description. The valid question of how language interacts with vision motivates us researchers to expand the horizons of computer vision area. In particular, ""vision to language"" is probably one of the most popular topics in the past five years, with a significant growth in both volume of publications and extensive applications, e.g., captioning, visual question answering, visual dialog, language navigation, etc. Such tasks boost visual perception with more comprehensive understanding and diverse linguistic representations. Going beyond the progresses made in ""vision to language,"" language can also contribute to vision understanding and offer new possibilities of visual content creation, i.e., ""language to vision."" The process performs as a prism through which to create visual content conditioning on the language inputs. This paper reviews the recent advances along these two dimensions: ""vision to language"" and ""language to vision."" More concretely, the former mainly focuses on the development of image/video captioning, as well as typical encoder-decoder structures and benchmarks, while the latter summarizes the technologies of visual content creation. The real-world deployment or services of vision and language are elaborated as well.",0
"Human intelligence relies on two essential capabilities, vision and language. These abilities enable people to carry out tasks by combining visual and linguistic information, allowing them to describe what they see or imagine a scene based on language. The interaction between language and vision has become a popular research topic in computer vision, particularly the concept of ""vision to language."" This area has seen significant growth in both publications and applications, including visual question answering, language navigation, and visual dialog. Additionally, language can also contribute to vision understanding and support visual content creation, known as ""language to vision."" This process utilizes language inputs to create visual content. This paper explores recent developments in both ""vision to language"" and ""language to vision."" The former mainly focuses on image/video captioning, encoder-decoder structures, and benchmarks, while the latter summarizes visual content creation technologies. The paper also discusses real-world deployment and services of vision and language.",1
"Deep neural networks (DNNs) have emerged as a popular mathematical tool for function approximation due to their capability of modelling highly nonlinear functions. Their applications range from image classification and natural language processing to learning-based control. Despite their empirical successes, there is still a lack of theoretical understanding of the representative power of such deep architectures. In this work, we provide a theoretical analysis of the expressiveness of fully-connected, feedforward DNNs with 1-Lipschitz activation functions. In particular, we characterize the expressiveness of a DNN by its Lipchitz constant. By leveraging random matrix theory, we show that, given sufficiently large and randomly distributed weights, the expected upper and lower bounds of the Lipschitz constant of a DNN and hence their expressiveness increase exponentially with depth and polynomially with width, which gives rise to the benefit of the depth of DNN architectures for efficient function approximation. This observation is consistent with established results based on alternative expressiveness measures of DNNs. In contrast to most of the existing work, our analysis based on the Lipschitz properties of DNNs is applicable to a wider range of activation nonlinearities and potentially allows us to make sensible comparisons between the complexity of a DNN and the function to be approximated by the DNN. We consider this work to be a step towards understanding the expressive power of DNNs and towards designing appropriate deep architectures for practical applications such as system control.",0
"Deep neural networks (DNNs) are a widely used mathematical tool for function approximation because they are capable of modelling highly nonlinear functions. They have found applications in diverse fields such as image classification, natural language processing and learning-based control. However, despite their practical success, there is a lack of theoretical understanding of the representative power of these deep architectures. In this study, we focus on fully-connected, feedforward DNNs with 1-Lipschitz activation functions and provide a theoretical analysis of their expressiveness. We characterize the expressiveness of a DNN based on its Lipchitz constant and show that, with sufficiently large and randomly distributed weights, the expected upper and lower bounds of the Lipschitz constant increase exponentially with depth and polynomially with width. This observation is consistent with established results based on alternative expressiveness measures of DNNs. Our analysis based on the Lipschitz properties of DNNs is applicable to a wider range of activation nonlinearities and allows for sensible comparisons between the complexity of a DNN and the function it approximates. We believe this work is an important step towards understanding the expressive power of DNNs and designing appropriate deep architectures for practical applications like system control.",1
"Learning an embedding for a large collection of items is a popular approach to overcome the computational limitations associated to one-hot encodings. The aim of item embedding is to learn a low dimensional space for the representations, able to capture with its geometry relevant features or relationships for the data at hand. This can be achieved for example by exploiting adjacencies among items in large sets of unlabelled data. In this paper we interpret in an Information Geometric framework the item embeddings obtained from conditional models. By exploiting the $\alpha$-geometry of the exponential family, first introduced by Amari, we introduce a family of natural $\alpha$-embeddings represented by vectors in the tangent space of the probability simplex, which includes as a special case standard approaches available in the literature. A typical example is given by word embeddings, commonly used in natural language processing, such as Word2Vec and GloVe. In our analysis, we show how the $\alpha$-deformation parameter can impact on standard evaluation tasks.",0
"To overcome computational limitations associated with one-hot encodings, many opt to learn an embedding for a large collection of items. The goal of item embedding is to develop a low dimensional space that can effectively capture relevant features and relationships for the given data using its geometry. One way to achieve this is by leveraging adjacencies among items in large sets of unlabelled data. In this paper, we utilize an Information Geometric framework to interpret item embeddings obtained from conditional models. By exploiting the $\alpha$-geometry of the exponential family, we introduce a family of natural $\alpha$-embeddings that are represented by vectors in the tangent space of the probability simplex. This family includes standard approaches available in the literature as a special case, such as Word2Vec and GloVe, which are commonly used in natural language processing. Our analysis demonstrates how the $\alpha$-deformation parameter can impact standard evaluation tasks.",1
"Learning text representation is crucial for text classification and other language related tasks. There are a diverse set of text representation networks in the literature, and how to find the optimal one is a non-trivial problem. Recently, the emerging Neural Architecture Search (NAS) techniques have demonstrated good potential to solve the problem. Nevertheless, most of the existing works of NAS focus on the search algorithms and pay little attention to the search space. In this paper, we argue that the search space is also an important human prior to the success of NAS in different applications. Thus, we propose a novel search space tailored for text representation. Through automatic search, the discovered network architecture outperforms state-of-the-art models on various public datasets on text classification and natural language inference tasks. Furthermore, some of the design principles found in the automatic network agree well with human intuition.",0
"Acquiring knowledge about text representation is crucial for tasks involving language, such as text classification. A wide range of text representation networks are available in literature, and determining the optimal one is a challenging problem. Recently, Neural Architecture Search (NAS) techniques have shown promise in addressing this issue. However, most NAS studies focus on search algorithms and neglect the significance of the search space. This paper argues that the search space is also a critical factor in the success of NAS in different applications. Consequently, a new search space designed for text representation is proposed. The discovered network architecture outperforms existing models on various public datasets for text classification and natural language inference tasks. Additionally, some of the design principles discovered in the automatic network align with human intuition.",1
"Recently, pre-trained language representation flourishes as the mainstay of the natural language understanding community, e.g., BERT. These pre-trained language representations can create state-of-the-art results on a wide range of downstream tasks. Along with continuous significant performance improvement, the size and complexity of these pre-trained neural models continue to increase rapidly. Is it possible to compress these large-scale language representation models? How will the pruned language representation affect the downstream multi-task transfer learning objectives? In this paper, we propose Reweighted Proximal Pruning (RPP), a new pruning method specifically designed for a large-scale language representation model. Through experiments on SQuAD and the GLUE benchmark suite, we show that proximal pruned BERT keeps high accuracy for both the pre-training task and the downstream multiple fine-tuning tasks at high prune ratio. RPP provides a new perspective to help us analyze what large-scale language representation might learn. Additionally, RPP makes it possible to deploy a large state-of-the-art language representation model such as BERT on a series of distinct devices (e.g., online servers, mobile phones, and edge devices).",0
"The natural language understanding community has recently embraced pre-trained language representation models such as BERT, which can achieve impressive results across a broad range of tasks. However, these models are becoming increasingly large and complex, raising questions about whether they can be compressed and how this might affect downstream multi-task transfer learning objectives. This paper presents a new pruning method called Reweighted Proximal Pruning (RPP) that is specifically designed for large-scale language representation models. The experiments conducted on SQuAD and the GLUE benchmark suite demonstrate that BERT can maintain high accuracy even when significantly pruned using RPP. This approach not only offers insights into what large-scale language representation models might learn, but also makes it possible to deploy them on a variety of devices, including online servers, mobile phones, and edge devices.",1
"In this paper, we propose a novel controllable text-to-image generative adversarial network (ControlGAN), which can effectively synthesise high-quality images and also control parts of the image generation according to natural language descriptions. To achieve this, we introduce a word-level spatial and channel-wise attention-driven generator that can disentangle different visual attributes, and allow the model to focus on generating and manipulating subregions corresponding to the most relevant words. Also, a word-level discriminator is proposed to provide fine-grained supervisory feedback by correlating words with image regions, facilitating training an effective generator which is able to manipulate specific visual attributes without affecting the generation of other content. Furthermore, perceptual loss is adopted to reduce the randomness involved in the image generation, and to encourage the generator to manipulate specific attributes required in the modified text. Extensive experiments on benchmark datasets demonstrate that our method outperforms existing state of the art, and is able to effectively manipulate synthetic images using natural language descriptions. Code is available at https://github.com/mrlibw/ControlGAN.",0
"The ControlGAN is a newly proposed text-to-image generative adversarial network that can produce high-quality images and manipulate specific visual attributes based on natural language descriptions. The model includes a word-level spatial and channel-wise attention-driven generator that can identify and manipulate subregions related to the most relevant words, as well as a word-level discriminator that provides fine-grained feedback to ensure the generator accurately manipulates specific visual attributes without affecting other content. Additionally, the model uses perceptual loss to reduce randomness and encourage the generator to manipulate specific attributes required in the modified text. Results of extensive experiments on benchmark datasets show that the proposed method outperforms existing state-of-the-art techniques and effectively manipulates synthetic images using natural language descriptions. The code for the ControlGAN is available at https://github.com/mrlibw/ControlGAN.",1
"Exchanging gradients is a widely used method in modern multi-node machine learning system (e.g., distributed training, collaborative learning). For a long time, people believed that gradients are safe to share: i.e., the training data will not be leaked by gradient exchange. However, we show that it is possible to obtain the private training data from the publicly shared gradients. We name this leakage as Deep Leakage from Gradient and empirically validate the effectiveness on both computer vision and natural language processing tasks. Experimental results show that our attack is much stronger than previous approaches: the recovery is pixel-wise accurate for images and token-wise matching for texts. We want to raise people's awareness to rethink the gradient's safety. Finally, we discuss several possible strategies to prevent such deep leakage. The most effective defense method is gradient pruning.",0
"The exchanging of gradients is a commonly used technique in present-day multi-node machine learning systems, including distributed training and collaborative learning. Previously, it was believed that sharing gradients was secure and would not result in the leakage of training data. However, our research demonstrates that the private training data can be obtained from publicly shared gradients. We have named this phenomenon ""Deep Leakage from Gradient"" and have confirmed its effectiveness through experiments conducted on computer vision and natural language processing tasks. Our attack is more potent than previous methods, yielding pixel-wise accuracy for images and token-wise matching for texts. We hope to raise awareness of the potential risks associated with gradient sharing and have discussed some potential strategies to prevent Deep Leakage from Gradient. Gradient pruning is the most effective defense mechanism.",1
"Reasoning with knowledge expressed in natural language and Knowledge Bases (KBs) is a major challenge for Artificial Intelligence, with applications in machine reading, dialogue, and question answering. General neural architectures that jointly learn representations and transformations of text are very data-inefficient, and it is hard to analyse their reasoning process. These issues are addressed by end-to-end differentiable reasoning systems such as Neural Theorem Provers (NTPs), although they can only be used with small-scale symbolic KBs. In this paper we first propose Greedy NTPs (GNTPs), an extension to NTPs addressing their complexity and scalability limitations, thus making them applicable to real-world datasets. This result is achieved by dynamically constructing the computation graph of NTPs and including only the most promising proof paths during inference, thus obtaining orders of magnitude more efficient models. Then, we propose a novel approach for jointly reasoning over KBs and textual mentions, by embedding logic facts and natural language sentences in a shared embedding space. We show that GNTPs perform on par with NTPs at a fraction of their cost while achieving competitive link prediction results on large datasets, providing explanations for predictions, and inducing interpretable models. Source code, datasets, and supplementary material are available online at https://github.com/uclnlp/gntp.",0
"Artificial Intelligence faces a significant challenge when reasoning with natural language and Knowledge Bases (KBs), which has applications in dialogue, machine reading, and question answering. Although general neural architectures that jointly learn representations and transformations of text are available, they are data-inefficient, and it is difficult to analyze their reasoning process. Neural Theorem Provers (NTPs) are end-to-end differentiable reasoning systems that address these issues, but they can only be used with small-scale symbolic KBs. In this paper, we propose Greedy NTPs (GNTPs), which extend NTPs by addressing their complexity and scalability limitations, making them applicable to real-world datasets. By dynamically constructing the computation graph of NTPs and including only the most promising proof paths during inference, we obtain orders of magnitude more efficient models. Additionally, we introduce a novel approach for jointly reasoning over KBs and textual mentions by embedding logic facts and natural language sentences in a shared embedding space. We demonstrate that GNTPs perform on par with NTPs at a fraction of their cost while achieving competitive link prediction results on large datasets, providing explanations for predictions, and inducing interpretable models. We have made the source code, datasets, and supplementary material available online at https://github.com/uclnlp/gntp.",1
"Altering the content of an image with photo editing tools is a tedious task for an inexperienced user. Especially, when modifying the visual attributes of a specific object in an image without affecting other constituents such as background etc. To simplify the process of image manipulation and to provide more control to users, it is better to utilize a simpler interface like natural language. Therefore, in this paper, we address the challenge of manipulating images using natural language description. We propose the Two-sidEd Attentive conditional Generative Adversarial Network (TEA-cGAN) to generate semantically manipulated images while preserving other contents such as background intact. TEA-cGAN uses fine-grained attention both in the generator and discriminator of Generative Adversarial Network (GAN) based framework at different scales. Experimental results show that TEA-cGAN which generates 128x128 and 256x256 resolution images outperforms existing methods on CUB and Oxford-102 datasets both quantitatively and qualitatively.",0
"An inexperienced user may find altering the content of an image with photo editing tools to be a tedious task, especially when they need to modify the visual attributes of a particular object without affecting other constituents such as the background. To provide users with a simpler interface and more control over the process of image manipulation, it is suggested to use natural language. This paper addresses the challenge of manipulating images using natural language descriptions and proposes TEA-cGAN, a Two-sidEd Attentive conditional Generative Adversarial Network. TEA-cGAN generates semantically manipulated images while preserving other contents, such as the background, using fine-grained attention in the generator and discriminator of a GAN-based framework at different scales. Experimental results show that TEA-cGAN outperforms existing methods on CUB and Oxford-102 datasets both quantitatively and qualitatively, generating high-resolution images of 128x128 and 256x256.",1
"App classification is useful in a number of applications such as adding apps to an app store or building a user model based on the installed apps. Presently there are a number of existing methods to classify apps based on a given taxonomy on the basis of their text metadata. However, text based methods for app classification may not work in all cases, such as when the text descriptions are in a different language, or missing, or inadequate to classify the app. One solution in such cases is to utilize the app images to supplement the text description. In this paper, we evaluate a number of approaches in which app images can be used to classify the apps. In one approach, we use Optical character recognition (OCR) to extract text from images, which is then used to supplement the text description of the app. In another, we use pic2vec to convert the app images into vectors, then train an SVM to classify the vectors to the correct app label. In another, we use the captionbot.ai tool to generate natural language descriptions from the app images. Finally, we use a method to detect and label objects in the app images and use a voting technique to determine the category of the app based on all the images. We compare the performance of our image-based techniques to classify a number of apps in our dataset. We use a text based SVM app classifier as our base and obtained an improved classification accuracy of 96% for some classes when app images are added.",0
"App classification serves multiple purposes such as app store organization and user profile creation based on installed apps. Although current app classification methods rely on text metadata, this may not always be effective when text descriptions are missing, inadequate, or in a different language. One solution is to use app images to supplement text descriptions. This paper evaluates various approaches to app classification using images. These include using OCR to extract text from images to supplement text descriptions, utilizing pic2vec to convert images into vectors and train an SVM, using captionbot.ai to generate natural language descriptions, and object detection and labeling with a voting technique. Our image-based techniques show improved classification accuracy of up to 96% for some classes compared to our base text-based SVM app classifier when app images are added.",1
"We propose a new contextual-compositional neural network layer that handles out-of-vocabulary (OOV) words in natural language processing (NLP) tagging tasks. This layer consists of a model that attends to both the character sequence and the context in which the OOV words appear. We show that our model learns to generate task-specific \textit{and} sentence-dependent OOV word representations without the need for pre-training on an embedding table, unlike previous attempts. We insert our layer in the state-of-the-art tagging model of \citet{plank2016multilingual} and thoroughly evaluate its contribution on 23 different languages on the task of jointly tagging part-of-speech and morphosyntactic attributes. Our OOV handling method successfully improves performances of this model on every language but one to achieve a new state-of-the-art on the Universal Dependencies Dataset 1.4.",0
"Our proposition introduces a novel contextual-compositional neural network layer that addresses the issue of out-of-vocabulary (OOV) words in natural language processing (NLP) tagging tasks. This layer comprises a model that takes into account both the character sequence and the context in which OOV words are found. We demonstrate that our model can create task-specific and sentence-dependent OOV word representations without requiring pre-training on an embedding table, as opposed to prior attempts. To evaluate the effectiveness of our approach, we integrate our layer into the state-of-the-art tagging model of Plank et al. (2016) and carefully assess its impact on 23 different languages in terms of tagging part-of-speech and morphosyntactic attributes. Our OOV handling method effectively enhances the performance of this model on all languages except one, resulting in a new state-of-the-art on the Universal Dependencies Dataset 1.4.",1
"Our research focuses on studying and developing methods for reducing the dimensionality of large datasets, common in biomedical applications. A major problem when learning information about patients based on genetic sequencing data is that there are often more feature variables (genetic data) than observations (patients). This makes direct supervised learning difficult. One way of reducing the feature space is to use latent Dirichlet allocation in order to group genetic variants in an unsupervised manner. Latent Dirichlet allocation is a common model in natural language processing, which describes a document as a mixture of topics, each with a probability of generating certain words. This can be generalized as a Bayesian tensor decomposition to account for multiple feature variables. While we made some progress improving and modifying these methods, our significant contributions are with hierarchical topic modeling. We developed distinct methods of incorporating hierarchical topic modeling, based on nested Chinese restaurant processes and Pachinko Allocation Machine, into Bayesian tensor decompositions. We apply these models to predict whether or not patients have autism spectrum disorder based on genetic sequencing data. We examine a dataset from National Database for Autism Research consisting of paired siblings -- one with autism, and the other without -- and counts of their genetic variants. Additionally, we linked the genes with their Reactome biological pathways. We combine this information into a tensor of patients, counts of their genetic variants, and the membership of these genes in pathways. Once we decompose this tensor, we use logistic regression on the reduced features in order to predict if patients have autism. We also perform a similar analysis of a dataset of patients with one of four common types of cancer (breast, lung, prostate, and colorectal).",0
"Our research aims to investigate and develop techniques for decreasing the dimensionality of voluminous datasets that are prevalent in biomedical fields. An obstacle when gathering insights about patients through genetic sequencing data is the abundance of feature variables (genetic data) in comparison to observations (patients), which impedes direct supervised learning. One solution for reducing the feature space is to utilize latent Dirichlet allocation, a commonly used model in natural language processing, to group genetic variants in an unsupervised fashion. This approach can be extended to a Bayesian tensor decomposition to account for multiple feature variables. We have made improvements to these methods, but our significant contributions lie in our development of hierarchical topic modeling using nested Chinese restaurant processes and Pachinko Allocation Machine, which we incorporate into Bayesian tensor decompositions. Our models are applied to predict autism spectrum disorder using genetic sequencing data from a National Database for Autism Research dataset that includes paired siblings, one with autism and one without, and their genetic variant counts. We also link the genes with their Reactome biological pathways and combine this information into a tensor of patients, genetic variant counts, and gene pathway membership. After decomposing this tensor, we utilize logistic regression on the reduced features to predict autism in patients. We also perform a similar analysis on a dataset of patients with breast, lung, prostate, or colorectal cancer.",1
"We propose a scalable Bayesian preference learning method for jointly predicting the preferences of individuals as well as the consensus of a crowd from pairwise labels. Peoples' opinions often differ greatly, making it difficult to predict their preferences from small amounts of personal data. Individual biases also make it harder to infer the consensus of a crowd when there are few labels per item. We address these challenges by combining matrix factorisation with Gaussian processes, using a Bayesian approach to account for uncertainty arising from noisy and sparse data. Our method exploits input features, such as text embeddings and user metadata, to predict preferences for new items and users that are not in the training set. As previous solutions based on Gaussian processes do not scale to large numbers of users, items or pairwise labels, we propose a stochastic variational inference approach that limits computational and memory costs. Our experiments on a recommendation task show that our method is competitive with previous approaches despite our scalable inference approximation. We demonstrate the method's scalability on a natural language processing task with thousands of users and items, and show improvements over the state of the art on this task. We make our software publicly available for future work.",0
"Our proposed method utilizes Bayesian preference learning that can scale to predict both individual preferences and crowd consensus from pairwise labels. It is challenging to predict individual preferences accurately with limited personal data as people's opinions vary widely. Moreover, inferring crowd consensus with few labels per item becomes more difficult due to individual biases. To overcome these issues, we combine matrix factorization and Gaussian processes, employing a Bayesian approach to account for uncertainties from sparse and noisy data. Our method incorporates input features such as text embeddings and user metadata to predict preferences for new users and items not in the training set. As existing Gaussian process solutions cannot handle large numbers of pairwise labels, users, or items, we propose a stochastic variational inference approach that reduces computational and memory costs. Our experiments demonstrate that despite the scalable inference approximation, our method outperforms previous solutions in the recommendation task. We also demonstrate its scalability in a natural language processing task with thousands of users and items. We provide our software publicly for future research.",1
"A key requirement for supervised machine learning is labeled training data, which is created by annotating unlabeled data with the appropriate class. Because this process can in many cases not be done by machines, labeling needs to be performed by human domain experts. This process tends to be expensive both in time and money, and is prone to errors. Additionally, reviewing an entire labeled dataset manually is often prohibitively costly, so many real world datasets contain mislabeled instances.   To address this issue, we present in this paper a non-parametric end-to-end pipeline to find mislabeled instances in numerical, image and natural language datasets. We evaluate our system quantitatively by adding a small number of label noise to 29 datasets, and show that we find mislabeled instances with an average precision of more than 0.84 when reviewing our system's top 1\% recommendation. We then apply our system to publicly available datasets and find mislabeled instances in CIFAR-100, Fashion-MNIST, and others. Finally, we publish the code and an applicable implementation of our approach.",0
"Supervised machine learning relies on labeled training data, which involves annotating unlabeled data with the corresponding class. Human domain experts are usually required for this task as it cannot be automated. However, labeling can be expensive in terms of both time and money and may contain errors. Mislabeling issues can arise in real-world datasets, and manual review of the entire dataset can be too costly. To address this problem, we introduce a non-parametric pipeline that can identify mislabeled instances in numerical, image, and natural language datasets. Our system was evaluated quantitatively by introducing label noise to 29 datasets, and we found mislabeled instances with an average precision of over 0.84 when reviewing the top 1% recommendation. We also applied our method to publicly available datasets such as CIFAR-100 and Fashion-MNIST. Lastly, we share the code and an applicable implementation of our approach.",1
"Recent advancements in language representation models such as BERT have led to a rapid improvement in numerous natural language processing tasks. However, language models usually consist of a few hundred million trainable parameters with embedding space distributed across multiple layers, thus making them challenging to be fine-tuned for a specific task or to be transferred to a new domain. To determine whether there are task-specific neurons that can be exploited for unsupervised transfer learning, we introduce a method for selecting the most important neurons to solve a specific classification task. This algorithm is further extended to multi-source transfer learning by computing the importance of neurons for several single-source transfer learning scenarios between different subsets of data sources. Besides, a task-specific fingerprint for each data source is obtained based on the percentage of the selected neurons in each layer. We perform extensive experiments in unsupervised transfer learning for sentiment analysis, natural language inference and sentence similarity, and compare our results with the existing literature and baselines. Significantly, we found that the source and target data sources with higher degrees of similarity between their task-specific fingerprints demonstrate a better transferability property. We conclude that our method can lead to better performance using just a few hundred task-specific and interpretable neurons.",0
"The improvement in natural language processing tasks has been rapid due to recent advancements in language representation models such as BERT. However, these models are challenging to fine-tune for a specific task or to transfer to a new domain because they consist of numerous trainable parameters distributed across multiple layers. To explore the possibility of unsupervised transfer learning using task-specific neurons, we have developed an algorithm for selecting the most important neurons for a particular classification task. This algorithm has also been extended to multi-source transfer learning scenarios by computing the importance of neurons for several single-source transfer learning cases between different data sources subsets. Additionally, we have obtained a task-specific fingerprint for each data source based on the percentage of the selected neurons in each layer. Our experiments on unsupervised transfer learning for sentiment analysis, natural language inference, and sentence similarity have shown promising results, and we have observed that the transferability property is enhanced when the source and target data sources have more similarities in their task-specific fingerprints. In conclusion, our method can lead to better performance using a few hundred task-specific and interpretable neurons.",1
"Recent times have witnessed sharp improvements in reinforcement learning tasks using deep reinforcement learning techniques like Deep Q Networks, Policy Gradients, Actor Critic methods which are based on deep learning based models and back-propagation of gradients to train such models. An active area of research in reinforcement learning is about training agents to play complex video games, which so far has been something accomplished only by human intelligence. Some state of the art performances in video game playing using deep reinforcement learning are obtained by processing the sequence of frames from video games, passing them through a convolutional network to obtain features and then using recurrent neural networks to figure out the action leading to optimal rewards. The recurrent neural network will learn to extract the meaningful signal out of the sequence of such features. In this work, we propose a method utilizing a transformer network which have recently replaced RNNs in Natural Language Processing (NLP), and perform experiments to compare with existing methods.",0
"Advanced progress has been made in reinforcement learning tasks through the use of deep reinforcement learning techniques, including Deep Q Networks, Policy Gradients, and Actor Critic methods. These methods rely on deep learning models and back-propagation of gradients to train them. A current area of interest in reinforcement learning is training agents to play complex video games, an achievement previously only accomplished by human intelligence. Recent progress in this area has been made by processing frames from video games through a convolutional network to derive features and using recurrent neural networks to determine the optimal action for maximum rewards. The recurrent neural network extracts meaningful signals from the feature sequence. Our proposed method utilizes a transformer network, which has replaced RNNs in NLP, and we conduct experiments to compare it with existing methods.",1
"Typical active learning strategies are designed for tasks, such as classification, with the assumption that the output space is mutually exclusive. The assumption that these tasks always have exactly one correct answer has resulted in the creation of numerous uncertainty-based measurements, such as entropy and least confidence, which operate over a model's outputs. Unfortunately, many real-world vision tasks, like visual question answering and image captioning, have multiple correct answers, causing these measurements to overestimate uncertainty and sometimes perform worse than a random sampling baseline. In this paper, we propose a new paradigm that estimates uncertainty in the model's internal hidden space instead of the model's output space. We specifically study a manifestation of this problem for visual question answer generation (VQA), where the aim is not to classify the correct answer but to produce a natural language answer, given an image and a question. Our method overcomes the paraphrastic nature of language. It requires a semantic space that structures the model's output concepts and that enables the usage of techniques like dropout-based Bayesian uncertainty. We build a visual-semantic space that embeds paraphrases close together for any existing VQA model. We empirically show state-of-art active learning results on the task of VQA on two datasets, being 5 times more cost-efficient on Visual Genome and 3 times more cost-efficient on VQA 2.0.",0
"Active learning strategies are commonly used for tasks like classification, assuming that the output space is mutually exclusive and has only one correct answer. This has led to the development of uncertainty-based measurements, such as entropy and least confidence, which operate over a model's outputs. However, real-world vision tasks like visual question answering and image captioning often have multiple correct answers, making these measurements less effective and sometimes even worse than random sampling. In this study, we propose a new approach that estimates uncertainty in the model's internal hidden space instead of the output space. Specifically, we focus on visual question answer generation (VQA), where the goal is to produce a natural language answer given an image and a question. Our method overcomes the challenges of paraphrastic language by utilizing a semantic space that structures the model's output concepts and allows for techniques like dropout-based Bayesian uncertainty. We have created a visual-semantic space that embeds paraphrases close together for any existing VQA model. Our approach has demonstrated state-of-the-art active learning results on the VQA task, being 5 times more cost-efficient on Visual Genome and 3 times more cost-efficient on VQA 2.0.",1
"Searching persons in large-scale image databases with the query of natural language description is a more practical important applications in video surveillance. Intuitively, for person search, the core issue should be visual-textual association, which is still an extremely challenging task, due to the contradiction between the high abstraction of textual description and the intuitive expression of visual images. However, for this task, while positive image-text pairs are always well provided, most existing methods doesn't tackle this problem effectively by mining more reasonable negative pairs. In this paper, we proposed a novel visual-textual association approach with visual and textual attention, and cross-modality hardest and semi-hard negative pair mining. In order to evaluate the effectiveness and feasibility of the proposed approach, we conduct extensive experiments on typical person search datasdet: CUHK-PEDES, in which our approach achieves the top1 score of 55.32% as a new state-of-the-art. Besides, we also evaluate the semi-hard pair mining approach in COCO caption dataset, and validate the effectiveness and complementarity of the methods.",0
"An important practical application in video surveillance is searching for individuals in large image databases using natural language descriptions. The main challenge in person search is the visual-textual association, which remains a difficult task due to the discrepancy between the high level of abstraction in textual descriptions and the intuitive expression of visual images. While positive image-text pairs are readily available, most existing methods do not effectively address the problem by mining more appropriate negative pairs. In this study, we propose a novel approach using visual and textual attention, and cross-modality hardest and semi-hard negative pair mining to improve visual-textual association. We evaluate our approach on CUHK-PEDES dataset, achieving a top1 score of 55.32%, which is a new state-of-the-art. We also validate the effectiveness and complementarity of the method on COCO caption dataset using the semi-hard pair mining approach.",1
"Temporal grounding entails establishing a correspondence between natural language event descriptions and their visual depictions. Compositional modeling becomes central: we first ground atomic descriptions ""girl eating an apple,"" ""batter hitting the ball"" to short video segments, and then establish the temporal relationships between the segments. This compositional structure enables models to recognize a wider variety of events not seen during training through recognizing their atomic sub-events. Explicit temporal modeling accounts for a wide variety of temporal relationships that can be expressed in language: e.g., in the description ""girl stands up from the table after eating an apple"" the visual ordering of the events is reversed, with first ""eating an apple"" followed by ""standing up from the table."" We leverage these observations to develop a unified deep architecture, CTG-Net, to perform temporal grounding of natural language event descriptions to videos. We demonstrate that our system outperforms prior state-of-the-art methods on the DiDeMo, Tempo-TL, and Tempo-HL temporal grounding datasets.",0
"Temporal grounding involves establishing a correlation between natural language event depictions and their visual representations. In this process, compositional modeling plays a crucial role. Initially, we ground simple descriptions like ""girl eating an apple"" and ""batter hitting the ball"" to brief video segments. Next, we establish a temporal relationship between these segments. This approach enables the models to recognize a broader range of events, including those not encountered during training, by identifying their fundamental sub-events. Explicit temporal modeling incorporates various temporal relationships that can be expressed in language, such as reversing the visual order of events in the sentence ""girl stands up from the table after eating an apple."" We utilize these insights to develop a CTG-Net deep architecture, which performs temporal grounding of natural language event depictions to videos. Our system surpasses previous state-of-the-art methods on the DiDeMo, Tempo-TL, and Tempo-HL temporal grounding datasets.",1
"Recurrent neural networks (RNNs) are a widely used tool for modeling sequential data, yet they are often treated as inscrutable black boxes. Given a trained recurrent network, we would like to reverse engineer it--to obtain a quantitative, interpretable description of how it solves a particular task. Even for simple tasks, a detailed understanding of how recurrent networks work, or a prescription for how to develop such an understanding, remains elusive. In this work, we use tools from dynamical systems analysis to reverse engineer recurrent networks trained to perform sentiment classification, a foundational natural language processing task. Given a trained network, we find fixed points of the recurrent dynamics and linearize the nonlinear system around these fixed points. Despite their theoretical capacity to implement complex, high-dimensional computations, we find that trained networks converge to highly interpretable, low-dimensional representations. In particular, the topological structure of the fixed points and corresponding linearized dynamics reveal an approximate line attractor within the RNN, which we can use to quantitatively understand how the RNN solves the sentiment analysis task. Finally, we find this mechanism present across RNN architectures (including LSTMs, GRUs, and vanilla RNNs) trained on multiple datasets, suggesting that our findings are not unique to a particular architecture or dataset. Overall, these results demonstrate that surprisingly universal and human interpretable computations can arise across a range of recurrent networks.",0
"Recurrent neural networks (RNNs) are commonly employed for modeling sequential data, however, they are often considered as impenetrable black boxes. The goal is to reverse engineer a trained RNN in order to attain a quantitative and comprehensible explanation of how it accomplishes a specific task. Even for basic tasks, a solid comprehension of RNN functioning or a method to acquire such knowledge is yet to be discovered. In this study, we have utilized dynamical systems analysis tools to reverse engineer RNNs that have been trained for sentiment classification, an essential natural language processing task. We have identified fixed points of the recurrent dynamics and have linearized the nonlinear system surrounding these fixed points. Despite the theoretical ability to execute intricate and high-dimensional computations, the trained networks converge to easily interpretable, low-dimensional representations. Specifically, the topology of the fixed points and the corresponding linearized dynamics reveal an approximate line attractor within the RNN, which can be employed for a quantitative understanding of how the RNN accomplishes the sentiment analysis task. Furthermore, this mechanism is present in various RNN architectures, including LSTMs, GRUs, and vanilla RNNs, trained on multiple datasets, suggesting that our findings are not restricted to a specific architecture or dataset. In conclusion, these results demonstrate the emergence of surprisingly universal and understandable computations across an array of recurrent networks.",1
"We present a first attempt to elucidate a theoretical and empirical approach to design the reward provided by a natural language environment to some structure learning agent. To this end, we revisit the Information Theory of unsupervised induction of phrase-structure grammars to characterize the behavior of simulated actions modeled as set-valued random variables (random sets of linguistic samples) constituting semantic structures. Our results showed empirical evidence of that simulated semantic structures (Open Information Extraction triplets) can be distinguished from randomly constructed ones by observing the Mutual Information among their constituents. This suggests the possibility of rewarding structure learning agents without using pretrained structural analyzers (oracle actors/experts).",0
"Our study aims to develop a theoretical and practical approach for designing rewards in a natural language environment for structure learning agents. We achieve this by examining the Information Theory of unsupervised induction of phrase-structure grammars to analyze the behavior of simulated actions, which are modeled as set-valued random variables comprising semantic structures. Our findings indicate that semantic structures produced by simulated actions (Open Information Extraction triplets) can be distinguished from randomly generated ones through Mutual Information analysis. This discovery suggests that structure learning agents can be rewarded without relying on pretrained structural analyzers (oracle actors/experts).",1
"Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this survey, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art graph neural networks into four categories, namely recurrent graph neural networks, convolutional graph neural networks, graph autoencoders, and spatial-temporal graph neural networks. We further discuss the applications of graph neural networks across various domains and summarize the open source codes, benchmark data sets, and model evaluation of graph neural networks. Finally, we propose potential research directions in this rapidly growing field.",0
"Recent years have seen a revolutionary impact of deep learning on various machine learning tasks, including image classification, video processing, speech recognition, and natural language understanding. However, the increasing number of applications generating data from non-Euclidean domains has led to representation of complex relationships and interdependencies between objects using graph data. This complexity has posed significant challenges to existing machine learning algorithms. Consequently, many studies have emerged that extend deep learning approaches to graph data. This survey presents a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. A new taxonomy divides state-of-the-art GNNs into four categories, namely recurrent graph neural networks, convolutional graph neural networks, graph autoencoders, and spatial-temporal graph neural networks. Applications of GNNs across various domains are discussed, and open source codes, benchmark data sets, and model evaluation of GNNs are summarized. Finally, potential research directions in this rapidly growing field are proposed.",1
"In recent years, deep-learning-based visual object trackers have been studied thoroughly, but handling occlusions and/or rapid motion of the target remains challenging. In this work, we argue that conditioning on the natural language (NL) description of a target provides information for longer-term invariance, and thus helps cope with typical tracking challenges. However, deriving a formulation to combine the strengths of appearance-based tracking with the language modality is not straightforward. We propose a novel deep tracking-by-detection formulation that can take advantage of NL descriptions. Regions that are related to the given NL description are generated by a proposal network during the detection phase of the tracker. Our LSTM based tracker then predicts the update of the target from regions proposed by the NL based detection phase. In benchmarks, our method is competitive with state of the art trackers, while it outperforms all other trackers on targets with unambiguous and precise language annotations. It also beats the state-of-the-art NL tracker when initializing without a bounding box. Our method runs at over 30 fps on a single GPU.",0
"Although deep-learning-based visual object trackers have been extensively researched in recent years, they still struggle with handling occlusions and rapid motion of the target. To address this issue, we propose conditioning on the natural language (NL) description of a target to provide information for longer-term invariance, which can help overcome typical tracking challenges. However, combining the strengths of appearance-based tracking with the language modality is not a straightforward task. To address this challenge, we present a novel deep tracking-by-detection formulation that can utilize NL descriptions. During the detection phase of the tracker, a proposal network generates regions related to the given NL description. Our LSTM based tracker then predicts the target update from the regions proposed by the NL based detection phase. Our method is competitive with state of the art trackers in benchmarks, and outperforms all other trackers on targets with unambiguous and precise language annotations. Additionally, our method beats the state-of-the-art NL tracker when initializing without a bounding box. It runs at over 30 fps on a single GPU.",1
"A new prior is proposed for learning representations of high-level concepts of the kind we manipulate with language. This prior can be combined with other priors in order to help disentangling abstract factors from each other. It is inspired by cognitive neuroscience theories of consciousness, seen as a bottleneck through which just a few elements, after having been selected by attention from a broader pool, are then broadcast and condition further processing, both in perception and decision-making. The set of recently selected elements one becomes aware of is seen as forming a low-dimensional conscious state. This conscious state is combining the few concepts constituting a conscious thought, i.e., what one is immediately conscious of at a particular moment. We claim that this architectural and information-processing constraint corresponds to assumptions about the joint distribution between high-level concepts. To the extent that these assumptions are generally true (and the form of natural language seems consistent with them), they can form a useful prior for representation learning. A low-dimensional thought or conscious state is analogous to a sentence: it involves only a few variables and yet can make a statement with very high probability of being true. This is consistent with a joint distribution (over high-level concepts) which has the form of a sparse factor graph, i.e., where the dependencies captured by each factor of the factor graph involve only very few variables while creating a strong dip in the overall energy function. The consciousness prior also makes it natural to map conscious states to natural language utterances or to express classical AI knowledge in a form similar to facts and rules, albeit capturing uncertainty as well as efficient search mechanisms implemented by attention mechanisms.",0
"The article presents a novel approach to learn high-level concept representations that are utilized in language processing. The proposed prior aids in distinguishing abstract factors by combining with other priors. It draws inspiration from cognitive neuroscience theories of consciousness, which suggest that a few selected elements are broadcast and condition further processing in perception and decision making. The conscious state is regarded as a low-dimensional state formed by recently selected elements, which combine to create a conscious thought. The article posits that this constraint corresponds to the joint distribution between high-level concepts, making it a useful prior for representation learning. The consciousness prior is similar to a sentence, involving only a few variables, yet with a high probability of being true. The joint distribution has the form of a sparse factor graph, with dependencies captured by each factor involving a few variables, creating a strong dip in the overall energy function. The consciousness prior also facilitates mapping conscious states to natural language expressions or classical AI knowledge, similar to facts and rules, with uncertainty and efficient search mechanisms implemented by attention mechanisms.",1
"Sequential modelling with self-attention has achieved cutting edge performances in natural language processing. With advantages in model flexibility, computation complexity and interpretability, self-attention is gradually becoming a key component in event sequence models. However, like most other sequence models, self-attention does not account for the time span between events and thus captures sequential signals rather than temporal patterns. Without relying on recurrent network structures, self-attention recognizes event orderings via positional encoding. To bridge the gap between modelling time-independent and time-dependent event sequence, we introduce a functional feature map that embeds time span into high-dimensional spaces. By constructing the associated translation-invariant time kernel function, we reveal the functional forms of the feature map under classic functional function analysis results, namely Bochner's Theorem and Mercer's Theorem. We propose several models to learn the functional time representation and the interactions with event representation. These methods are evaluated on real-world datasets under various continuous-time event sequence prediction tasks. The experiments reveal that the proposed methods compare favorably to baseline models while also capturing useful time-event interactions.",0
"Self-attention has emerged as a powerful tool in natural language processing, especially when combined with sequential modelling. Its benefits include increased model flexibility, reduced computation complexity, and improved interpretability, making it a popular component in event sequence models. However, self-attention fails to account for the duration between events, limiting its ability to capture temporal patterns. Rather than relying on recurrent network structures, self-attention uses positional encoding to recognize event orderings. To address this limitation, we introduce a functional feature map that embeds time span into high-dimensional spaces. By constructing a translation-invariant time kernel function, we reveal the functional forms of the feature map using classic functional function analysis results such as Bochner's Theorem and Mercer's Theorem. We propose several models to learn the functional time representation and its interactions with event representation, which we evaluate on real-world datasets for various continuous-time event sequence prediction tasks. Our experiments show that our proposed methods outperform baseline models and capture useful time-event interactions.",1
"A code generation system generates programming language code based on an input natural language description. State-of-the-art approaches rely on neural networks for code generation. However, these code generators suffer from two problems. One is the long dependency problem, where a code element often depends on another far-away code element. A variable reference, for example, depends on its definition, which may appear quite a few lines before. The other problem is structure modeling, as programs contain rich structural information. In this paper, we propose a novel tree-based neural architecture, TreeGen, for code generation. TreeGen uses the attention mechanism of Transformers to alleviate the long-dependency problem, and introduces a novel AST reader (encoder) to incorporate grammar rules and AST structures into the network. We evaluated TreeGen on a Python benchmark, HearthStone, and two semantic parsing benchmarks, ATIS and GEO. TreeGen outperformed the previous state-of-the-art approach by 4.5 percentage points on HearthStone, and achieved the best accuracy among neural network-based approaches on ATIS (89.1%) and GEO (89.6%). We also conducted an ablation test to better understand each component of our model.",0
"The generation of programming language code from natural language descriptions is facilitated by a code generation system. While neural networks are commonly employed in current approaches, they face two challenges. The first is the long dependency problem, where a code element may depend on another that is situated far away. For instance, a variable reference depends on its definition, which may appear several lines ahead. The second issue is structure modeling, as programs contain abundant structural information. In this study, we introduce TreeGen, a novel tree-based neural architecture for code generation. TreeGen employs Transformers' attention mechanism to mitigate the long-dependency problem and introduces a unique AST reader (encoder) to incorporate grammar rules and AST structures into the network. We assessed TreeGen on HearthStone, a Python benchmark, as well as two semantic parsing benchmarks, ATIS and GEO. Compared to the previous state-of-the-art approach, TreeGen outperformed by 4.5 percentage points on HearthStone and achieved the highest accuracy among neural network-based approaches on ATIS (89.1%) and GEO (89.6%). We also conducted an ablation test to gain a better understanding of each component of our model.",1
"In this paper we present a new framework for time-series modeling that combines the best of traditional statistical models and neural networks. We focus on time-series with long-range dependencies, needed for monitoring fine granularity data (e.g. minutes, seconds, milliseconds), prevalent in operational use-cases.   Traditional models, such as auto-regression fitted with least squares (Classic-AR) can model time-series with a concise and interpretable model. When dealing with long-range dependencies, Classic-AR models can become intractably slow to fit for large data. Recently, sequence-to-sequence models, such as Recurrent Neural Networks, which were originally intended for natural language processing, have become popular for time-series. However, they can be overly complex for typical time-series data and lack interpretability.   A scalable and interpretable model is needed to bridge the statistical and deep learning-based approaches. As a first step towards this goal, we propose modelling AR-process dynamics using a feed-forward neural network approach, termed AR-Net. We show that AR-Net is as interpretable as Classic-AR but also scales to long-range dependencies.   Our results lead to three major conclusions: First, AR-Net learns identical AR-coefficients as Classic-AR, thus being equally interpretable. Second, the computational complexity with respect to the order of the AR process, is linear for AR-Net as compared to a quadratic for Classic-AR. This makes it possible to model long-range dependencies within fine granularity data. Third, by introducing regularization, AR-Net automatically selects and learns sparse AR-coefficients. This eliminates the need to know the exact order of the AR-process and allows to learn sparse weights for a model with long-range dependencies.",0
"A novel framework for time-series modeling is presented in this study, which combines the benefits of both traditional statistical models and neural networks. The focus is on time-series data with long-range dependencies, such as those found in operational scenarios with fine granularity (e.g., seconds, minutes, milliseconds). While traditional models like Classic-AR are precise and easy to interpret, they can become computationally infeasible for large data sets with long-range dependencies. Conversely, while deep learning models like Recurrent Neural Networks (RNNs) can handle long-range dependencies, they are often too complex and lack interpretability. Therefore, a scalable and interpretable model that bridges statistical and deep learning-based approaches is required. The proposed solution is an AR-Net model, which uses a feed-forward neural network approach to model AR-process dynamics. The results show that AR-Net is as interpretable as Classic-AR, but it can scale to long-range dependencies. Additionally, AR-Net automatically selects and learns sparse AR-coefficients by introducing regularization, eliminating the need to know the exact order of the AR-process. This allows for learning sparse weights for a model with long-range dependencies. The study's findings lead to three key conclusions: AR-Net learns identical AR-coefficients as Classic-AR, has linear computational complexity with respect to the order of the AR process, and can model long-range dependencies within fine granularity data.",1
"Deep latent variable models (LVM) such as variational auto-encoder (VAE) have recently played an important role in text generation. One key factor is the exploitation of smooth latent structures to guide the generation. However, the representation power of VAEs is limited due to two reasons: (1) the Gaussian assumption is often made on the variational posteriors; and meanwhile (2) a notorious ""posterior collapse"" issue occurs. In this paper, we advocate sample-based representations of variational distributions for natural language, leading to implicit latent features, which can provide flexible representation power compared with Gaussian-based posteriors. We further develop an LVM to directly match the aggregated posterior to the prior. It can be viewed as a natural extension of VAEs with a regularization of maximizing mutual information, mitigating the ""posterior collapse"" issue. We demonstrate the effectiveness and versatility of our models in various text generation scenarios, including language modeling, unaligned style transfer, and dialog response generation. The source code to reproduce our experimental results is available on GitHub.",0
"Text generation has been significantly influenced by deep latent variable models like variational auto-encoder (VAE). These models exploit smooth latent structures to guide the generation process. However, VAEs have limited representation power due to the Gaussian assumption made on variational posteriors and the notorious ""posterior collapse"" issue. To address these limitations, this paper proposes sample-based representations of variational distributions for natural language. These representations provide more flexible representation power compared to Gaussian-based posteriors. The paper also introduces an LVM that matches the aggregated posterior to the prior and maximizes mutual information to mitigate the ""posterior collapse"" issue. The effectiveness and versatility of these models are demonstrated in various text generation scenarios, and the source code is available on GitHub for reproducibility.",1
"Generating animations from natural language sentences finds its applications in a a number of domains such as movie script visualization, virtual human animation and, robot motion planning. These sentences can describe different kinds of actions, speeds and direction of these actions, and possibly a target destination. The core modeling challenge in this language-to-pose application is how to map linguistic concepts to motion animations.   In this paper, we address this multimodal problem by introducing a neural architecture called Joint Language to Pose (or JL2P), which learns a joint embedding of language and pose. This joint embedding space is learned end-to-end using a curriculum learning approach which emphasizes shorter and easier sequences first before moving to longer and harder ones. We evaluate our proposed model on a publicly available corpus of 3D pose data and human-annotated sentences. Both objective metrics and human judgment evaluation confirm that our proposed approach is able to generate more accurate animations and are deemed visually more representative by humans than other data driven approaches.",0
"The use of natural language sentences to create animations has diverse applications, including visualizing movie scripts, animating virtual humans, and planning robot motion. Such sentences can convey information on the type of actions, their speed and direction, and a possible destination. The main challenge in this language-to-pose process is mapping linguistic concepts to motion animations. This paper proposes a solution to this multimodal problem through a neural architecture called Joint Language to Pose (JL2P), which learns a joint embedding of language and pose. The curriculum learning approach prioritizes shorter and easier sequences before tackling longer and more challenging ones. Our proposed model is evaluated on a publicly available corpus of 3D pose data and human-annotated sentences using objective metrics and human judgment evaluation. The results demonstrate that our approach generates more accurate animations and is deemed visually more representative by humans than other data-driven approaches.",1
"Learning powerful data embeddings has become a center piece in machine learning, especially in natural language processing and computer vision domains. The crux of these embeddings is that they are pretrained on huge corpus of data in a unsupervised fashion, sometimes aided with transfer learning. However currently in the graph learning domain, embeddings learned through existing graph neural networks (GNNs) are task dependent and thus cannot be shared across different datasets. In this paper, we present a first powerful and theoretically guaranteed graph neural network that is designed to learn task-independent graph embeddings, thereafter referred to as deep universal graph embedding (DUGNN). Our DUGNN model incorporates a novel graph neural network (as a universal graph encoder) and leverages rich Graph Kernels (as a multi-task graph decoder) for both unsupervised learning and (task-specific) adaptive supervised learning. By learning task-independent graph embeddings across diverse datasets, DUGNN also reaps the benefits of transfer learning. Through extensive experiments and ablation studies, we show that the proposed DUGNN model consistently outperforms both the existing state-of-art GNN models and Graph Kernels by an increased accuracy of 3% - 8% on graph classification benchmark datasets.",0
"Machine learning has placed great emphasis on developing effective data embeddings, especially in the fields of natural language processing and computer vision. These embeddings are typically trained on large amounts of data in an unsupervised manner, often utilizing transfer learning. However, current graph neural networks (GNNs) produce task-specific embeddings that cannot be applied to different datasets. This paper introduces the deep universal graph embedding (DUGNN), a novel GNN model that generates task-independent graph embeddings using a universal graph encoder and rich Graph Kernels as a multi-task graph decoder. DUGNN can learn unsupervised and task-specific adaptive supervised learning. By generating task-independent graph embeddings across multiple datasets, DUGNN leverages the benefits of transfer learning. The proposed DUGNN model consistently outperforms existing state-of-the-art GNN models and Graph Kernels by achieving an increased accuracy of 3% - 8% on graph classification benchmark datasets, as demonstrated through extensive experiments and ablation studies.",1
"Recent advances in artificial intelligence research have led to a profusion of studies that apply deep learning to problems in image analysis and natural language processing among others. Additionally, the availability of open-source computational frameworks has lowered the barriers to implementing state-of-the-art methods across multiple domains. Albeit leading to major performance breakthroughs in some tasks, effective dissemination of deep learning algorithms remains challenging, inhibiting reproducibility and benchmarking studies, impeding further validation, and ultimately hindering their effectiveness in the cumulative scientific progress. In developing a platform for sharing research outputs, we present ModelHub.AI (www.modelhub.ai), a community-driven container-based software engine and platform for the structured dissemination of deep learning models. For contributors, the engine controls data flow throughout the inference cycle, while the contributor-facing standard template exposes model-specific functions including inference, as well as pre- and post-processing. Python and RESTful Application programming interfaces (APIs) enable users to interact with models hosted on ModelHub.AI and allows both researchers and developers to utilize models out-of-the-box. ModelHub.AI is domain-, data-, and framework-agnostic, catering to different workflows and contributors' preferences.",0
"The development of artificial intelligence has led to an abundance of studies utilizing deep learning for image analysis and natural language processing, among other applications. The availability of open-source computational frameworks has made it easier to implement these cutting-edge methods across various fields. While deep learning has shown significant progress in certain tasks, the effective sharing of algorithms remains a challenge, hindering the ability to reproduce and benchmark studies, validate findings, and contribute to scientific progress. To address this issue, we have created ModelHub.AI, a community-driven software engine and platform for the structured dissemination of deep learning models. The platform uses container-based technology and standard templates to control data flow and expose model-specific functions to contributors. Users can interact with models hosted on ModelHub.AI through Python and RESTful APIs, making it accessible to both researchers and developers. The platform is designed to be flexible and adaptable to different workflows and preferences, making it framework-agnostic and accommodating to various domains and data types.",1
"Pre-trained word embeddings encode general word semantics and lexical regularities of natural language, and have proven useful across many NLP tasks, including word sense disambiguation, machine translation, and sentiment analysis, to name a few. In supervised tasks such as multiclass text classification (the focus of this article) it seems appealing to enhance word representations with ad-hoc embeddings that encode task-specific information. We propose (supervised) word-class embeddings (WCEs), and show that, when concatenated to (unsupervised) pre-trained word embeddings, they substantially facilitate the training of deep-learning models in multiclass classification by topic. We show empirical evidence that WCEs yield a consistent improvement in multiclass classification accuracy, using four popular neural architectures and six widely used and publicly available datasets for multiclass text classification. Our code that implements WCEs is publicly available at https://github.com/AlexMoreo/word-class-embeddings",0
"Word embeddings that are pre-trained capture the general semantics and regularities of language, and are useful for various NLP tasks such as machine translation, sentiment analysis, and word sense disambiguation. When it comes to supervised tasks like multiclass text classification, it is tempting to enhance word representations with ad-hoc embeddings that contain task-specific information. Our proposal is supervised word-class embeddings (WCEs), which, when combined with unsupervised pre-trained word embeddings, significantly improve deep-learning model training in multiclass classification by topic. We present empirical evidence that WCEs enhance multiclass classification accuracy across four popular neural architectures and six widely used datasets for multiclass text classification. Those interested in implementing WCEs can access our code at https://github.com/AlexMoreo/word-class-embeddings.",1
"Recurrent neural networks (RNN) have been successfully applied to various sequential decision-making tasks, natural language processing applications, and time-series predictions. Such networks are usually trained through back-propagation through time (BPTT) which is prohibitively expensive, especially when the length of the time dependencies and the number of hidden neurons increase. To reduce the training time, extreme learning machines (ELMs) have been recently applied to RNN training, reaching a 99\% speedup on some applications. Due to its non-iterative nature, ELM training, when parallelized, has the potential to reach higher speedups than BPTT.   In this work, we present \opt, an optimized parallel RNN training algorithm based on ELM that takes advantage of the GPU shared memory and of parallel QR factorization algorithms to efficiently reach optimal solutions. The theoretical analysis of the proposed algorithm is presented on six RNN architectures, including LSTM and GRU, and its performance is empirically tested on ten time-series prediction applications. \opt~is shown to reach up to 845 times speedup over its sequential counterpart and to require up to 20x less time to train than parallel BPTT.",0
"Sequential decision-making tasks, natural language processing applications, and time-series predictions have all benefited from the successful application of recurrent neural networks (RNNs). However, the standard training method, back-propagation through time (BPTT), becomes prohibitively expensive as the length of time dependencies and hidden neurons increase. To address this issue, recent research has explored the use of extreme learning machines (ELMs) for RNN training, which can offer a 99% speedup on certain applications. By leveraging the non-iterative nature of ELM training and parallelization, it is possible to achieve even higher speedups compared to BPTT. This paper introduces an optimized parallel RNN training algorithm called \opt, which takes advantage of GPU shared memory and parallel QR factorization algorithms to reach optimal solutions efficiently. The algorithm is theoretically analyzed using six RNN architectures, including LSTM and GRU, and is empirically tested on ten time-series prediction applications. Results show that \opt~can achieve up to 845 times speedup over its sequential counterpart and requires up to 20 times less time to train than parallel BPTT.",1
"How does one represent an action? How does one describe an action that we have never seen before? Such questions are addressed by the Zero Shot Learning paradigm, where a model is trained on only a subset of classes and is evaluated on its ability to correctly classify an example from a class it has never seen before. In this work, we present a body pose based zero shot action recognition network and demonstrate its performance on the NTU RGB-D dataset. Our model learns to jointly encapsulate visual similarities based on pose features of the action performer as well as similarities in the natural language descriptions of the unseen action class names. We demonstrate how this pose-language semantic space encodes knowledge which allows our model to correctly predict actions not seen during training.",0
"The Zero Shot Learning paradigm tackles the challenge of representing and describing unfamiliar actions. It involves training a model on a subset of classes and evaluating its ability to classify examples from new classes. Our research introduces a zero shot action recognition network based on body poses and tests its performance using the NTU RGB-D dataset. The model captures similarities in both the performer's pose features and natural language descriptions of unseen action classes. By establishing a pose-language semantic space, our model acquires knowledge that enables it to accurately predict actions that were not included in the training process.",1
"(Very early draft)Traditional supervised learning keeps pushing convolution neural network(CNN) achieving state-of-art performance. However, lack of large-scale annotation data is always a big problem due to the high cost of it, even ImageNet dataset is over-fitted by complex models now. The success of unsupervised learning method represented by the Bert model in natural language processing(NLP) field shows its great potential. And it makes that unlimited training samples becomes possible and the great universal generalization ability changes NLP research direction directly. In this article, we purpose a novel unsupervised learning method based on contrastive predictive coding. Under that, we are able to train model with any non-annotation images and improve model's performance to reach state-of-art performance at the same level of model complexity. Beside that, since the number of training images could be unlimited amplification, an universal large-scale pre-trained computer vision model is possible in the future.",0
"The traditional approach to supervised learning has relied heavily on convolution neural networks (CNNs) to achieve impressive results. However, a significant issue in this method is the lack of large-scale annotated data due to the high cost of obtaining it. Even the ImageNet dataset has become saturated by complex models. In contrast, the success of unsupervised learning, as demonstrated by the Bert model in natural language processing (NLP), has revealed the potential of unlimited training samples and universal generalization ability. This has redirected NLP research in a new direction. This article proposes a novel unsupervised learning method based on contrastive predictive coding, which can train models with non-annotated images and improve their performance to reach state-of-the-art levels of complexity. Furthermore, with the possibility of unlimited training images, an universal, large-scale pre-trained computer vision model may be achievable in the future.",1
"The ability to infer the intentions of others, predict their goals, and deduce their plans are critical features for intelligent agents. For a long time, several approaches investigated the use of symbolic representations and inferences with limited success, principally because it is difficult to capture the cognitive knowledge behind human decisions explicitly. The trend, nowadays, is increasingly focusing on learning to infer intentions directly from data, using deep learning in particular. We are now observing interesting applications of intent classification in natural language processing, visual activity recognition, and emerging approaches in other domains. This paper discusses a novel approach combining few-shot and transfer learning with cross-domain features, to learn to infer the intent of an agent navigating in physical environments, executing arbitrary long sequences of actions to achieve their goals. Experiments in synthetic environments demonstrate improved performance in terms of learning from few samples and generalizing to unseen configurations, compared to a deep-learning baseline approach.",0
"Intelligent agents require the ability to understand and anticipate the intentions, goals, and plans of others. Past attempts to achieve this through symbolic representations and inferences have had limited success due to the difficulty of explicitly capturing human cognitive knowledge. Current trends are moving towards using deep learning to directly learn how to infer intentions from data. This approach has already shown promise in natural language processing and visual activity recognition, among other domains. In this paper, we propose a new approach that combines few-shot and transfer learning with cross-domain features to infer the intentions of an agent navigating physical environments and executing long sequences of actions. Our experiments in synthetic environments demonstrate improved performance in learning from limited samples and generalizing to new scenarios, compared to a baseline deep-learning approach.",1
"Contemporary deep learning based video captioning follows encoder-decoder framework. In encoder, visual features are extracted with 2D/3D Convolutional Neural Networks (CNNs) and a transformed version of those features is passed to the decoder. The decoder uses word embeddings and a language model to map visual features to natural language captions. Due to its composite nature, the encoder-decoder pipeline provides the freedom of multiple choices for each of its components, e.g the choices of CNNs models, feature transformations, word embeddings, and language models etc. Component selection can have drastic effects on the overall video captioning performance. However, current literature is void of any systematic investigation in this regard. This article fills this gap by providing the first thorough empirical analysis of the role that each major component plays in a contemporary video captioning pipeline. We perform extensive experiments by varying the constituent components of the video captioning framework, and quantify the performance gains that are possible by mere component selection. We use the popular MSVD dataset as the test-bed, and demonstrate that substantial performance gains are possible by careful selection of the constituent components without major changes to the pipeline itself. These results are expected to provide guiding principles for future research in the fast growing direction of video captioning.",0
"The current approach to video captioning using deep learning involves an encoder-decoder framework. The encoder uses 2D/3D Convolutional Neural Networks (CNNs) to extract visual features, which are then transformed and passed to the decoder. The decoder uses word embeddings and a language model to create natural language captions. As the encoder-decoder pipeline is composed of multiple components, such as CNN models, feature transformations, word embeddings, and language models, the selection of each component can greatly impact the video captioning performance. However, there is a lack of systematic investigation in current literature on this topic. This article addresses this gap by conducting empirical analysis on the role of each major component in contemporary video captioning. By varying these components and conducting extensive experiments on the MSVD dataset, we demonstrate that significant performance gains can be achieved through careful selection of these components without major changes to the pipeline itself. These findings provide valuable guidance for future research in the rapidly growing field of video captioning.",1
"Recent state-of-the-art natural language understanding models, such as BERT and XLNet, score a pair of sentences (A and B) using multiple cross-attention operations - a process in which each word in sentence A attends to all words in sentence B and vice versa. As a result, computing the similarity between a query sentence and a set of candidate sentences, requires the propagation of all query-candidate sentence-pairs throughout a stack of cross-attention layers. This exhaustive process becomes computationally prohibitive when the number of candidate sentences is large. In contrast, sentence embedding techniques learn a sentence-to-vector mapping and compute the similarity between the sentence vectors via simple elementary operations. In this paper, we introduce Distilled Sentence Embedding (DSE) - a model that is based on knowledge distillation from cross-attentive models, focusing on sentence-pair tasks. The outline of DSE is as follows: Given a cross-attentive teacher model (e.g. a fine-tuned BERT), we train a sentence embedding based student model to reconstruct the sentence-pair scores obtained by the teacher model. We empirically demonstrate the effectiveness of DSE on five GLUE sentence-pair tasks. DSE significantly outperforms several ELMO variants and other sentence embedding methods, while accelerating computation of the query-candidate sentence-pairs similarities by several orders of magnitude, with an average relative degradation of 4.6% compared to BERT. Furthermore, we show that DSE produces sentence embeddings that reach state-of-the-art performance on universal sentence representation benchmarks. Our code is made publicly available at https://github.com/microsoft/Distilled-Sentence-Embedding.",0
"State-of-the-art natural language understanding models, including BERT and XLNet, employ multiple cross-attention operations to score pairs of sentences. This involves each word in one sentence attending to all words in the other sentence, resulting in a computationally exhaustive process when comparing a query sentence to a large set of candidate sentences. In contrast, sentence embedding techniques use a simple mapping of sentences to vectors and compute similarity via basic operations. The Distilled Sentence Embedding (DSE) model introduced in this paper is based on knowledge distillation from cross-attentive models, with a focus on sentence-pair tasks. DSE trains a sentence embedding-based student model to reconstruct the scores obtained by a cross-attentive teacher model. Empirical results show that DSE outperforms several other sentence embedding methods, while significantly accelerating computation of query-candidate sentence-pair similarities. Additionally, DSE produces sentence embeddings that achieve state-of-the-art performance on universal sentence representation benchmarks. Code for DSE is publicly available at https://github.com/microsoft/Distilled-Sentence-Embedding.",1
"Humans excel in continuously learning with small data without forgetting how to solve old problems. However, neural networks require large datasets to compute latent representations across different tasks while minimizing a loss function. For example, a natural language understanding (NLU) system will often deal with emerging entities during its deployment as interactions with users in realistic scenarios will generate new and infrequent names, events, and locations. Here, we address this scenario by introducing an RL trainable controller that disentangles the representation learning of a neural encoder from its memory management role.   Our proposed solution is straightforward and simple: we train a controller to execute an optimal sequence of reading and writing operations on an external memory with the goal of leveraging diverse activations from the past and provide accurate predictions. Our approach is named Learning to Control (LTC) and allows few-shot learning with two degrees of memory plasticity. We experimentally show that our system obtains accurate results for few-shot learning of entity recognition in the Stanford Task-Oriented Dialogue dataset.",0
"While humans excel at learning continually with limited data without forgetting how to solve previous problems, neural networks require large datasets to compute latent representations for different tasks while minimizing loss. For instance, natural language understanding (NLU) systems must often deal with new and infrequent names, events, and locations that arise during real-life interactions with users. To address this, we propose a simple RL trainable controller that separates a neural encoder's representation learning from its memory management role. Our approach, called Learning to Control (LTC), trains a controller to execute an optimal sequence of reading and writing operations on an external memory to leverage diverse activations from the past and provide accurate predictions. With two degrees of memory plasticity, our LTC approach enables few-shot learning and achieves precise results for entity recognition in the Stanford Task-Oriented Dialogue dataset.",1
"Recent success suggests that deep neural control networks are likely to be a key component of self-driving vehicles. These networks are trained on large datasets to imitate human actions, but they lack semantic understanding of image contents. This makes them brittle and potentially unsafe in situations that do not match training data. Here, we propose to address this issue by augmenting training data with natural language advice from a human. Advice includes guidance about what to do and where to attend. We present the first step toward advice giving, where we train an end-to-end vehicle controller that accepts advice. The controller adapts the way it attends to the scene (visual attention) and the control (steering and speed). Attention mechanisms tie controller behavior to salient objects in the advice. We evaluate our model on a novel advisable driving dataset with manually annotated human-to-vehicle advice called Honda Research Institute-Advice Dataset (HAD). We show that taking advice improves the performance of the end-to-end network, while the network cues on a variety of visual features that are provided by advice. The dataset is available at https://usa.honda-ri.com/HAD.",0
"The recent achievements suggest that self-driving vehicles will likely depend on deep neural control networks as a crucial element. These networks are taught on extensive datasets to copy human actions, but they don't fully comprehend the meaning of the image contents. Consequently, they can be fragile and hazardous in situations that deviate from the training data. To address this issue, we suggest including natural language guidance from a human to the training data. This advice would offer direction on what actions to take and where to focus. We have taken the first step towards this by training a vehicle controller that accepts advice and adjusts its behavior accordingly. The controller uses visual attention mechanisms to connect its actions with salient objects in the advice. We tested our model on a new dataset, Honda Research Institute-Advice Dataset (HAD), which has human-to-vehicle advice annotations. Our results demonstrate that guidance enhances the performance of the end-to-end network and allows it to cue on various visual features provided by the advice. The dataset is available at https://usa.honda-ri.com/HAD.",1
"Chest radiography is an extremely powerful imaging modality, allowing for a detailed inspection of a patient's thorax, but requiring specialized training for proper interpretation. With the advent of high performance general purpose computer vision algorithms, the accurate automated analysis of chest radiographs is becoming increasingly of interest to researchers. However, a key challenge in the development of these techniques is the lack of sufficient data. Here we describe MIMIC-CXR-JPG v2.0.0, a large dataset of 377,110 chest x-rays associated with 227,827 imaging studies sourced from the Beth Israel Deaconess Medical Center between 2011 - 2016. Images are provided with 14 labels derived from two natural language processing tools applied to the corresponding free-text radiology reports. MIMIC-CXR-JPG is derived entirely from the MIMIC-CXR database, and aims to provide a convenient processed version of MIMIC-CXR, as well as to provide a standard reference for data splits and image labels. All images have been de-identified to protect patient privacy. The dataset is made freely available to facilitate and encourage a wide range of research in medical computer vision.",0
"Although chest radiography is a powerful tool for thorax examination, it requires specialized interpretation skills. Researchers are increasingly interested in using high-performance computer vision algorithms for accurate automated analysis of chest radiographs. However, a major issue in developing such techniques is the lack of adequate data. To address this, we present MIMIC-CXR-JPG v2.0.0, a massive dataset of 377,110 chest x-rays from 227,827 imaging studies conducted at Beth Israel Deaconess Medical Center between 2011 and 2016. The dataset includes 14 labels obtained from two natural language processing tools applied to the corresponding free-text radiology reports. MIMIC-CXR-JPG is a processed version of the MIMIC-CXR database and serves as a standard reference for data splits and image labels. All images are de-identified to protect patient privacy, and the dataset is freely available to encourage a wide range of research in medical computer vision.",1
"Although machine learning has become a powerful tool to augment doctors in clinical analysis, the immense amount of labeled data that is necessary to train supervised learning approaches burdens each development task as time and resource intensive. The vast majority of dense clinical information is stored in written reports, detailing pertinent patient information. The challenge with utilizing natural language data for standard model development is due to the complex nature of the modality. In this research, a model pipeline was developed to utilize an unsupervised approach to train an encoder-language model, a recurrent network, to generate document encodings; which then can be used as features passed into a decoder-classifier model that requires magnitudes less labeled data than previous approaches to differentiate between fine-grained disease classes accurately. The language model was trained on unlabeled radiology reports from the Massachusetts General Hospital Radiology Department (n=218,159) and terminated with a loss of 1.62. The classification models were trained on three labeled datasets of head CT studies of reported patients, presenting large vessel occlusion (n=1403), acute ischemic strokes (n=331), and intracranial hemorrhage (n=4350), to identify a variety of different findings directly from the radiology report data; resulting in AUCs of 0.98, 0.95, and 0.99, respectively, for the large vessel occlusion, acute ischemic stroke, and intracranial hemorrhage datasets. The output encodings are able to be used in conjunction with imaging data, to create models that can process a multitude of different modalities. The ability to automatically extract relevant features from textual data allows for faster model development and integration of textual modality, overall, allowing clinical reports to become a more viable input for more encompassing and accurate deep learning models.",0
"Although machine learning has proven valuable in assisting physicians with clinical analysis, the requirement for a vast quantity of labeled data to train supervised learning approaches is a time-consuming and resource-intensive task. Most clinical data is stored in written reports, which pose a challenge for natural language processing due to their complexity. As part of a study, an unsupervised approach was employed to train an encoder-language model, a recurrent network, on 218,159 unlabeled radiology reports from the Massachusetts General Hospital Radiology Department. The resulting model pipeline generated document encodings that were used as features in a decoder-classifier model to accurately differentiate between fine-grained disease classes. The classifier models were trained on three labeled datasets, resulting in AUCs of 0.98, 0.95, and 0.99 for large vessel occlusion, acute ischemic stroke, and intracranial hemorrhage, respectively. The output encodings can be used in combination with imaging data, enabling the creation of deep learning models capable of processing multiple modalities. This ability to automatically extract relevant features from textual data enhances the integration of textual modality, ultimately increasing the accuracy of deep learning models and making clinical reports a more valuable input.",1
"The significant progress on Generative Adversarial Networks (GANs) have made it possible to generate surprisingly realistic images for single object based on natural language descriptions. However, controlled generation of images for multiple entities with explicit interactions is still difficult to achieve due to the scene layout generation heavily suffer from the diversity object scaling and spatial locations. In this paper, we proposed a novel framework for generating realistic image layout from textual scene graphs. In our framework, a spatial constraint module is designed to fit reasonable scaling and spatial layout of object pairs with considering relationship between them. Moreover, a contextual fusion module is introduced for fusing pair-wise spatial information in terms of object dependency in scene graph. By using these two modules, our proposed framework tends to generate more commonsense layout which is helpful for realistic image generation. Experimental results including quantitative results, qualitative results and user studies on two different scene graph datasets demonstrate our proposed framework's ability to generate complex and logical layout with multiple objects from scene graph.",0
"The advancement of Generative Adversarial Networks (GANs) has facilitated the production of remarkably lifelike images of individual objects based on natural language descriptions. However, creating controlled image generation for multiple entities with explicit interactions remains challenging. This is due to the difficulty in generating scene layouts that accommodate the diversity of object scaling and spatial locations. In this study, we present a new framework for generating realistic image layouts from textual scene graphs. Our approach includes a spatial constraint module that considers the relationship between object pairs and fits them into a reasonable scaling and spatial layout. Additionally, we introduce a contextual fusion module that fuses pair-wise spatial information in terms of object dependency in the scene graph. By utilizing these two modules, our proposed framework generates more sensible layouts that are useful for producing realistic images. We conducted several experiments, including quantitative and qualitative evaluations, and user studies on two different scene graph datasets. Our results demonstrate the ability of our proposed framework to generate complex and coherent layouts for multiple objects from the scene graph.",1
"There has been an increasing interest in the area of emergent communication between agents which learn to play referential signalling games with realistic images. In this work, we consider the signalling game setting of Havrylov and Titov and investigate the effect of the feature extractor's weights and of the task being solved on the visual semantics learned or captured by the models. We impose various augmentation to the input images and additional tasks in the game with the aim to induce visual representations which capture conceptual properties of images. Through our set of experiments, we demonstrate that communication systems which capture visual semantics can be learned in a completely self-supervised manner by playing the right types of game.",0
"Emergent communication between agents that learn to play referential signalling games with realistic images has become increasingly popular. This study focuses on the signalling game setting of Havrylov and Titov and examines how the weights of the feature extractor and the task being solved affect the visual semantics learned by the models. The input images are enhanced with various augmentations and extra tasks are added to the game to encourage visual representations that capture the conceptual properties of the images. Through a series of experiments, it is shown that communication systems that capture visual semantics can be learned through self-supervised gameplay.",1
"Describing a video automatically with natural language is a challenging task in the area of computer vision. In most cases, the on-site situation of great events is reported in news, but the situation of the off-site spectators in the entrance and exit is neglected which also arouses people's interest. Since the deployment of reporters in the entrance and exit costs lots of manpower, how to automatically describe the behavior of a crowd of off-site spectators is significant and remains a problem. To tackle this problem, we propose a new task called crowd video captioning (CVC) which aims to describe the crowd of spectators. We also provide baseline methods for this task and evaluate them on the dataset WorldExpo'10. Our experimental results show that captioning models have a fairly deep understanding of the crowd in video and perform satisfactorily in the CVC task.",0
"Computer vision faces a difficult challenge in describing videos in natural language. News reports usually cover the on-site situation of major events, but they tend to neglect the behavior of off-site spectators at the entrance and exit, which also piques people's curiosity. However, deploying reporters in these areas is costly in terms of manpower. Therefore, automatically describing the behavior of a crowd of off-site spectators is a significant and persistent issue. To address this, we propose a new task called crowd video captioning (CVC), which aims to describe the crowd of spectators. We also present baseline methods for this task and evaluate them using the WorldExpo'10 dataset. Our experimental results show that captioning models have a deep understanding of the crowd in video and perform well in the CVC task.",1
"The past decade has seen a remarkable series of advances in machine learning, and in particular deep learning approaches based on artificial neural networks, to improve our abilities to build more accurate systems across a broad range of areas, including computer vision, speech recognition, language translation, and natural language understanding tasks. This paper is a companion paper to a keynote talk at the 2020 International Solid-State Circuits Conference (ISSCC) discussing some of the advances in machine learning, and their implications on the kinds of computational devices we need to build, especially in the post-Moore's Law-era. It also discusses some of the ways that machine learning may also be able to help with some aspects of the circuit design process. Finally, it provides a sketch of at least one interesting direction towards much larger-scale multi-task models that are sparsely activated and employ much more dynamic, example- and task-based routing than the machine learning models of today.",0
"Over the past 10 years, there have been significant progressions in machine learning, specifically in deep learning techniques that rely on artificial neural networks, enabling us to develop more precise systems in various fields such as computer vision, speech recognition, language translation, and natural language comprehension. This article serves as an accompanying piece to a keynote speech delivered at the 2020 International Solid-State Circuits Conference (ISSCC), highlighting the advancements in machine learning and their impact on the types of computational devices we require, particularly in the post-Moore's Law era. Additionally, it explores how machine learning could potentially aid in certain aspects of circuit design. Lastly, it briefly touches on one potential avenue to explore in the development of much larger multi-task models that are sparsely activated and use more dynamic, task-based routing compared to present machine learning models.",1
"In drug-discovery-related tasks such as virtual screening, machine learning is emerging as a promising way to predict molecular properties. Conventionally, molecular fingerprints (numerical representations of molecules) are calculated through rule-based algorithms that map molecules to a sparse discrete space. However, these algorithms perform poorly for shallow prediction models or small datasets. To address this issue, we present SMILES Transformer. Inspired by Transformer and pre-trained language models from natural language processing, SMILES Transformer learns molecular fingerprints through unsupervised pre-training of the sequence-to-sequence language model using a huge corpus of SMILES, a text representation system for molecules. We performed benchmarks on 10 datasets against existing fingerprints and graph-based methods and demonstrated the superiority of the proposed algorithms in small-data settings where pre-training facilitated good generalization. Moreover, we define a novel metric to concurrently measure model accuracy and data efficiency.",0
"Machine learning is becoming an increasingly promising method for predicting molecular properties in drug discovery tasks such as virtual screening. However, conventional rule-based algorithms for calculating molecular fingerprints, which are numerical representations of molecules, are often inadequate for shallow prediction models or small datasets. To address this issue, we introduce SMILES Transformer, which is inspired by Transformer and pre-trained language models used in natural language processing. This method learns molecular fingerprints by unsupervised pre-training of a sequence-to-sequence language model using a large corpus of SMILES, a text-based representation system for molecules. We evaluated the performance of SMILES Transformer against existing fingerprints and graph-based methods on 10 datasets and demonstrated superior results in small-data settings where pre-training allowed for better generalization. Additionally, we created a novel metric that measures both model accuracy and data efficiency simultaneously.",1
"This paper explores the task of interactive image retrieval using natural language queries, where a user progressively provides input queries to refine a set of retrieval results. Moreover, our work explores this problem in the context of complex image scenes containing multiple objects. We propose Drill-down, an effective framework for encoding multiple queries with an efficient compact state representation that significantly extends current methods for single-round image retrieval. We show that using multiple rounds of natural language queries as input can be surprisingly effective to find arbitrarily specific images of complex scenes. Furthermore, we find that existing image datasets with textual captions can provide a surprisingly effective form of weak supervision for this task. We compare our method with existing sequential encoding and embedding networks, demonstrating superior performance on two proposed benchmarks: automatic image retrieval on a simulated scenario that uses region captions as queries, and interactive image retrieval using real queries from human evaluators.",0
"The objective of this research is to investigate interactive image retrieval using natural language queries. The user provides a series of input queries to refine retrieval results. Additionally, the study examines this problem in the context of complex image scenes with multiple objects. The authors introduce a framework called Drill-down that encodes multiple queries with a compact state representation, enhancing current methods for single-round image retrieval. The study shows that using multiple rounds of natural language queries can be surprisingly effective in finding highly specific images of complex scenes. The authors find that existing image datasets with textual captions offer a surprisingly effective form of weak supervision for this task. The study compares the proposed method with existing sequential encoding and embedding networks and demonstrates superior performance on two proposed benchmarks: automatic image retrieval on a simulated scenario that uses region captions as queries and interactive image retrieval using real queries from human evaluators.",1
"Autonomous reinforcement learning agents, like children, do not have access to predefined goals and reward functions. They must discover potential goals, learn their own reward functions and engage in their own learning trajectory. Children, however, benefit from exposure to language, helping to organize and mediate their thought. We propose LE2 (Language Enhanced Exploration), a learning algorithm leveraging intrinsic motivations and natural language (NL) interactions with a descriptive social partner (SP). Using NL descriptions from the SP, it can learn an NL-conditioned reward function to formulate goals for intrinsically motivated goal exploration and learn a goal-conditioned policy. By exploring, collecting descriptions from the SP and jointly learning the reward function and the policy, the agent grounds NL descriptions into real behavioral goals. From simple goals discovered early to more complex goals discovered by experimenting on simpler ones, our agent autonomously builds its own behavioral repertoire. This naturally occurring curriculum is supplemented by an active learning curriculum resulting from the agent's intrinsic motivations. Experiments are presented with a simulated robotic arm that interacts with several objects including tools.",0
"Autonomous reinforcement learning agents, much like children, have to discover potential goals and establish their own reward functions as they do not have access to pre-existing ones. Children, on the other hand, have the advantage of exposure to language to help with cognitive organization. Our proposed learning algorithm, LE2, combines intrinsic motivations with natural language interactions with a social partner to learn an NL-conditioned reward function, formulate goals for intrinsically motivated goal exploration, and learn a goal-conditioned policy. Through exploration and joint learning of the reward function and policy, the agent can ground NL descriptions into real behavioral goals. The agent builds its own behavioral repertoire through a naturally occurring curriculum supplemented by an active learning curriculum resulting from its intrinsic motivations. We conducted experiments with a simulated robotic arm interacting with tools and various objects.",1
"The massive progress of machine learning has seen its application over a variety of domains in the past decade. But how do we develop a systematic, scalable and modular strategy to validate machine-learning systems? We present, to the best of our knowledge, the first approach, which provides a systematic test framework for machine-learning systems that accepts grammar-based inputs. Our OGMA approach automatically discovers erroneous behaviours in classifiers and leverages these erroneous behaviours to improve the respective models. OGMA leverages inherent robustness properties present in any well trained machine-learning model to direct test generation and thus, implementing a scalable test generation methodology. To evaluate our OGMA approach, we have tested it on three real world natural language processing (NLP) classifiers. We have found thousands of erroneous behaviours in these systems. We also compare OGMA with a random test generation approach and observe that OGMA is more effective than such random test generation by up to 489%.",0
"Over the past decade, machine learning has made significant strides and has been applied to various domains. However, the challenge is to establish a methodical, adaptable, and modular approach for validating machine learning systems. Our team has introduced the first-ever approach that provides a systematic test framework for machine learning systems that can accept grammar-based inputs. The OGMA approach automatically detects flawed behaviors in classifiers and utilizes them to enhance the respective models. By leveraging the inherent robustness properties of a well-trained machine learning model, OGMA directs test generation, resulting in an efficient and scalable methodology. We have evaluated the OGMA approach on three natural language processing (NLP) classifiers and discovered thousands of flawed behaviors. Furthermore, we have compared OGMA with a random test generation approach and have observed that OGMA is up to 489% more effective than the random test generation approach.",1
"In this paper, we introduce a contextual grounding approach that captures the context in corresponding text entities and image regions to improve the grounding accuracy. Specifically, the proposed architecture accepts pre-trained text token embeddings and image object features from an off-the-shelf object detector as input. Additional encoding to capture the positional and spatial information can be added to enhance the feature quality. There are separate text and image branches facilitating respective architectural refinements for different modalities. The text branch is pre-trained on a large-scale masked language modeling task while the image branch is trained from scratch. Next, the model learns the contextual representations of the text tokens and image objects through layers of high-order interaction respectively. The final grounding head ranks the correspondence between the textual and visual representations through cross-modal interaction. In the evaluation, we show that our model achieves the state-of-the-art grounding accuracy of 71.36% over the Flickr30K Entities dataset. No additional pre-training is necessary to deliver competitive results compared with related work that often requires task-agnostic and task-specific pre-training on cross-modal dadasets. The implementation is publicly available at https://gitlab.com/necla-ml/grounding.",0
"This paper presents a contextual grounding approach to enhance the accuracy of grounding by capturing the context in text entities and image regions. The proposed architecture takes pre-trained text token embeddings and image object features from an object detector as input. Additional encoding can be added to capture positional and spatial information to improve feature quality. The model has separate text and image branches that are pre-trained on different tasks. The model then learns contextual representations of text tokens and image objects through high-order interactions. The final grounding head ranks the correspondence between textual and visual representations through cross-modal interaction. The model achieves state-of-the-art grounding accuracy of 71.36% over the Flickr30K Entities dataset without additional pre-training compared to related work. The implementation is publicly available at https://gitlab.com/necla-ml/grounding.",1
"Automatically describing video content with natural language has been attracting much attention in CV and NLP communities. Most existing methods predict one word at a time, and by feeding the last generated word back as input at the next time, while the other generated words are not fully exploited. Furthermore, traditional methods optimize the model using all the training samples in each epoch without considering their learning situations, which leads to a lot of unnecessary training and can not target the difficult samples. To address these issues, we propose a text-based dynamic attention model named TDAM, which imposes a dynamic attention mechanism on all the generated words with the motivation to improve the context semantic information and enhance the overall control of the whole sentence. Moreover, the text-based dynamic attention mechanism and the visual attention mechanism are linked together to focus on the important words. They can benefit from each other during training. Accordingly, the model is trained through two steps: ""starting from scratch"" and ""checking for gaps"". The former uses all the samples to optimize the model, while the latter only trains for samples with poor control. Experimental results on the popular datasets MSVD and MSR-VTT demonstrate that our non-ensemble model outperforms the state-of-the-art video captioning benchmarks.",0
"The use of natural language to describe video content has generated much interest in the computer vision (CV) and natural language processing (NLP) communities. However, current methods only predict one word at a time, without fully utilizing the other generated words. Additionally, traditional models train using all samples in each epoch, leading to unnecessary training and difficulty targeting difficult samples. To address these issues, we propose a text-based dynamic attention model (TDAM) that introduces a dynamic attention mechanism to all generated words, enhancing the overall control of the sentence and improving contextual semantic information. Furthermore, the model combines the text-based dynamic attention mechanism with the visual attention mechanism to focus on important words and mutually benefit from each other during training. The model is trained in two steps: ""starting from scratch"" and ""checking for gaps"", where the former optimizes the model using all samples, and the latter targets poorly controlled samples. Our experimental results on popular datasets demonstrate that our non-ensemble model outperforms current state-of-the-art video captioning benchmarks.",1
"Program synthesis of general-purpose source code from natural language specifications is challenging due to the need to reason about high-level patterns in the target program and low-level implementation details at the same time. In this work, we present PATOIS, a system that allows a neural program synthesizer to explicitly interleave high-level and low-level reasoning at every generation step. It accomplishes this by automatically mining common code idioms from a given corpus, incorporating them into the underlying language for neural synthesis, and training a tree-based neural synthesizer to use these idioms during code generation. We evaluate PATOIS on two complex semantic parsing datasets and show that using learned code idioms improves the synthesizer's accuracy.",0
"Synthesizing general-purpose source code from natural language specifications presents a challenge as it requires the ability to reason about both high-level patterns and low-level implementation details simultaneously. The present study introduces PATOIS, a system that enables a neural program synthesizer to effectively combine high-level and low-level reasoning throughout the generation process. By identifying common code idioms in a given corpus, PATOIS incorporates them into the neural synthesis language and trains a tree-based neural synthesizer to utilize these idioms when generating code. The system is tested on two complex semantic parsing datasets, and the results demonstrate that incorporating learned code idioms enhances the accuracy of the synthesizer.",1
"Accurately annotating large scale dataset is notoriously expensive both in time and in money. Although acquiring low-quality-annotated dataset can be much cheaper, it often badly damages the performance of trained models when using such dataset without particular treatment. Various methods have been proposed for learning with noisy labels. However, most methods only handle limited kinds of noise patterns, require auxiliary information or steps (e.g. , knowing or estimating the noise transition matrix), or lack theoretical justification. In this paper, we propose a novel information-theoretic loss function, $\mathcal{L}_{DMI}$, for training deep neural networks robust to label noise. The core of $\mathcal{L}_{DMI}$ is a generalized version of mutual information, termed Determinant based Mutual Information (DMI), which is not only information-monotone but also relatively invariant. \emph{To the best of our knowledge, $\mathcal{L}_{DMI}$ is the first loss function that is provably robust to instance-independent label noise, regardless of noise pattern, and it can be applied to any existing classification neural networks straightforwardly without any auxiliary information}. In addition to theoretical justification, we also empirically show that using $\mathcal{L}_{DMI}$ outperforms all other counterparts in the classification task on both image dataset and natural language dataset include Fashion-MNIST, CIFAR-10, Dogs vs. Cats, MR with a variety of synthesized noise patterns and noise amounts, as well as a real-world dataset Clothing1M. Codes are available at https://github.com/Newbeeer/L_DMI .",0
"Large scale dataset annotation is an expensive and time-consuming task. While low-quality annotated datasets can be acquired at a lower cost, they negatively impact the performance of trained models. Although various methods have been proposed to learn with noisy labels, most only handle specific types of noise patterns and require auxiliary information or steps. To address this, we propose a novel information-theoretic loss function, $\mathcal{L}_{DMI}$, for training deep neural networks that are robust to label noise. This loss function is the first to be provably robust to instance-independent label noise, regardless of noise pattern, and can be applied to any existing classification neural networks without any auxiliary information. Empirical results show that using $\mathcal{L}_{DMI}$ outperforms other methods in classification tasks on various datasets, including image and natural language datasets with synthesized and real-world noise patterns. Code is available at https://github.com/Newbeeer/L_DMI.",1
"As a general-purpose generative model architecture, VAE has been widely used in the field of image and natural language processing. VAE maps high dimensional sample data into continuous latent variables with unsupervised learning. Sampling in the latent variable space of the feature, VAE can construct new image or text data. As a general-purpose generation model, the vanilla VAE can not fit well with various data sets and neural networks with different structures. Because of the need to balance the accuracy of reconstruction and the convenience of latent variable sampling in the training process, VAE often has problems known as ""posterior collapse"". images reconstructed by VAE are also often blurred. In this paper, we analyze the main cause of these problem, which is the lack of mutual information between the sample variable and the latent feature variable during the training process. To maintain mutual information in model training, we propose to use the auxiliary softmax multi-classification network structure to improve the training effect of VAE, named VAE-AS. We use MNIST and Omniglot data sets to test the VAE-AS model. Based on the test results, It can be show that VAE-AS has obvious effects on the mutual information adjusting and solving the posterior collapse problem.",0
"VAE, a widely used generative model architecture in image and natural language processing, utilizes unsupervised learning to map high dimensional sample data into continuous latent variables. By sampling in the latent variable space of the feature, VAE can generate new image or text data. However, vanilla VAE struggles to fit various data sets and neural networks with different structures due to the need to balance accuracy of reconstruction and convenience of latent variable sampling in training, which often results in ""posterior collapse"" and blurry reconstructed images. In this paper, we identify the main cause of these issues as the lack of mutual information between the sample variable and the latent feature variable during training. To address this, we introduce a new model called VAE-AS, which uses an auxiliary softmax multi-classification network structure to improve training by maintaining mutual information. We test VAE-AS on the MNIST and Omniglot data sets and demonstrate its effectiveness in adjusting mutual information and solving the posterior collapse problem.",1
"Deep learning achieves state-of-the-art results in many tasks in computer vision and natural language processing. However, recent works have shown that deep networks can be vulnerable to adversarial perturbations, which raised a serious robustness issue of deep networks. Adversarial training, typically formulated as a robust optimization problem, is an effective way of improving the robustness of deep networks. A major drawback of existing adversarial training algorithms is the computational overhead of the generation of adversarial examples, typically far greater than that of the network training. This leads to the unbearable overall computational cost of adversarial training. In this paper, we show that adversarial training can be cast as a discrete time differential game. Through analyzing the Pontryagin's Maximal Principle (PMP) of the problem, we observe that the adversary update is only coupled with the parameters of the first layer of the network. This inspires us to restrict most of the forward and back propagation within the first layer of the network during adversary updates. This effectively reduces the total number of full forward and backward propagation to only one for each group of adversary updates. Therefore, we refer to this algorithm YOPO (You Only Propagate Once). Numerical experiments demonstrate that YOPO can achieve comparable defense accuracy with approximately 1/5 ~ 1/4 GPU time of the projected gradient descent (PGD) algorithm. Our codes are available at https://https://github.com/a1600012888/YOPO-You-Only-Propagate-Once.",0
"While deep learning has proven to be highly effective in computer vision and natural language processing tasks, recent research has uncovered a significant vulnerability of deep networks to adversarial perturbations. In order to address this robustness issue, adversarial training has been proposed as a means of improving the resilience of deep networks. However, the computational overhead associated with the generation of these adversarial examples has been a major challenge with existing algorithms. In this paper, we present YOPO (You Only Propagate Once), a novel adversarial training approach that leverages Pontryagin's Maximal Principle to reduce the overall computational cost. By restricting most of the forward and back propagation to the first layer of the network during adversary updates, we are able to achieve comparable defense accuracy with only 1/5 ~ 1/4 of the GPU time required by traditional approaches like projected gradient descent (PGD). Our code is publicly available at https://github.com/a1600012888/YOPO-You-Only-Propagate-Once.",1
"In recent years, huge amounts of unstructured textual data on the Internet are a big difficulty for AI algorithms to provide the best recommendations for users and their search queries. Since the Internet became widespread, a lot of research has been done in the field of Natural Language Processing (NLP) and machine learning. Almost every solution transforms documents into Vector Space Models (VSM) in order to apply AI algorithms over them. One such approach is based on Case-Based Reasoning (CBR). Therefore, the most important part of those systems is to compute the similarity between numerical data points. In 2016, the new similarity TS-SS metric is proposed, which showed state-of-the-art results in the field of textual mining for unsupervised learning. However, no one before has investigated its performances for supervised learning (classification task). In this work, we devised a CBR system capable of finding the most similar documents for a given query aiming to investigate performances of the new state-of-the-art metric, TS-SS, in addition to the two other geometrical similarity measures --- Euclidean distance and Cosine similarity --- that showed the best predictive results over several benchmark corpora. The results show surprising inappropriateness of TS-SS measure for high dimensional features.",0
"AI algorithms face a major challenge in providing accurate recommendations for users and their search queries due to the vast amounts of unstructured textual data on the Internet. Natural Language Processing (NLP) and machine learning have been extensively studied since the Internet became widespread. Document transformation into Vector Space Models (VSM) is a common approach for applying AI algorithms. Case-Based Reasoning (CBR) is one example of such an approach, with the similarity between numerical data points being a critical component. In 2016, a new similarity metric called TS-SS was proposed that showed superior results for unsupervised learning, but its performance for supervised learning had not been explored. In this study, we developed a CBR system to investigate the performance of TS-SS compared to two other similarity measures, Euclidean distance and Cosine similarity, which had shown the best predictive results across various benchmark corpora. Surprisingly, the results demonstrated the inadequacy of the TS-SS measure for high dimensional features.",1
"Emotion recognition is a classic field of research with a typical setup extracting features and feeding them through a classifier for prediction. On the other hand, generative models jointly capture the distributional relationship between emotions and the feature profiles. Relatively recently, Generative Adversarial Networks (GANs) have surfaced as a new class of generative models and have shown considerable success in modeling distributions in the fields of computer vision and natural language understanding. In this work, we experiment with variants of GAN architectures to generate feature vectors corresponding to an emotion in two ways: (i) A generator is trained with samples from a mixture prior. Each mixture component corresponds to an emotional class and can be sampled to generate features from the corresponding emotion. (ii) A one-hot vector corresponding to an emotion can be explicitly used to generate the features. We perform analysis on such models and also propose different metrics used to measure the performance of the GAN models in their ability to generate realistic synthetic samples. Apart from evaluation on a given dataset of interest, we perform a cross-corpus study where we study the utility of the synthetic samples as additional training data in low resource conditions.",0
"The research field of emotion recognition typically involves extracting features and using a classifier to predict emotions. However, generative models provide a different approach by capturing the relationship between emotions and feature profiles. Recently, Generative Adversarial Networks (GANs) have shown success in modeling distributions in computer vision and natural language understanding. This study explores the use of GANs to generate feature vectors for specific emotions in two ways: (i) by training a generator with samples from a mixture prior where each component corresponds to an emotional class, and (ii) by using a one-hot vector for a specific emotion to generate features. The study proposes metrics to measure the performance of GAN models in generating realistic synthetic samples and evaluates their utility as additional training data in low resource conditions through a cross-corpus study.",1
"Matrices satisfying the Restricted Isometry Property (RIP) play an important role in the areas of compressed sensing and statistical learning. RIP matrices with optimal parameters are mainly obtained via probabilistic arguments, as explicit constructions seem hard. It is therefore interesting to ask whether a fixed matrix can be incorporated into a construction of restricted isometries. In this paper, we construct a new broad ensemble of random matrices with dependent entries that satisfy the restricted isometry property. Our construction starts with a fixed (deterministic) matrix $X$ satisfying some simple stable rank condition, and we show that the matrix $XR$, where $R$ is a random matrix drawn from various popular probabilistic models (including, subgaussian, sparse, low-randomness, satisfying convex concentration property), satisfies the RIP with high probability. These theorems have various applications in signal recovery, random matrix theory, dimensionality reduction, etc. Additionally, motivated by an application for understanding the effectiveness of word vector embeddings popular in natural language processing and machine learning applications, we investigate the RIP of the matrix $XR^{(l)}$ where $R^{(l)}$ is formed by taking all possible (disregarding order) $l$-way entrywise products of the columns of a random matrix $R$.",0
"Matrices that meet the Restricted Isometry Property (RIP) are significant in compressed sensing and statistical learning. RIP matrices with optimal parameters are commonly achieved through probabilistic reasoning as explicit constructions are challenging. It is intriguing to consider whether a fixed matrix can be included in a construction of restricted isometries. In this paper, we create a new diverse collection of random matrices with dependent entries that fulfill the restricted isometry property. Our construction starts with a fixed (deterministic) matrix X that meets a simple stable rank condition. We then demonstrate that the matrix XR, where R is a random matrix chosen from various popular probabilistic models (such as subgaussian, sparse, low-randomness, and satisfying convex concentration property), meets the RIP with high likelihood. These theorems have numerous applications in signal recovery, random matrix theory, dimensionality reduction, and more. Additionally, we investigate the RIP of the matrix XR^(l), where R^(l) is obtained by taking all possible (ignoring order) l-way entrywise products of the columns of a random matrix R, motivated by an application for understanding the effectiveness of word vector embeddings commonly used in natural language processing and machine learning applications.",1
"In practical applications of machine learning, it is often desirable to identify and abstain on examples where the model's predictions are likely to be incorrect. Much of the prior work on this topic focused on out-of-distribution detection or performance metrics such as top-k accuracy. Comparatively little attention was given to metrics such as area-under-the-curve or Cohen's Kappa, which are extremely relevant for imbalanced datasets. Abstention strategies aimed at top-k accuracy can produce poor results on these metrics when applied to imbalanced datasets, even when all examples are in-distribution. We propose a framework to address this gap. Our framework leverages the insight that calibrated probability estimates can be used as a proxy for the true class labels, thereby allowing us to estimate the change in an arbitrary metric if an example were abstained on. Using this framework, we derive computationally efficient metric-specific abstention algorithms for optimizing the sensitivity at a target specificity level, the area under the ROC, and the weighted Cohen's Kappa. Because our method relies only on calibrated probability estimates, we further show that by leveraging recent work on domain adaptation under label shift, we can generalize to test-set distributions that may have a different class imbalance compared to the training set distribution. On various experiments involving medical imaging, natural language processing, computer vision and genomics, we demonstrate the effectiveness of our approach. Source code available at https://github.com/blindauth/abstention. Colab notebooks reproducing results available at https://github.com/blindauth/abstention_experiments.",0
"In machine learning, it is often important to identify and abstain from examples where the model may make incorrect predictions. Previous research has mainly focused on out-of-distribution detection and top-k accuracy, rather than metrics such as area-under-the-curve or Cohen's Kappa, which are more relevant for imbalanced datasets. Abstention strategies that prioritize top-k accuracy may not perform well on imbalanced datasets, even when all examples are within the distribution. To address this issue, we propose a framework that uses calibrated probability estimates as a proxy for true class labels, allowing us to estimate the impact of abstaining on a given metric. Our framework includes metric-specific abstention algorithms for optimizing sensitivity at a specific specificity level, area under the ROC, and weighted Cohen's Kappa. We show that our method can generalize to test-set distributions with different class imbalances using recent work on domain adaptation under label shift. We demonstrate the effectiveness of our approach on various experiments involving medical imaging, natural language processing, computer vision, and genomics. Our source code and Colab notebooks reproducing results are available on GitHub at https://github.com/blindauth/abstention and https://github.com/blindauth/abstention_experiments.",1
"Transformer architectures show significant promise for natural language processing. Given that a single pretrained model can be fine-tuned to perform well on many different tasks, these networks appear to extract generally useful linguistic features. A natural question is how such networks represent this information internally. This paper describes qualitative and quantitative investigations of one particularly effective model, BERT. At a high level, linguistic features seem to be represented in separate semantic and syntactic subspaces. We find evidence of a fine-grained geometric representation of word senses. We also present empirical descriptions of syntactic representations in both attention matrices and individual word embeddings, as well as a mathematical argument to explain the geometry of these representations.",0
"The potential of transformer architectures in natural language processing is significant. These networks have the ability to extract generally useful linguistic features, making it possible to fine-tune a single pretrained model for various tasks. However, it's unclear how these networks represent information internally. This study focuses on BERT, a particularly effective model, and conducts both qualitative and quantitative investigations. The findings suggest that linguistic features are represented in distinct semantic and syntactic subspaces. Furthermore, the study identifies evidence of detailed geometric representation of word senses, provides empirical descriptions of syntactic representations in attention matrices and individual word embeddings, and presents a mathematical argument to explain the geometry of these representations.",1
"Program comprehension is a fundamental task in software development and maintenance processes. Software developers often need to understand a large amount of existing code before they can develop new features or fix bugs in existing programs. Being able to process programming language code automatically and provide summaries of code functionality accurately can significantly help developers to reduce time spent in code navigation and understanding, and thus increase productivity. Different from natural language articles, source code in programming languages often follows rigid syntactical structures and there can exist dependencies among code elements that are located far away from each other through complex control flows and data flows. Existing studies on tree-based convolutional neural networks (TBCNN) and gated graph neural networks (GGNN) are not able to capture essential semantic dependencies among code elements accurately. In this paper, we propose novel tree-based capsule networks (TreeCaps) and relevant techniques for processing program code in an automated way that encodes code syntactical structures and captures code dependencies more accurately. Based on evaluation on programs written in different programming languages, we show that our TreeCaps-based approach can outperform other approaches in classifying the functionalities of many programs.",0
"In software development and maintenance, understanding existing code is crucial for developers to develop new features or fix bugs. However, processing programming language code manually can be time-consuming and reduce productivity. Unlike natural language articles, source code follows strict syntactical structures and may have dependencies located far away from each other. Existing studies on TBCNN and GGNN cannot accurately capture semantic dependencies among code elements. Therefore, we propose TreeCaps, a novel automated program comprehension approach that encodes code syntactical structures and captures code dependencies more accurately. Our evaluation shows that TreeCaps outperforms other approaches in classifying program functionalities across different programming languages.",1
"In this paper, we present Gamma-LSTM, an enhanced long short term memory (LSTM) unit, to enable learning of hierarchical representations through multiple stages of temporal abstractions. Gamma memory, a hierarchical memory unit, forms the central memory of Gamma-LSTM with gates to regulate the information flow into various levels of hierarchy, thus providing the unit with a control to pick the appropriate level of hierarchy to process the input at a given instant of time. We demonstrate better performance of Gamma-LSTM model regular and stacked LSTMs in two settings (pixel-by-pixel MNIST digit classification and natural language inference) placing emphasis on the ability to generalize over long sequences.",0
"The aim of this paper is to introduce Gamma-LSTM, a more advanced version of the traditional long short term memory (LSTM) unit. This unit has the capability to learn hierarchical representations through multiple stages of temporal abstractions. Gamma memory, which is a hierarchical memory unit, functions as the primary memory of Gamma-LSTM. It contains gates that regulate the flow of information into various levels of hierarchy, giving the unit control to select the appropriate level of hierarchy to process input at any given time. The Gamma-LSTM model outperforms regular and stacked LSTMs in two settings, namely pixel-by-pixel MNIST digit classification and natural language inference, especially in the ability to generalize over long sequences.",1
"Cancer is one of the leading cause of death, worldwide. Many believe that genomic data will enable us to better predict the survival time of these patients, which will lead to better, more personalized treatment options and patient care. As standard survival prediction models have a hard time coping with the high-dimensionality of such gene expression (GE) data, many projects use some dimensionality reduction techniques to overcome this hurdle. We introduce a novel methodology, inspired by topic modeling from the natural language domain, to derive expressive features from the high-dimensional GE data. There, a document is represented as a mixture over a relatively small number of topics, where each topic corresponds to a distribution over the words; here, to accommodate the heterogeneity of a patient's cancer, we represent each patient (~document) as a mixture over cancer-topics, where each cancer-topic is a mixture over GE values (~words). This required some extensions to the standard LDA model eg: to accommodate the ""real-valued"" expression values - leading to our novel ""discretized"" Latent Dirichlet Allocation (dLDA) procedure. We initially focus on the METABRIC dataset, which describes breast cancer patients using the r=49,576 GE values, from microarrays. Our results show that our approach provides survival estimates that are more accurate than standard models, in terms of the standard Concordance measure. We then validate this approach by running it on the Pan-kidney (KIPAN) dataset, over r=15,529 GE values - here using the mRNAseq modality - and find that it again achieves excellent results. In both cases, we also show that the resulting model is calibrated, using the recent ""D-calibrated"" measure. These successes, in two different cancer types and expression modalities, demonstrates the generality, and the effectiveness, of this approach.",0
"Cancer is a leading cause of death worldwide, and there is a belief that genomic data can improve survival predictions for patients. However, standard models struggle to handle the high-dimensional gene expression data, so some projects use dimensionality reduction techniques. We introduce a novel methodology inspired by natural language topic modeling to create expressive features from the data. Our approach represents each patient as a mixture of cancer-topics, which are a mixture of gene expression values. We used the METABRIC and Pan-kidney datasets to demonstrate the accuracy and effectiveness of our approach, which outperformed standard models in terms of survival estimates and calibration. Our results show that this approach is generalizable, making it a promising option for personalized treatment options and patient care.",1
"Deep Neural Networks are often though to lack interpretability due to the distributed nature of their internal representations. In contrast, humans can generally justify, in natural language, for their answer to a visual question with simple common sense reasoning. However, human introspection abilities have their own limits as one often struggles to justify for the recognition process behind our lowest level feature recognition ability: for instance, it is difficult to precisely explain why a given texture seems more characteristic of the surface of a finger nail rather than a plastic bottle. In this paper, we showcase an application in which deep learning models can actually help human experts justify for their own low-level visual recognition process: We study the problem of assessing the adhesive potency of copper sheets from microscopic pictures of their surface. Although highly trained material experts are able to qualitatively assess the surface adhesive potency, they are often unable to precisely justify for their decision process. We present a model that, under careful design considerations, is able to provide visual clues for human experts to understand and justify for their own recognition process. Not only can our model assist human experts in their interpretation of the surface characteristics, we show how this model can be used to test different hypothesis of the copper surface response to different manufacturing processes.",0
"Due to the distributed nature of their internal representations, Deep Neural Networks are often considered to lack interpretability. In contrast, humans can typically provide natural language justifications for their responses to visual questions using simple common sense reasoning. However, human introspection has its limitations, as it can be challenging to explain the recognition process behind our lowest level feature recognition ability. For example, it is difficult to precisely articulate why a particular texture is more characteristic of a fingernail surface as opposed to a plastic bottle. In this study, we demonstrate an application where deep learning models can aid human experts in justifying their own low-level visual recognition process. Specifically, we investigate the evaluation of adhesive potency in copper sheets using microscopic images of their surface. While highly trained material experts can qualitatively assess surface adhesive potency, they often struggle to explain their decision-making process. Our model is designed to provide visual clues that assist human experts in understanding and justifying their recognition process. Additionally, we demonstrate how this model can be used to test various hypotheses regarding the copper surface response to different manufacturing processes.",1
"Word embeddings learnt from large corpora have been adopted in various applications in natural language processing and served as the general input representations to learning systems. Recently, a series of post-processing methods have been proposed to boost the performance of word embeddings on similarity comparison and analogy retrieval tasks, and some have been adapted to compose sentence representations. The general hypothesis behind these methods is that by enforcing the embedding space to be more isotropic, the similarity between words can be better expressed. We view these methods as an approach to shrink the covariance/gram matrix, which is estimated by learning word vectors, towards a scaled identity matrix. By optimising an objective in the semi-Riemannian manifold with Centralised Kernel Alignment (CKA), we are able to search for the optimal shrinkage parameter, and provide a post-processing method to smooth the spectrum of learnt word vectors which yields improved performance on downstream tasks.",0
"Large corpora have been used to teach word embeddings that have been employed in various natural language processing applications and served as input representations for learning systems. Recent post-processing methods have been introduced to enhance the performance of word embeddings in similarity comparison and analogy retrieval tasks, and some have also been modified to create sentence representations. The underlying notion behind these methods is that by making the embedding space more isotropic, it will be better able to express the similarity between words. We consider these methods as a way to decrease the covariance/gram matrix, which is estimated by learning word vectors, towards a scaled identity matrix. By optimizing a semi-Riemannian manifold objective with Centralised Kernel Alignment (CKA), we can look for the ideal shrinkage parameter and provide a post-processing technique that smooths the spectrum of learned word vectors, resulting in improved results in downstream tasks.",1
"Visual grounding, a task to ground (i.e., localize) natural language in images, essentially requires composite visual reasoning. However, existing methods over-simplify the composite nature of language into a monolithic sentence embedding or a coarse composition of subject-predicate-object triplet. In this paper, we propose to ground natural language in an intuitive, explainable, and composite fashion as it should be. In particular, we develop a novel modular network called Neural Module Tree network (NMTree) that regularizes the visual grounding along the dependency parsing tree of the sentence, where each node is a neural module that calculates visual attention according to its linguistic feature, and the grounding score is accumulated in a bottom-up direction where as needed. NMTree disentangles the visual grounding from the composite reasoning, allowing the former to only focus on primitive and easy-to-generalize patterns. To reduce the impact of parsing errors, we train the modules and their assembly end-to-end by using the Gumbel-Softmax approximation and its straight-through gradient estimator, accounting for the discrete nature of module assembly. Overall, the proposed NMTree consistently outperforms the state-of-the-arts on several benchmarks. Qualitative results show explainable grounding score calculation in great detail.",0
"The task of visual grounding involves localizing natural language in images and requires complex visual reasoning. However, current methods oversimplify language by using a single sentence embedding or a basic subject-predicate-object triplet. In this paper, we propose a more intuitive and detailed approach to visual grounding. Our Neural Module Tree network (NMTree) is a modular network that uses a dependency parsing tree to regulate visual grounding. Each node is a neural module that calculates visual attention based on linguistic features, and scores accumulate in a bottom-up manner. NMTree separates visual grounding from composite reasoning, allowing it to focus on simple and easily generalizable patterns. To account for parsing errors, we train the network end-to-end using Gumbel-Softmax approximation and its straight-through gradient estimator. Our results show that NMTree outperforms current methods and provides detailed explanations of grounding score calculations.",1
"How can we teach a robot to predict what will happen next for an activity it has never seen before? We address this problem of zero-shot anticipation by presenting a hierarchical model that generalizes instructional knowledge from large-scale text-corpora and transfers the knowledge to the visual domain. Given a portion of an instructional video, our model predicts coherent and plausible actions multiple steps into the future, all in rich natural language. To demonstrate the anticipation capabilities of our model, we introduce the Tasty Videos dataset, a collection of 2511 recipes for zero-shot learning, recognition and anticipation.",0
"Our focus is on how to instruct a robot to anticipate future events in a novel activity. To tackle this challenge, we propose a hierarchical model that utilizes information from extensive text-corpora to create a transferable knowledge base for visual comprehension. Using a segment of an instructional video, our model generates logical and believable descriptions of several future actions in natural language. To showcase the proficiency of our model in anticipation, we introduce the Tasty Videos dataset, comprising of 2511 recipes for zero-shot recognition, learning, and anticipation.",1
"Time Series Classification (TSC) has been an important and challenging task in data mining, especially on multivariate time series and multi-view time series data sets. Meanwhile, transfer learning has been widely applied in computer vision and natural language processing applications to improve deep neural network's generalization capabilities. However, very few previous works applied transfer learning framework to time series mining problems. Particularly, the technique of measuring similarities between source domain and target domain based on dynamic representation such as density estimation with importance sampling has never been combined with transfer learning framework. In this paper, we first proposed a general adaptive transfer learning framework for multi-view time series data, which shows strong ability in storing inter-view importance value in the process of knowledge transfer. Next, we represented inter-view importance through some time series similarity measurements and approximated the posterior distribution in latent space for the importance sampling via density estimation techniques. We then computed the matrix norm of sampled importance value, which controls the degree of knowledge transfer in pre-training process. We further evaluated our work, applied it to many other time series classification tasks, and observed that our architecture maintained desirable generalization ability. Finally, we concluded that our framework could be adapted with deep learning techniques to receive significant model performance improvements.",0
"Data mining faces a significant challenge in Time Series Classification (TSC), particularly when dealing with multivariate time series and multi-view time series datasets. Transfer learning has proven successful in improving deep neural network generalization in computer vision and natural language processing applications, yet it remains underutilized in time series mining. This is especially true when it comes to measuring similarities between source domain and target domain using dynamic representation techniques such as density estimation with importance sampling. In this study, we introduce a novel adaptive transfer learning framework for multi-view time series data that stores inter-view importance values during knowledge transfer. We also incorporate time series similarity measurements to represent inter-view importance and approximate the posterior distribution via density estimation techniques. The matrix norm of sampled importance values is then computed to control the degree of knowledge transfer during pre-training. Our work is evaluated across various time series classification tasks, demonstrating strong generalization ability. We conclude that the framework can be adapted with deep learning techniques to achieve significant model performance improvements.",1
"In this paper, we investigate a novel problem of telling the difference between image pairs in natural language. Compared to previous approaches for single image captioning, it is challenging to fetch linguistic representation from two independent visual information. To this end, we have proposed an effective encoder-decoder caption framework based on Hyper Convolution Net. In addition, a series of novel feature fusing techniques for pairwise visual information fusing are introduced and a discriminating referee is proposed to evaluate the pipeline. Because of the lack of appropriate datasets to support this task, we have collected and annotated a large new dataset with Amazon Mechanical Turk (AMT) for generating captions in a pairwise manner (with 14764 images and 26710 image pairs in total). The dataset is the first one on the relative difference caption task that provides descriptions in free language. We evaluate the effectiveness of our model on two datasets in the field and it outperforms the state-of-the-art approach by a large margin.",0
"The focus of this study is to distinguish between pairs of images using natural language, which is a unique challenge compared to previous methods that only involve single image captioning. The difficulty lies in extracting linguistic representation from two independent visual sources. To address this, we have proposed a Hyper Convolution Net based encoder-decoder caption framework, along with novel feature fusing techniques for pairwise visual information and a discriminating referee for pipeline evaluation. Due to the lack of suitable datasets, we have created a large new dataset with AMT, consisting of 14764 images and 26710 image pairs, which is the first to provide free language descriptions for the relative difference caption task. Our model has been evaluated on two datasets and has significantly outperformed the state-of-the-art approach.",1
"In order to successfully perform tasks specified by natural language instructions, an artificial agent operating in a visual world needs to map words, concepts, and actions from the instruction to visual elements in its environment. This association is termed as Task-Oriented Grounding. In this work, we propose a novel Dynamic Attention Network architecture for the efficient multi-modal fusion of text and visual representations which can generate a robust definition of state for the policy learner. Our model assumes no prior knowledge from visual and textual domains and is an end to end trainable. For a 3D visual world where the observation changes continuously, the attention on the visual elements tends to be highly co-related from a one-time step to the next. We term this as ""Dynamic Attention"". In this work, we show that Dynamic Attention helps in achieving grounding and also aids in the policy learning objective. Since most practical robotic applications take place in the real world where the observation space is continuous, our framework can be used as a generalized multi-modal fusion unit for robotic control through natural language. We show the effectiveness of using 1D convolution over Gated Attention Hadamard product on the rate of convergence of the network. We demonstrate that the cell-state of a Long Short Term Memory (LSTM) is a natural choice for modeling Dynamic Attention and shows through visualization that the generated attention is very close to how humans tend to focus on the environment.",0
"For an artificial agent to effectively carry out natural language instructions in a visual environment, it must connect words, concepts, and actions in the instructions to the visual elements in its surroundings. This connection is referred to as Task-Oriented Grounding. Our study introduces a new architecture, the Dynamic Attention Network, for streamlined multi-modal fusion of visual and text representations. This generates a reliable definition of state for the policy learner. Our model is trainable end to end and does not require prior knowledge of visual or textual domains. In a 3D visual world, where observation is continually changing, visual element attention tends to be highly correlated from one time step to the next, which we call ""Dynamic Attention"". Our research demonstrates that Dynamic Attention is helpful in achieving grounding and supports the policy learning objective. Since most practical robotics applications occur in the real world, where observation space is continuous, our framework can serve as a general multi-modal fusion unit for robotic control using natural language. We also reveal the effectiveness of using 1D convolution over Gated Attention Hadamard product in the network's convergence rate. We show that the cell-state of a Long Short Term Memory (LSTM) is a natural fit for modeling Dynamic Attention, and visualization indicates that the generated attention is similar to how humans tend to focus on the environment.",1
"Owing to their ability to both effectively integrate information over long time horizons and scale to massive amounts of data, self-attention architectures have recently shown breakthrough success in natural language processing (NLP), achieving state-of-the-art results in domains such as language modeling and machine translation. Harnessing the transformer's ability to process long time horizons of information could provide a similar performance boost in partially observable reinforcement learning (RL) domains, but the large-scale transformers used in NLP have yet to be successfully applied to the RL setting. In this work we demonstrate that the standard transformer architecture is difficult to optimize, which was previously observed in the supervised learning setting but becomes especially pronounced with RL objectives. We propose architectural modifications that substantially improve the stability and learning speed of the original Transformer and XL variant. The proposed architecture, the Gated Transformer-XL (GTrXL), surpasses LSTMs on challenging memory environments and achieves state-of-the-art results on the multi-task DMLab-30 benchmark suite, exceeding the performance of an external memory architecture. We show that the GTrXL, trained using the same losses, has stability and performance that consistently matches or exceeds a competitive LSTM baseline, including on more reactive tasks where memory is less critical. GTrXL offers an easy-to-train, simple-to-implement but substantially more expressive architectural alternative to the standard multi-layer LSTM ubiquitously used for RL agents in partially observable environments.",0
"Self-attention architectures have shown remarkable success in natural language processing due to their ability to integrate information over long periods and handle vast amounts of data. The transformer's capacity to process extended information could similarly improve performance in partially observable reinforcement learning domains, but large-scale transformers used in NLP have yet to be successfully applied to RL. This study demonstrates that the standard transformer architecture is challenging to optimize, particularly in RL objectives, and proposes architectural modifications that significantly enhance the stability and learning speed of the original Transformer and XL variant. The proposed architecture, Gated Transformer-XL (GTrXL), outperforms LSTMs in challenging memory environments and sets a new benchmark on the multi-task DMLab-30. It shows consistent performance that matches or exceeds a competitive LSTM baseline, even on more reactive tasks where memory is less important. GTrXL provides a simpler, more expressive, and easy-to-train alternative to the widely used multi-layer LSTM for RL agents in partially observable environments.",1
"We present a new task of query auto-completion for estimating instance probabilities. We complete a user query prefix conditioned upon an image. Given the complete query, we fine tune a BERT embedding for estimating probabilities of a broad set of instances. The resulting instance probabilities are used for selection while being agnostic to the segmentation or attention mechanism. Our results demonstrate that auto-completion using both language and vision performs better than using only language, and that fine tuning a BERT embedding allows to efficiently rank instances in the image. In the spirit of reproducible research we make our data, models, and code available.",0
"In this study, we introduce a novel query auto-completion task aimed at estimating instance probabilities. Our approach involves completing a user query prefix based on an image, and then fine-tuning a BERT embedding to estimate probabilities for a wide variety of instances. By using both language and vision in the auto-completion process, we achieve better results compared to using language alone. Furthermore, fine-tuning the BERT embedding enables efficient ranking of instances within the image, regardless of segmentation or attention mechanism. We have made our data, models, and code available in the spirit of reproducible research.",1
"Referring expressions are natural language descriptions that identify a particular object within a scene and are widely used in our daily conversations. In this work, we focus on segmenting the object in an image specified by a referring expression. To this end, we propose an end-to-end trainable comprehension network that consists of the language and visual encoders to extract feature representations from both domains. We introduce the spatial-aware dynamic filters to transfer knowledge from text to image, and effectively capture the spatial information of the specified object. To better communicate between the language and visual modules, we employ a caption generation network that takes features shared across both domains as input, and improves both representations via a consistency that enforces the generated sentence to be similar to the given referring expression. We evaluate the proposed framework on two referring expression datasets and show that our method performs favorably against the state-of-the-art algorithms.",0
"The usage of referring expressions in daily conversations is common as they describe a specific object in a scene. This study concentrates on segmenting an object in an image, which is identified by a referring expression. A comprehension network that includes language and visual encoders is proposed, which extract feature representations from both areas. To capture the spatial details of the object, spatial-aware dynamic filters are introduced to transfer knowledge from text to image. A caption generation network enhances the representations by enforcing consistency between the generated sentence and the given referring expression. The proposed framework is evaluated on two referring expression datasets, and the results demonstrate that our method outperforms existing algorithms.",1
"We investigate time-dependent data analysis from the perspective of recurrent kernel machines, from which models with hidden units and gated memory cells arise naturally. By considering dynamic gating of the memory cell, a model closely related to the long short-term memory (LSTM) recurrent neural network is derived. Extending this setup to $n$-gram filters, the convolutional neural network (CNN), Gated CNN, and recurrent additive network (RAN) are also recovered as special cases. Our analysis provides a new perspective on the LSTM, while also extending it to $n$-gram convolutional filters. Experiments are performed on natural language processing tasks and on analysis of local field potentials (neuroscience). We demonstrate that the variants we derive from kernels perform on par or even better than traditional neural methods. For the neuroscience application, the new models demonstrate significant improvements relative to the prior state of the art.",0
"The focus of our investigation is on analyzing time-dependent data using recurrent kernel machines, which naturally give rise to models with hidden units and gated memory cells. By incorporating dynamic gating of the memory cell, we derive a model that is closely related to the long short-term memory (LSTM) recurrent neural network. We further extend this approach to include $n$-gram filters, which allows us to recover the convolutional neural network (CNN), Gated CNN, and recurrent additive network (RAN) as special cases. Our analysis not only provides a fresh perspective on the LSTM but also extends it to $n$-gram convolutional filters. We evaluate the performance of our models on natural language processing tasks and the analysis of local field potentials in neuroscience. Our experiments demonstrate that the models derived from kernels perform equally well or even better than traditional neural methods. Furthermore, our new models show significant improvements over the prior state of the art for the neuroscience application.",1
"Interactive programming with interleaved code snippet cells and natural language markdown is recently gaining popularity in the form of Jupyter notebooks, which accelerate prototyping and collaboration. To study code generation conditioned on a long context history, we present JuICe, a corpus of 1.5 million examples with a curated test set of 3.7K instances based on online programming assignments. Compared with existing contextual code generation datasets, JuICe provides refined human-curated data, open-domain code, and an order of magnitude more training data. Using JuICe, we train models for two tasks: (1) generation of the API call sequence in a code cell, and (2) full code cell generation, both conditioned on the NL-Code history up to a particular code cell. Experiments using current baseline code generation models show that both context and distant supervision aid in generation, and that the dataset is challenging for current systems.",0
"Jupyter notebooks have become increasingly popular for interactive programming, as they allow for interleaved code snippet cells and natural language markdown, enabling faster prototyping and collaboration. To investigate code generation conditioned on extensive context history, we introduce JuICe, a corpus of 1.5 million examples and a curated test set of 3.7K instances derived from online programming assignments. Compared to existing contextual code generation datasets, JuICe offers refined human-curated data, open-domain code, and significantly more training data. Our study with JuICe focuses on two tasks: (1) generating the API call sequence in a code cell, and (2) full code cell generation, both based on the NL-Code history up to a specific code cell. Our experiments, using existing baseline code generation models, demonstrate that context and distant supervision can improve generation, and that the dataset poses a challenge for current systems.",1
"ICU readmission is associated with longer hospitalization, mortality and adverse outcomes. An early recognition of ICU re-admission can help prevent patients from worse situation and lower treatment cost. As the abundance of Electronics Health Records (EHR), it is popular to design clinical decision tools with machine learning technique manipulating on healthcare large scale data. We designed data-driven predictive models to estimate the risk of ICU readmission. The discharge summary of each hospital admission was carefully represented by natural language processing techniques. Unified Medical Language System (UMLS) was further used to standardize inconsistency of discharge summaries. 5 machine learning classifiers were adopted to construct predictive models. The best configuration yielded a competitive AUC of 0.748. Our work suggests that natural language processing of discharge summaries is capable to send clinicians warning of unplanned 30-day readmission upon discharge.",0
"The occurrence of ICU readmission is linked to extended hospital stays, mortality, and unfavorable results. Early detection of ICU re-entry can prevent patients from worsening and reduce treatment expenses. With the abundance of Electronic Health Records (EHR), it is popular to create clinical decision tools employing machine learning techniques that manipulate vast healthcare data. Our study involved designing data-driven predictive models to estimate the risk of ICU readmission. We used natural language processing techniques to carefully represent the discharge summary of each hospital admission, and the Unified Medical Language System (UMLS) was used to standardize inconsistencies in discharge summaries. Five machine learning classifiers were utilized to construct predictive models, with the best configuration resulting in an impressive AUC of 0.748. Our findings suggest that natural language processing of discharge summaries can provide clinicians with a warning of unplanned 30-day readmission upon discharge.",1
"This paper explores a novel approach to achieving emergent compositional communication in multi-agent systems. We propose a training regime implementing template transfer, the idea of carrying over learned biases across contexts. In our method, a sender-receiver pair is first trained with disentangled loss functions and then the receiver is transferred to train a new sender with a standard loss. Unlike other methods (e.g. the obverter algorithm), our approach does not require imposing inductive biases on the architecture of the agents. We experimentally show the emergence of compositional communication using topographical similarity, zero-shot generalization and context independence as evaluation metrics. The presented approach is connected to an important line of work in semiotics and developmental psycholinguistics: it supports a conjecture that compositional communication is scaffolded on simpler communication protocols.",0
"In this article, a new method for achieving emergent compositional communication in multi-agent systems is explored. The proposed training regime utilizes template transfer, which involves transferring learned biases across contexts. In this approach, a sender-receiver pair is initially trained using disentangled loss functions, and then the receiver is transferred to train a new sender with a standard loss. Unlike other methods such as the obverter algorithm, this approach does not require imposing inductive biases on the architecture of the agents. The emergence of compositional communication is experimentally demonstrated using topographical similarity, zero-shot generalization, and context independence as evaluation metrics. This approach aligns with the idea that compositional communication is scaffolded on simpler communication protocols, which is an important line of work in semiotics and developmental psycholinguistics.",1
"Deep learning models have achieved huge success in numerous fields, such as computer vision and natural language processing. However, unlike such fields, it is hard to apply traditional deep learning models on the graph data due to the 'node-orderless' property. Normally, adjacency matrices will cast an artificial and random node-order on the graphs, which renders the performance of deep models on graph classification tasks extremely erratic, and the representations learned by such models lack clear interpretability. To eliminate the unnecessary node-order constraint, we propose a novel model named Isomorphic Neural Network (IsoNN), which learns the graph representation by extracting its isomorphic features via the graph matching between input graph and templates. IsoNN has two main components: graph isomorphic feature extraction component and classification component. The graph isomorphic feature extraction component utilizes a set of subgraph templates as the kernel variables to learn the possible subgraph patterns existing in the input graph and then computes the isomorphic features. A set of permutation matrices is used in the component to break the node-order brought by the matrix representation. Three fully-connected layers are used as the classification component in IsoNN. Extensive experiments are conducted on benchmark datasets, the experimental results can demonstrate the effectiveness of ISONN, especially compared with both classic and state-of-the-art graph classification methods.",0
"Numerous fields, including computer vision and natural language processing, have seen immense success with deep learning models. However, applying traditional deep learning models to graph data is challenging due to its ""node-orderless"" property. Adjacency matrices impose an artificial and random node-order, leading to erratic performance and unclear interpretability of learned representations in graph classification tasks. To address this constraint, we propose a novel model, Isomorphic Neural Network (IsoNN), which extracts isomorphic features via graph matching between input graphs and templates. IsoNN comprises a graph isomorphic feature extraction component and a classification component. The former employs a set of subgraph templates to learn possible subgraph patterns and compute isomorphic features, using permutation matrices to eliminate node-order. The latter includes three fully-connected layers. Extensive experiments on benchmark datasets demonstrate the effectiveness of IsoNN, surpassing both classic and state-of-the-art graph classification methods.",1
"Neural text generation is a key tool in natural language applications, but it is well known there are major problems at its core. In particular, standard likelihood training and decoding leads to dull and repetitive outputs. While some post-hoc fixes have been proposed, in particular top-$k$ and nucleus sampling, they do not address the fact that the token-level probabilities predicted by the model are poor. In this paper we show that the likelihood objective itself is at fault, resulting in a model that assigns too much probability to sequences containing repeats and frequent words, unlike those from the human training distribution. We propose a new objective, unlikelihood training, which forces unlikely generations to be assigned lower probability by the model. We show that both token and sequence level unlikelihood training give less repetitive, less dull text while maintaining perplexity, giving superior generations using standard greedy or beam search. According to human evaluations, our approach with standard beam search also outperforms the currently popular decoding methods of nucleus sampling or beam blocking, thus providing a strong alternative to existing techniques.",0
"Natural language applications heavily rely on neural text generation, but it is widely recognized that there are significant issues with this tool. Specifically, the use of standard likelihood training and decoding often results in uninteresting and repetitive outputs. Although some solutions, such as top-$k$ and nucleus sampling, have been proposed to address this problem, they fail to improve the token-level probabilities generated by the model. Our research demonstrates that the likelihood objective itself is problematic, as it tends to overemphasize repeated and common words, unlike human training data. To solve this issue, we introduce a novel objective called unlikelihood training, which downgrades the probability of unlikely text generations. Our approach, which incorporates both token and sequence-level unlikelihood training, produces text that is less repetitive and more engaging, while still maintaining perplexity and generating superior results with standard greedy or beam search. Moreover, our method outperforms popular decoding techniques like nucleus sampling or beam blocking according to human evaluations, providing a more effective alternative to existing approaches.",1
"One of the primary challenges of visual storytelling is developing techniques that can maintain the context of the story over long event sequences to generate human-like stories. In this paper, we propose a hierarchical deep learning architecture based on encoder-decoder networks to address this problem. To better help our network maintain this context while also generating long and diverse sentences, we incorporate natural language image descriptions along with the images themselves to generate each story sentence. We evaluate our system on the Visual Storytelling (VIST) dataset and show that our method outperforms state-of-the-art techniques on a suite of different automatic evaluation metrics. The empirical results from this evaluation demonstrate the necessities of different components of our proposed architecture and shows the effectiveness of the architecture for visual storytelling.",0
"The primary obstacle of visual storytelling is creating techniques that can sustain the story's context during lengthy event sequences to generate stories that resemble human narratives. This paper proposes a hierarchical deep learning structure that employs encoder-decoder networks to tackle this issue. To enhance the network's ability to preserve context while producing a variety of long sentences, we integrate natural language image descriptions with the images themselves to generate each story sentence. Our system is assessed on the Visual Storytelling (VIST) dataset, and we demonstrate that our approach outperforms current techniques across a range of automated evaluation metrics. The empirical findings from this evaluation highlight the significance of each component in our suggested structure and show its effectiveness for visual storytelling.",1
"We present SentiMATE, a novel end-to-end Deep Learning model for Chess, employing Natural Language Processing that aims to learn an effective evaluation function assessing move quality. This function is pre-trained on the sentiment of commentary associated with the training moves and is used to guide and optimize the agent's game-playing decision making. The contributions of this research are three-fold: we build and put forward both a classifier which extracts commentary describing the quality of Chess moves in vast commentary datasets, and a Sentiment Analysis model trained on Chess commentary to accurately predict the quality of said moves, to then use those predictions to evaluate the optimal next move of a Chess agent. Both classifiers achieve over 90 % classification accuracy. Lastly, we present a Chess engine, SentiMATE, which evaluates Chess moves based on a pre-trained sentiment evaluation function. Our results exhibit strong evidence to support our initial hypothesis - ""Can Natural Language Processing be used to train a novel and sample efficient evaluation function in Chess Engines?"" - as we integrate our evaluation function into modern Chess engines and play against agents with traditional Chess move evaluation functions, beating both random agents and a DeepChess implementation at a level-one search depth - representing the number of moves a traditional Chess agent (employing the alpha-beta search algorithm) looks ahead in order to evaluate a given chess state.",0
"Introducing SentiMATE, a novel Deep Learning model designed for Chess that utilizes Natural Language Processing to create an effective evaluation function for assessing move quality. The evaluation function is pre-trained on commentary sentiment linked to training moves and is used to guide and optimize the decision-making process of the Chess agent during gameplay. This research presents three contributions, which include the development of a classifier for extracting commentary descriptions of move quality from vast datasets, a Sentiment Analysis model trained on Chess commentary for accurately predicting move quality, and the creation of SentiMATE, a Chess engine that evaluates moves based on a pre-trained sentiment evaluation function. Both classifiers achieved over 90% accuracy. The study's hypothesis, ""Can Natural Language Processing be used to train a novel and sample efficient evaluation function in Chess Engines?"" is supported by the results, which show that our evaluation function can be integrated into modern Chess engines and outperform traditional Chess move evaluation functions when playing against random agents and a DeepChess implementation at a level-one search depth.",1
"In Vision-and-Language Navigation (VLN), an embodied agent needs to reach a target destination with the only guidance of a natural language instruction. To explore the environment and progress towards the target location, the agent must perform a series of low-level actions, such as rotate, before stepping ahead. In this paper, we propose to exploit dynamic convolutional filters to encode the visual information and the lingual description in an efficient way. Differently from some previous works that abstract from the agent perspective and use high-level navigation spaces, we design a policy which decodes the information provided by dynamic convolution into a series of low-level, agent friendly actions. Results show that our model exploiting dynamic filters performs better than other architectures with traditional convolution, being the new state of the art for embodied VLN in the low-level action space. Additionally, we attempt to categorize recent work on VLN depending on their architectural choices and distinguish two main groups: we call them low-level actions and high-level actions models. To the best of our knowledge, we are the first to propose this analysis and categorization for VLN.",0
"The task of Vision-and-Language Navigation (VLN) requires an embodied agent to navigate to a specified destination using only natural language instructions. To achieve this, the agent must execute a sequence of low-level actions such as rotation and stepping forward. In this study, we propose using dynamic convolutional filters to encode visual information and natural language descriptions in an efficient manner. Our approach differs from previous works that use high-level navigation spaces and abstract from the agent's perspective. Instead, we design a policy that decodes dynamic convolution information into low-level, agent-friendly actions. Our results show that our model outperforms traditional convolutional architectures, making it the new state-of-the-art for embodied VLN in the low-level action space. Furthermore, we provide a categorization of recent VLN work based on their architectural choices, distinguishing between low-level actions and high-level actions models. To the best of our knowledge, this is the first analysis and categorization proposed for VLN.",1
"Overparameterized transformer networks have obtained state of the art results in various natural language processing tasks, such as machine translation, language modeling, and question answering. These models contain hundreds of millions of parameters, necessitating a large amount of computation and making them prone to overfitting. In this work, we explore LayerDrop, a form of structured dropout, which has a regularization effect during training and allows for efficient pruning at inference time. In particular, we show that it is possible to select sub-networks of any depth from one large network without having to finetune them and with limited impact on performance. We demonstrate the effectiveness of our approach by improving the state of the art on machine translation, language modeling, summarization, question answering, and language understanding benchmarks. Moreover, we show that our approach leads to small BERT-like models of higher quality compared to training from scratch or using distillation.",0
"Various natural language processing tasks, such as machine translation, language modeling, and question answering, have seen state of the art results from overparameterized transformer networks. However, these models have hundreds of millions of parameters, which can lead to overfitting and require a significant amount of computation. This paper explores LayerDrop, a type of structured dropout that offers regularization during training and allows for efficient pruning during inference. The research demonstrates that sub-networks of any depth can be selected from one large network without the need for finetuning, with little impact on performance. The effectiveness of this approach is highlighted by improved results on machine translation, language modeling, summarization, question answering, and language understanding benchmarks. Additionally, the paper shows that this approach leads to small BERT-like models of higher quality compared to training from scratch or using distillation.",1
"The growing ubiquity of Social Media data offers an attractive perspective for improving the quality of machine learning-based models in several fields, ranging from Computer Vision to Natural Language Processing. In this paper we focus on Facebook posts paired with reactions of multiple users, and we investigate their relationships with classes of emotions that are typically considered in the task of emotion detection. We are inspired by the idea of introducing a connection between reactions and emotions by means of First-Order Logic formulas, and we propose an end-to-end neural model that is able to jointly learn to detect emotions and predict Facebook reactions in a multi-task environment, where the logic formulas are converted into polynomial constraints. Our model is trained using a large collection of unsupervised texts together with data labeled with emotion classes and Facebook posts that include reactions. An extended experimental analysis that leverages a large collection of Facebook posts shows that the tasks of emotion classification and reaction prediction can both benefit from their interaction.",0
"The increasing prevalence of Social Media data presents an enticing opportunity to enhance the quality of machine learning-driven models in various fields, such as Computer Vision and Natural Language Processing. This study centers on Facebook posts, combined with reactions from multiple users, and explores their connections to different emotional categories typically used in emotion detection. Our approach is founded on linking reactions and emotions by using First-Order Logic formulas, and we propose a neural model that can simultaneously learn to identify emotions and anticipate Facebook reactions within a multi-task setting. In this model, logical formulas are transformed into polynomial constraints. We train the model using a vast pool of unsupervised texts, along with labeled data on emotions and Facebook posts that contain reactions. Our comprehensive experimental analysis, based on a sizable set of Facebook posts, reveals that the interaction between the tasks of emotion classification and reaction prediction can be mutually beneficial.",1
"This study compares the effectiveness and robustness of multi-class categorization of Amazon product data using transfer learning on pre-trained contextualized language models. Specifically, we fine-tuned BERT and XLNet, two bidirectional models that have achieved state-of-the-art performance on many natural language tasks and benchmarks, including text classification. While existing classification studies and benchmarks focus on binary targets, with the exception of ordinal ranking tasks, here we examine the robustness of such models as the number of classes grows from 1 to 20. Our experiments demonstrate an approximately linear decrease in performance metrics (i.e., precision, recall, $F_1$ score, and accuracy) with the number of class labels. BERT consistently outperforms XLNet using identical hyperparameters on the entire range of class label quantities for categorizing products based on their textual descriptions. BERT is also more affordable than XLNet in terms of the computational cost (i.e., time and memory) required for training. In all cases studied, the performance degradation rates were estimated to be 1% per additional class label.",0
"This research study evaluates how well pre-trained contextualized language models perform in multi-class categorization of Amazon product data using transfer learning. BERT and XLNet, two bidirectional models with exceptional performance in natural language tasks, were fine-tuned and compared. Unlike previous studies, which focused on binary targets, our study examined the robustness of these models as the number of classes increased up to 20. Our experiments revealed a linear decrease in performance metrics, including precision, recall, $F_1$ score, and accuracy, as the number of class labels increased. BERT consistently outperformed XLNet in all cases studied, and it required less computational cost in terms of time and memory for training. We estimated that the performance degradation rates were approximately 1% per additional class label.",1
"Existing methods using generative adversarial approaches for Zero-Shot Learning (ZSL) aim to generate realistic visual features from class semantics by a single generative network, which is highly under-constrained. As a result, the previous methods cannot guarantee that the generated visual features can truthfully reflect the corresponding semantics. To address this issue, we propose a novel method named Cycle-consistent Adversarial Networks for Zero-Shot Learning (CANZSL). It encourages a visual feature generator to synthesize realistic visual features from semantics, and then inversely translate back synthesized the visual feature to corresponding semantic space by a semantic feature generator. Furthermore, in this paper a more challenging and practical ZSL problem is considered where the original semantics are from natural language with irrelevant words instead of clean semantics that are widely used in previous work. Specifically, a multi-modal consistent bidirectional generative adversarial network is trained to handle unseen instances by leveraging noise in the natural language. A forward one-to-many mapping from one text description to multiple visual features is coupled with an inverse many-to-one mapping from the visual space to the semantic space. Thus, a multi-modal cycle-consistency loss between the synthesized semantic representations and the ground truth can be learned and leveraged to enforce the generated semantic features to approximate to the real distribution in semantic space. Extensive experiments are conducted to demonstrate that our method consistently outperforms state-of-the-art approaches on natural language-based zero-shot learning tasks.",0
"Previous methods utilizing generative adversarial techniques for Zero-Shot Learning (ZSL) have been focused on generating realistic visual features from class semantics using a single generative network, which is highly under-constrained. As a result, these methods cannot ensure that the generated visual features accurately reflect the corresponding semantics. To overcome this problem, we propose a new method called Cycle-consistent Adversarial Networks for Zero-Shot Learning (CANZSL). This method encourages a visual feature generator to produce realistic visual features from semantics and then translates them back to the corresponding semantic space using a semantic feature generator. Additionally, we address a more challenging and practical ZSL issue in this paper, where the original semantics contain irrelevant words and are from natural language. A multi-modal consistent bidirectional generative adversarial network is trained to handle unseen instances by leveraging noise in the natural language. The method uses a forward one-to-many mapping and an inverse many-to-one mapping to couple the visual space to the semantic space. A multi-modal cycle-consistency loss is used to enforce the generated semantic features to approximate to the real distribution in semantic space. Our method consistently outperforms state-of-the-art approaches on natural language-based zero-shot learning tasks, as demonstrated by extensive experiments.",1
"Information in electronic health records (EHR), such as clinical narratives, examination reports, lab measurements, demographics, and other patient encounter entries, can be transformed into appropriate data representations that can be used for downstream clinical machine learning tasks using representation learning. Learning better representations is critical to improve the performance of downstream tasks. Due to the advances in machine learning, we now can learn better and meaningful representations from EHR through disentangling the underlying factors inside data and distilling large amounts of information and knowledge from heterogeneous EHR sources. In this chapter, we first introduce the background of learning representations and reasons why we need good EHR representations in machine learning for medicine and healthcare in Section 1. Next, we explain the commonly-used machine learning and evaluation methods for representation learning using a deep learning approach in Section 2. Following that, we review recent related studies of learning patient state representation from EHR for clinical machine learning tasks in Section 3. Finally, in Section 4 we discuss more techniques, studies, and challenges for learning natural language representations when free texts, such as clinical notes, examination reports, or biomedical literature are used. We also discuss challenges and opportunities in these rapidly growing research fields.",0
"The data stored in electronic health records (EHRs), including clinical narratives, exam reports, lab results, demographics, and other patient information, can be transformed into appropriate data representations for clinical machine learning tasks using representation learning. Learning better representations is crucial to enhance downstream task performance. Recent advances in machine learning have made it possible to extract meaningful representations from heterogeneous EHR sources by separating underlying factors and extracting large amounts of data. In this chapter, we explore the importance of good EHR representations in machine learning for healthcare and medicine, followed by an explanation of commonly used machine learning and evaluation methods for representation learning using a deep learning approach. We also review recent studies on learning patient state representation from EHR for clinical machine learning tasks and discuss techniques, studies, and challenges for learning natural language representations when working with free text, such as clinical notes and biomedical literature. Finally, we explore the challenges and opportunities in these rapidly growing research fields.",1
"Standard autoregressive seq2seq models are easily trained by max-likelihood, but tend to show poor results under small-data conditions. We introduce a class of seq2seq models, GAMs (Global Autoregressive Models), which combine an autoregressive component with a log-linear component, allowing the use of global \textit{a priori} features to compensate for lack of data. We train these models in two steps. In the first step, we obtain an \emph{unnormalized} GAM that maximizes the likelihood of the data, but is improper for fast inference or evaluation. In the second step, we use this GAM to train (by distillation) a second autoregressive model that approximates the \emph{normalized} distribution associated with the GAM, and can be used for fast inference and evaluation. Our experiments focus on language modelling under synthetic conditions and show a strong perplexity reduction of using the second autoregressive model over the standard one.",0
"Seq2seq models that rely solely on autoregression and are trained using max-likelihood may not perform well when dealing with small data. To address this issue, we propose a new class of seq2seq models called Global Autoregressive Models (GAMs). GAMs merge an autoregressive component with a log-linear component, which allows us to incorporate global a priori features to compensate for the lack of data. We train GAMs in two steps. First, we obtain an unnormalized GAM that maximizes the likelihood of the data but cannot be used for fast inference or evaluation. Then, we use this GAM to train a second autoregressive model via distillation that approximates the normalized distribution associated with the GAM. This second model can be used for fast inference and evaluation. We evaluated our method in language modelling under synthetic conditions and observed a significant reduction in perplexity when using the second autoregressive model compared to the standard one.",1
"Systems that can associate images with their spoken audio captions are an important step towards visually grounded language learning. We describe a scalable method to automatically generate diverse audio for image captioning datasets. This supports pretraining deep networks for encoding both audio and images, which we do via a dual encoder that learns to align latent representations from both modalities. We show that a masked margin softmax loss for such models is superior to the standard triplet loss. We fine-tune these models on the Flickr8k Audio Captions Corpus and obtain state-of-the-art results---improving recall in the top 10 from 29.6% to 49.5%. We also obtain human ratings on retrieval outputs to better assess the impact of incidentally matching image-caption pairs that were not associated in the data, finding that automatic evaluation substantially underestimates the quality of the retrieved results.",0
"Developing systems that can match images with their corresponding audio captions is a crucial advancement in language learning that relies on visual cues. This article outlines a method for generating diverse audio for image captioning datasets in a scalable manner. By using a dual encoder that can align latent representations from both audio and images, deep networks can be pretrained to encode both modalities. Our research shows that using a masked margin softmax loss is more effective than the standard triplet loss for these models. After fine-tuning our models on the Flickr8k Audio Captions Corpus, we achieved state-of-the-art results, improving the recall in the top 10 from 29.6% to 49.5%. Additionally, we conducted human ratings on retrieval outputs to better evaluate the quality of incidentally matching image-caption pairs that were not associated in the data. Our findings indicate that automatic evaluation methods underestimate the quality of the retrieved results.",1
"Personal assistant AI systems such as Siri, Cortana, and Alexa have become widely used as a means to accomplish tasks through natural language commands. However, components in these systems generally rely on supervised machine learning algorithms that require large amounts of hand-annotated training data, which is expensive and time consuming to collect. The ability to incorporate unsupervised, weakly supervised, or distantly supervised data holds significant promise in overcoming this bottleneck. In this paper, we describe a framework that leverages user engagement signals (user behaviors that demonstrate a positive or negative response to content) to automatically create granular entity labels for training data augmentation. Strategies such as multi-task learning and validation using an external knowledge base are employed to incorporate the engagement annotated data and to boost the model's accuracy on a sequence labeling task. Our results show that learning from data automatically labeled by user engagement signals achieves significant accuracy gains in a production deep learning system, when measured on both the sequence labeling task as well as on user facing results produced by the system end-to-end. We believe this is the first use of user engagement signals to help generate training data for a sequence labeling task on a large scale, and can be applied in practical settings to speed up new feature deployment when little human annotated data is available.",0
"AI personal assistants such as Siri, Cortana, and Alexa have become increasingly popular for completing tasks through natural language commands. However, these systems usually rely on supervised machine learning algorithms, which require extensive amounts of hand-annotated training data that is both expensive and time-consuming to collect. To address this issue, the use of unsupervised, weakly supervised, or distantly supervised data has significant potential. This paper presents a framework that utilizes user engagement signals to automatically generate granular entity labels for training data augmentation. This approach includes multi-task learning and external knowledge base validation to incorporate the engagement annotated data and improve the model's accuracy. The results demonstrate that learning from data labeled by user engagement signals leads to significant accuracy improvements in a production deep learning system, both in the sequence labeling task and in user-facing results. This is the first instance of using user engagement signals to generate training data for a sequence labeling task on a large scale, and it can be applied in practical settings to expedite new feature deployment when annotated data is scarce.",1
"Referring expression comprehension aims to locate the object instance described by a natural language referring expression in an image. This task is compositional and inherently requires visual reasoning on top of the relationships among the objects in the image. Meanwhile, the visual reasoning process is guided by the linguistic structure of the referring expression. However, existing approaches treat the objects in isolation or only explore the first-order relationships between objects without being aligned with the potential complexity of the expression. Thus it is hard for them to adapt to the grounding of complex referring expressions. In this paper, we explore the problem of referring expression comprehension from the perspective of language-driven visual reasoning, and propose a dynamic graph attention network to perform multi-step reasoning by modeling both the relationships among the objects in the image and the linguistic structure of the expression. In particular, we construct a graph for the image with the nodes and edges corresponding to the objects and their relationships respectively, propose a differential analyzer to predict a language-guided visual reasoning process, and perform stepwise reasoning on top of the graph to update the compound object representation at every node. Experimental results demonstrate that the proposed method can not only significantly surpass all existing state-of-the-art algorithms across three common benchmark datasets, but also generate interpretable visual evidences for stepwisely locating the objects referred to in complex language descriptions.",0
"The objective of referring expression comprehension is to identify the particular object instance portrayed by a natural language referring expression in an image. This task requires visual reasoning, which is compositional and necessitates an understanding of relationships among objects in the image. However, current approaches only consider objects individually or examine first-order relationships between them, failing to align with the complexity of the expression and impeding the comprehension of complex referring expressions. This study proposes a language-driven visual reasoning approach that utilizes a dynamic graph attention network to perform multi-step reasoning by modeling both the relationships among the objects in the image and the linguistic structure of the expression. The method constructs a graph for the image, predicts a language-guided visual reasoning process using a differential analyzer, and performs stepwise reasoning to update the compound object representation at each node. The proposed approach outperforms existing state-of-the-art algorithms and generates interpretable visual evidence for locating objects referred to in complex language descriptions.",1
"With the widespread use of AI systems and applications in our everyday lives, it is important to take fairness issues into consideration while designing and engineering these types of systems. Such systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that the decisions do not reflect discriminatory behavior toward certain groups or populations. We have recently seen work in machine learning, natural language processing, and deep learning that addresses such challenges in different subdomains. With the commercialization of these systems, researchers are becoming aware of the biases that these applications can contain and have attempted to address them. In this survey we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined in order to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and how they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.",0
"Incorporating fairness considerations is crucial when designing and engineering AI systems, which are increasingly prevalent in our daily lives and have the potential to make significant and life-altering decisions. It is imperative to ensure that such decisions are not discriminatory towards specific groups or populations. Recent work in machine learning, natural language processing, and deep learning has attempted to address these challenges in various subdomains, and researchers are becoming more aware of the biases that these applications may contain. This survey examines real-world applications that have shown biases, identifies sources of bias that may affect AI applications, and proposes a taxonomy for fairness definitions to avoid existing bias. The survey also explores different domains and subdomains in AI, highlighting instances of unfair outcomes in state-of-the-art methods and how researchers have attempted to address them. While there remain many future directions and solutions for mitigating bias in AI systems, we hope this survey will inspire researchers to address these issues in their respective fields.",1
"Recently, generating adversarial examples has become an important means of measuring robustness of a deep learning model. Adversarial examples help us identify the susceptibilities of the model and further counter those vulnerabilities by applying adversarial training techniques. In natural language domain, small perturbations in the form of misspellings or paraphrases can drastically change the semantics of the text. We propose a reinforcement learning based approach towards generating adversarial examples in black-box settings. We demonstrate that our method is able to fool well-trained models for (a) IMDB sentiment classification task and (b) AG's news corpus news categorization task with significantly high success rates. We find that the adversarial examples generated are semantics-preserving perturbations to the original text.",0
"Generating adversarial examples has emerged as a crucial method for evaluating the resilience of deep learning models. Such examples enable us to pinpoint the model's vulnerabilities and implement adversarial training techniques to overcome them. In the context of natural language, even minor alterations like misspellings or paraphrases can dramatically alter the text's meaning. To address this, we propose a reinforcement learning-based method for generating adversarial examples in black-box settings. Our approach successfully deceives well-trained models for both IMDB sentiment classification and AG's news corpus news categorization tasks. Notably, we observe that the generated adversarial examples are semantically coherent modifications of the original text.",1
"Neural sequence generation is typically performed token-by-token and left-to-right. Whenever a token is generated only previously produced tokens are taken into consideration. In contrast, for problems such as sequence classification, bidirectional attention, which takes both past and future tokens into consideration, has been shown to perform much better. We propose to make the sequence generation process bidirectional by employing special placeholder tokens. Treated as a node in a fully connected graph, a placeholder token can take past and future tokens into consideration when generating the actual output token. We verify the effectiveness of our approach experimentally on two conversational tasks where the proposed bidirectional model outperforms competitive baselines by a large margin.",0
"The usual method for neural sequence generation involves generating tokens one at a time and in a left-to-right manner. Only the tokens generated thus far are considered when creating a new token. Conversely, bidirectional attention, which considers both past and future tokens, has proven to be more effective for sequence classification. To make the sequence generation process bidirectional, we recommend using special placeholder tokens. These tokens can be treated as nodes in a fully connected graph and can take into account both past and future tokens when generating the actual output token. We conducted experiments on two conversational tasks and found that our proposed bidirectional model significantly outperformed other competing models.",1
"We present a machine learning pipeline and model that uses the entire uncurated EHR for prediction of in-hospital mortality at arbitrary time intervals, using all available chart, lab and output events, without the need for pre-processing or feature engineering. Data for more than 45,000 American ICU patients from the MIMIC-III database were used to develop an ICU mortality prediction model. All chart, lab and output events were treated by the model in the same manner inspired by Natural Language Processing (NLP). Patient events were discretized by percentile and mapped to learnt embeddings before being passed to a Recurrent Neural Network (RNN) to provide early prediction of in-patient mortality risk. We compared mortality predictions with the Simplified Acute Physiology Score II (SAPS II) and the Oxford Acute Severity of Illness Score (OASIS). Data were split into an independent test set (10%) and a ten-fold cross-validation was carried out during training to avoid overfitting. 13,233 distinct variables with heterogeneous data types were included without manual selection or pre-processing. Recordings in the first few hours of a patient's stay were found to be strongly predictive of mortality, outperforming models using SAPS II and OASIS scores within just 2 hours and achieving a state of the art Area Under the Receiver Operating Characteristic (AUROC) value of 0.80 (95% CI 0.79-0.80) at 12 hours vs 0.70 and 0.66 for SAPS II and OASIS at 24 hours respectively. Our model achieves a very strong performance of AUROC 0.86 (95% CI 0.85-0.86) for in-patient mortality prediction after 48 hours on the MIMIC-III dataset. Predictive performance increases over the first 48 hours of the ICU stay, but suffers from diminishing returns, providing rationale for time-limited trials of critical care and suggesting that the timing of decision making can be optimised and individualised.",0
"Our study introduces a machine learning pipeline and model that utilizes the complete, unprocessed electronic health records (EHR) to predict in-hospital mortality at any time interval. The model employs all available chart, lab, and output events and does not require any pre-processing or feature engineering. We employed data from over 45,000 American ICU patients in the MIMIC-III database to develop an ICU mortality prediction model. The model treated all chart, lab, and output events equally, inspired by Natural Language Processing (NLP). Patient events were discretized by percentile and mapped to learned embeddings before being passed to a Recurrent Neural Network (RNN) to provide early prediction of in-patient mortality risk. We compared our model's predictions to those of the Simplified Acute Physiology Score II (SAPS II) and the Oxford Acute Severity of Illness Score (OASIS). During training, we conducted a ten-fold cross-validation to avoid overfitting, and an independent test set (10%) was used for validation. Our approach included over 13,000 distinct variables with various data types without manual selection or pre-processing. Our results indicated that the recordings from the first few hours of a patient's stay were strongly predictive of mortality, outperforming SAPS II and OASIS scores within just two hours. Our model achieved a state-of-the-art Area Under the Receiver Operating Characteristic (AUROC) value of 0.80 (95% CI 0.79-0.80) at 12 hours, compared to 0.70 and 0.66 for SAPS II and OASIS at 24 hours, respectively. Additionally, our model achieved a strong performance of AUROC 0.86 (95% CI 0.85-0.86) for in-patient mortality prediction after 48 hours on the MIMIC-III dataset. Our findings suggest that predictive performance increases over the first 48 hours of the ICU stay but diminishes after that period. This highlights the importance of time-limited trials of critical care and individualized decision-making.",1
"Automatic question generation is an important problem in natural language processing. In this paper we propose a novel adaptive copying recurrent neural network model to tackle the problem of question generation from sentences and paragraphs. The proposed model adds a copying mechanism component onto a bidirectional LSTM architecture to generate more suitable questions adaptively from the input data. Our experimental results show the proposed model can outperform the state-of-the-art question generation methods in terms of BLEU and ROUGE evaluation scores.",0
"The problem of generating questions automatically is significant in the field of natural language processing. Our paper presents a new and adaptable copying recurrent neural network model that aims to address this issue by generating questions from sentences and paragraphs. With the addition of a copying mechanism component to a bidirectional LSTM architecture, the model can produce more appropriate questions according to the input data. In our experiments, the proposed model demonstrated superior performance compared to other question generation methods, as evaluated by BLEU and ROUGE scores.",1
"Image captioning models are usually evaluated on their ability to describe a held-out set of images, not on their ability to generalize to unseen concepts. We study the problem of compositional generalization, which measures how well a model composes unseen combinations of concepts when describing images. State-of-the-art image captioning models show poor generalization performance on this task. We propose a multi-task model to address the poor performance, that combines caption generation and image--sentence ranking, and uses a decoding mechanism that re-ranks the captions according their similarity to the image. This model is substantially better at generalizing to unseen combinations of concepts compared to state-of-the-art captioning models.",0
"The typical way to evaluate image captioning models is based on their ability to describe a set of images that have not been previously seen, rather than their capacity to apply knowledge to new concepts. Our research focuses on the issue of compositional generalization, which measures how well a model can create descriptions that combine new concepts when presented with images. Current advanced image captioning models have demonstrated weak performance in this regard. To address this problem, we propose a multi-task model that incorporates both caption generation and image-sentence ranking, and employs a decoding mechanism that prioritizes captions based on their similarity to the image. Our model outperforms state-of-the-art captioning models significantly in its ability to generalize to new combinations of concepts.",1
"NeMo (Neural Modules) is a Python framework-agnostic toolkit for creating AI applications through re-usability, abstraction, and composition. NeMo is built around neural modules, conceptual blocks of neural networks that take typed inputs and produce typed outputs. Such modules typically represent data layers, encoders, decoders, language models, loss functions, or methods of combining activations. NeMo makes it easy to combine and re-use these building blocks while providing a level of semantic correctness checking via its neural type system. The toolkit comes with extendable collections of pre-built modules for automatic speech recognition and natural language processing. Furthermore, NeMo provides built-in support for distributed training and mixed precision on latest NVIDIA GPUs. NeMo is open-source https://github.com/NVIDIA/NeMo",0
"NeMo (Neural Modules) is a toolkit in Python that is not specific to any framework and can be used to create AI applications through reusability, abstraction, and composition. The core of NeMo is based on neural modules, which are blocks of neural networks that receive typed inputs and produce typed outputs. These modules are usually used to represent data layers, encoders, decoders, language models, loss functions, or methods of combining activations. NeMo facilitates the process of combining and reusing these building blocks, while ensuring semantic correctness through its neural type system. The toolkit also includes pre-built collections of modules for automatic speech recognition and natural language processing, and supports distributed training and mixed precision on the latest NVIDIA GPUs. NeMo is open-source and available on https://github.com/NVIDIA/NeMo.",1
"Finding an optimal assignment between two sets of objects is a fundamental problem arising in many applications, including the matching of `bag-of-words' representations in natural language processing and computer vision. Solving the assignment problem typically requires cubic time and its pairwise computation is expensive on large datasets. In this paper, we develop an algorithm which can find an optimal assignment in linear time when the cost function between objects is represented by a tree distance. We employ the method to approximate the edit distance between two graphs by matching their vertices in linear time. To this end, we propose two tree distances, the first of which reflects discrete and structural differences between vertices, and the second of which can be used to compare continuous labels. We verify the effectiveness and efficiency of our methods using synthetic and real-world datasets.",0
"The task of determining the best match between two sets of objects is a crucial issue that arises in various domains such as natural language processing and computer vision, where ""bag-of-words"" representations are used. However, solving this problem is usually time-consuming as it requires cubic time, and the pairwise computation can be expensive when dealing with large datasets. This study aims to present a new algorithm that can find the optimal assignment in linear time when the cost function is based on a tree distance. Additionally, we propose two types of tree distances: one that reflects discrete and structural differences between vertices and another that can compare continuous labels. By utilizing these methods, we can approximate the edit distance between two graphs by matching their vertices in linear time. The efficacy and efficiency of our approach are validated through experiments on synthetic and real-world datasets.",1
"While models for Visual Question Answering (VQA) have steadily improved over the years, interacting with one quickly reveals that these models lack consistency. For instance, if a model answers ""red"" to ""What color is the balloon?"", it might answer ""no"" if asked, ""Is the balloon red?"". These responses violate simple notions of entailment and raise questions about how effectively VQA models ground language. In this work, we introduce a dataset, ConVQA, and metrics that enable quantitative evaluation of consistency in VQA. For a given observable fact in an image (e.g. the balloon's color), we generate a set of logically consistent question-answer (QA) pairs (e.g. Is the balloon red?) and also collect a human-annotated set of common-sense based consistent QA pairs (e.g. Is the balloon the same color as tomato sauce?). Further, we propose a consistency-improving data augmentation module, a Consistency Teacher Module (CTM). CTM automatically generates entailed (or similar-intent) questions for a source QA pair and fine-tunes the VQA model if the VQA's answer to the entailed question is consistent with the source QA pair. We demonstrate that our CTM-based training improves the consistency of VQA models on the ConVQA datasets and is a strong baseline for further research.",0
"The progress of models for Visual Question Answering (VQA) has been steady over time; however, it is apparent upon interacting with them that they lack consistency. For example, if a model answers ""red"" to the question ""What color is the balloon?"", it may answer ""no"" when asked ""Is the balloon red?"", which contradicts basic entailment concepts, and raises doubts about the efficacy of VQA models in grounding language. To address this issue, we present the ConVQA dataset and metrics that allow for a quantitative assessment of consistency in VQA. We generate logically consistent question-answer (QA) pairs for observable facts in an image and gather human-annotated common-sense based QA pairs. Moreover, we suggest a Consistency Teacher Module (CTM) that augments data to enhance consistency. The CTM automatically creates entailed or similar-intent questions for a source QA pair and fine-tunes the VQA model if the model's answer to the entailed question is consistent with the source QA pair. The use of CTM-based training enhances the consistency of VQA models on the ConVQA dataset, providing a strong foundation for further research.",1
"It is always well believed that parsing an image into constituent visual patterns would be helpful for understanding and representing an image. Nevertheless, there has not been evidence in support of the idea on describing an image with a natural-language utterance. In this paper, we introduce a new design to model a hierarchy from instance level (segmentation), region level (detection) to the whole image to delve into a thorough image understanding for captioning. Specifically, we present a HIerarchy Parsing (HIP) architecture that novelly integrates hierarchical structure into image encoder. Technically, an image decomposes into a set of regions and some of the regions are resolved into finer ones. Each region then regresses to an instance, i.e., foreground of the region. Such process naturally builds a hierarchal tree. A tree-structured Long Short-Term Memory (Tree-LSTM) network is then employed to interpret the hierarchal structure and enhance all the instance-level, region-level and image-level features. Our HIP is appealing in view that it is pluggable to any neural captioning models. Extensive experiments on COCO image captioning dataset demonstrate the superiority of HIP. More remarkably, HIP plus a top-down attention-based LSTM decoder increases CIDEr-D performance from 120.1% to 127.2% on COCO Karpathy test split. When further endowing instance-level and region-level features from HIP with semantic relation learnt through Graph Convolutional Networks (GCN), CIDEr-D is boosted up to 130.6%.",0
"The idea that breaking down an image into visual patterns would aid in comprehending and representing it has always been widely accepted. However, there has been no evidence supporting the notion of describing an image using natural language. This paper introduces a new approach to modeling a hierarchy, starting from the instance level (segmentation), then moving to the region level (detection), and ultimately, the entire image, to achieve a comprehensive image understanding for captioning. The HIerarchy Parsing (HIP) architecture integrates a novel hierarchical structure into the image encoder. The image is broken down into regions, some of which are further resolved into smaller ones, and each region regresses to an instance, i.e., the foreground of the region. This process creates a hierarchical tree. A tree-structured Long Short-Term Memory (Tree-LSTM) network is used to interpret this structure and enhance all the instance-level, region-level, and image-level features. HIP is easily incorporated into any neural captioning model. Extensive experiments on the COCO image captioning dataset demonstrate that HIP outperforms other methods. When combined with a top-down attention-based LSTM decoder, CIDEr-D performance increases from 120.1% to 127.2% on the COCO Karpathy test split. When instance-level and region-level features from HIP are combined with semantic relation learned through Graph Convolutional Networks (GCN), CIDEr-D is boosted up to 130.6%.",1
"Recently, kernelized locality sensitive hashcodes have been successfully employed as representations of natural language text, especially showing high relevance to biomedical relation extraction tasks. In this paper, we propose to optimize the hashcode representations in a nearly unsupervised manner, in which we only use data points, but not their class labels, for learning. The optimized hashcode representations are then fed to a supervised classifier following the prior work. This nearly unsupervised approach allows fine-grained optimization of each hash function, which is particularly suitable for building hashcode representations generalizing from a training set to a test set. We empirically evaluate the proposed approach for biomedical relation extraction tasks, obtaining significant accuracy improvements w.r.t. state-of-the-art supervised and semi-supervised approaches.",0
"Natural language text can be effectively represented using kernelized locality sensitive hashcodes, which have demonstrated a strong correlation with biomedical relation extraction tasks. Our paper proposes a nearly unsupervised method to enhance the hashcode representations, using only data points and not their class labels. After optimization, the hashcode representations are incorporated into a supervised classifier, as previously done. This approach allows for precise optimization of each hash function, making it ideal for creating hashcode representations that can be applied to both training and test sets. Our empirical study of biomedical relation extraction tasks shows that our proposed approach achieves significant accuracy improvements over state-of-the-art supervised and semi-supervised methods.",1
"Visualization refers to our ability to create an image in our head based on the text we read or the words we hear. It is one of the many skills that makes reading comprehension possible. Convolutional Neural Networks (CNN) are an excellent tool for recognizing and classifying text documents. In addition, it can generate images conditioned on natural language. In this work, we utilize CNNs capabilities to generate realistic images representative of the text illustrating the semantic concept. We conducted various experiments to highlight the capacity of the proposed model to generate representative images of the text descriptions used as input to the proposed model.",0
"The skill of visualization enables us to form a mental picture from the information we read or hear, thus aiding reading comprehension. Convolutional Neural Networks (CNNs) are highly effective at classifying and identifying text documents, and can also generate images based on natural language. Our research employs CNNs to create lifelike images that accurately depict the semantic concept described in the text. Through a series of experiments, we demonstrate the model's ability to generate images that accurately represent the input text descriptions.",1
"Significant advances have been made in Natural Language Processing (NLP) modelling since the beginning of 2018. The new approaches allow for accurate results, even when there is little labelled data, because these NLP models can benefit from training on both task-agnostic and task-specific unlabelled data. However, these advantages come with significant size and computational costs. This workshop paper outlines how our proposed convolutional student architecture, having been trained by a distillation process from a large-scale model, can achieve 300x inference speedup and 39x reduction in parameter count. In some cases, the student model performance surpasses its teacher on the studied tasks.",0
"Natural Language Processing (NLP) modelling has seen significant progress since the beginning of 2018. These new approaches produce precise results, even with limited labelled data, as they leverage training on both task-agnostic and task-specific unlabelled data. Nonetheless, there are substantial size and computational costs associated with these benefits. This workshop paper presents our convolutional student architecture proposal, which has undergone distillation training from a large-scale model. The student model achieves a 300x inference speedup and a 39x reduction in parameter count. In some instances, the student model outperforms its teacher on the tasks studied.",1
"Research in natural language processing proceeds, in part, by demonstrating that new models achieve superior performance (e.g., accuracy) on held-out test data, compared to previous results. In this paper, we demonstrate that test-set performance scores alone are insufficient for drawing accurate conclusions about which model performs best. We argue for reporting additional details, especially performance on validation data obtained during model development. We present a novel technique for doing so: expected validation performance of the best-found model as a function of computation budget (i.e., the number of hyperparameter search trials or the overall training time). Using our approach, we find multiple recent model comparisons where authors would have reached a different conclusion if they had used more (or less) computation. Our approach also allows us to estimate the amount of computation required to obtain a given accuracy; applying it to several recently published results yields massive variation across papers, from hours to weeks. We conclude with a set of best practices for reporting experimental results which allow for robust future comparisons, and provide code to allow researchers to use our technique.",0
"The advancement of natural language processing research involves showcasing new models that outperform previous ones in terms of accuracy on test data. However, relying solely on test-set performance scores may not provide an accurate evaluation of the best-performing model. In this paper, we propose the importance of reporting additional details, particularly the performance on validation data obtained during model development. We introduce a novel technique that estimates the expected validation performance of the best model based on the computation budget, such as the number of hyperparameter search trials or overall training time. Our approach reveals that some recent model comparisons could have led to different conclusions if more or less computation was used. Additionally, we can predict the amount of computation needed to achieve a specific accuracy, and our analysis of various studies indicates a significant variation in the required time from hours to weeks. To promote accurate and reliable comparisons in future research, we provide a set of best practices for reporting experimental results, along with code to facilitate the use of our technique.",1
"There have been a few recent methods proposed in text to video moment retrieval using natural language queries, but requiring full supervision during training. However, acquiring a large number of training videos with temporal boundary annotations for each text description is extremely time-consuming and often not scalable. In order to cope with this issue, in this work, we introduce the problem of learning from weak labels for the task of text to video moment retrieval. The weak nature of the supervision is because, during training, we only have access to the video-text pairs rather than the temporal extent of the video to which different text descriptions relate. We propose a joint visual-semantic embedding based framework that learns the notion of relevant segments from video using only video-level sentence descriptions. Specifically, our main idea is to utilize latent alignment between video frames and sentence descriptions using Text-Guided Attention (TGA). TGA is then used during the test phase to retrieve relevant moments. Experiments on two benchmark datasets demonstrate that our method achieves comparable performance to state-of-the-art fully supervised approaches.",0
"Recently, some techniques have been suggested to retrieve video moments from text queries using natural language, but they require complete supervision during training. However, obtaining numerous training videos with time-boundary annotations for each text description is highly time-consuming and often impractical. To address this issue, we introduce the concept of learning from weak labels for text-to-video moment retrieval. The supervision is weak because we only have access to video-text pairs rather than the temporal span of the video corresponding to different text descriptions during training. We propose a framework based on joint visual-semantic embedding that learns relevant segments from video using only sentence descriptions at the video level. Our approach employs Text-Guided Attention (TGA) to exploit the latent alignment between video frames and sentence descriptions, which is then used during testing to retrieve relevant moments. Experiments on two benchmark datasets show that our method achieves comparable performance to state-of-the-art fully supervised techniques.",1
"While Reinforcement Learning (RL) approaches lead to significant achievements in a variety of areas in recent history, natural language tasks remained mostly unaffected, due to the compositional and combinatorial nature that makes them notoriously hard to optimize. With the emerging field of Text-Based Games (TBGs), researchers try to bridge this gap. Inspired by the success of RL algorithms on Atari games, the idea is to develop new methods in a restricted game world and then gradually move to more complex environments. Previous work in the area of TBGs has mainly focused on solving individual games. We, however, consider the task of designing an agent that not just succeeds in a single game, but performs well across a whole family of games, sharing the same theme. In this work, we present our deep RL agent--LeDeepChef--that shows generalization capabilities to never-before-seen games of the same family with different environments and task descriptions. The agent participated in Microsoft Research's ""First TextWorld Problems: A Language and Reinforcement Learning Challenge"" and outperformed all but one competitor on the final test set. The games from the challenge all share the same theme, namely cooking in a modern house environment, but differ significantly in the arrangement of the rooms, the presented objects, and the specific goal (recipe to cook). To build an agent that achieves high scores across a whole family of games, we use an actor-critic framework and prune the action-space by using ideas from hierarchical reinforcement learning and a specialized module trained on a recipe database.",0
"Despite significant advancements in Reinforcement Learning (RL) approaches, natural language tasks have proven to be particularly challenging to optimize due to their complex compositional and combinatorial nature. To address this issue, researchers have turned to Text-Based Games (TBGs) as a means of bridging the gap. While previous work in this field has focused on solving individual games, we aim to design an agent that can perform well across a whole family of games that share a common theme. Our deep RL agent, LeDeepChef, demonstrates generalization capabilities in never-before-seen games of the same family, with different environments and task descriptions. The agent participated in Microsoft Research's ""First TextWorld Problems: A Language and Reinforcement Learning Challenge"" and outperformed all but one competitor on the final test set. To achieve this level of success, we used an actor-critic framework and implemented ideas from hierarchical reinforcement learning and a specialized module trained on a recipe database to prune the action-space.",1
"A recent trend observed in traditionally challenging fields such as computer vision and natural language processing has been the significant performance gains shown by deep learning (DL). In many different research fields, DL models have been evolving rapidly and become ubiquitous. Despite researchers' excitement, unfortunately, most software developers are not DL experts and oftentimes have a difficult time following the booming DL research outputs. As a result, it usually takes a significant amount of time for the latest superior DL models to prevail in industry. This issue is further exacerbated by the common use of sundry incompatible DL programming frameworks, such as Tensorflow, PyTorch, Theano, etc. To address this issue, we propose a system, called Model Asset Exchange (MAX), that avails developers of easy access to state-of-the-art DL models. Regardless of the underlying DL programming frameworks, it provides an open source Python library (called the MAX framework) that wraps DL models and unifies programming interfaces with our standardized RESTful APIs. These RESTful APIs enable developers to exploit the wrapped DL models for inference tasks without the need to fully understand different DL programming frameworks. Using MAX, we have wrapped and open-sourced more than 30 state-of-the-art DL models from various research fields, including computer vision, natural language processing and signal processing, etc. In the end, we selectively demonstrate two web applications that are built on top of MAX, as well as the process of adding a DL model to MAX.",0
"DL has shown significant performance gains in challenging fields like computer vision and natural language processing. While DL models have evolved rapidly and become ubiquitous across different research fields, most software developers are not DL experts and struggle to follow the booming research outputs. This makes it difficult for the latest superior DL models to prevail in industry, especially due to the use of incompatible DL programming frameworks like Tensorflow, PyTorch, Theano, etc. To address this issue, we propose a system called Model Asset Exchange (MAX) that provides easy access to state-of-the-art DL models for developers regardless of the underlying programming frameworks. MAX offers an open source Python library, the MAX framework, that wraps DL models and unifies programming interfaces with standardized RESTful APIs. These APIs enable developers to use the wrapped DL models for inference tasks without needing to fully understand different programming frameworks. We have open-sourced over 30 state-of-the-art DL models from various fields, including computer vision and natural language processing. We also demonstrate two web applications built on MAX and the process of adding a DL model to MAX.",1
"The recent success of natural language understanding (NLU) systems has been troubled by results highlighting the failure of these models to generalize in a systematic and robust way. In this work, we introduce a diagnostic benchmark suite, named CLUTRR, to clarify some key issues related to the robustness and systematicity of NLU systems. Motivated by classic work on inductive logic programming, CLUTRR requires that an NLU system infer kinship relations between characters in short stories. Successful performance on this task requires both extracting relationships between entities, as well as inferring the logical rules governing these relationships. CLUTRR allows us to precisely measure a model's ability for systematic generalization by evaluating on held-out combinations of logical rules, and it allows us to evaluate a model's robustness by adding curated noise facts. Our empirical results highlight a substantial performance gap between state-of-the-art NLU models (e.g., BERT and MAC) and a graph neural network model that works directly with symbolic inputs---with the graph-based model exhibiting both stronger generalization and greater robustness.",0
"NLU systems have achieved recent success, but their ability to generalize in a consistent and reliable manner has been called into question. To address this issue, we have created a diagnostic benchmark suite called CLUTRR. Inspired by inductive logic programming, this suite assesses the ability of an NLU system to infer kinship relations between characters in short stories by extracting relationships and logical rules. With CLUTRR, we can measure a model's systematic generalization by testing on new combinations of logical rules and evaluate its robustness by introducing curated noise facts. Our findings demonstrate a significant performance disparity between state-of-the-art models like BERT and MAC and a graph neural network model that directly processes symbolic inputs. The graph-based model exhibits superior generalization and robustness.",1
"Deep Learning has attracted considerable attention across multiple application domains, including computer vision, signal processing and natural language processing. Although quite a few single node deep learning frameworks exist, such as tensorflow, pytorch and keras, we still lack a complete processing structure that can accommodate large scale data processing, version control, and deployment, all while staying agnostic of any specific single node framework. To bridge this gap, this paper proposes a new, higher level framework, i.e. Nemesyst, which uses databases along with model sequentialisation to allow processes to be fed unique and transformed data at the point of need. This facilitates near real-time application and makes models available for further training or use at any node that has access to the database simultaneously. Nemesyst is well suited as an application framework for internet of things aggregated control systems, deploying deep learning techniques to optimise individual machines in massive networks. To demonstrate this framework, we adopted a case study in a novel domain; deploying deep learning to optimise the high speed control of electrical power consumed by a massive internet of things network of retail refrigeration systems in proportion to load available on the UK National Grid (a demand side response). The case study demonstrated for the first time in such a setting how deep learning models, such as Recurrent Neural Networks (vanilla and Long-Short-Term Memory) and Generative Adversarial Networks paired with Nemesyst, achieve compelling performance, whilst still being malleable to future adjustments as both the data and requirements inevitably change over time.",0
"The field of Deep Learning has piqued interest in various domains, such as signal processing, natural language processing, and computer vision. While single node frameworks like tensorflow, pytorch, and keras exist, they fall short in terms of accommodating large scale data processing, version control, and deployment without being specific to any particular framework. To overcome this challenge, this article proposes a higher level framework, Nemesyst, which utilizes databases and model sequentialisation. This allows for quick application and accessibility of models for further training or use in any node with database access. Nemesyst is ideal for internet of things control systems, deploying deep learning to optimize individual machines within huge networks. A case study was conducted in which deep learning models like Recurrent Neural Networks and Generative Adversarial Networks, paired with Nemesyst, demonstrated impressive performance while accommodating future changes in data and requirements. The study focused on optimizing the electrical power consumed by retail refrigeration systems in response to the UK National Grid's load availability.",1
"Deep learning systems thrive on abundance of labeled training data but such data is not always available, calling for alternative methods of supervision. One such method is expectation regularization (XR) (Mann and McCallum, 2007), where models are trained based on expected label proportions. We propose a novel application of the XR framework for transfer learning between related tasks, where knowing the labels of task A provides an estimation of the label proportion of task B. We then use a model trained for A to label a large corpus, and use this corpus with an XR loss to train a model for task B. To make the XR framework applicable to large-scale deep-learning setups, we propose a stochastic batched approximation procedure. We demonstrate the approach on the task of Aspect-based Sentiment classification, where we effectively use a sentence-level sentiment predictor to train accurate aspect-based predictor. The method improves upon fully supervised neural system trained on aspect-level data, and is also cumulative with LM-based pretraining, as we demonstrate by improving a BERT-based Aspect-based Sentiment model.",0
"Labeled training data is crucial for the success of deep learning systems, but it is not always readily available. This creates a need for alternative methods of supervision. Expectation regularization (XR) is one such method that trains models based on expected label proportions. Our proposal applies the XR framework to transfer learning between related tasks. By using a model trained for task A to estimate label proportions for task B, we can label a large corpus and train a model for task B using an XR loss. To make this framework feasible for large-scale deep-learning setups, we introduce a stochastic batched approximation procedure. We demonstrate the effectiveness of this approach on the task of Aspect-based Sentiment classification. By using a sentence-level sentiment predictor to train an aspect-based predictor, we improve upon a fully supervised neural system trained on aspect-level data. Furthermore, our method is cumulative with LM-based pretraining, as we show by enhancing a BERT-based Aspect-based Sentiment model.",1
"We propose weakly supervised language localization networks (WSLLN) to detect events in long, untrimmed videos given language queries. To learn the correspondence between visual segments and texts, most previous methods require temporal coordinates (start and end times) of events for training, which leads to high costs of annotation. WSLLN relieves the annotation burden by training with only video-sentence pairs without accessing to temporal locations of events. With a simple end-to-end structure, WSLLN measures segment-text consistency and conducts segment selection (conditioned on the text) simultaneously. Results from both are merged and optimized as a video-sentence matching problem. Experiments on ActivityNet Captions and DiDeMo demonstrate that WSLLN achieves state-of-the-art performance.",0
"Our proposal is for the implementation of weakly supervised language localization networks (WSLLN) that can identify events in lengthy, uncut videos based on language queries. Traditional methods typically require temporal coordinates (start and end times) of events during training to establish the relationship between visual segments and texts, which can be costly in terms of annotation. WSLLN, on the other hand, reduces the annotation load by only utilizing video-sentence pairs without requiring access to temporal event locations. With its straightforward end-to-end design, WSLLN measures the consistency between the segment and text and simultaneously conducts segment selection based on the text. Both results are then merged and optimized as a video-sentence matching problem. Our experiments using ActivityNet Captions and DiDeMo demonstrate that WSLLN performs exceptionally well and achieves state-of-the-art results.",1
"The scientific literature is a rich source of information for data mining with conceptual knowledge graphs; the open science movement has enriched this literature with complementary source code that implements scientific models. To exploit this new resource, we construct a knowledge graph using unsupervised learning methods to identify conceptual entities. We associate source code entities to these natural language concepts using word embedding and clustering techniques. Practical naming conventions for methods and functions tend to reflect the concept(s) they implement. We take advantage of this specificity by presenting a novel process for joint clustering text concepts that combines word-embeddings, nonlinear dimensionality reduction, and clustering techniques to assist in understanding, organizing, and comparing software in the open science ecosystem. With our pipeline, we aim to assist scientists in building on existing models in their discipline when making novel models for new phenomena. By combining source code and conceptual information, our knowledge graph enhances corpus-wide understanding of scientific literature.",0
"The scientific literature is a valuable resource for data mining through the use of conceptual knowledge graphs. With the open science movement, the literature has been further enriched with source code that implements scientific models. To make use of this new resource, we employ unsupervised learning techniques to create a knowledge graph that identifies conceptual entities. We then utilize word embedding and clustering techniques to associate source code entities with these natural language concepts. Naming conventions for methods and functions often reflect the concepts they embody, which we leverage through a novel process for jointly clustering text concepts. This process utilizes word embeddings, nonlinear dimensionality reduction, and clustering techniques to help organize and compare software in the open science ecosystem. Our pipeline aims to assist scientists in building on existing models to create new models for novel phenomena. By combining source code and conceptual information, our knowledge graph enhances the understanding of scientific literature on a wider scale.",1
"Frameworks for writing, compiling, and optimizing deep learning (DL) models have recently enabled progress in areas like computer vision and natural language processing. Extending these frameworks to accommodate the rapidly diversifying landscape of DL models and hardware platforms presents challenging tradeoffs between expressivity, composability, and portability. We present Relay, a new compiler framework for DL. Relay's functional, statically typed intermediate representation (IR) unifies and generalizes existing DL IRs to express state-of-the-art models. The introduction of Relay's expressive IR requires careful design of domain-specific optimizations, addressed via Relay's extension mechanisms. Using these extension mechanisms, Relay supports a unified compiler that can target a variety of hardware platforms. Our evaluation demonstrates Relay's competitive performance for a broad class of models and devices (CPUs, GPUs, and emerging accelerators). Relay's design demonstrates how a unified IR can provide expressivity, composability, and portability without compromising performance.",0
"Recent advancements in deep learning (DL) frameworks have facilitated progress in computer vision and natural language processing. However, accommodating the increasingly diverse landscape of DL models and hardware platforms presents a challenge in balancing expressivity, composability, and portability. To address this, we introduce Relay, a new DL compiler framework. Relay's functional and statically typed intermediate representation (IR) unifies existing DL IRs to express advanced models. Our design of Relay's expressive IR requires domain-specific optimizations, which we address through Relay's extension mechanisms. Moreover, Relay supports a unified compiler that can target various hardware platforms, as demonstrated in our evaluation of its competitive performance on a range of models and devices. Relay's design showcases the ability of a unified IR to provide expressivity, composability, and portability without compromising performance.",1
"Developing successful sign language recognition, generation, and translation systems requires expertise in a wide range of fields, including computer vision, computer graphics, natural language processing, human-computer interaction, linguistics, and Deaf culture. Despite the need for deep interdisciplinary knowledge, existing research occurs in separate disciplinary silos, and tackles separate portions of the sign language processing pipeline. This leads to three key questions: 1) What does an interdisciplinary view of the current landscape reveal? 2) What are the biggest challenges facing the field? and 3) What are the calls to action for people working in the field? To help answer these questions, we brought together a diverse group of experts for a two-day workshop. This paper presents the results of that interdisciplinary workshop, providing key background that is often overlooked by computer scientists, a review of the state-of-the-art, a set of pressing challenges, and a call to action for the research community.",0
"Expertise in various fields such as computer vision, computer graphics, natural language processing, human-computer interaction, linguistics, and Deaf culture is imperative to develop effective sign language recognition, generation, and translation systems. However, current research is limited to separate disciplinary silos, addressing only certain aspects of the sign language processing pipeline. This leads to three essential questions: What does an interdisciplinary perspective reveal about the current landscape? What are the primary challenges faced by the field? And what are the necessary actions for the research community? To address these questions, an interdisciplinary workshop was conducted, bringing together a diverse group of experts for two days. This paper presents the workshop's outcomes, including key background information overlooked by computer scientists, a review of the state-of-the-art, pressing challenges, and a call to action for the research community.",1
"A phrase grounding system localizes a particular object in an image referred to by a natural language query. In previous work, the phrases were restricted to have nouns that were encountered in training, we extend the task to Zero-Shot Grounding(ZSG) which can include novel, ""unseen"" nouns. Current phrase grounding systems use an explicit object detection network in a 2-stage framework where one stage generates sparse proposals and the other stage evaluates them. In the ZSG setting, generating appropriate proposals itself becomes an obstacle as the proposal generator is trained on the entities common in the detection and grounding datasets. We propose a new single-stage model called ZSGNet which combines the detector network and the grounding system and predicts classification scores and regression parameters. Evaluation of ZSG system brings additional subtleties due to the influence of the relationship between the query and learned categories; we define four distinct conditions that incorporate different levels of difficulty. We also introduce new datasets, sub-sampled from Flickr30k Entities and Visual Genome, that enable evaluations for the four conditions. Our experiments show that ZSGNet achieves state-of-the-art performance on Flickr30k and ReferIt under the usual ""seen"" settings and performs significantly better than baseline in the zero-shot setting.",0
"Previously, phrase grounding systems were limited to using nouns encountered during training to locate objects in images based on natural language queries. However, we have expanded this task to include Zero-Shot Grounding (ZSG), which allows for the inclusion of novel and previously unseen nouns. Current systems use a 2-stage approach with an explicit object detection network that generates sparse proposals and evaluates them. However, in ZSG, generating appropriate proposals becomes a challenge since the proposal generator is trained on entities common in detection and grounding datasets. To address this, we propose a new single-stage model, called ZSGNet, that combines the detector network and grounding system to predict classification scores and regression parameters. Evaluating ZSG systems involves considering the relationship between the query and learned categories, which we define in four distinct conditions with varying levels of difficulty. We also introduce new datasets from Flickr30k Entities and Visual Genome for evaluating these conditions. Our experiments show that ZSGNet performs better than the baseline system in the zero-shot setting and achieves state-of-the-art performance on Flickr30k and ReferIt under the usual ""seen"" settings.",1
"How to aggregate multi-view representations of a 3D object into an informative and discriminative one remains a key challenge for multi-view 3D object retrieval. Existing methods either use view-wise pooling strategies which neglect the spatial information across different views or employ recurrent neural networks which may face the efficiency problem. To address these issues, we propose an effective and efficient framework called View N-gram Network (VNN). Inspired by n-gram models in natural language processing, VNN divides the view sequence into a set of visual n-grams, which involve overlapping consecutive view sub-sequences. By doing so, spatial information across multiple views is captured, which helps to learn a discriminative global embedding for each 3D object. Experiments on 3D shape retrieval benchmarks, including ModelNet10, ModelNet40 and ShapeNetCore55 datasets, demonstrate the superiority of our proposed method.",0
"The challenge of creating a comprehensive and distinctive representation for a 3D object from multiple views remains a significant obstacle in multi-view 3D object retrieval. Current methods either ignore spatial information across different views by using view-wise pooling strategies or use recurrent neural networks which may be inefficient. To overcome these challenges, we suggest a competent and efficient strategy called View N-gram Network (VNN). Inspired by n-gram models used in natural language processing, VNN breaks down the view sequence into a series of visual n-grams that contain overlapping, consecutive view sub-sequences. This captures spatial information across multiple views, enabling the learning of a distinctive global embedding for each 3D object. Experiments conducted on 3D shape retrieval benchmarks, including ModelNet10, ModelNet40, and ShapeNetCore55 datasets, demonstrate the superiority of our approach.",1
"The collaborative ranking problem has been an important open research question as most recommendation problems can be naturally formulated as ranking problems. While much of collaborative ranking methodology assumes static ranking data, the importance of temporal information to improving ranking performance is increasingly apparent. Recent advances in deep learning, especially the discovery of various attention mechanisms and newer architectures in addition to widely used RNN and CNN in natural language processing, have allowed us to make better use of the temporal ordering of items that each user has engaged with. In particular, the SASRec model, inspired by the popular Transformer model in natural languages processing, has achieved state-of-art results in the temporal collaborative ranking problem and enjoyed more than 10x speed-up when compared to earlier CNN/RNN-based methods. However, SASRec is inherently an un-personalized model and does not include personalized user embeddings. To overcome this limitation, we propose a Personalized Transformer (SSE-PT) model, outperforming SASRec by almost 5% in terms of NDCG@10 on 5 real-world datasets. Furthermore, after examining some random users' engagement history and corresponding attention heat maps used during the inference stage, we find our model is not only more interpretable but also able to focus on recent engagement patterns for each user. Moreover, our SSE-PT model with a slight modification, which we call SSE-PT++, can handle extremely long sequences and outperform SASRec in ranking results with comparable training speed, striking a balance between performance and speed requirements. Code and data are open sourced at https://github.com/wuliwei9278/SSE-PT.",0
"The problem of collaborative ranking is a significant research question as most recommendation issues are naturally formulated as ranking problems. While much of the methodology for collaborative ranking assumes static ranking data, the importance of temporal information for enhancing ranking performance is becoming increasingly evident. Recent advancements in deep learning have enabled us to make better use of the temporal ordering of items that each user has interacted with, thanks to various attention mechanisms and newer architectures like RNN and CNN. The SASRec model, inspired by the Transformer model in natural language processing, has achieved state-of-the-art results in the temporal collaborative ranking problem and is much faster than earlier CNN/RNN-based methods. However, SASRec is not personalized and lacks user embeddings. To address this limitation, we propose a Personalized Transformer (SSE-PT) model that outperforms SASRec by almost 5% in terms of NDCG@10 on 5 real-world datasets. Our SSE-PT model is more interpretable and can focus on recent engagement patterns for each user. Additionally, our SSE-PT++ model can handle extremely long sequences and outperform SASRec in ranking results with comparable training speed. We have open-sourced our code and data on https://github.com/wuliwei9278/SSE-PT.",1
"Vision-and-Language Navigation (VLN) tasks such as Room-to-Room (R2R) require machine agents to interpret natural language instructions and learn to act in visually realistic environments to achieve navigation goals. The overall task requires competence in several perception problems: successful agents combine spatio-temporal, vision and language understanding to produce appropriate action sequences. Our approach adapts pre-trained vision and language representations to relevant in-domain tasks making them more effective for VLN. Specifically, the representations are adapted to solve both a cross-modal sequence alignment and sequence coherence task. In the sequence alignment task, the model determines whether an instruction corresponds to a sequence of visual frames. In the sequence coherence task, the model determines whether the perceptual sequences are predictive sequentially in the instruction-conditioned latent space. By transferring the domain-adapted representations, we improve competitive agents in R2R as measured by the success rate weighted by path length (SPL) metric.",0
"To complete tasks like Room-to-Room (R2R), machine agents must understand natural language commands and navigate visually complex environments. This involves solving several perception challenges, including spatio-temporal, vision, and language understanding, to create effective action sequences. Our method enhances pre-existing vision and language representations for VLN by adapting them to in-domain tasks, specifically, cross-modal sequence alignment and sequence coherence. In the former, the model determines if instructions correspond to visual frames, while in the latter, it assesses if perceptual sequences are sequentially predictive in the instruction-conditioned latent space. By transferring these domain-adapted representations, we improve agent performance in R2R, as measured by the success rate weighted by path length (SPL) metric.",1
"Data collection is a major bottleneck in machine learning and an active research topic in multiple communities. There are largely two reasons data collection has recently become a critical issue. First, as machine learning is becoming more widely-used, we are seeing new applications that do not necessarily have enough labeled data. Second, unlike traditional machine learning, deep learning techniques automatically generate features, which saves feature engineering costs, but in return may require larger amounts of labeled data. Interestingly, recent research in data collection comes not only from the machine learning, natural language, and computer vision communities, but also from the data management community due to the importance of handling large amounts of data. In this survey, we perform a comprehensive study of data collection from a data management point of view. Data collection largely consists of data acquisition, data labeling, and improvement of existing data or models. We provide a research landscape of these operations, provide guidelines on which technique to use when, and identify interesting research challenges. The integration of machine learning and data management for data collection is part of a larger trend of Big data and Artificial Intelligence (AI) integration and opens many opportunities for new research.",0
"The collection of data is a significant challenge in machine learning and an active area of study in various fields. There are two main reasons why data collection has become critical recently. Firstly, the increasing use of machine learning has led to new applications that lack sufficient labeled data. Secondly, deep learning techniques, which automatically generate features, have reduced feature engineering costs but require more labeled data. Interestingly, the data management community is also contributing to research on data collection due to the importance of handling large amounts of data. This survey provides a comprehensive examination of data collection from a data management perspective, including data acquisition, labeling, and improvement of existing data or models. We present a research landscape of these operations, offer advice on when to use certain techniques, and identify interesting research challenges. The integration of machine learning and data management for data collection is part of a larger trend of Big Data and Artificial Intelligence (AI) integration, creating many exciting opportunities for new research.",1
"Classification-as-a-Service (CaaS) is widely deployed today in machine intelligence stacks for a vastly diverse set of applications including anything from medical prognosis to computer vision tasks to natural language processing to identity fraud detection. The computing power required for training complex models on large datasets to perform inference to solve these problems can be very resource-intensive. A CaaS provider may cheat a customer by fraudulently bypassing expensive training procedures in favor of weaker, less computationally-intensive algorithms which yield results of reduced quality. Given a classification service supplier $S$, intermediary CaaS provider $P$ claiming to use $S$ as a classification backend, and customer $C$, our work addresses the following questions: (i) how can $P$'s claim to be using $S$ be verified by $C$? (ii) how might $S$ make performance guarantees that may be verified by $C$? and (iii) how might one design a decentralized system that incentivizes service proofing and accountability? To this end, we propose a variety of methods for $C$ to evaluate the service claims made by $P$ using probabilistic performance metrics, instance seeding, and steganography. We also propose a method of measuring the robustness of a model using a blackbox adversarial procedure, which may then be used as a benchmark or comparison to a claim made by $S$. Finally, we propose the design of a smart contract-based decentralized system that incentivizes service accountability to serve as a trusted Quality of Service (QoS) auditor.",0
"Classification-as-a-Service (CaaS) is utilized in various machine intelligence stacks for diverse applications such as medical prognosis, computer vision tasks, natural language processing, and identity fraud detection. The process of training complex models on massive datasets to solve these problems can be computationally demanding, requiring substantial computing power. Unfortunately, some CaaS providers may defraud customers by avoiding expensive training procedures and instead using weaker, less computationally-intensive algorithms that produce inferior results. Our research aims to address the following issues: (i) how can a customer verify an intermediary CaaS provider's claim to use a specific classification service supplier? (ii) How can a classification service supplier provide verifiable performance guarantees to customers? (iii) How can a decentralized system be designed to promote accountability and service proofing? To achieve these objectives, we suggest various methods for customers to assess service claims using probabilistic performance metrics, instance seeding, and steganography. Additionally, we propose a technique for measuring model robustness using a blackbox adversarial procedure, which can serve as a benchmark or comparison to a supplier's claims. Finally, we propose a smart contract-based decentralized system that encourages service accountability and functions as a trusted Quality of Service (QoS) auditor.",1
"We present ViLBERT (short for Vision-and-Language BERT), a model for learning task-agnostic joint representations of image content and natural language. We extend the popular BERT architecture to a multi-modal two-stream model, pro-cessing both visual and textual inputs in separate streams that interact through co-attentional transformer layers. We pretrain our model through two proxy tasks on the large, automatically collected Conceptual Captions dataset and then transfer it to multiple established vision-and-language tasks -- visual question answering, visual commonsense reasoning, referring expressions, and caption-based image retrieval -- by making only minor additions to the base architecture. We observe significant improvements across tasks compared to existing task-specific models -- achieving state-of-the-art on all four tasks. Our work represents a shift away from learning groundings between vision and language only as part of task training and towards treating visual grounding as a pretrainable and transferable capability.",0
"Our model, ViLBERT (an abbreviation for Vision-and-Language BERT), is capable of acquiring task-independent joint representations of natural language and image content. To achieve this, we have enhanced the widely used BERT architecture to become a two-stream model that processes visual and textual inputs separately, with co-attentional transformer layers facilitating interaction between the two streams. We have trained the model through two proxy tasks on the Conceptual Captions dataset, which was automatically collected on a large scale. We have then made minor modifications to the base architecture and transferred it to a number of established vision-and-language tasks, such as visual question answering, visual commonsense reasoning, referring expressions, and caption-based image retrieval. Our results have shown significant improvements across all tasks compared to existing task-specific models, achieving state-of-the-art performance. Our approach represents a departure from the conventional practice of learning the connections between vision and language as part of task training and instead, focuses on treating visual grounding as a pretrainable and transferable capability.",1
"Fusing multi-modality information is known to be able to effectively bring significant improvement in video classification. However, the most popular method up to now is still simply fusing each stream's prediction scores at the last stage. A valid question is whether there exists a more effective method to fuse information cross modality. With the development of attention mechanism in natural language processing, there emerge many successful applications of attention in the field of computer vision. In this paper, we propose a cross-modality attention operation, which can obtain information from other modality in a more effective way than two-stream. Correspondingly we implement a compatible block named CMA block, which is a wrapper of our proposed attention operation. CMA can be plugged into many existing architectures. In the experiments, we comprehensively compare our method with two-stream and non-local models widely used in video classification. All experiments clearly demonstrate strong performance superiority by our proposed method. We also analyze the advantages of the CMA block by visualizing the attention map, which intuitively shows how the block helps the final prediction.",0
"Video classification can see significant improvements through the fusion of multi-modality information. However, the most common method thus far has been simply combining the prediction scores of each stream at the final stage. It is worth exploring if there is a more effective approach to cross-modality information fusion. Attention mechanism has been successful in natural language processing and is now being applied to computer vision. This paper proposes a cross-modality attention operation that can acquire information from other modality in a more effective manner than the two-stream method. To implement this, a CMA block, which wraps our proposed attention operation, has been developed. This block can be incorporated into various existing architectures. In comprehensive experiments, our method has been compared to the widely used two-stream and non-local models in video classification. The results have demonstrated the superior performance of our proposed method. Through visualizing the attention map, the advantages of the CMA block have also been analyzed, which intuitively depicts how the block enhances the final prediction.",1
"Many common sequential data sources, such as source code and natural language, have a natural tree-structured representation. These trees can be generated by fitting a sequence to a grammar, yielding a hierarchical ordering of the tokens in the sequence. This structure encodes a high degree of syntactic information, making it ideal for problems such as grammar correction. However, little work has been done to develop neural networks that can operate on and exploit tree-structured data. In this paper we present the Tree-Transformer \textemdash{} a novel neural network architecture designed to translate between arbitrary input and output trees. We applied this architecture to correction tasks in both the source code and natural language domains. On source code, our model achieved an improvement of $25\%$ $\text{F}0.5$ over the best sequential method. On natural language, we achieved comparable results to the most complex state of the art systems, obtaining a $10\%$ improvement in recall on the CoNLL 2014 benchmark and the highest to date $\text{F}0.5$ score on the AESW benchmark of $50.43$.",0
"It is common for sequential data sources like source code and natural language to have a tree-structured representation, which is created by applying a grammar to the sequence. This hierarchical ordering of sequence tokens contains a significant amount of syntactic information, making it well-suited for tasks like grammar correction. However, there has been limited research into developing neural networks capable of working with and leveraging tree-structured data. This research paper introduces the Tree-Transformer, a new neural network architecture designed to translate between input and output trees of any type. The architecture was used for correction tasks in source code and natural language domains, achieving a $25\%$ improvement in F0.5 on source code over the best sequential method, and comparable results to the most advanced state-of-the-art systems in natural language. The Tree-Transformer obtained a $10\%$ increase in recall on the CoNLL 2014 benchmark and the highest F0.5 score to date on the AESW benchmark at 50.43.",1
"Image caption generation is a long standing and challenging problem at the intersection of computer vision and natural language processing. A number of recently proposed approaches utilize a fully supervised object recognition model within the captioning approach. Such models, however, tend to generate sentences which only consist of objects predicted by the recognition models, excluding instances of the classes without labelled training examples. In this paper, we propose a new challenging scenario that targets the image captioning problem in a fully zero-shot learning setting, where the goal is to be able to generate captions of test images containing objects that are not seen during training. The proposed approach jointly uses a novel zero-shot object detection model and a template-based sentence generator. Our experiments show promising results on the COCO dataset.",0
"The problem of generating captions for images is a difficult and long-standing task that requires expertise in both computer vision and natural language processing. Some recent methods have incorporated supervised object recognition models into their captioning approach. However, these models tend to only produce sentences describing objects that were predicted by the recognition models, leaving out classes without labeled training examples. This paper introduces a new challenge that aims to tackle the image captioning problem in a fully zero-shot learning context. The goal is to generate captions for test images that contain unseen objects during training. To accomplish this, the proposed method combines a novel zero-shot object detection model with a template-based sentence generator. The results of our experiments on the COCO dataset are promising.",1
"Learning text-video embeddings usually requires a dataset of video clips with manually provided captions. However, such datasets are expensive and time consuming to create and therefore difficult to obtain on a large scale. In this work, we propose instead to learn such embeddings from video data with readily available natural language annotations in the form of automatically transcribed narrations. The contributions of this work are three-fold. First, we introduce HowTo100M: a large-scale dataset of 136 million video clips sourced from 1.22M narrated instructional web videos depicting humans performing and describing over 23k different visual tasks. Our data collection procedure is fast, scalable and does not require any additional manual annotation. Second, we demonstrate that a text-video embedding trained on this data leads to state-of-the-art results for text-to-video retrieval and action localization on instructional video datasets such as YouCook2 or CrossTask. Finally, we show that this embedding transfers well to other domains: fine-tuning on generic Youtube videos (MSR-VTT dataset) and movies (LSMDC dataset) outperforms models trained on these datasets alone. Our dataset, code and models will be publicly available at: www.di.ens.fr/willow/research/howto100m/.",0
"Usually, to learn text-video embeddings, one needs a dataset of video clips with manually provided captions. However, creating such datasets is a time-consuming and expensive process, which makes it challenging to obtain large-scale datasets. Instead, this study proposes to learn embeddings from video data with readily available natural language annotations in the form of automatically transcribed narrations. The study has three contributions. Firstly, it introduces the HowTo100M dataset, which comprises 136 million video clips sourced from 1.22M narrated instructional web videos, depicting humans performing and describing over 23k different visual tasks. The data collection process is fast, scalable, and does not require additional manual annotation. Secondly, the study demonstrates that a text-video embedding trained on this data leads to state-of-the-art results for text-to-video retrieval and action localization on instructional video datasets like YouCook2 or CrossTask. Finally, the study shows that this embedding transfers well to other domains. Fine-tuning on generic Youtube videos (MSR-VTT dataset) and movies (LSMDC dataset) outperforms models trained on these datasets alone. The dataset, code, and models will be publicly available at www.di.ens.fr/willow/research/howto100m/.",1
"In this paper, we introduce the task of retrieving relevant video moments from a large corpus of untrimmed, unsegmented videos given a natural language query. Our task poses unique challenges as a system must efficiently identify both the relevant videos and localize the relevant moments in the videos. This task is in contrast to prior work that localizes relevant moments in a single video or searches a large collection of already-segmented videos. For our task, we introduce Clip Alignment with Language (CAL), a model that aligns features for a natural language query to a sequence of short video clips that compose a candidate moment in a video. Our approach goes beyond prior work that aggregates video features over a candidate moment by allowing for finer clip alignment. Moreover, our approach is amenable to efficient indexing of the resulting clip-level representations, which makes it suitable for moment localization in large video collections. We evaluate our approach on three recently proposed datasets for temporal localization of moments in video with natural language extended to our video corpus moment retrieval setting: DiDeMo, Charades-STA, and ActivityNet-captions. We show that our CAL model outperforms the recently proposed Moment Context Network (MCN) on all criteria across all datasets on our proposed task, obtaining an 8%-85% and 11%-47% boost for average recall and median rank, respectively, and achieves 5x faster retrieval and 8x smaller index size with a 500K video corpus.",0
"This paper presents a new challenge of retrieving relevant moments from a large collection of untrimmed videos using natural language queries. The task requires identifying relevant videos and localizing the relevant moments within them. This task differs from previous work that focused on localizing moments in a single video or searching through pre-segmented videos. To tackle this challenge, we propose the Clip Alignment with Language (CAL) model, which aligns features for a natural language query to short video clips that compose a candidate moment. Our approach allows for finer clip alignment and efficient indexing, making it suitable for moment localization in large video collections. We evaluate our approach on three datasets and show that CAL outperforms the Moment Context Network (MCN) on all criteria, achieving an 8%-85% and 11%-47% boost for average recall and median rank, respectively. Additionally, CAL is 5x faster and requires an 8x smaller index size for a 500K video corpus.",1
"Pricing a rental property on Airbnb is a challenging task for the owner as it determines the number of customers for the place. On the other hand, customers have to evaluate an offered price with minimal knowledge of an optimal value for the property. This paper aims to develop a reliable price prediction model using machine learning, deep learning, and natural language processing techniques to aid both the property owners and the customers with price evaluation given minimal available information about the property. Features of the rentals, owner characteristics, and the customer reviews will comprise the predictors, and a range of methods from linear regression to tree-based models, support-vector regression (SVR), K-means Clustering (KMC), and neural networks (NNs) will be used for creating the prediction model.",0
"Determining the appropriate price for a rental property on Airbnb presents a challenge for owners, as it can impact the number of potential customers. Customers, on the other hand, face difficulty in evaluating the offered price with limited knowledge of the property's optimal value. This research paper aims to develop a dependable price prediction model through the utilization of machine learning, deep learning, and natural language processing techniques. The model will assist both property owners and potential customers in evaluating prices with minimal available information about the property. The model will be created by utilizing predictors such as rental features, owner characteristics, and customer reviews, and a range of methods, including linear regression, tree-based models, support-vector regression (SVR), K-means Clustering (KMC), and neural networks (NNs).",1
"The sample inefficiency of standard deep reinforcement learning methods precludes their application to many real-world problems. Methods which leverage human demonstrations require fewer samples but have been researched less. As demonstrated in the computer vision and natural language processing communities, large-scale datasets have the capacity to facilitate research by serving as an experimental and benchmarking platform for new methods. However, existing datasets compatible with reinforcement learning simulators do not have sufficient scale, structure, and quality to enable the further development and evaluation of methods focused on using human examples. Therefore, we introduce a comprehensive, large-scale, simulator-paired dataset of human demonstrations: MineRL. The dataset consists of over 60 million automatically annotated state-action pairs across a variety of related tasks in Minecraft, a dynamic, 3D, open-world environment. We present a novel data collection scheme which allows for the ongoing introduction of new tasks and the gathering of complete state information suitable for a variety of methods. We demonstrate the hierarchality, diversity, and scale of the MineRL dataset. Further, we show the difficulty of the Minecraft domain along with the potential of MineRL in developing techniques to solve key research challenges within it.",0
"Many real-world problems cannot be solved using standard deep reinforcement learning methods due to their sample inefficiency. Although methods that rely on human demonstrations require fewer samples, they have not been researched as extensively. Large-scale datasets have been proven useful in facilitating research in computer vision and natural language processing. However, existing datasets that can be used with reinforcement learning simulators lack the necessary size, structure, and quality to enable the development and evaluation of methods that utilize human examples. To address this issue, we introduce MineRL, a comprehensive, large-scale dataset of human demonstrations paired with simulators. The dataset contains over 60 million state-action pairs across various tasks in Minecraft, an open-world, 3D environment. Our data collection scheme allows for the introduction of new tasks and the gathering of complete state information suitable for various methods. We demonstrate the scale, diversity, and hierarchality of the MineRL dataset, as well as the potential it holds in solving key research challenges within the Minecraft domain.",1
"The automatic generation of radiology reports given medical radiographs has significant potential to operationally and improve clinical patient care. A number of prior works have focused on this problem, employing advanced methods from computer vision and natural language generation to produce readable reports. However, these works often fail to account for the particular nuances of the radiology domain, and, in particular, the critical importance of clinical accuracy in the resulting generated reports. In this work, we present a domain-aware automatic chest X-ray radiology report generation system which first predicts what topics will be discussed in the report, then conditionally generates sentences corresponding to these topics. The resulting system is fine-tuned using reinforcement learning, considering both readability and clinical accuracy, as assessed by the proposed Clinically Coherent Reward. We verify this system on two datasets, Open-I and MIMIC-CXR, and demonstrate that our model offers marked improvements on both language generation metrics and CheXpert assessed accuracy over a variety of competitive baselines.",0
"Generating radiology reports automatically from medical radiographs has significant potential to improve clinical patient care and streamline operations. While previous works have employed advanced methods from computer vision and natural language generation to produce readable reports, they often overlook the nuances of the radiology domain and the importance of clinical accuracy in the generated reports. This study presents a domain-aware system for generating automatic chest X-ray radiology reports that predicts report topics and generates sentences accordingly. The system is fine-tuned using reinforcement learning, taking into account both readability and clinical accuracy assessed by the proposed Clinically Coherent Reward. The model is evaluated on the Open-I and MIMIC-CXR datasets and outperforms competitive baselines in both language generation metrics and CheXpert assessed accuracy.",1
"When describing images with natural language, the descriptions can be made more informative if tuned using downstream tasks. This is often achieved by training two networks: a ""speaker network"" that generates sentences given an image, and a ""listener network"" that uses them to perform a task. Unfortunately, training multiple networks jointly to communicate to achieve a joint task, faces two major challenges. First, the descriptions generated by a speaker network are discrete and stochastic, making optimization very hard and inefficient. Second, joint training usually causes the vocabulary used during communication to drift and diverge from natural language.   We describe an approach that addresses both challenges. We first develop a new effective optimization based on partial-sampling from a multinomial distribution combined with straight-through gradient updates, which we name PSST for Partial-Sampling Straight-Through. Second, we show that the generated descriptions can be kept close to natural by constraining them to be similar to human descriptions. Together, this approach creates descriptions that are both more discriminative and more natural than previous approaches. Evaluations on the standard COCO benchmark show that PSST Multinomial dramatically improve the recall@10 from 60% to 86% maintaining comparable language naturalness, and human evaluations show that it also increases naturalness while keeping the discriminative power of generated captions.",0
"By using downstream tasks, natural language descriptions of images can be made more informative. This involves training two networks: a ""speaker network"" that generates sentences for an image, and a ""listener network"" that uses these sentences to perform a task. However, training multiple networks to communicate for a joint task faces two major challenges. Firstly, the discrete and stochastic nature of the descriptions generated by the speaker network makes optimization difficult and inefficient. Secondly, joint training causes the vocabulary used during communication to diverge from natural language. To address these challenges, we introduce an approach that utilizes partial-sampling from a multinomial distribution combined with straight-through gradient updates, which we name PSST for Partial-Sampling Straight-Through. Additionally, we constrain the generated descriptions to be similar to human descriptions in order to keep them close to natural language. This approach creates descriptions that are both more informative and natural than previous methods. Evaluations on the COCO benchmark show that PSST Multinomial significantly improves recall@10 from 60% to 86%, while maintaining comparable language naturalness. Furthermore, human evaluations indicate that this approach increases naturalness while preserving the discriminative power of the generated captions.",1
"For any financial organization, computing accurate quarterly forecasts for various products is one of the most critical operations. As the granularity at which forecasts are needed increases, traditional statistical time series models may not scale well. We apply deep neural networks in the forecasting domain by experimenting with techniques from Natural Language Processing (Encoder-Decoder LSTMs) and Computer Vision (Dilated CNNs), as well as incorporating transfer learning. A novel contribution of this paper is the application of curriculum learning to neural network models built for time series forecasting. We illustrate the performance of our models using Microsoft's revenue data corresponding to Enterprise, and Small, Medium & Corporate products, spanning approximately 60 regions across the globe for 8 different business segments, and totaling in the order of tens of billions of USD. We compare our models' performance to the ensemble model of traditional statistics and machine learning techniques currently used by Microsoft Finance. With this in-production model as a baseline, our experiments yield an approximately 30% improvement in overall accuracy on test data. We find that our curriculum learning LSTM-based model performs best, showing that it is reasonable to implement our proposed methods without overfitting on medium-sized data.",0
"Accurately forecasting various products on a quarterly basis is a crucial operation for financial organizations. However, traditional statistical time series models may not be able to handle the increased granularity required for accurate forecasts. To address this issue, we experimented with applying deep neural networks using techniques from Natural Language Processing and Computer Vision, while also incorporating transfer learning. Our paper introduces the use of curriculum learning for neural network models in time series forecasting. We tested the performance of our models using Microsoft's revenue data for Enterprise, Small, Medium & Corporate products across 60 regions worldwide and 8 different business segments, totaling in the order of tens of billions of USD. We compared our models' performance to the ensemble model of traditional statistics and machine learning techniques currently used by Microsoft Finance. Our experiments showed an approximately 30% improvement in overall accuracy on test data, with our curriculum learning LSTM-based model performing best. We concluded that our proposed methods are reasonable to implement without overfitting on medium-sized data.",1
"Deep neural networks have achieved state-of-art performance in many domains including computer vision, natural language processing and self-driving cars. However, they are very computationally expensive and memory intensive which raises significant challenges when it comes to deploy or train them on strict latency applications or resource-limited environments. As a result, many attempts have been introduced to accelerate and compress deep learning models, however the majority were not able to maintain the same accuracy of the baseline models. In this paper, we describe EnSyth, a deep learning ensemble approach to enhance the predictability of compact neural network's models. First, we generate a set of diverse compressed deep learning models using different hyperparameters for a pruning method, after that we utilise ensemble learning to synthesise the outputs of the compressed models to compose a new pool of classifiers. Finally, we apply backward elimination on the generated pool to explore the best performing combinations of models. On CIFAR-10, CIFAR-5 data-sets with LeNet-5, EnSyth outperforms the predictability of the baseline model.",0
"State-of-the-art performance has been achieved by deep neural networks in numerous fields such as computer vision, natural language processing, and self-driving cars. However, they are computationally expensive and require significant memory, which poses challenges when it comes to deploying or training them in latency-sensitive applications or resource-limited environments. Many attempts have been made to accelerate and compress deep learning models, but the majority fail to maintain the same accuracy as the baseline models. This paper presents EnSyth, a deep learning ensemble approach that enhances the predictability of compact neural network models. First, a set of diverse compressed deep learning models is generated using different hyperparameters for pruning. Then, ensemble learning is used to synthesize the outputs of the compressed models to create a new pool of classifiers. Finally, backward elimination is applied to the generated pool to identify the best performing combinations of models. On CIFAR-10 and CIFAR-5 datasets with LeNet-5, EnSyth outperforms the predictability of the baseline model.",1
"With the rapid growth of video data and the increasing demands of various applications such as intelligent video search and assistance toward visually-impaired people, video captioning task has received a lot of attention recently in computer vision and natural language processing fields. The state-of-the-art video captioning methods focus more on encoding the temporal information, while lack of effective ways to remove irrelevant temporal information and also neglecting the spatial details. However, the current RNN encoding module in single time order can be influenced by the irrelevant temporal information, especially the irrelevant temporal information is at the beginning of the encoding. In addition, neglecting spatial information will lead to the relationship confusion of the words and detailed loss. Therefore, in this paper, we propose a novel recurrent video encoding method and a novel visual spatial feature for the video captioning task. The recurrent encoding module encodes the video twice with the predicted key frame to avoid the irrelevant temporal information often occurring at the beginning and the end of a video. The novel spatial features represent the spatial information in different regions of a video and enrich the details of a caption. Experiments on two benchmark datasets show superior performance of the proposed method.",0
"Recently, there has been a lot of interest in the video captioning task due to the rapid growth of video data and increasing demands for various applications such as intelligent video search and assistance for visually-impaired individuals. While current state-of-the-art video captioning methods focus on encoding temporal information, they lack effective ways to remove irrelevant temporal information and neglect spatial details. The current RNN encoding module in a single time order can be influenced by irrelevant temporal information, particularly if it occurs at the beginning of the encoding. Additionally, neglecting spatial information can cause confusion in word relationships and loss of detail. Therefore, this paper proposes a new recurrent video encoding method and a novel visual spatial feature for the video captioning task. The recurrent encoding module encodes the video twice with the predicted key frame to avoid irrelevant temporal information at the beginning or end of a video. The novel spatial features represent spatial information in different regions of a video and enhance caption details. The proposed method demonstrates superior performance on two benchmark datasets.",1
"Deep learning has been shown to achieve impressive results in several domains like computer vision and natural language processing. A key element of this success has been the development of new loss functions, like the popular cross-entropy loss, which has been shown to provide faster convergence and to reduce the vanishing gradient problem in very deep structures. While the cross-entropy loss is usually justified from a probabilistic perspective, this paper shows an alternative and more direct interpretation of this loss in terms of t-norms and their associated generator functions, and derives a general relation between loss functions and t-norms. In particular, the presented work shows intriguing results leading to the development of a novel class of loss functions. These losses can be exploited in any supervised learning task and which could lead to faster convergence rates that the commonly employed cross-entropy loss.",0
"Impressive results have been achieved in various domains including computer vision and natural language processing through the use of deep learning. This success has been attributed to the development of new loss functions, such as the cross-entropy loss, which has proven to be effective in reducing the vanishing gradient problem in deep structures and promoting faster convergence. While the cross-entropy loss is commonly justified from a probabilistic standpoint, this paper offers a different interpretation of the loss by examining t-norms and their corresponding generator functions, ultimately establishing a general relationship between loss functions and t-norms. The presented study also introduces a new class of loss functions that can be used in any supervised learning task, and suggests that these losses may lead to even faster convergence rates than the widely used cross-entropy loss.",1
"One of the questions that arises when designing models that learn to solve multiple tasks simultaneously is how much of the available training budget should be devoted to each individual task. We refer to any formalized approach to addressing this problem (learned or otherwise) as a task selection policy. In this work we provide an empirical evaluation of the performance of some common task selection policies in a synthetic bandit-style setting, as well as on the GLUE benchmark for natural language understanding. We connect task selection policy learning to existing work on automated curriculum learning and off-policy evaluation, and suggest a method based on counterfactual estimation that leads to improved model performance in our experimental settings.",0
"When creating models that can tackle multiple tasks at once, a question that arises is how to allocate the training budget for each individual task. This is where a task selection policy comes into play. We aim to evaluate the effectiveness of different task selection policies through empirical research, using a synthetic bandit-style setting and the GLUE benchmark for natural language understanding. Our approach connects task selection policy learning with automated curriculum learning and off-policy evaluation. To improve model performance in our experiments, we propose using counterfactual estimation.",1
"As a fundamental problem of natural language processing, it is important to measure the distance between different documents. Among the existing methods, the Word Mover's Distance (WMD) has shown remarkable success in document semantic matching for its clear physical insight as a parameter-free model. However, WMD is essentially based on the classical Wasserstein metric, thus it often fails to robustly represent the semantic similarity between texts of different lengths. In this paper, we apply the newly developed Wasserstein-Fisher-Rao (WFR) metric from unbalanced optimal transport theory to measure the distance between different documents. The proposed WFR document distance maintains the great interpretability and simplicity as WMD. We demonstrate that the WFR document distance has significant advantages when comparing the texts of different lengths. In addition, an accelerated Sinkhorn based algorithm with GPU implementation has been developed for the fast computation of WFR distances. The KNN classification results on eight datasets have shown its clear improvement over WMD.",0
"Measuring the distance between various documents is a crucial task in natural language processing. Despite the availability of various techniques, the Word Mover's Distance (WMD) is widely used due to its parameter-free model and physical insights. However, it has been observed that WMD, which is based on the classical Wasserstein metric, fails to represent the semantic similarity between texts of varying lengths. To address this issue, we propose using the Wasserstein-Fisher-Rao (WFR) metric from unbalanced optimal transport theory. WFR maintains the simplicity and interpretability of WMD while providing significant advantages in comparing texts of different lengths. We also introduce an accelerated Sinkhorn algorithm with GPU implementation for fast WFR computation. Our KNN classification results on eight datasets demonstrate the superiority of WFR over WMD.",1
"Bayesian nonparametric approaches, in particular the Pitman-Yor process and the associated two-parameter Chinese Restaurant process, have been successfully used in applications where the data exhibit a power-law behavior. Examples include natural language processing, natural images or networks. There is also growing empirical evidence that some datasets exhibit a two-regime power-law behavior: one regime for small frequencies, and a second regime, with a different exponent, for high frequencies. In this paper, we introduce a class of completely random measures which are doubly regularly-varying. Contrary to the Pitman-Yor process, we show that when completely random measures in this class are normalized to obtain random probability measures and associated random partitions, such partitions exhibit a double power-law behavior. We discuss in particular three models within this class: the beta prime process (Broderick et al. (2015, 2018), a novel process called generalized BFRY process, and a mixture construction. We derive efficient Markov chain Monte Carlo algorithms to estimate the parameters of these models. Finally, we show that the proposed models provide a better fit than the Pitman-Yor process on various datasets.",0
"Bayesian nonparametric methods, such as the Pitman-Yor process and the two-parameter Chinese Restaurant process, have been effectively utilized in scenarios where data displays a power-law trend, such as natural language processing, natural images, and networks. Recent research indicates that certain datasets exhibit a dual-regime power-law behavior, with distinct exponents for small and high frequencies. This study introduces a category of completely random measures that are doubly regularly-varying. Unlike the Pitman-Yor process, we demonstrate that when these measures are normalized to generate random probability measures and partitions, these partitions exhibit a double power-law trend. We analyze three models within this category: the beta prime process, the novel generalized BFRY process, and a mixture construction. We develop efficient Markov chain Monte Carlo algorithms to estimate the models' parameters and demonstrate that the proposed models provide a better fit than the Pitman-Yor process in various datasets.",1
"Attention operators have been widely applied in various fields, including computer vision, natural language processing, and network embedding learning. Attention operators on graph data enables learnable weights when aggregating information from neighboring nodes. However, graph attention operators (GAOs) consume excessive computational resources, preventing their applications on large graphs. In addition, GAOs belong to the family of soft attention, instead of hard attention, which has been shown to yield better performance. In this work, we propose novel hard graph attention operator (hGAO) and channel-wise graph attention operator (cGAO). hGAO uses the hard attention mechanism by attending to only important nodes. Compared to GAO, hGAO improves performance and saves computational cost by only attending to important nodes. To further reduce the requirements on computational resources, we propose the cGAO that performs attention operations along channels. cGAO avoids the dependency on the adjacency matrix, leading to dramatic reductions in computational resource requirements. Experimental results demonstrate that our proposed deep models with the new operators achieve consistently better performance. Comparison results also indicates that hGAO achieves significantly better performance than GAO on both node and graph embedding tasks. Efficiency comparison shows that our cGAO leads to dramatic savings in computational resources, making them applicable to large graphs.",0
"The use of attention operators has become widespread across various fields, such as computer vision, natural language processing, and network embedding learning. When applied to graph data, attention operators allow for the learning of weights when gathering information from neighboring nodes. However, graph attention operators (GAOs) are known to consume too much computational power, making them unsuitable for large graphs. Moreover, GAOs belong to the soft attention family, which has been proven to be less effective than hard attention. To address these issues, we introduce two novel graph attention operators: the hard graph attention operator (hGAO) and the channel-wise graph attention operator (cGAO). The hGAO utilizes hard attention to focus solely on important nodes, leading to better performance and reduced computational costs. The cGAO performs attention operations along channels, eliminating the need for the adjacency matrix and drastically reducing computational requirements. Our experimental results demonstrate that our proposed deep models with these new operators consistently outperform previous methods. Specifically, our hGAO achieves significantly better performance than GAO on both node and graph embedding tasks, while our cGAO offers dramatic savings in computational resources, making them suitable for large graphs.",1
"Deep Neural Networks have shown tremendous success in the area of object recognition, image classification and natural language processing. However, designing optimal Neural Network architectures that can learn and output arbitrary graphs is an ongoing research problem. The objective of this survey is to summarize and discuss the latest advances in methods to Learn Representations of Graph Data. We start by identifying commonly used types of graph data and review basics of graph theory. This is followed by a discussion of the relationships between graph kernel methods and neural networks. Next we identify the major approaches used for learning representations of graph data namely: Kernel approaches, Convolutional approaches, Graph neural networks approaches, Graph embedding approaches and Probabilistic approaches. A variety of methods under each of the approaches are discussed and the survey is concluded with a brief discussion of the future of learning representation of graph data.",0
"Great strides have been made by Deep Neural Networks in the fields of natural language processing, object recognition, and image classification. Nonetheless, it remains a challenge to construct ideal Neural Network architectures that can learn and produce arbitrary graphs. The purpose of this review is to condense and analyze the latest developments in the techniques used to Learn Representations of Graph Data. Initially, we will identify the various types of graph data that are widely used and review essential aspects of graph theory. We then examine the connections between graph kernel methods and neural networks. We will then discuss the primary methods used to learn representations of graph data. These include Kernel approaches, Convolutional approaches, Graph neural network approaches, Graph embedding approaches, and Probabilistic approaches. We will delve into a range of techniques within each approach, and the review concludes with a brief discussion of the future of learning representation of graph data.",1
"Machine learning plays an increasing role in intelligent tutoring systems as both the amount of data available and specialization among students grow. Nowadays, these systems are frequently deployed on mobile applications. Users on such mobile education platforms are dynamic, frequently being added, accessing the application with varying levels of focus, and changing while using the service. The education material itself, on the other hand, is often static and is an exhaustible resource whose use in tasks such as problem recommendation must be optimized. The ability to update user models with respect to educational material in real-time is thus essential; however, existing approaches require time-consuming re-training of user features whenever new data is added. In this paper, we introduce a neural pedagogical agent for real-time user modeling in the task of predicting user response correctness, a central task for mobile education applications. Our model, inspired by work in natural language processing on sequence modeling and machine translation, updates user features in real-time via bidirectional recurrent neural networks with an attention mechanism over embedded question-response pairs. We experiment on the mobile education application SantaTOEIC, which has 559k users, 66M response data points as well as a set of 10k study problems each expert-annotated with topic tags and gathered since 2016. Our model outperforms existing approaches over several metrics in predicting user response correctness, notably out-performing other methods on new users without large question-response histories. Additionally, our attention mechanism and annotated tag set allow us to create an interpretable education platform, with a smart review system that addresses the aforementioned issue of varied user attention and problem exhaustion.",0
"As students become more specialized and the amount of data available increases, machine learning is playing a greater role in intelligent tutoring systems. These systems are now commonly found on mobile applications, which are used by dynamic users with varying levels of focus. However, the educational material provided is often static and exhaustible, which requires optimization for tasks like problem recommendation. Real-time updating of user models with respect to educational material is crucial but existing approaches require time-consuming re-training of user features. In this paper, we propose a neural pedagogical agent for real-time user modeling, which is inspired by natural language processing and machine translation. Our model uses bidirectional recurrent neural networks with an attention mechanism over embedded question-response pairs to update user features in real-time. We test our model on the SantaTOEIC mobile education application and our results demonstrate that our approach outperforms existing methods in predicting user response correctness. Furthermore, our attention mechanism and annotated tag set enable the creation of an interpretable education platform with a smart review system that addresses the issue of varied user attention and problem exhaustion.",1
"Open-ended video question answering aims to automatically generate the natural-language answer from referenced video contents according to the given question. Currently, most existing approaches focus on short-form video question answering with multi-modal recurrent encoder-decoder networks. Although these works have achieved promising performance, they may still be ineffectively applied to long-form video question answering due to the lack of long-range dependency modeling and the suffering from the heavy computational cost. To tackle these problems, we propose a fast Hierarchical Convolutional Self-Attention encoder-decoder network(HCSA). Concretely, we first develop a hierarchical convolutional self-attention encoder to efficiently model long-form video contents, which builds the hierarchical structure for video sequences and captures question-aware long-range dependencies from video context. We then devise a multi-scale attentive decoder to incorporate multi-layer video representations for answer generation, which avoids the information missing of the top encoder layer. The extensive experiments show the effectiveness and efficiency of our method.",0
"The objective of open-ended video question answering is to produce a natural-language response based on referenced video material and the given query. At present, the majority of existing techniques concentrate on answering questions related to short-form videos using multi-modal recurrent encoder-decoder networks. Although these models have demonstrated promising results, they may not be well-suited for answering queries about long-form videos due to the absence of long-range dependency modeling and the high computational cost involved. To address these issues, we propose a quick Hierarchical Convolutional Self-Attention encoder-decoder network(HCSA). Specifically, we first develop a hierarchical convolutional self-attention encoder to efficiently model long-form video content by establishing a hierarchical structure for video sequences and capturing question-aware long-range dependencies from video context. We then design a multi-scale attentive decoder to incorporate multi-layer video representations for answer generation, which prevents the loss of information from the top encoder layer. Our extensive experiments confirm the effectiveness and efficiency of our approach.",1
"When translating natural language questions into SQL queries to answer questions from a database, we would like our methods to generalize to domains and database schemas outside of the training set. To handle complex questions and database schemas with a neural encoder-decoder paradigm, it is critical to properly encode the schema as part of the input with the question. In this paper, we use relation-aware self-attention within the encoder so that it can reason about how the tables and columns in the provided schema relate to each other and use this information in interpreting the question. We achieve significant gains on the recently-released Spider dataset with 42.94% exact match accuracy, compared to the 18.96% reported in published work.",0
"Our goal is to create a method for translating natural language questions into SQL queries that can be applied to various domains and database schemas beyond the training set. To effectively tackle complex questions and database schemas using a neural encoder-decoder paradigm, it is vital to properly encode the schema alongside the question input. Our approach involves implementing relation-aware self-attention within the encoder, enabling it to reason about how tables and columns in the schema are related to one another and utilize this knowledge to interpret the question. As a result, we have achieved significant improvements on the Spider dataset, with an exact match accuracy of 42.94%, compared to the published work's 18.96%.",1
"Due to the surprisingly good representation power of complex distributions, neural network (NN) classifiers are widely used in many tasks which include natural language processing, computer vision and cyber security. In recent works, people noticed the existence of adversarial examples. These adversarial examples break the NN classifiers' underlying assumption that the environment is attack free and can easily mislead fully trained NN classifier without noticeable changes. Among defensive methods, adversarial training is a popular choice. However, original adversarial training with single-step adversarial examples (Single-Adv) can not defend against iterative adversarial examples. Although adversarial training with iterative adversarial examples (Iter-Adv) can defend against iterative adversarial examples, it consumes too much computational power and hence is not scalable. In this paper, we analyze Iter-Adv techniques and identify two of their empirical properties. Based on these properties, we propose modifications which enhance Single-Adv to perform competitively as Iter-Adv. Through preliminary evaluation, we show that the proposed method enhances the test accuracy of state-of-the-art (SOTA) Single-Adv defensive method against iterative adversarial examples by up to 16.93% while reducing its training cost by 28.75%.",0
"Neural network (NN) classifiers are commonly used in various tasks such as natural language processing, computer vision, and cyber security due to their ability to accurately represent complex distributions. However, researchers have discovered the presence of adversarial examples that can deceive fully trained NN classifiers by exploiting the assumption that the environment is attack-free. Adversarial training is a popular method to counter this, but single-step adversarial examples used in original adversarial training (Single-Adv) are ineffective against iterative adversarial examples. Although adversarial training with iterative adversarial examples (Iter-Adv) can defend against them, it is computationally expensive and not scalable. In this study, we identify two empirical properties of Iter-Adv techniques and propose modifications that improve the performance of Single-Adv to match that of Iter-Adv. Our preliminary evaluation shows that our proposed method enhances the test accuracy of state-of-the-art Single-Adv against iterative adversarial examples by up to 16.93% while reducing training costs by 28.75%.",1
"Generating textual descriptions for images has been an attractive problem for the computer vision and natural language processing researchers in recent years. Dozens of models based on deep learning have been proposed to solve this problem. The existing approaches are based on neural encoder-decoder structures equipped with the attention mechanism. These methods strive to train decoders to minimize the log likelihood of the next word in a sentence given the previous ones, which results in the sparsity of the output space. In this work, we propose a new approach to train decoders to regress the word embedding of the next word with respect to the previous ones instead of minimizing the log likelihood. The proposed method is able to learn and extract long-term information and can generate longer fine-grained captions without introducing any external memory cell. Furthermore, decoders trained by the proposed technique can take the importance of the generated words into consideration while generating captions. In addition, a novel semantic attention mechanism is proposed that guides attention points through the image, taking the meaning of the previously generated word into account. We evaluate the proposed approach with the MS-COCO dataset. The proposed model outperformed the state of the art models especially in generating longer captions. It achieved a CIDEr score equal to 125.0 and a BLEU-4 score equal to 50.5, while the best scores of the state of the art models are 117.1 and 48.0, respectively.",0
"Recently, researchers in computer vision and natural language processing have been fascinated by the challenge of generating textual descriptions for images. Many deep learning models have been suggested to tackle this issue, with the existing methods relying on neural encoder-decoder structures that include an attention mechanism. These techniques aim to minimize the log likelihood of the next word in a sentence given the previous ones, resulting in a sparse output space. This study proposes a new approach to training decoders to regress the word embedding of the next word based on the previous ones, rather than minimizing the log likelihood. This method can extract long-term information and generate longer, more detailed captions without requiring external memory cells. Additionally, the proposed decoders can consider the importance of generated words when producing captions. Furthermore, a novel semantic attention mechanism is introduced, which takes the meaning of previously generated words into account when guiding attention points through the image. The proposed approach is evaluated using the MS-COCO dataset, and it outperforms the state-of-the-art models, particularly in generating longer captions. The proposed model achieves a CIDEr score of 125.0 and a BLEU-4 score of 50.5, whereas the best scores of the state-of-the-art models are 117.1 and 48.0, respectively.",1
"One of the most exciting technology breakthroughs in the last few years has been the rise of deep learning. State-of-the-art deep learning models are being widely deployed in academia and industry, across a variety of areas, from image analysis to natural language processing. These models have grown from fledgling research subjects to mature techniques in real-world use. The increasing scale of data, computational power and the associated algorithmic innovations are the main drivers for the progress we see in this field. These developments also have a huge potential for the automotive industry and therefore the interest in deep learning-based technology is growing. A lot of the product innovations, such as self-driving cars, parking and lane-change assist or safety functions, such as autonomous emergency braking, are powered by deep learning algorithms. Deep learning is poised to offer gains in performance and functionality for most ADAS (Advanced Driver Assistance System) solutions. Virtual sensing for vehicle dynamics application, vehicle inspection/heath monitoring, automated driving and data-driven product development are key areas that are expected to get the most attention. This article provides an overview of the recent advances and some associated challenges in deep learning techniques in the context of automotive applications.",0
"The rise of deep learning has been one of the most exciting technological advancements in recent years. State-of-the-art models are now widely used in academia and industry, with applications ranging from image analysis to natural language processing. These models have evolved from experimental subjects to established techniques that are utilized in real-world scenarios. The progress we see in this field is due to the increasing scale of data, computational power, and algorithmic innovations. The automotive industry is particularly interested in deep learning-based technology, as it has the potential to revolutionize product innovations such as self-driving cars, parking and lane-change assist, and safety functions like autonomous emergency braking. Deep learning is expected to improve performance and functionality in most ADAS solutions, with key areas of focus including virtual sensing for vehicle dynamics application, vehicle inspection/health monitoring, automated driving, and data-driven product development. This article provides an overview of recent advancements and the associated challenges in utilizing deep learning techniques for automotive applications.",1
"We propose a sign language translation system based on human keypoint estimation. It is well-known that many problems in the field of computer vision require a massive amount of dataset to train deep neural network models. The situation is even worse when it comes to the sign language translation problem as it is far more difficult to collect high-quality training data. In this paper, we introduce the KETI (short for Korea Electronics Technology Institute) sign language dataset which consists of 14,672 videos of high resolution and quality. Considering the fact that each country has a different and unique sign language, the KETI sign language dataset can be the starting line for further research on the Korean sign language translation. Using the KETI sign language dataset, we develop a neural network model for translating sign videos into natural language sentences by utilizing the human keypoints extracted from a face, hands, and body parts. The obtained human keypoint vector is normalized by the mean and standard deviation of the keypoints and used as input to our translation model based on the sequence-to-sequence architecture. As a result, we show that our approach is robust even when the size of the training data is not sufficient. Our translation model achieves 93.28% (55.28%, respectively) translation accuracy on the validation set (test set, respectively) for 105 sentences that can be used in emergency situations. We compare several types of our neural sign translation models based on different attention mechanisms in terms of classical metrics for measuring the translation performance.",0
"Our proposal is a sign language translation system that employs human keypoint estimation. It is widely known that computer vision problems require extensive datasets to train deep neural network models. The difficulty is compounded when it comes to sign language translation due to the challenge of obtaining high-quality training data. To address this issue, we present the KETI sign language dataset, which comprises 14,672 high-resolution videos. This dataset can serve as a foundation for further research on Korean sign language translation, taking into account the unique sign languages of different countries. Using this dataset, we create a neural network model that uses human keypoints extracted from the face, hands, and body parts to translate sign videos into natural language sentences. We normalize the human keypoint vector by the mean and standard deviation of the keypoints and feed it into our translation model, which is based on a sequence-to-sequence architecture. Our approach is highly robust even when the training data is insufficient, achieving a translation accuracy of 93.28% (55.28%, respectively) on the validation set (test set, respectively) for 105 emergency sentences. We compare various neural sign translation models based on different attention mechanisms using classical metrics to measure the translation performance.",1
"Any given classification problem can be modeled using multi-class or One-vs-All (OVA) architecture. An OVA system consists of as many OVA models as the number of classes, providing the advantage of asynchrony, where each OVA model can be re-trained independent of other models. This is particularly advantageous in settings where scalable model training is a consideration (for instance in an industrial environment where multiple and frequent updates need to be made to the classification system). In this paper, we conduct empirical analysis on realizing independent updates to OVA models and its impact on the accuracy of the overall OVA system. Given that asynchronous updates lead to differences in training datasets for OVA models, we first define a metric to quantify the differences in datasets. Thereafter, using Natural Language Understanding as a task of interest, we estimate the impact of three factors: (i) number of classes, (ii) number of data points and, (iii) divergences in training datasets across OVA models; on the OVA system accuracy. Finally, we observe the accuracy impact of increased asynchrony in a Spoken Language Understanding system. We analyze the results and establish that the proposed metric correlates strongly with the model performances in both the experimental settings.",0
"A classification problem can be solved using either multi-class or One-vs-All (OVA) architecture. The OVA approach involves creating an OVA model for each class, allowing for independent re-training of each model, which is beneficial in situations where model training needs to be scalable, such as in an industrial environment where frequent updates are required. This study examines the impact of independent updates on OVA models and the overall accuracy of the OVA system. To assess the effect of asynchronous updates, a metric is defined to measure the differences in training datasets for OVA models. The study focuses on Natural Language Understanding as the task of interest and evaluates the impact of the number of classes, number of data points, and divergences in training datasets across OVA models on the accuracy of the system. The study also measures the accuracy impact of increased asynchrony in a Spoken Language Understanding system. The results show a strong correlation between the proposed metric and the model performances in both experimental settings.",1
"Neural machine translation models are used to automatically generate a document from given source code since this can be regarded as a machine translation task. Source code summarization is one of the components for automatic document generation, which generates a summary in natural language from given source code. This suggests that techniques used in neural machine translation, such as Long Short-Term Memory (LSTM), can be used for source code summarization. However, there is a considerable difference between source code and natural language: Source code is essentially {\em structured}, having loops and conditional branching, etc. Therefore, there is some obstacle to apply known machine translation models to source code.   Abstract syntax trees (ASTs) capture these structural properties and play an important role in recent machine learning studies on source code. Tree-LSTM is proposed as a generalization of LSTMs for tree-structured data. However, there is a critical issue when applying it to ASTs: It cannot handle a tree that contains nodes having an arbitrary number of children and their order simultaneously, which ASTs generally have such nodes. To address this issue, we propose an extension of Tree-LSTM, which we call \emph{Multi-way Tree-LSTM} and apply it for source code summarization. As a result of computational experiments, our proposal achieved better results when compared with several state-of-the-art techniques.",0
"Neural machine translation models are utilized in the automatic generation of documents from provided source code. This process is considered a machine translation task, with source code summarization serving as a key component in generating a natural language summary from the given code. Although techniques like Long Short-Term Memory (LSTM) used in neural machine translation can be applied to source code summarization, there is a significant difference in structure between source code and natural language. Source code is structured with loops and conditional branching, creating an obstacle to using known machine translation models. Recent machine learning studies on source code have used Abstract Syntax Trees (ASTs) to capture structural properties. However, ASTs typically contain nodes with an arbitrary number of children and their order simultaneously, which cannot be handled by Tree-LSTM. To address this issue, we propose a new method called Multi-way Tree-LSTM for source code summarization. Our computational experiments show that this approach outperforms several state-of-the-art techniques.",1
"Whether it is computer vision, natural language processing or speech recognition, the essence of these applications is to obtain powerful feature representations that make downstream applications completion more efficient. Taking image recognition as an example, whether it is hand-crafted low-level feature representation or feature representation extracted by a convolutional neural networks(CNNs), the goal is to extract features that better represent image features, thereby improving classification accuracy. However, we observed that image feature representations share a large common vector and a few top dominating directions. To address this problems, we propose a simple but effective postprocessing method to render off-the-shelf feature representations even stronger by eliminating the common mean vector from off-the-shelf feature representations. The postprocessing is empirically validated on a variety of datasets and feature extraction methods.such as VGG, LBP, and HOG. Some experiments show that the features that have been post-processed by postprocessing algorithm can get better results than original ones.",0
"The core of computer vision, natural language processing, and speech recognition lies in obtaining strong feature representations that enhance the efficiency of downstream applications. For instance, in image recognition, the aim is to extract features that accurately represent image features and improve classification accuracy, whether it is through hand-crafted low-level feature representation or feature representation from a convolutional neural network. However, we have observed that image feature representations have a large common vector and few dominant directions, which poses a challenge. To overcome this, we propose a straightforward yet effective postprocessing method that strengthens off-the-shelf feature representations by removing the common mean vector. Our method has been tested on various datasets and feature extraction methods, including VGG, LBP, and HOG, and we have observed that postprocessed features yield better results than the original ones.",1
"In this work, we describe practical lessons we have learned from successfully using contextual bandits (CBs) to improve key business metrics of the Microsoft Virtual Agent for customer support. While our current use cases focus on single step einforcement learning (RL) and mostly in the domain of natural language processing and information retrieval we believe many of our findings are generally applicable. Through this article, we highlight certain issues that RL practitioners may encounter in similar types of applications as well as offer practical solutions to these challenges.",0
"This article details the valuable insights we have gained from implementing contextual bandits (CBs) to enhance crucial business metrics for Microsoft Virtual Agent's customer support. Our current employment of single step reinforcement learning (RL) is primarily concentrated on natural language processing and information retrieval. Nevertheless, we believe that our discoveries have universal relevance. Our discussion addresses typical challenges that RL practitioners may confront in comparable applications, while concurrently offering practical remedies.",1
"City Logistics is characterized by multiple stakeholders that often have different views of such a complex system. From a public policy perspective, identifying stakeholders, issues and trends is a daunting challenge, only partially addressed by traditional observation systems. Nowadays, social media is one of the biggest channels of public expression and is often used to communicate opinions and content related to City Logistics. The idea of this research is that analysing social media content could help in understanding the public perception of City logistics. This paper offers a methodology for collecting content from Twitter and implementing Machine Learning techniques (Unsupervised Learning and Natural Language Processing), to perform content and sentiment analysis. The proposed methodology is applied to more than 110 000 tweets containing City Logistics key-terms. Results allowed the building of an Interest Map of concepts and a Sentiment Analysis to determine if City Logistics entries are positive, negative or neutral.",0
"City Logistics is a complex system that involves multiple stakeholders with differing perspectives. Identifying issues and trends from a public policy standpoint can be challenging, and traditional observation systems only partially address the issue. Nowadays, social media serves as a major platform for public expression and is often used to share opinions and content related to City Logistics. This study proposes that analyzing social media content can aid in understanding the public perception of City Logistics. The research presents a methodology for gathering content from Twitter and utilizing Machine Learning techniques such as Unsupervised Learning and Natural Language Processing to carry out content and sentiment analysis. The proposed methodology was used to analyze over 110,000 tweets that contained City Logistics keywords. Results were used to create an Interest Map of concepts and perform Sentiment Analysis to determine whether City Logistics entries were positive, negative, or neutral.",1
"In this work\footnote {This work was supported in part by the National Science Foundation under grant IIS-1212948.}, we present a method to represent a video with a sequence of words, and learn the temporal sequencing of such words as the key information for predicting and recognizing human actions. We leverage core concepts from the Natural Language Processing (NLP) literature used in sentence classification to solve the problems of action prediction and action recognition. Each frame is converted into a word that is represented as a vector using the Bag of Visual Words (BoW) encoding method. The words are then combined into a sentence to represent the video, as a sentence. The sequence of words in different actions are learned with a simple but effective Temporal Convolutional Neural Network (T-CNN) that captures the temporal sequencing of information in a video sentence. We demonstrate that a key characteristic of the proposed method is its low-latency, i.e. its ability to predict an action accurately with a partial sequence (sentence). Experiments on two datasets, \textit{UCF101} and \textit{HMDB51} show that the method on average reaches 95\% of its accuracy within half the video frames. Results, also demonstrate that our method achieves compatible state-of-the-art performance in action recognition (i.e. at the completion of the sentence) in addition to action prediction.",0
"This study introduces a technique for representing a video as a sequence of words and utilizing the temporal sequencing of these words to predict and recognize human actions. The approach draws on concepts from Natural Language Processing (NLP) literature, specifically sentence classification, to address the challenges of action recognition and prediction. The BoW method is used to encode each frame as a vector and convert it into a word, which is then combined to form a sentence that represents the video. A T-CNN is employed to learn the sequence of words in different actions and capture the temporal sequencing of information in a video sentence. The proposed method's key feature is its low-latency, meaning it can accurately predict an action using a partial sequence of words. The results show that the approach achieves state-of-the-art performance in action recognition and prediction by achieving 95% accuracy within half of the video frames.",1
"Recent advances in multi-modal vision and language tasks enable a new set of applications. In this paper, we consider the task of generating natural language fashion feedback on outfit images. We collect a unique dataset, which contains outfit images and corresponding positive and constructive fashion feedback. We treat each feedback type separately, and train deep generative encoder-decoder models with visual attention, similar to the standard image captioning pipeline. Following this approach, the generated sentences tend to be too general and non-informative. We propose an alternative decoding technique based on the Maximum Mutual Information objective function, which leads to more diverse and detailed responses. We evaluate our model with common language metrics, and also show human evaluation results. This technology is applied within the ``Alexa, how do I look?'' feature, publicly available in Echo Look devices.",0
"A new range of applications has emerged due to recent advancements in multi-modal vision and language tasks. The aim of this study is to create natural language fashion feedback on outfit images. To achieve this, we built a unique dataset consisting of outfit images and their corresponding positive and constructive fashion feedback. We approached the task by training deep generative encoder-decoder models with visual attention, treating each feedback type separately, similar to the standard image captioning pipeline. However, the output generated by this method was too generic and uninformative. Thus, we proposed an alternative decoding technique based on the Maximum Mutual Information objective function, which resulted in more diverse and detailed responses. Our model was evaluated using common language metrics and human evaluation results. This technology is now available to the public through the ""Alexa, how do I look?"" feature in Echo Look devices.",1
"Video captioning aims to automatically generate natural language descriptions of video content, which has drawn a lot of attention recent years. Generating accurate and fine-grained captions needs to not only understand the global content of video, but also capture the detailed object information. Meanwhile, video representations have great impact on the quality of generated captions. Thus, it is important for video captioning to capture salient objects with their detailed temporal dynamics, and represent them using discriminative spatio-temporal representations. In this paper, we propose a new video captioning approach based on object-aware aggregation with bidirectional temporal graph (OA-BTG), which captures detailed temporal dynamics for salient objects in video, and learns discriminative spatio-temporal representations by performing object-aware local feature aggregation on detected object regions. The main novelties and advantages are: (1) Bidirectional temporal graph: A bidirectional temporal graph is constructed along and reversely along the temporal order, which provides complementary ways to capture the temporal trajectories for each salient object. (2) Object-aware aggregation: Learnable VLAD (Vector of Locally Aggregated Descriptors) models are constructed on object temporal trajectories and global frame sequence, which performs object-aware aggregation to learn discriminative representations. A hierarchical attention mechanism is also developed to distinguish different contributions of multiple objects. Experiments on two widely-used datasets demonstrate our OA-BTG achieves state-of-the-art performance in terms of BLEU@4, METEOR and CIDEr metrics.",0
"The objective of video captioning is to generate natural language descriptions for video content automatically, which has become a popular research topic in recent years. In order to produce accurate and precise captions, it is essential to comprehend not only the overall video content but also the specific object details. Moreover, the quality of generated captions is significantly impacted by how the video is represented. Therefore, video captioning demands the identification of important objects and their temporal dynamics, and the use of distinctive spatio-temporal representations. This study introduces a novel video captioning method, OA-BTG, which utilizes object-aware aggregation with bidirectional temporal graph to capture detailed temporal dynamics for salient objects and learn discriminative spatio-temporal representations. The study's key advantages are its bidirectional temporal graph, object-aware aggregation, and hierarchical attention mechanism, which distinguish different contributions of multiple objects. The study's results demonstrate that OA-BTG outperforms prior methods based on BLEU@4, METEOR, and CIDEr metrics on two popular datasets.",1
"Variational autoencoders (VAEs) with an auto-regressive decoder have been applied for many natural language processing (NLP) tasks. The VAE objective consists of two terms, (i) reconstruction and (ii) KL regularization, balanced by a weighting hyper-parameter \beta. One notorious training difficulty is that the KL term tends to vanish. In this paper we study scheduling schemes for \beta, and show that KL vanishing is caused by the lack of good latent codes in training the decoder at the beginning of optimization. To remedy this, we propose a cyclical annealing schedule, which repeats the process of increasing \beta multiple times. This new procedure allows the progressive learning of more meaningful latent codes, by leveraging the informative representations of previous cycles as warm re-starts. The effectiveness of cyclical annealing is validated on a broad range of NLP tasks, including language modeling, dialog response generation and unsupervised language pre-training.",0
"Many natural language processing (NLP) tasks have utilized variational autoencoders (VAEs) equipped with auto-regressive decoders. The VAE objective has two components: (i) reconstruction and (ii) KL regularization, with a weighting hyper-parameter \beta to balance them. One of the main challenges in training VAEs is the tendency of the KL term to disappear. This study examines scheduling methods for \beta and finds that the lack of good latent codes when initializing the decoder during optimization is the reason for KL vanishing. A cyclical annealing schedule is proposed to address this issue, which involves increasing \beta multiple times and taking advantage of the informative representations from previous cycles as warm re-starts to enable the gradual learning of more meaningful latent codes. The efficacy of cyclical annealing is demonstrated in a variety of NLP tasks, such as language modeling, dialog response generation, and unsupervised language pre-training.",1
"To be successful in real-world tasks, Reinforcement Learning (RL) needs to exploit the compositional, relational, and hierarchical structure of the world, and learn to transfer it to the task at hand. Recent advances in representation learning for language make it possible to build models that acquire world knowledge from text corpora and integrate this knowledge into downstream decision making problems. We thus argue that the time is right to investigate a tight integration of natural language understanding into RL in particular. We survey the state of the field, including work on instruction following, text games, and learning from textual domain knowledge. Finally, we call for the development of new environments as well as further investigation into the potential uses of recent Natural Language Processing (NLP) techniques for such tasks.",0
"In order for Reinforcement Learning (RL) to achieve success in practical applications, it must utilize the hierarchical, relational, and compositional structure of the environment and apply it to the given task. With the recent advancements in language representation, it is now possible to create models that learn about the world from text data and integrate this knowledge into decision-making problems. Therefore, we propose that it is an opportune moment to explore the integration of natural language understanding into RL. This paper provides an overview of the current state of the field, including research on instruction following, text-based games, and learning from textual knowledge. Additionally, we encourage the development of new environments and further investigation into the potential uses of recent Natural Language Processing (NLP) techniques for these tasks.",1
"We propose a new approach to address the text classification problems when learning with partial labels is beneficial. Instead of offering each training sample a set of candidate labels, we assign negative-oriented labels to the ambiguous training examples if they are unlikely fall into certain classes. We construct our new maximum likelihood estimators with self-correction property, and prove that under some conditions, our estimators converge faster. Also we discuss the advantages of applying one of our estimator to a fully supervised learning problem. The proposed method has potential applicability in many areas, such as crowdsourcing, natural language processing and medical image analysis.",0
"Our proposal introduces a fresh strategy for tackling text classification issues that benefit from partial labeling. Rather than providing a selection of label options to each training sample, we assign negative labels to ambiguous examples that are unlikely to fit into specific classes. We have created maximum likelihood estimators that possess self-correction capabilities, and we have demonstrated that under certain circumstances, our estimators converge more quickly. Additionally, we explore the benefits of implementing one of our estimators in fully supervised learning situations. Our approach has potential relevance in numerous fields, including crowdsourcing, natural language processing, and medical image analysis.",1
"Machine Learning (ML) and Deep Learning (DL) models have achieved state-of-the-art performance on multiple learning tasks, from vision to natural language modelling. With the growing adoption of ML and DL to many areas of computer science, recent research has also started focusing on the security properties of these models. There has been a lot of work undertaken to understand if (deep) neural network architectures are resilient to black-box adversarial attacks which craft perturbed input samples that fool the classifier without knowing the architecture used. Recent work has also focused on the transferability of adversarial attacks and found that adversarial attacks are generally easily transferable between models, datasets, and techniques. However, such attacks and their analysis have not been covered from the perspective of unsupervised machine learning algorithms. In this paper, we seek to bridge this gap through multiple contributions. We first provide a strong (iterative) black-box adversarial attack that can craft adversarial samples which will be incorrectly clustered irrespective of the choice of clustering algorithm. We choose 4 prominent clustering algorithms, and a real-world dataset to show the working of the proposed adversarial algorithm. Using these clustering algorithms we also carry out a simple study of cross-technique adversarial attack transferability.",0
"ML and DL models have become highly effective in various learning tasks, including vision and natural language modelling. As their usage becomes more widespread, researchers have begun investigating the security properties of these models. Specifically, there has been significant exploration into the resilience of (deep) neural network architectures to black-box adversarial attacks, which manipulate input samples to deceive the classifier without knowledge of the architecture being used. Recent studies have also shown that adversarial attacks can be easily transferred between models, datasets, and techniques. However, prior research has not examined unsupervised machine learning algorithms in this context. This paper aims to address this gap by presenting a strong iterative black-box adversarial attack that can produce incorrectly clustered adversarial samples, regardless of the chosen clustering algorithm. The proposed algorithm is tested on a real-world dataset using four prominent clustering techniques, and the transferability of cross-technique adversarial attacks is studied.",1
"In this paper, we propose Text2Scene, a model that generates various forms of compositional scene representations from natural language descriptions. Unlike recent works, our method does NOT use Generative Adversarial Networks (GANs). Text2Scene instead learns to sequentially generate objects and their attributes (location, size, appearance, etc) at every time step by attending to different parts of the input text and the current status of the generated scene. We show that under minor modifications, the proposed framework can handle the generation of different forms of scene representations, including cartoon-like scenes, object layouts corresponding to real images, and synthetic images. Our method is not only competitive when compared with state-of-the-art GAN-based methods using automatic metrics and superior based on human judgments but also has the advantage of producing interpretable results.",0
"The paper presents Text2Scene, a model that can create various forms of compositional scene representations from natural language descriptions. Unlike recent research, Text2Scene does not rely on Generative Adversarial Networks (GANs). Instead, the model learns to generate objects and their attributes (such as location, size, and appearance) sequentially at each time step by attending to different parts of the input text and the current status of the scene being generated. The proposed framework can create different types of scene representations, including cartoon-like scenes, object layouts corresponding to real images, and synthetic images, with only minor modifications. Text2Scene is not only competitive with state-of-the-art GAN-based methods using automatic metrics, but also produces more interpretable results and is superior in human judgments.",1
"Deep Learning (DL) has become a crucial technology for Artificial Intelligence (AI). It is a powerful technique to automatically extract high-level features from complex data which can be exploited for applications such as computer vision, natural language processing, cybersecurity, communications, and so on. For the particular case of computer vision, several algorithms like object detection in real time videos have been proposed and they work well on Desktop GPUs and distributed computing platforms. However these algorithms are still heavy for mobile and embedded visual applications. The rapid spreading of smart portable devices and the emerging 5G network are introducing new smart multimedia applications in mobile environments. As a consequence, the possibility of implementing deep neural networks to mobile environments has attracted a lot of researchers. This paper presents emerging deep learning acceleration techniques that can enable the delivery of real time visual recognition into the hands of end users, anytime and anywhere.",0
"Artificial Intelligence (AI) heavily relies on Deep Learning (DL) technology, as it can automatically extract complex data's high-level features for various applications. These applications include computer vision, natural language processing, cybersecurity, and communications. While object detection algorithms for computer vision work well on Desktop GPUs and distributed computing platforms, they still pose a challenge for mobile and embedded visual applications. With the increasing use of smart portable devices and the emergence of 5G networks, the demand for smart multimedia applications in mobile environments is growing. Hence, researchers are exploring methods to implement deep neural networks in mobile environments. This paper discusses emerging DL acceleration techniques that can facilitate real-time visual recognition for end-users, regardless of their location.",1
"Figures, such as bar charts, pie charts, and line plots, are widely used to convey important information in a concise format. They are usually human-friendly but difficult for computers to process automatically. In this work, we investigate the problem of figure captioning where the goal is to automatically generate a natural language description of the figure. While natural image captioning has been studied extensively, figure captioning has received relatively little attention and remains a challenging problem. First, we introduce a new dataset for figure captioning, FigCAP, based on FigureQA. Second, we propose two novel attention mechanisms. To achieve accurate generation of labels in figures, we propose Label Maps Attention. To model the relations between figure labels, we propose Relation Maps Attention. Third, we use sequence-level training with reinforcement learning in order to directly optimizes evaluation metrics, which alleviates the exposure bias issue and further improves the models in generating long captions. Extensive experiments show that the proposed method outperforms the baselines, thus demonstrating a significant potential for the automatic captioning of vast repositories of figures.",0
"The utilization of figures like bar charts, pie charts, and line plots is widespread, as they present important information in a concise manner. Although they are easily understandable by humans, computers find them difficult to process automatically. This study focuses on the challenging task of figure captioning, which involves producing a natural language description of the figure. While natural image captioning has been extensively researched, figure captioning has received limited attention. To address this, we introduce FigCAP, a new dataset, based on FigureQA. We also propose two innovative attention mechanisms: Label Maps Attention for precise label generation, and Relation Maps Attention for modeling label relationships. Furthermore, we adopt sequence-level training with reinforcement learning to optimize evaluation metrics and mitigate the exposure bias issue, ultimately improving the models' ability to generate long captions. Our experimental results demonstrate that our proposed method surpasses the benchmarks, indicating significant potential for the automatic captioning of vast figure repositories.",1
"The Transformer architecture is widely used in natural language processing. Despite its success, the design principle of the Transformer remains elusive. In this paper, we provide a novel perspective towards understanding the architecture: we show that the Transformer can be mathematically interpreted as a numerical Ordinary Differential Equation (ODE) solver for a convection-diffusion equation in a multi-particle dynamic system. In particular, how words in a sentence are abstracted into contexts by passing through the layers of the Transformer can be interpreted as approximating multiple particles' movement in the space using the Lie-Trotter splitting scheme and the Euler's method. Given this ODE's perspective, the rich literature of numerical analysis can be brought to guide us in designing effective structures beyond the Transformer. As an example, we propose to replace the Lie-Trotter splitting scheme by the Strang-Marchuk splitting scheme, a scheme that is more commonly used and with much lower local truncation errors. The Strang-Marchuk splitting scheme suggests that the self-attention and position-wise feed-forward network (FFN) sub-layers should not be treated equally. Instead, in each layer, two position-wise FFN sub-layers should be used, and the self-attention sub-layer is placed in between. This leads to a brand new architecture. Such an FFN-attention-FFN layer is ""Macaron-like"", and thus we call the network with this new architecture the Macaron Net. Through extensive experiments, we show that the Macaron Net is superior to the Transformer on both supervised and unsupervised learning tasks. The reproducible codes and pretrained models can be found at https://github.com/zhuohan123/macaron-net",0
"Despite its success in natural language processing, the Transformer architecture's design principle remains unclear. This paper provides a new perspective by interpreting the Transformer as a numerical Ordinary Differential Equation (ODE) solver for a convection-diffusion equation in a multi-particle dynamic system. The process of abstracting words into contexts through the Transformer's layers is compared to approximating multiple particles' movement in space using the Lie-Trotter splitting scheme and Euler's method. By viewing the Transformer through an ODE lens, we can rely on numerical analysis techniques to design better structures. For instance, we propose the Macaron Net, a new architecture that replaces the Lie-Trotter splitting scheme with the Strang-Marchuk splitting scheme. The Macaron Net uses two position-wise feed-forward network (FFN) sub-layers and a self-attention sub-layer in each layer, leading to a superior performance over the Transformer on supervised and unsupervised learning tasks. The reproducible codes and pretrained models for the Macaron Net can be found at https://github.com/zhuohan123/macaron-net.",1
"With the maturity of visual detection techniques, we are more ambitious in describing visual content with open-vocabulary, fine-grained and free-form language, i.e., the task of image captioning. In particular, we are interested in generating longer, richer and more fine-grained sentences and paragraphs as image descriptions. Image captioning can be translated to the task of sequential language prediction given visual content, where the output sequence forms natural language description with plausible grammar. However, existing image captioning methods focus only on language policy while not visual policy, and thus fail to capture visual context that are crucial for compositional reasoning such as object relationships (e.g., ""man riding horse"") and visual comparisons (e.g., ""small(er) cat""). This issue is especially severe when generating longer sequences such as a paragraph. To fill the gap, we propose a Context-Aware Visual Policy network (CAVP) for fine-grained image-to-language generation: image sentence captioning and image paragraph captioning. During captioning, CAVP explicitly considers the previous visual attentions as context, and decides whether the context is used for the current word/sentence generation given the current visual attention. Compared against traditional visual attention mechanism that only fixes a single visual region at each step, CAVP can attend to complex visual compositions over time. The whole image captioning model -- CAVP and its subsequent language policy network -- can be efficiently optimized end-to-end by using an actor-critic policy gradient method. We have demonstrated the effectiveness of CAVP by state-of-the-art performances on MS-COCO and Stanford captioning datasets, using various metrics and sensible visualizations of qualitative visual context.",0
"As visual detection techniques become more sophisticated, our aspirations for describing visual content with intricate and versatile language have grown. This is evident in the task of image captioning, which involves generating longer, more detailed sentences and paragraphs as image descriptions. However, current image captioning methods tend to focus solely on language policy and overlook the importance of visual policy. As a result, they struggle to capture visual context that is essential for compositional reasoning, such as object relationships and visual comparisons, especially when generating longer sequences like paragraphs. To address this issue, we propose a Context-Aware Visual Policy network (CAVP) for fine-grained image-to-language generation. CAVP considers previous visual attentions as context during captioning and determines whether to use that context for the current word/sentence generation based on the current visual attention. Unlike traditional visual attention mechanisms that only focus on a single visual region at a time, CAVP can attend to complex visual compositions over time. The entire image captioning model, including CAVP and its subsequent language policy network, can be efficiently optimized end-to-end using an actor-critic policy gradient method. We have demonstrated the effectiveness of CAVP through state-of-the-art performances on MS-COCO and Stanford captioning datasets, using various metrics and sensible visualizations of qualitative visual context.",1
"Grounding natural language in images, such as localizing ""the black dog on the left of the tree"", is one of the core problems in artificial intelligence, as it needs to comprehend the fine-grained and compositional language space. However, existing solutions rely on the association between the holistic language features and visual features, while neglect the nature of compositional reasoning implied in the language. In this paper, we propose a natural language grounding model that can automatically compose a binary tree structure for parsing the language and then perform visual reasoning along the tree in a bottom-up fashion. We call our model RVG-TREE: Recursive Grounding Tree, which is inspired by the intuition that any language expression can be recursively decomposed into two constituent parts, and the grounding confidence score can be recursively accumulated by calculating their grounding scores returned by sub-trees. RVG-TREE can be trained end-to-end by using the Straight-Through Gumbel-Softmax estimator that allows the gradients from the continuous score functions passing through the discrete tree construction. Experiments on several benchmarks show that our model achieves the state-of-the-art performance with more explainable reasoning.",0
"Artificial intelligence faces a major challenge of grounding natural language in images, such as identifying the location of ""the black dog on the left of the tree"". This requires understanding the complex and composite language space. However, current solutions overlook the compositional reasoning implicit in the language, relying only on the association between global language features and visual features. To address this, we introduce the RVG-TREE model, which automatically composes a binary tree structure to parse the language and perform visual reasoning in a bottom-up manner. The model accumulates grounding confidence scores by recursively calculating the scores returned by sub-trees. RVG-TREE can be trained end-to-end using the Straight-Through Gumbel-Softmax estimator, enabling gradients to pass through the discrete tree construction. Our experiments on various benchmarks demonstrate that our model achieves state-of-the-art performance while providing more explainable reasoning.",1
"In this work, we propose a goal-driven collaborative task that combines language, perception, and action. Specifically, we develop a Collaborative image-Drawing game between two agents, called CoDraw. Our game is grounded in a virtual world that contains movable clip art objects. The game involves two players: a Teller and a Drawer. The Teller sees an abstract scene containing multiple clip art pieces in a semantically meaningful configuration, while the Drawer tries to reconstruct the scene on an empty canvas using available clip art pieces. The two players communicate with each other using natural language. We collect the CoDraw dataset of ~10K dialogs consisting of ~138K messages exchanged between human players. We define protocols and metrics to evaluate learned agents in this testbed, highlighting the need for a novel ""crosstalk"" evaluation condition which pairs agents trained independently on disjoint subsets of the training data. We present models for our task and benchmark them using both fully automated evaluation and by having them play the game live with humans.",0
"Our proposal introduces a collaborative task that integrates language, perception, and action. The task involves a game called CoDraw that requires two players, a Teller and a Drawer, to work together in a virtual world with movable clip art objects. The Teller observes an abstract scene with multiple clip art pieces arranged in a semantically meaningful configuration, while the Drawer attempts to replicate the scene on a blank canvas using the available clip art pieces. Both players communicate using natural language. We collected a CoDraw dataset of approximately 10,000 dialogues composed of around 138,000 messages exchanged between human players. We established protocols and metrics to assess trained agents in this experimental setting, highlighting the importance of a new ""crosstalk"" evaluation condition that pairs agents trained on distinct subsets of the training data. We present models for our task and evaluate their performance using automated evaluation methods and live gameplay with human players.",1
"Exploiting relationships among objects has achieved remarkable progress in interpreting images or videos by natural language. Most existing methods resort to first detecting objects and their relationships, and then generating textual descriptions, which heavily depends on pre-trained detectors and leads to performance drop when facing problems of heavy occlusion, tiny-size objects and long-tail in object detection. In addition, the separate procedure of detecting and captioning results in semantic inconsistency between the pre-defined object/relation categories and the target lexical words. We exploit prior human commonsense knowledge for reasoning relationships between objects without any pre-trained detectors and reaching semantic coherency within one image or video in captioning. The prior knowledge (e.g., in the form of knowledge graph) provides commonsense semantic correlation and constraint between objects that are not explicit in the image and video, serving as useful guidance to build semantic graph for sentence generation. Particularly, we present a joint reasoning method that incorporates 1) commonsense reasoning for embedding image or video regions into semantic space to build semantic graph and 2) relational reasoning for encoding semantic graph to generate sentences. Extensive experiments on the MS-COCO image captioning benchmark and the MSVD video captioning benchmark validate the superiority of our method on leveraging prior commonsense knowledge to enhance relational reasoning for visual captioning.",0
"The use of object relationships has greatly advanced the interpretation of images and videos through natural language. However, most current methods rely on detecting objects and their relationships and generating textual descriptions, which heavily rely on pre-trained detectors and suffer from performance issues when dealing with occlusion, small objects, and rare objects. Additionally, detecting and captioning separately leads to semantic inconsistency between object/relation categories and target lexical words. Our approach utilizes prior human commonsense knowledge to reason relationships between objects and achieve semantic coherence within one image or video. This prior knowledge, in the form of a knowledge graph, provides useful guidance for building a semantic graph for sentence generation. Our joint reasoning method incorporates commonsense reasoning for embedding image or video regions into a semantic space to build a semantic graph and relational reasoning for encoding the semantic graph for sentence generation. We have conducted extensive experiments on the MS-COCO image captioning benchmark and the MSVD video captioning benchmark, which validate the superiority of our method in leveraging prior commonsense knowledge to enhance relational reasoning for visual captioning.",1
"In recent years, transfer learning gained particular interest in the field of vision and natural language processing. In the research field of vision, e.g., deep neural networks and transfer learning techniques achieve almost perfect classification scores within minutes. Nonetheless, these techniques are not yet widely applied in other domains. Therefore, this article identifies critical challenges and shows potential solutions for power forecasts in the field of renewable energies. It proposes a framework utilizing transfer learning techniques in wind power forecasts with limited or no historical data. On the one hand, this allows evaluating the applicability of transfer learning in the field of renewable energy. On the other hand, by developing automatic procedures, we assure that the proposed methods provide a framework that applies to domains in organic computing as well.",0
"The field of vision and natural language processing has recently shown a particular interest in transfer learning. This approach, which includes deep neural networks, has led to near-perfect classification scores in just a few minutes. However, these techniques have not yet been widely applied in other areas. Therefore, this article aims to identify critical challenges and potential solutions for power forecasts in renewable energies. To achieve this, a framework that utilizes transfer learning techniques in wind power forecasts with limited or no historical data is proposed. This serves to evaluate the applicability of transfer learning in renewable energy and ensures that the proposed methods are applicable to other domains in organic computing through the development of automatic procedures.",1
"In this paper, the problem of describing visual contents of a video sequence with natural language is addressed. Unlike previous video captioning work mainly exploiting the cues of video contents to make a language description, we propose a reconstruction network (RecNet) in a novel encoder-decoder-reconstructor architecture, which leverages both forward (video to sentence) and backward (sentence to video) flows for video captioning. Specifically, the encoder-decoder component makes use of the forward flow to produce a sentence description based on the encoded video semantic features. Two types of reconstructors are subsequently proposed to employ the backward flow and reproduce the video features from local and global perspectives, respectively, capitalizing on the hidden state sequence generated by the decoder. Moreover, in order to make a comprehensive reconstruction of the video features, we propose to fuse the two types of reconstructors together. The generation loss yielded by the encoder-decoder component and the reconstruction loss introduced by the reconstructor are jointly cast into training the proposed RecNet in an end-to-end fashion. Furthermore, the RecNet is fine-tuned by CIDEr optimization via reinforcement learning, which significantly boosts the captioning performance. Experimental results on benchmark datasets demonstrate that the proposed reconstructor can boost the performance of video captioning consistently.",0
"This paper addresses the challenge of describing the visual contents of a video using natural language. Previous video captioning approaches have relied solely on video content cues to generate language descriptions. However, our proposed solution, called the reconstruction network (RecNet), adopts a novel encoder-decoder-reconstructor architecture that incorporates both forward and backward flows for video captioning. The encoder-decoder component uses the forward flow to generate a sentence description based on the encoded video semantic features. We introduce two types of reconstructors that use the backward flow to reproduce the video features from local and global perspectives, respectively, leveraging the hidden state sequence generated by the decoder. To achieve a comprehensive reconstruction of the video features, we fuse the two types of reconstructors. Training the RecNet is accomplished through joint optimization of the generation loss produced by the encoder-decoder component and the reconstruction loss introduced by the reconstructor. We further enhance the RecNet performance through CIDEr optimization via reinforcement learning. Experimental results on benchmark datasets show that the proposed reconstructor consistently improves video captioning performance.",1
"Deep neural networks (DNNs) have recently achieved state-of-the-art performance and provide significant progress in many machine learning tasks, such as image classification, speech processing, natural language processing, etc. However, recent studies have shown that DNNs are vulnerable to adversarial attacks. For instance, in the image classification domain, adding small imperceptible perturbations to the input image is sufficient to fool the DNN and to cause misclassification. The perturbed image, called \textit{adversarial example}, should be visually as close as possible to the original image. However, all the works proposed in the literature for generating adversarial examples have used the $L_{p}$ norms ($L_{0}$, $L_{2}$ and $L_{\infty}$) as distance metrics to quantify the similarity between the original image and the adversarial example. Nonetheless, the $L_{p}$ norms do not correlate with human judgment, making them not suitable to reliably assess the perceptual similarity/fidelity of adversarial examples. In this paper, we present a database for visual fidelity assessment of adversarial examples. We describe the creation of the database and evaluate the performance of fifteen state-of-the-art full-reference (FR) image fidelity assessment metrics that could substitute $L_{p}$ norms. The database as well as subjective scores are publicly available to help designing new metrics for adversarial examples and to facilitate future research works.",0
"In recent times, deep neural networks (DNNs) have exhibited exceptional performance and advancement in different machine learning tasks like natural language processing, speech processing, image classification, etc. However, recent research has demonstrated that DNNs can be susceptible to adversarial attacks. For example, in image classification, appending small, undetectable perturbations to the input image can deceive the DNN and result in misclassification. The perturbed image, also known as an ""adversarial example,"" should appear visually similar to the original image. Nevertheless, all research conducted thus far on generating adversarial examples has used $L_{p}$ norms ($L_{0}$, $L_{2}$, and $L_{\infty}$) as metrics to measure the distance between the original image and the adversarial example. However, since the $L_{p}$ norms are not in agreement with human judgement, they cannot be relied upon to assess the perceptual similarity/fidelity of adversarial examples. Consequently, we have compiled a database for visual fidelity assessment of adversarial examples. We have elaborated on the formation of the database and analyzed the performance of fifteen state-of-the-art full-reference (FR) image fidelity evaluation metrics that could be alternatives to $L_{p}$ norms. The database and subjective scores are open to the public to aid in the creation of new metrics for adversarial examples and facilitate further research.",1
"We consider the problem of learning from sparse and underspecified rewards, where an agent receives a complex input, such as a natural language instruction, and needs to generate a complex response, such as an action sequence, while only receiving binary success-failure feedback. Such success-failure rewards are often underspecified: they do not distinguish between purposeful and accidental success. Generalization from underspecified rewards hinges on discounting spurious trajectories that attain accidental success, while learning from sparse feedback requires effective exploration. We address exploration by using a mode covering direction of KL divergence to collect a diverse set of successful trajectories, followed by a mode seeking KL divergence to train a robust policy. We propose Meta Reward Learning (MeRL) to construct an auxiliary reward function that provides more refined feedback for learning. The parameters of the auxiliary reward function are optimized with respect to the validation performance of a trained policy. The MeRL approach outperforms our alternative reward learning technique based on Bayesian Optimization, and achieves the state-of-the-art on weakly-supervised semantic parsing. It improves previous work by 1.2% and 2.4% on WikiTableQuestions and WikiSQL datasets respectively.",0
"The issue of acquiring knowledge from meager and ambiguous rewards is examined in this paper. When an agent is given a complex input, like a natural language command, and must generate a complex response, such as a sequence of actions, while only receiving binary feedback indicating success or failure, the rewards are considered sparse and underspecified. These rewards do not distinguish between deliberate and unintentional successes, making generalization difficult. To learn from these rewards, one must discard spurious trajectories that lead to accidental success and engage in effective exploration. To accomplish this, the authors employ a mode covering KL divergence to obtain a varied collection of successful trajectories, followed by a mode-seeking KL divergence to train a robust policy. To provide more refined feedback for learning, they develop Meta Reward Learning (MeRL), which constructs an auxiliary reward function. The auxiliary reward function's parameters are adjusted based on a trained policy's validation performance. The MeRL method outperforms an alternative reward learning technique based on Bayesian Optimization and achieves state-of-the-art results on weakly-supervised semantic parsing. It outperforms previous work by 1.2% and 2.4% on the WikiTableQuestions and WikiSQL datasets, respectively.",1
"Recent reinforcement learning (RL) approaches have shown strong performance in complex domains such as Atari games, but are often highly sample inefficient. A common approach to reduce interaction time with the environment is to use reward shaping, which involves carefully designing reward functions that provide the agent intermediate rewards for progress towards the goal. However, designing appropriate shaping rewards is known to be difficult as well as time-consuming. In this work, we address this problem by using natural language instructions to perform reward shaping. We propose the LanguagE-Action Reward Network (LEARN), a framework that maps free-form natural language instructions to intermediate rewards based on actions taken by the agent. These intermediate language-based rewards can seamlessly be integrated into any standard reinforcement learning algorithm. We experiment with Montezuma's Revenge from the Atari Learning Environment, a popular benchmark in RL. Our experiments on a diverse set of 15 tasks demonstrate that, for the same number of interactions with the environment, language-based rewards lead to successful completion of the task 60% more often on average, compared to learning without language.",0
"Although recent reinforcement learning (RL) techniques have exhibited impressive performance in intricate domains like Atari games, they suffer from being highly inefficient in terms of the samples required. A common method to reduce interaction time with the environment is reward shaping, which involves creating reward functions that give the agent intermediate rewards for making progress towards the goal. However, shaping appropriate rewards is a difficult and time-consuming task. To tackle this issue, we propose a solution which employs natural language instructions to perform reward shaping. Our framework, called LanguagE-Action Reward Network (LEARN), maps free-form natural language instructions to intermediate rewards based on actions taken by the agent. These intermediate language-based rewards can be easily integrated into any standard reinforcement learning algorithm. We conducted experiments using Montezuma's Revenge, a popular benchmark in RL, and our results show that, for the same number of interactions with the environment, language-based rewards lead to successful task completion 60% more often on average than learning without language across a diverse set of 15 tasks.",1
"Inspired by the recent successes of deep learning on Computer Vision and Natural Language Processing, we present a deep learning approach for recognizing scanned receipts. The recognition system has two main modules: text detection based on Connectionist Text Proposal Network and text recognition based on Attention-based Encoder-Decoder. We also proposed pre-processing to extract receipt area and OCR verification to ignore handwriting. The experiments on the dataset of the Robust Reading Challenge on Scanned Receipts OCR and Information Extraction 2019 demonstrate that the accuracies were improved by integrating the pre-processing and the OCR verification. Our recognition system achieved 71.9% of the F1 score for detection and recognition task.",0
"We introduce a deep learning technique for scanned receipt recognition, following the recent triumphs of deep learning in Computer Vision and Natural Language Processing. The recognition system comprises two primary components: text detection, which utilizes a Connectionist Text Proposal Network, and text recognition, which uses an Attention-based Encoder-Decoder. Additionally, we suggest pre-processing to extract the receipt area and OCR verification to avoid handwriting. Our experiments on the Robust Reading Challenge on Scanned Receipts OCR and Information Extraction 2019 dataset indicate that the incorporation of pre-processing and OCR verification enhanced the accuracies. Our recognition system attained a 71.9% F1 score for detection and recognition task.",1
"Vision-to-language tasks aim to integrate computer vision and natural language processing together, which has attracted the attention of many researchers. For typical approaches, they encode image into feature representations and decode it into natural language sentences. While they neglect high-level semantic concepts and subtle relationships between image regions and natural language elements. To make full use of these information, this paper attempt to exploit the text guided attention and semantic-guided attention (SA) to find the more correlated spatial information and reduce the semantic gap between vision and language. Our method includes two level attention networks. One is the text-guided attention network which is used to select the text-related regions. The other is SA network which is used to highlight the concept-related regions and the region-related concepts. At last, all these information are incorporated to generate captions or answers. Practically, image captioning and visual question answering experiments have been carried out, and the experimental results have shown the excellent performance of the proposed approach.",0
"The integration of computer vision and natural language processing in vision-to-language tasks has captured the attention of numerous researchers. However, traditional approaches tend to overlook high-level semantic concepts and subtle relationships between image regions and natural language components. To address this, our paper proposes a method that utilizes text-guided attention and semantic-guided attention (SA) to identify more correlated spatial information and minimize the semantic gap between vision and language. Our method comprises two attention networks - the text-guided attention network and the SA network - which select text-related regions and highlight concept-related regions and region-related concepts, respectively. By incorporating all this information, we can generate captions or answers. Our approach was tested through image captioning and visual question answering experiments, and the results demonstrate its exceptional performance.",1
"How can local-search methods such as stochastic gradient descent (SGD) avoid bad local minima in training multi-layer neural networks? Why can they fit random labels even given non-convex and non-smooth architectures? Most existing theory only covers networks with one hidden layer, so can we go deeper?   In this paper, we focus on recurrent neural networks (RNNs) which are multi-layer networks widely used in natural language processing. They are harder to analyze than feedforward neural networks, because the $\textit{same}$ recurrent unit is repeatedly applied across the entire time horizon of length $L$, which is analogous to feedforward networks of depth $L$. We show when the number of neurons is sufficiently large, meaning polynomial in the training data size and in $L$, then SGD is capable of minimizing the regression loss in the linear convergence rate. This gives theoretical evidence of how RNNs can memorize data.   More importantly, in this paper we build general toolkits to analyze multi-layer networks with ReLU activations. For instance, we prove why ReLU activations can prevent exponential gradient explosion or vanishing, and build a perturbation theory to analyze first-order approximation of multi-layer networks.",0
"The article discusses the ability of local-search methods like stochastic gradient descent (SGD) to avoid unfavorable local minima during the training of multi-layer neural networks. It also explores how such methods can fit random labels even with non-smooth or non-convex architectures. While most existing research only focuses on single hidden layer networks, the paper concentrates on recurrent neural networks (RNNs), which are multi-layer networks commonly used in natural language processing. RNNs are more challenging to analyze than feedforward neural networks because the same recurrent unit is repeatedly applied throughout the time horizon of length L. The paper argues that when the number of neurons is sufficiently large, SGD can minimize regression loss linearly, providing theoretical evidence of how RNNs can memorize data. The article also develops general toolkits for analyzing multi-layer networks with ReLU activations, including a perturbation theory for analyzing first-order approximation and proving why ReLU activations can prevent exponential gradient explosion or vanishing.",1
"Recent breakthroughs in the field of deep learning have led to advancements in a broad spectrum of tasks in computer vision, audio processing, natural language processing and other areas. In most instances where these tasks are deployed in real-world scenarios, the models used in them have been shown to be susceptible to adversarial attacks, making it imperative for us to address the challenge of their adversarial robustness. Existing techniques for adversarial robustness fall into three broad categories: defensive distillation techniques, adversarial training techniques, and randomized or non-deterministic model based techniques. In this paper, we propose a novel neural network paradigm that falls under the category of randomized models for adversarial robustness, but differs from all existing techniques under this category in that it models each parameter of the network as a statistical distribution with learnable parameters. We show experimentally that this framework is highly robust to a variety of white-box and black-box adversarial attacks, while preserving the task-specific performance of the traditional neural network model.",0
"Advancements in various areas including computer vision, audio processing, and natural language processing have been made possible due to recent breakthroughs in deep learning. However, these models are prone to adversarial attacks in real-world scenarios, highlighting the need for adversarial robustness. Currently, there are three main categories of techniques for achieving adversarial robustness: defensive distillation, adversarial training, and randomized or non-deterministic model-based techniques. This paper proposes a new neural network paradigm that falls under the randomized models category, but is unique in that it models each parameter of the network as a statistical distribution with learnable parameters. The framework is experimentally shown to be highly robust to various white-box and black-box adversarial attacks while maintaining the traditional neural network model's performance in task-specific contexts.",1
"Language grounded image understanding tasks have often been proposed as a method for evaluating progress in artificial intelligence. Ideally, these tasks should test a plethora of capabilities that integrate computer vision, reasoning, and natural language understanding. However, rather than behaving as visual Turing tests, recent studies have demonstrated state-of-the-art systems are achieving good performance through flaws in datasets and evaluation procedures. We review the current state of affairs and outline a path forward.",0
"To measure advancements in artificial intelligence, language-based image comprehension challenges have frequently been suggested. The objective is to design tasks that assess a range of abilities, encompassing computer vision, reasoning, and comprehension of natural language. Nonetheless, these tasks should not merely function as visual Turing tests. Recently, research has shown that cutting-edge systems are attaining satisfactory results due to flaws in the datasets and evaluation methods. In this article, we evaluate the current situation and propose a plan for the future.",1
"Attention mechanisms and non-local mean operations in general are key ingredients in many state-of-the-art deep learning techniques. In particular, the Transformer model based on multi-head self-attention has recently achieved great success in natural language processing and computer vision. However, the vanilla algorithm computing the Transformer of an image with n pixels has O(n^2) complexity, which is often painfully slow and sometimes prohibitively expensive for large-scale image data. In this paper, we propose a fast randomized algorithm --- SCRAM --- that only requires O(n log(n)) time to produce an image attention map. Such a dramatic acceleration is attributed to our insight that attention maps on real-world images usually exhibit (1) spatial coherence and (2) sparse structure. The central idea of SCRAM is to employ PatchMatch, a randomized correspondence algorithm, to quickly pinpoint the most compatible key (argmax) for each query first, and then exploit that knowledge to design a sparse approximation to non-local mean operations. Using the argmax (mode) to dynamically construct the sparse approximation distinguishes our algorithm from all of the existing sparse approximate methods and makes it very efficient. Moreover, SCRAM is a broadly applicable approximation to any non-local mean layer in contrast to some other sparse approximations that can only approximate self-attention. Our preliminary experimental results suggest that SCRAM is indeed promising for speeding up or scaling up the computation of attention maps in the Transformer.",0
"Many advanced deep learning techniques rely on attention mechanisms and non-local mean operations. The Transformer model, which utilizes multi-head self-attention, has demonstrated remarkable success in computer vision and natural language processing. However, the standard algorithm for computing the Transformer of an n-pixel image has an O(n^2) complexity, which can prove to be slow and costly for large-scale image data. This paper introduces a randomized algorithm called SCRAM, which can produce an image attention map in O(n log(n)) time. The key to this acceleration lies in the observation that attention maps on real-world images often exhibit spatial coherence and sparsity. SCRAM leverages these characteristics by using a randomized correspondence algorithm called PatchMatch to identify the most compatible key for each query. This information is then used to design a sparse approximation to non-local mean operations. Unlike other sparse approximations, SCRAM dynamically constructs the sparse approximation using the argmax (mode), making it very efficient. Additionally, SCRAM is applicable to any non-local mean layer, not just self-attention. Preliminary experimental results show that SCRAM is a promising method for accelerating or scaling up attention map computation in the Transformer.",1
"Standard methods in deep learning for natural language processing fail to capture the compositional structure of human language that allows for systematic generalization outside of the training distribution. However, human learners readily generalize in this way, e.g. by applying known grammatical rules to novel words. Inspired by work in neuroscience suggesting separate brain systems for syntactic and semantic processing, we implement a modification to standard approaches in neural machine translation, imposing an analogous separation. The novel model, which we call Syntactic Attention, substantially outperforms standard methods in deep learning on the SCAN dataset, a compositional generalization task, without any hand-engineered features or additional supervision. Our work suggests that separating syntactic from semantic learning may be a useful heuristic for capturing compositional structure.",0
"Conventional deep learning techniques used in natural language processing are inadequate in capturing the fundamental compositional structure of human language that enables systematic generalization beyond the scope of the training dataset. On the other hand, humans are capable of generalizing effortlessly by applying their knowledge of grammatical rules to new words. We have taken inspiration from neuroscience research that indicates the presence of distinct brain systems for semantic and syntactic processing and have introduced a modification to the standard neural machine translation methods that incorporates a similar separation. Our novel model, Syntactic Attention, performs significantly better than standard deep learning approaches on the SCAN dataset, which tests compositional generalization without the need for any manual feature engineering or extra supervision. Our study suggests that separating syntactic and semantic learning could prove to be a valuable heuristic in capturing the compositional structure of language.",1
"In recent years, deep learning has achieved remarkable achievements in many fields, including computer vision, natural language processing, speech recognition and others. Adequate training data is the key to ensure the effectiveness of the deep models. However, obtaining valid data requires a lot of time and labor resources. Data augmentation (DA) is an effective alternative approach, which can generate new labeled data based on existing data using label-preserving transformations. Although we can benefit a lot from DA, designing appropriate DA policies requires a lot of expert experience and time consumption, and the evaluation of searching the optimal policies is costly. So we raise a new question in this paper: how to achieve automated data augmentation at as low cost as possible? We propose a method named BO-Aug for automating the process by finding the optimal DA policies using the Bayesian optimization approach. Our method can find the optimal policies at a relatively low search cost, and the searched policies based on a specific dataset are transferable across different neural network architectures or even different datasets. We validate the BO-Aug on three widely used image classification datasets, including CIFAR-10, CIFAR-100 and SVHN. Experimental results show that the proposed method can achieve state-of-the-art or near advanced classification accuracy. Code to reproduce our experiments is available at https://github.com/zhangxiaozao/BO-Aug.",0
"In many areas such as computer vision, natural language processing, and speech recognition, deep learning has made remarkable advancements in recent years. However, effective and sufficient training data is crucial for the performance of deep models. Obtaining valid data is a time and labor-intensive task, and as an alternative approach, data augmentation (DA) can generate new labeled data based on existing data by using label-preserving transformations. Although DA can be very beneficial, it requires a lot of expert experience and time to design appropriate DA policies, and the optimal policies evaluation is costly. Therefore, this paper introduces a new question: how can we achieve automated data augmentation at a low cost? The proposed method, BO-Aug, uses the Bayesian optimization approach to find optimal DA policies, automating the process. The method can find optimal policies at a relatively low search cost, and the searched policies are transferable across different neural network architectures or even different datasets. BO-Aug is validated on three widely used image classification datasets, CIFAR-10, CIFAR-100, and SVHN, and the experimental results illustrate that the proposed method can achieve near advanced classification accuracy. The code to reproduce the experiments is available at https://github.com/zhangxiaozao/BO-Aug.",1
"In this work we present a new efficient approach to Human Action Recognition called Video Transformer Network (VTN). It leverages the latest advances in Computer Vision and Natural Language Processing and applies them to video understanding. The proposed method allows us to create lightweight CNN models that achieve high accuracy and real-time speed using just an RGB mono camera and general purpose CPU. Furthermore, we explain how to improve accuracy by distilling from multiple models with different modalities into a single model. We conduct a comparison with state-of-the-art methods and show that our approach performs on par with most of them on famous Action Recognition datasets. We benchmark the inference time of the models using the modern inference framework and argue that our approach compares favorably with other methods in terms of speed/accuracy trade-off, running at 56 FPS on CPU. The models and the training code are available.",0
"The following passage introduces a new approach to Human Action Recognition called Video Transformer Network (VTN), which utilizes the latest developments in Computer Vision and Natural Language Processing to improve video understanding. The technique enables the development of lightweight CNN models that achieve high accuracy and real-time speed using only an RGB mono camera and general-purpose CPU. Additionally, the passage details how to enhance accuracy by consolidating multiple models with different modalities into a single model. The authors compare their method with other state-of-the-art techniques and demonstrate that it performs similarly on Action Recognition datasets. Using modern inference frameworks, the authors benchmark the models' inference time, and argue that their approach outperforms other methods in terms of speed/accuracy trade-off, running at 56 FPS on CPU. The training code and models are available.",1
"Text generation is an important Natural Language Processing task with various applications. Although several metrics have already been introduced to evaluate the text generation methods, each of them has its own shortcomings. The most widely used metrics such as BLEU only consider the quality of generated sentences and neglect their diversity. For example, repeatedly generation of only one high quality sentence would result in a high BLEU score. On the other hand, the more recent metric introduced to evaluate the diversity of generated texts known as Self-BLEU ignores the quality of generated texts. In this paper, we propose metrics to evaluate both the quality and diversity simultaneously by approximating the distance of the learned generative model and the real data distribution. For this purpose, we first introduce a metric that approximates this distance using n-gram based measures. Then, a feature-based measure which is based on a recent highly deep model trained on a large text corpus called BERT is introduced. Finally, for oracle training mode in which the generator's density can also be calculated, we propose to use the distance measures between the corresponding explicit distributions. Eventually, the most popular and recent text generation models are evaluated using both the existing and the proposed metrics and the preferences of the proposed metrics are determined.",0
"Generating text is a crucial task in Natural Language Processing and has numerous applications. However, the existing metrics for evaluating text generation methods have their own limitations. Commonly used metrics like BLEU only consider the quality of generated sentences and disregard their diversity. This means that generating the same high-quality sentence multiple times can result in a high BLEU score. On the other hand, Self-BLEU, a more recent metric for measuring diversity, doesn't consider the quality of generated text. In this study, we propose a set of metrics that evaluate both the quality and diversity of generated text by approximating the distance between the learned generative model and the real data distribution. We introduce an n-gram based metric to calculate this distance, as well as a feature-based metric based on BERT, a highly deep model trained on a large text corpus. Moreover, we propose to use the distance measures between corresponding explicit distributions for oracle training mode. We evaluate popular text generation models using both the existing and proposed metrics to determine the preferences of the new metrics.",1
"Self-attention (SA) mechanisms can capture effectively global dependencies in deep neural networks, and have been applied to natural language processing and image processing successfully. However, SA modules for image reconstruction have high time and space complexity, which restrict their applications to higher-resolution images. In this paper, we refine the SA module in self-attention generative adversarial networks (SAGAN) via adapting a non-local operation, revising the connectivity among the units in SA module and re-implementing its computational pattern, such that its time and space complexity is reduced from $\text{O}(n^2)$ to $\text{O}(n)$, but it is still equivalent to the original SA module. Further, we explore the principles behind the module and discover that our module is a special kind of channel attention mechanisms. Experimental results based on two benchmark datasets of image reconstruction, verify that under the same computational environment, two models can achieve comparable effectiveness for image reconstruction, but the proposed one runs faster and takes up less memory space.",0
"Self-attention (SA) mechanisms are effective in capturing global dependencies in deep neural networks, and have been successfully applied in natural language processing and image processing. However, the high time and space complexity of SA modules for image reconstruction limits their application to higher-resolution images. This paper presents a refined SA module in self-attention generative adversarial networks (SAGAN) that utilizes a non-local operation, modifies the connectivity among SA module units, and re-implements its computational pattern to reduce its time and space complexity from $\text{O}(n^2)$ to $\text{O}(n)$, while maintaining its equivalence to the original SA module. Additionally, we discovered that our module is a special type of channel attention mechanism. Experimental results using two benchmark datasets for image reconstruction demonstrate that our proposed module achieves comparable effectiveness for image reconstruction under the same computational environment, but runs faster and requires less memory space than the original SA module.",1
"Image captioning aims to automatically generate a natural language description of a given image, and most state-of-the-art models have adopted an encoder-decoder framework. The framework consists of a convolution neural network (CNN)-based image encoder that extracts region-based visual features from the input image, and an recurrent neural network (RNN)-based caption decoder that generates the output caption words based on the visual features with the attention mechanism. Despite the success of existing studies, current methods only model the co-attention that characterizes the inter-modal interactions while neglecting the self-attention that characterizes the intra-modal interactions. Inspired by the success of the Transformer model in machine translation, here we extend it to a Multimodal Transformer (MT) model for image captioning. Compared to existing image captioning approaches, the MT model simultaneously captures intra- and inter-modal interactions in a unified attention block. Due to the in-depth modular composition of such attention blocks, the MT model can perform complex multimodal reasoning and output accurate captions. Moreover, to further improve the image captioning performance, multi-view visual features are seamlessly introduced into the MT model. We quantitatively and qualitatively evaluate our approach using the benchmark MSCOCO image captioning dataset and conduct extensive ablation studies to investigate the reasons behind its effectiveness. The experimental results show that our method significantly outperforms the previous state-of-the-art methods. With an ensemble of seven models, our solution ranks the 1st place on the real-time leaderboard of the MSCOCO image captioning challenge at the time of the writing of this paper.",0
"The objective of image captioning is to create a description of an image using natural language, and the most advanced models use an encoder-decoder framework. This framework includes a convolution neural network (CNN) that functions as an image encoder, extracting visual features from specific regions of the input image. Additionally, it includes a recurrent neural network (RNN) that serves as a caption decoder, generating the caption words based on the visual features and attention mechanism. Although existing studies have been successful, they only focus on co-attention, neglecting self-attention, which characterizes intra-modal interactions. We were inspired by the Transformer model's success in machine translation and extended it to create a Multimodal Transformer (MT) model for image captioning. Unlike other approaches, the MT model captures both intra- and inter-modal interactions in a unified attention block, allowing for complex multimodal reasoning and accurate captions. We also introduced multi-view visual features to enhance image captioning performance. Our approach was evaluated using the MSCOCO image captioning dataset, and extensive ablation studies were conducted to understand its effectiveness. Our method significantly outperformed previous state-of-the-art approaches, and our ensemble of seven models ranked first on the real-time leaderboard of the MSCOCO image captioning challenge at the time of writing.",1
"Deep Neural Networks have achieved remarkable success in computer vision, natural language processing, and audio tasks.",0
"Computer vision, natural language processing, and audio tasks have all seen impressive levels of accomplishment thanks to Deep Neural Networks.",1
"Machine learning models that take computer program source code as input typically use Natural Language Processing (NLP) techniques. However, a major challenge is that code is written using an open, rapidly changing vocabulary due to, e.g., the coinage of new variable and method names. Reasoning over such a vocabulary is not something for which most NLP methods are designed. We introduce a Graph-Structured Cache to address this problem; this cache contains a node for each new word the model encounters with edges connecting each word to its occurrences in the code. We find that combining this graph-structured cache strategy with recent Graph-Neural-Network-based models for supervised learning on code improves the models' performance on a code completion task and a variable naming task --- with over $100\%$ relative improvement on the latter --- at the cost of a moderate increase in computation time.",0
"When machine learning models use computer program source code as input, they often rely on Natural Language Processing (NLP) techniques. However, the use of an open, rapidly changing vocabulary in code, such as the creation of new variable and method names, presents a significant challenge. Most NLP methods are not designed to handle this type of vocabulary. To address this issue, we propose the use of a Graph-Structured Cache. This cache contains a node for each new word the model encounters in the code, with edges linking the word to its occurrences. By combining this strategy with recent Graph-Neural-Network-based models for supervised learning on code, we can enhance the models' performance on tasks such as code completion and variable naming. While this approach does come with a moderate increase in computation time, we have achieved over a 100% relative improvement in the variable naming task.",1
"This research strives for natural language moment retrieval in long, untrimmed video streams. The problem is not trivial especially when a video contains multiple moments of interests and the language describes complex temporal dependencies, which often happens in real scenarios. We identify two crucial challenges: semantic misalignment and structural misalignment. However, existing approaches treat different moments separately and do not explicitly model complex moment-wise temporal relations. In this paper, we present Moment Alignment Network (MAN), a novel framework that unifies the candidate moment encoding and temporal structural reasoning in a single-shot feed-forward network. MAN naturally assigns candidate moment representations aligned with language semantics over different temporal locations and scales. Most importantly, we propose to explicitly model moment-wise temporal relations as a structured graph and devise an iterative graph adjustment network to jointly learn the best structure in an end-to-end manner. We evaluate the proposed approach on two challenging public benchmarks DiDeMo and Charades-STA, where our MAN significantly outperforms the state-of-the-art by a large margin.",0
"The aim of this study is to achieve natural language moment retrieval in lengthy, untrimmed video streams, which is a difficult task when a video contains multiple moments of interest and the language used describes complex temporal dependencies, as is often the case in real-life scenarios. Two major difficulties have been identified: semantic and structural misalignment. However, current methods treat different moments separately and do not explicitly model complex moment-wise temporal relations. In this paper, the Moment Alignment Network (MAN) is introduced, which combines candidate moment encoding and temporal structural reasoning in a single-shot feed-forward network. MAN assigns candidate moment representations according to language semantics, across different temporal scales and locations. The proposed approach also explicitly models moment-wise temporal relations as a structured graph and utilizes an iterative graph adjustment network to learn the best structure in an end-to-end manner. MAN was evaluated on two challenging public benchmarks, DiDeMo and Charades-STA, where it significantly outperformed the state-of-the-art by a large margin.",1
"Neural networks (NN) are considered as black-boxes due to the lack of explainability and transparency of their decisions. This significantly hampers their deployment in environments where explainability is essential along with the accuracy of the system. Recently, significant efforts have been made for the interpretability of these deep networks with the aim to open up the black-box. However, most of these approaches are specifically developed for visual modalities. In addition, the interpretations provided by these systems require expert knowledge and understanding for intelligibility. This indicates a vital gap between the explainability provided by the systems and the novice user. To bridge this gap, we present a novel framework i.e. Time-Series eXplanation (TSXplain) system which produces a natural language based explanation of the decision taken by a NN. It uses the extracted statistical features to describe the decision of a NN, merging the deep learning world with that of statistics. The two-level explanation provides ample description of the decision made by the network to aid an expert as well as a novice user alike. Our survey and reliability assessment test confirm that the generated explanations are meaningful and correct. We believe that generating natural language based descriptions of the network's decisions is a big step towards opening up the black-box.",0
"Due to their lack of transparency, neural networks (NN) are often perceived as black-boxes, hindering their deployment in settings where explainability is crucial for accuracy. While there have been efforts to improve the interpretability of deep networks, most approaches are limited to visual modalities and require expert knowledge to understand. This creates a gap between the system's explainability and the comprehension of novice users. To address this, we propose a novel framework called Time-Series eXplanation (TSXplain) that utilizes statistical features to generate natural language-based explanations of NN decisions. This two-level explanation is designed to aid both experts and novices in understanding the network's decision-making process. Our survey and reliability assessment test confirmed the meaningfulness and accuracy of the generated explanations, which we believe is a significant step towards opening up the black-box of neural networks.",1
"Click through rate (CTR) estimation is a fundamental task in personalized advertising and recommender systems. Recent years have witnessed the success of both the deep learning based model and attention mechanism in various tasks in computer vision (CV) and natural language processing (NLP). How to combine the attention mechanism with deep CTR model is a promising direction because it may ensemble the advantages of both sides. Although some CTR model such as Attentional Factorization Machine (AFM) has been proposed to model the weight of second order interaction features, we posit the evaluation of feature importance before explicit feature interaction procedure is also important for CTR prediction tasks because the model can learn to selectively highlight the informative features and suppress less useful ones if the task has many input features. In this paper, we propose a new neural CTR model named Field Attentive Deep Field-aware Factorization Machine (FAT-DeepFFM) by combining the Deep Field-aware Factorization Machine (DeepFFM) with Compose-Excitation network (CENet) field attention mechanism which is proposed by us as an enhanced version of Squeeze-Excitation network (SENet) to highlight the feature importance. We conduct extensive experiments on two real-world datasets and the experiment results show that FAT-DeepFFM achieves the best performance and obtains different improvements over the state-of-the-art methods. We also compare two kinds of attention mechanisms (attention before explicit feature interaction vs. attention after explicit feature interaction) and demonstrate that the former one outperforms the latter one significantly.",0
"CTR estimation is a crucial task in personalized advertising and recommender systems. The use of deep learning models and attention mechanisms has proven successful in various tasks in computer vision and natural language processing. Combining these mechanisms with deep CTR models presents a promising direction as it can take advantage of their strengths. While some models like AFM have been proposed to model the weight of second-order interaction features, evaluating feature importance before explicit feature interaction is essential for CTR prediction tasks. This ensures that the model selectively highlights informative features and suppresses less useful ones. In this study, we propose a new neural CTR model, FAT-DeepFFM, by combining DeepFFM with CENet field attention mechanism to highlight feature importance. The results of extensive experiments on two real-world datasets show that FAT-DeepFFM outperforms the state-of-the-art methods. We also demonstrate that attention before explicit feature interaction is significantly better than attention after explicit feature interaction.",1
"Multi-task learning shares information between related tasks, sometimes reducing the number of parameters required. State-of-the-art results across multiple natural language understanding tasks in the GLUE benchmark have previously used transfer from a single large task: unsupervised pre-training with BERT, where a separate BERT model was fine-tuned for each task. We explore multi-task approaches that share a single BERT model with a small number of additional task-specific parameters. Using new adaptation modules, PALs or `projected attention layers', we match the performance of separately fine-tuned models on the GLUE benchmark with roughly 7 times fewer parameters, and obtain state-of-the-art results on the Recognizing Textual Entailment dataset.",0
"Sharing information between tasks, multi-task learning can reduce the required number of parameters. The GLUE benchmark has previously achieved state-of-the-art results in multiple natural language understanding tasks by transferring from a single large task, which involved unsupervised pre-training with BERT, resulting in a separate fine-tuned BERT model for each task. However, our study explores multi-task approaches that utilize a single BERT model with a limited number of task-specific parameters. By introducing new adaptation modules, known as PALs or `projected attention layers', we were able to match the performance of separately fine-tuned models on the GLUE benchmark with significantly fewer parameters (roughly 7 times less). Additionally, we achieved state-of-the-art results on the Recognizing Textual Entailment dataset.",1
"We address the problem of graph classification based only on structural information. Inspired by natural language processing techniques (NLP), our model sequentially embeds information to estimate class membership probabilities. Besides, we experiment with NLP-like variational regularization techniques, making the model predict the next node in the sequence as it reads it. We experimentally show that our model achieves state-of-the-art classification results on several standard molecular datasets. Finally, we perform a qualitative analysis and give some insights on whether the node prediction helps the model better classify graphs.",0
"Our focus is on the issue of graph classification using only structural information. We have taken inspiration from natural language processing (NLP) methods and have developed a model that sequentially incorporates information to calculate the probabilities of class membership. Additionally, we have experimented with variational regularization techniques similar to those used in NLP, where the model predicts the next node in the sequence as it reads it. Our experiments indicate that our model produces classification outcomes that are at the forefront of the industry on various standard molecular datasets. Lastly, we conduct a qualitative analysis and provide some insights into whether node prediction enhances the model's ability to accurately classify graphs.",1
"Benefiting from the advancement of computer vision, natural language processing and information retrieval techniques, visual question answering (VQA), which aims to answer questions about an image or a video, has received lots of attentions over the past few years. Although some progress has been achieved so far, several studies have pointed out that current VQA models are heavily affected by the \emph{language prior problem}, which means they tend to answer questions based on the co-occurrence patterns of question keywords (e.g., how many) and answers (e.g., 2) instead of understanding images and questions. Existing methods attempt to solve this problem by either balancing the biased datasets or forcing models to better understand images. However, only marginal effects and even performance deterioration are observed for the first and second solution, respectively. In addition, another important issue is the lack of measurement to quantitatively measure the extent of the language prior effect, which severely hinders the advancement of related techniques.   In this paper, we make contributions to solve the above problems from two perspectives. Firstly, we design a metric to quantitatively measure the language prior effect of VQA models. The proposed metric has been demonstrated to be effective in our empirical studies. Secondly, we propose a regularization method (i.e., score regularization module) to enhance current VQA models by alleviating the language prior problem as well as boosting the backbone model performance. The proposed score regularization module adopts a pair-wise learning strategy, which makes the VQA models answer the question based on the reasoning of the image (upon this question) instead of basing on question-answer patterns observed in the biased training set. The score regularization module is flexible to be integrated into various VQA models.",0
"Over the past few years, visual question answering (VQA) has gained significant attention due to the advancements in computer vision, natural language processing, and information retrieval techniques. VQA aims to answer questions about images or videos, but current models are heavily influenced by the language prior problem, resulting in biased solutions that rely on question-answer patterns rather than image understanding. Existing solutions have attempted to balance biased datasets or improve image comprehension, but with limited success. Another challenge is the lack of a metric to quantitatively measure the language prior effect, which hinders progress. This paper proposes two solutions: a metric to measure the language prior effect and a score regularization module to enhance VQA models by addressing the language prior problem and improving model performance through a pair-wise learning strategy. The proposed module is flexible and can be integrated into various VQA models.",1
"Sequence models assign probabilities to variable-length sequences such as natural language texts. The ability of sequence models to capture temporal dependence can be characterized by the temporal scaling of correlation and mutual information. In this paper, we study the mutual information of recurrent neural networks (RNNs) including long short-term memories and self-attention networks such as Transformers. Through a combination of theoretical study of linear RNNs and empirical study of nonlinear RNNs, we find their mutual information decays exponentially in temporal distance. On the other hand, Transformers can capture long-range mutual information more efficiently, making them preferable in modeling sequences with slow power-law mutual information, such as natural languages and stock prices. We discuss the connection of these results with statistical mechanics. We also point out the non-uniformity problem in many natural language datasets. We hope this work provides a new perspective in understanding the expressive power of sequence models and shed new light on improving the architecture of them.",0
"Probabilities for variable-length sequences, like natural language texts, are assigned by sequence models. The temporal scaling of correlation and mutual information is used to characterize the ability of sequence models to capture temporal dependence. This paper examines the mutual information of recurrent neural networks, including long short-term memories and self-attention networks like Transformers. By combining theoretical and empirical studies, it was discovered that mutual information decays exponentially with temporal distance for linear RNNs, whereas Transformers can more efficiently capture long-range mutual information. This makes them the preferred option for modeling sequences with slow power-law mutual information, such as natural languages and stock prices. The results are connected to statistical mechanics, and the non-uniformity problem in many natural language datasets is discussed, with the hope that this work provides a new perspective on improving the architecture of sequence models.",1
"Point cloud registration is a key problem for computer vision applied to robotics, medical imaging, and other applications. This problem involves finding a rigid transformation from one point cloud into another so that they align. Iterative Closest Point (ICP) and its variants provide simple and easily-implemented iterative methods for this task, but these algorithms can converge to spurious local optima. To address local optima and other difficulties in the ICP pipeline, we propose a learning-based method, titled Deep Closest Point (DCP), inspired by recent techniques in computer vision and natural language processing. Our model consists of three parts: a point cloud embedding network, an attention-based module combined with a pointer generation layer, to approximate combinatorial matching, and a differentiable singular value decomposition (SVD) layer to extract the final rigid transformation. We train our model end-to-end on the ModelNet40 dataset and show in several settings that it performs better than ICP, its variants (e.g., Go-ICP, FGR), and the recently-proposed learning-based method PointNetLK. Beyond providing a state-of-the-art registration technique, we evaluate the suitability of our learned features transferred to unseen objects. We also provide preliminary analysis of our learned model to help understand whether domain-specific and/or global features facilitate rigid registration.",0
"The registration of point clouds is a significant obstacle in applying computer vision to various fields, such as medical imaging and robotics. The task involves finding a rigid transformation that aligns one point cloud with another. While the Iterative Closest Point (ICP) and its variations are straightforward and practical iterative methods for this task, they can converge to incorrect local optima. To tackle these challenges, we present Deep Closest Point (DCP), a learning-based technique inspired by recent advances in natural language processing and computer vision. Our model includes a point cloud embedding network, an attention-based module with a pointer generation layer to approximate combinatorial matching, and a differentiable singular value decomposition (SVD) layer for extracting the final rigid transformation. We train our model end-to-end on the ModelNet40 dataset and demonstrate superior performance over ICP, its variations (e.g., Go-ICP, FGR), and the recently introduced learning-based algorithm PointNetLK in various settings. In addition to providing a state-of-the-art registration method, we also assess the suitability of our learned features for unseen objects and conduct preliminary analysis to investigate whether domain-specific or global features enhance rigid registration.",1
"In recent years, increasingly augmentation of health data, such as patient Electronic Health Records (EHR), are becoming readily available. This provides an unprecedented opportunity for knowledge discovery and data mining algorithms to dig insights from them, which can, later on, be helpful to the improvement of the quality of care delivery. Predictive modeling of clinical risk, including in-hospital mortality, hospital readmission, chronic disease onset, condition exacerbation, etc., from patient EHR, is one of the health data analytic problems that attract most of the interests. The reason is not only because the problem is important in clinical settings, but also there are challenges working with EHR such as sparsity, irregularity, temporality, etc. Different from applications in other domains such as computer vision and natural language processing, the labeled data samples in medicine (patients) are relatively limited, which creates lots of troubles for effective predictive model learning, especially for complicated models such as deep learning. In this paper, we propose MetaPred, a meta-learning for clinical risk prediction from longitudinal patient EHRs. In particular, in order to predict the target risk where there are limited data samples, we train a meta-learner from a set of related risk prediction tasks which learns how a good predictor is learned. The meta-learned can then be directly used in target risk prediction, and the limited available samples can be used for further fine-tuning the model performance. The effectiveness of MetaPred is tested on a real patient EHR repository from Oregon Health & Science University. We are able to demonstrate that with CNN and RNN as base predictors, MetaPred can achieve much better performance for predicting target risk with low resources comparing with the predictor trained on the limited samples available for this risk.",0
"Health data, such as patient Electronic Health Records (EHR), are increasingly available, providing a unique opportunity for data mining algorithms to extract insights and improve the quality of care delivery. One significant challenge in health data analysis is predictive modeling of clinical risk, which includes in-hospital mortality, hospital readmission, and chronic disease onset, among others. Despite the importance of this problem, there are difficulties in working with EHR, including sparsity, irregularity, and temporality. Additionally, the limited availability of labeled data samples in medicine presents challenges for effective predictive model learning, especially with complex models such as deep learning. In this paper, we propose MetaPred, a meta-learning approach for clinical risk prediction from longitudinal patient EHRs. We train a meta-learner from related risk prediction tasks to predict target risk with limited data samples. The meta-learner can be used directly in target risk prediction, and the limited available samples can be used for further fine-tuning the model. We demonstrate the effectiveness of MetaPred on a real patient EHR repository from Oregon Health & Science University, showing that with CNN and RNN as base predictors, MetaPred performs significantly better than the predictor trained on limited samples.",1
"Vanilla convolutional neural networks are known to provide superior performance not only in image recognition tasks but also in natural language processing and time series analysis. One of the strengths of convolutional layers is the ability to learn features about spatial relations in the input domain using various parameterized convolutional kernels. However, in time series analysis learning such spatial relations is not necessarily required nor effective. In such cases, kernels which model temporal dependencies or kernels with broader spatial resolutions are recommended for more efficient training as proposed by dilation kernels. However, the dilation has to be fixed a priori which limits the flexibility of the kernels. We propose generalized dilation networks which generalize the initial dilations in two aspects. First we derive an end-to-end learnable architecture for dilation layers where also the dilation rate can be learned. Second we break up the strict dilation structure, in that we develop kernels operating independently in the input space.",0
"Convolutional neural networks with vanilla architecture have been recognized for their exceptional performance in tasks involving image recognition, natural language processing, and time series analysis. The strength of the convolutional layer lies in its ability to acquire features related to spatial relations in the input domain utilizing parameterized convolutional kernels. However, in time series analysis, learning spatial relations is not mandatory, and it may not be effective. In such cases, dilation kernels that model temporal dependencies or kernels with broader spatial resolutions are recommended for efficient training. Nonetheless, the dilation must be fixed beforehand, which limits the flexibility of the kernels. To address this, we propose generalized dilation networks that improve on the initial dilations in two ways. Firstly, we develop an end-to-end learnable architecture for dilation layers, allowing the dilation rate to be learned. Secondly, we break away from the strict dilation structure by creating kernels that work independently in the input space.",1
"Automatically captioning images with natural language sentences is an important research topic. State of the art models are able to produce human-like sentences. These models typically describe the depicted scene as a whole and do not target specific objects of interest or emotional relationships between these objects in the image. However, marketing companies require to describe these important attributes of a given scene. In our case, objects of interest are consumer goods, which are usually identifiable by a product logo and are associated with certain brands. From a marketing point of view, it is desirable to also evaluate the emotional context of a trademarked product, i.e., whether it appears in a positive or a negative connotation. We address the problem of finding brands in images and deriving corresponding captions by introducing a modified image captioning network. We also add a third output modality, which simultaneously produces real-valued image ratings. Our network is trained using a classification-aware loss function in order to stimulate the generation of sentences with an emphasis on words identifying the brand of a product. We evaluate our model on a dataset of images depicting interactions between humans and branded products. The introduced network improves mean class accuracy by 24.5 percent. Thanks to adding the third output modality, it also considerably improves the quality of generated captions for images depicting branded products.",0
"The research of automatically generating natural language captions for images is crucial. The latest models have the ability to create sentences that resemble those produced by humans. However, these models typically focus on describing the entire scene rather than concentrating on specific objects of interest or the emotional relationships between them in the image. Marketing companies, on the other hand, need to describe these significant features of a given scene, particularly when it comes to consumer goods, which are identifiable by a product logo and associated with specific brands. It is desirable to evaluate the emotional context of a trademarked product, whether it is portrayed positively or negatively, from a marketing standpoint. We have addressed the challenge of identifying brands in images and creating relevant captions by modifying the existing image captioning network and adding a third output modality that produces real-valued image ratings. Our model is trained with a classification-aware loss function to emphasize words identifying the brand of a product in the generated sentences. We have evaluated our model on a dataset of images that depict interactions between people and branded products, and our network has improved the mean class accuracy by 24.5 percent. The addition of the third output modality has also significantly enhanced the quality of the generated captions for images portraying branded products.",1
"Domain classification is the task of mapping spoken language utterances to one of the natural language understanding domains in intelligent personal digital assistants (IPDAs). This is a major component in mainstream IPDAs in industry. Apart from official domains, thousands of third-party domains are also created by external developers to enhance the capability of IPDAs. As more domains are developed rapidly, the question of how to continuously accommodate the new domains still remains challenging. Moreover, existing continual learning approaches do not address the problem of incorporating personalized information dynamically for better domain classification. In this paper, we propose CoNDA, a neural network based approach for domain classification that supports incremental learning of new classes. Empirical evaluation shows that CoNDA achieves high accuracy and outperforms baselines by a large margin on both incrementally added new domains and existing domains.",0
"The main task of intelligent personal digital assistants (IPDAs) is to map spoken language utterances to natural language understanding domains through domain classification. This is a crucial component in the IPDA industry, where official domains are often supplemented by third-party domains created by external developers. Despite the rapid development of new domains, accommodating them remains a challenge. Existing continual learning methods also do not address the issue of dynamically incorporating personalized information to improve domain classification. To address these challenges, we propose CoNDA, a neural network approach that supports incremental learning of new classes. Empirical evaluation demonstrates that CoNDA achieves high accuracy and outperforms baseline models on both existing and newly added domains.",1
"Edit distance, also known as Levenshtein distance, is an essential way to compare two strings that proved to be particularly useful in the analysis of genetic sequences and natural language processing. However, edit distance is a discrete function that is known to be hard to optimize. This fact hampers the use of this metric in Machine Learning. Even as simple algorithm as K-means fails to cluster a set of sequences using edit distance if they are of variable length and abundance. In this paper we propose a novel metric - soft edit distance (SED), which is a smooth approximation of edit distance. It is differentiable and therefore it is possible to optimize it with gradient methods. Similar to original edit distance, SED as well as its derivatives can be calculated with recurrent formulas at polynomial time. We prove usefulness of the proposed metric on synthetic datasets and clustering of biological sequences.",0
"The comparison of two strings, also known as edit distance or Levenshtein distance, has been widely used in the analysis of genetic sequences and natural language processing. However, due to its discrete nature, edit distance has proven difficult to optimize, thereby limiting its applicability in Machine Learning. This has become an issue even for simple algorithms such as K-means, which are unable to cluster sequences of varying length and abundance using edit distance. To address this problem, we introduce a new metric called soft edit distance (SED), which is a smooth approximation of edit distance and can be optimized using gradient methods as it is differentiable. The calculation of SED, as well its derivatives, can be done using recurrent formulas at polynomial time, similar to the original edit distance. We demonstrate the usefulness of SED with synthetic datasets and biological sequence clustering.",1
"The abundance of open-source code, coupled with the success of recent advances in deep learning for natural language processing, has given rise to a promising new application of machine learning to source code. In this work, we explore the use of a Siamese recurrent neural network model on Python source code to create vectors which capture the semantics of code. We evaluate the quality of embeddings by identifying which problem from a programming competition the code solves. Our model significantly outperforms a bag-of-tokens embedding, providing promising results for improving code embeddings that can be used in future software engineering tasks.",0
"Recent advancements in deep learning for natural language processing have led to a promising new application of machine learning to source code, thanks to the abundance of open-source code. Our study focuses on Python source code, where we utilize a Siamese recurrent neural network model to generate vectors that capture the semantics of code. To evaluate the quality of embeddings, we use a programming competition problem identification technique. Our model demonstrates superior performance in comparison to bag-of-tokens embedding, indicating promising results for enhancing code embeddings in future software engineering tasks.",1
"Time Series Classification (TSC) problems are encountered in many real life data mining tasks ranging from medicine and security to human activity recognition and food safety. With the recent success of deep neural networks in various domains such as computer vision and natural language processing, researchers started adopting these techniques for solving time series data mining problems. However, to the best of our knowledge, no previous work has considered the vulnerability of deep learning models to adversarial time series examples, which could potentially make them unreliable in situations where the decision taken by the classifier is crucial such as in medicine and security. For computer vision problems, such attacks have been shown to be very easy to perform by altering the image and adding an imperceptible amount of noise to trick the network into wrongly classifying the input image. Following this line of work, we propose to leverage existing adversarial attack mechanisms to add a special noise to the input time series in order to decrease the network's confidence when classifying instances at test time. Our results reveal that current state-of-the-art deep learning time series classifiers are vulnerable to adversarial attacks which can have major consequences in multiple domains such as food safety and quality assurance.",0
"TSC problems arise in various real-life data mining tasks, including medicine, security, human activity recognition, and food safety. The success of deep neural networks in computer vision and natural language processing has led researchers to adopt them for solving time series data mining problems. However, no previous work has explored the vulnerability of deep learning models to adversarial time series examples. These attacks can make them unreliable in critical situations, such as in medicine and security. Adversarial attacks have been shown to be easy to perform in computer vision by altering images and adding imperceptible noise to misclassify input images. Based on this, we propose introducing a special noise to the input time series to decrease the network's confidence during classification at test time. Our findings demonstrate that current state-of-the-art deep learning time series classifiers are susceptible to adversarial attacks, which can have significant consequences in domains such as food safety and quality assurance.",1
"Deep neural networks have revolutionized many fields such as computer vision and natural language processing. Inspired by this recent success, deep learning started to show promising results for Time Series Classification (TSC). However, neural networks are still behind the state-of-the-art TSC algorithms, that are currently composed of ensembles of 37 non deep learning based classifiers. We attribute this gap in performance due to the lack of neural network ensembles for TSC. Therefore in this paper, we show how an ensemble of 60 deep learning models can significantly improve upon the current state-of-the-art performance of neural networks for TSC, when evaluated over the UCR/UEA archive: the largest publicly available benchmark for time series analysis. Finally, we show how our proposed Neural Network Ensemble (NNE) is the first time series classifier to outperform COTE while reaching similar performance to the current state-of-the-art ensemble HIVE-COTE.",0
"The emergence of deep neural networks has brought about a drastic change in various fields, such as computer vision and natural language processing. This success has led to the exploration of deep learning's potential in Time Series Classification (TSC), which has shown promising results. However, current TSC algorithms that consist of ensembles of 37 non-deep learning-based classifiers still outperform neural networks. We believe that the lack of neural network ensembles for TSC is the reason for this performance gap. Thus, in this paper, we present how an ensemble of 60 deep learning models can significantly enhance the state-of-the-art performance of neural networks for TSC. Our proposed Neural Network Ensemble (NNE) outperforms COTE and performs similarly to the current state-of-the-art ensemble HIVE-COTE, making it the first time series classifier to achieve this feat. Our findings are based on the UCR/UEA archive, which is the most extensive publicly available benchmark for time series analysis.",1
"Machine learning has been widely applied to various applications, some of which involve training with privacy-sensitive data. A modest number of data breaches have been studied, including credit card information in natural language data and identities from face dataset. However, most of these studies focus on supervised learning models. As deep reinforcement learning (DRL) has been deployed in a number of real-world systems, such as indoor robot navigation, whether trained DRL policies can leak private information requires in-depth study. To explore such privacy breaches in general, we mainly propose two methods: environment dynamics search via genetic algorithm and candidate inference based on shadow policies. We conduct extensive experiments to demonstrate such privacy vulnerabilities in DRL under various settings. We leverage the proposed algorithms to infer floor plans from some trained Grid World navigation DRL agents with LiDAR perception. The proposed algorithm can correctly infer most of the floor plans and reaches an average recovery rate of 95.83% using policy gradient trained agents. In addition, we are able to recover the robot configuration in continuous control environments and an autonomous driving simulator with high accuracy. To the best of our knowledge, this is the first work to investigate privacy leakage in DRL settings and we show that DRL-based agents do potentially leak privacy-sensitive information from the trained policies.",0
"Various applications have utilized machine learning, some of which involve training with data that requires privacy protection. While a few data breaches have been analyzed, mainly involving natural language data with credit card information and face datasets with identities, most studies have concentrated on supervised learning models. As deep reinforcement learning (DRL) has been implemented in several real-world systems, like indoor robot navigation, it is necessary to investigate whether trained DRL policies can divulge private data. To examine such privacy breaches more broadly, we suggest two methods: environment dynamics search via genetic algorithm and candidate inference based on shadow policies. We perform comprehensive experiments demonstrating that DRL is susceptible to privacy vulnerabilities under multiple settings. We utilize the proposed algorithms to infer floor plans from Grid World navigation DRL agents with LiDAR perception. Using policy gradient trained agents, the proposed algorithm recovers most of the floor plans and achieves an average recovery rate of 95.83%. We also obtain high accuracy in recovering the robot configuration in continuous control environments and an autonomous driving simulator. This is the first study to investigate privacy breaches in DRL settings, revealing that DRL-based agents have the potential to disclose private information from the trained policies.",1
"The task of Natural Language Inference (NLI) is widely modeled as supervised sentence pair classification. While there has been a lot of work recently on generating explanations of the predictions of classifiers on a single piece of text, there have been no attempts to generate explanations of classifiers operating on pairs of sentences. In this paper, we show that it is possible to generate token-level explanations for NLI without the need for training data explicitly annotated for this purpose. We use a simple LSTM architecture and evaluate both LIME and Anchor explanations for this task. We compare these to a Multiple Instance Learning (MIL) method that uses thresholded attention make token-level predictions. The approach we present in this paper is a novel extension of zero-shot single-sentence tagging to sentence pairs for NLI. We conduct our experiments on the well-studied SNLI dataset that was recently augmented with manually annotation of the tokens that explain the entailment relation. We find that our white-box MIL-based method, while orders of magnitude faster, does not reach the same accuracy as the black-box methods.",0
"The prevalent method for Natural Language Inference (NLI) involves supervised sentence pair classification. Although recent studies have focused on generating explanations of classifiers for individual text pieces, none have attempted to do so for sentence pairs. This study presents a novel approach for generating token-level explanations for NLI without requiring specific training data for this purpose. The study uses a simple LSTM architecture and compares LIME and Anchor explanations to a Multiple Instance Learning (MIL) method that employs thresholded attention for token-level predictions. The approach extends zero-shot single-sentence tagging to sentence pairs for NLI. The study evaluates its approach on the SNLI dataset, which was recently augmented with manually annotated tokens that explain the entailment relation. The authors conclude that their white-box MIL-based method is much faster but less accurate than the black-box methods.",1
"It is a big challenge of computer vision to make machine automatically describe the content of an image with a natural language sentence. Previous works have made great progress on this task, but they only use the global or local image feature, which may lose some important subtle or global information of an image. In this paper, we propose a model with 3-gated model which fuses the global and local image features together for the task of image caption generation. The model mainly has three gated structures. 1) Gate for the global image feature, which can adaptively decide when and how much the global image feature should be imported into the sentence generator. 2) The gated recurrent neural network (RNN) is used as the sentence generator. 3) The gated feedback method for stacking RNN is employed to increase the capability of nonlinearity fitting. More specially, the global and local image features are combined together in this paper, which makes full use of the image information. The global image feature is controlled by the first gate and the local image feature is selected by the attention mechanism. With the latter two gates, the relationship between image and text can be well explored, which improves the performance of the language part as well as the multi-modal embedding part. Experimental results show that our proposed method outperforms the state-of-the-art for image caption generation.",0
"The automatic description of image content through natural language sentences is a difficult task for computer vision. While previous attempts have made progress, they have only utilized either global or local image features, causing the loss of crucial information. This paper introduces a 3-gated model that combines both global and local image features for image caption generation. The model includes three gated structures: one to control the global image feature, one for the gated recurrent neural network (RNN) sentence generator, and one for the gated feedback method to enhance nonlinearity fitting. The combination of global and local image features is achieved through the first gate and an attention mechanism selects the local image feature. With the latter two gates, the relationship between image and text is effectively explored, resulting in improved performance. Experimental results show that our proposed method outperforms the current state-of-the-art for image caption generation.",1
"Using a natural language sentence to describe the content of an image is a challenging but very important task. It is challenging because a description must not only capture objects contained in the image and the relationships among them, but also be relevant and grammatically correct. In this paper a multi-modal embedding model based on gated recurrent units (GRU) which can generate variable-length description for a given image. In the training step, we apply the convolutional neural network (CNN) to extract the image feature. Then the feature is imported into the multi-modal GRU as well as the corresponding sentence representations. The multi-modal GRU learns the inter-modal relations between image and sentence. And in the testing step, when an image is imported to our multi-modal GRU model, a sentence which describes the image content is generated. The experimental results demonstrate that our multi-modal GRU model obtains the state-of-the-art performance on Flickr8K, Flickr30K and MS COCO datasets.",0
"Generating a natural language description for an image is a challenging yet crucial task as it requires capturing the objects and relationships within the image while maintaining relevance and grammatical correctness. This paper presents a multi-modal embedding model that utilizes gated recurrent units (GRU) to produce variable-length descriptions for an image. During training, a convolutional neural network (CNN) is utilized to extract image features which are then fed into the multi-modal GRU along with corresponding sentence representations. The multi-modal GRU learns the inter-modal relations between the image and sentence. During testing, the multi-modal GRU generates a sentence that describes the image content. The experimental results show that the multi-modal GRU model achieves state-of-the-art performance on Flickr8K, Flickr30K, and MS COCO datasets.",1
"Distributed representations of sentences have become ubiquitous in natural language processing tasks. In this paper, we consider a continual learning scenario for sentence representations: Given a sequence of corpora, we aim to optimize the sentence encoder with respect to the new corpus while maintaining its accuracy on the old corpora. To address this problem, we propose to initialize sentence encoders with the help of corpus-independent features, and then sequentially update sentence encoders using Boolean operations of conceptor matrices to learn corpus-dependent features. We evaluate our approach on semantic textual similarity tasks and show that our proposed sentence encoder can continually learn features from new corpora while retaining its competence on previously encountered corpora.",0
"The use of distributed representations for sentences is widespread in natural language processing. This study examines the scenario of continual learning for sentence representations, where the goal is to optimize the sentence encoder for a new corpus while retaining accuracy for previous corpora. To tackle this challenge, we suggest initializing sentence encoders with corpus-independent features, followed by updating them using Boolean operations of conceptor matrices to learn corpus-dependent features. Our method is evaluated on semantic textual similarity tasks, demonstrating the ability of our sentence encoder to continually learn new features from recent corpora while maintaining proficiency for previous ones.",1
"Literary critics often attempt to uncover meaning in a single work of literature through careful reading and analysis. Applying natural language processing methods to aid in such literary analyses remains a challenge in digital humanities. While most previous work focuses on ""distant reading"" by algorithmically discovering high-level patterns from large collections of literary works, here we sharpen the focus of our methods to a single literary theory about Italo Calvino's postmodern novel Invisible Cities, which consists of 55 short descriptions of imaginary cities. Calvino has provided a classification of these cities into eleven thematic groups, but literary scholars disagree as to how trustworthy his categorization is. Due to the unique structure of this novel, we can computationally weigh in on this debate: we leverage pretrained contextualized representations to embed each city's description and use unsupervised methods to cluster these embeddings. Additionally, we compare results of our computational approach to similarity judgments generated by human readers. Our work is a first step towards incorporating natural language processing into literary criticism.",0
"The task of discovering meaning in a single piece of literature through close examination and analysis is a common endeavor undertaken by literary critics. However, implementing natural language processing techniques to assist with such analyses remains a challenge in the realm of digital humanities. While previous efforts have primarily focused on ""distant reading"" techniques that involve algorithmic identification of high-level patterns across large collections of literary works, our approach is specifically tailored to a single literary theory pertaining to Italo Calvino's postmodern novel Invisible Cities, which is comprised of 55 brief descriptions of fictional cities. Calvino himself has categorized these cities into eleven thematic groups, but there is disagreement among literary scholars regarding the reliability of his classification. Our computational approach allows us to contribute to this debate by utilizing pretrained contextualized representations to embed each city's description and applying unsupervised clustering methods to these embeddings. Furthermore, we compare the results of our approach with similarity judgments made by human readers. This marks a preliminary step towards merging natural language processing with literary criticism.",1
"Describing what has changed in a scene can be useful to a user, but only if generated text focuses on what is semantically relevant. It is thus important to distinguish distractors (e.g. a viewpoint change) from relevant changes (e.g. an object has moved). We present a novel Dual Dynamic Attention Model (DUDA) to perform robust Change Captioning. Our model learns to distinguish distractors from semantic changes, localize the changes via Dual Attention over ""before"" and ""after"" images, and accurately describe them in natural language via Dynamic Speaker, by adaptively focusing on the necessary visual inputs (e.g. ""before"" or ""after"" image). To study the problem in depth, we collect a CLEVR-Change dataset, built off the CLEVR engine, with 5 types of scene changes. We benchmark a number of baselines on our dataset, and systematically study different change types and robustness to distractors. We show the superiority of our DUDA model in terms of both change captioning and localization. We also show that our approach is general, obtaining state-of-the-art results on the recent realistic Spot-the-Diff dataset which has no distractors.",0
"If text is to be useful to a user in describing changes in a scene, it must focus on semantically relevant information. To achieve this, it is crucial to differentiate between distractors and relevant changes. For example, a change in viewpoint is a distractor, while a change in the position of an object is relevant. We have developed a Dual Dynamic Attention Model (DUDA) to perform Change Captioning with accuracy and robustness. Our model employs Dual Attention to localize changes in ""before"" and ""after"" images, and Dynamic Speaker to adaptively focus on necessary visual inputs to describe the changes naturally. To fully explore this problem, we created the CLEVR-Change dataset, which encompasses 5 types of scene changes. We benchmarked various baselines on our dataset and analyzed their robustness to distractors. Our DUDA model outperformed these baselines in both change captioning and localization. Furthermore, our approach is general and achieved state-of-the-art results on the Spot-the-Diff dataset, which has no distractors.",1
"To solve the problem that convolutional neural networks (CNNs) are difficult to process non-grid type relational data like graphs, Kipf et al. proposed a graph convolutional neural network (GCN). The core idea of the GCN is to perform two-fold informational fusion for each node in a given graph during each iteration: the fusion of graph structure information and the fusion of node feature dimensions. Because of the characteristic of the combinatorial generalizations, GCN has been widely used in the fields of scene semantic relationship analysis, natural language processing and few-shot learning etc. However, due to its two-fold informational fusion involves mathematical irreversible calculations, it is hard to explain the decision reason for the prediction of the each node classification. Unfortunately, most of the existing attribution analysis methods concentrate on the models like CNNs, which are utilized to process grid-like data. It is difficult to apply those analysis methods to the GCN directly. It is because compared with the independence among CNNs input data, there is correlation between the GCN input data. This resulting in the existing attribution analysis methods can only obtain the partial model contribution from the central node features to the final decision of the GCN, but ignores the other model contribution from central node features and its neighbor nodes features to that decision. To this end, we propose a gradient attribution analysis method for the GCN called Node Attribution Method (NAM), which can get the model contribution from not only the central node but also its neighbor nodes to the GCN output. We also propose the Node Importance Visualization (NIV) method to visualize the central node and its neighbor nodes based on the value of the contribution...",0
"Kipf et al. introduced a graph convolutional neural network (GCN) to address the issue of convolutional neural networks (CNNs) struggling with non-grid relational data, such as graphs. GCN performs two-fold informational fusion during each iteration for every node in a given graph, combining graph structure information and node feature dimensions. Due to its combinatorial generalizations, GCN is widely used in various fields, but its two-fold informational fusion involves mathematical irreversible calculations, making it difficult to explain each node classification's decision reason. Existing attribution analysis methods are inadequate because GCN's input data have correlations, unlike CNNs. Consequently, we propose the Node Attribution Method (NAM), a gradient attribution analysis approach for GCN that considers not only the central node but also its neighbor nodes' model contribution to the GCN output. Additionally, the Node Importance Visualization (NIV) method visualizes the central node and its neighbor nodes based on the value of the contribution.",1
"Deep learning has arguably achieved tremendous success in recent years. In simple words, deep learning uses the composition of many nonlinear functions to model the complex dependency between input features and labels. While neural networks have a long history, recent advances have greatly improved their performance in computer vision, natural language processing, etc. From the statistical and scientific perspective, it is natural to ask: What is deep learning? What are the new characteristics of deep learning, compared with classical methods? What are the theoretical foundations of deep learning? To answer these questions, we introduce common neural network models (e.g., convolutional neural nets, recurrent neural nets, generative adversarial nets) and training techniques (e.g., stochastic gradient descent, dropout, batch normalization) from a statistical point of view. Along the way, we highlight new characteristics of deep learning (including depth and over-parametrization) and explain their practical and theoretical benefits. We also sample recent results on theories of deep learning, many of which are only suggestive. While a complete understanding of deep learning remains elusive, we hope that our perspectives and discussions serve as a stimulus for new statistical research.",0
"In recent years, deep learning has achieved significant success using complex compositions of nonlinear functions to model the relationship between input features and labels. Although neural networks have been around for a long time, recent advances have improved their performance in areas such as computer vision and natural language processing. From a statistical and scientific perspective, it is natural to ask what deep learning is, how it differs from classical methods, and what its theoretical foundations are. To address these questions, this article provides a statistical overview of common neural network models and training techniques, highlighting the new characteristics of deep learning, such as depth and over-parametrization. Additionally, recent theoretical results are discussed, although a complete understanding of deep learning remains elusive. The aim is to stimulate new statistical research in this field.",1
"Humor is a unique and creative communicative behavior displayed during social interactions. It is produced in a multimodal manner, through the usage of words (text), gestures (vision) and prosodic cues (acoustic). Understanding humor from these three modalities falls within boundaries of multimodal language; a recent research trend in natural language processing that models natural language as it happens in face-to-face communication. Although humor detection is an established research area in NLP, in a multimodal context it is an understudied area. This paper presents a diverse multimodal dataset, called UR-FUNNY, to open the door to understanding multimodal language used in expressing humor. The dataset and accompanying studies, present a framework in multimodal humor detection for the natural language processing community. UR-FUNNY is publicly available for research.",0
"During social interactions, humor is a distinct and imaginative way of communicating. It is created through a combination of words (text), gestures (vision), and prosodic cues (acoustic). The study of humor in these three modalities is part of multimodal language, a recent area of research in natural language processing that models communication in face-to-face situations. While humor detection is an established field in NLP, its application in a multimodal context has been largely unexplored. This paper introduces UR-FUNNY, a diverse multimodal dataset designed to facilitate the understanding of humor expressed through multiple modalities. The dataset and accompanying studies provide a framework for the natural language processing community to explore multimodal humor detection. UR-FUNNY is publicly available for research purposes.",1
"In this paper, we propose a novel multi-task learning method based on the deep convolutional network. The proposed deep network has four convolutional layers, three max-pooling layers, and two parallel fully connected layers. To adjust the deep network to multi-task learning problem, we propose to learn a low-rank deep network so that the relation among different tasks can be explored. We proposed to minimize the number of independent parameter rows of one fully connected layer to explore the relations among different tasks, which is measured by the nuclear norm of the parameter of one fully connected layer, and seek a low-rank parameter matrix. Meanwhile, we also propose to regularize another fully connected layer by sparsity penalty, so that the useful features learned by the lower layers can be selected. The learning problem is solved by an iterative algorithm based on gradient descent and back-propagation algorithms. The proposed algorithm is evaluated over benchmark data sets of multiple face attribute prediction, multi-task natural language processing, and joint economics index predictions. The evaluation results show the advantage of the low-rank deep CNN model over multi-task problems.",0
"This paper introduces a fresh approach to multi-task learning using a deep convolutional network. The deep network comprises four convolutional layers, three max-pooling layers, and two parallel fully connected layers. To adapt the deep network to multi-task learning, a low-rank deep network is learned to explore the connection between different tasks. The number of independent parameter rows of one fully connected layer is minimized to examine the relationship among different tasks, utilizing the nuclear norm of the parameter of one fully connected layer and seeking a low-rank parameter matrix. Additionally, another fully connected layer is regularized by sparsity penalty to select useful features learned by the lower layers. The learning problem is solved by an iterative algorithm based on gradient descent and back-propagation algorithms. The proposed algorithm is evaluated using benchmark data sets of multiple face attribute prediction, multi-task natural language processing, and joint economics index predictions, and the results demonstrate the advantage of the low-rank deep CNN model over multi-task problems.",1
"We propose Visual Query Detection (VQD), a new visual grounding task. In VQD, a system is guided by natural language to localize a variable number of objects in an image. VQD is related to visual referring expression recognition, where the task is to localize only one object. We describe the first dataset for VQD and we propose baseline algorithms that demonstrate the difficulty of the task compared to referring expression recognition.",0
"Our proposal is the introduction of Visual Query Detection (VQD), a novel visual grounding task that involves using natural language to locate a varying number of objects within an image. While VQD shares similarities with visual referring expression recognition, which focuses on finding only one object, it presents unique challenges. We also present the inaugural dataset for VQD and offer baseline algorithms that highlight the task's complexity when compared to referring expression recognition.",1
"This paper presents a novel method to manipulate the visual appearance (pose and attribute) of a person image according to natural language descriptions. Our method can be boiled down to two stages: 1) text guided pose generation and 2) visual appearance transferred image synthesis. In the first stage, our method infers a reasonable target human pose based on the text. In the second stage, our method synthesizes a realistic and appearance transferred person image according to the text in conjunction with the target pose. Our method extracts sufficient information from the text and establishes a mapping between the image space and the language space, making generating and editing images corresponding to the description possible. We conduct extensive experiments to reveal the effectiveness of our method, as well as using the VQA Perceptual Score as a metric for evaluating the method. It shows for the first time that we can automatically edit the person image from the natural language descriptions.",0
"The paper introduces a new technique for modifying a person's visual appearance, including their pose and attributes, based on natural language descriptions. The method consists of two stages: text-guided pose creation and image synthesis with transferred appearance. In the first stage, the approach predicts a feasible human target pose from the text. In the second stage, it generates a realistic and visually transformed person image by combining the text with the target pose. By mapping the image space and language space, the approach extracts sufficient information from the text, enabling the creation and alteration of images based on descriptions. The study includes extensive experiments to demonstrate the effectiveness of the method and uses the VQA Perceptual Score as a metric to evaluate its performance. This study showcases the ability to automatically edit a person's image based on natural language descriptions for the first time.",1
"We consider the problem of referring image segmentation. Given an input image and a natural language expression, the goal is to segment the object referred by the language expression in the image. Existing works in this area treat the language expression and the input image separately in their representations. They do not sufficiently capture long-range correlations between these two modalities. In this paper, we propose a cross-modal self-attention (CMSA) module that effectively captures the long-range dependencies between linguistic and visual features. Our model can adaptively focus on informative words in the referring expression and important regions in the input image. In addition, we propose a gated multi-level fusion module to selectively integrate self-attentive cross-modal features corresponding to different levels in the image. This module controls the information flow of features at different levels. We validate the proposed approach on four evaluation datasets. Our proposed approach consistently outperforms existing state-of-the-art methods.",0
"The focus of our study is image segmentation through language referral. Our aim is to segment an object in an image that is referred to by a natural language expression. Existing research in this field has not been successful in capturing long-range correlations between the language expression and the input image in their representations. We propose a new method using a cross-modal self-attention (CMSA) module that effectively captures these long-range dependencies. Our model can selectively focus on significant words in the language expression and important regions in the image for optimal results. Additionally, we introduce a gated multi-level fusion module that can control the flow of information between the different levels of features. We evaluate our approach on four datasets and consistently outperform the current state-of-the-art methods.",1
"Over the past few years machine learning has seen a renewed explosion of interest, following a number of studies showing the effectiveness of neural networks in a range of tasks which had previously been considered incredibly hard. Neural networks' effectiveness in the fields of image recognition and natural language processing stems primarily from the vast amounts of data available to companies and researchers, coupled with the huge amounts of compute power available in modern accelerators such as GPUs, FPGAs and ASICs. There are a number of approaches available to developers for utilizing GPGPU technologies such as SYCL, OpenCL and CUDA, however many applications require the same low level mathematical routines. Libraries dedicated to accelerating these common routines allow developers to easily make full use of the available hardware without requiring low level knowledge of the hardware themselves, however such libraries are often provided by hardware manufacturers for specific hardware such as cuDNN for Nvidia hardware or MIOpen for AMD hardware.   SYCL-DNN is a new open-source library dedicated to providing accelerated routines for neural network operations which are hardware and vendor agnostic. Built on top of the SYCL open standard and written entirely in standard C++, SYCL-DNN allows a user to easily accelerate neural network code for a wide range of hardware using a modern C++ interface. The library is tested on AMD's OpenCL for GPU, Intel's OpenCL for CPU and GPU, ARM's OpenCL for Mali GPUs as well as ComputeAorta's OpenCL for R-Car CV engine and host CPU. In this talk we will present performance figures for SYCL-DNN on this range of hardware, and discuss how high performance was achieved on such a varied set of accelerators with such different hardware features.",0
"In recent years, machine learning has experienced a resurgence of interest due to studies showcasing the efficacy of neural networks in previously challenging tasks like image recognition and natural language processing. This success can be attributed to the large amount of available data and compute power from modern accelerators like GPUs, FPGAs, and ASICs. Developers have various options for utilizing GPGPU technologies, such as SYCL, OpenCL, and CUDA, but many applications require the same mathematical routines. Hardware manufacturers often provide libraries dedicated to accelerating these common routines, but they are specific to their hardware. However, SYCL-DNN is a new open-source library that accelerates neural network operations and is hardware and vendor agnostic. Written in standard C++ and built on top of the SYCL open standard, SYCL-DNN provides a modern C++ interface that allows users to easily accelerate neural network code for a wide range of hardware. The library is tested on various accelerators, including AMD's OpenCL for GPU, Intel's OpenCL for CPU and GPU, ARM's OpenCL for Mali GPUs, and ComputeAorta's OpenCL for R-Car CV engine and host CPU. This talk will discuss SYCL-DNN's performance figures on this diverse range of hardware and how it achieved high performance despite the different hardware features.",1
"Referring object detection and referring image segmentation are important tasks that require joint understanding of visual information and natural language. Yet there has been evidence that current benchmark datasets suffer from bias, and current state-of-the-art models cannot be easily evaluated on their intermediate reasoning process. To address these issues and complement similar efforts in visual question answering, we build CLEVR-Ref+, a synthetic diagnostic dataset for referring expression comprehension. The precise locations and attributes of the objects are readily available, and the referring expressions are automatically associated with functional programs. The synthetic nature allows control over dataset bias (through sampling strategy), and the modular programs enable intermediate reasoning ground truth without human annotators.   In addition to evaluating several state-of-the-art models on CLEVR-Ref+, we also propose IEP-Ref, a module network approach that significantly outperforms other models on our dataset. In particular, we present two interesting and important findings using IEP-Ref: (1) the module trained to transform feature maps into segmentation masks can be attached to any intermediate module to reveal the entire reasoning process step-by-step; (2) even if all training data has at least one object referred, IEP-Ref can correctly predict no-foreground when presented with false-premise referring expressions. To the best of our knowledge, this is the first direct and quantitative proof that neural modules behave in the way they are intended.",0
"The tasks of object detection and image segmentation rely on a joint understanding of visual information and natural language, making them crucial. However, current benchmark datasets display bias and evaluating state-of-the-art models on their intermediate reasoning process is difficult. To address these issues, we created CLEVR-Ref+, a synthetic diagnostic dataset that includes precise object locations and attributes and automatically associated functional programs with referring expressions. The synthetic nature of the dataset allows control over bias and the modular programs enable intermediate reasoning without human annotators. We evaluated several models on the dataset, including our proposed IEP-Ref module network approach, which outperformed other models. Our findings demonstrated that the module trained to transform feature maps into segmentation masks can be used to reveal the entire reasoning process step-by-step, and that IEP-Ref correctly predicted no-foreground for false-premise referring expressions. This is the first quantitative proof of neural modules behaving as intended.",1
"The techniques of deep learning have become the state of the art methodology for executing complicated tasks from various domains of computer vision, natural language processing, and several other areas. Due to its rapid development and promising benchmarks in those fields, researchers started experimenting with this technique to perform in the area of, especially in intrusion detection related tasks. Deep learning is a subset and a natural extension of classical Machine learning and an evolved model of neural networks. This paper contemplates and discusses all the methodologies related to the leading edge Deep learning and Neural network models purposing to the arena of Intrusion Detection Systems.",0
"Deep learning techniques have emerged as the most advanced approach for accomplishing complex tasks in computer vision, natural language processing, and other domains. The remarkable progress and impressive results achieved in these areas have led researchers to explore the use of deep learning for intrusion detection tasks. Deep learning is a subset and an evolved form of neural networks that extends classical machine learning. This paper evaluates and analyzes the various methodologies related to cutting-edge deep learning and neural network models in the context of intrusion detection systems.",1
"Deep learning models have lately shown great performance in various fields such as computer vision, speech recognition, speech translation, and natural language processing. However, alongside their state-of-the-art performance, it is still generally unclear what is the source of their generalization ability. Thus, an important question is what makes deep neural networks able to generalize well from the training set to new data. In this article, we provide an overview of the existing theory and bounds for the characterization of the generalization error of deep neural networks, combining both classical and more recent theoretical and empirical results.",0
"Recently, deep learning models have demonstrated impressive performance in different domains such as computer vision, speech recognition, speech translation, and natural language processing. Despite their exceptional performance, it remains unclear what enables them to generalize effectively from the training set to new data. Therefore, a critical question is what accounts for the generalization ability of deep neural networks. This article presents a summary of current theories and limits for the description of the generalization error of deep neural networks. It combines classical and contemporary theoretical and empirical findings.",1
"Vision-language navigation (VLN) is the task of navigating an embodied agent to carry out natural language instructions inside real 3D environments. In this paper, we study how to address three critical challenges for this task: the cross-modal grounding, the ill-posed feedback, and the generalization problems. First, we propose a novel Reinforced Cross-Modal Matching (RCM) approach that enforces cross-modal grounding both locally and globally via reinforcement learning (RL). Particularly, a matching critic is used to provide an intrinsic reward to encourage global matching between instructions and trajectories, and a reasoning navigator is employed to perform cross-modal grounding in the local visual scene. Evaluation on a VLN benchmark dataset shows that our RCM model significantly outperforms previous methods by 10% on SPL and achieves the new state-of-the-art performance. To improve the generalizability of the learned policy, we further introduce a Self-Supervised Imitation Learning (SIL) method to explore unseen environments by imitating its own past, good decisions. We demonstrate that SIL can approximate a better and more efficient policy, which tremendously minimizes the success rate performance gap between seen and unseen environments (from 30.7% to 11.7%).",0
"The objective of Vision-language navigation (VLN) is to guide a physical agent through real 3D environments based on natural language instructions. This study focuses on addressing three key challenges that arise in this task: cross-modal grounding, ill-posed feedback, and generalization difficulties. To tackle these issues, the authors propose a Reinforced Cross-Modal Matching (RCM) approach that enforces cross-modal grounding locally and globally through reinforcement learning. The RCM model is evaluated on a VLN benchmark dataset and demonstrates a 10% improvement in SPL over previous methods, achieving a new state-of-the-art performance. To enhance generalizability, a Self-Supervised Imitation Learning (SIL) method is introduced to enable the agent to imitate its own past successful decisions in unseen environments. This significantly reduces the success rate performance gap between seen and unseen environments from 30.7% to 11.7%.",1
"Natural language processing has improved tremendously after the success of word embedding techniques such as word2vec. Recently, the same idea has been applied on source code with encouraging results. In this survey, we aim to collect and discuss the usage of word embedding techniques on programs and source code. The articles in this survey have been collected by asking authors of related work and with an extensive search on Google Scholar. Each article is categorized into five categories: 1. embedding of tokens 2. embedding of functions or methods 3. embedding of sequences or sets of method calls 4. embedding of binary code 5. other embeddings. We also provide links to experimental data and show some remarkable visualization of code embeddings. In summary, word embedding has been successfully applied on different granularities of source code. With access to countless open-source repositories, we see a great potential of applying other data-driven natural language processing techniques on source code in the future.",0
"Following the success of word2vec, word embedding techniques have undergone significant advancements in natural language processing. Recently, these techniques have been extended to source code, yielding promising results. The objective of this survey is to gather and discuss the usage of word embedding techniques on source code and programs. To compile this report, we have collected articles from authors of related work and conducted extensive research on Google Scholar. The articles have been classified into five categories, including embedding of tokens, functions, methods, sequences or sets of method calls, binary code, and other embeddings. Additionally, we have provided links to experimental data and presented visualizations of code embeddings. In conclusion, word embedding techniques have been effectively applied to different levels of source code. With the vast number of open-source repositories available, we anticipate a bright future for applying other data-driven natural language processing techniques to source code.",1
"Adversarial examples are important for understanding the behavior of neural models, and can improve their robustness through adversarial training. Recent work in natural language processing generated adversarial examples by assuming white-box access to the attacked model, and optimizing the input directly against it (Ebrahimi et al., 2018). In this work, we show that the knowledge implicit in the optimization procedure can be distilled into another more efficient neural network. We train a model to emulate the behavior of a white-box attack and show that it generalizes well across examples. Moreover, it reduces adversarial example generation time by 19x-39x. We also show that our approach transfers to a black-box setting, by attacking The Google Perspective API and exposing its vulnerability. Our attack flips the API-predicted label in 42\% of the generated examples, while humans maintain high-accuracy in predicting the gold label.",0
"Understanding the behavior of neural models and enhancing their robustness through adversarial training can be achieved by studying adversarial examples. A recent study in natural language processing created such examples by accessing the attacked model as a white-box and optimizing the input against it. We present a method where the knowledge acquired during optimization can be distilled into a more efficient neural network, effectively emulating the behavior of a white-box attack. Our model demonstrates exceptional generalization ability across examples and reduces the time taken for adversarial example generation by 19x-39x. We tested our approach on The Google Perspective API in a black-box setting, exposing its vulnerability by flipping the API-predicted label in 42% of the generated examples while maintaining high accuracy in predicting the gold label.",1
"Graph convolutional networks (GCNs) have recently become one of the most powerful tools for graph analytics tasks in numerous applications, ranging from social networks and natural language processing to bioinformatics and chemoinformatics, thanks to their ability to capture the complex relationships between concepts. At present, the vast majority of GCNs use a neighborhood aggregation framework to learn a continuous and compact vector, then performing a pooling operation to generalize graph embedding for the classification task. These approaches have two disadvantages in the graph classification task: (1)when only the largest sub-graph structure ($k$-hop neighbor) is used for neighborhood aggregation, a large amount of early-stage information is lost during the graph convolution step; (2) simple average/sum pooling or max pooling utilized, which loses the characteristics of each node and the topology between nodes. In this paper, we propose a novel framework called, dual attention graph convolutional networks (DAGCN) to address these problems. DAGCN automatically learns the importance of neighbors at different hops using a novel attention graph convolution layer, and then employs a second attention component, a self-attention pooling layer, to generalize the graph representation from the various aspects of a matrix graph embedding. The dual attention network is trained in an end-to-end manner for the graph classification task. We compare our model with state-of-the-art graph kernels and other deep learning methods. The experimental results show that our framework not only outperforms other baselines but also achieves a better rate of convergence.",0
"Recently, graph convolutional networks (GCNs) have gained significant popularity as a powerful tool for graph analytics tasks in various applications such as social networks, bioinformatics, and natural language processing. The reason for this is their ability to capture intricate relationships between concepts. However, the majority of GCNs use a neighborhood aggregation framework for learning a continuous and compact vector followed by a pooling operation that generalizes graph embedding for the classification task. Such approaches have two major drawbacks in the graph classification task. Firstly, when only the largest sub-graph structure is used for neighborhood aggregation, a substantial amount of early-stage data is lost during the graph convolution step. Secondly, the pooling operation used in these approaches average/sum or max pooling loses the characteristics of each node and the topology between nodes. In this paper, we propose a novel framework, called dual attention graph convolutional networks (DAGCN), to address these issues. Using a novel attention graph convolution layer, DAGCN learns the importance of neighbors at different hops and then employs a self-attention pooling layer to generalize the graph representation from various aspects of a matrix graph embedding. The dual attention network is trained end-to-end for the graph classification task and surpasses state-of-the-art graph kernels and other deep learning methods in terms of performance and convergence rate.",1
"Recently, deep learning based natural language processing techniques are being extensively used to deal with spam mail, censorship evaluation in social networks, among others. However, there is only a couple of works evaluating the vulnerabilities of such deep neural networks. Here, we go beyond attacks to investigate, for the first time, universal rules, i.e., rules that are sample agnostic and therefore could turn any text sample in an adversarial one. In fact, the universal rules do not use any information from the method itself (no information from the method, gradient information or training dataset information is used), making them black-box universal attacks. In other words, the universal rules are sample and method agnostic. By proposing a coevolutionary optimization algorithm we show that it is possible to create universal rules that can automatically craft imperceptible adversarial samples (only less than five perturbations which are close to misspelling are inserted in the text sample). A comparison with a random search algorithm further justifies the strength of the method. Thus, universal rules for fooling networks are here shown to exist. Hopefully, the results from this work will impact the development of yet more sample and model agnostic attacks as well as their defenses, culminating in perhaps a new age for artificial intelligence.",0
"Natural language processing techniques based on deep learning have become increasingly popular in dealing with issues such as spam mail and censorship evaluation on social networks. However, there is limited research on the vulnerabilities of these deep neural networks. Our study goes beyond traditional attacks by exploring universal rules that can turn any text sample into an adversarial one. These rules are sample agnostic and do not rely on any information from the method, gradient information, or training dataset, making them black-box universal attacks. Using a coevolutionary optimization algorithm, we demonstrate that it is possible to create imperceptible adversarial samples with only a few perturbations that are close to misspelling. A comparison with a random search algorithm highlights the effectiveness of the method. Our findings reveal the existence of universal rules for fooling networks and have the potential to contribute to the development of more model and sample agnostic attacks and defenses, leading to a new era in artificial intelligence.",1
"Dense captioning aims at simultaneously localizing semantic regions and describing these regions-of-interest (ROIs) with short phrases or sentences in natural language. Previous studies have shown remarkable progresses, but they are often vulnerable to the aperture problem that a caption generated by the features inside one ROI lacks contextual coherence with its surrounding context in the input image. In this work, we investigate contextual reasoning based on multi-scale message propagations from the neighboring contents to the target ROIs. To this end, we design a novel end-to-end context and attribute grounded dense captioning framework consisting of 1) a contextual visual mining module and 2) a multi-level attribute grounded description generation module. Knowing that captions often co-occur with the linguistic attributes (such as who, what and where), we also incorporate an auxiliary supervision from hierarchical linguistic attributes to augment the distinctiveness of the learned captions. Extensive experiments and ablation studies on Visual Genome dataset demonstrate the superiority of the proposed model in comparison to state-of-the-art methods.",0
"The goal of dense captioning is to identify and describe specific regions of interest in an image using short sentences or phrases in natural language. While previous research has made notable advancements in this area, a common issue is that captions generated based on features within a particular region may not be contextually coherent with the surrounding image. In this paper, we propose a new approach that leverages multi-scale message propagation to improve contextual reasoning. Our end-to-end framework includes a contextual visual mining module and a multi-level attribute grounded description generation module. Additionally, we incorporate hierarchical linguistic attributes to enhance the distinctiveness of generated captions. Through extensive experimentation and analysis on the Visual Genome dataset, we demonstrate the superior performance of our proposed model compared to existing state-of-the-art methods.",1
"In this paper, we present LaSOT, a high-quality benchmark for Large-scale Single Object Tracking. LaSOT consists of 1,400 sequences with more than 3.5M frames in total. Each frame in these sequences is carefully and manually annotated with a bounding box, making LaSOT the largest, to the best of our knowledge, densely annotated tracking benchmark. The average video length of LaSOT is more than 2,500 frames, and each sequence comprises various challenges deriving from the wild where target objects may disappear and re-appear again in the view. By releasing LaSOT, we expect to provide the community with a large-scale dedicated benchmark with high quality for both the training of deep trackers and the veritable evaluation of tracking algorithms. Moreover, considering the close connections of visual appearance and natural language, we enrich LaSOT by providing additional language specification, aiming at encouraging the exploration of natural linguistic feature for tracking. A thorough experimental evaluation of 35 tracking algorithms on LaSOT is presented with detailed analysis, and the results demonstrate that there is still a big room for improvements.",0
"This paper introduces LaSOT, a benchmark for Large-scale Single Object Tracking, which offers high-quality tracking challenges. LaSOT comprises 1,400 sequences with over 3.5M frames, and each frame is manually annotated with a bounding box, making it the largest densely annotated tracking benchmark. The sequences are over 2,500 frames long and present various challenges that arise from unpredictable movements of the object. LaSOT will provide a comprehensive benchmark for both training and evaluating deep trackers, and it also includes additional language specifications to encourage linguistic feature exploration for tracking. We present a thorough evaluation of 35 tracking algorithms on LaSOT, which shows that there is a lot of room for improvements.",1
"Though image-to-sequence generation models have become overwhelmingly popular in human-computer communications, they suffer from strongly favoring safe generic questions (""What is in this picture?""). Generating uninformative but relevant questions is not sufficient or useful. We argue that a good question is one that has a tightly focused purpose --- one that is aimed at expecting a specific type of response. We build a model that maximizes mutual information between the image, the expected answer and the generated question. To overcome the non-differentiability of discrete natural language tokens, we introduce a variational continuous latent space onto which the expected answers project. We regularize this latent space with a second latent space that ensures clustering of similar answers. Even when we don't know the expected answer, this second latent space can generate goal-driven questions specifically aimed at extracting objects (""what is the person throwing""), attributes, (""What kind of shirt is the person wearing?""), color (""what color is the frisbee?""), material (""What material is the frisbee?""), etc. We quantitatively show that our model is able to retain information about an expected answer category, resulting in more diverse, goal-driven questions. We launch our model on a set of real world images and extract previously unseen visual concepts.",0
"Image-to-sequence generation models have gained immense popularity in human-computer communication, but they have a propensity for generating generic and uninformative questions such as ""What is in this picture?"". We believe that a good question should have a specific purpose and elicit a particular type of response. To address this, we have developed a model that maximizes mutual information between the image, expected answer, and generated question. To overcome the non-differentiability of natural language tokens, we use a variational continuous latent space that is regulated by another latent space to cluster similar answers. Our model can generate goal-driven questions that extract objects, attributes, color, materials, etc., even when the expected answer is unknown. We demonstrate the effectiveness of our model by extracting new visual concepts from real-world images.",1
"Labeling training datasets has become a key barrier to building medical machine learning models. One strategy is to generate training labels programmatically, for example by applying natural language processing pipelines to text reports associated with imaging studies. We propose cross-modal data programming, which generalizes this intuitive strategy in a theoretically-grounded way that enables simpler, clinician-driven input, reduces required labeling time, and improves with additional unlabeled data. In this approach, clinicians generate training labels for models defined over a target modality (e.g. images or time series) by writing rules over an auxiliary modality (e.g. text reports). The resulting technical challenge consists of estimating the accuracies and correlations of these rules; we extend a recent unsupervised generative modeling technique to handle this cross-modal setting in a provably consistent way. Across four applications in radiography, computed tomography, and electroencephalography, and using only several hours of clinician time, our approach matches or exceeds the efficacy of physician-months of hand-labeling with statistical significance, demonstrating a fundamentally faster and more flexible way of building machine learning models in medicine.",0
"Building medical machine learning models faces a significant obstacle in labeling training datasets. One way to address this is to produce training labels programmatically by utilizing natural language processing pipelines on text reports that correspond with imaging studies. Our proposed solution is cross-modal data programming, which extends this approach in a theoretically-sound manner. It allows for simpler clinician input, reduces the time required for labeling, and improves with additional unlabeled data. Clinicians create training labels for models defined over a specific modality (e.g. images or time series) by constructing rules over an auxiliary modality (e.g. text reports). Estimating the accuracies and correlations of these rules constitutes a technical challenge, which we address by expanding an unsupervised generative modeling technique to handle this cross-modal setting consistently. Through our approach, which we applied to four radiography, computed tomography, and electroencephalography applications, and with only a few hours of clinician time, we achieved efficacy that matches or surpasses that of physician-months of hand-labeling with statistical significance. This demonstrates a faster and more adaptable way to build machine learning models in medicine.",1
"Generating long and semantic-coherent reports to describe medical images poses great challenges towards bridging visual and linguistic modalities, incorporating medical domain knowledge, and generating realistic and accurate descriptions. We propose a novel Knowledge-driven Encode, Retrieve, Paraphrase (KERP) approach which reconciles traditional knowledge- and retrieval-based methods with modern learning-based methods for accurate and robust medical report generation. Specifically, KERP decomposes medical report generation into explicit medical abnormality graph learning and subsequent natural language modeling. KERP first employs an Encode module that transforms visual features into a structured abnormality graph by incorporating prior medical knowledge; then a Retrieve module that retrieves text templates based on the detected abnormalities; and lastly, a Paraphrase module that rewrites the templates according to specific cases. The core of KERP is a proposed generic implementation unit---Graph Transformer (GTR) that dynamically transforms high-level semantics between graph-structured data of multiple domains such as knowledge graphs, images and sequences. Experiments show that the proposed approach generates structured and robust reports supported with accurate abnormality description and explainable attentive regions, achieving the state-of-the-art results on two medical report benchmarks, with the best medical abnormality and disease classification accuracy and improved human evaluation performance.",0
"Generating coherent and lengthy reports to describe medical images is a challenging task that requires bridging visual and linguistic modalities, incorporating medical domain knowledge, and producing realistic and accurate descriptions. To address this, we propose the Knowledge-driven Encode, Retrieve, Paraphrase (KERP) approach, which combines traditional knowledge- and retrieval-based methods with modern learning-based methods to generate accurate and robust medical reports. Specifically, KERP breaks down medical report generation into explicit medical abnormality graph learning and subsequent natural language modeling. This is achieved through an Encode module that transforms visual features into a structured abnormality graph with prior medical knowledge, a Retrieve module that retrieves text templates based on the detected abnormalities, and a Paraphrase module that rewrites the templates according to specific cases. At the core of KERP is a Graph Transformer (GTR) unit that dynamically transforms high-level semantics between graph-structured data of multiple domains such as knowledge graphs, images, and sequences. Our experiments show that KERP generates structured and robust reports with accurate abnormality descriptions and explainable attentive regions, achieving state-of-the-art results on two medical report benchmarks, with improved human evaluation performance and the best medical abnormality and disease classification accuracy.",1
"Over the past few years, deep learning techniques have achieved tremendous success in many visual understanding tasks such as object detection, image segmentation, and caption generation. Despite this thriving in computer vision and natural language processing, deep learning has not yet shown significant impact in robotics. Due to the gap between theory and application, there are many challenges when applying the results of deep learning to the real robotic systems. In this study, our long-term goal is to bridge the gap between computer vision and robotics by developing visual methods that can be used in real robots. In particular, this work tackles two fundamental visual problems for autonomous robotic manipulation: affordance detection and fine-grained action understanding. Theoretically, we propose different deep architectures to further improves the state of the art in each problem. Empirically, we show that the outcomes of our proposed methods can be applied in real robots and allow them to perform useful manipulation tasks.",0
"Deep learning techniques have been highly successful in various visual understanding tasks, such as image segmentation, object detection, and caption generation, in recent years. However, despite its prospering use in computer vision and natural language processing, deep learning has not made a significant impact on robotics. Bridging the gap between theory and application is challenging, and applying deep learning results to real robotic systems poses many challenges. The study aims to develop visual methods to be used in real robots to bridge the gap between computer vision and robotics. Two fundamental visual problems for autonomous robotic manipulation, affordance detection and fine-grained action understanding, are addressed in this work. We propose various deep architectures theoretically to improve the state of the art in each problem. Empirically, our proposed methods' outcomes demonstrate their applicability in real robots, enabling them to perform useful manipulation tasks.",1
"Learning with a primary objective, such as softmax cross entropy for classification and sequence generation, has been the norm for training deep neural networks for years. Although being a widely-adopted approach, using cross entropy as the primary objective exploits mostly the information from the ground-truth class for maximizing data likelihood, and largely ignores information from the complement (incorrect) classes. We argue that, in addition to the primary objective, training also using a complement objective that leverages information from the complement classes can be effective in improving model performance. This motivates us to study a new training paradigm that maximizes the likelihood of the groundtruth class while neutralizing the probabilities of the complement classes. We conduct extensive experiments on multiple tasks ranging from computer vision to natural language understanding. The experimental results confirm that, compared to the conventional training with just one primary objective, training also with the complement objective further improves the performance of the state-of-the-art models across all tasks. In addition to the accuracy improvement, we also show that models trained with both primary and complement objectives are more robust to single-step adversarial attacks.",0
"For years, deep neural networks have been trained with a primary objective, typically softmax cross entropy, for tasks like classification and sequence generation. However, this approach primarily focuses on maximizing data likelihood using only ground-truth class information, while ignoring information from incorrect classes. To address this issue, we propose a new training paradigm that includes a complement objective to leverage information from complement classes and improve model performance. Our experiments across various tasks demonstrate that training with both primary and complement objectives enhances model accuracy and robustness to single-step adversarial attacks.",1
"In order to bring artificial agents into our lives, we will need to go beyond supervised learning on closed datasets to having the ability to continuously expand knowledge. Inspired by a student learning in a classroom, we present an agent that can continuously learn by posing natural language questions to humans. Our agent is composed of three interacting modules, one that performs captioning, another that generates questions and a decision maker that learns when to ask questions by implicitly reasoning about the uncertainty of the agent and expertise of the teacher. As compared to current active learning methods which query images for full captions, our agent is able to ask pointed questions to improve the generated captions. The agent trains on the improved captions, expanding its knowledge. We show that our approach achieves better performance using less human supervision than the baselines on the challenging MSCOCO dataset.",0
"To integrate artificial agents into our daily routines, we must move beyond supervised learning with limited datasets and instead develop the ability to continually acquire knowledge. Drawing inspiration from a student in a classroom, we introduce an agent that can learn continuously by asking humans natural language questions. The agent consists of three modules that interact: one for captioning, another for generating queries, and a decision maker that learns when to pose questions by assessing the agent's uncertainty and the expertise of the teacher. Unlike current active learning techniques that request complete image captions, our agent can ask targeted questions to enhance the quality of the generated descriptions. The agent then trains on the enhanced captions, expanding its knowledge. Our approach demonstrates superior performance with less human input than the standard methods on the difficult MSCOCO dataset.",1
"This paper attacks the challenging problem of zero-example video retrieval. In such a retrieval paradigm, an end user searches for unlabeled videos by ad-hoc queries described in natural language text with no visual example provided. Given videos as sequences of frames and queries as sequences of words, an effective sequence-to-sequence cross-modal matching is required. The majority of existing methods are concept based, extracting relevant concepts from queries and videos and accordingly establishing associations between the two modalities. In contrast, this paper takes a concept-free approach, proposing a dual deep encoding network that encodes videos and queries into powerful dense representations of their own. Dual encoding is conceptually simple, practically effective and end-to-end. As experiments on three benchmarks, i.e. MSR-VTT, TRECVID 2016 and 2017 Ad-hoc Video Search show, the proposed solution establishes a new state-of-the-art for zero-example video retrieval.",0
"The objective of this article is to address the complex issue of retrieving videos with no examples provided. In this type of retrieval system, the user provides natural language text queries without any visual reference. The videos are represented as frames and the queries as words, therefore a sequence-to-sequence cross-modal matching method is necessary. Most current methods are concept-based and establish associations between videos and queries based on relevant concepts extracted from them. However, this paper proposes a concept-free approach with a dual deep encoding network that generates powerful dense representations for both videos and queries. This method is straightforward, efficient and end-to-end. Through experiments on three benchmarks, MSR-VTT, TRECVID 2016 and 2017 Ad-hoc Video Search, the proposed solution has been shown to achieve a new state-of-the-art for zero-example video retrieval.",1
"The hyperbolic manifold is a smooth manifold of negative constant curvature. While the hyperbolic manifold is well-studied in the literature, it has gained interest in the machine learning and natural language processing communities lately due to its usefulness in modeling continuous hierarchies. Tasks with hierarchical structures are ubiquitous in those fields and there is a general interest to learning hyperbolic representations or embeddings of such tasks. Additionally, these embeddings of related tasks may also share a low-rank subspace. In this work, we propose to learn hyperbolic embeddings such that they also lie in a low-dimensional subspace. In particular, we consider the problem of learning a low-rank factorization of hyperbolic embeddings. We cast these problems as manifold optimization problems and propose computationally efficient algorithms. Empirical results illustrate the efficacy of the proposed approach.",0
"The hyperbolic manifold, which is a smooth manifold with negative constant curvature, has been extensively studied in literature. Recently, the machine learning and natural language processing communities have shown interest in this manifold due to its ability to model continuous hierarchies, which are commonly found in these fields. Learning hyperbolic representations or embeddings of tasks with hierarchical structures is of great interest. Moreover, related tasks' embeddings may also share a low-rank subspace. In this study, we propose learning hyperbolic embeddings that lie in a low-dimensional subspace. Our focus is on the problem of learning a low-rank factorization of hyperbolic embeddings. We formulate these problems as manifold optimization problems and offer computationally efficient algorithms. Empirical results support the effectiveness of our proposed approach.",1
"Image captioning aims at automatically generating descriptions of an image in natural language. This is a challenging problem in the field of artificial intelligence that has recently received significant attention in the computer vision and natural language processing. Among the existing approaches, visual retrieval based methods have been proven to be highly effective. These approaches search for similar images, then build a caption for the query image based on the captions of the retrieved images. In this study, we present a method for visual retrieval based image captioning, in which we use a multi criteria decision making algorithm to effectively combine several criteria with proportional impact weights to retrieve the most relevant caption for the query image. The main idea of the proposed approach is to design a mechanism to retrieve more semantically relevant captions with the query image and then selecting the most appropriate caption by imitation of the human act based on a weighted multi-criteria decision making algorithm. Experiments conducted on MS COCO benchmark dataset have shown that proposed method provides much more effective results in compare to the state-of-the-art models by using criteria with proportional impact weights .",0
"The objective of image captioning is to generate descriptions of an image in natural language, which is a complicated task in the field of artificial intelligence. In recent times, this has received significant attention in computer vision and natural language processing. One of the most effective methods is visual retrieval based image captioning, where similar images are searched, and a caption is built for the query image based on the captions of the retrieved images. In this particular study, we propose a visual retrieval-based image captioning method that combines several criteria using a multi-criteria decision-making algorithm with proportional impact weights. The approach aims to retrieve semantically relevant captions by mimicking human behavior and then selecting the most appropriate caption. The proposed method was tested on the MS COCO benchmark dataset and showed more effective results compared to the state-of-the-art models that used criteria with proportional impact weights.",1
"Visual Grounding (VG) aims to locate the most relevant region in an image, based on a flexible natural language query but not a pre-defined label, thus it can be a more useful technique than object detection in practice. Most state-of-the-art methods in VG operate in a two-stage manner, wherein the first stage an object detector is adopted to generate a set of object proposals from the input image and the second stage is simply formulated as a cross-modal matching problem that finds the best match between the language query and all region proposals. This is rather inefficient because there might be hundreds of proposals produced in the first stage that need to be compared in the second stage, not to mention this strategy performs inaccurately. In this paper, we propose an simple, intuitive and much more elegant one-stage detection based method that joints the region proposal and matching stage as a single detection network. The detection is conditioned on the input query with a stack of novel Relation-to-Attention modules that transform the image-to-query relationship to an relation map, which is used to predict the bounding box directly without proposing large numbers of useless region proposals. During the inference, our approach is about 20x ~ 30x faster than previous methods and, remarkably, it achieves 18% ~ 41% absolute performance improvement on top of the state-of-the-art results on several benchmark datasets. We release our code and all the pre-trained models at https://github.com/openblack/rvg.",0
"The goal of Visual Grounding (VG) is to identify the most pertinent area of an image based on a flexible natural language inquiry, rather than a pre-determined label. As a result, VG can be more effective than object detection in practical applications. Most modern VG techniques employ a two-stage process, where an object detector is used in the first stage to produce a set of object proposals from the input image, and the second stage involves a cross-modal matching problem that seeks the best match between the language inquiry and all region proposals. However, this approach is inefficient and can produce inaccurate results due to the large number of proposals generated in the first stage. In our study, we propose a one-stage detection-based method that combines the region proposal and matching stages into a single detection network. Our method uses Relation-to-Attention modules to transform the image-to-query relationship into a relation map, which is used to directly predict the bounding box without generating numerous unnecessary region proposals. Our approach is significantly faster than previous methods and achieves a substantial improvement in performance on multiple benchmark datasets. We have made our code and pre-trained models available at https://github.com/openblack/rvg.",1
"Visual Query Answering (VQA) is of great significance in offering people convenience: one can raise a question for details of objects, or high-level understanding about the scene, over an image. This paper proposes a novel method to address the VQA problem. In contrast to prior works, our method that targets single scene VQA, replies on graph-based techniques and involves reasoning. In a nutshell, our approach is centered on three graphs. The first graph, referred to as inference graph GI , is constructed via learning over labeled data. The other two graphs, referred to as query graph Q and entity-attribute graph GEA, are generated from natural language query Qnl and image Img, that are issued from users, respectively. As GEA often does not take sufficient information to answer Q, we develop techniques to infer missing information of GEA with GI . Based on GEA and Q, we provide techniques to find matches of Q in GEA, as the answer of Qnl in Img. Unlike commonly used VQA methods that are based on end-to-end neural networks, our graph-based method shows well-designed reasoning capability, and thus is highly interpretable. We also create a dataset on soccer match (Soccer-VQA) with rich annotations. The experimental results show that our approach outperforms the state-of-the-art method and has high potential for future investigation.",0
"Visual Query Answering (VQA) is a convenient way for people to obtain details about objects or high-level understanding of a scene through an image. This paper proposes a new method for addressing the VQA problem that focuses on single scene VQA and relies on graph-based techniques and reasoning. The approach involves three graphs: the inference graph GI, which is constructed through learning from labeled data, and the query graph Q and entity-attribute graph GEA, which are generated from natural language queries and images from users. To address missing information in GEA, we use techniques to infer information from GI. Using GEA and Q, we identify matches of Q in GEA to provide answers to Qnl in Img. Our graph-based method shows strong reasoning capabilities and is highly interpretable, unlike end-to-end neural network-based VQA methods. We also introduce a soccer match dataset (Soccer-VQA) with rich annotations and demonstrate that our approach outperforms the state-of-the-art method, indicating its potential for future research.",1
"Multilayer switch networks are proposed as artificial generators of high-dimensional discrete data (e.g., binary vectors, categorical data, natural language, network log files, and discrete-valued time series). Unlike deconvolution networks which generate continuous-valued data and which consist of upsampling filters and reverse pooling layers, multilayer switch networks are composed of adaptive switches which model conditional distributions of discrete random variables. An interpretable, statistical framework is introduced for training these nonlinear networks based on a maximum-likelihood objective function. To learn network parameters, stochastic gradient descent is applied to the objective. This direct optimization is stable until convergence, and does not involve back-propagation over separate encoder and decoder networks, or adversarial training of dueling networks. While training remains tractable for moderately sized networks, Markov-chain Monte Carlo (MCMC) approximations of gradients are derived for deep networks which contain latent variables. The statistical framework is evaluated on synthetic data, high-dimensional binary data of handwritten digits, and web-crawled natural language data. Aspects of the model's framework such as interpretability, computational complexity, and generalization ability are discussed.",0
"Multilayer switch networks are suggested as a means of generating high-dimensional discrete data including binary vectors, categorical data, natural language, network log files, and discrete-valued time series. These networks differ from deconvolution networks in that they use adaptive switches to model conditional distributions of discrete random variables, as opposed to upsampling filters and reverse pooling layers which generate continuous-valued data. The proposed statistical framework for training these nonlinear networks is based on a maximum-likelihood objective function and utilizes stochastic gradient descent for direct optimization. This approach is stable until convergence and does not require back-propagation over separate encoder and decoder networks, or adversarial training of dueling networks. While training is manageable for moderately sized networks, deep networks with latent variables require Markov-chain Monte Carlo (MCMC) approximations of gradients. The statistical framework is evaluated with synthetic data, high-dimensional binary data of handwritten digits, and web-crawled natural language data. The paper concludes by discussing aspects of the model's framework, including interpretability, computational complexity, and generalization ability.",1
"We propose a method for efficient training of Q-functions for continuous-state Markov Decision Processes (MDPs) such that the traces of the resulting policies satisfy a given Linear Temporal Logic (LTL) property. LTL, a modal logic, can express a wide range of time-dependent logical properties (including ""safety"") that are quite similar to patterns in natural language. We convert the LTL property into a limit deterministic Buchi automaton and construct an on-the-fly synchronised product MDP. The control policy is then synthesised by defining an adaptive reward function and by applying a modified neural fitted Q-iteration algorithm to the synchronised structure, assuming that no prior knowledge is available from the original MDP. The proposed method is evaluated in a numerical study to test the quality of the generated control policy and is compared with conventional methods for policy synthesis such as MDP abstraction (Voronoi quantizer) and approximate dynamic programming (fitted value iteration).",0
"Our proposed approach efficiently trains Q-functions for continuous-state Markov Decision Processes (MDPs), resulting in policy traces that adhere to a specified Linear Temporal Logic (LTL) property. LTL is a modal logic that can express a range of time-dependent logical properties, including those found in natural language. To achieve this, we convert the LTL property into a limit deterministic Buchi automaton and create an on-the-fly synchronised product MDP. We then synthesise the control policy using an adaptive reward function and a modified neural fitted Q-iteration algorithm, assuming no prior knowledge from the original MDP. We evaluated the effectiveness of our method through numerical testing and compared it to conventional policy synthesis techniques such as MDP abstraction (Voronoi quantizer) and approximate dynamic programming (fitted value iteration).",1
"Deep learning techniques are rapidly advanced recently, and becoming a necessity component for widespread systems. However, the inference process of deep learning is black-box, and not very suitable to safety-critical systems which must exhibit high transparency. In this paper, to address this black-box limitation, we develop a simple analysis method which consists of 1) structural feature analysis: lists of the features contributing to inference process, 2) linguistic feature analysis: lists of the natural language labels describing the visual attributes for each feature contributing to inference process, and 3) consistency analysis: measuring consistency among input data, inference (label), and the result of our structural and linguistic feature analysis. Our analysis is simplified to reflect the actual inference process for high transparency, whereas it does not include any additional black-box mechanisms such as LSTM for highly human readable results. We conduct experiments and discuss the results of our analysis qualitatively and quantitatively, and come to believe that our work improves the transparency of neural networks. Evaluated through 12,800 human tasks, 75% workers answer that input data and result of our feature analysis are consistent, and 70% workers answer that inference (label) and result of our feature analysis are consistent. In addition to the evaluation of the proposed analysis, we find that our analysis also provide suggestions, or possible next actions such as expanding neural network complexity or collecting training data to improve a neural network.",0
"Recently, deep learning techniques have advanced rapidly and have become an essential component for widespread systems. However, the inference process of deep learning is opaque and not suitable for safety-critical systems that require high transparency. To overcome this limitation, this paper presents a simple analysis method that includes structural feature analysis, linguistic feature analysis, and consistency analysis. The analysis is designed to reflect the actual inference process for high transparency without including any additional black-box mechanisms. The experiments conducted in this study demonstrate that our analysis improves the transparency of neural networks. The evaluation of the proposed analysis shows that 75% of workers found input data and the result of our feature analysis to be consistent, while 70% of workers found the inference (label) and the result of our feature analysis to be consistent. Furthermore, our analysis provides suggestions for possible next actions, such as expanding neural network complexity or collecting training data, to improve a neural network.",1
"Machine learning models, especially neural network (NN) classifiers, are widely used in many applications including natural language processing, computer vision and cybersecurity. They provide high accuracy under the assumption of attack-free scenarios. However, this assumption has been defied by the introduction of adversarial examples -- carefully perturbed samples of input that are usually misclassified. Many researchers have tried to develop a defense against adversarial examples; however, we are still far from achieving that goal. In this paper, we design a Generative Adversarial Net (GAN) based adversarial training defense, dubbed GanDef, which utilizes a competition game to regulate the feature selection during the training. We analytically show that GanDef can train a classifier so it can defend against adversarial examples. Through extensive evaluation on different white-box adversarial examples, the classifier trained by GanDef shows the same level of test accuracy as those trained by state-of-the-art adversarial training defenses. More importantly, GanDef-Comb, a variant of GanDef, could utilize the discriminator to achieve a dynamic trade-off between correctly classifying original and adversarial examples. As a result, it achieves the highest overall test accuracy when the ratio of adversarial examples exceeds 41.7%.",0
"Machine learning models, particularly neural network classifiers, are prevalent in various fields such as natural language processing, computer vision, and cybersecurity due to their high accuracy in attack-free situations. However, the emergence of adversarial examples, which are carefully manipulated input samples that are often misclassified, has challenged this assumption. Despite numerous attempts to develop a defense against adversarial examples, a viable solution remains elusive. This paper presents GanDef, a Generative Adversarial Net (GAN) based adversarial training defense that utilizes a competition game to manage feature selection during training and can effectively train a classifier to defend against adversarial examples. The classifier trained using GanDef demonstrates the same level of accuracy as state-of-the-art adversarial training defenses when evaluated against different white-box adversarial examples. Interestingly, GanDef-Comb, a variant of GanDef, leverages the discriminator to achieve a dynamic balance between correctly classifying original and adversarial examples, resulting in the highest overall accuracy when the ratio of adversarial examples exceeds 41.7%.",1
"There has been growing interest in using neural networks and deep learning techniques to create dialogue systems. Conversational recommendation is an interesting setting for the scientific exploration of dialogue with natural language as the associated discourse involves goal-driven dialogue that often transforms naturally into more free-form chat. This paper provides two contributions. First, until now there has been no publicly available large-scale dataset consisting of real-world dialogues centered around recommendations. To address this issue and to facilitate our exploration here, we have collected ReDial, a dataset consisting of over 10,000 conversations centered around the theme of providing movie recommendations. We make this data available to the community for further research. Second, we use this dataset to explore multiple facets of conversational recommendations. In particular we explore new neural architectures, mechanisms, and methods suitable for composing conversational recommendation systems. Our dataset allows us to systematically probe model sub-components addressing different parts of the overall problem domain ranging from: sentiment analysis and cold-start recommendation generation to detailed aspects of how natural language is used in this setting in the real world. We combine such sub-components into a full-blown dialogue system and examine its behavior.",0
"The use of neural networks and deep learning techniques to develop dialogue systems has piqued interest. One intriguing area of study is conversational recommendation, which involves goal-driven dialogue that often transitions into more informal chat. This paper presents two significant contributions. Firstly, there has been no publicly available large-scale dataset that features real-world dialogues centered around recommendations. Therefore, we created ReDial, a dataset containing over 10,000 conversations focused on providing movie recommendations, to facilitate research in this domain. Secondly, we utilize this dataset to explore various aspects of conversational recommendations. We examine new neural architectures, mechanisms, and methods appropriate for creating conversational recommendation systems. Our dataset enables us to systematically analyze model sub-components addressing different parts of the problem domain, including sentiment analysis and cold-start recommendation generation. We also investigate how natural language is used in this context in the real world. We integrate these sub-components into a comprehensive dialogue system and assess its performance.",1
"We identify a phenomenon, which we refer to as multi-model forgetting, that occurs when sequentially training multiple deep networks with partially-shared parameters; the performance of previously-trained models degrades as one optimizes a subsequent one, due to the overwriting of shared parameters. To overcome this, we introduce a statistically-justified weight plasticity loss that regularizes the learning of a model's shared parameters according to their importance for the previous models, and demonstrate its effectiveness when training two models sequentially and for neural architecture search. Adding weight plasticity in neural architecture search preserves the best models to the end of the search and yields improved results in both natural language processing and computer vision tasks.",0
"The article highlights a phenomenon known as multi-model forgetting, which occurs when training multiple deep networks with partially-shared parameters. This results in a decline in performance of previously-trained models as one optimizes a subsequent one, due to the overwriting of shared parameters. To address this issue, the authors present a weight plasticity loss that regulates the learning of shared parameters based on their significance for previous models. The effectiveness of this approach is demonstrated through sequential training of two models and in neural architecture search. Adding weight plasticity in neural architecture search leads to better results in natural language processing and computer vision tasks by preserving the best models until the end of the search.",1
"We introduce the problem of learning distributed representations of edits. By combining a ""neural editor"" with an ""edit encoder"", our models learn to represent the salient information of an edit and can be used to apply edits to new inputs. We experiment on natural language and source code edit data. Our evaluation yields promising results that suggest that our neural network models learn to capture the structure and semantics of edits. We hope that this interesting task and data source will inspire other researchers to work further on this problem.",0
"Our aim is to address the issue of acquiring distributed representations of edits. Our approach involves utilizing a ""neural editor"" in conjunction with an ""edit encoder"" to enable our models to comprehend and apply the essential aspects of an edit on fresh inputs. Through our experimentation on both natural language and source code edit data, we have obtained encouraging outcomes that demonstrate the ability of our neural network models to comprehend the structure and meaning of edits. We anticipate that this noteworthy task and data source will motivate other researchers to delve deeper into this problem.",1
"The ability to generate natural language sequences from source code snippets has a variety of applications such as code summarization, documentation, and retrieval. Sequence-to-sequence (seq2seq) models, adopted from neural machine translation (NMT), have achieved state-of-the-art performance on these tasks by treating source code as a sequence of tokens. We present ${\rm {\scriptsize CODE2SEQ}}$: an alternative approach that leverages the syntactic structure of programming languages to better encode source code. Our model represents a code snippet as the set of compositional paths in its abstract syntax tree (AST) and uses attention to select the relevant paths while decoding. We demonstrate the effectiveness of our approach for two tasks, two programming languages, and four datasets of up to $16$M examples. Our model significantly outperforms previous models that were specifically designed for programming languages, as well as state-of-the-art NMT models. An interactive online demo of our model is available at http://code2seq.org. Our code, data and trained models are available at http://github.com/tech-srl/code2seq.",0
"Generating natural language sequences from source code snippets has numerous practical applications, including code summarization, documentation, and retrieval. While sequence-to-sequence (seq2seq) models have achieved impressive results in these tasks by treating source code as a sequence of tokens, they lack the ability to incorporate the syntactic structure of programming languages. To address this limitation, we introduce ${\rm {\scriptsize CODE2SEQ}}$, an alternative approach that encodes code snippets as the set of compositional paths in their abstract syntax trees (ASTs) and utilizes attention to select relevant paths during decoding. Our model outperforms previous models designed for programming languages and state-of-the-art NMT models across two tasks, two programming languages, and four datasets with up to $16$M examples. An interactive online demo of our model is available at http://code2seq.org, and our code, data, and trained models are accessible at http://github.com/tech-srl/code2seq.",1
"Reinforcement learning is a promising framework for solving control problems, but its use in practical situations is hampered by the fact that reward functions are often difficult to engineer. Specifying goals and tasks for autonomous machines, such as robots, is a significant challenge: conventionally, reward functions and goal states have been used to communicate objectives. But people can communicate objectives to each other simply by describing or demonstrating them. How can we build learning algorithms that will allow us to tell machines what we want them to do? In this work, we investigate the problem of grounding language commands as reward functions using inverse reinforcement learning, and argue that language-conditioned rewards are more transferable than language-conditioned policies to new environments. We propose language-conditioned reward learning (LC-RL), which grounds language commands as a reward function represented by a deep neural network. We demonstrate that our model learns rewards that transfer to novel tasks and environments on realistic, high-dimensional visual environments with natural language commands, whereas directly learning a language-conditioned policy leads to poor performance.",0
"Although reinforcement learning shows promise in controlling problems, the difficulty of engineering reward functions often hinders its practical use. Goals and tasks for autonomous machines are challenging to specify as traditional methods rely on reward functions and goal states to convey objectives. Conversely, people can communicate objectives by merely describing or demonstrating them. Therefore, the question arises: how can we develop learning algorithms that enable machines to understand our demands? In this study, we explore the issue of grounding language commands as reward functions using inverse reinforcement learning. We propose language-conditioned reward learning (LC-RL), which utilizes a deep neural network to represent language commands as a reward function. We argue that language-conditioned rewards are more transferable than language-conditioned policies to new environments. Our results indicate that our model learns rewards that transfer to novel tasks and environments on high-dimensional visual environments with natural language commands. In contrast, directly learning a language-conditioned policy leads to unsatisfactory results.",1
"Program synthesis from natural language (NL) is practical for humans and, once technically feasible, would significantly facilitate software development and revolutionize end-user programming. We present SAPS, an end-to-end neural network capable of mapping relatively complex, multi-sentence NL specifications to snippets of executable code. The proposed architecture relies exclusively on neural components, and is trained on abstract syntax trees, combined with a pretrained word embedding and a bi-directional multi-layer LSTM for processing of word sequences. The decoder features a doubly-recurrent LSTM, for which we propose novel signal propagation schemes and soft attention mechanism. When applied to a large dataset of problems proposed in a previous study, SAPS performs on par with or better than the method proposed there, producing correct programs in over 92% of cases. In contrast to other methods, it does not require post-processing of the resulting programs, and uses a fixed-dimensional latent representation as the only interface between the NL analyzer and the source code generator.",0
"The practicality of natural language program synthesis for humans is immense and, once technically feasible, could drastically improve software development and end-user programming. Our article introduces SAPS, an end-to-end neural network that can convert complicated, multi-sentence NL specifications into executable code snippets. The architecture of SAPS relies exclusively on neural components and is trained on abstract syntax trees, as well as a pretrained word embedding and a bi-directional multi-layer LSTM for processing word sequences. The decoder features a doubly-recurrent LSTM with novel signal propagation schemes and a soft attention mechanism. When applied to a vast dataset of problems suggested in a previous study, SAPS performs as well as or better than the proposed method, creating correct programs in over 92% of cases. Unlike other techniques, it does not require post-processing of the resulting programs and only uses a fixed-dimensional latent representation as the interface between the NL analyzer and source code generator.",1
"Sequence transduction models have been widely explored in many natural language processing tasks. However, the target sequence usually consists of discrete tokens which represent word indices in a given vocabulary. We barely see the case where target sequence is composed of continuous vectors, where each vector is an element of a time series taken successively in a temporal domain. In this work, we introduce a new data set, named Action Generation Data Set (AGDS) which is specifically designed to carry out the task of caption-to-action generation. This data set contains caption-action pairs. The caption is comprised of a sequence of words describing the interactive movement between two people, and the action is a captured sequence of poses representing the movement. This data set is introduced to study the ability of generating continuous sequences through sequence transduction models. We also propose a model to innovatively combine Multi-Head Attention (MHA) and Generative Adversarial Network (GAN) together. In our model, we have one generator to generate actions from captions and three discriminators where each of them is designed to carry out a unique functionality: caption-action consistency discriminator, pose discriminator and pose transition discriminator. This novel design allowed us to achieve plausible generation performance which is demonstrated in the experiments.",0
"Many natural language processing tasks have explored sequence transduction models, but typically, the target sequence is composed of discrete tokens representing word indices. It is rare to encounter a target sequence made up of continuous vectors from a time series. This study introduces the Action Generation Data Set (AGDS), which contains pairs of captions and actions designed for caption-to-action generation. The captions describe the interactive movement between two people, while the actions represent a captured sequence of poses. AGDS was created to examine the potential of sequence transduction models in generating continuous sequences. Additionally, a model combining Multi-Head Attention (MHA) and Generative Adversarial Network (GAN) was introduced. This model includes one generator and three discriminators, each with a unique functionality: caption-action consistency discriminator, pose discriminator, and pose transition discriminator. The model design achieved plausible generation performance, as demonstrated in the experiments.",1
"Sparse reward is one of the most challenging problems in reinforcement learning (RL). Hindsight Experience Replay (HER) attempts to address this issue by converting a failed experience to a successful one by relabeling the goals. Despite its effectiveness, HER has limited applicability because it lacks a compact and universal goal representation. We present Augmenting experienCe via TeacheR's adviCE (ACTRCE), an efficient reinforcement learning technique that extends the HER framework using natural language as the goal representation. We first analyze the differences among goal representation, and show that ACTRCE can efficiently solve difficult reinforcement learning problems in challenging 3D navigation tasks, whereas HER with non-language goal representation failed to learn. We also show that with language goal representations, the agent can generalize to unseen instructions, and even generalize to instructions with unseen lexicons. We further demonstrate it is crucial to use hindsight advice to solve challenging tasks, and even small amount of advice is sufficient for the agent to achieve good performance.",0
"Reinforcement learning (RL) faces a difficult problem with sparse rewards, which is a major challenge. To combat this issue, Hindsight Experience Replay (HER) has been developed to transform unsuccessful experiences into successful ones by changing the goals. However, HER has limited practicality as it lacks a standardized, concise goal representation. Our proposed solution, Augmenting experienCe via TeacheR's adviCE (ACTRCE), is an effective RL technique that enhances HER by utilizing natural language as a goal representation. Our analysis demonstrates the superiority of language goal representations, as ACTRCE efficiently tackles complex 3D navigation tasks that HER with non-language goals failed to learn. We also show that ACTRCE can generalize to new instructions and even unfamiliar lexicons. Our research emphasizes the importance of hindsight advice to overcome challenging tasks, and even small amounts of guidance can lead to good performance.",1
"Memory-augmented neural networks (MANNs) are designed for question-answering tasks. It is difficult to run a MANN effectively on accelerators designed for other neural networks (NNs), in particular on mobile devices, because MANNs require recurrent data paths and various types of operations related to external memory access. We implement an accelerator for MANNs on a field-programmable gate array (FPGA) based on a data flow architecture. Inference times are also reduced by inference thresholding, which is a data-based maximum inner-product search specialized for natural language tasks. Measurements on the bAbI data show that the energy efficiency of the accelerator (FLOPS/kJ) was higher than that of an NVIDIA TITAN V GPU by a factor of about 125, increasing to 140 with inference thresholding",0
"MANNs, which are primarily used for question-answering tasks, pose a challenge when it comes to running them effectively on accelerators intended for other neural networks. This challenge is particularly apparent on mobile devices due to the MANNs' need for recurrent data paths and diverse operations related to external memory access. To overcome this challenge, we developed an accelerator for MANNs on an FPGA, using a data flow architecture. We also implemented inference thresholding, a specialized maximum inner-product search for natural language tasks, to further reduce inference times. Tests conducted on the bAbI data indicated that the accelerator's energy efficiency (FLOPS/kJ) was significantly higher than that of an NVIDIA TITAN V GPU, with a factor of approximately 125, which increased to 140 with inference thresholding.",1
"The recent advances of deep learning in both computer vision (CV) and natural language processing (NLP) provide us a new way of understanding semantics, by which we can deal with more challenging tasks such as automatic description generation from natural images. In this challenge, the encoder-decoder framework has achieved promising performance when a convolutional neural network (CNN) is used as image encoder and a recurrent neural network (RNN) as decoder. In this paper, we introduce a sequential guiding network that guides the decoder during word generation. The new model is an extension of the encoder-decoder framework with attention that has an additional guiding long short-term memory (LSTM) and can be trained in an end-to-end manner by using image/descriptions pairs. We validate our approach by conducting extensive experiments on a benchmark dataset, i.e., MS COCO Captions. The proposed model achieves significant improvement comparing to the other state-of-the-art deep learning models.",0
"The advancements in deep learning for computer vision and natural language processing have opened up a new way of comprehending semantics. This allows us to tackle more complex tasks such as generating automatic descriptions from natural images. The encoder-decoder framework, which utilizes a convolutional neural network as image encoder and a recurrent neural network as decoder, has shown promising results in this area. In this study, we introduce a sequential guiding network that assists the decoder in word generation. Our model is an extension of the encoder-decoder framework with attention and includes a guiding long short-term memory. It can be trained end-to-end using image/descriptions pairs. To validate our approach, we conducted extensive experiments on the MS COCO Captions benchmark dataset. Our proposed model outperformed other state-of-the-art deep learning models.",1
"The Generative Adversarial Network (GAN) has recently been applied to generate synthetic images from text. Despite significant advances, most current state-of-the-art algorithms are regular-grid region based; when attention is used, it is mainly applied between individual regular-grid regions and a word. These approaches are sufficient to generate images that contain a single object in its foreground, such as a ""bird"" or ""flower"". However, natural languages often involve complex foreground objects and the background may also constitute a variable portion of the generated image. Therefore, the regular-grid based image attention weights may not necessarily concentrate on the intended foreground region(s), which in turn, results in an unnatural looking image. Additionally, individual words such as ""a"", ""blue"" and ""shirt"" do not necessarily provide a full visual context unless they are applied together. For this reason, in our paper, we proposed a novel method in which we introduced an additional set of attentions between true-grid regions and word phrases. The true-grid region is derived using a set of auxiliary bounding boxes. These auxiliary bounding boxes serve as superior location indicators to where the alignment and attention should be drawn with the word phrases. Word phrases are derived from analysing Part-of-Speech (POS) results. We perform experiments on this novel network architecture using the Microsoft Common Objects in Context (MSCOCO) dataset and the model generates $256 \times 256$ conditioned on a short sentence description. Our proposed approach is capable of generating more realistic images compared with the current state-of-the-art algorithms.",0
"Recently, the Generative Adversarial Network (GAN) has been utilized to produce synthetic images from text. While there have been significant advancements, most of the state-of-the-art algorithms are based on regular-grid regions. When attention is applied, it is mainly between individual regular-grid regions and a word. These methods are suitable for generating images with a single object in the foreground, like a ""bird"" or ""flower"". However, natural languages often involve complex foreground objects, and the background may also constitute a variable portion of the image. Consequently, the regular-grid-based image attention weights may not focus on the intended foreground region(s), resulting in an unnatural-looking image. Furthermore, individual words like ""a,"" ""blue,"" and ""shirt"" may not provide a complete visual context unless they are used together. Hence, we proposed a new approach in our paper, introducing an additional set of attentions between true-grid regions and word phrases. We derive the true-grid region using auxiliary bounding boxes, which serve as superior location indicators, guiding the alignment and attention towards the intended region with the word phrases. We derive word phrases by analyzing Part-of-Speech (POS) results. We conducted experiments using the Microsoft Common Objects in Context (MSCOCO) dataset and generated $256 \times 256$ images conditioned on a short sentence description. Our proposed method can generate more realistic images than the current state-of-the-art algorithms.",1
"Word embedding is a powerful tool in natural language processing. In this paper we consider the problem of word embedding composition \--- given vector representations of two words, compute a vector for the entire phrase. We give a generative model that can capture specific syntactic relations between words. Under our model, we prove that the correlations between three words (measured by their PMI) form a tensor that has an approximate low rank Tucker decomposition. The result of the Tucker decomposition gives the word embeddings as well as a core tensor, which can be used to produce better compositions of the word embeddings. We also complement our theoretical results with experiments that verify our assumptions, and demonstrate the effectiveness of the new composition method.",0
"In natural language processing, word embedding is a useful tool. The problem of composing word embeddings is explored in this paper, where the objective is to calculate a vector for a complete phrase by using the vector representations of two words. A generative model is presented that can capture specific syntactic relationships between words. The paper proves that the correlations between three words, as measured by their PMI, can form a tensor that has an approximate low-rank Tucker decomposition under the presented model. The Tucker decomposition produces the word embeddings and a core tensor that can be used to improve the composition of word embeddings. To verify the assumptions made, experiments are conducted, and the effectiveness of the new composition method is demonstrated.",1
"We define general linguistic intelligence as the ability to reuse previously acquired knowledge about a language's lexicon, syntax, semantics, and pragmatic conventions to adapt to new tasks quickly. Using this definition, we analyze state-of-the-art natural language understanding models and conduct an extensive empirical investigation to evaluate them against these criteria through a series of experiments that assess the task-independence of the knowledge being acquired by the learning process. In addition to task performance, we propose a new evaluation metric based on an online encoding of the test data that quantifies how quickly an existing agent (model) learns a new task. Our results show that while the field has made impressive progress in terms of model architectures that generalize to many tasks, these models still require a lot of in-domain training examples (e.g., for fine tuning, training task-specific modules), and are prone to catastrophic forgetting. Moreover, we find that far from solving general tasks (e.g., document question answering), our models are overfitting to the quirks of particular datasets (e.g., SQuAD). We discuss missing components and conjecture on how to make progress toward general linguistic intelligence.",0
"Our definition of general linguistic intelligence is the capacity to utilize previously acquired knowledge of a language's lexicon, syntax, semantics, and pragmatic conventions to quickly adapt to new tasks. We use this definition to analyze current natural language understanding models and conduct extensive empirical research to assess their ability to meet these standards through a series of experiments that evaluate the task-independence of the knowledge learned through the learning process. In addition to measuring task performance, we propose a novel evaluation metric that calculates how rapidly an existing agent (model) learns a new task based on an online encoding of the test data. Our findings demonstrate that while the field has made significant advancements in terms of model architectures that generalize to multiple tasks, these models still necessitate a substantial amount of in-domain training examples (e.g., for fine-tuning, training task-specific modules) and are susceptible to catastrophic forgetting. Furthermore, we discovered that our models are overfitting to the peculiarities of specific datasets (e.g., SQuAD) rather than addressing general tasks (e.g., document question answering). We discuss the missing components and hypothesize on how to make strides toward general linguistic intelligence.",1
"Behavioral skills or policies for autonomous agents are conventionally learned from reward functions, via reinforcement learning, or from demonstrations, via imitation learning. However, both modes of task specification have their disadvantages: reward functions require manual engineering, while demonstrations require a human expert to be able to actually perform the task in order to generate the demonstration. Instruction following from natural language instructions provides an appealing alternative: in the same way that we can specify goals to other humans simply by speaking or writing, we would like to be able to specify tasks for our machines. However, a single instruction may be insufficient to fully communicate our intent or, even if it is, may be insufficient for an autonomous agent to actually understand how to perform the desired task. In this work, we propose an interactive formulation of the task specification problem, where iterative language corrections are provided to an autonomous agent, guiding it in acquiring the desired skill. Our proposed language-guided policy learning algorithm can integrate an instruction and a sequence of corrections to acquire new skills very quickly. In our experiments, we show that this method can enable a policy to follow instructions and corrections for simulated navigation and manipulation tasks, substantially outperforming direct, non-interactive instruction following.",0
"Conventional methods for teaching behavioral skills or policies to autonomous agents involve learning from reward functions through reinforcement learning or from demonstrations via imitation learning. However, both methods have drawbacks. Reward functions require manual engineering, while demonstrations require a human expert to perform the task. Using natural language instructions to guide the behavior of machines is an attractive alternative. However, a single instruction may not be enough to fully communicate the desired task, or an autonomous agent may not understand how to perform it. To address this, we propose an interactive approach to task specification, where iterative language corrections guide the autonomous agent in acquiring the desired skill. Our language-guided policy learning algorithm integrates instructions and corrections to quickly acquire new skills. Our experiments show that this method outperforms direct, non-interactive instruction following for simulated navigation and manipulation tasks.",1
"It is important to detect anomalous inputs when deploying machine learning systems. The use of larger and more complex inputs in deep learning magnifies the difficulty of distinguishing between anomalous and in-distribution examples. At the same time, diverse image and text data are available in enormous quantities. We propose leveraging these data to improve deep anomaly detection by training anomaly detectors against an auxiliary dataset of outliers, an approach we call Outlier Exposure (OE). This enables anomaly detectors to generalize and detect unseen anomalies. In extensive experiments on natural language processing and small- and large-scale vision tasks, we find that Outlier Exposure significantly improves detection performance. We also observe that cutting-edge generative models trained on CIFAR-10 may assign higher likelihoods to SVHN images than to CIFAR-10 images; we use OE to mitigate this issue. We also analyze the flexibility and robustness of Outlier Exposure, and identify characteristics of the auxiliary dataset that improve performance.",0
"The identification of abnormal inputs is crucial in the implementation of machine learning systems. The difficulty in distinguishing between anomalous and in-distribution examples is amplified by the use of larger and more complex inputs in deep learning. However, with the availability of vast amounts of diverse image and text data, we suggest using these data to enhance deep anomaly detection through the training of anomaly detectors using an additional dataset of outliers, which we term Outlier Exposure (OE). This approach allows for the generalization of anomaly detectors to recognize unseen anomalies. Through extensive experiments on natural language processing and small- and large-scale vision tasks, we have discovered that Outlier Exposure significantly enhances detection performance. We have also identified that advanced generative models trained on CIFAR-10 may assign higher likelihoods to SVHN images than to CIFAR-10 images; we use OE to address this issue. Additionally, we examine the versatility and resilience of Outlier Exposure and identify features of the auxiliary dataset that boost performance.",1
"The growing importance of massive datasets used for deep learning makes robustness to label noise a critical property for classifiers to have. Sources of label noise include automatic labeling, non-expert labeling, and label corruption by data poisoning adversaries. Numerous previous works assume that no source of labels can be trusted. We relax this assumption and assume that a small subset of the training data is trusted. This enables substantial label corruption robustness performance gains. In addition, particularly severe label noise can be combated by using a set of trusted data with clean labels. We utilize trusted data by proposing a loss correction technique that utilizes trusted examples in a data-efficient manner to mitigate the effects of label noise on deep neural network classifiers. Across vision and natural language processing tasks, we experiment with various label noises at several strengths, and show that our method significantly outperforms existing methods.",0
"The significance of massive datasets in deep learning necessitates that classifiers possess robustness to label noise. Label noise can arise from automatic labeling, non-expert labeling, and label corruption by data poisoning adversaries. While previous works assume that no labels can be trusted, we propose a relaxation of this assumption by assuming a small subset of the training data is trustworthy. This approach results in substantial improvements in label corruption robustness performance. For severe label noise, a set of trusted data with clean labels can be used to mitigate its impact. We propose a loss correction technique that utilizes trusted data in a data-efficient manner to combat the effects of label noise on deep neural network classifiers. Our method outperforms existing methods across vision and natural language processing tasks, as we demonstrate through experiments involving various label noises of different strengths.",1
"Image captioning is the process of generating a natural language description of an image. Most current image captioning models, however, do not take into account the emotional aspect of an image, which is very relevant to activities and interpersonal relationships represented therein. Towards developing a model that can produce human-like captions incorporating these, we use facial expression features extracted from images including human faces, with the aim of improving the descriptive ability of the model. In this work, we present two variants of our Face-Cap model, which embed facial expression features in different ways, to generate image captions. Using all standard evaluation metrics, our Face-Cap models outperform a state-of-the-art baseline model for generating image captions when applied to an image caption dataset extracted from the standard Flickr 30K dataset, consisting of around 11K images containing faces. An analysis of the captions finds that, perhaps surprisingly, the improvement in caption quality appears to come not from the addition of adjectives linked to emotional aspects of the images, but from more variety in the actions described in the captions.",0
"The process of generating a natural language description of an image is known as image captioning. However, current image captioning models do not consider the emotional aspect of an image, which is crucial to activities and interpersonal relationships depicted in the image. To create a model that can produce human-like captions incorporating emotional aspects, we utilize facial expression features extracted from images containing human faces. Our aim is to enhance the model's descriptive ability. We present two versions of our Face-Cap model, which include facial expression features in different ways to generate image captions. When applied to an image caption dataset extracted from the standard Flickr 30K dataset, consisting of around 11K images containing faces, our Face-Cap models outperform a state-of-the-art baseline model for generating image captions using all standard evaluation metrics. Surprisingly, an analysis of the captions reveals that the improvement in caption quality results from increased variety in the actions described in the captions, rather than the addition of adjectives associated with emotional aspects of the images.",1
"The task of video grounding, which temporally localizes a natural language description in a video, plays an important role in understanding videos. Existing studies have adopted strategies of sliding window over the entire video or exhaustively ranking all possible clip-sentence pairs in a pre-segmented video, which inevitably suffer from exhaustively enumerated candidates. To alleviate this problem, we formulate this task as a problem of sequential decision making by learning an agent which regulates the temporal grounding boundaries progressively based on its policy. Specifically, we propose a reinforcement learning based framework improved by multi-task learning and it shows steady performance gains by considering additional supervised boundary information during training. Our proposed framework achieves state-of-the-art performance on ActivityNet'18 DenseCaption dataset and Charades-STA dataset while observing only 10 or less clips per video.",0
"Understanding videos involves the important task of video grounding, which involves localizing a natural language description in a video. Current methods involve sliding a window over the entire video or ranking all possible clip-sentence pairs in a pre-segmented video, which can result in an exhaustive list of candidates. To address this issue, our approach formulates this task as a problem of sequential decision making and uses a reinforcement learning-based framework that incorporates multi-task learning. By including supervised boundary information during training, our proposed framework shows consistent improvements and achieves state-of-the-art performance on the ActivityNet'18 DenseCaption dataset and Charades-STA dataset while only observing 10 or fewer clips per video.",1
"We introduce a new inference task - Visual Entailment (VE) - which differs from traditional Textual Entailment (TE) tasks whereby a premise is defined by an image, rather than a natural language sentence as in TE tasks. A novel dataset SNLI-VE (publicly available at https://github.com/necla-ml/SNLI-VE) is proposed for VE tasks based on the Stanford Natural Language Inference corpus and Flickr30k. We introduce a differentiable architecture called the Explainable Visual Entailment model (EVE) to tackle the VE problem. EVE and several other state-of-the-art visual question answering (VQA) based models are evaluated on the SNLI-VE dataset, facilitating grounded language understanding and providing insights on how modern VQA based models perform.",0
"Our novel contribution is the introduction of a Visual Entailment (VE) inference task, which sets itself apart from traditional Textual Entailment (TE) tasks by using images rather than natural language sentences to define a premise. To facilitate this task, we have created a dataset called SNLI-VE, which is publicly available at https://github.com/necla-ml/SNLI-VE and is based on the Stanford Natural Language Inference corpus and Flickr30k. To tackle the VE problem, we have developed a differentiable architecture called the Explainable Visual Entailment model (EVE). We have evaluated EVE and various other state-of-the-art visual question answering (VQA) based models on the SNLI-VE dataset. This evaluation provides insights on how modern VQA based models perform and promotes grounded language understanding.",1
"Existing visual reasoning datasets such as Visual Question Answering (VQA), often suffer from biases conditioned on the question, image or answer distributions. The recently proposed CLEVR dataset addresses these limitations and requires fine-grained reasoning but the dataset is synthetic and consists of similar objects and sentence structures across the dataset.   In this paper, we introduce a new inference task, Visual Entailment (VE) - consisting of image-sentence pairs whereby a premise is defined by an image, rather than a natural language sentence as in traditional Textual Entailment tasks. The goal of a trained VE model is to predict whether the image semantically entails the text. To realize this task, we build a dataset SNLI-VE based on the Stanford Natural Language Inference corpus and Flickr30k dataset. We evaluate various existing VQA baselines and build a model called Explainable Visual Entailment (EVE) system to address the VE task. EVE achieves up to 71% accuracy and outperforms several other state-of-the-art VQA based models. Finally, we demonstrate the explainability of EVE through cross-modal attention visualizations. The SNLI-VE dataset is publicly available at https://github.com/ necla-ml/SNLI-VE.",0
"Current datasets for visual reasoning, such as Visual Question Answering (VQA), often exhibit biases depending on the distribution of questions, images, or answers. Although the CLEVR dataset has been recently introduced to address these limitations, it still suffers from synthetic data and uniformity across objects and sentence structures. To overcome these issues, we propose a new inference task, Visual Entailment (VE), which involves pairs of images and sentences as premises instead of natural language sentences. The VE model aims to predict whether the image semantically entails the text. To create the dataset for this task, we used the Stanford Natural Language Inference corpus and Flickr30k dataset, resulting in the SNLI-VE dataset. We evaluated various VQA baselines and developed the Explainable Visual Entailment (EVE) system to tackle the VE task. Our results showed that EVE outperformed other state-of-the-art VQA models with up to 71% accuracy. Additionally, we demonstrated the explainability of EVE through cross-modal attention visualizations. The SNLI-VE dataset is publicly available on GitHub at https://github.com/necla-ml/SNLI-VE.",1
"Deep neural networks (DNNs) have been widely used in the fields such as natural language processing, computer vision and image recognition. But several studies have been shown that deep neural networks can be easily fooled by artificial examples with some perturbations, which are widely known as adversarial examples. Adversarial examples can be used to attack deep neural networks or to improve the robustness of deep neural networks. A common way of generating adversarial examples is to first generate some noises and then add them into original examples. In practice, different examples have different noise-sensitive. To generate an effective adversarial example, it may be necessary to add a lot of noise to low noise-sensitive example, which may make the adversarial example meaningless. In this paper, we propose a noise-sensitivity-analysis-based test prioritization technique to pick out examples by their noise sensitivity. We construct an experiment to validate our approach on four image sets and two DNN models, which shows that examples are sensitive to noise and our method can effectively pick out examples by their noise sensitivity.",0
"DNNs have gained widespread use in natural language processing, computer vision, and image recognition. However, studies have revealed that they can be easily fooled by adversarial examples, which are artificial examples with perturbations. These examples can be utilized to attack DNNs or enhance their robustness. Typically, adversarial examples are created by adding noise to original examples, but different examples have varying noise sensitivity. Adding excessive noise to low noise-sensitive examples may render the adversarial example meaningless. To address this issue, we introduce a test prioritization technique based on noise sensitivity analysis, which selects examples according to their noise sensitivity. Our approach was validated through an experiment on four image sets and two DNN models, which confirmed that our method can effectively identify noise-sensitive examples.",1
"Assisted by the availability of data and high performance computing, deep learning techniques have achieved breakthroughs and surpassed human performance empirically in difficult tasks, including object recognition, speech recognition, and natural language processing. As they are being used in critical applications, understanding underlying mechanisms for their successes and limitations is imperative. In this paper, we show that overfitting, one of the fundamental issues in deep neural networks, is due to continuous gradient updating and scale sensitiveness of cross entropy loss. By separating samples into correctly and incorrectly classified ones, we show that they behave very differently, where the loss decreases in the correct ones and increases in the incorrect ones. Furthermore, by analyzing dynamics during training, we propose a consensus-based classification algorithm that enables us to avoid overfitting and significantly improve the classification accuracy especially when the number of training samples is limited. As each trained neural network depends on extrinsic factors such as initial values as well as training data, requiring consensus among multiple models reduces extrinsic factors substantially; for statistically independent models, the reduction is exponential. Compared to ensemble algorithms, the proposed algorithm avoids overgeneralization by not classifying ambiguous inputs. Systematic experimental results demonstrate the effectiveness of the proposed algorithm. For example, using only 1000 training samples from MNIST dataset, the proposed algorithm achieves 95% accuracy, significantly higher than any of the individual models, with 90% of the test samples classified.",0
"Thanks to the availability of data and high performance computing, deep learning methods have made remarkable progress and have empirically surpassed human performance in challenging tasks, such as object recognition, speech recognition, and natural language processing. Since these techniques are being applied in crucial applications, comprehending the underlying mechanisms of their achievements and limitations is essential. This article demonstrates that one of the primary problems in deep neural networks, overfitting, results from continuous gradient updating and the sensitivity of cross entropy loss to scale. By segregating correctly and incorrectly classified samples, we illustrate that they behave differently, where the loss diminishes in the correct ones and increases in the incorrect ones. Additionally, by scrutinizing the dynamics during training, we propose a consensus-based classification algorithm that avoids overfitting and significantly improves classification accuracy, particularly when the number of training samples is limited. Since each trained neural network is influenced by extraneous factors such as initial values and training data, requiring agreement among numerous models decreases extrinsic factors substantially; for statistically independent models, the reduction is exponential. In contrast to ensemble algorithms, the suggested algorithm avoids overgeneralization by refraining from classifying ambiguous inputs. The method's effectiveness is demonstrated through systematic experimental results. For instance, using only 1000 training samples from the MNIST dataset, the proposed algorithm achieves 95% accuracy, notably higher than any of the individual models, with 90% of the test samples classified.",1
"Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related AI technology, increasing transparency into how well AI technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.",0
"The use of trained machine learning models has become increasingly prevalent in various fields, including law enforcement, medicine, education, and employment, where they are utilized for high-impact tasks. To ensure that the intended use cases of these models are clearly defined and to minimize their usage in unsuitable contexts, it is recommended that documentation detailing their performance characteristics be provided alongside the models. To address this need for transparency, a framework called model cards is proposed in this paper. Model cards are concise documents that accompany trained machine learning models and provide benchmarked evaluation in diverse conditions, including different cultural, demographic, or phenotypic groups, and intersectional groups relevant to the intended application domains. The cards also disclose the intended usage context, performance evaluation procedures, and other pertinent information. While the framework primarily focuses on human-centered machine learning models in computer vision and natural language processing, it can apply to any trained machine learning model. To illustrate the concept, model cards for two supervised models, one detecting smiling faces in images and the other detecting toxic comments in text, are provided. The aim of proposing model cards is to promote responsible democratization of machine learning and related AI technology, by increasing transparency into the performance of AI technology. The hope is that this framework will encourage those releasing trained machine learning models to provide detailed evaluation numbers and other relevant documentation.",1
"We present Memory Augmented Policy Optimization (MAPO), a simple and novel way to leverage a memory buffer of promising trajectories to reduce the variance of policy gradient estimate. MAPO is applicable to deterministic environments with discrete actions, such as structured prediction and combinatorial optimization tasks. We express the expected return objective as a weighted sum of two terms: an expectation over the high-reward trajectories inside the memory buffer, and a separate expectation over trajectories outside the buffer. To make an efficient algorithm of MAPO, we propose: (1) memory weight clipping to accelerate and stabilize training; (2) systematic exploration to discover high-reward trajectories; (3) distributed sampling from inside and outside of the memory buffer to scale up training. MAPO improves the sample efficiency and robustness of policy gradient, especially on tasks with sparse rewards. We evaluate MAPO on weakly supervised program synthesis from natural language (semantic parsing). On the WikiTableQuestions benchmark, we improve the state-of-the-art by 2.6%, achieving an accuracy of 46.3%. On the WikiSQL benchmark, MAPO achieves an accuracy of 74.9% with only weak supervision, outperforming several strong baselines with full supervision. Our source code is available at https://github.com/crazydonkey200/neural-symbolic-machines",0
"A new method called Memory Augmented Policy Optimization (MAPO) is introduced in this paper. The approach utilizes a memory buffer of successful trajectories to reduce the variance of the policy gradient estimate in deterministic environments with discrete actions. This is particularly useful for tasks such as structured prediction and combinatorial optimization. The expected return objective is expressed as a weighted sum of two terms, one for high-reward trajectories within the buffer, and another for those outside of it. To create an efficient algorithm for MAPO, memory weight clipping, systematic exploration, and distributed sampling are proposed. MAPO improves the sample efficiency and robustness of policy gradient, especially in tasks with sparse rewards. The approach was evaluated on weakly supervised program synthesis from natural language, specifically on the WikiTableQuestions and WikiSQL benchmarks. MAPO achieved state-of-the-art results with an accuracy of 46.3% and 74.9% respectively, outperforming strong baselines with full supervision. The source code is available at https://github.com/crazydonkey200/neural-symbolic-machines.",1
"Automatically generating the descriptions of an image, i.e., image captioning, is an important and fundamental topic in artificial intelligence, which bridges the gap between computer vision and natural language processing. Based on the successful deep learning models, especially the CNN model and Long Short-Term Memories (LSTMs) with attention mechanism, we propose a hierarchical attention model by utilizing both of the global CNN features and the local object features for more effective feature representation and reasoning in image captioning. The generative adversarial network (GAN), together with a reinforcement learning (RL) algorithm, is applied to solve the exposure bias problem in RNN-based supervised training for language problems. In addition, through the automatic measurement of the consistency between the generated caption and the image content by the discriminator in the GAN framework and RL optimization, we make the finally generated sentences more accurate and natural. Comprehensive experiments show the improved performance of the hierarchical attention mechanism and the effectiveness of our RL-based optimization method. Our model achieves state-of-the-art results on several important metrics in the MSCOCO dataset, using only greedy inference.",0
"The generation of image descriptions, also known as image captioning, is a crucial topic in the field of artificial intelligence. It serves as a link between computer vision and natural language processing. Our proposal is a hierarchical attention model that utilizes both global CNN features and local object features to create a more effective feature representation and reasoning in image captioning. To address the exposure bias problem in RNN-based supervised training, we use a generative adversarial network (GAN) and a reinforcement learning (RL) algorithm. The discriminator in the GAN framework and RL optimization automatically measure the consistency between the generated caption and the image content to produce more accurate and natural sentences. Comprehensive experiments demonstrate the improved performance of the hierarchical attention mechanism and the effectiveness of our RL-based optimization method. Our model achieves state-of-the-art results on several important metrics in the MSCOCO dataset, using only greedy inference.",1
"We combine Recurrent Neural Networks with Tensor Product Representations to learn combinatorial representations of sequential data. This improves symbolic interpretation and systematic generalisation. Our architecture is trained end-to-end through gradient descent on a variety of simple natural language reasoning tasks, significantly outperforming the latest state-of-the-art models in single-task and all-tasks settings. We also augment a subset of the data such that training and test data exhibit large systematic differences and show that our approach generalises better than the previous state-of-the-art.",0
"By integrating Tensor Product Representations with Recurrent Neural Networks, we acquire the ability to understand and generalize combinatorial representations of sequential data, improving symbolic interpretation. To achieve this, we train our architecture end-to-end via gradient descent on multiple elementary natural language reasoning tasks. Our approach outperforms the latest state-of-the-art models in both single-task and all-tasks settings. Furthermore, we add a subset of data to our training and testing datasets that display significant systematic differences, and demonstrate that our method outperforms the previous best model in terms of generalization.",1
"Recent improvements to Generative Adversarial Networks (GANs) have made it possible to generate realistic images in high resolution based on natural language descriptions such as image captions. Furthermore, conditional GANs allow us to control the image generation process through labels or even natural language descriptions. However, fine-grained control of the image layout, i.e. where in the image specific objects should be located, is still difficult to achieve. This is especially true for images that should contain multiple distinct objects at different spatial locations. We introduce a new approach which allows us to control the location of arbitrarily many objects within an image by adding an object pathway to both the generator and the discriminator. Our approach does not need a detailed semantic layout but only bounding boxes and the respective labels of the desired objects are needed. The object pathway focuses solely on the individual objects and is iteratively applied at the locations specified by the bounding boxes. The global pathway focuses on the image background and the general image layout. We perform experiments on the Multi-MNIST, CLEVR, and the more complex MS-COCO data set. Our experiments show that through the use of the object pathway we can control object locations within images and can model complex scenes with multiple objects at various locations. We further show that the object pathway focuses on the individual objects and learns features relevant for these, while the global pathway focuses on global image characteristics and the image background.",0
"Recent advancements in Generative Adversarial Networks (GANs) have made it possible to create lifelike high-resolution images based on natural language descriptions, including image captions. Conditional GANs have also enabled us to regulate the image generation process using labels or even natural language descriptions. However, achieving precise control over the image layout, particularly for images that require multiple distinct objects at various spatial locations, remains challenging. To address this issue, we present a novel approach that integrates an object pathway into both the generator and discriminator, allowing us to control the location of an unlimited number of objects within an image using only bounding boxes and object labels. Our approach focuses solely on the individual objects and is applied iteratively at the specified bounding box locations, while the global pathway attends to the image background and general image layout. We tested our method on the Multi-MNIST, CLEVR, and more intricate MS-COCO datasets, demonstrating that the object pathway can model complex scenes containing multiple objects at different locations. Additionally, we found that the object pathway learns features relevant to individual objects, while the global pathway attends to the overall image characteristics and background.",1
"Recent progress has been made in using attention based encoder-decoder framework for image and video captioning. Most existing decoders apply the attention mechanism to every generated word including both visual words (e.g., ""gun"" and ""shooting"") and non-visual words (e.g. ""the"", ""a""). However, these non-visual words can be easily predicted using natural language model without considering visual signals or attention. Imposing attention mechanism on non-visual words could mislead and decrease the overall performance of visual captioning. Furthermore, the hierarchy of LSTMs enables more complex representation of visual data, capturing information at different scales. To address these issues, we propose a hierarchical LSTM with adaptive attention (hLSTMat) approach for image and video captioning. Specifically, the proposed framework utilizes the spatial or temporal attention for selecting specific regions or frames to predict the related words, while the adaptive attention is for deciding whether to depend on the visual information or the language context information. Also, a hierarchical LSTMs is designed to simultaneously consider both low-level visual information and high-level language context information to support the caption generation. We initially design our hLSTMat for video captioning task. Then, we further refine it and apply it to image captioning task. To demonstrate the effectiveness of our proposed framework, we test our method on both video and image captioning tasks. Experimental results show that our approach achieves the state-of-the-art performance for most of the evaluation metrics on both tasks. The effect of important components is also well exploited in the ablation study.",0
"The use of an attention based encoder-decoder framework has advanced image and video captioning. However, attention mechanisms are commonly applied to all generated words, including non-visual words that can be predicted without visual signals or attention. Including attention mechanisms for non-visual words can be misleading and decrease the overall performance of visual captioning. To address this issue, we propose a hierarchical LSTM with adaptive attention (hLSTMat) approach for image and video captioning. Our framework uses spatial or temporal attention to select specific regions or frames for predicting related words and adaptive attention to decide whether to depend on visual or language context information. Additionally, our hierarchical LSTMs consider low-level visual information and high-level language context information to support caption generation. We initially designed hLSTMat for video captioning and refined it for image captioning. Our approach achieved state-of-the-art performance for most evaluation metrics on both tasks, as demonstrated through experimental results. We also conducted an ablation study to explore the effect of important components.",1
"In several natural language tasks, labeled sequences are available in separate domains (say, languages), but the goal is to label sequences with mixed domain (such as code-switched text). Or, we may have available models for labeling whole passages (say, with sentiments), which we would like to exploit toward better position-specific label inference (say, target-dependent sentiment annotation). A key characteristic shared across such tasks is that different positions in a primary instance can benefit from different `experts' trained from auxiliary data, but labeled primary instances are scarce, and labeling the best expert for each position entails unacceptable cognitive burden. We propose GITNet, a unified position-sensitive multi-task recurrent neural network (RNN) architecture for such applications. Auxiliary and primary tasks need not share training instances. Auxiliary RNNs are trained over auxiliary instances. A primary instance is also submitted to each auxiliary RNN, but their state sequences are gated and merged into a novel composite state sequence tailored to the primary inference task. Our approach is in sharp contrast to recent multi-task networks like the cross-stitch and sluice network, which do not control state transfer at such fine granularity. We demonstrate the superiority of GIRNet using three applications: sentiment classification of code-switched passages, part-of-speech tagging of code-switched text, and target position-sensitive annotation of sentiment in monolingual passages. In all cases, we establish new state-of-the-art performance beyond recent competitive baselines.",0
"There are situations where labeled sequences are available in different domains, but the goal is to label mixed domain sequences. Alternatively, models may exist for labeling entire passages, but there is a need to improve position-specific label inference. In both cases, different positions in the sequence can benefit from different experts trained from auxiliary data, but labeled primary instances are limited and labeling the best expert for each position is impractical. To address this, we propose GITNet, a unified position-sensitive multi-task RNN architecture that allows auxiliary and primary tasks to be trained separately. Auxiliary RNNs are trained over auxiliary instances, while primary instances are submitted to each auxiliary RNN, and their state sequences are gated and merged into a composite state sequence tailored to the primary inference task. Our approach is distinct from other multi-task networks that do not control state transfer at such fine granularity. We demonstrate the effectiveness of GITNet in three applications: sentiment classification of code-switched passages, part-of-speech tagging of code-switched text, and target position-sensitive annotation of sentiment in monolingual passages, where we achieve new state-of-the-art performance beyond recent baselines.",1
"The goal of this work is to recognise phrases and sentences being spoken by a talking face, with or without the audio. Unlike previous works that have focussed on recognising a limited number of words or phrases, we tackle lip reading as an open-world problem - unconstrained natural language sentences, and in the wild videos. Our key contributions are: (1) we compare two models for lip reading, one using a CTC loss, and the other using a sequence-to-sequence loss. Both models are built on top of the transformer self-attention architecture; (2) we investigate to what extent lip reading is complementary to audio speech recognition, especially when the audio signal is noisy; (3) we introduce and publicly release a new dataset for audio-visual speech recognition, LRS2-BBC, consisting of thousands of natural sentences from British television. The models that we train surpass the performance of all previous work on a lip reading benchmark dataset by a significant margin.",0
"This work aims to identify phrases and sentences spoken by a talking face, with or without accompanying audio. Unlike previous studies that have concentrated on recognizing a limited set of words or phrases, we approach lip reading as an open-world challenge, encompassing unconstrained natural language sentences and videos obtained in natural settings. Our primary contributions include: (1) comparing two lip reading models, based on the CTC loss and the sequence-to-sequence loss, both using transformer self-attention architecture; (2) exploring the complementarity of lip reading and audio speech recognition, especially in the presence of noisy audio signals; (3) presenting a new dataset for audio-visual speech recognition, LRS2-BBC, comprising thousands of natural sentences from British television, and making it publicly available. The models we train outperform all previous works on a lip reading benchmark dataset by a significant margin.",1
"Learning in environments with large state and action spaces, and sparse rewards, can hinder a Reinforcement Learning (RL) agent's learning through trial-and-error. For instance, following natural language instructions on the Web (such as booking a flight ticket) leads to RL settings where input vocabulary and number of actionable elements on a page can grow very large. Even though recent approaches improve the success rate on relatively simple environments with the help of human demonstrations to guide the exploration, they still fail in environments where the set of possible instructions can reach millions. We approach the aforementioned problems from a different perspective and propose guided RL approaches that can generate unbounded amount of experience for an agent to learn from. Instead of learning from a complicated instruction with a large vocabulary, we decompose it into multiple sub-instructions and schedule a curriculum in which an agent is tasked with a gradually increasing subset of these relatively easier sub-instructions. In addition, when the expert demonstrations are not available, we propose a novel meta-learning framework that generates new instruction following tasks and trains the agent more effectively. We train DQN, deep reinforcement learning agent, with Q-value function approximated with a novel QWeb neural network architecture on these smaller, synthetic instructions. We evaluate the ability of our agent to generalize to new instructions on World of Bits benchmark, on forms with up to 100 elements, supporting 14 million possible instructions. The QWeb agent outperforms the baseline without using any human demonstration achieving 100% success rate on several difficult environments.",0
"Reinforcement Learning (RL) agents can struggle to learn in environments with large state and action spaces, as well as sparse rewards. This is particularly evident in settings that involve following natural language instructions on the Web, such as booking a flight ticket, where the input vocabulary and actionable elements can become overwhelming. Although some recent approaches have improved success rates by using human demonstrations to guide exploration in relatively simple environments, they still fail in more complex environments where the number of possible instructions can reach millions. Our proposed approach takes a different perspective by introducing guided RL approaches that can provide an unbounded amount of experience for agents to learn from. Instead of learning from a single, complicated instruction, we break it down into multiple sub-instructions and schedule a curriculum that gradually increases in complexity. In cases where expert demonstrations are not available, we use a novel meta-learning framework that generates new instruction-following tasks and trains agents more effectively. We train a DQN agent using a QWeb neural network architecture to approximate the Q-value function on these smaller, synthetic instructions. Our agent demonstrates exceptional generalization abilities on the World of Bits benchmark, achieving a 100% success rate on several challenging environments without the use of any human demonstrations.",1
"Existing methods for interactive image retrieval have demonstrated the merit of integrating user feedback, improving retrieval results. However, most current systems rely on restricted forms of user feedback, such as binary relevance responses, or feedback based on a fixed set of relative attributes, which limits their impact. In this paper, we introduce a new approach to interactive image search that enables users to provide feedback via natural language, allowing for more natural and effective interaction. We formulate the task of dialog-based interactive image retrieval as a reinforcement learning problem, and reward the dialog system for improving the rank of the target image during each dialog turn. To mitigate the cumbersome and costly process of collecting human-machine conversations as the dialog system learns, we train our system with a user simulator, which is itself trained to describe the differences between target and candidate images. The efficacy of our approach is demonstrated in a footwear retrieval application. Experiments on both simulated and real-world data show that 1) our proposed learning framework achieves better accuracy than other supervised and reinforcement learning baselines and 2) user feedback based on natural language rather than pre-specified attributes leads to more effective retrieval results, and a more natural and expressive communication interface.",0
"Interactive image retrieval methods have shown success by integrating user feedback to improve retrieval results. However, current systems rely on limited forms of user feedback, such as binary relevance responses or feedback based on fixed attributes, which limits their effectiveness. This paper presents a new approach to interactive image search that allows users to provide feedback via natural language, resulting in more natural and effective interaction. The task of dialog-based interactive image retrieval is formulated as a reinforcement learning problem, where the dialog system is rewarded for improving the rank of the target image during each dialog turn. To overcome the challenge of gathering human-machine conversations during the dialog system's learning process, a user simulator is trained to describe the differences between target and candidate images. The effectiveness of this approach is demonstrated in a footwear retrieval application, where experiments on simulated and real-world data show that our proposed learning framework achieves better accuracy than other supervised and reinforcement learning methods. Additionally, user feedback based on natural language leads to more effective retrieval results and a more expressive communication interface.",1
"Image Captioning is a task that requires models to acquire a multi-modal understanding of the world and to express this understanding in natural language text. While the state-of-the-art for this task has rapidly improved in terms of n-gram metrics, these models tend to output the same generic captions for similar images. In this work, we address this limitation and train a model that generates more diverse and specific captions through an unsupervised training approach that incorporates a learning signal from an Image Retrieval model. We summarize previous results and improve the state-of-the-art on caption diversity and novelty. We make our source code publicly available online.",0
"The task of Image Captioning necessitates models to comprehend the world through multiple perspectives and articulate this comprehension in natural language text. Despite the remarkable advancements in n-gram metrics, the present-day models have a tendency to produce generic captions for comparable images. To overcome this obstacle, we have developed a model that generates more varied and precise captions via an unsupervised training approach that involves a learning signal from an Image Retrieval model. Our research summarizes prior outcomes and enhances the state-of-the-art in caption diversity and originality. We have released our source code online for public access.",1
"Deep Neural networks are efficient and flexible models that perform well for a variety of tasks such as image, speech recognition and natural language understanding. In particular, convolutional neural networks (CNN) generate a keen interest among researchers in computer vision and more specifically in classification tasks. CNN architecture and related hyperparameters are generally correlated to the nature of the processed task as the network extracts complex and relevant characteristics allowing the optimal convergence. Designing such architectures requires significant human expertise, substantial computation time and doesn't always lead to the optimal network. Model configuration topic has been extensively studied in machine learning without leading to a standard automatic method. This survey focuses on reviewing and discussing the current progress in automating CNN architecture search.",0
"Efficient and flexible, Deep Neural Networks are suitable for various tasks like speech recognition, image recognition, and natural language understanding. Convolutional Neural Networks (CNN) are of particular interest to computer vision researchers, specifically for classification tasks. The CNN architecture and hyperparameters are typically linked to the nature of the task as the network extracts complex and relevant features, allowing for optimal convergence. However, designing such architectures requires human expertise, computation time, and may not always result in the best network. Despite extensive research in machine learning, there is no standard automatic method for model configuration. This survey aims to review and discuss the current progress in automating CNN architecture search.",1
"Progress in image captioning is gradually getting complex as researchers try to generalized the model and define the representation between visual features and natural language processing. This work tried to define such kind of relationship in the form of representation called Tensor Product Representation (TPR) which generalized the scheme of language modeling and structuring the linguistic attributes (related to grammar and parts of speech of language) which will provide a much better structure and grammatically correct sentence. TPR enables better and unique representation and structuring of the feature space and will enable better sentence composition from these representations. A large part of the different ways of defining and improving these TPR are discussed and their performance with respect to the traditional procedures and feature representations are evaluated for image captioning application. The new models achieved considerable improvement than the corresponding previous architectures.",0
"Researchers are working to improve image captioning by developing more sophisticated models that can generalize and establish a relationship between visual features and natural language processing. One approach is to use Tensor Product Representation (TPR) to structure linguistic attributes, such as grammar and parts of speech, in order to generate grammatically correct sentences. TPR provides a better and more unique representation of the feature space, resulting in better sentence composition. This study evaluates the performance of various TPR models for image captioning, comparing them to traditional procedures and feature representations. The new models demonstrate significant improvements over previous architectures.",1
"What has happened in machine learning lately, and what does it mean for the future of medical image analysis? Machine learning has witnessed a tremendous amount of attention over the last few years. The current boom started around 2009 when so-called deep artificial neural networks began outperforming other established models on a number of important benchmarks. Deep neural networks are now the state-of-the-art machine learning models across a variety of areas, from image analysis to natural language processing, and widely deployed in academia and industry. These developments have a huge potential for medical imaging technology, medical data analysis, medical diagnostics and healthcare in general, slowly being realized. We provide a short overview of recent advances and some associated challenges in machine learning applied to medical image processing and image analysis. As this has become a very broad and fast expanding field we will not survey the entire landscape of applications, but put particular focus on deep learning in MRI.   Our aim is threefold: (i) give a brief introduction to deep learning with pointers to core references; (ii) indicate how deep learning has been applied to the entire MRI processing chain, from acquisition to image retrieval, from segmentation to disease prediction; (iii) provide a starting point for people interested in experimenting and perhaps contributing to the field of machine learning for medical imaging by pointing out good educational resources, state-of-the-art open-source code, and interesting sources of data and problems related medical imaging.",0
"In recent years, machine learning has gained significant attention, particularly due to the success of deep artificial neural networks surpassing other models on important benchmarks. These networks are now the leading machine learning models, utilized broadly in academic and industrial settings, for applications such as image analysis and natural language processing. Their potential in medical imaging technology, data analysis, diagnostics, and healthcare is progressively being realized. This article provides a brief summary of the advances and challenges in machine learning applied to medical image processing and analysis, with a focus on deep learning in MRI. The objective is to introduce deep learning, its application to the entire MRI processing chain, and to provide resources for those interested in contributing to the field of machine learning in medical imaging.",1
"Deep neural networks are the state-of-the-art methods for many real-world tasks, such as computer vision, natural language processing and speech recognition. For all its popularity, deep neural networks are also criticized for consuming a lot of memory and draining battery life of devices during training and inference. This makes it hard to deploy these models on mobile or embedded devices which have tight resource constraints. Quantization is recognized as one of the most effective approaches to satisfy the extreme memory requirements that deep neural network models demand. Instead of adopting 32-bit floating point format to represent weights, quantized representations store weights using more compact formats such as integers or even binary numbers. Despite a possible degradation in predictive performance, quantization provides a potential solution to greatly reduce the model size and the energy consumption. In this survey, we give a thorough review of different aspects of quantized neural networks. Current challenges and trends of quantized neural networks are also discussed.",0
"State-of-the-art techniques for real-world tasks, including computer vision, natural language processing and speech recognition, are based on deep neural networks. Despite their popularity, these networks are criticized for their high memory consumption and battery usage during training and inference, making them difficult to deploy on mobile or embedded devices with limited resources. To address this issue, quantization is recognized as an effective approach to satisfy the memory requirements of these models by using more compact formats such as integers or binary numbers to represent weights, thereby reducing model size and energy consumption. While there may be a decrease in predictive performance, quantization offers a potential solution to these problems. This survey provides a comprehensive overview of different aspects of quantized neural networks, including current challenges and trends.",1
"Recent advancements in the area of Computer Vision with state-of-art Neural Networks has given a boost to Optical Character Recognition (OCR) accuracies. However, extracting characters/text alone is often insufficient for relevant information extraction as documents also have a visual structure that is not captured by OCR. Extracting information from tables, charts, footnotes, boxes, headings and retrieving the corresponding structured representation for the document remains a challenge and finds application in a large number of real-world use cases. In this paper, we propose a novel enterprise based end-to-end framework called DeepReader which facilitates information extraction from document images via identification of visual entities and populating a meta relational model across different entities in the document image. The model schema allows for an easy to understand abstraction of the entities detected by the deep vision models and the relationships between them. DeepReader has a suite of state-of-the-art vision algorithms which are applied to recognize handwritten and printed text, eliminate noisy effects, identify the type of documents and detect visual entities like tables, lines and boxes. Deep Reader maps the extracted entities into a rich relational schema so as to capture all the relevant relationships between entities (words, textboxes, lines etc) detected in the document. Relevant information and fields can then be extracted from the document by writing SQL queries on top of the relationship tables. A natural language based interface is added on top of the relationship schema so that a non-technical user, specifying the queries in natural language, can fetch the information with minimal effort. In this paper, we also demonstrate many different capabilities of Deep Reader and report results on a real-world use case.",0
"Optical Character Recognition (OCR) accuracy has been enhanced by Computer Vision and state-of-the-art Neural Networks. However, OCR alone is inadequate for extracting relevant information from documents, as documents possess a visual structure that OCR does not capture. The task of extracting information from tables, charts, footnotes, boxes, headings, and obtaining the corresponding structured representation for the document remains a challenge, with numerous real-world use cases. This paper presents a novel enterprise-based end-to-end framework known as DeepReader that allows for information extraction from document images by identifying visual entities and creating a meta relational model across different entities in the document image. DeepReader comprises state-of-the-art vision algorithms that recognize handwritten and printed text, eliminate noisy effects, and identify the type of documents and visual entities like tables, lines, and boxes. DeepReader maps the extracted entities into a rich relational schema to capture all relevant relationships between entities (words, textboxes, lines, etc.) detected in the document. Relevant information and fields can then be extracted from the document by writing SQL queries on top of the relationship tables. A natural language interface is included to enable non-technical users to fetch information with minimal effort by specifying queries in natural language. This paper also showcases the various capabilities of DeepReader and reports results on a real-world use case.",1
"The task in referring expression comprehension is to localise the object instance in an image described by a referring expression phrased in natural language. As a language-to-vision matching task, the key to this problem is to learn a discriminative object feature that can adapt to the expression used. To avoid ambiguity, the expression normally tends to describe not only the properties of the referent itself, but also its relationships to its neighbourhood. To capture and exploit this important information we propose a graph-based, language-guided attention mechanism. Being composed of node attention component and edge attention component, the proposed graph attention mechanism explicitly represents inter-object relationships, and properties with a flexibility and power impossible with competing approaches. Furthermore, the proposed graph attention mechanism enables the comprehension decision to be visualisable and explainable. Experiments on three referring expression comprehension datasets show the advantage of the proposed approach.",0
"The aim of understanding referring expressions is to identify the specific object in an image that is being described using natural language. This requires finding a distinctive object feature that can match the language used. To avoid any confusion, the expression should include not only the object's properties but also its relationship with its surroundings. To capture and make use of this important information, we suggest using a graph-based, language-led attention mechanism. This mechanism comprises a node attention component and an edge attention component, which depict the inter-object relationships and properties in a more flexible and powerful way than other methods. Additionally, the proposed graph attention mechanism allows for comprehension decisions to be visualized and explained. Our experiments on three referring expression comprehension datasets demonstrate the advantages of this approach.",1
"Deep neural networks (DNNs) have witnessed as a powerful approach in this year by solving long-standing Artificial intelligence (AI) supervised and unsupervised tasks exists in natural language processing, speech processing, computer vision and others. In this paper, we attempt to apply DNNs on three different cyber security use cases: Android malware classification, incident detection and fraud detection. The data set of each use case contains real known benign and malicious activities samples. The efficient network architecture for DNN is chosen by conducting various trails of experiments for network parameters and network structures. The experiments of such chosen efficient configurations of DNNs are run up to 1000 epochs with learning rate set in the range [0.01-0.5]. Experiments of DNN performed well in comparison to the classical machine learning algorithms in all cases of experiments of cyber security use cases. This is due to the fact that DNNs implicitly extract and build better features, identifies the characteristics of the data that lead to better accuracy. The best accuracy obtained by DNN and XGBoost on Android malware classification 0.940 and 0.741, incident detection 1.00 and 0.997 fraud detection 0.972 and 0.916 respectively.",0
"This year, DNNs have proven to be a powerful tool in solving longstanding AI problems in various fields such as natural language processing, speech processing, and computer vision. In this study, we explore the application of DNNs in three different cyber security use cases: Android malware classification, incident detection, and fraud detection, each with a dataset containing real known benign and malicious activity samples. We choose an efficient network architecture for DNN by conducting various experiments on network parameters and structures, running up to 1000 epochs with a learning rate set between 0.01-0.5. The DNN experiments outperformed classical machine learning algorithms in all cyber security use cases by implicitly extracting and building better features, identifying data characteristics that improve accuracy. The DNN and XGBoost achieved the best accuracy in Android malware classification at 0.940 and 0.741, incident detection at 1.00 and 0.997, and fraud detection at 0.972 and 0.916, respectively.",1
"Phrase Grounding aims to detect and localize objects in images that are referred to and are queried by natural language phrases. Phrase grounding finds applications in tasks such as Visual Dialog, Visual Search and Image-text co-reference resolution. In this paper, we present a framework that leverages information such as phrase category, relationships among neighboring phrases in a sentence and context to improve the performance of phrase grounding systems. We propose three modules: Proposal Indexing Network(PIN); Inter-phrase Regression Network(IRN) and Proposal Ranking Network(PRN) each of which analyze the region proposals of an image at increasing levels of detail by incorporating the above information. Also, in the absence of ground-truth spatial locations of the phrases(weakly-supervised), we propose knowledge transfer mechanisms that leverages the framework of PIN module. We demonstrate the effectiveness of our approach on the Flickr 30k Entities and ReferItGame datasets, for which we achieve improvements over state-of-the-art approaches in both supervised and weakly-supervised variants.",0
"The aim of Phrase Grounding is to identify and locate objects in images that are mentioned in natural language phrases. This technique is useful for tasks like Visual Dialog, Visual Search, and Image-text co-reference resolution. Our paper presents a framework that utilizes information such as the category of the phrase, relationships between neighboring phrases, and context to enhance the performance of phrase grounding systems. We introduce three modules, namely, Proposal Indexing Network (PIN), Inter-phrase Regression Network (IRN), and Proposal Ranking Network (PRN), each of which scrutinizes the region proposals of an image in greater detail by including the aforementioned information. We also propose knowledge transfer mechanisms to aid weakly-supervised learning in the absence of spatial locations of ground-truth phrases. Our approach shows significant improvements over state-of-the-art methods in both supervised and weakly-supervised variants for the Flickr 30k Entities and ReferItGame datasets.",1
"Automatically generating a natural language sentence to describe the content of an input video is a very challenging problem. It is an essential multimodal task in which auditory and visual contents are equally important. Although audio information has been exploited to improve video captioning in previous works, it is usually regarded as an additional feature fed into a black box fusion machine. How are the words in the generated sentences associated with the auditory and visual modalities? The problem is still not investigated. In this paper, we make the first attempt to design an interpretable audio-visual video captioning network to discover the association between words in sentences and audio-visual sequences. To achieve this, we propose a multimodal convolutional neural network-based audio-visual video captioning framework and introduce a modality-aware module for exploring modality selection during sentence generation. Besides, we collect new audio captioning and visual captioning datasets for further exploring the interactions between auditory and visual modalities for high-level video understanding. Extensive experiments demonstrate that the modality-aware module makes our model interpretable on modality selection during sentence generation. Even with the added interpretability, our video captioning network can still achieve comparable performance with recent state-of-the-art methods.",0
"Generating a sentence in natural language to describe the contents of a video is a highly complex task that involves both auditory and visual elements. While previous research has utilized audio information to enhance video captioning, it has typically been viewed as an auxiliary feature incorporated into a fusion machine. The question of how the words in generated sentences correlate to the auditory and visual modalities remains unexplored. This study endeavors to address this issue by developing an interpretable audio-visual video captioning network that identifies the connection between words in sentences and audio-visual sequences. To accomplish this, a multimodal convolutional neural network-based audio-visual video captioning framework is proposed, along with a modality-aware module that investigates modality selection during sentence generation. Additionally, new audio captioning and visual captioning datasets are collected to investigate the interplay between auditory and visual modalities for high-level video comprehension. Extensive experiments indicate that our model's modality-aware module enhances interpretability during sentence generation. Despite this added interpretability, our video captioning network can still achieve comparable performance to the most recent state-of-the-art approaches.",1
"Optimal parameter initialization remains a crucial problem for neural network training. A poor weight initialization may take longer to train and/or converge to sub-optimal solutions. Here, we propose a method of weight re-initialization by repeated annealing and injection of noise in the training process. We implement this through a cyclical batch size schedule motivated by a Bayesian perspective of neural network training. We evaluate our methods through extensive experiments on tasks in language modeling, natural language inference, and image classification. We demonstrate the ability of our method to improve language modeling performance by up to 7.91 perplexity and reduce training iterations by up to $61\%$, in addition to its flexibility in enabling snapshot ensembling and use with adversarial training.",0
"Neural network training heavily relies on optimal parameter initialization. If the weight initialization is poor, the training process may take longer to converge and result in sub-optimal solutions. To address this issue, we propose a method of weight re-initialization by repeatedly annealing and injecting noise during the training process. We achieve this by implementing a cyclical batch size schedule based on a Bayesian perspective of neural network training. Our approach is evaluated through extensive experiments on language modeling, natural language inference, and image classification tasks. Our results demonstrate that our method can enhance language modeling performance by up to 7.91 perplexity and reduce training iterations by up to $61\%$. Furthermore, our approach is flexible and can be used for snapshot ensembling and adversarial training.",1
"For the task of generating complex outputs such as source code, editing existing outputs can be easier than generating complex outputs from scratch. With this motivation, we propose an approach that first retrieves a training example based on the input (e.g., natural language description) and then edits it to the desired output (e.g., code). Our contribution is a computationally efficient method for learning a retrieval model that embeds the input in a task-dependent way without relying on a hand-crafted metric or incurring the expense of jointly training the retriever with the editor. Our retrieve-and-edit framework can be applied on top of any base model. We show that on a new autocomplete task for GitHub Python code and the Hearthstone cards benchmark, retrieve-and-edit significantly boosts the performance of a vanilla sequence-to-sequence model on both tasks.",0
"To create complex outputs like source code, it may be easier to modify existing outputs rather than starting from scratch. To address this, we propose an approach that retrieves a training example based on the input, such as a natural language description, and then edits it to produce the desired output, such as code. Our contribution is an efficient method for learning a retrieval model that embeds the input in a task-specific way, without the need for a hand-crafted metric or joint training of the retriever and editor. This retrieve-and-edit framework can be used with any base model. We demonstrate that our approach significantly improves the performance of a vanilla sequence-to-sequence model on a new autocomplete task for GitHub Python code and the Hearthstone cards benchmark.",1
"Although neural networks can achieve very high predictive performance on various different tasks such as image recognition or natural language processing, they are often considered as opaque ""black boxes"". The difficulty of interpreting the predictions of a neural network often prevents its use in fields where explainability is important, such as the financial industry where regulators and auditors often insist on this aspect. In this paper, we present a way to assess the relative input features importance of a neural network based on the sensitivity of the model output with respect to its input. This method has the advantage of being fast to compute, it can provide both global and local levels of explanations and is applicable for many types of neural network architectures. We illustrate the performance of this method on both synthetic and real data and compare it with other interpretation techniques. This method is implemented into an open-source Python package that allows its users to easily generate and visualize explanations for their neural networks.",0
"Despite their ability to achieve high predictive performance in tasks like image recognition and natural language processing, neural networks are often viewed as opaque ""black boxes."" This lack of interpretability can limit their use in fields where explainability is crucial, such as finance, where regulators and auditors require transparency. This paper introduces a method for assessing the importance of input features in a neural network based on the sensitivity of the model output to its input. This approach is fast, provides both global and local levels of explanation, and can be applied to various neural network architectures. We demonstrate the effectiveness of this method on synthetic and real data and compare it to other interpretation techniques. The method is implemented in an open-source Python package that enables users to generate and visualize explanations for their neural networks with ease.",1
"Markov networks are widely used in many Machine Learning applications including natural language processing, computer vision, and bioinformatics . Learning Markov networks have many complications ranging from intractable computations involved to the possibility of learning a model with a huge number of parameters. In this report, we provide a computationally tractable greedy heuristic for learning Markov networks structure. The proposed heuristic results in a model with a limited predefined number of parameters. We ran our method on 3 fully-observed real datasets, and we observed that our method is doing comparably good to the state of the art methods.",0
"Markov networks find extensive usage in numerous Machine Learning applications such as natural language processing, bioinformatics, and computer vision. Learning these networks pose several challenges, from intricate computations to learning a model with numerous parameters. This report presents a computationally feasible and greedy heuristic to learn the structure of Markov networks, resulting in a model with a predetermined and restricted number of parameters. We tested our approach on three fully-observed real datasets and found it to perform comparably well with the current state of the art methods.",1
"In this paper, we propose an end-to-end capsule network for pixel level localization of actors and actions present in a video. The localization is performed based on a natural language query through which an actor and action are specified. We propose to encode both the video as well as textual input in the form of capsules, which provide more effective representation in comparison with standard convolution based features. We introduce a novel capsule based attention mechanism for fusion of video and text capsules for text selected video segmentation. The attention mechanism is performed via joint EM routing over video and text capsules for text selected actor and action localization. The existing works on actor-action localization are mainly focused on localization in a single frame instead of the full video. Different from existing works, we propose to perform the localization on all frames of the video. To validate the potential of the proposed network for actor and action localization on all the frames of a video, we extend an existing actor-action dataset (A2D) with annotations for all the frames. The experimental evaluation demonstrates the effectiveness of the proposed capsule network for text selective actor and action localization in videos, and it also improves upon the performance of the existing state-of-the art works on single frame-based localization.",0
"The purpose of this paper is to introduce a novel approach for accurately localizing actors and actions in videos based on a natural language query. Our proposed end-to-end capsule network uses capsules to encode the video and textual input for more effective representation. We also introduce a capsule-based attention mechanism that fuses video and text capsules for text-selected video segmentation. Unlike previous works, which focus on localization in a single frame, our approach performs localization on all frames of the video. To validate our network's potential, we extend an existing dataset with annotations for all frames and demonstrate the effectiveness of our proposed capsule network in localizing actors and actions in videos. Our approach improves upon the performance of existing state-of-the-art works on single frame-based localization.",1
"Nowadays, deep learning can be employed to a wide ranges of fields including medicine, engineering, etc. In deep learning, Convolutional Neural Network (CNN) is extensively used in the pattern and sequence recognition, video analysis, natural language processing, spam detection, topic categorization, regression analysis, speech recognition, image classification, object detection, segmentation, face recognition, robotics, and control. The benefits associated with its near human level accuracies in large applications lead to the growing acceptance of CNN in recent years. The primary contribution of this paper is to analyze the impact of the pattern of the hidden layers of a CNN over the overall performance of the network. To demonstrate this influence, we applied neural network with different layers on the Modified National Institute of Standards and Technology (MNIST) dataset. Also, is to observe the variations of accuracies of the network for various numbers of hidden layers and epochs and to make comparison and contrast among them. The system is trained utilizing stochastic gradient and backpropagation algorithm and tested with feedforward algorithm.",0
"At present, deep learning has widespread applications in a variety of fields, such as medicine and engineering. In particular, Convolutional Neural Networks (CNNs) are widely used in pattern and sequence recognition, video analysis, natural language processing, spam detection, topic categorization, regression analysis, speech recognition, image classification, object detection, segmentation, face recognition, robotics, and control, due to their near-human level accuracy and associated benefits. Therefore, the growing acceptance of CNNs in recent years is evident. This paper aims to analyze the impact of the hidden layer patterns of a CNN on its overall performance. To achieve this goal, we applied neural networks with diverse layers on the Modified National Institute of Standards and Technology (MNIST) dataset, to observe the variations in accuracy for different numbers of hidden layers and epochs, and to compare and contrast the results. The system was trained using stochastic gradient and backpropagation algorithms and evaluated using a feedforward algorithm.",1
"With the recent success of embeddings in natural language processing, research has been conducted into applying similar methods to code analysis. Most works attempt to process the code directly or use a syntactic tree representation, treating it like sentences written in a natural language. However, none of the existing methods are sufficient to comprehend program semantics robustly, due to structural features such as function calls, branching, and interchangeable order of statements. In this paper, we propose a novel processing technique to learn code semantics, and apply it to a variety of program analysis tasks. In particular, we stipulate that a robust distributional hypothesis of code applies to both human- and machine-generated programs. Following this hypothesis, we define an embedding space, inst2vec, based on an Intermediate Representation (IR) of the code that is independent of the source programming language. We provide a novel definition of contextual flow for this IR, leveraging both the underlying data- and control-flow of the program. We then analyze the embeddings qualitatively using analogies and clustering, and evaluate the learned representation on three different high-level tasks. We show that even without fine-tuning, a single RNN architecture and fixed inst2vec embeddings outperform specialized approaches for performance prediction (compute device mapping, optimal thread coarsening); and algorithm classification from raw code (104 classes), where we set a new state-of-the-art.",0
"Research has explored the use of embedding techniques from natural language processing in code analysis, but current methods fall short in comprehending program semantics due to structural features. This paper proposes a novel processing technique, using an Intermediate Representation (IR) of the code that is language-independent, to learn code semantics and apply it to program analysis tasks. The authors suggest that a distributional hypothesis of code applies to both human- and machine-generated programs and define an embedding space called inst2vec based on the IR. They introduce a definition of contextual flow that leverages data- and control-flow of the program and evaluate the embeddings using analogies and clustering. The authors demonstrate that their approach outperforms specialized approaches for performance prediction and algorithm classification from raw code, setting a new state-of-the-art.",1
"Human beings understand natural language description and could able to imagine a corresponding visual for the same. For example, given a description of the interior of a house, we could imagine its structure and arrangements of furniture. Automatic synthesis of real-world images from text descriptions has been explored in the computer vision community. However, there is no such attempt in the area of document images, like floor plans. Floor plan synthesis from sketches, as well as data-driven models, were proposed earlier. Ours is the first attempt to render building floor plan images from textual description automatically. Here, the input is a natural language description of the internal structure and furniture arrangements within a house, and the output is the 2D floor plan image of the same. We have experimented on publicly available benchmark floor plan datasets. We were able to render realistic synthesized floor plan images from the description written in English.",0
"Human beings have the ability to comprehend natural language descriptions and visualize corresponding images, such as envisioning the layout of furniture within a house based on its description. While automatic synthesis of real-world images from text descriptions has been explored by the computer vision community, there have been no attempts in the realm of document images like floor plans. Previous proposals for floor plan synthesis from sketches and data-driven models have been made, but our approach is the first to automatically generate building floor plan images from textual descriptions. By utilizing a natural language description of the internal structure and furniture arrangements of a house as input, our method produces a 2D floor plan image of the same. Our experiments on publicly available benchmark floor plan datasets demonstrate our ability to generate realistic floor plan images from English descriptions.",1
"Visual question answering (VQA) demands simultaneous comprehension of both the image visual content and natural language questions. In some cases, the reasoning needs the help of common sense or general knowledge which usually appear in the form of text. Current methods jointly embed both the visual information and the textual feature into the same space. However, how to model the complex interactions between the two different modalities is not an easy task. In contrast to struggling on multimodal feature fusion, in this paper, we propose to unify all the input information by natural language so as to convert VQA into a machine reading comprehension problem. With this transformation, our method not only can tackle VQA datasets that focus on observation based questions, but can also be naturally extended to handle knowledge-based VQA which requires to explore large-scale external knowledge base. It is a step towards being able to exploit large volumes of text and natural language processing techniques to address VQA problem. Two types of models are proposed to deal with open-ended VQA and multiple-choice VQA respectively. We evaluate our models on three VQA benchmarks. The comparable performance with the state-of-the-art demonstrates the effectiveness of the proposed method.",0
"The comprehension of both visual content and natural language questions is required for visual question answering (VQA), which can sometimes necessitate the use of common sense or general knowledge in text form. Currently, methods embed both visual and textual features into the same space but modeling the complex interactions between the two modalities is challenging. In this paper, we propose unifying all input information with natural language to convert VQA into a machine reading comprehension problem. Our method can handle observation-based questions and knowledge-based VQA, which requires exploring a large-scale knowledge base. This approach can exploit natural language processing techniques and large volumes of text to address the VQA problem. Our paper proposes two models to deal with open-ended and multiple-choice VQA, and we evaluate them on three VQA benchmarks. The proposed method is effective, as demonstrated by its comparable performance with the state-of-the-art.",1
"This paper addresses the problem of manipulating images using natural language description. Our task aims to semantically modify visual attributes of an object in an image according to the text describing the new visual appearance. Although existing methods synthesize images having new attributes, they do not fully preserve text-irrelevant contents of the original image. In this paper, we propose the text-adaptive generative adversarial network (TAGAN) to generate semantically manipulated images while preserving text-irrelevant contents. The key to our method is the text-adaptive discriminator that creates word-level local discriminators according to input text to classify fine-grained attributes independently. With this discriminator, the generator learns to generate images where only regions that correspond to the given text are modified. Experimental results show that our method outperforms existing methods on CUB and Oxford-102 datasets, and our results were mostly preferred on a user study. Extensive analysis shows that our method is able to effectively disentangle visual attributes and produce pleasing outputs.",0
"The objective of this essay is to explore the issue of modifying images using natural language descriptions. Our goal is to adjust the visual characteristics of an object in a photo based on a textual description of the new appearance, while preserving the irrelevant elements of the original image. Although current techniques can generate images with new attributes, they do not maintain the non-textual aspects of the source image. To address this issue, we introduce the text-adaptive generative adversarial network (TAGAN), which generates semantically modified images while retaining text-irrelevant details. Our method relies on a text-adaptive discriminator that creates local discriminators at the word level, based on the input text, to classify fine-grained attributes independently. As a result, the generator produces images in which only regions corresponding to the given text are modified. Our experimental results demonstrate that our method outperforms existing methods on the CUB and Oxford-102 datasets, and that users generally preferred our results in a user study. Our analysis indicates that our method can effectively disentangle visual attributes and generate aesthetically pleasing outputs.",1
"The increase in the number of Internet users and the strong interaction brought by Web 2.0 made the Opinion Mining an important task in the area of natural language processing. Although several methods are capable of performing this task, few use multi-label classification, where there is a group of true labels for each example. This type of classification is useful for situations where the opinions are analyzed from the perspective of the reader, this happens because each person can have different interpretations and opinions on the same subject. This paper discuss the efficiency of problem transformation methods combined with different classification algorithms for the task of multi-label classification of reactions in news texts. To do that, extensive tests were carried out on two news corpora written in Brazilian Portuguese annotated with reactions. A new corpus called BFRC-PT is presented. In the tests performed, the highest number of correct predictions was obtained with the Classifier Chains method combined with the Random Forest algorithm. When considering the class distribution, the best results were obtained with the Binary Relevance method combined with the LSTM and Random Forest algorithms.",0
"The importance of Opinion Mining has grown due to the increase in Internet users and the strong interaction of Web 2.0. While several methods can perform this task, few employ multi-label classification, which is useful when analyzing opinions from the reader's perspective. This is because different people can have different interpretations and opinions on the same subject. This paper examines the efficiency of problem transformation methods combined with various classification algorithms for multi-label classification of reactions in news texts. The study includes extensive tests on two news corpora written in Brazilian Portuguese and annotated with reactions. A new corpus, BFRC-PT, is presented. The Classifier Chains method combined with the Random Forest algorithm produced the highest number of correct predictions in the tests performed. However, considering class distribution, the Binary Relevance method combined with the LSTM and Random Forest algorithms yielded the best results.",1
"Convolutional Neural Networks (CNN) has become more popular choice for various tasks such as computer vision, speech recognition and natural language processing. Thanks to their large computational capability and throughput, GPUs ,which are not power efficient and therefore does not suit low power systems such as mobile devices, are the most common platform for both training and inferencing tasks. Recent studies has shown that FPGAs can provide a good alternative to GPUs as a CNN accelerator, due to their re-configurable nature, low power and small latency. In order for FPGA-based accelerators outperform GPUs in inference task, both the parameters of the network and the activations must be quantized. While most works use uniform quantizers for both parameters and activations, it is not always the optimal one, and a non-uniform quantizer need to be considered. In this work we introduce a custom hardware-friendly approach to implement non-uniform quantizers. In addition, we use a single scale integer representation of both parameters and activations, for both training and inference. The combined method yields a hardware efficient non-uniform quantizer, fit for real-time applications. We have tested our method on CIFAR-10 and CIFAR-100 image classification datasets with ResNet-18 and VGG-like architectures, and saw little degradation in accuracy.",0
"CNNs have become a popular choice for various tasks, including computer vision, speech recognition, and natural language processing. GPUs are commonly used as the platform for both training and inferencing due to their large computational capability and throughput, although they are not suitable for low power systems like mobile devices. Recent studies have shown that FPGAs can provide a good alternative to GPUs as a CNN accelerator due to their reconfigurable nature, low power, and small latency. To outperform GPUs in inference tasks, both the parameters of the network and activations must be quantized. Although most works use uniform quantizers for both parameters and activations, a non-uniform quantizer should be considered, as it is not always the optimal choice. In this work, we introduce a custom hardware-friendly approach to implement non-uniform quantizers and use a single scale integer representation of both parameters and activations for both training and inference. The combined method yields a hardware-efficient non-uniform quantizer suitable for real-time applications. We tested our method on CIFAR-10 and CIFAR-100 image classification datasets with ResNet-18 and VGG-like architectures and observed minimal degradation in accuracy.",1
"The tracking-by-detection framework requires a set of positive and negative training samples to learn robust tracking models for precise localization of target objects. However, existing tracking models mostly treat different samples independently while ignores the relationship information among them. In this paper, we propose a novel structure-aware deep neural network to overcome such limitations. In particular, we construct a graph to represent the pairwise relationships among training samples, and additionally take the natural language as the supervised information to learn both feature representations and classifiers robustly. To refine the states of the target and re-track the target when it is back to view from heavy occlusion and out of view, we elaborately design a novel subnetwork to learn the target-driven visual attentions from the guidance of both visual and natural language cues. Extensive experiments on five tracking benchmark datasets validated the effectiveness of our proposed method.",0
"The framework for tracking-by-detection necessitates having positive and negative training samples to create strong tracking models for accurately locating target objects. However, current tracking models treat samples independently and disregard their relationship information. This paper presents a unique structure-aware deep neural network to address this issue. A graph is created to depict the pairwise relationships among training samples, and natural language is utilized as supervised information to learn both feature representations and robust classifiers. A novel subnetwork is designed to refine the target states and re-track it when it comes back into view after heavy occlusion or being out of view. The subnetwork learns target-driven visual attentions from visual and natural language cues. The effectiveness of the proposed method is validated through extensive experiments on five tracking benchmark datasets.",1
"With more than 300 million people depressed worldwide, depression is a global problem. Due to access barriers such as social stigma, cost, and treatment availability, 60% of mentally-ill adults do not receive any mental health services. Effective and efficient diagnosis relies on detecting clinical symptoms of depression. Automatic detection of depressive symptoms would potentially improve diagnostic accuracy and availability, leading to faster intervention. In this work, we present a machine learning method for measuring the severity of depressive symptoms. Our multi-modal method uses 3D facial expressions and spoken language, commonly available from modern cell phones. It demonstrates an average error of 3.67 points (15.3% relative) on the clinically-validated Patient Health Questionnaire (PHQ) scale. For detecting major depressive disorder, our model demonstrates 83.3% sensitivity and 82.6% specificity. Overall, this paper shows how speech recognition, computer vision, and natural language processing can be combined to assist mental health patients and practitioners. This technology could be deployed to cell phones worldwide and facilitate low-cost universal access to mental health care.",0
"Depression is a widespread issue affecting more than 300 million people globally. However, many individuals do not receive mental health services due to obstacles such as social stigma, cost, and limited availability of treatment. To improve diagnostic accuracy and intervention speed, it is crucial to detect clinical symptoms of depression effectively. Therefore, we propose a machine learning approach that utilizes 3D facial expressions and spoken language, both readily available on modern cell phones, to measure depression severity. Our method demonstrates a 3.67-point average error (15.3% relative) on the clinically-validated Patient Health Questionnaire scale and an 83.3% sensitivity and 82.6% specificity for detecting major depressive disorder. By combining speech recognition, computer vision, and natural language processing, this technology has the potential to aid mental health patients and practitioners worldwide, providing low-cost universal access to mental health care.",1
"Coronary Artery Disease (CAD) is one of the leading causes of death worldwide, and so it is very important to correctly diagnose patients with the disease. For medical diagnosis, machine learning is a useful tool, however features and algorithms must be carefully selected to get accurate classification. To this effect, three feature selection methods have been used on 13 input features from the Cleveland dataset with 297 entries, and 7 were selected. The selected features were used to train three different classifiers, which are SVM, Na\""ive Bayes and KNN using 10-fold cross-validation. The resulting models evaluated using Accuracy, Recall, Specificity and Precision. It is found that the Na\""ive Bayes classifier performs the best on this dataset and features, outperforming or matching SVM and KNN in all the four evaluation parameters used and achieving an accuracy of 84%.",0
"Accurately diagnosing patients with Coronary Artery Disease (CAD) is crucial, as it ranks among the top causes of death globally. Machine learning can aid medical diagnosis, but precise selection of features and algorithms is essential to ensure accurate classification. In this study, three feature selection methods were applied to 13 input features from the Cleveland dataset (297 entries), resulting in the selection of seven features. These features were used to train three classifiers (SVM, Na\""ive Bayes, and KNN), which were evaluated using Accuracy, Recall, Specificity, and Precision through 10-fold cross-validation. The Na\""ive Bayes classifier demonstrated superior performance, achieving an accuracy of 84% and outperforming SVM and KNN in all four evaluation parameters.",1
"We give an overview of recent exciting achievements of deep reinforcement learning (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value function, in particular, Deep Q-Network (DQN), policy, reward, model, planning, and exploration. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, neural architecture design, business management, finance, healthcare, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions.   Please see Deep Reinforcement Learning, arXiv:1810.06339, for a significant update.",0
"In this article, we provide an overview of recent advancements in deep reinforcement learning (RL). We cover six core elements, six important mechanisms, and twelve applications. Our discussion begins with an introduction to machine learning, deep learning, and reinforcement learning. We then delve into the six core elements of RL, which include value function, specifically Deep Q-Network (DQN), policy, reward, model, planning, and exploration. Following this, we explore the important mechanisms utilized in RL, such as attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. Furthermore, we examine various RL applications, including games like AlphaGo, robotics, natural language processing (e.g. dialogue systems, machine translation, and text generation), computer vision, neural architecture design, business management, finance, healthcare, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We also provide a collection of RL resources and briefly mention topics not covered in this article. Finally, we summarize our findings and conclude with discussions. For more up-to-date information, please refer to the article titled ""Deep Reinforcement Learning"" on arXiv:1810.06339.",1
"We show that correspondence analysis (CA) is equivalent to defining a Gini index with appropriately scaled one-hot encoding. Using this relation, we introduce a nonlinear kernel extension to CA. This extended CA gives a known analysis for natural language via specialized kernels that use an appropriate contingency table. We propose a semi-supervised CA, which is a special case of the kernel extension to CA. Because CA requires excessive memory if applied to numerous categories, CA has not been used for natural language processing. We address this problem by introducing delayed evaluation to randomized singular value decomposition. The memory-efficient CA is then applied to a word-vector representation task. We propose a tail-cut kernel, which is an extension to the skip-gram within the kernel extension to CA. Our tail-cut kernel outperforms existing word-vector representation methods.",0
"The equivalence of defining a Gini index with appropriately scaled one-hot encoding to correspondence analysis (CA) is demonstrated in our study. We also introduce a nonlinear kernel extension to CA using this relationship. The extended CA can be used for natural language analysis with specialized kernels that use an appropriate contingency table. To make CA memory-efficient for word-vector representation tasks, we propose a semi-supervised CA with delayed evaluation to randomized singular value decomposition. This addresses the problem of excessive memory requirements when CA is applied to numerous categories. Additionally, we propose a tail-cut kernel, which is an extension to the skip-gram within the kernel extension to CA. Our tail-cut kernel performs better than existing word-vector representation methods.",1
"There are two major approaches for sequence labeling. One is the probabilistic gradient-based methods such as conditional random fields (CRF) and neural networks (e.g., RNN), which have high accuracy but drawbacks: slow training, and no support of search-based optimization (which is important in many cases). The other is the search-based learning methods such as structured perceptron and margin infused relaxed algorithm (MIRA), which have fast training but also drawbacks: low accuracy, no probabilistic information, and non-convergence in real-world tasks. We propose a novel and ""easy"" solution, a search-based probabilistic online learning method, to address most of those issues. The method is ""easy"", because the optimization algorithm at the training stage is as simple as the decoding algorithm at the test stage. This method searches the output candidates, derives probabilities, and conducts efficient online learning. We show that this method with fast training and theoretical guarantee of convergence, which is easy to implement, can support search-based optimization and obtain top accuracy. Experiments on well-known tasks show that our method has better accuracy than CRF and BiLSTM\footnote{The SAPO code is released at \url{https://github.com/lancopku/SAPO}.}.",0
"Sequence labeling can be approached in two major ways. The first is probabilistic gradient-based methods, such as conditional random fields (CRF) and neural networks like RNN. While these methods are highly accurate, they have drawbacks such as slow training and no support for search-based optimization, which is important in many cases. The second approach is search-based learning methods, including structured perceptron and margin infused relaxed algorithm (MIRA), which have fast training but low accuracy, no probabilistic information, and non-convergence in real-world tasks. To address these issues, we propose a novel search-based probabilistic online learning method that is ""easy"" to implement. Our method searches output candidates, derives probabilities, and conducts efficient online learning. It has fast training, theoretical guarantee of convergence, and supports search-based optimization to obtain top accuracy. Experiments on well-known tasks show that our method outperforms CRF and BiLSTM. The SAPO code is available at https://github.com/lancopku/SAPO.",1
"There has been a drastic growth of research in Generative Adversarial Nets (GANs) in the past few years. Proposed in 2014, GAN has been applied to various applications such as computer vision and natural language processing, and achieves impressive performance. Among the many applications of GAN, image synthesis is the most well-studied one, and research in this area has already demonstrated the great potential of using GAN in image synthesis. In this paper, we provide a taxonomy of methods used in image synthesis, review different models for text-to-image synthesis and image-to-image translation, and discuss some evaluation metrics as well as possible future research directions in image synthesis with GAN.",0
"In recent years, there has been a significant increase in research on Generative Adversarial Nets (GANs). GAN was introduced in 2014 and has been utilized in multiple fields, such as computer vision and natural language processing, with impressive results. Image synthesis is one of the most heavily researched applications of GAN, and studies in this area have already demonstrated its potential. This paper presents a classification of approaches used in image synthesis, examines various models for text-to-image synthesis and image-to-image translation, and explores evaluation metrics and potential future research areas for GAN-based image synthesis.",1
"Localizing natural language phrases in images is a challenging problem that requires joint understanding of both the textual and visual modalities. In the unsupervised setting, lack of supervisory signals exacerbate this difficulty. In this paper, we propose a novel framework for unsupervised visual grounding which uses concept learning as a proxy task to obtain self-supervision. The simple intuition behind this idea is to encourage the model to localize to regions which can explain some semantic property in the data, in our case, the property being the presence of a concept in a set of images. We present thorough quantitative and qualitative experiments to demonstrate the efficacy of our approach and show a 5.6% improvement over the current state of the art on Visual Genome dataset, a 5.8% improvement on the ReferItGame dataset and comparable to state-of-art performance on the Flickr30k dataset.",0
"The task of localizing natural language phrases in images is a difficult one that requires a comprehensive understanding of both the textual and visual aspects. This challenge is exacerbated in unsupervised settings where there are no supervisory signals to aid in the process. To address this issue, we propose a new framework for unsupervised visual grounding that employs concept learning as a proxy task to achieve self-supervision. Essentially, our approach encourages the model to locate regions that can account for some semantic feature in the data, namely the existence of a concept in a set of images. Through extensive quantitative and qualitative experiments, we demonstrate the effectiveness of our method, which outperforms the current state-of-the-art by 5.6% on the Visual Genome dataset and 5.8% on the ReferItGame dataset, and achieves comparable results to the state-of-the-art on the Flickr30k dataset.",1
"Recent successes and advances in Deep Neural Networks (DNN) in machine vision and Natural Language Processing (NLP) have motivated their use in traditional signal processing and communications systems. In this paper, we present results of such applications to the problem of automatic modulation recognition. Variations in wireless communication channels are represented by statistical channel models and their parameterization will increase with the advent of 5G. In this paper, we report effect of simple two path channel model on our naive deep neural network based implementation. We also report impact of adversarial perturbation to the input signal.",0
"The success and progress of Deep Neural Networks (DNN) in machine vision and Natural Language Processing (NLP) have sparked interest in their implementation in conventional signal processing and communications systems. The objective of this study is to demonstrate the application of DNN to the issue of automatic modulation recognition. Statistical channel models are used to represent variations in wireless communication channels, with increasing parameterization expected in the wake of 5G. The study evaluates the effect of a basic two-path channel model on a naive DNN-based implementation, as well as the impact of adversarial perturbation on the input signal.",1
"Code generation maps a program description to executable source code in a programming language. Existing approaches mainly rely on a recurrent neural network (RNN) as the decoder. However, we find that a program contains significantly more tokens than a natural language sentence, and thus it may be inappropriate for RNN to capture such a long sequence. In this paper, we propose a grammar-based structural convolutional neural network (CNN) for code generation. Our model generates a program by predicting the grammar rules of the programming language; we design several CNN modules, including the tree-based convolution and pre-order convolution, whose information is further aggregated by dedicated attentive pooling layers. Experimental results on the HearthStone benchmark dataset show that our CNN code generator significantly outperforms the previous state-of-the-art method by 5 percentage points; additional experiments on several semantic parsing tasks demonstrate the robustness of our model. We also conduct in-depth ablation test to better understand each component of our model.",0
"The process of code generation involves transforming a program description into executable source code using a programming language. Currently, the prevalent method utilizes a recurrent neural network (RNN) as the decoder. However, due to the substantial number of tokens present in a program compared to a natural language sentence, RNNs may not be suitable for capturing such lengthy sequences. In this study, we propose a grammar-based structural convolutional neural network (CNN) for code generation. Our model predicts the programming language's grammar rules to generate a program, and we incorporate several CNN modules, including the tree-based convolution and pre-order convolution. These modules' information is then combined using designated attentive pooling layers. Our experiments on the HearthStone benchmark dataset demonstrate that our CNN code generator outperforms the previous state-of-the-art method by 5 percentage points. We also conducted additional experiments on several semantic parsing tasks to establish our model's robustness. To further understand each component's contribution to our model, we conducted an in-depth ablation test.",1
"In this paper we consider the binary similarity problem that consists in determining if two binary functions are similar only considering their compiled form. This problem is know to be crucial in several application scenarios, such as copyright disputes, malware analysis, vulnerability detection, etc. The current state-of-the-art solutions in this field work by creating an embedding model that maps binary functions into vectors in $\mathbb{R}^{n}$. Such embedding model captures syntactic and semantic similarity between binaries, i.e., similar binary functions are mapped to points that are close in the vector space. This strategy has many advantages, one of them is the possibility to precompute embeddings of several binary functions, and then compare them with simple geometric operations (e.g., dot product). In [32] functions are first transformed in Annotated Control Flow Graphs (ACFGs) constituted by manually engineered features and then graphs are embedded into vectors using a deep neural network architecture. In this paper we propose and test several ways to compute annotated control flow graphs that use unsupervised approaches for feature learning, without incurring a human bias. Our methods are inspired after techniques used in the natural language processing community (e.g., we use word2vec to encode assembly instructions). We show that our approach is indeed successful, and it leads to better performance than previous state-of-the-art solutions. Furthermore, we report on a qualitative analysis of functions embeddings. We found interesting cases in which embeddings are clustered according to the semantic of the original binary function.",0
"This paper focuses on the binary similarity problem, which involves determining whether two binary functions are similar based solely on their compiled form. This problem is crucial in various scenarios, such as copyright disputes, malware analysis, and vulnerability detection. Current solutions involve creating an embedding model that maps binary functions into vectors in $\mathbb{R}^{n}$, capturing syntactic and semantic similarity between binaries. This approach allows for precomputed embeddings to be compared using simple geometric operations. Previous work transformed functions into Annotated Control Flow Graphs (ACFGs) with manually engineered features, which were then embedded using a deep neural network. In this paper, we propose and test unsupervised approaches for feature learning to compute ACFGs, inspired by natural language processing techniques such as word2vec for encoding assembly instructions. Our approach outperformed previous state-of-the-art solutions and resulted in embeddings clustered based on the original binary function's semantic. We also provide a qualitative analysis of the embeddings.",1
"Deep learning (DL) has recently achieved tremendous success in a variety of cutting-edge applications, e.g., image recognition, speech and natural language processing, and autonomous driving. Besides the available big data and hardware evolution, DL frameworks and platforms play a key role to catalyze the research, development, and deployment of DL intelligent solutions. However, the difference in computation paradigm, architecture design and implementation of existing DL frameworks and platforms brings challenges for DL software development, deployment, maintenance, and migration. Up to the present, it still lacks a comprehensive study on how current diverse DL frameworks and platforms influence the DL software development process.   In this paper, we initiate the first step towards the investigation on how existing state-of-the-art DL frameworks (i.e., TensorFlow, Theano, and Torch) and platforms (i.e., server/desktop, web, and mobile) support the DL software development activities. We perform an in-depth and comparative evaluation on metrics such as learning accuracy, DL model size, robustness, and performance, on state-of-the-art DL frameworks across platforms using two popular datasets MNIST and CIFAR-10. Our study reveals that existing DL frameworks still suffer from compatibility issues, which becomes even more severe when it comes to different platforms. We pinpoint the current challenges and opportunities towards developing high quality and compatible DL systems. To ignite further investigation along this direction to address urgent industrial demands of intelligent solutions, we make all of our assembled feasible toolchain and dataset publicly available.",0
"DL has achieved significant success in various advanced applications such as image recognition, natural language processing, and autonomous driving. DL frameworks and platforms have played a crucial role in promoting research, development, and deployment of DL intelligent solutions, along with the availability of big data and hardware advancements. However, existing DL frameworks and platforms differ in computation paradigm, architecture design, and implementation, posing challenges for DL software development, deployment, maintenance, and migration. Currently, there is no comprehensive study on how diverse DL frameworks and platforms affect the DL software development process. This paper takes the first step in investigating how state-of-the-art DL frameworks (TensorFlow, Theano, and Torch) and platforms (server/desktop, web, and mobile) support DL software development activities. The study compares learning accuracy, DL model size, robustness, and performance metrics on these frameworks and platforms using two popular datasets (MNIST and CIFAR-10). The study reveals compatibility issues with existing DL frameworks, especially when applied to different platforms. The study identifies current challenges and opportunities towards developing high-quality and compatible DL systems. To encourage further research in this area to address industrial demands for intelligent solutions, the paper makes all feasible toolchain and dataset publicly available.",1
"Neural Network is a powerful Machine Learning tool that shows outstanding performance in Computer Vision, Natural Language Processing, and Artificial Intelligence. In particular, recently proposed ResNet architecture and its modifications produce state-of-the-art results in image classification problems. ResNet and most of the previously proposed architectures have a fixed structure and apply the same transformation to all input images. In this work, we develop a ResNet-based model that dynamically selects Computational Units (CU) for each input object from a learned set of transformations. Dynamic selection allows the network to learn a sequence of useful transformations and apply only required units to predict the image label. We compare our model to ResNet-38 architecture and achieve better results than the original ResNet on CIFAR-10.1 test set. While examining the produced paths, we discovered that the network learned different routes for images from different classes and similar routes for similar images.",0
"The Neural Network is a remarkable tool for Machine Learning, with exceptional capabilities in Artificial Intelligence, Natural Language Processing, and Computer Vision. The ResNet design, which has been recently introduced, along with its modifications, have demonstrated their effectiveness in image classification problems, producing top-notch outcomes. Nevertheless, ResNet and the other existing architectures follow a fixed structure and execute the same transformation for all input images. In this study, we have created a ResNet-based model that can dynamically choose Computational Units (CU) for each input object from a set of learned transformations. This dynamic selection enables the network to learn useful transformations in a sequence and use only the necessary units to forecast the image label accurately. Our model has been compared to the ResNet-38 architecture, and we have achieved better results than the original ResNet on the CIFAR-10.1 test set. During our examination, we have discovered that the network has learned distinct paths for images from various classes and similar paths for comparable images.",1
"While it has become common to perform automated translations on natural language, performing translations between different representations of mathematical formulae has thus far not been possible. We implemented the first translator for mathematical formulae based on recursive neural networks. We chose recursive neural networks because mathematical formulae inherently include a structural encoding. In our implementation, we developed new techniques and topologies for recursive tree-to-tree neural networks based on multi-variate multi-valued Long Short-Term Memory cells. We propose a novel approach for mini-batch training that utilizes clustering and tree traversal. We evaluate our translator and analyze the behavior of our proposed topologies and techniques based on a translation from generic LaTeX to the semantic LaTeX notation. We use the semantic LaTeX notation from the Digital Library for Mathematical Formulae and the Digital Repository for Mathematical Formulae at the National Institute for Standards and Technology. We find that a simple heuristics-based clustering algorithm outperforms the conventional clustering algorithms on the task of clustering binary trees of mathematical formulae with respect to their topology. Furthermore, we find a mask for the loss function, which can prevent the neural network from finding a local minimum of the loss function. Given our preliminary results, a complete translation from formula to formula is not yet possible. However, we achieved a prediction accuracy of 47.05% for predicting symbols at the correct position and an accuracy of 92.3% when ignoring the predicted position. Concluding, our work advances the field of recursive neural networks by improving the training speed and quality of training. In the future, we will work towards a complete translation allowing a machine-interpretation of LaTeX formulae.",0
"Although automated translations of natural language have become commonplace, translating mathematical formulae between different representations has not been achievable until now. Our team has developed the first mathematical formula translator using recursive neural networks, as these networks can encode structure, which is crucial in mathematical formulae. Our implementation uses new techniques and topologies for recursive tree-to-tree neural networks based on multi-variate multi-valued Long Short-Term Memory cells. We propose a novel mini-batch training approach utilizing clustering and tree traversal, which we evaluate by translating generic LaTeX to semantic LaTeX notation. We analyze our translator's behavior and find that a simple heuristics-based clustering algorithm outperforms conventional clustering algorithms for clustering binary trees of mathematical formulae regarding their topology. We also discover a mask for the loss function that prevents the neural network from finding a local minimum of the loss function. Although we cannot yet achieve complete formula-to-formula translation, our work advances the field of recursive neural networks by improving training speed and quality. Our future goal is to enable machine-interpretation of LaTeX formulae by developing a complete translation. Our preliminary results show a prediction accuracy of 47.05% for predicting symbols at the correct position and an accuracy of 92.3% when ignoring the predicted position.",1
"Acute kidney injury (AKI) in critically ill patients is associated with significant morbidity and mortality. Development of novel methods to identify patients with AKI earlier will allow for testing of novel strategies to prevent or reduce the complications of AKI. We developed data-driven prediction models to estimate the risk of new AKI onset. We generated models from clinical notes within the first 24 hours following intensive care unit (ICU) admission extracted from Medical Information Mart for Intensive Care III (MIMIC-III). From the clinical notes, we generated clinically meaningful word and concept representations and embeddings, respectively. Five supervised learning classifiers and knowledge-guided deep learning architecture were used to construct prediction models. The best configuration yielded a competitive AUC of 0.779. Our work suggests that natural language processing of clinical notes can be applied to assist clinicians in identifying the risk of incident AKI onset in critically ill patients upon admission to the ICU.",0
"Critically ill patients who suffer from Acute Kidney Injury (AKI) face serious health risks and an increased chance of mortality. To combat this issue, it is crucial to develop new methods that can identify AKI in patients earlier, allowing for the testing of novel preventative strategies. Our team has developed data-driven prediction models that can estimate the risk of AKI onset in patients. These models were created using clinical notes obtained within the first 24 hours of treatment in the intensive care unit (ICU) from the Medical Information Mart for Intensive Care III (MIMIC-III). Using clinically meaningful word and concept representations and embeddings, we employed five supervised learning classifiers and a knowledge-guided deep learning architecture to construct the prediction models. Our best configuration yielded an AUC of 0.779, proving the efficacy of our approach. This study demonstrates that natural language processing can significantly assist clinicians in identifying the risk of incident AKI onset in critically ill patients upon admission to the ICU.",1
"Volatility is a quantity of measurement for the price movements of stocks or options which indicates the uncertainty within financial markets. As an indicator of the level of risk or the degree of variation, volatility is important to analyse the financial market, and it is taken into consideration in various decision-making processes in financial activities. On the other hand, recent advancement in deep learning techniques has shown strong capabilities in modelling sequential data, such as speech and natural language. In this paper, we empirically study the applicability of the latest deep structures with respect to the volatility modelling problem, through which we aim to provide an empirical guidance for the theoretical analysis of the marriage between deep learning techniques and financial applications in the future. We examine both the traditional approaches and the deep sequential models on the task of volatility prediction, including the most recent variants of convolutional and recurrent networks, such as the dilated architecture. Accordingly, experiments with real-world stock price datasets are performed on a set of 1314 daily stock series for 2018 days of transaction. The evaluation and comparison are based on the negative log likelihood (NLL) of real-world stock price time series. The result shows that the dilated neural models, including dilated CNN and Dilated RNN, produce most accurate estimation and prediction, outperforming various widely-used deterministic models in the GARCH family and several recently proposed stochastic models. In addition, the high flexibility and rich expressive power are validated in this study.",0
"The uncertainty that exists in financial markets can be measured through volatility, which indicates the level of risk or variation in the price movements of stocks or options. Understanding volatility is crucial for analyzing financial markets and making important decisions in financial activities. Recently, deep learning techniques have demonstrated impressive abilities in modeling sequential data, such as speech and natural language. This study aims to explore the potential of these techniques in the context of volatility modeling. We compare traditional approaches with deep sequential models, including variants of convolutional and recurrent networks like the dilated architecture, using real-world stock price datasets. Our evaluation and comparison are based on the negative log likelihood (NLL) of real-world stock price time series. The results demonstrate that dilated neural models, including dilated CNN and Dilated RNN, provide the most accurate estimation and prediction, outperforming various widely-used deterministic models in the GARCH family and several recently proposed stochastic models. This study validates the high flexibility and rich expressive power of deep learning techniques in this context and provides empirical guidance for future theoretical analyses of their potential in financial applications.",1
"Neural attention has become central to many state-of-the-art models in natural language processing and related domains. Attention networks are an easy-to-train and effective method for softly simulating alignment; however, the approach does not marginalize over latent alignments in a probabilistic sense. This property makes it difficult to compare attention to other alignment approaches, to compose it with probabilistic models, and to perform posterior inference conditioned on observed data. A related latent approach, hard attention, fixes these issues, but is generally harder to train and less accurate. This work considers variational attention networks, alternatives to soft and hard attention for learning latent variable alignment models, with tighter approximation bounds based on amortized variational inference. We further propose methods for reducing the variance of gradients to make these approaches computationally feasible. Experiments show that for machine translation and visual question answering, inefficient exact latent variable models outperform standard neural attention, but these gains go away when using hard attention based training. On the other hand, variational attention retains most of the performance gain but with training speed comparable to neural attention.",0
"Many cutting-edge models in natural language processing and related fields rely heavily on neural attention. While attention networks are a simple and effective way to simulate alignment, they lack the ability to marginalize over latent alignments in a probabilistic manner. This limitation makes it challenging to compare attention to other alignment approaches, integrate it with probabilistic models, and perform posterior inference based on observed data. Hard attention is a related approach that solves these issues, but it is more difficult to train and less accurate. This study introduces variational attention networks, which are alternative methods for learning latent variable alignment models that provide tighter approximation bounds through amortized variational inference. The study also proposes approaches to decrease the variance of gradients to make these methods computationally feasible. The results of experiments demonstrate that, for machine translation and visual question answering, inefficient exact latent variable models outperform standard neural attention, but this advantage disappears when using hard attention-based training. However, variational attention maintains most of the performance gain while having training speed comparable to neural attention.",1
"This paper introduces a novel method for the representation of images that is semantic by nature, addressing the question of computation intelligibility in computer vision tasks. More specifically, our proposition is to introduce what we call a semantic bottleneck in the processing pipeline, which is a crossing point in which the representation of the image is entirely expressed with natural language , while retaining the efficiency of numerical representations. We show that our approach is able to generate semantic representations that give state-of-the-art results on semantic content-based image retrieval and also perform very well on image classification tasks. Intelligibility is evaluated through user centered experiments for failure detection.",0
"In this paper, a new approach for representing images that is inherently semantic is presented, aimed at enhancing comprehension in computer vision applications. The proposed method involves the introduction of a ""semantic bottleneck"" in the processing pipeline, which serves as a point of intersection where the image representation is conveyed entirely through natural language, while maintaining the efficiency of numerical representations. It is demonstrated that this approach can generate semantic representations that achieve top-notch outcomes in semantic content-based image retrieval and also excel in image classification tasks. The intelligibility of the method is evaluated by means of user-centered experiments for failure detection.",1
"Adversarial examples can be defined as inputs to a model which induce a mistake - where the model output is different than that of an oracle, perhaps in surprising or malicious ways. Original models of adversarial attacks are primarily studied in the context of classification and computer vision tasks. While several attacks have been proposed in natural language processing (NLP) settings, they often vary in defining the parameters of an attack and what a successful attack would look like. The goal of this work is to propose a unifying model of adversarial examples suitable for NLP tasks in both generative and classification settings. We define the notion of adversarial gain: based in control theory, it is a measure of the change in the output of a system relative to the perturbation of the input (caused by the so-called adversary) presented to the learner. This definition, as we show, can be used under different feature spaces and distance conditions to determine attack or defense effectiveness across different intuitive manifolds. This notion of adversarial gain not only provides a useful way for evaluating adversaries and defenses, but can act as a building block for future work in robustness under adversaries due to its rooted nature in stability and manifold theory.",0
"Adversarial examples refer to inputs that cause a model to make a mistake, resulting in an output that differs from an oracle's in unexpected or harmful ways. While original models of adversarial attacks are mainly studied in computer vision and classification tasks, attacks in natural language processing (NLP) settings vary in their attack parameters and success criteria. The aim of this study is to introduce a unified model of adversarial examples that applies to both NLP classification and generative tasks. We propose a concept called adversarial gain, which measures the change in a system's output relative to the input perturbation caused by an adversary. This definition can be used across different feature spaces and distance conditions to assess the effectiveness of attacks and defenses on different intuitive manifolds. Adversarial gain is rooted in stability and manifold theory, making it a valuable tool for evaluating adversaries and defenses and a foundation for future research on robustness under adversaries.",1
"Fake news are nowadays an issue of pressing concern, given their recent rise as a potential threat to high-quality journalism and well-informed public discourse. The Fake News Challenge (FNC-1) was organized in 2017 to encourage the development of machine learning-based classification systems for stance detection (i.e., for identifying whether a particular news article agrees, disagrees, discusses, or is unrelated to a particular news headline), thus helping in the detection and analysis of possible instances of fake news. This article presents a new approach to tackle this stance detection problem, based on the combination of string similarity features with a deep neural architecture that leverages ideas previously advanced in the context of learning efficient text representations, document classification, and natural language inference. Specifically, we use bi-directional Recurrent Neural Networks, together with max-pooling over the temporal/sequential dimension and neural attention, for representing (i) the headline, (ii) the first two sentences of the news article, and (iii) the entire news article. These representations are then combined/compared, complemented with similarity features inspired on other FNC-1 approaches, and passed to a final layer that predicts the stance of the article towards the headline. We also explore the use of external sources of information, specifically large datasets of sentence pairs originally proposed for training and evaluating natural language inference methods, in order to pre-train specific components of the neural network architecture (e.g., the RNNs used for encoding sentences). The obtained results attest to the effectiveness of the proposed ideas and show that our model, particularly when considering pre-training and the combination of neural representations together with similarity features, slightly outperforms the previous state-of-the-art.",0
"The rise of fake news has become a serious concern in recent times, posing a threat to credible journalism and informed public discourse. In response, the Fake News Challenge (FNC-1) was established in 2017 to promote the development of machine learning-based classification systems for stance detection. This involves identifying whether a news article agrees, disagrees, discusses or is unrelated to a particular news headline. This article introduces a novel approach to tackle stance detection by combining string similarity features with a deep neural architecture that leverages ideas previously applied in learning efficient text representations, document classification, and natural language inference. The approach employs bi-directional Recurrent Neural Networks, max-pooling over the temporal/sequential dimension, and neural attention to represent the headline, the first two sentences of the news article, and the entire news article. These representations are merged with similarity features inspired by other FNC-1 approaches and fed to a final layer that predicts the article's stance towards the headline. The study also explores the use of external sources of information, such as large datasets of sentence pairs, to pre-train specific components of the neural network architecture. The results demonstrate the effectiveness of the proposed ideas and indicate that the model outperforms the previous state-of-the-art, particularly when pre-training and combining neural representations with similarity features.",1
"Deep learning models have become state of the art for natural language processing (NLP) tasks, however deploying these models in production system poses significant memory constraints. Existing compression methods are either lossy or introduce significant latency. We propose a compression method that leverages low rank matrix factorization during training,to compress the word embedding layer which represents the size bottleneck for most NLP models. Our models are trained, compressed and then further re-trained on the downstream task to recover accuracy while maintaining the reduced size. Empirically, we show that the proposed method can achieve 90% compression with minimal impact in accuracy for sentence classification tasks, and outperforms alternative methods like fixed-point quantization or offline word embedding compression. We also analyze the inference time and storage space for our method through FLOP calculations, showing that we can compress DNN models by a configurable ratio and regain accuracy loss without introducing additional latency compared to fixed point quantization. Finally, we introduce a novel learning rate schedule, the Cyclically Annealed Learning Rate (CALR), which we empirically demonstrate to outperform other popular adaptive learning rate algorithms on a sentence classification benchmark.",0
"Although deep learning models have proven to be highly effective for natural language processing (NLP) tasks, their implementation in production systems is restricted by significant memory constraints. The current compression techniques available either cause loss of data or introduce substantial latency. As a solution, we propose a compression method that utilizes low rank matrix factorization during training to compress the word embedding layer, which is typically the bottleneck for most NLP models. Our models are trained, compressed, and then re-trained on downstream tasks to regain accuracy while retaining the reduced size. Our empirical data shows that our proposed method can compress models by 90% with minimal impact on accuracy for sentence classification tasks, outperforming alternative methods such as fixed-point quantization or offline word embedding compression. We also evaluate our method's inference time and storage space through FLOP calculations and demonstrate that we can compress DNN models while regaining accuracy loss without introducing additional latency compared to fixed point quantization. Finally, we introduce a novel learning rate schedule, the Cyclically Annealed Learning Rate (CALR), which we show through empirical evidence to be superior to other popular adaptive learning rate algorithms on a sentence classification benchmark.",1
"Visual Question answering is a challenging problem requiring a combination of concepts from Computer Vision and Natural Language Processing. Most existing approaches use a two streams strategy, computing image and question features that are consequently merged using a variety of techniques. Nonetheless, very few rely on higher level image representations, which can capture semantic and spatial relationships. In this paper, we propose a novel graph-based approach for Visual Question Answering. Our method combines a graph learner module, which learns a question specific graph representation of the input image, with the recent concept of graph convolutions, aiming to learn image representations that capture question specific interactions. We test our approach on the VQA v2 dataset using a simple baseline architecture enhanced by the proposed graph learner module. We obtain promising results with 66.18% accuracy and demonstrate the interpretability of the proposed method. Code can be found at github.com/aimbrain/vqa-project.",0
"Visual Question Answering is a complex issue that requires a blend of concepts from Computer Vision and Natural Language Processing. Many existing methods use a two-stream approach to compute image and question features, which are then merged using various techniques. However, there are only a few methods that rely on higher-level image representations that can capture semantic and spatial relationships. This study introduces a new graph-based approach for Visual Question Answering, which combines a graph learner module with graph convolutions to learn image representations that capture question-specific interactions. The proposed method is tested on the VQA v2 dataset using a simple baseline architecture and achieves promising results with 66.18% accuracy. The method is also shown to be interpretable. The code for this study can be found at github.com/aimbrain/vqa-project.",1
"Recurrent Neural Network (RNN) has been successfully applied in many sequence learning problems. Such as handwriting recognition, image description, natural language processing and video motion analysis. After years of development, researchers have improved the internal structure of the RNN and introduced many variants. Among others, Gated Recurrent Unit (GRU) is one of the most widely used RNN model. However, GRU lacks the capability of adaptively paying attention to certain regions or locations, so that it may cause information redundancy or loss during leaning. In this paper, we propose a RNN model, called Recurrent Attention Unit (RAU), which seamlessly integrates the attention mechanism into the interior of GRU by adding an attention gate. The attention gate can enhance GRU's ability to remember long-term memory and help memory cells quickly discard unimportant content. RAU is capable of extracting information from the sequential data by adaptively selecting a sequence of regions or locations and pay more attention to the selected regions during learning. Extensive experiments on image classification, sentiment classification and language modeling show that RAU consistently outperforms GRU and other baseline methods.",0
"Numerous applications have utilized Recurrent Neural Network (RNN) to solve sequence learning problems such as image description, natural language processing, handwriting recognition, and video motion analysis. Over time, researchers have improved the RNN's internal structure and introduced various models, including the widely used Gated Recurrent Unit (GRU). However, GRU lacks the ability to selectively pay attention to specific regions or locations, leading to information redundancy or loss while learning. To address this issue, we present the Recurrent Attention Unit (RAU), which integrates the attention mechanism into GRU's interior by adding an attention gate. The attention gate boosts GRU's capacity to retain long-term memory while quickly discarding irrelevant content. RAU selectively extracts information from sequential data by focusing on specific regions, resulting in superior performance compared to GRU and other baseline methods, as demonstrated in extensive experiments on sentiment classification, language modeling, and image classification.",1
"MAC Net is a compositional attention network designed for Visual Question Answering. We propose a modified MAC net architecture for Natural Language Question Answering. Question Answering typically requires Language Understanding and multi-step Reasoning. MAC net's unique architecture - the separation between memory and control, facilitates data-driven iterative reasoning. This makes it an ideal candidate for solving tasks that involve logical reasoning. Our experiments with 20 bAbI tasks demonstrate the value of MAC net as a data-efficient and interpretable architecture for Natural Language Question Answering. The transparent nature of MAC net provides a highly granular view of the reasoning steps taken by the network in answering a query.",0
"The MAC Net was initially created as a compositional attention network for Visual Question Answering. However, we have developed a modified version of the MAC net architecture that is suitable for Natural Language Question Answering since this type of task requires Language Understanding and multi-step Reasoning. The unique structure of the MAC net - where memory and control are separated - facilitates data-driven iterative reasoning, making it an excellent choice for tasks that involve logical reasoning. Our experiments on 20 bAbI tasks have shown that MAC net is a data-efficient and interpretable architecture for Natural Language Question Answering. The transparent nature of MAC net allows for a detailed understanding of the reasoning steps taken by the network to answer a query.",1
"Many scene text recognition approaches are based on purely visual information and ignore the semantic relation between scene and text. In this paper, we tackle this problem from natural language processing perspective to fill the gap between language and vision. We propose a post-processing approach to improve scene text recognition accuracy by using occurrence probabilities of words (unigram language model), and the semantic correlation between scene and text. For this, we initially rely on an off-the-shelf deep neural network, already trained with a large amount of data, which provides a series of text hypotheses per input image. These hypotheses are then re-ranked using word frequencies and semantic relatedness with objects or scenes in the image. As a result of this combination, the performance of the original network is boosted with almost no additional cost. We validate our approach on ICDAR'17 dataset.",0
"The majority of current methods for recognizing scene text rely solely on visual information and do not consider the relationship between the text and the surrounding scene. This paper addresses this issue by incorporating natural language processing techniques to bridge the gap between language and vision. Our proposed approach involves post-processing the results of a pre-trained deep neural network using the occurrence probabilities of words and the semantic connections between the text and the objects or scenes in the image. By re-ranking the text hypotheses generated by the network, we significantly improve the accuracy of scene text recognition without incurring significant additional costs. Our approach is evaluated on the ICDAR'17 dataset and achieves promising results.",1
"Navigation guided by natural language instructions presents a challenging reasoning problem for instruction followers. Natural language instructions typically identify only a few high-level decisions and landmarks rather than complete low-level motor behaviors; much of the missing information must be inferred based on perceptual context. In machine learning settings, this is doubly challenging: it is difficult to collect enough annotated data to enable learning of this reasoning process from scratch, and also difficult to implement the reasoning process using generic sequence models. Here we describe an approach to vision-and-language navigation that addresses both these issues with an embedded speaker model. We use this speaker model to (1) synthesize new instructions for data augmentation and to (2) implement pragmatic reasoning, which evaluates how well candidate action sequences explain an instruction. Both steps are supported by a panoramic action space that reflects the granularity of human-generated instructions. Experiments show that all three components of this approach---speaker-driven data augmentation, pragmatic reasoning and panoramic action space---dramatically improve the performance of a baseline instruction follower, more than doubling the success rate over the best existing approach on a standard benchmark.",0
"The task of navigating through natural language instructions poses a challenging problem for those who follow them. These instructions usually provide only general directions and landmarks, leaving out specific actions that must be inferred based on the surrounding context. This presents an even greater challenge in machine learning, as it is difficult to gather enough annotated data to teach the reasoning process from the ground up, and generic sequence models struggle to implement it. Our approach to vision-and-language navigation addresses these issues by incorporating a speaker model. This model is used to create new instructions to augment the data, and also to implement pragmatic reasoning that evaluates how well a sequence of actions explains an instruction. Additionally, our approach includes a panoramic action space that reflects the level of detail in human-generated instructions. In experiments, our method significantly outperforms the best existing approach, more than doubling the success rate of a baseline instruction follower on a standard benchmark.",1
"Images may have elements containing text and a bounding box associated with them, for example, text identified via optical character recognition on a computer screen image, or a natural image with labeled objects. We present an end-to-end trainable architecture to incorporate the information from these elements and the image to segment/identify the part of the image a natural language expression is referring to. We calculate an embedding for each element and then project it onto the corresponding location (i.e., the associated bounding box) of the image feature map. We show that this architecture gives an improvement in resolving referring expressions, over only using the image, and other methods that incorporate the element information. We demonstrate experimental results on the referring expression datasets based on COCO, and on a webpage image referring expression dataset that we developed.",0
"The paragraph discusses how images can contain text elements and bounding boxes, which can be used to identify specific parts of the image. To achieve this, the authors propose an end-to-end trainable architecture that combines information from these elements and the image. The architecture involves calculating an embedding for each element and projecting it onto the corresponding location of the image feature map. According to the authors, this approach improves upon existing methods that only use the image or incorporate element information. The authors demonstrate the effectiveness of their approach through experiments on referring expression datasets based on COCO and a webpage image dataset they created.",1
"State of the art deep learning models have made steady progress in the fields of computer vision and natural language processing, at the expense of growing model sizes and computational complexity. Deploying these models on low power and mobile devices poses a challenge due to their limited compute capabilities and strict energy budgets. One solution that has generated significant research interest is deploying highly quantized models that operate on low precision inputs and weights less than eight bits, trading off accuracy for performance. These models have a significantly reduced memory footprint (up to 32x reduction) and can replace multiply-accumulates with bitwise operations during compute intensive convolution and fully connected layers.   Most deep learning frameworks rely on highly engineered linear algebra libraries such as ATLAS or Intel's MKL to implement efficient deep learning operators. To date, none of the popular deep learning directly support low precision operators, partly due to a lack of optimized low precision libraries. In this paper we introduce a work flow to quickly generate high performance low precision deep learning operators for arbitrary precision that target multiple CPU architectures and include optimizations such as memory tiling and vectorization. We present an extensive case study on low power ARM Cortex-A53 CPU, and show how we can generate 1-bit, 2-bit convolutions with speedups up to 16x over an optimized 16-bit integer baseline and 2.3x better than handwritten implementations.",0
"Advancements in deep learning models have had positive effects on computer vision and natural language processing, but have also resulted in larger model sizes and increased computational complexity. This has created a challenge for low power and mobile devices, which have limited compute capabilities and energy budgets. One solution that has gained attention is using highly quantized models that operate on low precision inputs and weights less than eight bits, sacrificing accuracy for performance. These models have a significantly reduced memory footprint and can replace multiply-accumulates with bitwise operations. However, most deep learning frameworks rely on linear algebra libraries that do not support low precision operators, due to a lack of optimized low precision libraries. This paper introduces a workflow that can generate high performance low precision deep learning operators for arbitrary precision that target multiple CPU architectures and include optimizations such as memory tiling and vectorization. The paper presents a case study on low power ARM Cortex-A53 CPU and demonstrates the ability to generate 1-bit, 2-bit convolutions with speedups up to 16x over an optimized 16-bit integer baseline and 2.3x better than handwritten implementations.",1
"Using a large number of parameters , deep neural networks have achieved remarkable performance on computer vison and natural language processing tasks. However the networks usually suffer from overfitting by using too much parameters. Dropout is a widely use method to deal with overfitting. Although dropout can significantly regularize densely connected layers in neural networks, it leads to suboptimal results when using for convolutional layers. To track this problem, we propose DropFilter, a new dropout method for convolutional layers. DropFilter randomly suppresses the outputs of some filters. Because it is observed that co-adaptions are more likely to occurs inter filters rather than intra filters in convolutional layers. Using DropFilter, we remarkably improve the performance of convolutional networks on CIFAR and ImageNet.",0
"Deep neural networks have produced impressive results in computer vision and natural language processing tasks by utilizing a large number of parameters. However, the excessive use of parameters often leads to overfitting. To address this issue, dropout has become a popular method for regularization. While dropout is effective in regulating densely connected layers, it is not optimal when used with convolutional layers. To overcome this limitation, we introduce DropFilter, a novel dropout method that randomly suppresses the outputs of certain filters. This approach targets inter-filter co-adaptations, which are more prevalent than intra-filter co-adaptations in convolutional layers. By incorporating DropFilter, we have achieved substantial improvements in the performance of convolutional networks on CIFAR and ImageNet datasets.",1
"The recent advances in deep neural networks (DNNs) make them attractive for embedded systems. However, it can take a long time for DNNs to make an inference on resource-constrained computing devices. Model compression techniques can address the computation issue of deep inference on embedded devices. This technique is highly attractive, as it does not rely on specialized hardware, or computation-offloading that is often infeasible due to privacy concerns or high latency. However, it remains unclear how model compression techniques perform across a wide range of DNNs. To design efficient embedded deep learning solutions, we need to understand their behaviors. This work develops a quantitative approach to characterize model compression techniques on a representative embedded deep learning architecture, the NVIDIA Jetson Tx2. We perform extensive experiments by considering 11 influential neural network architectures from the image classification and the natural language processing domains. We experimentally show that how two mainstream compression techniques, data quantization and pruning, perform on these network architectures and the implications of compression techniques to the model storage size, inference time, energy consumption and performance metrics. We demonstrate that there are opportunities to achieve fast deep inference on embedded systems, but one must carefully choose the compression settings. Our results provide insights on when and how to apply model compression techniques and guidelines for designing efficient embedded deep learning systems.",0
"The deep neural networks (DNNs) have become appealing for embedded systems due to recent advancements. However, DNNs may take a considerable amount of time to perform inference on computing devices with limited resources. To overcome this issue, model compression techniques can be utilized, which are attractive since they do not require specialized hardware or computation-offloading, which may not be feasible due to privacy concerns or high latency. Nevertheless, it is still unclear how these techniques perform across various DNNs. To design effective embedded deep learning solutions, it is necessary to comprehend their behavior. This study presents a quantitative method to characterize model compression techniques on the NVIDIA Jetson Tx2, a representative embedded deep learning architecture. We conducted extensive experiments on 11 influential neural network architectures from image classification and natural language processing domains. Our experiments show how two prevalent compression techniques, data quantization, and pruning, perform on these network architectures, and their implications for model storage size, inference time, energy consumption, and performance metrics. Our findings reveal that achieving fast deep inference on embedded systems is possible, but careful selection of compression settings is required. Our results offer insights on when and how to apply model compression techniques and provide guidelines for designing efficient embedded deep learning systems.",1
"Deep learning has shown high performances in various types of tasks from visual recognition to natural language processing, which indicates superior flexibility and adaptivity of deep learning. To understand this phenomenon theoretically, we develop a new approximation and estimation error analysis of deep learning with the ReLU activation for functions in a Besov space and its variant with mixed smoothness. The Besov space is a considerably general function space including the Holder space and Sobolev space, and especially can capture spatial inhomogeneity of smoothness. Through the analysis in the Besov space, it is shown that deep learning can achieve the minimax optimal rate and outperform any non-adaptive (linear) estimator such as kernel ridge regression, which shows that deep learning has higher adaptivity to the spatial inhomogeneity of the target function than other estimators such as linear ones. In addition to this, it is shown that deep learning can avoid the curse of dimensionality if the target function is in a mixed smooth Besov space. We also show that the dependency of the convergence rate on the dimensionality is tight due to its minimax optimality. These results support high adaptivity of deep learning and its superior ability as a feature extractor.",0
"Deep learning has displayed exceptional versatility and adaptability, showcasing superior performance in tasks ranging from visual recognition to natural language processing. To gain a theoretical understanding of this phenomenon, we have developed a novel approximation and estimation error analysis of deep learning with ReLU activation for functions in the Besov space and its mixed smoothness variant. The Besov space is an extensive function space that includes the Holder and Sobolev spaces and can capture the spatial unevenness of smoothness. Our analysis in the Besov space reveals that deep learning can achieve the minimax optimal rate, surpassing non-adaptive estimators like kernel ridge regression. This indicates that deep learning has a greater adaptability to spatial inhomogeneity than other estimators, including linear ones. Furthermore, we demonstrate that deep learning can avoid the curse of dimensionality when the target function is in a mixed smooth Besov space. Our findings support the notion that deep learning possesses remarkable adaptability and serves as an exceptional feature extractor.",1
"Principal component analysis (PCA) is widely used for dimension reduction and embedding of real data in social network analysis, information retrieval, and natural language processing, etc. In this work we propose a fast randomized PCA algorithm for processing large sparse data. The algorithm has similar accuracy to the basic randomized SVD (rPCA) algorithm (Halko et al., 2011), but is largely optimized for sparse data. It also has good flexibility to trade off runtime against accuracy for practical usage. Experiments on real data show that the proposed algorithm is up to 9.1X faster than the basic rPCA algorithm without accuracy loss, and is up to 20X faster than the svds in Matlab with little error. The algorithm computes the first 100 principal components of a large information retrieval data with 12,869,521 persons and 323,899 keywords in less than 400 seconds on a 24-core machine, while all conventional methods fail due to the out-of-memory issue.",0
"PCA is a commonly used method for reducing dimensions and embedding real data in various fields such as social network analysis, natural language processing, and information retrieval. This paper presents a rapid, randomized PCA algorithm specifically designed for large, sparse data. While it maintains comparable accuracy to the basic randomized SVD (rPCA) algorithm (Halko et al., 2011), it is optimized for sparse data and can balance runtime and accuracy for practical use. Real data experiments show that this method is up to 9.1 times faster than the basic rPCA algorithm with no loss in accuracy, and up to 20 times faster than the svds in Matlab with minimal error. The algorithm can efficiently calculate the first 100 principal components of a large information retrieval dataset with 12,869,521 individuals and 323,899 keywords in under 400 seconds with a 24-core machine, whereas traditional methods encounter out-of-memory issues.",1
"Image captioning is an interdisciplinary research problem that stands between computer vision and natural language processing. The task is to generate a textual description of the content of an image. The typical model used for image captioning is an encoder-decoder deep network, where the encoder captures the essence of an image while the decoder is responsible for generating a sentence describing the image. Attention mechanisms can be used to automatically focus the decoder on parts of the image which are relevant to predict the next word. In this paper, we explore different decoders and attentional models popular in neural machine translation, namely attentional recurrent neural networks, self-attentional transformers, and fully-convolutional networks, which represent the current state of the art of neural machine translation. The image captioning module is available as part of SOCKEYE at https://github.com/awslabs/sockeye which tutorial can be found at https://awslabs.github.io/sockeye/image_captioning.html .",0
"The generation of textual descriptions for images is a research problem that involves both computer vision and natural language processing. This is achieved through the use of an encoder-decoder deep network, where the encoder captures the essence of the image and the decoder produces a sentence describing its content. To predict the next word, attention mechanisms can be employed to focus the decoder on relevant parts of the image. In this study, we investigate various decoders and attentional models commonly used in neural machine translation, including attentional recurrent neural networks, self-attentional transformers, and fully-convolutional networks, which represent the current state of the art of neural machine translation. The image captioning module is available as part of SOCKEYE, and a tutorial can be accessed at https://awslabs.github.io/sockeye/image_captioning.html.",1
"We discuss deep reinforcement learning in an overview style. We draw a big picture, filled with details. We discuss six core elements, six important mechanisms, and twelve applications, focusing on contemporary work, and in historical contexts. We start with background of artificial intelligence, machine learning, deep learning, and reinforcement learning (RL), with resources. Next we discuss RL core elements, including value function, policy, reward, model, exploration vs. exploitation, and representation. Then we discuss important mechanisms for RL, including attention and memory, unsupervised learning, hierarchical RL, multi-agent RL, relational RL, and learning to learn. After that, we discuss RL applications, including games, robotics, natural language processing (NLP), computer vision, finance, business management, healthcare, education, energy, transportation, computer systems, and, science, engineering, and art. Finally we summarize briefly, discuss challenges and opportunities, and close with an epilogue.",0
"In this overview, we delve into the topic of deep reinforcement learning, providing a comprehensive yet detailed perspective. Our discussion covers six core elements, six significant mechanisms, and twelve applications, with a focus on both contemporary and historical contexts. We begin by providing background information on artificial intelligence, machine learning, deep learning, and reinforcement learning (RL), with relevant resources. Moving on, we explore the core elements of RL, which include value function, policy, reward, model, exploration vs. exploitation, and representation. Additionally, we discuss key mechanisms that underpin RL, such as attention and memory, unsupervised learning, hierarchical RL, multi-agent RL, relational RL, and learning to learn. Furthermore, we examine a wide range of RL applications, including games, robotics, natural language processing (NLP), computer vision, finance, business management, healthcare, education, energy, transportation, computer systems, and science, engineering, and art. Finally, we provide a brief summary, highlight challenges and opportunities, and conclude with an epilogue.",1
"Recent breakthroughs in computer vision and natural language processing have spurred interest in challenging multi-modal tasks such as visual question-answering and visual dialogue. For such tasks, one successful approach is to condition image-based convolutional network computation on language via Feature-wise Linear Modulation (FiLM) layers, i.e., per-channel scaling and shifting. We propose to generate the parameters of FiLM layers going up the hierarchy of a convolutional network in a multi-hop fashion rather than all at once, as in prior work. By alternating between attending to the language input and generating FiLM layer parameters, this approach is better able to scale to settings with longer input sequences such as dialogue. We demonstrate that multi-hop FiLM generation achieves state-of-the-art for the short input sequence task ReferIt --- on-par with single-hop FiLM generation --- while also significantly outperforming prior state-of-the-art and single-hop FiLM generation on the GuessWhat?! visual dialogue task.",0
"The advancement in computer vision and natural language processing has stimulated interest in complex multi-modal tasks such as visual dialogue and visual question-answering. One successful approach to tackle these tasks is to use Feature-wise Linear Modulation (FiLM) layers, which involve scaling and shifting the per-channel computation of image-based convolutional networks with language input. However, in this study, we propose a new approach to generate FiLM layer parameters multi-hop up the hierarchy of a convolutional network, rather than all at once, as previously done. This method alternates between attending to the language input and generating FiLM layer parameters, making it more effective in settings with longer input sequences such as dialogue. Our results show that multi-hop FiLM generation outperforms previous state-of-the-art models and single-hop FiLM generation for the GuessWhat?! visual dialogue task, while achieving the same performance as single-hop FiLM generation for the short input sequence task ReferIt.",1
"Language Models (LMs) are important components in several Natural Language Processing systems. Recurrent Neural Network LMs composed of LSTM units, especially those augmented with an external memory, have achieved state-of-the-art results. However, these models still struggle to process long sequences which are more likely to contain long-distance dependencies because of information fading and a bias towards more recent information. In this paper we demonstrate an effective mechanism for retrieving information in a memory augmented LSTM LM based on attending to information in memory in proportion to the number of timesteps the LSTM gating mechanism persisted the information.",0
"Several Natural Language Processing systems rely on Language Models (LMs) as crucial components. Among these, Recurrent Neural Network LMs that consist of LSTM units have shown exceptional performance, particularly when combined with external memory. Nevertheless, these models encounter difficulties when processing long sequences that tend to include long-distance dependencies due to information decay and a preference towards newer information. This paper presents a successful approach for retrieving information in a memory augmented LSTM LM by attending to information in memory based on the duration that the LSTM gating mechanism retained the information.",1
"Automatic generation of natural language from images has attracted extensive attention. In this paper, we take one step further to investigate generation of poetic language (with multiple lines) to an image for automatic poetry creation. This task involves multiple challenges, including discovering poetic clues from the image (e.g., hope from green), and generating poems to satisfy both relevance to the image and poeticness in language level. To solve the above challenges, we formulate the task of poem generation into two correlated sub-tasks by multi-adversarial training via policy gradient, through which the cross-modal relevance and poetic language style can be ensured. To extract poetic clues from images, we propose to learn a deep coupled visual-poetic embedding, in which the poetic representation from objects, sentiments and scenes in an image can be jointly learned. Two discriminative networks are further introduced to guide the poem generation, including a multi-modal discriminator and a poem-style discriminator. To facilitate the research, we have released two poem datasets by human annotators with two distinct properties: 1) the first human annotated image-to-poem pair dataset (with 8,292 pairs in total), and 2) to-date the largest public English poem corpus dataset (with 92,265 different poems in total). Extensive experiments are conducted with 8K images, among which 1.5K image are randomly picked for evaluation. Both objective and subjective evaluations show the superior performances against the state-of-the-art methods for poem generation from images. Turing test carried out with over 500 human subjects, among which 30 evaluators are poetry experts, demonstrates the effectiveness of our approach.",0
"Considerable attention has been given to the automatic production of natural language from images. This study takes a step further by investigating the generation of poetic language, consisting of multiple lines, for the creation of automatic poetry. This task is challenging, requiring the identification of poetic cues from an image, such as the representation of hope through green, and the generation of poetry that meets the requirements of both image relevance and poetic language. To address these challenges, the task of poem generation is formulated into two related sub-tasks, which are trained through multi-adversarial training via policy gradient, ensuring cross-modal relevance and poetic language style. A deep coupled visual-poetic embedding is proposed to extract poetic cues from images, while two discriminative networks, a multi-modal discriminator and a poem-style discriminator, guide poem generation. Two poem datasets have been released to facilitate research, including the first human-annotated image-to-poem pair dataset and the largest public English poem corpus dataset to date. Extensive experiments have been conducted with 8,292 pairs of images and poems, including 1.5K images for evaluation. Both objective and subjective evaluations demonstrate superior performance compared to state-of-the-art methods. A Turing test involving over 500 human subjects, including 30 poetry experts, verifies the effectiveness of the proposed approach.",1
"Conditional Generative Adversarial Networks (GANs) for cross-domain image-to-image translation have made much progress recently. Depending on the task complexity, thousands to millions of labeled image pairs are needed to train a conditional GAN. However, human labeling is expensive, even impractical, and large quantities of data may not always be available. Inspired by dual learning from natural language translation, we develop a novel dual-GAN mechanism, which enables image translators to be trained from two sets of unlabeled images from two domains. In our architecture, the primal GAN learns to translate images from domain U to those in domain V, while the dual GAN learns to invert the task. The closed loop made by the primal and dual tasks allows images from either domain to be translated and then reconstructed. Hence a loss function that accounts for the reconstruction error of images can be used to train the translators. Experiments on multiple image translation tasks with unlabeled data show considerable performance gain of DualGAN over a single GAN. For some tasks, DualGAN can even achieve comparable or slightly better results than conditional GAN trained on fully labeled data.",0
"Recent advancements in Conditional Generative Adversarial Networks (GANs) for image-to-image translation across domains have been significant. However, training a conditional GAN requires thousands to millions of labeled image pairs, which can be expensive and impractical. Moreover, large amounts of data may not always be available. Inspired by the dual learning approach used in natural language translation, we have developed a novel dual-GAN mechanism that can be trained with two sets of unlabeled images from two domains. Our architecture involves a primal GAN that translates images from domain U to those in domain V, and a dual GAN that learns to invert this task. The primal and dual tasks form a closed loop that allows translations and reconstructions of images from either domain. We have used a loss function that accounts for the reconstruction error of images to train the translators. Our experiments with multiple image translation tasks using unlabeled data have shown that DualGAN outperforms a single GAN. In some cases, DualGAN even achieves comparable or slightly better results than a conditional GAN trained on fully labeled data.",1
"Convolutional Neural Networks (CNN) are very popular in many fields including computer vision, speech recognition, natural language processing, to name a few. Though deep learning leads to groundbreaking performance in these domains, the networks used are very demanding computationally and are far from real-time even on a GPU, which is not power efficient and therefore does not suit low power systems such as mobile devices. To overcome this challenge, some solutions have been proposed for quantizing the weights and activations of these networks, which accelerate the runtime significantly. Yet, this acceleration comes at the cost of a larger error. The \uniqname method proposed in this work trains quantized neural networks by noise injection and a learned clamping, which improve the accuracy. This leads to state-of-the-art results on various regression and classification tasks, e.g., ImageNet classification with architectures such as ResNet-18/34/50 with low as 3-bit weights and activations. We implement the proposed solution on an FPGA to demonstrate its applicability for low power real-time applications. The implementation of the paper is available at https://github.com/Lancer555/NICE",0
"Many fields, such as computer vision, speech recognition, and natural language processing, rely on Convolutional Neural Networks (CNN). While deep learning has revolutionized these domains, the networks used are computationally intensive and not suitable for real-time applications on low power systems like mobile devices. To address this issue, researchers have proposed quantizing the weights and activations of these networks, which significantly speeds up the runtime. However, this approach often results in a larger error. The authors of this study present a new method called \uniqname, which trains quantized neural networks using noise injection and learned clamping to improve accuracy. The proposed method achieves state-of-the-art results on various regression and classification tasks, including ImageNet classification with low as 3-bit weights and activations using architectures such as ResNet-18/34/50. The authors also implement their solution on an FPGA to demonstrate its applicability for low power real-time applications. The implementation code is available on Github at https://github.com/Lancer555/NICE.",1
"Deep learning has transformed computer vision, natural language processing, and speech recognition\cite{badrinarayanan2017segnet, dong2016image, ren2017faster, ji20133d}. However, two critical questions remain obscure: (1) why do deep neural networks generalize better than shallow networks; and (2) does it always hold that a deeper network leads to better performance? Specifically, letting $L$ be the number of convolutional and pooling layers in a deep neural network, and $n$ be the size of the training sample, we derive an upper bound on the expected generalization error for this network, i.e.,   \begin{eqnarray*}   \mathbb{E}[R(W)-R_S(W)] \leq \exp{\left(-\frac{L}{2}\log{\frac{1}{\eta}}\right)}\sqrt{\frac{2\sigma^2}{n}I(S,W) }   \end{eqnarray*} where $\sigma >0$ is a constant depending on the loss function, $0<\eta<1$ is a constant depending on the information loss for each convolutional or pooling layer, and $I(S, W)$ is the mutual information between the training sample $S$ and the output hypothesis $W$. This upper bound shows that as the number of convolutional and pooling layers $L$ increases in the network, the expected generalization error will decrease exponentially to zero. Layers with strict information loss, such as the convolutional layers, reduce the generalization error for the whole network; this answers the first question. However, algorithms with zero expected generalization error does not imply a small test error or $\mathbb{E}[R(W)]$. This is because $\mathbb{E}[R_S(W)]$ is large when the information for fitting the data is lost as the number of layers increases. This suggests that the claim `the deeper the better' is conditioned on a small training error or $\mathbb{E}[R_S(W)]$. Finally, we show that deep learning satisfies a weak notion of stability and the sample complexity of deep neural networks will decrease as $L$ increases.",0
"Computer vision, natural language processing, and speech recognition have been revolutionized by deep learning techniques \cite{badrinarayanan2017segnet, dong2016image, ren2017faster, ji20133d}. However, there are still two important questions that remain unanswered: (1) what is the reason behind the superior generalization ability of deep neural networks compared to shallow ones; and (2) is it always true that a deeper network leads to better performance? To address these questions, we establish an upper bound on the expected generalization error of a deep neural network with $L$ convolutional and pooling layers and a training sample size of $n$. Specifically, we show that the expected generalization error decreases exponentially to zero as the number of convolutional and pooling layers increases. This is due to the fact that layers with strict information loss, such as convolutional layers, decrease the generalization error for the entire network. However, a zero expected generalization error does not guarantee a small test error or $\mathbb{E}[R(W)]$, as $\mathbb{E}[R_S(W)]$ can become large when information is lost as the number of layers increases. Therefore, the claim that ""the deeper the better"" is only valid if the training error or $\mathbb{E}[R_S(W)]$ is small. Finally, we demonstrate that deep learning satisfies a weak notion of stability, and that the sample complexity of deep neural networks decreases as the number of layers increases.",1
"Traffic cameras are commonly deployed monitoring components in road infrastructure networks, providing operators visual information about conditions at critical points in the network. However, human observers are often limited in their ability to process simultaneous information sources. Recent advancements in computer vision, driven by deep learning methods, have enabled general object recognition, unlocking opportunities for camera-based sensing beyond the existing human observer paradigm. In this paper, we present a Natural Language Processing (NLP)-inspired approach, entitled Bag-of-Label-Words (BoLW), for analyzing image data sets using exclusively textual labels. The BoLW model represents the data in a conventional matrix form, enabling data compression and decomposition techniques, while preserving semantic interpretability. We apply the Latent Dirichlet Allocation (LDA) topic model to decompose the label data into a small number of semantic topics. To illustrate our approach, we use freeway camera images collected from the Boston area between December 2017-January 2018. We analyze the cameras' sensitivity to weather events; identify temporal traffic patterns; and analyze the impact of infrequent events, such as the winter holidays and the ""bomb cyclone"" winter storm. This study demonstrates the flexibility of our approach, which allows us to analyze weather events and freeway traffic using only traffic camera image labels.",0
"Traffic cameras are widely used monitoring tools in road infrastructure networks that provide operators with visual information on critical points in the network. However, human observers have limitations in processing multiple sources of information simultaneously. With the development of deep learning methods, computer vision has made significant progress in general object recognition, creating new opportunities for camera-based sensing beyond human observation. In this study, we introduce the Bag-of-Label-Words (BoLW) approach, which is inspired by Natural Language Processing (NLP) and uses only textual labels to analyze image datasets. The BoLW model represents data in a traditional matrix form and allows for data compression and decomposition techniques while maintaining semantic interpretability. We apply the Latent Dirichlet Allocation (LDA) topic model to decompose the label data into a few semantic topics. We demonstrate the flexibility of our approach by analyzing freeway camera images collected from the Boston area between December 2017 and January 2018. We examine the cameras' sensitivity to weather events, identify temporal traffic patterns, and analyze the impact of infrequent events such as the winter holidays and the ""bomb cyclone"" winter storm. Our study shows how we can analyze weather events and freeway traffic using only traffic camera image labels.",1
"Recently, generative adversarial networks have gained a lot of popularity for image generation tasks. However, such models are associated with complex learning mechanisms and demand very large relevant datasets. This work borrows concepts from image and video captioning models to form an image generative framework. The model is trained in a similar fashion as recurrent captioning model and uses the learned weights for image generation. This is done in an inverse direction, where the input is a caption and the output is an image. The vector representation of the sentence and frames are extracted from an encoder-decoder model which is initially trained on similar sentence and image pairs. Our model conditions image generation on a natural language caption. We leverage a sequence-to-sequence model to generate synthetic captions that have the same meaning for having a robust image generation. One key advantage of our method is that the traditional image captioning datasets can be used for synthetic sentence paraphrases. Results indicate that images generated through multiple captions are better at capturing the semantic meaning of the family of captions.",0
"Generative adversarial networks have become popular for image generation, but require large datasets and complex learning mechanisms. This study uses concepts from image and video captioning models to create an image generative framework. By training the model in a similar fashion to a recurrent captioning model, the learned weights are used for image generation. The model conditions image generation on a natural language caption, using a sequence-to-sequence model to generate synthetic captions that have the same meaning. One advantage is that traditional image captioning datasets can be used for synthetic sentence paraphrases. Results show that images generated through multiple captions better capture the semantic meaning of the captions.",1
"Automatic generation of textual video descriptions that are time-aligned with video content is a long-standing goal in computer vision. The task is challenging due to the difficulty of bridging the semantic gap between the visual and natural language domains. This paper addresses the task of automatically generating an alignment between a set of instructions and a first person video demonstrating an activity. The sparse descriptions and ambiguity of written instructions create significant alignment challenges. The key to our approach is the use of egocentric cues to generate a concise set of action proposals, which are then matched to recipe steps using object recognition and computational linguistic techniques. We obtain promising results on both the Extended GTEA Gaze+ dataset and the Bristol Egocentric Object Interactions Dataset.",0
"The objective of creating textual descriptions for videos that correspond with the timing of the visual content has been a longstanding aim in the field of computer vision. This task is difficult because of the challenge of connecting the meaning between the visual and natural language domains. This paper tackles the task of automatically aligning a set of directions with a first-person video that illustrates an action. The limited and potentially unclear nature of written instructions poses significant challenges for alignment. Our approach relies on egocentric cues to generate a brief list of proposed actions that are then matched with recipe steps using object recognition and computational linguistic techniques. Encouraging results were obtained on both the Extended GTEA Gaze+ dataset and the Bristol Egocentric Object Interactions Dataset.,1
"Deep generative models have achieved remarkable success in various data domains, including images, time series, and natural languages. There remain, however, substantial challenges for combinatorial structures, including graphs. One of the key challenges lies in the difficulty of ensuring semantic validity in context. For examples, in molecular graphs, the number of bonding-electron pairs must not exceed the valence of an atom; whereas in protein interaction networks, two proteins may be connected only when they belong to the same or correlated gene ontology terms. These constraints are not easy to be incorporated into a generative model. In this work, we propose a regularization framework for variational autoencoders as a step toward semantic validity. We focus on the matrix representation of graphs and formulate penalty terms that regularize the output distribution of the decoder to encourage the satisfaction of validity constraints. Experimental results confirm a much higher likelihood of sampling valid graphs in our approach, compared with others reported in the literature.",0
"Various data domains, such as images, time series, and natural languages, have seen remarkable success with deep generative models. However, generating combinatorial structures like graphs presents substantial challenges. Ensuring semantic validity in context is particularly difficult. For instance, molecular graphs must have bonding-electron pairs that do not exceed the valence of an atom, while protein interaction networks need proteins to belong to the same or correlated gene ontology terms for a connection. Incorporating these constraints into a generative model is complex. This study proposes a regularization framework for variational autoencoders to address semantic validity. The focus is on the matrix representation of graphs, and penalty terms are formulated to encourage validity constraint satisfaction in the decoder's output distribution. Experimental results show our approach has a much higher likelihood of sampling valid graphs than those reported in the literature.",1
"Describing visual data into natural language is a very challenging task, at the intersection of computer vision, natural language processing and machine learning. Language goes well beyond the description of physical objects and their interactions and can convey the same abstract idea in many ways. It is both about content at the highest semantic level as well as about fluent form. Here we propose an approach to describe videos in natural language by reaching a consensus among multiple encoder-decoder networks. Finding such a consensual linguistic description, which shares common properties with a larger group, has a better chance to convey the correct meaning. We propose and train several network architectures and use different types of image, audio and video features. Each model produces its own description of the input video and the best one is chosen through an efficient, two-phase consensus process. We demonstrate the strength of our approach by obtaining state of the art results on the challenging MSR-VTT dataset.",0
"The task of converting visual data into natural language is an arduous one, requiring expertise in computer vision, natural language processing, and machine learning. Language encompasses more than just physical objects and their interactions, as it can convey abstract concepts in various ways. The process involves both semantic content and fluent form. Our proposed approach involves using multiple encoder-decoder networks to arrive at a consensus on a linguistic description of videos. This method increases the likelihood of conveying the intended meaning by finding a shared description with common features among a larger group. We trained various network architectures and employed different types of features, including image, audio, and video. Each model generates its own description of the input video, and the most appropriate one is chosen through a two-phase consensus process. Our approach has delivered outstanding results on the challenging MSR-VTT dataset.",1
"A major area of growth within deep learning has been the study and implementation of convolutional neural networks. The general explanation within the deep learning community of the robustness of convolutional neural networks (CNNs) within image recognition rests upon the idea that CNNs are able to extract localized features. However, recent developments in fields such as Natural Language Processing are demonstrating that this paradigm may be incorrect. In this paper, we analyze the current state of the field concerning CNN's and present a hypothesis that provides a novel explanation for the robustness of CNN models. From there, we demonstrate the effectiveness of our approach by presenting novel deep CNN frame interpolation architecture that is comparable to the state of the art interpolation models with a fraction of the complexity.",0
"The study and application of convolutional neural networks have seen significant growth in the field of deep learning. The deep learning community attributes the strength of CNNs in image recognition to their ability to extract localized features. However, recent advances in Natural Language Processing suggest that this view may not be entirely accurate. This article examines the current status of CNNs and proposes an innovative hypothesis to explain their robustness. Furthermore, we showcase the effectiveness of our approach by introducing a new deep CNN framework for frame interpolation, which is comparable to existing models but with significantly less complexity.",1
"Medical applications challenge today's text categorization techniques by demanding both high accuracy and ease-of-interpretation. Although deep learning has provided a leap ahead in accuracy, this leap comes at the sacrifice of interpretability. To address this accuracy-interpretability challenge, we here introduce, for the first time, a text categorization approach that leverages the recently introduced Tsetlin Machine. In all brevity, we represent the terms of a text as propositional variables. From these, we capture categories using simple propositional formulae, such as: if ""rash"" and ""reaction"" and ""penicillin"" then Allergy. The Tsetlin Machine learns these formulae from a labelled text, utilizing conjunctive clauses to represent the particular facets of each category. Indeed, even the absence of terms (negated features) can be used for categorization purposes. Our empirical comparison with Na\""ive Bayes, decision trees, linear support vector machines (SVMs), random forest, long short-term memory (LSTM) neural networks, and other techniques, is quite conclusive. The Tsetlin Machine either performs on par with or outperforms all of the evaluated methods on both the 20 Newsgroups and IMDb datasets, as well as on a non-public clinical dataset. On average, the Tsetlin Machine delivers the best recall and precision scores across the datasets. Finally, our GPU implementation of the Tsetlin Machine executes 5 to 15 times faster than the CPU implementation, depending on the dataset. We thus believe that our novel approach can have a significant impact on a wide range of text analysis applications, forming a promising starting point for deeper natural language understanding with the Tsetlin Machine.",0
"The field of medicine poses a challenge for text categorization techniques as accuracy and ease of interpretation are both crucial. While deep learning has improved accuracy, it has come at the cost of interpretability. To address this challenge, we introduce a new approach that utilizes the Tsetlin Machine, which represents terms as propositional variables and captures categories using simple propositional formulae. The Tsetlin Machine learns these formulae from labelled text, using conjunctive clauses to represent different facets of each category. Our empirical comparison with other techniques shows that the Tsetlin Machine performs either on par or better than all evaluated methods on various datasets. Furthermore, our GPU implementation executes faster than the CPU implementation. We believe that this approach can significantly impact text analysis applications and serve as a promising starting point for deeper natural language understanding with the Tsetlin Machine.",1
"Process Mining consists of techniques where logs created by operative systems are transformed into process models. In process mining tools it is often desired to be able to classify ongoing process instances, e.g., to predict how long the process will still require to complete, or to classify process instances to different classes based only on the activities that have occurred in the process instance thus far. Recurrent neural networks and its subclasses, such as Gated Recurrent Unit (GRU) and Long Short-Term Memory (LSTM), have been demonstrated to be able to learn relevant temporal features for subsequent classification tasks. In this paper we apply recurrent neural networks to classifying process instances. The proposed model is trained in a supervised fashion using labeled process instances extracted from event log traces. This is the first time we know of GRU having been used in classifying business process instances. Our main experimental results shows that GRU outperforms LSTM remarkably in training time while giving almost identical accuracies to LSTM models. Additional contributions of our paper are improving the classification model training time by filtering infrequent activities, which is a technique commonly used, e.g., in Natural Language Processing (NLP).",0
"The transformation of system logs into process models constitutes Process Mining techniques. The ability to classify ongoing process instances is a common requirement in Process Mining tools, such as predicting the process completion time or classifying instances into different categories based on the activities that have taken place. Recurrent neural networks, a subclass of which includes Gated Recurrent Unit (GRU) and Long Short-Term Memory (LSTM), have proven to be effective in learning relevant temporal features for classification tasks. In this study, we implement recurrent neural networks for process instance classification. We train the proposed model in a supervised manner, using labeled process instances extracted from event log traces. To our knowledge, this is the first time GRU has been used in business process instance classification. Our experimental results reveal that GRU outperforms LSTM significantly in terms of training time, while producing almost identical accuracies to LSTM models. Additionally, we contribute to the field by improving the classification model training time through the filtering of infrequent activities, a technique commonly employed in Natural Language Processing (NLP).",1
"Many tasks in natural language understanding require learning relationships between two sequences for various tasks such as natural language inference, paraphrasing and entailment. These aforementioned tasks are similar in nature, yet they are often modeled individually. Knowledge transfer can be effective for closely related tasks. However, transferring all knowledge, some of which irrelevant for a target task, can lead to sub-optimal results due to \textit{negative} transfer. Hence, this paper focuses on the transferability of both instances and parameters across natural language understanding tasks by proposing an ensemble-based transfer learning method. \newline The primary contribution of this paper is the combination of both \textit{Dropout} and \textit{Bagging} for improved transferability in neural networks, referred to as \textit{Dropping} herein. We present a straightforward yet novel approach for incorporating source \textit{Dropping} Networks to a target task for few-shot learning that mitigates \textit{negative} transfer. This is achieved by using a decaying parameter chosen according to the slope changes of a smoothed spline error curve at sub-intervals during training. We compare the proposed approach against hard parameter sharing and soft parameter sharing transfer methods in the few-shot learning case. We also compare against models that are fully trained on the target task in the standard supervised learning setup. The aforementioned adjustment leads to improved transfer learning performance and comparable results to the current state of the art only using a fraction of the data from the target task.",0
"Learning relationships between two sequences is essential in natural language understanding for tasks such as natural language inference, paraphrasing, and entailment. Although these tasks share similarities, they are often modeled individually, resulting in sub-optimal results when transferring all knowledge between them. To address this issue, this paper proposes an ensemble-based transfer learning method that focuses on the transferability of both instances and parameters across natural language understanding tasks. The paper introduces a novel approach called ""Dropping,"" which combines Dropout and Bagging for improved transferability in neural networks. The method incorporates source Dropping Networks to a target task for few-shot learning, mitigating negative transfer. This is achieved by using a decaying parameter selected based on the slope changes of a smoothed spline error curve during training. The proposed approach outperforms hard and soft parameter sharing transfer methods in the few-shot learning case and is comparable to models fully trained on the target task using only a fraction of the data. The primary contribution of this paper is the combination of Dropout and Bagging for improved transferability in neural networks.",1
"Deep learning algorithms excel at extracting patterns from raw data, and with large datasets, they have been very successful in computer vision and natural language applications. However, in other domains, large datasets on which to learn representations from may not exist. In this work, we develop a novel multimodal CNN-MLP neural network architecture that utilizes both domain-specific feature engineering as well as learned representations from raw data. We illustrate the effectiveness of such network designs in the chemical sciences, for predicting biodegradability. DeepBioD, a multimodal CNN-MLP network is more accurate than either standalone network designs, and achieves an error classification rate of 0.125 that is 27% lower than the current state-of-the-art. Thus, our work indicates that combining traditional feature engineering with representation learning can be effective, particularly in situations where labeled data is limited.",0
"The ability of deep learning algorithms to detect patterns in raw data has led to their success in computer vision and natural language applications with large datasets. However, some domains may not have an ample amount of data for learning representations. To address this issue, we have introduced a new neural network architecture called multimodal CNN-MLP, which combines domain-specific feature engineering and learned representations from raw data. Our experiments in the chemical sciences demonstrate the effectiveness of this approach in predicting biodegradability. The DeepBioD network, which utilizes this architecture, outperforms standalone networks and achieves a lower error classification rate of 0.125, which is 27% better than the current state-of-the-art. Therefore, our findings suggest that a combination of traditional feature engineering and representation learning can be useful, especially when labeled data is scarce.",1
"Deep neural networks (DNN) excel at extracting patterns. Through representation learning and automated feature engineering on large datasets, such models have been highly successful in computer vision and natural language applications. Designing optimal network architectures from a principled or rational approach however has been less than successful, with the best successful approaches utilizing an additional machine learning algorithm to tune the network hyperparameters. However, in many technical fields, there exist established domain knowledge and understanding about the subject matter. In this work, we develop a novel furcated neural network architecture that utilizes domain knowledge as high-level design principles of the network. We demonstrate proof-of-concept by developing IL-Net, a furcated network for predicting the properties of ionic liquids, which is a class of complex multi-chemicals entities. Compared to existing state-of-the-art approaches, we show that furcated networks can improve model accuracy by approximately 20-35%, without using additional labeled data. Lastly, we distill two key design principles for furcated networks that can be adapted to other domains.",0
"DNNs are highly skilled at recognizing patterns, particularly in computer vision and natural language applications, due to their representation learning and automated feature engineering on large datasets. However, designing optimal network architectures from a rational perspective has proven difficult, with machine learning algorithms often used to tune network hyperparameters. Nevertheless, many technical fields have established domain knowledge and understanding, which can be utilized as high-level design principles for networks. This study presents a novel furcated neural network architecture that incorporates domain knowledge, exemplified by the development of IL-Net, a furcated network for predicting the properties of ionic liquids. Results show that furcated networks can improve model accuracy by 20-35% without requiring additional labeled data. Additionally, two key design principles for furcated networks are identified and can be applied to other domains.",1
"Recently, Reinforcement Learning (RL) approaches have demonstrated advanced performance in image captioning by directly optimizing the metric used for testing. However, this shaped reward introduces learning biases, which reduces the readability of generated text. In addition, the large sample space makes training unstable and slow. To alleviate these issues, we propose a simple coherent solution that constrains the action space using an n-gram language prior. Quantitative and qualitative evaluations on benchmarks show that RL with the simple add-on module performs favorably against its counterpart in terms of both readability and speed of convergence. Human evaluation results show that our model is more human readable and graceful. The implementation will become publicly available upon the acceptance of the paper.",0
"Recently, image captioning has seen improved performance with Reinforcement Learning (RL) approaches, which directly optimize the testing metric. However, this shaped reward may introduce biases that reduce the readability of generated text. Additionally, the large sample space can lead to unstable and slow training. To address these issues, we propose a solution that restricts the action space using an n-gram language prior. Our quantitative and qualitative evaluations on benchmarks demonstrate that the RL approach with this simple add-on module performs favorably compared to its counterpart, with better readability and faster convergence. Human evaluation results also show that our model generates more human-readable and graceful captions. Upon acceptance of the paper, the implementation will be publicly available.",1
"Recently it has shown that the policy-gradient methods for reinforcement learning have been utilized to train deep end-to-end systems on natural language processing tasks. What's more, with the complexity of understanding image content and diverse ways of describing image content in natural language, image captioning has been a challenging problem to deal with. To the best of our knowledge, most state-of-the-art methods follow a pattern of sequential model, such as recurrent neural networks (RNN). However, in this paper, we propose a novel architecture for image captioning with deep reinforcement learning to optimize image captioning tasks. We utilize two networks called ""policy network"" and ""value network"" to collaboratively generate the captions of images. The experiments are conducted on Microsoft COCO dataset, and the experimental results have verified the effectiveness of the proposed method.",0
"Recently, deep end-to-end systems for natural language processing tasks have been trained using policy-gradient methods for reinforcement learning. Image captioning has been a challenging problem due to the complexity of understanding image content and diverse ways of describing it in natural language. Most state-of-the-art methods follow a sequential model, such as RNN. However, this paper proposes a novel architecture for image captioning using deep reinforcement learning to optimize the task. The proposed method utilizes two networks, namely ""policy network"" and ""value network,"" to collaboratively generate captions for images. The effectiveness of the proposed method is verified through experiments conducted on Microsoft COCO dataset.",1
"Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. This paper aims at generating plausible natural language sentences, whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics. We propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposition of semantic structures. With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns highly interpretable representations from even only word annotations, and produces realistic sentences with desired attributes. Quantitative evaluation validates the accuracy of sentence and attribute generation.",0
"Generating and manipulating text in a generic manner is a difficult task with limited success, especially when compared to recent advancements in deep generative modeling in the visual domain. The purpose of this study is to generate natural language sentences that are both plausible and dynamically controlled by learning disentangled latent representations with designated semantics. To accomplish this, we propose a new neural generative model that combines variational auto-encoders and holistic attribute discriminators to effectively impose semantic structures. Our model includes a differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of the generator and discriminators. Even with only word annotations, our model learns highly interpretable representations and produces realistic sentences with desired attributes. Quantitative evaluation confirms the accuracy of the sentence and attribute generation.",1
"Deep learning has demonstrated tremendous success in variety of application domains in the past few years. This new field of machine learning has been growing rapidly and applied in most of the application domains with some new modalities of applications, which helps to open new opportunity. There are different methods have been proposed on different category of learning approaches, which includes supervised, semi-supervised and un-supervised learning. The experimental results show state-of-the-art performance of deep learning over traditional machine learning approaches in the field of Image Processing, Computer Vision, Speech Recognition, Machine Translation, Art, Medical imaging, Medical information processing, Robotics and control, Bio-informatics, Natural Language Processing (NLP), Cyber security, and many more. This report presents a brief survey on development of DL approaches, including Deep Neural Network (DNN), Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) including Long Short Term Memory (LSTM) and Gated Recurrent Units (GRU), Auto-Encoder (AE), Deep Belief Network (DBN), Generative Adversarial Network (GAN), and Deep Reinforcement Learning (DRL). In addition, we have included recent development of proposed advanced variant DL techniques based on the mentioned DL approaches. Furthermore, DL approaches have explored and evaluated in different application domains are also included in this survey. We have also comprised recently developed frameworks, SDKs, and benchmark datasets that are used for implementing and evaluating deep learning approaches. There are some surveys have published on Deep Learning in Neural Networks [1, 38] and a survey on RL [234]. However, those papers have not discussed the individual advanced techniques for training large scale deep learning models and the recently developed method of generative models [1].",0
"In recent years, deep learning has proven to be highly successful across various application domains. This relatively new field of machine learning has grown rapidly and has been widely applied with new modalities, creating new opportunities. Different learning approaches, such as supervised, semi-supervised, and un-supervised learning, have been proposed and experimental results have shown that deep learning outperforms traditional machine learning approaches in many areas, including Image Processing, Computer Vision, Speech Recognition, Machine Translation, Art, Medical imaging, Medical information processing, Robotics and control, Bio-informatics, Natural Language Processing (NLP), and Cyber security. This report provides a brief survey of the development of deep learning approaches, including Deep Neural Network (DNN), Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), Auto-Encoder (AE), Deep Belief Network (DBN), Generative Adversarial Network (GAN), and Deep Reinforcement Learning (DRL). Additionally, this report includes advanced variant DL techniques based on these approaches, evaluations of DL approaches in different application domains, and recently developed frameworks, SDKs, and benchmark datasets for implementing and evaluating deep learning approaches. While previous surveys have been published on Deep Learning in Neural Networks and RL, they have not discussed individual advanced techniques for training large scale deep learning models and recently developed generative models.",1
"We introduce MASSES, a simple evaluation metric for the task of Visual Question Answering (VQA). In its standard form, the VQA task is operationalized as follows: Given an image and an open-ended question in natural language, systems are required to provide a suitable answer. Currently, model performance is evaluated by means of a somehow simplistic metric: If the predicted answer is chosen by at least 3 human annotators out of 10, then it is 100% correct. Though intuitively valuable, this metric has some important limitations. First, it ignores whether the predicted answer is the one selected by the Majority (MA) of annotators. Second, it does not account for the quantitative Subjectivity (S) of the answers in the sample (and dataset). Third, information about the Semantic Similarity (SES) of the responses is completely neglected. Based on such limitations, we propose a multi-component metric that accounts for all these issues. We show that our metric is effective in providing a more fine-grained evaluation both on the quantitative and qualitative level.",0
"We present MASSES, a straightforward measure for evaluating Visual Question Answering (VQA) performance. The VQA task involves answering open-ended questions in natural language based on an image. Currently, models are assessed by a basic metric: if at least 3 of 10 human annotators select the predicted answer, it is considered 100% correct. However, this metric has limitations. It does not consider if the predicted answer is chosen by the majority of annotators, account for the subjective nature of answers, or incorporate semantic similarity. To address these limitations, we propose a multi-component metric that provides a more detailed evaluation. Our metric is effective in assessing both quantitative and qualitative aspects of VQA.",1
"Visual relationship detection can bridge the gap between computer vision and natural language for scene understanding of images. Different from pure object recognition tasks, the relation triplets of subject-predicate-object lie on an extreme diversity space, such as \textit{person-behind-person} and \textit{car-behind-building}, while suffering from the problem of combinatorial explosion. In this paper, we propose a context-dependent diffusion network (CDDN) framework to deal with visual relationship detection. To capture the interactions of different object instances, two types of graphs, word semantic graph and visual scene graph, are constructed to encode global context interdependency. The semantic graph is built through language priors to model semantic correlations across objects, whilst the visual scene graph defines the connections of scene objects so as to utilize the surrounding scene information. For the graph-structured data, we design a diffusion network to adaptively aggregate information from contexts, which can effectively learn latent representations of visual relationships and well cater to visual relationship detection in view of its isomorphic invariance to graphs. Experiments on two widely-used datasets demonstrate that our proposed method is more effective and achieves the state-of-the-art performance.",0
"The detection of visual relationships can serve as a bridge between computer vision and natural language, enabling a better understanding of image scenes. Unlike simple object recognition tasks, subject-predicate-object relation triplets encompass a wide range of diversity, including examples such as ""person-behind-person"" and ""car-behind-building,"" making the problem of combinatorial explosion a challenge. To address this, we have developed a context-dependent diffusion network (CDDN) framework that leverages two types of graphs: a word semantic graph and a visual scene graph. These graphs are used to capture the interdependencies of different object instances, and are designed to encode global context through the use of language priors and surrounding scene information. By using a diffusion network to adaptively aggregate information from contexts, our method effectively learns latent representations of visual relationships, making it isomorphic invariant to graphs. Our experiments on two commonly-used datasets demonstrate that our approach is more effective and achieves state-of-the-art performance.",1
"Recurrent neural networks have become ubiquitous in computing representations of sequential data, especially textual data in natural language processing. In particular, Bidirectional LSTMs are at the heart of several neural models achieving state-of-the-art performance in a wide variety of tasks in NLP. However, BiLSTMs are known to suffer from sequential bias - the contextual representation of a token is heavily influenced by tokens close to it in a sentence. We propose a general and effective improvement to the BiLSTM model which encodes each suffix and prefix of a sequence of tokens in both forward and reverse directions. We call our model Suffix Bidirectional LSTM or SuBiLSTM. This introduces an alternate bias that favors long range dependencies. We apply SuBiLSTMs to several tasks that require sentence modeling. We demonstrate that using SuBiLSTM instead of a BiLSTM in existing models leads to improvements in performance in learning general sentence representations, text classification, textual entailment and paraphrase detection. Using SuBiLSTM we achieve new state-of-the-art results for fine-grained sentiment classification and question classification.",0
"Recurrent neural networks are commonly used in processing sequential data, especially in natural language processing where Bidirectional LSTMs are a popular choice. However, these models have a flaw known as sequential bias, whereby a token's context is heavily influenced by nearby tokens. To address this, we propose the SuBiLSTM model which encodes both suffixes and prefixes of a token sequence in forward and reverse directions. This introduces a bias favoring long-range dependencies. When applied to various sentence modeling tasks, SuBiLSTM outperforms BiLSTM in learning general sentence representations, text classification, textual entailment, and paraphrase detection. We even achieve new state-of-the-art results for fine-grained sentiment classification and question classification using SuBiLSTM.",1
"Planetary exploration missions with Mars rovers are complicated, which generally require elaborated task planning by human experts, from the path to take to the images to capture. NASA has been using this process to acquire over 22 million images from the planet Mars. In order to improve the degree of automation and thus efficiency in this process, we propose a system for planetary rovers to actively search for prominence of prespecified scientific features in captured images. Scientists can prespecify such search tasks in natural language and upload them to a rover, on which the deployed system constantly captions captured images with a deep image captioning network and compare the auto-generated captions to the prespecified search tasks by certain metrics so as to prioritize those images for transmission. As a beneficial side effect, the proposed system can also be deployed to ground-based planetary data systems as a content-based search engine.",0
"Missions to explore planets with Mars rovers can be complex and require extensive planning by human experts, from determining the best route to take to selecting which images to capture. NASA has utilized this process to gather over 22 million images from Mars. To increase efficiency and automation, we suggest a system where planetary rovers can actively search for predetermined scientific features in captured images. Scientists can input search tasks in natural language, which the system will use to caption images with a deep image captioning network. The system will then compare the auto-generated captions to the predetermined search tasks using specific metrics, allowing it to prioritize which images to transmit. Additionally, this system can also function as a content-based search engine for ground-based planetary data systems.",1
"The task of conducting visually grounded dialog involves learning goal-oriented cooperative dialog between autonomous agents who exchange information about a scene through several rounds of questions and answers in natural language. We posit that requiring artificial agents to adhere to the rules of human language, while also requiring them to maximize information exchange through dialog is an ill-posed problem. We observe that humans do not stray from a common language because they are social creatures who live in communities, and have to communicate with many people everyday, so it is far easier to stick to a common language even at the cost of some efficiency loss. Using this as inspiration, we propose and evaluate a multi-agent community-based dialog framework where each agent interacts with, and learns from, multiple agents, and show that this community-enforced regularization results in more relevant and coherent dialog (as judged by human evaluators) without sacrificing task performance (as judged by quantitative metrics).",0
"The visually grounded dialog task involves teaching autonomous agents how to engage in goal-oriented cooperative dialog. This involves exchanging information about a scene through multiple rounds of questions and answers in natural language. However, expecting artificial agents to follow human language rules while maximizing information exchange through dialog is problematic. Humans stick to a common language because they are social creatures who communicate with many people daily, even at the cost of some efficiency loss. To address this, we propose a multi-agent community-based dialog framework where each agent interacts with and learns from multiple agents. Our approach results in more relevant and coherent dialog (according to human evaluators) without sacrificing task performance (according to quantitative metrics).",1
"Localizing moments in a longer video via natural language queries is a new, challenging task at the intersection of language and video understanding. Though moment localization with natural language is similar to other language and vision tasks like natural language object retrieval in images, moment localization offers an interesting opportunity to model temporal dependencies and reasoning in text. We propose a new model that explicitly reasons about different temporal segments in a video, and shows that temporal context is important for localizing phrases which include temporal language. To benchmark whether our model, and other recent video localization models, can effectively reason about temporal language, we collect the novel TEMPOral reasoning in video and language (TEMPO) dataset. Our dataset consists of two parts: a dataset with real videos and template sentences (TEMPO - Template Language) which allows for controlled studies on temporal language, and a human language dataset which consists of temporal sentences annotated by humans (TEMPO - Human Language).",0
"A fresh and difficult task at the junction of language and video comprehension is localizing specific moments in lengthy videos by using natural language queries. Although the process of localizing moments through natural language is akin to other tasks involving language and vision, such as natural language object retrieval in images, moment localization presents an intriguing opportunity to model temporal dependencies and reasoning in text. We present a new model that carefully considers various temporal segments in a video and demonstrates that temporal context plays a crucial role in identifying phrases containing temporal language. To evaluate the effectiveness of our model and other recent video localization models in reasoning about temporal language, we have created the innovative TEMPOral reasoning in video and language (TEMPO) dataset. Our dataset comprises two components: TEMPO - Template Language, which contains real videos and template sentences for controlled studies on temporal language, and TEMPO - Human Language, which consists of temporal sentences annotated by humans.",1
"Recent success in deep reinforcement learning is having an agent learn how to play Go and beat the world champion without any prior knowledge of the game. In that task, the agent has to make a decision on what action to take based on the positions of the pieces. Person Search is recently explored using natural language based text description of images for video surveillance applications (S.Li et.al). We see (Fu.et al) provides an end to end approach for object-based retrieval using deep reinforcement learning without constraints placed on which objects are being detected. However, we believe for real-world applications such as person search defining specific constraints which identify a person as opposed to starting with a general object detection will have benefits in terms of performance and computational resources required. In our task, Deep reinforcement learning would localize the person in an image by reshaping the sizes of the bounding boxes. Deep Reinforcement learning with appropriate constraints would look only for the relevant person in the image as opposed to an unconstrained approach where each individual objects in the image are ranked. For person search, the agent is trying to form a tight bounding box around the person in the image who matches the description. The bounding box is initialized to the full image and at each time step, the agent makes a decision on how to change the current bounding box so that it has a tighter bound around the person based on the description of the person and the pixel values of the current bounding box. After the agent takes an action, it will be given a reward based on the Intersection over Union (IoU) of the current bounding box and the ground truth box. Once the agent believes that the bounding box is covering the person, it will indicate that the person is found.",0
"The recent triumph in deep reinforcement learning involves an agent being able to master playing Go and defeating the world champion without any prior knowledge of the game. Another recent development is the use of natural language-based text descriptions of images for video surveillance applications, as explored by S. Li et al. Additionally, Fu et al. have offered an end-to-end approach for object-based retrieval through deep reinforcement learning, without any constraints on object detection. However, we believe that for practical applications, such as person search, defining specific constraints for identifying a person would be beneficial for performance and computational resources. In our task, deep reinforcement learning reshapes bounding box sizes to locate a person in an image. With appropriate constraints, deep reinforcement learning would only search for the relevant person, rather than ranking each object in the image. The agent's objective in person search is to create a tightly bound box around the individual described in the image. At each time step, the agent decides how to modify the current bounding box based on the person's description and the pixel values of the box. The agent receives a reward based on the Intersection over Union (IoU) of the current bounding box and the ground truth box, and once the bounding box covers the person, the agent signals that the person is found.",1
"The question we answer with this work is: can we convert a text document into an image to exploit best image classification models to classify documents? To answer this question we present a novel text classification method which converts a text document into an encoded image, using word embedding and capabilities of Convolutional Neural Networks (CNNs), successfully employed in image classification. We evaluate our approach by obtaining promising results on some well-known benchmark datasets for text classification. This work allows the application of many of the advanced CNN architectures developed for Computer Vision to Natural Language Processing. We test the proposed approach on a multi-modal dataset, proving that it is possible to use a single deep model to represent text and image in the same feature space.",0
"We aim to determine whether it is feasible to utilize image classification models to classify text documents by converting them into images. To address this, we introduce a new method for text classification that leverages word embedding and Convolutional Neural Networks (CNNs), commonly used in image classification, to encode text documents as images. Our approach is evaluated on established text classification benchmark datasets, yielding promising results. This work provides an opportunity to apply advanced CNN architectures developed for Computer Vision to Natural Language Processing. We test the proposed method on a multi-modal dataset, demonstrating that a single deep model can represent both text and image in the same feature space.",1
"Text segmentation plays an important role in various Natural Language Processing (NLP) tasks like summarization, context understanding, document indexing and document noise removal. Previous methods for this task require manual feature engineering, huge memory requirements and large execution times. To the best of our knowledge, this paper is the first one to present a novel supervised neural approach for text segmentation. Specifically, we propose an attention-based bidirectional LSTM model where sentence embeddings are learned using CNNs and the segments are predicted based on contextual information. This model can automatically handle variable sized context information. Compared to the existing competitive baselines, the proposed model shows a performance improvement of ~7% in WinDiff score on three benchmark datasets.",0
"The segmentation of text is crucial in various NLP tasks such as document indexing, summarization, context understanding, and document noise removal. Previous methods for this task have relied on manual feature engineering, large memory requirements, and extended execution times. This paper presents a novel approach to text segmentation, which is the first supervised neural model to our knowledge. Our proposed model employs an attention-based bidirectional LSTM architecture that utilizes CNNs to learn sentence embeddings. The model predicts segments based on contextual information and can handle variable sized context. Compared to existing competitive baselines, our model demonstrates a performance improvement of approximately 7% in WinDiff score across three benchmark datasets.",1
"In recent years, China, the United States and other countries, Google and other high-tech companies have increased investment in artificial intelligence. Deep learning is one of the current artificial intelligence research's key areas. This paper analyzes and summarizes the latest progress and future research directions of deep learning. Firstly, three basic models of deep learning are outlined, including multilayer perceptrons, convolutional neural networks, and recurrent neural networks. On this basis, we further analyze the emerging new models of convolution neural networks and recurrent neural networks. This paper then summarizes deep learning's applications in many areas of artificial intelligence, including speech processing, computer vision, natural language processing and so on. Finally, this paper discusses the existing problems of deep learning and gives the corresponding possible solutions.",0
"Artificial intelligence investment has increased significantly in recent years, with countries like China, the United States, and high-tech companies like Google contributing to this trend. Among the key areas of research in artificial intelligence is deep learning, which is the focus of this paper. The paper provides an analysis and summary of the latest progress and future research directions in deep learning. Three basic models of deep learning are outlined, namely multilayer perceptrons, convolutional neural networks, and recurrent neural networks. The paper further examines the emerging new models of convolution neural networks and recurrent neural networks. In addition, the paper highlights deep learning's applications in various areas of artificial intelligence, including speech processing, computer vision, and natural language processing. Finally, the paper discusses the existing problems of deep learning and proposes possible solutions.",1
"We propose a novel attentive sequence to sequence translator (ASST) for clip localization in videos by natural language descriptions. We make two contributions. First, we propose a bi-directional Recurrent Neural Network (RNN) with a finely calibrated vision-language attentive mechanism to comprehensively understand the free-formed natural language descriptions. The RNN parses natural language descriptions in two directions, and the attentive model attends every meaningful word or phrase to each frame, thereby resulting in a more detailed understanding of video content and description semantics. Second, we design a hierarchical architecture for the network to jointly model language descriptions and video content. Given a video-description pair, the network generates a matrix representation, i.e., a sequence of vectors. Each vector in the matrix represents a video frame conditioned by the description. The 2D representation not only preserves the temporal dependencies of frames but also provides an effective way to perform frame-level video-language matching. The hierarchical architecture exploits video content with multiple granularities, ranging from subtle details to global context. Integration of the multiple granularities yields a robust representation for multi-level video-language abstraction. We validate the effectiveness of our ASST on two large-scale datasets. Our ASST outperforms the state-of-the-art by $4.28\%$ in Rank$@1$ on the DiDeMo dataset. On the Charades-STA dataset, we significantly improve the state-of-the-art by $13.41\%$ in Rank$@1,IoU=0.5$.",0
"We have developed an innovative attentive sequence to sequence translator (ASST) that can localize clips in videos based on natural language descriptions. We have made two key contributions. Firstly, we have created a bi-directional Recurrent Neural Network (RNN) that uses a finely tuned vision-language attentive mechanism to fully comprehend free-form natural language descriptions. The RNN parses language descriptions in both directions, and the attentive model focuses on every meaningful word or phrase in each frame, resulting in a more detailed understanding of video content and description semantics. Secondly, we have designed a hierarchical architecture for the network that can jointly model language descriptions and video content. By using a video-description pair, the network generates a matrix representation, which is a sequence of vectors. Each vector represents a video frame that is conditioned by the description. This 2D representation preserves the temporal dependencies of frames and provides an effective way to perform frame-level video-language matching. The hierarchical architecture exploits video content with multiple granularities, ranging from subtle details to global context, and integrates these granularities to yield a robust representation for multi-level video-language abstraction. We have tested our ASST on two large-scale datasets and found that it outperforms the state-of-the-art by $4.28\%$ in Rank$@1$ on the DiDeMo dataset and significantly improves the state-of-the-art by $13.41\%$ in Rank$@1,IoU=0.5$ on the Charades-STA dataset.",1
"Inducing sparseness while training neural networks has been shown to yield models with a lower memory footprint but similar effectiveness to dense models. However, sparseness is typically induced starting from a dense model, and thus this advantage does not hold during training. We propose techniques to enforce sparseness upfront in recurrent sequence models for NLP applications, to also benefit training. First, in language modeling, we show how to increase hidden state sizes in recurrent layers without increasing the number of parameters, leading to more expressive models. Second, for sequence labeling, we show that word embeddings with predefined sparseness lead to similar performance as dense embeddings, at a fraction of the number of trainable parameters.",0
"The act of inducing sparseness in neural networks during training has been demonstrated to result in models that require less memory but are equally effective as dense models. However, this advantage is not present during training since sparseness is usually induced from a dense model. To address this, we suggest methods to impose sparseness directly in recurrent sequence models used for natural language processing (NLP) applications, which will also improve training. Firstly, we demonstrate how to increase the size of hidden states in recurrent layers for language modeling without increasing the number of parameters, resulting in more expressive models. Secondly, we show that predetermined sparse word embeddings for sequence labeling perform similarly to dense embeddings but have fewer trainable parameters.",1
"Adversarial examples are inputs to machine learning models designed to cause the model to make a mistake. They are useful for understanding the shortcomings of machine learning models, interpreting their results, and for regularisation. In NLP, however, most example generation strategies produce input text by using known, pre-specified semantic transformations, requiring significant manual effort and in-depth understanding of the problem and domain. In this paper, we investigate the problem of automatically generating adversarial examples that violate a set of given First-Order Logic constraints in Natural Language Inference (NLI). We reduce the problem of identifying such adversarial examples to a combinatorial optimisation problem, by maximising a quantity measuring the degree of violation of such constraints and by using a language model for generating linguistically-plausible examples. Furthermore, we propose a method for adversarially regularising neural NLI models for incorporating background knowledge. Our results show that, while the proposed method does not always improve results on the SNLI and MultiNLI datasets, it significantly and consistently increases the predictive accuracy on adversarially-crafted datasets -- up to a 79.6% relative improvement -- while drastically reducing the number of background knowledge violations. Furthermore, we show that adversarial examples transfer among model architectures, and that the proposed adversarial training procedure improves the robustness of NLI models to adversarial examples.",0
"Adversarial examples are inputs given to machine learning models to induce errors and expose their weaknesses. They aid in comprehending the limitations of machine learning models, interpreting their outcomes, and promoting regularisation. However, generating adversarial examples in natural language processing requires significant manual effort and expertise in the problem and domain, as most strategies entail known, pre-specified semantic transformations. This study explores the automatic generation of adversarial examples that breach a set of predefined First-Order Logic restrictions in Natural Language Inference (NLI). We transform the identification of such adversarial examples into a combinatorial optimisation problem by maximising a metric that gauges the extent of constraint violation and using a language model to generate linguistically-credible examples. Additionally, we recommend a technique for adversarially regularising neural NLI models by incorporating background knowledge. Our results indicate that although the proposed method does not always enhance SNLI and MultiNLI datasets' performance, it consistently and significantly improves the predictive accuracy of adversarially-crafted datasets by up to a 79.6% relative increase while reducing the number of background knowledge violations. Furthermore, we demonstrate that adversarial examples are transferable across model architectures and that the proposed adversarial training approach enhances NLI models' resilience to adversarial examples.",1
"Extracting information from electronic health records (EHR) is a challenging task since it requires prior knowledge of the reports and some natural language processing algorithm (NLP). With the growing number of EHR implementations, such knowledge is increasingly challenging to obtain in an efficient manner. We address this challenge by proposing a novel methodology to analyze large sets of EHRs using a modified Count Sketch data streaming algorithm termed DreamNLP. By using DreamNLP, we generate a dictionary of frequently occurring terms or heavy hitters in the EHRs using low computational memory compared to conventional counting approach other NLP programs use. We demonstrate the extraction of the most important breast diagnosis features from the EHRs in a set of patients that underwent breast imaging. Based on the analysis, extraction of these terms would be useful for defining important features for downstream tasks such as machine learning for precision medicine.",0
"It is difficult to extract information from electronic health records (EHR) as it requires knowledge of the reports and natural language processing algorithms (NLP). Obtaining this knowledge efficiently is increasingly difficult with the rise of EHR implementations. To overcome this challenge, we propose a new methodology called DreamNLP, which uses a modified Count Sketch data streaming algorithm to analyze large sets of EHRs. DreamNLP generates a dictionary of frequently occurring terms in the EHRs using less computational memory than conventional NLP programs. We demonstrate the extraction of important breast diagnosis features from EHRs of patients who underwent breast imaging. This analysis can be useful for downstream tasks such as machine learning for precision medicine.",1
"Graph Convolutional Neural Networks (GCNNs) are the most recent exciting advancement in deep learning field and their applications are quickly spreading in multi-cross-domains including bioinformatics, chemoinformatics, social networks, natural language processing and computer vision. In this paper, we expose and tackle some of the basic weaknesses of a GCNN model with a capsule idea presented in \cite{hinton2011transforming} and propose our Graph Capsule Network (GCAPS-CNN) model. In addition, we design our GCAPS-CNN model to solve especially graph classification problem which current GCNN models find challenging. Through extensive experiments, we show that our proposed Graph Capsule Network can significantly outperforms both the existing state-of-art deep learning methods and graph kernels on graph classification benchmark datasets.",0
"The field of deep learning has been recently revolutionized by Graph Convolutional Neural Networks (GCNNs), which have found widespread application in various domains such as bioinformatics, chemoinformatics, social networks, natural language processing, and computer vision. This paper addresses some of the fundamental flaws of the GCNN model presented in \cite{hinton2011transforming} and introduces the Graph Capsule Network (GCAPS-CNN) model. The GCAPS-CNN model has been specifically designed to tackle graph classification problems that are known to be challenging for current GCNN models. Our extensive experiments demonstrate that the proposed Graph Capsule Network model outperforms state-of-the-art deep learning methods and graph kernels on benchmark datasets for graph classification.",1
"In this paper, we propose Dynamic Self-Attention (DSA), a new self-attention mechanism for sentence embedding. We design DSA by modifying dynamic routing in capsule network (Sabouretal.,2017) for natural language processing. DSA attends to informative words with a dynamic weight vector. We achieve new state-of-the-art results among sentence encoding methods in Stanford Natural Language Inference (SNLI) dataset with the least number of parameters, while showing comparative results in Stanford Sentiment Treebank (SST) dataset.",0
"The introduction of Dynamic Self-Attention (DSA) is outlined in this paper as a novel approach for sentence embedding. Our design is based on the modification of dynamic routing in capsule network (Sabouretal.,2017) for natural language processing. DSA utilizes a dynamic weight vector to focus on significant words for better attention. Through our proposed method, we obtain the latest and most exceptional results on the Stanford Natural Language Inference (SNLI) dataset with the least number of parameters, and we demonstrate comparable outcomes in the Stanford Sentiment Treebank (SST) dataset.",1
"Many vision-language tasks can be reduced to the problem of sequence prediction for natural language output. In particular, recent advances in image captioning use deep reinforcement learning (RL) to alleviate the ""exposure bias"" during training: ground-truth subsequence is exposed in every step prediction, which introduces bias in test when only predicted subsequence is seen. However, existing RL-based image captioning methods only focus on the language policy while not the visual policy (e.g., visual attention), and thus fail to capture the visual context that are crucial for compositional reasoning such as visual relationships (e.g., ""man riding horse"") and comparisons (e.g., ""smaller cat""). To fill the gap, we propose a Context-Aware Visual Policy network (CAVP) for sequence-level image captioning. At every time step, CAVP explicitly accounts for the previous visual attentions as the context, and then decides whether the context is helpful for the current word generation given the current visual attention. Compared against traditional visual attention that only fixes a single image region at every step, CAVP can attend to complex visual compositions over time. The whole image captioning model --- CAVP and its subsequent language policy network --- can be efficiently optimized end-to-end by using an actor-critic policy gradient method with respect to any caption evaluation metric. We demonstrate the effectiveness of CAVP by state-of-the-art performances on MS-COCO offline split and online server, using various metrics and sensible visualizations of qualitative visual context. The code is available at https://github.com/daqingliu/CAVP",0
"The problem of producing natural language output for vision-language tasks can often be reduced to sequence prediction. Recent developments in deep reinforcement learning (RL) have been employed in image captioning to overcome the ""exposure bias"" issue during training, where only ground-truth subsequence is exposed in each prediction step leading to bias in testing when only predicted subsequence is seen. However, current RL-based image captioning methods focus only on language policy and not visual policy, such as visual attention. This results in failure to capture important visual context required for compositional reasoning such as visual relationships and comparisons. To address this issue, we introduce the Context-Aware Visual Policy network (CAVP) for sequence-level image captioning, which accounts for previous visual attentions as context and decides whether it is useful for current word generation considering current visual attention. CAVP can attend to complex visual compositions over time, unlike traditional visual attention, which is limited to a single image region at each step. Our model can be efficiently optimized end-to-end using an actor-critic policy gradient method with respect to any caption evaluation metric. We demonstrate the effectiveness of CAVP through state-of-the-art performance on MS-COCO offline split and online server, using various metrics and visualizations of qualitative visual context. Code is available at https://github.com/daqingliu/CAVP.",1
"Developing techniques for editing an outfit image through natural sentences and accordingly generating new outfits has promising applications for art, fashion and design. However, it is considered as a certainly challenging task since image manipulation should be carried out only on the relevant parts of the image while keeping the remaining sections untouched. Moreover, this manipulation process should generate an image that is as realistic as possible. In this work, we propose FiLMedGAN, which leverages feature-wise linear modulation (FiLM) to relate and transform visual features with natural language representations without using extra spatial information. Our experiments demonstrate that this approach, when combined with skip connections and total variation regularization, produces more plausible results than the baseline work, and has a better localization capability when generating new outfits consistent with the target description.",0
"The development of techniques for editing outfit images and creating new outfits based on natural language sentences has numerous applications in the fields of art, fashion, and design. However, this is a challenging task since editing should only occur on relevant parts of the image while preserving the rest. Additionally, the edited image must appear as realistic as possible. To address this challenge, we propose FiLMedGAN, which employs feature-wise linear modulation (FiLM) to transform visual features based on natural language without the need for extra spatial information. Our experiments demonstrate that incorporating skip connections and total variation regularization to FiLMedGAN produces more plausible results compared to the baseline approach and is better at localizing new outfits that match the target description.",1
"Computational modeling of human multimodal language is an emerging research area in natural language processing spanning the language, visual and acoustic modalities. Comprehending multimodal language requires modeling not only the interactions within each modality (intra-modal interactions) but more importantly the interactions between modalities (cross-modal interactions). In this paper, we propose the Recurrent Multistage Fusion Network (RMFN) which decomposes the fusion problem into multiple stages, each of them focused on a subset of multimodal signals for specialized, effective fusion. Cross-modal interactions are modeled using this multistage fusion approach which builds upon intermediate representations of previous stages. Temporal and intra-modal interactions are modeled by integrating our proposed fusion approach with a system of recurrent neural networks. The RMFN displays state-of-the-art performance in modeling human multimodal language across three public datasets relating to multimodal sentiment analysis, emotion recognition, and speaker traits recognition. We provide visualizations to show that each stage of fusion focuses on a different subset of multimodal signals, learning increasingly discriminative multimodal representations.",0
"The research area of computational modeling for human multimodal language is growing in the field of natural language processing, encompassing language, visual, and acoustic modalities. To comprehend multimodal language, it is necessary to model interactions within each modality (intra-modal interactions) as well as interactions between modalities (cross-modal interactions). This paper proposes the Recurrent Multistage Fusion Network (RMFN), which divides the fusion problem into various stages, each focusing on a subset of multimodal signals for specialized and effective fusion. The multistage fusion approach models cross-modal interactions by building on intermediate representations of previous stages. The proposed fusion approach integrates with a system of recurrent neural networks to model temporal and intra-modal interactions. The RMFN exhibits outstanding performance in modeling human multimodal language across three public datasets for multimodal sentiment analysis, emotion recognition, and speaker traits recognition. The paper provides visualizations that indicate each stage of fusion concentrates on a different subset of multimodal signals, learning increasingly discriminative multimodal representations.",1
