"Adversarial training has been proven to be a powerful regularization method to improve the generalization of models. However, current adversarial training methods only attack the original input sample or the embedding vectors, and their attacks lack coverage and diversity. To further enhance the breadth and depth of attack, we propose a novel masked weight adversarial training method called DropAttack, which enhances generalization of model by adding intentionally worst-case adversarial perturbations to both the input and hidden layers in different dimensions and minimize the adversarial risks generated by each layer. DropAttack is a general technique and can be adopt to a wide variety of neural networks with different architectures. To validate the effectiveness of the proposed method, we used five public datasets in the fields of natural language processing (NLP) and computer vision (CV) for experimental evaluating. We compare the proposed method with other adversarial training methods and regularization methods, and our method achieves state-of-the-art on all datasets. In addition, Dropattack can achieve the same performance when it use only a half training data compared to other standard training method. Theoretical analysis reveals that DropAttack can perform gradient regularization at random on some of the input and wight parameters of the model. Further visualization experiments show that DropAttack can push the minimum risk of the model to a lower and flatter loss landscapes. Our source code is publicly available on https://github.com/nishiwen1214/DropAttack.",0
"Although adversarial training has been shown to effectively improve model generalization, current methods only target the original input or embedding vectors, limiting their coverage and diversity. To address this, we introduce a new method called DropAttack, which applies worst-case adversarial perturbations to both input and hidden layers, across different dimensions, to enhance model generalization. DropAttack is a general technique that can be applied to various neural network architectures, and we demonstrate its effectiveness through experimental evaluation on five public datasets in natural language processing and computer vision. Compared to other adversarial and regularization methods, DropAttack achieves state-of-the-art performance on all datasets, and can do so with only half the training data. Theoretical analysis shows that DropAttack can perform gradient regularization on some input and weight parameters, while visualization experiments reveal that it can push the minimum risk of the model to a lower and flatter loss landscape. Our source code is available on GitHub.",1
"Multimodal sentiment analysis aims to recognize people's attitudes from multiple communication channels such as verbal content (i.e., text), voice, and facial expressions. It has become a vibrant and important research topic in natural language processing. Much research focuses on modeling the complex intra- and inter-modal interactions between different communication channels. However, current multimodal models with strong performance are often deep-learning-based techniques and work like black boxes. It is not clear how models utilize multimodal information for sentiment predictions. Despite recent advances in techniques for enhancing the explainability of machine learning models, they often target unimodal scenarios (e.g., images, sentences), and little research has been done on explaining multimodal models. In this paper, we present an interactive visual analytics system, M2Lens, to visualize and explain multimodal models for sentiment analysis. M2Lens provides explanations on intra- and inter-modal interactions at the global, subset, and local levels. Specifically, it summarizes the influence of three typical interaction types (i.e., dominance, complement, and conflict) on the model predictions. Moreover, M2Lens identifies frequent and influential multimodal features and supports the multi-faceted exploration of model behaviors from language, acoustic, and visual modalities. Through two case studies and expert interviews, we demonstrate our system can help users gain deep insights into the multimodal models for sentiment analysis.",0
"The objective of multimodal sentiment analysis is to identify people's attitudes through various communication channels like text, voice, and facial expressions. This has emerged as a crucial research topic in natural language processing. The focus of much research is on comprehending the intricate interactions between different communication channels. However, deep-learning-based multimodal models that have high performance are often like black boxes, and it is unclear how they use multimodal information for sentiment predictions. Despite recent advancements in explaining machine learning models, little research has been done on explaining multimodal models. Therefore, this paper presents an interactive visual analytics system, M2Lens, that explains and visualizes multimodal models for sentiment analysis. M2Lens allows explanations on three typical interaction types and identifies influential multimodal features. It supports the exploration of model behaviors from language, acoustic, and visual modalities. Through two case studies and expert interviews, it is demonstrated that M2Lens can help users gain deep insights into multimodal models for sentiment analysis.",1
"Deep neural network (DNN) model compression for efficient on-device inference is becoming increasingly important to reduce memory requirements and keep user data on-device. To this end, we propose a novel differentiable k-means clustering layer (DKM) and its application to train-time weight clustering-based DNN model compression. DKM casts k-means clustering as an attention problem and enables joint optimization of the parameters and clustering centroids. Unlike prior works that rely on additional regularizers and parameters, DKM-based compression keeps the original loss function and model architecture fixed. We evaluated DKM-based compression on various DNN models for computer vision and natural language processing (NLP) tasks. Our results demonstrate that DMK delivers superior compression and accuracy trade-off on ImageNet1k and GLUE benchmarks. For example, DKM-based compression can offer 74.5% top-1 ImageNet1k accuracy on ResNet50 DNN model with 3.3MB model size (29.4x model compression factor). For MobileNet-v1, which is a challenging DNN to compress, DKM delivers 62.8% top-1 ImageNet1k accuracy with 0.74 MB model size (22.4x model compression factor). This result is 6.8% higher top-1 accuracy and 33% relatively smaller model size than the current state-of-the-art DNN compression algorithms. Additionally, DKM enables compression of DistilBERT model by 11.8x with minimal (1.1%) accuracy loss on GLUE NLP benchmarks.",0
"Efficient on-device inference through Deep neural network (DNN) model compression is increasingly important to reduce memory requirements and keep user data on-device. We propose a novel approach, the differentiable k-means clustering layer (DKM), for DNN model compression during training. DKM addresses k-means clustering as an attention problem, allowing joint optimization of parameters and clustering centroids. Unlike prior works, DKM-based compression keeps the original loss function and model architecture fixed, without relying on additional regularizers and parameters. We evaluated DKM-based compression on various DNN models for computer vision and natural language processing (NLP) tasks, and our results show superior compression and accuracy trade-offs on ImageNet1k and GLUE benchmarks. For instance, DKM-based compression achieved 74.5% top-1 ImageNet1k accuracy on ResNet50 DNN model with only 3.3MB model size (29.4x model compression factor). DKM also delivered 62.8% top-1 ImageNet1k accuracy on challenging MobileNet-v1 with 0.74 MB model size (22.4x model compression factor), which is 6.8% higher top-1 accuracy and 33% relatively smaller model size than the current state-of-the-art DNN compression algorithms. Furthermore, DKM enables DistilBERT model compression by 11.8x with minimal (1.1%) accuracy loss on GLUE NLP benchmarks.",1
"Spatio-temporal representational learning has been widely adopted in various fields such as action recognition, video object segmentation, and action anticipation. Previous spatio-temporal representational learning approaches primarily employ ConvNets or sequential models,e.g., LSTM, to learn the intra-frame and inter-frame features. Recently, Transformer models have successfully dominated the study of natural language processing (NLP), image classification, etc. However, the pure-Transformer based spatio-temporal learning can be prohibitively costly on memory and computation to extract fine-grained features from a tiny patch. To tackle the training difficulty and enhance the spatio-temporal learning, we construct a shifted chunk Transformer with pure self-attention blocks. Leveraging the recent efficient Transformer design in NLP, this shifted chunk Transformer can learn hierarchical spatio-temporal features from a local tiny patch to a global video clip. Our shifted self-attention can also effectively model complicated inter-frame variances. Furthermore, we build a clip encoder based on Transformer to model long-term temporal dependencies. We conduct thorough ablation studies to validate each component and hyper-parameters in our shifted chunk Transformer, and it outperforms previous state-of-the-art approaches on Kinetics-400, Kinetics-600, UCF101, and HMDB51. Code and trained models will be released.",0
"Spatio-temporal representational learning has been extensively utilized in various fields, including action recognition, video object segmentation, and action anticipation. Traditional approaches to spatio-temporal representational learning have mainly employed ConvNets or sequential models, such as LSTM, to learn intra-frame and inter-frame features. While Transformer models have recently dominated in natural language processing (NLP), image classification, and other areas, pure-Transformer based spatio-temporal learning can be prohibitively expensive in terms of memory and computation, particularly when extracting fine-grained features from a small patch. To overcome these training difficulties and enhance spatio-temporal learning, we introduce a shifted chunk Transformer with pure self-attention blocks. By leveraging efficient Transformer design in NLP, our shifted chunk Transformer can learn hierarchical spatio-temporal features from a local tiny patch to a global video clip. Our shifted self-attention can also effectively capture complex inter-frame variances. Additionally, we construct a clip encoder based on Transformer to model long-term temporal dependencies. Our thorough ablation studies validate each component and hyper-parameter in our shifted chunk Transformer, which outperforms previous state-of-the-art approaches on Kinetics-400, Kinetics-600, UCF101, and HMDB51. Code and trained models will be made available.",1
"We introduce MADGRAD, a novel optimization method in the family of AdaGrad adaptive gradient methods. MADGRAD shows excellent performance on deep learning optimization problems from multiple fields, including classification and image-to-image tasks in vision, and recurrent and bidirectionally-masked models in natural language processing. For each of these tasks, MADGRAD matches or outperforms both SGD and ADAM in test set performance, even on problems for which adaptive methods normally perform poorly.",0
"MADGRAD, an innovative optimization method belonging to the AdaGrad adaptive gradient methods, is presented. It demonstrates outstanding results in optimizing deep learning problems across various domains such as vision, including classification and image-to-image tasks, and natural language processing, including recurrent and bidirectionally-masked models. In all these tasks, MADGRAD's performance on the test set is equal to or better than SGD and ADAM, even when adaptive methods usually struggle.",1
"Transformers have shown impressive performance in various natural language processing and computer vision tasks, due to the capability of modeling long-range dependencies. Recent progress has demonstrated to combine such transformers with CNN-based semantic image segmentation models is very promising. However, it is not well studied yet on how well a pure transformer based approach can achieve for image segmentation. In this work, we explore a novel framework for semantic image segmentation, which is encoder-decoder based Fully Transformer Networks (FTN). Specifically, we first propose a Pyramid Group Transformer (PGT) as the encoder for progressively learning hierarchical features, while reducing the computation complexity of the standard visual transformer(ViT). Then, we propose a Feature Pyramid Transformer (FPT) to fuse semantic-level and spatial-level information from multiple levels of the PGT encoder for semantic image segmentation. Surprisingly, this simple baseline can achieve new state-of-the-art results on multiple challenging semantic segmentation benchmarks, including PASCAL Context, ADE20K and COCO-Stuff. The source code will be released upon the publication of this work.",0
"The ability of transformers to model long-range dependencies has led to impressive performance in natural language processing and computer vision tasks. Recent advancements have shown that combining transformers with CNN-based semantic image segmentation models is promising. However, the effectiveness of a pure transformer-based approach for image segmentation has not been thoroughly studied. In this study, we introduce a novel framework for semantic image segmentation called Fully Transformer Networks (FTN), which employs an encoder-decoder architecture. Our approach includes a Pyramid Group Transformer (PGT) as the encoder, which progressively learns hierarchical features and reduces computation complexity. Additionally, we propose a Feature Pyramid Transformer (FPT) that combines semantic-level and spatial-level information from different levels of the PGT encoder to improve semantic image segmentation. Surprisingly, our simple baseline model outperforms existing state-of-the-art methods on challenging semantic segmentation benchmarks like PASCAL Context, ADE20K, and COCO-Stuff. We will release the source code upon publication of this work.",1
"Web Image Context Extraction (WICE) consists in obtaining the textual information describing an image using the content of the surrounding webpage. A common preprocessing step before performing WICE is to render the content of the webpage. When done at a large scale (e.g., for search engine indexation), it may become very computationally costly (up to several seconds per page). To avoid this cost, we introduce a novel WICE approach that combines Graph Neural Networks (GNNs) and Natural Language Processing models. Our method relies on a graph model containing both node types and text as features. The model is fed through several blocks of GNNs to extract the textual context. Since no labeled WICE dataset with ground truth exists, we train and evaluate the GNNs on a proxy task that consists in finding the semantically closest text to the image caption. We then interpret importance weights to find the most relevant text nodes and define them as the image context. Thanks to GNNs, our model is able to encode both structural and semantic information from the webpage. We show that our approach gives promising results to help address the large-scale WICE problem using only HTML data.",0
"The concept of Web Image Context Extraction (WICE) involves using the content of a webpage to gather textual information about an image. However, rendering the webpage content before performing WICE can be computationally expensive, especially at a large scale. To avoid this issue, we present a new WICE method that combines Graph Neural Networks (GNNs) and Natural Language Processing models. Our approach utilizes a graph model with node types and text features, and employs multiple blocks of GNNs to extract the textual context. Since there is no labeled WICE dataset with ground truth, we train and evaluate the GNNs on a proxy task of finding semantically similar text to the image caption. We use importance weights to identify the most relevant text nodes, which we define as the image context. By leveraging GNNs, our model can encode both structural and semantic information from the webpage, making it a promising solution for large-scale WICE problems using HTML data.",1
"There is an increasing demand for scalable algorithms capable of clustering and analyzing large time series datasets. The Kohonen self-organizing map (SOM) is a type of unsupervised artificial neural network for visualizing and clustering complex data, reducing the dimensionality of data, and selecting influential features. Like all clustering methods, the SOM requires a measure of similarity between input data (in this work time series). Dynamic time warping (DTW) is one such measure, and a top performer given that it accommodates the distortions when aligning time series. Despite its use in clustering, DTW is limited in practice because it is quadratic in runtime complexity with the length of the time series data. To address this, we present a new DTW-based clustering method, called SOMTimeS (a Self-Organizing Map for TIME Series), that scales better and runs faster than other DTW-based clustering algorithms, and has similar performance accuracy. The computational performance of SOMTimeS stems from its ability to prune unnecessary DTW computations during the SOM's training phase. We also implemented a similar pruning strategy for K-means for comparison with one of the top performing clustering algorithms. We evaluated the pruning effectiveness, accuracy, execution time and scalability on 112 benchmark time series datasets from the University of California, Riverside classification archive. We showed that for similar accuracy, the speed-up achieved for SOMTimeS and K-means was 1.8x on average; however, rates varied between 1x and 18x depending on the dataset. SOMTimeS and K-means pruned 43% and 50% of the total DTW computations, respectively. We applied SOMtimeS to natural language conversation data collected as part of a large healthcare cohort study of patient-clinician serious illness conversations to demonstrate the algorithm's utility with complex, temporally sequenced phenomena.",0
"The demand for scalable algorithms capable of clustering and analyzing large time series datasets is increasing. The Kohonen self-organizing map (SOM) is an unsupervised artificial neural network that reduces data dimensionality and selects influential features. However, the SOM requires a measure of similarity between input data, such as time series, and Dynamic time warping (DTW) is a commonly used measure that accommodates distortions when aligning time series. Unfortunately, DTW is limited in practice due to its quadratic runtime complexity. To address this, we propose SOMTimeS, a new DTW-based clustering method that prunes unnecessary computations during the SOM's training phase, resulting in faster runtime and similar accuracy to other DTW-based clustering algorithms. We also implement a similar pruning strategy for K-means for comparison. Our evaluation on 112 benchmark time series datasets shows that SOMTimeS and K-means prune 43% and 50% of total DTW computations, respectively, resulting in a speed-up of 1.8x on average for similar accuracy. The rate of speed-up varies between 1x and 18x depending on the dataset. We also demonstrate the utility of SOMTimeS on natural language conversation data from a healthcare cohort study.",1
"Interaction and navigation defined by natural language instructions in dynamic environments pose significant challenges for neural agents. This paper focuses on addressing two challenges: handling long sequence of subtasks, and understanding complex 0 instructions. We propose Episodic Transformer (E.T.), a multimodal transformer that encodes language inputs and the full episode history of visual observations and actions. To improve training, we leverage synthetic instructions as an intermediate representation that decouples understanding the visual appearance of an environment from the variations of natural language instructions. We demonstrate that encoding the history with a transformer is critical to solve compositional tasks, and that pretraining and joint training with synthetic instructions further improve the performance. Our approach sets a new state of the art on the challenging ALFRED benchmark, achieving 38.4% and 8.5% task success rates on seen and unseen test splits.",0
"Neural agents face significant difficulties when it comes to interacting and navigating in dynamic environments based on natural language instructions. This study addresses two specific challenges: managing long sequences of subtasks and comprehending complex 0 instructions. The solution proposed is the Episodic Transformer (E.T.), which is a multimodal transformer that encodes language inputs and the entire history of visual observations and actions. To enhance training, synthetic instructions are utilized as an intermediate representation that separates understanding the visual aspects of an environment from the variations in natural language instructions. The study demonstrates that encoding history with a transformer is essential for solving compositional tasks, and that pretraining and joint training with synthetic instructions further improves performance. Our approach outperforms the current state of the art on the ALFRED benchmark, achieving 38.4% and 8.5% task success rates on seen and unseen test splits.",1
"We investigate Referring Image Segmentation (RIS), which outputs a segmentation map corresponding to the given natural language description. To solve RIS efficiently, we need to understand each word's relationship with other words, each region in the image to other regions, and cross-modal alignment between linguistic and visual domains. We argue that one of the limiting factors in the recent methods is that they do not handle these interactions simultaneously. To this end, we propose a novel architecture called JRNet, which uses a Joint Reasoning Module(JRM) to concurrently capture the inter-modal and intra-modal interactions. The output of JRM is passed through a novel Cross-Modal Multi-Level Fusion (CMMLF) module which further refines the segmentation masks by exchanging contextual information across visual hierarchy through linguistic features acting as a bridge. We present thorough ablation studies and validate our approach's performance on four benchmark datasets, showing considerable performance gains over the existing state-of-the-art methods.",0
"In this study, we explore Referring Image Segmentation (RIS), which generates a segmentation map based on a natural language description. To effectively solve RIS, it is crucial to comprehend the relationships between words, regions in the image, and the alignment between linguistic and visual domains. However, previous methods have failed to address these interactions simultaneously, limiting their effectiveness. Therefore, we introduce a new architecture, JRNet, that utilizes a Joint Reasoning Module (JRM) to capture both inter-modal and intra-modal interactions concurrently. The resulting output from JRM is then refined through a Cross-Modal Multi-Level Fusion (CMMLF) module that exchanges contextual information across visual hierarchy through linguistic features. We conduct extensive ablation studies and demonstrate the superior performance of our approach on four benchmark datasets compared to existing state-of-the-art methods.",1
"Activation functions play a pivotal role in determining the training dynamics and neural network performance. The widely adopted activation function ReLU despite being simple and effective has few disadvantages including the Dying ReLU problem. In order to tackle such problems, we propose a novel activation function called Serf which is self-regularized and nonmonotonic in nature. Like Mish, Serf also belongs to the Swish family of functions. Based on several experiments on computer vision (image classification and object detection) and natural language processing (machine translation, sentiment classification and multimodal entailment) tasks with different state-of-the-art architectures, it is observed that Serf vastly outperforms ReLU (baseline) and other activation functions including both Swish and Mish, with a markedly bigger margin on deeper architectures. Ablation studies further demonstrate that Serf based architectures perform better than those of Swish and Mish in varying scenarios, validating the effectiveness and compatibility of Serf with varying depth, complexity, optimizers, learning rates, batch sizes, initializers and dropout rates. Finally, we investigate the mathematical relation between Swish and Serf, thereby showing the impact of preconditioner function ingrained in the first derivative of Serf which provides a regularization effect making gradients smoother and optimization faster.",0
"The performance and training dynamics of neural networks are largely impacted by activation functions. Despite the effectiveness of the commonly used ReLU activation function, it has some drawbacks, such as the Dying ReLU problem. To address these issues, a new activation function called Serf has been proposed. It belongs to the Swish family of functions, like Mish, and is self-regularized and nonmonotonic in nature. Several experiments were conducted on image classification, object detection, machine translation, sentiment classification, and multimodal entailment tasks with different architectures. The results showed that Serf outperformed ReLU and other functions, especially on deeper architectures. Ablation studies also confirmed that Serf-based architectures were more effective in various scenarios than those of Swish and Mish. Furthermore, the mathematical relationship between Swish and Serf was investigated, revealing the impact of the preconditioner function on the first derivative of Serf, which made gradients smoother and optimization faster.",1
"Most existing image retrieval systems use text queries as a way for the user to express what they are looking for. However, fine-grained image retrieval often requires the ability to also express where in the image the content they are looking for is. The text modality can only cumbersomely express such localization preferences, whereas pointing is a more natural fit. In this paper, we propose an image retrieval setup with a new form of multimodal queries, where the user simultaneously uses both spoken natural language (the what) and mouse traces over an empty canvas (the where) to express the characteristics of the desired target image. We then describe simple modifications to an existing image retrieval model, enabling it to operate in this setup. Qualitative and quantitative experiments show that our model effectively takes this spatial guidance into account, and provides significantly more accurate retrieval results compared to text-only equivalent systems.",0
"Currently, most image retrieval systems rely on text queries to understand the user's search criteria. However, when it comes to fine-grained image retrieval, it becomes essential for the user to specify the location of the desired content within the image. Using text to express such localization preferences can be complicated, whereas pointing is a more intuitive option. This paper proposes a novel approach to image retrieval that combines spoken natural language (the ""what"") with mouse traces on an empty canvas (the ""where"") to specify the target image's characteristics. The existing image retrieval model is modified to function in this new setup. The model's effectiveness is evaluated through qualitative and quantitative experiments, which demonstrate that it successfully accounts for spatial guidance and yields significantly more accurate retrieval results compared to text-only systems.",1
"Facial action unit (FAU) intensities are popular descriptors for the analysis of facial behavior. However, FAUs are sparsely represented when only a few are activated at a time. In this study, we explore the possibility of representing the dynamics of facial expressions by adopting algorithms used for word representation in natural language processing. Specifically, we perform clustering on a large dataset of temporal facial expressions with 5.3M frames before applying the Global Vector representation (GloVe) algorithm to learn the embeddings of the facial clusters. We evaluate the usefulness of our learned representations on two downstream tasks: schizophrenia symptom estimation and depression severity regression. These experimental results show the potential effectiveness of our approach for improving the assessment of mental health symptoms over baseline models that use FAU intensities alone.",0
"The analysis of facial behavior often relies on the intensity of Facial Action Units (FAUs). However, only a small number of FAUs may be activated at any given time, leading to incomplete representation. This study examines a new approach to capture the dynamics of facial expressions using algorithms typically used in natural language processing to represent words. The researchers performed clustering on a large dataset of temporal facial expressions and used the Global Vector representation (GloVe) algorithm to learn the embeddings of the facial clusters. The effectiveness of this approach was evaluated on two tasks related to mental health: estimating schizophrenia symptoms and regressing depression severity. Results suggest that this approach has potential for improving the assessment of mental health symptoms compared to traditional models that rely solely on FAU intensities.",1
"Infographics are documents designed to effectively communicate information using a combination of textual, graphical and visual elements. In this work, we explore the automatic understanding of infographic images by using Visual Question Answering technique.To this end, we present InfographicVQA, a new dataset that comprises a diverse collection of infographics along with natural language questions and answers annotations. The collected questions require methods to jointly reason over the document layout, textual content, graphical elements, and data visualizations. We curate the dataset with emphasis on questions that require elementary reasoning and basic arithmetic skills. Finally, we evaluate two strong baselines based on state of the art multi-modal VQA models, and establish baseline performance for the new task. The dataset, code and leaderboard will be made available at http://docvqa.org",0
"The aim of this study is to investigate the comprehension of infographics using Visual Question Answering technique. Infographics are documents that use a combination of text, graphics and visuals to effectively communicate information. To achieve this goal, a new dataset called InfographicVQA has been created, consisting of a diverse range of infographics with annotations of natural language questions and answers. The questions require reasoning over various components of the document such as layout, text, graphics, and visualizations, with a focus on elementary reasoning and basic arithmetic skills. Two strong baselines based on state-of-the-art multi-modal VQA models have been evaluated, establishing a baseline performance for the new task. Interested parties can access the dataset, code and leaderboard at http://docvqa.org.",1
"Transformer architectures have become the model of choice in natural language processing and are now being introduced into computer vision tasks such as image classification, object detection, and semantic segmentation. However, in the field of 0 pose estimation, convolutional architectures still remain dominant. In this work, we present PoseFormer, a purely transformer-based approach for 3D 0 pose estimation in videos without convolutional architectures involved. Inspired by recent developments in vision transformers, we design a spatial-temporal transformer structure to comprehensively model the 0 joint relations within each frame as well as the temporal correlations across frames, then output an accurate 3D 0 pose of the center frame. We quantitatively and qualitatively evaluate our method on two popular and standard benchmark datasets: 03.6M and MPI-INF-3DHP. Extensive experiments show that PoseFormer achieves state-of-the-art performance on both datasets. Code is available at \url{https://github.com/zczcwh/PoseFormer}",0
"The use of transformer architectures has become increasingly popular in natural language processing and has also been applied to computer vision tasks including image classification, object detection, and semantic segmentation. Despite this, convolutional architectures continue to be the dominant approach in 0 pose estimation. This paper introduces PoseFormer, a transformer-based method for 3D 0 pose estimation in videos that does not rely on convolutional architectures. Drawing on recent developments in vision transformers, the proposed approach uses a spatial-temporal transformer structure to model 0 joint relations within each frame and temporal correlations across frames, resulting in accurate 3D 0 pose estimation for the center frame. The method is evaluated on two standard benchmark datasets, 03.6M and MPI-INF-3DHP, and achieves state-of-the-art performance. Code for implementation is available at \url{https://github.com/zczcwh/PoseFormer}.",1
"The concern regarding users' data privacy has risen to its highest level due to the massive increase in communication platforms, social networking sites, and greater users' participation in online public discourse. An increasing number of people exchange private information via emails, text messages, and social media without being aware of the risks and implications. Researchers in the field of Natural Language Processing (NLP) have concentrated on creating tools and strategies to identify, categorize, and sanitize private information in text data since a substantial amount of data is exchanged in textual form. However, most of the detection methods solely rely on the existence of pre-identified keywords in the text and disregard the inference of the underlying meaning of the utterance in a specific context. Hence, in some situations, these tools and algorithms fail to detect disclosure, or the produced results are miss-classified. In this paper, we propose a multi-input, multi-output hybrid neural network which utilizes transfer-learning, linguistics, and metadata to learn the hidden patterns. Our goal is to better classify disclosure/non-disclosure content in terms of the context of situation. We trained and evaluated our model on a 0-annotated ground truth dataset, containing a total of 5,400 tweets. The results show that the proposed model was able to identify privacy disclosure through tweets with an accuracy of 77.4% while classifying the information type of those tweets with an impressive accuracy of 99%, by jointly learning for two separate tasks.",0
"The issue of safeguarding users' data privacy has become increasingly urgent given the proliferation of communication platforms, social media, and online public discourse. Many individuals share sensitive information via email, text messages, and social media without fully grasping the potential risks. To address this issue, Natural Language Processing (NLP) researchers have focused on developing methods to identify and sanitize private information in text data. However, most of the existing detection methods rely solely on pre-identified keywords and fail to account for the underlying meaning of the text in its specific context. In this paper, we propose a novel approach that utilizes a multi-input, multi-output hybrid neural network incorporating transfer-learning, linguistics, and metadata to better classify content in terms of its context. Our model was trained and evaluated on a 0-annotated dataset of 5,400 tweets, achieving an accuracy of 77.4% in identifying privacy disclosures and a remarkable 99% accuracy in classifying the type of information disclosed.",1
"Describing images using natural language is widely known as image captioning, which has made consistent progress due to the development of computer vision and natural language generation techniques. Though conventional captioning models achieve high accuracy based on popular metrics, i.e., BLEU, CIDEr, and SPICE, the ability of captions to distinguish the target image from other similar images is under-explored. To generate distinctive captions, a few pioneers employ contrastive learning or re-weighted the ground-truth captions, which focuses on one single input image. However, the relationships between objects in a similar image group (e.g., items or properties within the same album or fine-grained events) are neglected. In this paper, we improve the distinctiveness of image captions using a Group-based Distinctive Captioning Model (GdisCap), which compares each image with other images in one similar group and highlights the uniqueness of each image. In particular, we propose a group-based memory attention (GMA) module, which stores object features that are unique among the image group (i.e., with low similarity to objects in other images). These unique object features are highlighted when generating captions, resulting in more distinctive captions. Furthermore, the distinctive words in the ground-truth captions are selected to supervise the language decoder and GMA. Finally, we propose a new evaluation metric, distinctive word rate (DisWordRate) to measure the distinctiveness of captions. Quantitative results indicate that the proposed method significantly improves the distinctiveness of several baseline models, and achieves the state-of-the-art performance on both accuracy and distinctiveness. Results of a user study agree with the quantitative evaluation and demonstrate the rationality of the new metric DisWordRate.",0
"The process of describing images using natural language is referred to as image captioning, which has made substantial advancements due to the development of computer vision and natural language generation techniques. Although conventional captioning models exhibit high accuracy based on popular metrics such as BLEU, CIDEr, and SPICE, their ability to differentiate the target image from other similar images has not been adequately explored. Some pioneers have attempted to generate distinctive captions by utilizing contrastive learning or re-weighting the ground-truth captions, but such approaches focus only on a single input image and overlook the relationships between objects in a similar image group. To address this issue, we introduce a Group-based Distinctive Captioning Model (GdisCap) that enhances the distinctiveness of image captions by comparing each image with other images in a similar group and highlighting its uniqueness. We propose a group-based memory attention (GMA) module that stores object features unique to the image group and highlights them while generating captions, resulting in more distinctive captions. Additionally, we use distinctive words from the ground-truth captions to supervise the language decoder and GMA. Finally, we introduce a new evaluation metric, distinctive word rate (DisWordRate), to measure the distinctiveness of captions. Our quantitative results demonstrate that the proposed method improves the distinctiveness of several baseline models and achieves state-of-the-art performance in terms of both accuracy and distinctiveness. The results of a user study support our quantitative evaluation and validate the rationality of the DisWordRate metric.",1
"Vision-and-language navigation (VLN) aims to enable embodied agents to navigate in realistic environments using natural language instructions. Given the scarcity of domain-specific training data and the high diversity of image and language inputs, the generalization of VLN agents to unseen environments remains challenging. Recent methods explore pretraining to improve generalization, however, the use of generic image-caption datasets or existing small-scale VLN environments is suboptimal and results in limited improvements. In this work, we introduce BnB, a large-scale and diverse in-domain VLN dataset. We first collect image-caption (IC) pairs from hundreds of thousands of listings from online rental marketplaces. Using IC pairs we next propose automatic strategies to generate millions of VLN path-instruction (PI) pairs. We further propose a shuffling loss that improves the learning of temporal order inside PI pairs. We use BnB pretrain our Airbert model that can be adapted to discriminative and generative settings and show that it outperforms state of the art for Room-to-Room (R2R) navigation and Remote Referring Expression (REVERIE) benchmarks. Moreover, our in-domain pretraining significantly increases performance on a challenging few-shot VLN evaluation, where we train the model only on VLN instructions from a few houses.",0
"The objective of Vision-and-language navigation (VLN) is to enable embodied agents to navigate in realistic environments using natural language instructions. However, due to the limited availability of domain-specific training data and the diversity of image and language inputs, generalizing VLN agents to unseen environments is a challenging task. Recent methods have attempted to improve generalization through pretraining, but using generic image-caption datasets or existing small-scale VLN environments has resulted in limited improvements. To address this issue, we present BnB, a large-scale and diverse in-domain VLN dataset. We collect image-caption pairs from online rental marketplaces and use automatic strategies to generate millions of VLN path-instruction pairs. We also propose a shuffling loss to improve the learning of temporal order within the pairs. Our Airbert model is pre-trained on BnB and can be adapted to both discriminative and generative settings. We demonstrate that it outperforms the state of the art for Room-to-Room navigation and Remote Referring Expression benchmarks. Furthermore, our in-domain pretraining significantly improves performance on a challenging few-shot VLN evaluation, where the model is trained only on VLN instructions from a few houses.",1
"Neural networks are well-known to be vulnerable to imperceptible perturbations in the input, called adversarial examples, that result in misclassification. Generating adversarial examples for source code poses an additional challenge compared to the domains of images and natural language, because source code perturbations must retain the functional meaning of the code. We identify a striking relationship between token frequency statistics and learned token embeddings: the L2 norm of learned token embeddings increases with the frequency of the token except for the highest-frequnecy tokens. We leverage this relationship to construct a simple and efficient gradient-free method for generating state-of-the-art adversarial examples on models of code. Our method empirically outperforms competing gradient-based methods with less information and less computational effort.",0
"Adversarial examples, imperceptible changes in input leading to misclassification, are a well-known vulnerability in neural networks. However, generating adversarial examples for source code poses an extra challenge as the perturbations must maintain the code's functional meaning. We have found that there is a correlation between token frequency statistics and learned token embeddings, where the L2 norm of the embeddings increases with token frequency, except for the highest-frequency tokens. Leveraging this relationship, we have developed a gradient-free method to generate state-of-the-art adversarial examples on code models, which outperforms competing gradient-based methods with less information and computational effort.",1
"Recent deep-learning models have achieved impressive predictive performance by learning complex functions of many variables, often at the cost of interpretability. This chapter covers recent work aiming to interpret models by attributing importance to features and feature groups for a single prediction. Importantly, the proposed attributions assign importance to interactions between features, in addition to features in isolation. These attributions are shown to yield insights across real-world domains, including bio-imaging, cosmology image and natural-language processing. We then show how these attributions can be used to directly improve the generalization of a neural network or to distill it into a simple model. Throughout the chapter, we emphasize the use of reality checks to scrutinize the proposed interpretation techniques.",0
"The latest deep-learning models have achieved remarkable prediction accuracy by comprehending intricate functions of numerous variables, but this often results in a lack of interpretability. This section discusses recent efforts to interpret models by assigning importance to features and groups of features for a single prediction. Crucially, the proposed attributions value the interactions between features, not just the features in isolation. These attributions have been proven to provide insights in various domains, such as bio-imaging, cosmology image, and natural-language processing. Additionally, we demonstrate how these attributions can be utilized to enhance the generalization of a neural network or to simplify it. We stress the importance of reality checks to validate the proposed interpretation methods throughout this chapter.",1
"Recently, there has been an increasing number of efforts to introduce models capable of generating natural language explanations (NLEs) for their predictions on vision-language (VL) tasks. Such models are appealing, because they can provide 0-friendly and comprehensive explanations. However, there is a lack of comparison between existing methods, which is due to a lack of re-usable evaluation frameworks and a scarcity of datasets. In this work, we introduce e-ViL and e-SNLI-VE. e-ViL is a benchmark for explainable vision-language tasks that establishes a unified evaluation framework and provides the first comprehensive comparison of existing approaches that generate NLEs for VL tasks. It spans four models and three datasets and both automatic metrics and 0 evaluation are used to assess model-generated explanations. e-SNLI-VE is currently the largest existing VL dataset with NLEs (over 430k instances). We also propose a new model that combines UNITER, which learns joint embeddings of images and text, and GPT-2, a pre-trained language model that is well-suited for text generation. It surpasses the previous state of the art by a large margin across all datasets. Code and data are available here: https://github.com/maximek3/e-ViL.",0
"In recent times, the emergence of models capable of generating natural language explanations (NLEs) for their predictions on vision-language (VL) tasks has increased significantly. These models are desirable because they provide 0-friendly and comprehensive explanations. However, the lack of re-usable evaluation frameworks and a scarcity of datasets has resulted in a deficiency of comparison between existing methods. This paper introduces e-ViL and e-SNLI-VE, which are a benchmark for explainable vision-language tasks and the largest existing VL dataset with NLEs, respectively. e-ViL establishes a unified evaluation framework and offers the first comprehensive comparison of existing approaches that generate NLEs for VL tasks. It encompasses four models and three datasets, and both automatic metrics and 0 evaluation are employed to evaluate model-generated explanations. Additionally, a new model combining UNITER and GPT-2 is proposed and surpasses the previous state of the art by a large margin across all datasets. The code and data are available at https://github.com/maximek3/e-ViL.",1
"We propose UniT, a Unified Transformer model to simultaneously learn the most prominent tasks across different domains, ranging from object detection to natural language understanding and multimodal reasoning. Based on the transformer encoder-decoder architecture, our UniT model encodes each input modality with an encoder and makes predictions on each task with a shared decoder over the encoded input representations, followed by task-specific output heads. The entire model is jointly trained end-to-end with losses from each task. Compared to previous efforts on multi-task learning with transformers, we share the same model parameters across all tasks instead of separately fine-tuning task-specific models and handle a much higher variety of tasks across different domains. In our experiments, we learn 7 tasks jointly over 8 datasets, achieving strong performance on each task with significantly fewer parameters. Our code is available in MMF at https://mmf.sh.",0
"The UniT model we propose is a Unified Transformer that can learn multiple tasks in various domains, such as object detection, natural language understanding, and multimodal reasoning, simultaneously. Using the transformer encoder-decoder architecture, the UniT model encodes each input modality with an encoder and predicts each task with a shared decoder over the encoded input representations, followed by task-specific output heads. Unlike previous works on multi-task learning with transformers, we use the same model parameters for all tasks and handle a wider range of tasks across various domains. The entire model is trained end-to-end with losses from each task. In our experiments, we jointly learn 7 tasks across 8 datasets with remarkable performance on each task using fewer parameters. Our code is available in MMF at https://mmf.sh.",1
"For many fundamental scene understanding tasks, it is difficult or impossible to obtain per-pixel ground truth labels from real images. We address this challenge by introducing Hypersim, a photorealistic synthetic dataset for holistic indoor scene understanding. To create our dataset, we leverage a large repository of synthetic scenes created by professional artists, and we generate 77,400 images of 461 indoor scenes with detailed per-pixel labels and corresponding ground truth geometry. Our dataset: (1) relies exclusively on publicly available 3D assets; (2) includes complete scene geometry, material information, and lighting information for every scene; (3) includes dense per-pixel semantic instance segmentations and complete camera information for every image; and (4) factors every image into diffuse reflectance, diffuse illumination, and a non-diffuse residual term that captures view-dependent lighting effects.   We analyze our dataset at the level of scenes, objects, and pixels, and we analyze costs in terms of money, computation time, and annotation effort. Remarkably, we find that it is possible to generate our entire dataset from scratch, for roughly half the cost of training a popular open-source natural language processing model. We also evaluate sim-to-real transfer performance on two real-world scene understanding tasks - semantic segmentation and 3D shape prediction - where we find that pre-training on our dataset significantly improves performance on both tasks, and achieves state-of-the-art performance on the most challenging Pix3D test set. All of our rendered image data, as well as all the code we used to generate our dataset and perform our experiments, is available online.",0
"Obtaining per-pixel ground truth labels from real images for many fundamental scene understanding tasks is a challenge. To address this issue, we introduce Hypersim, a synthetic dataset that offers complete indoor scene understanding in photorealistic quality. We leveraged publicly available 3D assets to generate 77,400 images of 461 indoor scenes, complete with detailed per-pixel labels, ground truth geometry, and lighting information. Our dataset also includes dense per-pixel semantic instance segmentations and complete camera information for every image, and factors every image into diffuse reflectance, diffuse illumination, and a non-diffuse residual term capturing view-dependent lighting effects. We conducted a cost analysis in terms of money, computation time, and annotation effort, and remarkably found that our dataset can be generated from scratch at roughly half the cost of training a popular open-source natural language processing model. Furthermore, we evaluated our dataset's sim-to-real transfer performance on two real-world scene understanding tasks - semantic segmentation and 3D shape prediction - and found that pre-training on our dataset significantly improves performance on both tasks, achieving state-of-the-art performance on the most challenging Pix3D test set. We have made all our rendered image data and code available online.",1
"Protein is linked to almost every life process. Therefore, analyzing the biological structure and property of protein sequences is critical to the exploration of life, as well as disease detection and drug discovery. Traditional protein analysis methods tend to be labor-intensive and time-consuming. The emergence of deep learning models makes modeling data patterns in large quantities of data possible. Interdisciplinary researchers have begun to leverage deep learning methods to model large biological datasets, e.g. using long short-term memory and convolutional neural network for protein sequence classification. After millions of years of evolution, evolutionary information is encoded in protein sequences. Inspired by the similarity between natural language and protein sequences, we use large-scale language models to model evolutionary-scale protein sequences, encoding protein biology information in representation. Significant improvements are observed in both token-level and sequence-level tasks, demonstrating that our large-scale model can accurately capture evolution information from pretraining on evolutionary-scale individual sequences. Our code and model are available at https://github.com/THUDM/ProteinLM.",0
"The study of protein is crucial for understanding many life processes, as well as detecting diseases and discovering drugs. However, traditional analysis methods are time-consuming and labor-intensive. With the emergence of deep learning models, researchers can now model large biological datasets, such as protein sequences, using techniques like long short-term memory and convolutional neural networks. Evolutionary information is encoded in protein sequences, and researchers have found success in using large-scale language models to represent this information. The results demonstrate that pretraining on evolutionary-scale individual sequences accurately captures evolution information. The code and model are available at https://github.com/THUDM/ProteinLM.",1
"Deep learning research has recently witnessed an impressively fast-paced progress in a wide range of tasks including computer vision, natural language processing, and reinforcement learning. The extraordinary performance of these systems often gives the impression that they can be used to revolutionise our lives for the better. However, as recent works point out, these systems suffer from several issues that make them unreliable for use in the real world, including vulnerability to adversarial attacks (Szegedy et al. [248]), tendency to memorise noise (Zhang et al. [292]), being over-confident on incorrect predictions (miscalibration) (Guo et al. [99]), and unsuitability for handling private data (Gilad-Bachrach et al. [88]). In this thesis, we look at each of these issues in detail, investigate their causes, and propose computationally cheap algorithms for mitigating them in practice. To do this, we identify structures in deep neural networks that can be exploited to mitigate the above causes of unreliability of deep learning algorithms.",0
"In recent times, the field of deep learning has made impressive strides in various tasks like natural language processing, computer vision, and reinforcement learning. These systems have shown extraordinary performance, leading to the perception that they can transform our lives for the better. However, recent research indicates that these systems have several drawbacks that render them unreliable for real-world use. These issues include susceptibility to adversarial attacks, a tendency to memorize noise, overconfidence in incorrect predictions, and unsuitability for private data handling. In this thesis, we delve into each problem, identify the root causes, and propose cost-effective algorithms to mitigate them in practical situations. Our approach involves identifying structures in deep neural networks that can be leveraged to address the above issues of unreliability in deep learning algorithms.",1
"Deep Neural Networks (DNNs) have transformed the field of machine learning and are widely deployed in many applications involving image, video, speech and natural language processing. The increasing compute demands of DNNs have been widely addressed through Graphics Processing Units (GPUs) and specialized accelerators. However, as model sizes grow, these von Neumann architectures require very high memory bandwidth to keep the processing elements utilized as a majority of the data resides in the main memory. Processing in memory has been proposed as a promising solution for the memory wall bottleneck for ML workloads. In this work, we propose a new DRAM-based processing-in-memory (PIM) multiplication primitive coupled with intra-bank accumulation to accelerate matrix vector operations in ML workloads. The proposed multiplication primitive adds < 1% area overhead and does not require any change in the DRAM peripherals. Therefore, the proposed multiplication can be easily adopted in commodity DRAM chips. Subsequently, we design a DRAM-based PIM architecture, data mapping scheme and dataflow for executing DNNs within DRAM. System evaluations performed on networks like AlexNet, VGG16 and ResNet18 show that the proposed architecture, mapping, and data flow can provide up to 19.5x speedup over an NVIDIA Titan Xp GPU highlighting the need to overcome the memory bottleneck in future generations of DNN hardware.",0
"Machine learning has been revolutionized by Deep Neural Networks (DNNs), which are extensively utilized for image, video, speech, and natural language processing. Graphics Processing Units (GPUs) and specialized accelerators have been employed to handle the increasing compute demands of DNNs. However, as the model sizes increase, von Neumann architectures necessitate high memory bandwidth to keep the processing elements working, which is difficult as a significant amount of data resides in the main memory. Processing in memory has been recommended as a promising solution to address the memory wall bottleneck for ML workloads. This study proposes a new DRAM-based processing-in-memory (PIM) multiplication primitive coupled with intra-bank accumulation to speed up matrix vector operations in ML workloads. The proposed multiplication primitive incurs < 1% area overhead and doesn't require any modification to DRAM peripherals, making it easily applicable in commodity DRAM chips. Moreover, a DRAM-based PIM architecture, data mapping scheme, and dataflow are designed to execute DNNs within DRAM. System evaluations performed on networks such as AlexNet, VGG16, and ResNet18 reveal that the proposed architecture, mapping, and data flow can deliver up to 19.5x speedup over an NVIDIA Titan Xp GPU, emphasizing the importance of overcoming the memory bottleneck in future generations of DNN hardware.",1
"Deep generative models of 3D shapes have received a great deal of research interest. Yet, almost all of them generate discrete shape representations, such as voxels, point clouds, and polygon meshes. We present the first 3D generative model for a drastically different shape representation --- describing a shape as a sequence of computer-aided design (CAD) operations. Unlike meshes and point clouds, CAD models encode the user creation process of 3D shapes, widely used in numerous industrial and engineering design tasks. However, the sequential and irregular structure of CAD operations poses significant challenges for existing 3D generative models. Drawing an analogy between CAD operations and natural language, we propose a CAD generative network based on the Transformer. We demonstrate the performance of our model for both shape autoencoding and random shape generation. To train our network, we create a new CAD dataset consisting of 178,238 models and their CAD construction sequences. We have made this dataset publicly available to promote future research on this topic.",0
"Although there has been significant research interest in deep generative models of 3D shapes, most of these models generate shape representations that are discrete, such as voxels, point clouds, and polygon meshes. However, we have developed the first 3D generative model that uses a different shape representation - a sequence of computer-aided design (CAD) operations which encode the user creation process of 3D shapes. This representation is commonly used in various industrial and engineering design tasks. Nonetheless, the irregular and sequential structure of CAD operations presents significant challenges for existing 3D generative models. To address this, we propose a CAD generative network based on the Transformer, using an analogy between CAD operations and natural language. Our model shows excellent performance for both shape autoencoding and random shape generation. We trained our network using a new CAD dataset comprising 178,238 models and their CAD construction sequences, which we have made publicly available to support future research in this area.",1
"Vision-language Navigation (VLN) tasks require an agent to navigate step-by-step while perceiving the visual observations and comprehending a natural language instruction. Large data bias, which is caused by the disparity ratio between the small data scale and large navigation space, makes the VLN task challenging. Previous works have proposed various data augmentation methods to reduce data bias. However, these works do not explicitly reduce the data bias across different house scenes. Therefore, the agent would overfit to the seen scenes and achieve poor navigation performance in the unseen scenes. To tackle this problem, we propose the Random Environmental Mixup (REM) method, which generates cross-connected house scenes as augmented data via mixuping environment. Specifically, we first select key viewpoints according to the room connection graph for each scene. Then, we cross-connect the key views of different scenes to construct augmented scenes. Finally, we generate augmented instruction-path pairs in the cross-connected scenes. The experimental results on benchmark datasets demonstrate that our augmentation data via REM help the agent reduce its performance gap between the seen and unseen environment and improve the overall performance, making our model the best existing approach on the standard VLN benchmark.",0
"The navigation task of Vision-language Navigation (VLN) involves an agent navigating through a space while perceiving visual observations and comprehending natural language instructions. The VLN task is challenging due to the large data bias caused by the discrepancy between the small data scale and the vast navigation space. To address this issue, previous studies have proposed various data augmentation techniques, but they fail to account for the data bias across different house scenes. Consequently, the agent may overfit to the seen scenes and perform poorly in the unseen scenes. To overcome this limitation, we propose the Random Environmental Mixup (REM) method, which generates augmented data by cross-connecting house scenes using mixuping environment. We select key viewpoints from the room connection graph for each scene and cross-connect them to construct augmented scenes. Finally, we generate augmented instruction-path pairs in the cross-connected scenes. Our experimental results show that REM helps reduce the performance gap between seen and unseen environments, resulting in improved overall performance. As a result, our model outperforms existing approaches on the standard VLN benchmark.",1
"Image caption generation is one of the most challenging problems at the intersection of visual recognition and natural language modeling domains. In this work, we propose and study a practically important variant of this problem where test images may contain visual objects with no corresponding visual or textual training examples. For this problem, we propose a detection-driven approach based on a generalized zero-shot detection model and a template-based sentence generation model. In order to improve the detection component, we jointly define a class-to-class similarity based class representation and a practical score calibration mechanism. We also propose a novel evaluation metric that provides complimentary insights to the captioning outputs, by separately handling the visual and non-visual components of the captions. Our experiments show that the proposed zero-shot detection model obtains state-of-the-art performance on the MS-COCO dataset and the zero-shot captioning approach yields promising results.",0
"Generating captions for images is a complex task that involves both visual recognition and natural language modeling. In this study, we focus on a particularly difficult version of this problem where test images may contain visual objects that were not included in the training data. To address this challenge, we propose a detection-driven approach that utilizes a generalized zero-shot detection model and a template-based sentence generation model. We also introduce a class-to-class similarity based class representation and a score calibration mechanism to enhance the detection component. Additionally, we propose a novel evaluation metric that provides a more comprehensive assessment of the captioning outputs by separating the visual and non-visual components of the captions. Our experiments demonstrate that our zero-shot detection model achieves state-of-the-art performance on the MS-COCO dataset, and our zero-shot captioning approach shows promising results.",1
"Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the self-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent networks. Given its high performance and less need for vision-specific inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these vision transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also include efficient transformer methods for pushing transformer into real device-based applications. Furthermore, we also take a brief look at the self-attention mechanism in computer vision, as it is the base component in transformer. Toward the end of this paper, we discuss the challenges and provide several further research directions for vision transformers.",0
"Initially utilized in natural language processing, the transformer is a deep neural network that heavily relies on the self-attention mechanism. Due to its exceptional representation abilities, researchers are examining ways to implement the transformer in computer vision tasks. Transformer-based models have shown performance on par with, or better than, other network types such as convolutional and recurrent networks in various visual benchmarks. The transformer has gained more attention from the computer vision community due to its high performance and reduced need for inductive bias specific to vision. This paper provides a review of vision transformer models, categorizing them based on various tasks, analyzing their advantages and disadvantages. The categories explored include backbone networks, high/mid-level vision, low-level vision, and video processing. We also discuss efficient transformer methods for real device-based applications, and briefly examine the self-attention mechanism in computer vision, which is the fundamental component in transformer. Finally, we discuss the challenges and provide avenues for further research on vision transformers.",1
"Data processing and analytics are fundamental and pervasive. Algorithms play a vital role in data processing and analytics where many algorithm designs have incorporated heuristics and general rules from 0 knowledge and experience to improve their effectiveness. Recently, reinforcement learning, deep reinforcement learning (DRL) in particular, is increasingly explored and exploited in many areas because it can learn better strategies in complicated environments it is interacting with than statically designed algorithms. Motivated by this trend, we provide a comprehensive review of recent works focusing on utilizing deep reinforcement learning to improve data processing and analytics. First, we present an introduction to key concepts, theories, and methods in deep reinforcement learning. Next, we discuss deep reinforcement learning deployment on database systems, facilitating data processing and analytics in various aspects, including data organization, scheduling, tuning, and indexing. Then, we survey the application of deep reinforcement learning in data processing and analytics, ranging from data preparation, natural language interface to healthcare, fintech, etc. Finally, we discuss important open challenges and future research directions of using deep reinforcement learning in data processing and analytics.",0
"The use of algorithms in data processing and analytics is essential and widespread. These algorithms often incorporate 0 knowledge and experience as heuristics and general rules to enhance their effectiveness. Deep reinforcement learning (DRL) has become increasingly popular in various fields as it can learn better strategies in complex environments. In light of this trend, we aim to provide a comprehensive review of recent research that focuses on using DRL to improve data processing and analytics. Our review includes an introduction to key concepts, theories, and methods in DRL, followed by a discussion of its deployment in database systems to facilitate data organization, scheduling, tuning, and indexing. We also examine the application of DRL in various aspects of data processing and analytics, such as data preparation, natural language interface, healthcare, fintech, and more. Finally, we explore the open challenges and future research directions of using DRL in data processing and analytics.",1
"Recently proposed fine-grained 3D visual grounding is an essential and challenging task, whose goal is to identify the 3D object referred by a natural language sentence from other distractive objects of the same category. Existing works usually adopt dynamic graph networks to indirectly model the intra/inter-modal interactions, making the model difficult to distinguish the referred object from distractors due to the monolithic representations of visual and linguistic contents. In this work, we exploit Transformer for its natural suitability on permutation-invariant 3D point clouds data and propose a TransRefer3D network to extract entity-and-relation aware multimodal context among objects for more discriminative feature learning. Concretely, we devise an Entity-aware Attention (EA) module and a Relation-aware Attention (RA) module to conduct fine-grained cross-modal feature matching. Facilitated by co-attention operation, our EA module matches visual entity features with linguistic entity features while RA module matches pair-wise visual relation features with linguistic relation features, respectively. We further integrate EA and RA modules into an Entity-and-Relation aware Contextual Block (ERCB) and stack several ERCBs to form our TransRefer3D for hierarchical multimodal context modeling. Extensive experiments on both Nr3D and Sr3D datasets demonstrate that our proposed model significantly outperforms existing approaches by up to 10.6% and claims the new state-of-the-art. To the best of our knowledge, this is the first work investigating Transformer architecture for fine-grained 3D visual grounding task.",0
"The task of fine-grained 3D visual grounding has recently been proposed as a challenging yet essential task. Its objective is to identify the 3D object referred to in a natural language sentence while disregarding other distracting objects of the same category. Current works often use dynamic graph networks to indirectly model intra/inter-modal interactions, which makes it difficult to differentiate the referred object from distractors due to the visual and linguistic contents being represented monolithically. In this study, we propose a TransRefer3D network that uses Transformer, which is naturally suited for permutation-invariant 3D point clouds data, to extract entity-and-relation aware multimodal context among objects for more discriminative feature learning. To achieve this, we introduce an Entity-aware Attention (EA) module and a Relation-aware Attention (RA) module for fine-grained cross-modal feature matching. By utilizing co-attention operation, our EA module matches visual entity features with their linguistic counterparts, while our RA module matches pairwise visual relation features with their linguistic counterparts. We further integrate EA and RA modules into an Entity-and-Relation aware Contextual Block (ERCB) and stack several ERCBs to form our TransRefer3D for hierarchical multimodal context modeling. Our model outperforms existing approaches by up to 10.6% and claims the new state-of-the-art on both Nr3D and Sr3D datasets. To our knowledge, this is the first study that investigates the Transformer architecture for the fine-grained 3D visual grounding task.",1
"Video captioning aims to automatically generate natural language sentences that can describe the visual contents of a given video. Existing generative models like encoder-decoder frameworks cannot explicitly explore the object-level interactions and frame-level information from complex spatio-temporal data to generate semantic-rich captions. Our main contribution is to identify three key problems in a joint framework for future video summarization tasks. 1) Enhanced Object Proposal: we propose a novel Conditional Graph that can fuse spatio-temporal information into latent object proposal. 2) Visual Knowledge: Latent Proposal Aggregation is proposed to dynamically extract visual words with higher semantic levels. 3) Sentence Validation: A novel Discriminative Language Validator is proposed to verify generated captions so that key semantic concepts can be effectively preserved. Our experiments on two public datasets (MVSD and MSR-VTT) manifest significant improvements over state-of-the-art approaches on all metrics, especially for BLEU-4 and CIDEr. Our code is available at https://github.com/baiyang4/D-LSG-Video-Caption.",0
"The objective of video captioning is to automatically produce natural language sentences that can depict the visual aspects of a given video. However, existing generative models such as encoder-decoder frameworks do not explicitly consider the interactions among objects and frame-level details from complex spatio-temporal data to generate captions that are rich in semantics. Our contribution is to address three crucial issues in a joint framework for future video summarization tasks. Firstly, we propose an Enhanced Object Proposal by introducing a new Conditional Graph that can merge spatio-temporal information into latent object proposals. Secondly, we propose Visual Knowledge by using Latent Proposal Aggregation to dynamically extract visual words with higher semantic levels. Lastly, we propose a Discriminative Language Validator, which can validate the generated captions for key semantic concepts. Our experiments on two public datasets (MVSD and MSR-VTT) demonstrate significant improvements over state-of-the-art approaches in all metrics, particularly for BLEU-4 and CIDEr. Our code is publicly available at https://github.com/baiyang4/D-LSG-Video-Caption.",1
"Deep learning is a group of exciting new technologies for neural networks. Through a combination of advanced training techniques and neural network architectural components, it is now possible to create neural networks that can handle tabular data, images, text, and audio as both input and output. Deep learning allows a neural network to learn hierarchies of information in a way that is like the function of the 0 brain. This course will introduce the student to classic neural network structures, Convolution Neural Networks (CNN), Long Short-Term Memory (LSTM), Gated Recurrent Neural Networks (GRU), General Adversarial Networks (GAN), and reinforcement learning. Application of these architectures to computer vision, time series, security, natural language processing (NLP), and data generation will be covered. High-Performance Computing (HPC) aspects will demonstrate how deep learning can be leveraged both on graphical processing units (GPUs), as well as grids. Focus is primarily upon the application of deep learning to problems, with some introduction to mathematical foundations. Readers will use the Python programming language to implement deep learning using Google TensorFlow and Keras. It is not necessary to know Python prior to this book; however, familiarity with at least one programming language is assumed.",0
"Exciting new technologies for neural networks known as deep learning have emerged. By utilizing advanced training techniques and neural network architectural components, deep learning enables the creation of neural networks capable of handling diverse data types such as tabular data, images, text, and audio as both input and output. Deep learning mimics the hierarchical learning function of the 0 brain, allowing neural networks to learn complex information hierarchies. This course introduces classic neural network structures, including Convolution Neural Networks (CNN), Long Short-Term Memory (LSTM), Gated Recurrent Neural Networks (GRU), General Adversarial Networks (GAN), and reinforcement learning, and their applications to computer vision, time series, security, natural language processing (NLP), and data generation. High-Performance Computing (HPC) aspects are also covered, demonstrating how deep learning can be leveraged on graphical processing units (GPUs) and grids. The course focuses on the practical application of deep learning to problem-solving, with some introduction to mathematical foundations. Readers will use Python programming language to implement deep learning with Google TensorFlow and Keras. Prior knowledge of Python is not necessary, but familiarity with at least one programming language is assumed.",1
"We extend the task of composed image retrieval, where an input query consists of an image and short textual description of how to modify the image. Existing methods have only been applied to non-complex images within narrow domains, such as fashion products, thereby limiting the scope of study on in-depth visual reasoning in rich image and language contexts. To address this issue, we collect the Compose Image Retrieval on Real-life images (CIRR) dataset, which consists of over 36,000 pairs of crowd-sourced, open-domain images with 0-generated modifying text. To extend current methods to the open-domain, we propose CIRPLANT, a transformer based model that leverages rich pre-trained vision-and-language (V&L) knowledge for modifying visual features conditioned on natural language. Retrieval is then done by nearest neighbor lookup on the modified features. We demonstrate that with a relatively simple architecture, CIRPLANT outperforms existing methods on open-domain images, while matching state-of-the-art accuracy on the existing narrow datasets, such as fashion. Together with the release of CIRR, we believe this work will inspire further research on composed image retrieval.",0
"Our study focuses on expanding the task of composed image retrieval, which involves an input query composed of an image and a brief textual description detailing how to modify that image. However, current methods have only been applied to simple images within specific domains, such as fashion products, which limits the depth of visual reasoning in rich image and language contexts. To address this limitation, we gathered the Compose Image Retrieval on Real-life images (CIRR) dataset, which includes more than 36,000 pairs of open-domain images with 0-generated modifying text. To expand present methods to the open-domain, we propose CIRPLANT, a transformer-based model that employs pre-trained vision-and-language knowledge to modify visual features based on natural language. Retrieval is accomplished through nearest neighbor lookup on the modified features. Our findings demonstrate that even with a relatively simple architecture, CIRPLANT surpasses current methods on open-domain images while maintaining state-of-the-art accuracy on narrow datasets like fashion. We believe that the release of CIRR will inspire further research on composed image retrieval.",1
"Video-and-Language Inference is a recently proposed task for joint video-and-language understanding. This new task requires a model to draw inference on whether a natural language statement entails or contradicts a given video clip. In this paper, we study how to address three critical challenges for this task: judging the global correctness of the statement involved multiple semantic meanings, joint reasoning over video and subtitles, and modeling long-range relationships and complex social interactions. First, we propose an adaptive hierarchical graph network that achieves in-depth understanding of the video over complex interactions. Specifically, it performs joint reasoning over video and subtitles in three hierarchies, where the graph structure is adaptively adjusted according to the semantic structures of the statement. Secondly, we introduce semantic coherence learning to explicitly encourage the semantic coherence of the adaptive hierarchical graph network from three hierarchies. The semantic coherence learning can further improve the alignment between vision and linguistics, and the coherence across a sequence of video segments. Experimental results show that our method significantly outperforms the baseline by a large margin.",0
"The task of Video-and-Language Inference is a novel approach to understanding both video and language. It requires a model to determine if a statement in natural language agrees or disagrees with a given video clip. This paper examines the challenges of this task, including assessing the accuracy of a statement with multiple meanings, reasoning over video and subtitles simultaneously, and modeling complex social interactions. To address these challenges, an adaptive hierarchical graph network is proposed that performs joint reasoning over the video and subtitles in three hierarchies. This network is designed to adjust its structure based on the semantic structure of the statement. Additionally, semantic coherence learning is introduced to encourage coherence between the vision and linguistic aspects of the network. The results of experiments show that this approach significantly outperforms the baseline.",1
"To continuously improve quality and reflect changes in data, machine learning applications have to regularly retrain and update their core models. We show that a differential analysis of language model snapshots before and after an update can reveal a surprising amount of detailed information about changes in the training data. We propose two new metrics---\emph{differential score} and \emph{differential rank}---for analyzing the leakage due to updates of natural language models. We perform leakage analysis using these metrics across models trained on several different datasets using different methods and configurations. We discuss the privacy implications of our findings, propose mitigation strategies and evaluate their effect.",0
"In order to maintain high quality and accommodate changes in data, it is necessary for machine learning applications to regularly retrain and update their core models. Our study demonstrates that conducting a differential analysis on language model snapshots before and after an update can yield detailed information about changes in the training data. We introduce two novel metrics, namely the differential score and differential rank, for analyzing the privacy implications of updates on natural language models. Using these metrics, we conduct leakage analysis on models trained on a variety of datasets using different methods and configurations. We also discuss the privacy concerns associated with our findings, suggest ways to mitigate these issues, and evaluate their effectiveness.",1
"Recently, there have been breakthroughs in computer vision (""CV"") models that are more generalizable with the advent of models such as CLIP and ALIGN. In this paper, we analyze CLIP and highlight some of the challenges such models pose. CLIP reduces the need for task specific training data, potentially opening up many niche tasks to automation. CLIP also allows its users to flexibly specify image classification classes in natural language, which we find can shift how biases manifest. Additionally, through some preliminary probes we find that CLIP can inherit biases found in prior computer vision systems. Given the wide and unpredictable domain of uses for such models, this raises questions regarding what sufficiently safe behaviour for such systems may look like. These results add evidence to the growing body of work calling for a change in the notion of a 'better' model--to move beyond simply looking at higher accuracy at task-oriented capability evaluations, and towards a broader 'better' that takes into account deployment-critical features such as different use contexts, and people who interact with the model when thinking about model deployment.",0
"Recent advancements in computer vision models have led to the development of more generalizable models like CLIP and ALIGN. This study examines CLIP and identifies some of the challenges that come with such models. CLIP eliminates the need for specific training data, allowing for the automation of niche tasks. It also enables users to specify image classification using natural language, which can affect how biases are expressed. Moreover, the study reveals that CLIP can inherit biases from previous computer vision systems. Given the broad range of applications for such models, questions are raised about what constitutes safe behavior for such systems. These findings support the growing body of research calling for a shift in the definition of a ""better"" model, beyond mere accuracy, to include deployment-critical features and user interaction.",1
"In many sequence learning tasks, such as program synthesis and document summarization, a key problem is searching over a large space of possible output sequences. We propose to learn representations of the outputs that are specifically meant for search: rich enough to specify the desired output but compact enough to make search more efficient. Discrete latent codes are appealing for this purpose, as they naturally allow sophisticated combinatorial search strategies. The latent codes are learned using a self-supervised learning principle, in which first a discrete autoencoder is trained on the output sequences, and then the resulting latent codes are used as intermediate targets for the end-to-end sequence prediction task. Based on these insights, we introduce the \emph{Latent Programmer}, a program synthesis method that first predicts a discrete latent code from input/output examples, and then generates the program in the target language. We evaluate the Latent Programmer on two domains: synthesis of string transformation programs, and generation of programs from natural language descriptions. We demonstrate that the discrete latent representation significantly improves synthesis accuracy.",0
"The primary challenge in various sequence learning tasks, such as program synthesis and document summarization, is the exploration of an extensive range of potential output sequences. Our proposal involves acquiring knowledge of output representations that are specifically designed for searching. These representations should be sufficiently comprehensive to specify the desired output while also being concise to facilitate more efficient searching. Utilizing discrete latent codes is an attractive option since they enable sophisticated combinatorial search strategies. Our method trains these latent codes using a self-supervised learning principle. We first train a discrete autoencoder on the output sequences, and then use the resulting latent codes as intermediate targets for the end-to-end sequence prediction task. Our approach, the ""Latent Programmer,"" predicts a discrete latent code from input/output examples and then creates a program in the target language. We assess the efficacy of the Latent Programmer in two domains: string transformation program synthesis and program generation from natural language descriptions. Our results show that using a discrete latent representation significantly enhances the accuracy of program synthesis.",1
"Inspired by how the 0 brain employs a higher number of neural pathways when describing a highly focused subject, we show that deep attentive models used for the main vision-language task of image captioning, could be extended to achieve better performance. Image captioning bridges a gap between computer vision and natural language processing. Automated image captioning is used as a tool to eliminate the need for 0 agent for creating descriptive captions for unseen images.Automated image captioning is challenging and yet interesting. One reason is that AI based systems capable of generating sentences that describe an input image could be used in a wide variety of tasks beyond generating captions for unseen images found on web or uploaded to social media. For example, in biology and medical sciences, these systems could provide researchers and physicians with a brief linguistic description of relevant images, potentially expediting their work.",0
"We were inspired by the 0 brain's use of numerous neural pathways when describing a highly focused subject. Our research shows that deep attentive models used for image captioning, the main task that bridges computer vision and natural language processing, can be expanded to improve performance. Automated image captioning eliminates the need for 0 agents to create descriptive captions for new images, making it a challenging and intriguing process. One reason for its potential is that AI-based systems capable of generating sentences that describe input images can be used in various fields, such as biology and medical sciences, to provide researchers and physicians with brief linguistic descriptions that can accelerate their work.",1
"While convolutional neural networks have shown a tremendous impact on various computer vision tasks, they generally demonstrate limitations in explicitly modeling long-range dependencies due to the intrinsic locality of the convolution operation. Initially designed for natural language processing tasks, Transformers have emerged as alternative architectures with innate global self-attention mechanisms to capture long-range dependencies. In this paper, we propose TransDepth, an architecture that benefits from both convolutional neural networks and transformers. To avoid the network losing its ability to capture local-level details due to the adoption of transformers, we propose a novel decoder that employs attention mechanisms based on gates. Notably, this is the first paper that applies transformers to pixel-wise prediction problems involving continuous labels (i.e., monocular depth prediction and surface normal estimation). Extensive experiments demonstrate that the proposed TransDepth achieves state-of-the-art performance on three challenging datasets. Our code is available at: https://github.com/ygjwd12345/TransDepth.",0
"Convolutional neural networks have proven to be highly effective in various computer vision tasks, but due to the convolution operation's inherent locality, they struggle to model long-range dependencies. Transformers, on the other hand, were initially designed for natural language processing and possess global self-attention mechanisms capable of capturing such dependencies. This paper introduces TransDepth, an architecture that combines the strengths of both convolutional neural networks and transformers. To prevent the network from losing its ability to capture local-level details, we propose a novel decoder that employs attention mechanisms based on gates. This is the first paper to apply transformers to pixel-wise prediction problems involving continuous labels, such as monocular depth prediction and surface normal estimation. Extensive experiments demonstrate that TransDepth achieves state-of-the-art performance on three challenging datasets. The source code is available at: https://github.com/ygjwd12345/TransDepth.",1
"Neural agents trained in reinforcement learning settings can learn to communicate among themselves via discrete tokens, accomplishing as a team what agents would be unable to do alone. However, the current standard of using one-hot vectors as discrete communication tokens prevents agents from acquiring more desirable aspects of communication such as zero-shot understanding. Inspired by word embedding techniques from natural language processing, we propose neural agent architectures that enables them to communicate via discrete tokens derived from a learned, continuous space. We show in a decision theoretic framework that our technique optimizes communication over a wide range of scenarios, whereas one-hot tokens are only optimal under restrictive assumptions. In self-play experiments, we validate that our trained agents learn to cluster tokens in semantically-meaningful ways, allowing them communicate in noisy environments where other techniques fail. Lastly, we demonstrate both that agents using our method can effectively respond to novel 0 communication and that 0s can understand unlabeled emergent agent communication, outperforming the use of one-hot communication.",0
"Neural agents can learn to communicate with each other using discrete tokens in reinforcement learning settings, which allows them to accomplish more as a team than they could alone. However, the current practice of using one-hot vectors as the communication tokens has limitations, such as preventing agents from acquiring zero-shot understanding. To address this issue, we propose a new neural agent architecture that enables communication through discrete tokens derived from a continuous space, inspired by word embedding techniques from natural language processing. Our approach optimizes communication across a wide range of scenarios, unlike one-hot tokens which only work in specific situations. Our experiments show that our trained agents learn to cluster tokens in meaningful ways, which helps them communicate effectively in noisy environments where other methods fail. Lastly, we demonstrate that our approach is successful in responding to novel 0 communication and that 0s can understand the emergent agent communication without labels, outperforming one-hot communication.",1
"Meter-level load forecasting is crucial for efficient energy management and power system planning for Smart Grids (SGs), in tasks associated with regulation, dispatching, scheduling, and unit commitment of power grids. Although a variety of algorithms have been proposed and applied on the field, more accurate and robust models are still required: the overall utility cost of operations in SGs increases 10 million currency units if the load forecasting error increases 1%, and the mean absolute percentage error (MAPE) in forecasting is still much higher than 1%. Transformers have become the new state-of-the-art in a variety of tasks, including the ones in computer vision, natural language processing and time series forecasting, surpassing alternative neural models such as convolutional and recurrent neural networks. In this letter, we present a new state-of-the-art Transformer-based algorithm for the meter-level load forecasting task, which has surpassed the former state-of-the-art, LSTM, and the traditional benchmark, vanilla RNN, in all experiments by a margin of at least 13% in MAPE.",0
"Efficient energy management and power system planning for Smart Grids (SGs) rely heavily on accurate meter-level load forecasting. This is crucial for tasks such as regulation, dispatching, scheduling, and unit commitment of power grids. Despite various proposed algorithms, more precise and robust models are needed as even a 1% increase in load forecasting error can result in a 10 million currency unit rise in overall utility cost of operations for SGs. The mean absolute percentage error (MAPE) in forecasting is still significantly higher than 1%. Transformers have become the new standard in several fields, including computer vision, natural language processing, and time series forecasting, outperforming other neural models such as convolutional and recurrent neural networks. In this letter, we introduce a new Transformer-based algorithm that has surpassed the previous state-of-the-art model, LSTM, and the traditional benchmark, vanilla RNN, in all experiments by a minimum of 13% in MAPE for the meter-level load forecasting task.",1
"Deep learning on graphs has attracted significant interests recently. However, most of the works have focused on (semi-) supervised learning, resulting in shortcomings including heavy label reliance, poor generalization, and weak robustness. To address these issues, self-supervised learning (SSL), which extracts informative knowledge through well-designed pretext tasks without relying on manual labels, has become a promising and trending learning paradigm for graph data. Different from SSL on other domains like computer vision and natural language processing, SSL on graphs has an exclusive background, design ideas, and taxonomies. Under the umbrella of graph self-supervised learning, we present a timely and comprehensive review of the existing approaches which employ SSL techniques for graph data. We construct a unified framework that mathematically formalizes the paradigm of graph SSL. According to the objectives of pretext tasks, we divide these approaches into four categories: generation-based, auxiliary property-based, contrast-based, and hybrid approaches. We further conclude the applications of graph SSL across various research fields and summarize the commonly used datasets, evaluation benchmark, performance comparison and open-source codes of graph SSL. Finally, we discuss the remaining challenges and potential future directions in this research field.",0
"Recently, there has been a significant interest in deep learning on graphs. However, most of the research has focused on (semi-) supervised learning, which has resulted in drawbacks such as heavy reliance on labels, poor generalization, and weak robustness. To overcome these issues, self-supervised learning (SSL) has emerged as a promising and popular learning paradigm for graph data. Unlike SSL in other domains like computer vision and natural language processing, SSL on graphs has unique backgrounds, design ideas, and taxonomies. In this paper, we provide a comprehensive review of existing approaches that employ SSL techniques for graph data under the umbrella of graph self-supervised learning. We present a unified framework that mathematically formalizes the graph SSL paradigm. We categorize these approaches into four categories based on the objectives of pretext tasks: generation-based, auxiliary property-based, contrast-based, and hybrid approaches. Additionally, we summarize the applications of graph SSL across various research fields, commonly used datasets, evaluation benchmark, performance comparison, and open-source codes. Finally, we discuss the remaining challenges and potential future directions in this research field.",1
"Progress of machine learning in critical care has been difficult to track, in part due to absence of public benchmarks. Other fields of research (such as computer vision and natural language processing) have established various competitions and public benchmarks. Recent availability of large clinical datasets has enabled the possibility of establishing public benchmarks. Taking advantage of this opportunity, we propose a public benchmark suite to address four areas of critical care, namely mortality prediction, estimation of length of stay, patient phenotyping and risk of decompensation. We define each task and compare the performance of both clinical models as well as baseline and deep learning models using eICU critical care dataset of around 73,000 patients. This is the first public benchmark on a multi-centre critical care dataset, comparing the performance of clinical gold standard with our predictive model. We also investigate the impact of numerical variables as well as handling of categorical variables on each of the defined tasks. The source code, detailing our methods and experiments is publicly available such that anyone can replicate our results and build upon our work.",0
"The progress of machine learning in critical care has been a challenge to keep track of, partly due to the absence of public benchmarks. However, other research fields such as computer vision and natural language processing have established various competitions and public benchmarks. With the recent availability of large clinical datasets, the possibility of establishing public benchmarks has emerged. Consequently, we propose a public benchmark suite that addresses four critical care areas, namely mortality prediction, estimation of length of stay, patient phenotyping, and risk of decompensation. We define each task and compare the performance of clinical models, baseline models, and deep learning models using the eICU critical care dataset, which includes data from around 73,000 patients. This is the first public benchmark on a multi-center critical care dataset that compares the performance of a clinical gold standard with our predictive model. Furthermore, we investigate the impact of numerical variables and the handling of categorical variables on each task. Our source code, detailing our methods and experiments, is publicly available for anyone to replicate our results and build upon our work.",1
"Interventional causal models describe several joint distributions over some variables used to describe a system, one for each intervention setting. They provide a formal recipe for how to move between the different joint distributions and make predictions about the variables upon intervening on the system. Yet, it is difficult to formalise how we may change the underlying variables used to describe the system, say moving from fine-grained to coarse-grained variables. Here, we argue that compositionality is a desideratum for such model transformations and the associated errors: When abstracting a reference model M iteratively, first obtaining M' and then further simplifying that to obtain M'', we expect the composite transformation from M to M'' to exist and its error to be bounded by the errors incurred by each individual transformation step. Category theory, the study of mathematical objects via compositional transformations between them, offers a natural language to develop our framework for model transformations and abstractions. We introduce a category of finite interventional causal models and, leveraging theory of enriched categories, prove the desired compositionality properties for our framework.",0
"Interventional causal models represent multiple joint distributions for a system's variables, each for a different intervention setting. They offer a formal process for transitioning between these distributions and making predictions about the variables after intervening on the system. However, it is challenging to formalize how the underlying variables describing the system can be changed, such as moving from fine-grained to coarse-grained variables. Thus, we propose that a compositional approach is necessary for model transformations and associated errors, where the composite transformation from a reference model to a simplified model is expected to exist, and its error should be bounded by each individual transformation step's error. Category theory, which studies mathematical objects through compositional transformations, provides a natural language for our framework. We present a category of finite interventional causal models and, using enriched categories theory, establish the desired compositionality properties for our approach.",1
"Video captioning is an essential technology to understand scenes and describe events in natural language. To apply it to real-time monitoring, a system needs not only to describe events accurately but also to produce the captions as soon as possible. Low-latency captioning is needed to realize such functionality, but this research area for online video captioning has not been pursued yet. This paper proposes a novel approach to optimize each caption's output timing based on a trade-off between latency and caption quality. An audio-visual Trans-former is trained to generate ground-truth captions using only a small portion of all video frames, and to mimic outputs of a pre-trained Transformer to which all the frames are given. A CNN-based timing detector is also trained to detect a proper output timing, where the captions generated by the two Trans-formers become sufficiently close to each other. With the jointly trained Transformer and timing detector, a caption can be generated in the early stages of an event-triggered video clip, as soon as an event happens or when it can be forecasted. Experiments with the ActivityNet Captions dataset show that our approach achieves 94% of the caption quality of the upper bound given by the pre-trained Transformer using the entire video clips, using only 28% of frames from the beginning.",0
"In order to comprehend scenes and depict events in a natural language, video captioning is an indispensable technology. To employ it for real-time monitoring, a system must not only provide precise descriptions of events but also produce captions as quickly as possible. However, the research area for online video captioning with low latency has not been explored yet. This article presents a fresh approach to optimize the output timing of each caption by weighing the trade-off between latency and caption quality. An audio-visual Transformer is trained to produce accurate captions using only a small portion of video frames and to imitate the output of a pre-trained Transformer that receives all the frames. Additionally, a timing detector based on CNN is trained to identify the proper output timing where the captions generated by the two Transformers become sufficiently similar. With the jointly trained Transformer and timing detector, a caption can be generated at the initial stages of an event-triggered video clip as soon as an event occurs or is anticipated. Experiments with the ActivityNet Captions dataset reveal that our method achieves 94% of caption quality compared to the upper limit set by the pre-trained Transformer using the entire video clips, using only 28% of frames from the beginning.",1
"Most existing neural architecture search (NAS) algorithms are dedicated to the downstream tasks, e.g., image classification in computer vision. However, extensive experiments have shown that, prominent neural architectures, such as ResNet in computer vision and LSTM in natural language processing, are generally good at extracting patterns from the input data and perform well on different downstream tasks. These observations inspire us to ask: Is it necessary to use the performance of specific downstream tasks to evaluate and search for good neural architectures? Can we perform NAS effectively and efficiently while being agnostic to the downstream task? In this work, we attempt to affirmatively answer the above two questions and improve the state-of-the-art NAS solution by proposing a novel and generic NAS framework, termed Generic NAS (GenNAS). GenNAS does not use task-specific labels but instead adopts \textit{regression} on a set of manually designed synthetic signal bases for architecture evaluation. Such a self-supervised regression task can effectively evaluate the intrinsic power of an architecture to capture and transform the input signal patterns, and allow more sufficient usage of training samples. We then propose an automatic task search to optimize the combination of synthetic signals using limited downstream-task-specific labels, further improving the performance of GenNAS. We also thoroughly evaluate GenNAS's generality and end-to-end NAS performance on all search spaces, which outperforms almost all existing works with significant speedup.",0
"While most existing neural architecture search (NAS) algorithms focus on downstream tasks like image classification, it has been observed that well-known neural architectures like ResNet and LSTM perform well across different tasks. This raises the question of whether task-specific performance is necessary for evaluating and searching for good neural architectures. In this study, we propose a new NAS framework called Generic NAS (GenNAS), which is task-agnostic and uses regression on synthetic signal bases for architecture evaluation. This self-supervised approach allows for more efficient use of training samples. We also introduce an automatic task search that optimizes the combination of synthetic signals using limited downstream-task-specific labels, resulting in improved performance. Our evaluation shows that GenNAS outperforms existing works with significant speedup across all search spaces.",1
"Attention-based transformer networks have demonstrated promising potential as their applications extend from natural language processing to vision. However, despite the recent improvements, such as sub-quadratic attention approximation and various training enhancements, the compact vision transformers to date using the regular attention still fall short in comparison with its convnet counterparts, in terms of \textit{accuracy,} \textit{model size}, \textit{and} \textit{throughput}. This paper introduces a compact self-attention mechanism that is fundamental and highly generalizable. The proposed method reduces redundancy and improves efficiency on top of the existing attention optimizations. We show its drop-in applicability for both the regular attention mechanism and some most recent variants in vision transformers. As a result, we produced smaller and faster models with the same or better accuracies.",0
"Although attention-based transformer networks have shown great potential in various applications, from natural language processing to vision, compact vision transformers that use regular attention still lag behind convnet counterparts in terms of accuracy, model size, and throughput, despite recent improvements such as sub-quadratic attention approximation and various training enhancements. To address this issue, this paper presents a compact self-attention mechanism that improves efficiency and reduces redundancy beyond existing attention optimizations, and can be easily applied to both regular attention mechanisms and recent variants in vision transformers. Our results demonstrate that this approach can produce smaller and faster models with the same or better accuracies.",1
"Hypertext transfer protocol (HTTP) is one of the most widely used protocols on the Internet. As a consequence, most attacks (i.e., SQL injection, XSS) use HTTP as the transport mechanism. Therefore, it is crucial to develop an intelligent solution that would allow to effectively detect and filter out anomalies in HTTP traffic. Currently, most of the anomaly detection systems are either rule-based or trained using manually selected features. We propose utilizing modern unsupervised language representation model for embedding HTTP requests and then using it to classify anomalies in the traffic. The solution is motivated by methods used in Natural Language Processing (NLP) such as Doc2Vec which could potentially capture the true understanding of HTTP messages, and therefore improve the efficiency of Intrusion Detection System. In our work, we not only aim at generating a suitable embedding space, but also at the interpretability of the proposed model. We decided to use the current state-of-the-art RoBERTa, which, as far as we know, has never been used in a similar problem. To verify how the solution would work in real word conditions, we train the model using only legitimate traffic. We also try to explain the results based on clusters that occur in the vectorized requests space and a simple logistic regression classifier. We compared our approach with the similar, previously proposed methods. We evaluate the feasibility of our method on three different datasets: CSIC2010, CSE-CIC-IDS2018 and one that we prepared ourselves. The results we show are comparable to others or better, and most importantly - interpretable.",0
"HTTP is widely used on the Internet, and as a result, most attacks use HTTP to transport malicious code. Therefore, it is important to develop a solution that can detect and filter out anomalies in HTTP traffic. Current anomaly detection systems are either rule-based or trained using manually selected features. Our proposed solution involves using a modern unsupervised language representation model to embed HTTP requests and classify anomalies in the traffic. We chose to use RoBERTa, a state-of-the-art model that has not been used in a similar problem before. To test our solution, we trained the model using only legitimate traffic and evaluated it on three different datasets. We compared our results with previously proposed methods and found that our approach is comparable or better and, most importantly, interpretable. By examining clusters that occur in the vectorized request space and using a simple logistic regression classifier, we were able to explain the results of our model.",1
"Deep learning has achieved great success in a wide spectrum of multimedia applications such as image classification, natural language processing and multimodal data analysis. Recent years have seen the development of many deep learning frameworks that provide a high-level programming interface for users to design models, conduct training and deploy inference. However, it remains challenging to build an efficient end-to-end multimedia application with most existing frameworks. Specifically, in terms of usability, it is demanding for non-experts to implement deep learning models, obtain the right settings for the entire machine learning pipeline, manage models and datasets, and exploit external data sources all together. Further, in terms of adaptability, elastic computation solutions are much needed as the actual serving workload fluctuates constantly, and scaling the hardware resources to handle the fluctuating workload is typically infeasible. To address these challenges, we introduce SINGA-Easy, a new deep learning framework that provides distributed hyper-parameter tuning at the training stage, dynamic computational cost control at the inference stage, and intuitive user interactions with multimedia contents facilitated by model explanation. Our experiments on the training and deployment of multi-modality data analysis applications show that the framework is both usable and adaptable to dynamic inference loads. We implement SINGA-Easy on top of Apache SINGA and demonstrate our system with the entire machine learning life cycle.",0
"A wide range of multimedia applications, including image classification, natural language processing, and multimodal data analysis, have been highly successful using deep learning. Despite the development of several deep learning frameworks that offer a user-friendly programming interface for designing models, conducting training, and deploying inference, building a comprehensive multimedia application using existing frameworks still presents challenges. Specifically, non-experts may find it difficult to implement deep learning models, obtain the proper settings for the machine learning pipeline, manage models and datasets, and utilize external data sources. Furthermore, elastic computation solutions are necessary to handle fluctuating workloads, which can be difficult to scale using hardware resources. To overcome these challenges, we introduce SINGA-Easy, a new deep learning framework that includes distributed hyper-parameter tuning during training, dynamic computational cost control during inference, and intuitive user interactions with multimedia content facilitated by model explanation. Our experiments demonstrate that SINGA-Easy is both usable and adaptable to dynamic inference loads for multi-modality data analysis applications. We implement SINGA-Easy on top of Apache SINGA and showcase its full machine learning life cycle capabilities.",1
"The recently-proposed Perceiver model obtains good results on several domains (images, audio, multimodal, point clouds) while scaling linearly in compute and memory with the input size. While the Perceiver supports many kinds of inputs, it can only produce very simple outputs such as class scores. Perceiver IO overcomes this limitation without sacrificing the original's appealing properties by learning to flexibly query the model's latent space to produce outputs of arbitrary size and semantics. Perceiver IO still decouples model depth from data size and still scales linearly with data size, but now with respect to both input and output sizes. The full Perceiver IO model achieves strong results on tasks with highly structured output spaces, such as natural language and visual understanding, StarCraft II, and multi-task and multi-modal domains. As highlights, Perceiver IO matches a Transformer-based BERT baseline on the GLUE language benchmark without the need for input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation.",0
"The Perceiver model, which has been recently proposed, has demonstrated good performance across various domains such as images, audio, multimodal, and point clouds. It has the added advantage of linearly scaling with the input size in terms of compute and memory. However, the Perceiver has a limitation in that it can only produce simple outputs such as class scores. To address this, the Perceiver IO model has been developed, which can learn to query the model's latent space to produce outputs of any size and semantics. Despite this improvement, the Perceiver IO model still maintains the original properties of decoupling model depth from data size and scaling linearly with both input and output sizes. The Perceiver IO model has demonstrated strong performance on tasks with structured output spaces such as natural language, visual understanding, StarCraft II, and multi-task and multi-modal domains. Notably, it matches the performance of the Transformer-based BERT baseline on the GLUE language benchmark without the need for input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation.",1
"Can a generative model be trained to produce images from a specific domain, guided by a text prompt only, without seeing any image? In other words: can an image generator be trained blindly? Leveraging the semantic power of large scale Contrastive-Language-Image-Pre-training (CLIP) models, we present a text-driven method that allows shifting a generative model to new domains, without having to collect even a single image from those domains. We show that through natural language prompts and a few minutes of training, our method can adapt a generator across a multitude of domains characterized by diverse styles and shapes. Notably, many of these modifications would be difficult or outright impossible to reach with existing methods. We conduct an extensive set of experiments and comparisons across a wide range of domains. These demonstrate the effectiveness of our approach and show that our shifted models maintain the latent-space properties that make generative models appealing for downstream tasks.",0
"Is it possible to train a generative model to create images from a particular domain solely guided by a text prompt, without any visual input? To put it differently, can an image generator be trained without visual aid? By utilizing the semantic capabilities of large-scale Contrastive-Language-Image-Pre-training (CLIP) models, we introduce a text-based technique that enables the transformation of a generative model to new domains, without the need to gather any images from those domains. We prove that with natural language prompts and a brief training session, our method can adjust a generator to various domains distinguished by distinct styles and forms. Notably, several of these alterations would be difficult or impossible to achieve using existing techniques. We carry out a comprehensive set of experiments and comparisons across a wide variety of domains. These demonstrate the efficiency of our approach and indicate that our transformed models retain the latent-space features that make generative models desirable for downstream tasks.",1
"0-in-the-loop aims to train an accurate prediction model with minimum cost by integrating 0 knowledge and experience. 0s can provide training data for machine learning applications and directly accomplish some tasks that are hard for computers in the pipeline with the help of machine-based approaches. In this paper, we survey existing works on 0-in-the-loop from a data perspective and classify them into three categories with a progressive relationship: (1) the work of improving model performance from data processing, (2) the work of improving model performance through interventional model training, and (3) the design of the system independent 0-in-the-loop. Using the above categorization, we summarize major approaches in the field, along with their technical strengths/ weaknesses, we have simple classification and discussion in natural language processing, computer vision, and others. Besides, we provide some open challenges and opportunities. This survey intends to provide a high-level summarization for 0-in-the-loop and motivates interested readers to consider approaches for designing effective 0-in-the-loop solutions.",0
"The aim of 0-in-the-loop is to use 0 knowledge and experience to train an accurate prediction model with minimum cost. This involves 0s providing training data for machine learning applications and assisting with tasks that are difficult for computers. In this article, we examine existing works on 0-in-the-loop from a data perspective and classify them into three categories: improving model performance through data processing, interventional model training, and independent 0-in-the-loop system design. We summarize the major approaches in natural language processing, computer vision, and other fields, along with their strengths and weaknesses. Additionally, we outline some open challenges and opportunities. The purpose of this survey is to provide a comprehensive overview of 0-in-the-loop and encourage readers to consider effective approaches for designing this system.",1
"Learning continuously during all model lifetime is fundamental to deploy machine learning solutions robust to drifts in the data distribution. Advances in Continual Learning (CL) with recurrent neural networks could pave the way to a large number of applications where incoming data is non stationary, like natural language processing and robotics. However, the existing body of work on the topic is still fragmented, with approaches which are application-specific and whose assessment is based on heterogeneous learning protocols and datasets. In this paper, we organize the literature on CL for sequential data processing by providing a categorization of the contributions and a review of the benchmarks. We propose two new benchmarks for CL with sequential data based on existing datasets, whose characteristics resemble real-world applications. We also provide a broad empirical evaluation of CL and Recurrent Neural Networks in class-incremental scenario, by testing their ability to mitigate forgetting with a number of different strategies which are not specific to sequential data processing. Our results highlight the key role played by the sequence length and the importance of a clear specification of the CL scenario.",0
"Continuous learning throughout the entire lifespan of a model is essential for implementing robust machine learning solutions that can handle changes in data distribution. Recurrent neural networks have made significant progress in Continual Learning (CL), which could revolutionize various fields, such as natural language processing and robotics, where incoming data is not stationary. However, current research on CL is disjointed, with application-specific approaches and diverse learning protocols and datasets for assessment. This paper aims to categorize and review existing CL literature for sequential data processing, as well as propose two new benchmarks for CL with sequential data. The benchmarks will resemble real-world applications, and the study will provide a comprehensive evaluation of CL and Recurrent Neural Networks in a class-incremental scenario. The results emphasize the importance of sequence length and clear specification of the CL scenario. The study will not limit the strategies to sequential data processing.",1
"""How can we animate 3D-characters from a movie script or move robots by simply telling them what we would like them to do?"" ""How unstructured and complex can we make a sentence and still generate plausible movements from it?"" These are questions that need to be answered in the long-run, as the field is still in its infancy. Inspired by these problems, we present a new technique for generating compositional actions, which handles complex input sentences. Our output is a 3D pose sequence depicting the actions in the input sentence. We propose a hierarchical two-stream sequential model to explore a finer joint-level mapping between natural language sentences and 3D pose sequences corresponding to the given motion. We learn two manifold representations of the motion -- one each for the upper body and the lower body movements. Our model can generate plausible pose sequences for short sentences describing single actions as well as long compositional sentences describing multiple sequential and superimposed actions. We evaluate our proposed model on the publicly available KIT Motion-Language Dataset containing 3D pose data with 0-annotated sentences. Experimental results show that our model advances the state-of-the-art on text-based motion synthesis in objective evaluations by a margin of 50%. Qualitative evaluations based on a user study indicate that our synthesized motions are perceived to be the closest to the ground-truth motion captures for both short and compositional sentences.",0
"The field of animating 3D-characters and controlling robots through natural language commands is still in its early stages, leaving many questions unanswered. One such question is how complex a sentence can be while still generating realistic movements. To tackle this problem, a new technique for generating actions from complex input sentences has been developed. This technique uses a two-stream sequential model to map natural language sentences to 3D pose sequences, with separate representations for upper and lower body movements. This model can generate plausible pose sequences for both short and long sentences describing single or multiple actions. Evaluation on the KIT Motion-Language Dataset shows that this model outperforms previous methods by 50%, and user study results suggest that the synthesized motions are the most similar to ground-truth motion captures.",1
"Despite the progress in automatic detection of radiologic findings from chest X-ray (CXR) images in recent years, a quantitative evaluation of the explainability of these models is hampered by the lack of locally labeled datasets for different findings. With the exception of a few expert-labeled small-scale datasets for specific findings, such as pneumonia and pneumothorax, most of the CXR deep learning models to date are trained on global ""weak"" labels extracted from text reports, or trained via a joint image and unstructured text learning strategy. Inspired by the Visual Genome effort in the computer vision community, we constructed the first Chest ImaGenome dataset with a scene graph data structure to describe $242,072$ images. Local annotations are automatically produced using a joint rule-based natural language processing (NLP) and atlas-based bounding box detection pipeline. Through a radiologist constructed CXR ontology, the annotations for each CXR are connected as an anatomy-centered scene graph, useful for image-level reasoning and multimodal fusion applications. Overall, we provide: i) $1,256$ combinations of relation annotations between $29$ CXR anatomical locations (objects with bounding box coordinates) and their attributes, structured as a scene graph per image, ii) over $670,000$ localized comparison relations (for improved, worsened, or no change) between the anatomical locations across sequential exams, as well as ii) a manually annotated gold standard scene graph dataset from $500$ unique patients.",0
"Although there have been advancements in the automatic detection of radiologic findings from chest X-ray (CXR) images, the lack of locally labeled datasets for different findings impedes a quantitative evaluation of the explainability of these models. While there are some expert-labeled small-scale datasets available for specific findings like pneumonia and pneumothorax, most CXR deep learning models rely on global ""weak"" labels extracted from text reports, or are trained using a joint image and unstructured text learning strategy. Inspired by the Visual Genome project in the computer vision community, we have created the Chest ImaGenome dataset, which consists of $242,072$ images and utilizes a scene graph data structure to describe them. Local annotations are generated through a joint rule-based natural language processing (NLP) and atlas-based bounding box detection pipeline. Our dataset includes $1,256$ combinations of relation annotations between $29$ CXR anatomical locations, structured as a scene graph per image, over $670,000$ localized comparison relations between anatomical locations across sequential exams, and a manually annotated gold standard scene graph dataset from $500$ unique patients. Each CXR's annotations are connected as an anatomy-centered scene graph, facilitating image-level reasoning and multimodal fusion applications.",1
"Connecting Vision and Language plays an essential role in Generative Intelligence. For this reason, large research efforts have been devoted to image captioning, i.e. describing images with syntactically and semantically meaningful sentences. Starting from 2015 the task has generally been addressed with pipelines composed of a visual encoder and a language model for text generation. During these years, both components have evolved considerably through the exploitation of object regions, attributes, the introduction of multi-modal connections, fully-attentive approaches, and BERT-like early-fusion strategies. However, regardless of the impressive results, research in image captioning has not reached a conclusive answer yet. This work aims at providing a comprehensive overview of image captioning approaches, from visual encoding and text generation to training strategies, datasets, and evaluation metrics. In this respect, we quantitatively compare many relevant state-of-the-art approaches to identify the most impactful technical innovations in architectures and training strategies. Moreover, many variants of the problem and its open challenges are discussed. The final goal of this work is to serve as a tool for understanding the existing literature and highlighting the future directions for a research area where Computer Vision and Natural Language Processing can find an optimal synergy.",0
"Generative Intelligence heavily relies on the correlation between Vision and Language, and as a result, researchers have invested a significant amount of effort in the area of image captioning. This involves the creation of syntactically and semantically sound sentences that depict the content of images. Since 2015, image captioning has been approached using pipelines that consist of a visual encoder and a language model for text generation. Over time, these components have undergone substantial development through techniques such as object region exploitation, attribute integration, multi-modal connections, fully-attentive approaches, and BERT-like early-fusion strategies. Despite the impressive outcomes, image captioning research has yet to produce a definitive solution. Therefore, the goal of this study is to offer a comprehensive overview of image captioning approaches, including visual encoding, text generation, training strategies, datasets, and evaluation metrics. The paper also quantitatively compares various state-of-the-art approaches to identify the most impactful technical innovations in architectures and training strategies. Additionally, it discusses the different variants of the image captioning problem and its open challenges. Ultimately, this work aims to serve as a resource to understand the current literature and to suggest future directions for the field, which will allow Computer Vision and Natural Language Processing to achieve optimal synergy.",1
"Gradient quantization is an emerging technique in reducing communication costs in distributed learning. Existing gradient quantization algorithms often rely on engineering heuristics or empirical observations, lacking a systematic approach to dynamically quantize gradients. This paper addresses this issue by proposing a novel dynamically quantized SGD (DQ-SGD) framework, enabling us to dynamically adjust the quantization scheme for each gradient descent step by exploring the trade-off between communication cost and convergence error. We derive an upper bound, tight in some cases, of the convergence error for a restricted family of quantization schemes and loss functions. We design our DQ-SGD algorithm via minimizing the communication cost under the convergence error constraints. Finally, through extensive experiments on large-scale natural language processing and computer vision tasks on AG-News, CIFAR-10, and CIFAR-100 datasets, we demonstrate that our quantization scheme achieves better tradeoffs between the communication cost and learning performance than other state-of-the-art gradient quantization methods.",0
"The use of gradient quantization is a promising approach to reduce communication expenses in distributed learning. However, current algorithms for gradient quantization rely on ad-hoc techniques or empirical observations, which lack a systematic methodology to quantize gradients dynamically. This research proposes a new method called dynamically quantized SGD (DQ-SGD) that allows us to adjust the quantization scheme for every gradient descent step. This approach allows us to explore the trade-off between communication cost and convergence error. We establish an upper bound, which is precise in some cases, of the convergence error for specific quantization schemes and loss functions. Our DQ-SGD algorithm minimizes the communication cost under the constraint of convergence error. We conducted experiments on AG-News, CIFAR-10, and CIFAR-100 datasets using large-scale natural language processing and computer vision tasks. Our results demonstrate that our quantization scheme provides better trade-offs between communication cost and learning performance than other state-of-the-art gradient quantization methods.",1
"Vision-and-Language Navigation (VLN) is a challenging task in which an agent needs to follow a language-specified path to reach a target destination. The goal gets even harder as the actions available to the agent get simpler and move towards low-level, atomic interactions with the environment. This setting takes the name of low-level VLN. In this paper, we strive for the creation of an agent able to tackle three key issues: multi-modality, long-term dependencies, and adaptability towards different locomotive settings. To that end, we devise ""Perceive, Transform, and Act"" (PTA): a fully-attentive VLN architecture that leaves the recurrent approach behind and the first Transformer-like architecture incorporating three different modalities - natural language, images, and low-level actions for the agent control. In particular, we adopt an early fusion strategy to merge lingual and visual information efficiently in our encoder. We then propose to refine the decoding phase with a late fusion extension between the agent's history of actions and the perceptual modalities. We experimentally validate our model on two datasets: PTA achieves promising results in low-level VLN on R2R and achieves good performance in the recently proposed R4R benchmark. Our code is publicly available at https://github.com/aimagelab/perceive-transform-and-act.",0
"The task of Vision-and-Language Navigation (VLN) is complex and requires an agent to follow a path described in language to reach a destination. This task becomes even more difficult when the agent's available actions become simpler and more basic. This is referred to as low-level VLN. The aim of this paper is to address three key challenges in this task: multi-modality, long-term dependencies, and adaptability to different locomotive settings. The authors propose a new architecture called ""Perceive, Transform, and Act"" (PTA) that incorporates natural language, images, and low-level actions to control the agent. Early fusion is used to efficiently merge lingual and visual information in the encoder, and a late fusion extension is proposed to refine the decoding phase by incorporating the agent's history of actions and perceptual modalities. The model is experimentally validated on two datasets and achieves promising results in low-level VLN and good performance in the R4R benchmark. The code is publicly available at https://github.com/aimagelab/perceive-transform-and-act.",1
"Relative position encoding (RPE) is important for transformer to capture sequence ordering of input tokens. General efficacy has been proven in natural language processing. However, in computer vision, its efficacy is not well studied and even remains controversial, e.g., whether relative position encoding can work equally well as absolute position? In order to clarify this, we first review existing relative position encoding methods and analyze their pros and cons when applied in vision transformers. We then propose new relative position encoding methods dedicated to 2D images, called image RPE (iRPE). Our methods consider directional relative distance modeling as well as the interactions between queries and relative position embeddings in self-attention mechanism. The proposed iRPE methods are simple and lightweight. They can be easily plugged into transformer blocks. Experiments demonstrate that solely due to the proposed encoding methods, DeiT and DETR obtain up to 1.5% (top-1 Acc) and 1.3% (mAP) stable improvements over their original versions on ImageNet and COCO respectively, without tuning any extra hyperparameters such as learning rate and weight decay. Our ablation and analysis also yield interesting findings, some of which run counter to previous understanding. Code and models are open-sourced at https://github.com/microsoft/Cream/tree/main/iRPE.",0
"Relative position encoding (RPE) plays a crucial role in allowing transformers to accurately capture the ordering of input tokens in a sequence. Its effectiveness in natural language processing has been well-established, but its efficacy in computer vision remains a topic of debate. Specifically, it is unclear whether relative position encoding can perform as well as absolute position encoding. To address this issue, we examine existing relative position encoding techniques and evaluate their strengths and weaknesses when applied to vision transformers. We then propose new methods for image RPE (iRPE) that are tailored specifically to 2D images. Our iRPE methods incorporate directional relative distance modeling and interactions between queries and relative position embeddings in the self-attention mechanism. They are lightweight and can be easily integrated into transformer blocks. Our experiments reveal that our encoding methods alone can significantly improve the performance of DeiT and DETR on ImageNet and COCO, respectively, by up to 1.5% (top-1 Acc) and 1.3% (mAP), without requiring additional tuning of hyperparameters such as learning rate and weight decay. Our analysis of the results also uncovers some unexpected findings that challenge previous assumptions. The code and models for our iRPE methods are available on GitHub at https://github.com/microsoft/Cream/tree/main/iRPE.",1
"Compared with the visual grounding on 2D images, the natural-language-guided 3D object localization on point clouds is more challenging. In this paper, we propose a new model, named InstanceRefer, to achieve a superior 3D visual grounding through the grounding-by-matching strategy. In practice, our model first predicts the target category from the language descriptions using a simple language classification model. Then, based on the category, our model sifts out a small number of instance candidates (usually less than 20) from the panoptic segmentation of point clouds. Thus, the non-trivial 3D visual grounding task has been effectively re-formulated as a simplified instance-matching problem, considering that instance-level candidates are more rational than the redundant 3D object proposals. Subsequently, for each candidate, we perform the multi-level contextual inference, i.e., referring from instance attribute perception, instance-to-instance relation perception, and instance-to-background global localization perception, respectively. Eventually, the most relevant candidate is selected and localized by ranking confidence scores, which are obtained by the cooperative holistic visual-language feature matching. Experiments confirm that our method outperforms previous state-of-the-arts on ScanRefer online benchmark and Nr3D/Sr3D datasets.",0
"The difficulty of natural-language-guided 3D object localization on point clouds is greater than that of visual grounding on 2D images. To address this challenge, we introduce a new model, InstanceRefer, which utilizes a grounding-by-matching strategy. Our model predicts the target category from language descriptions and identifies a small number of instance candidates based on the category. This simplifies the 3D visual grounding task by focusing on instance-level candidates rather than redundant 3D object proposals. We perform multi-level contextual inference on each candidate, including instance attribute perception, instance-to-instance relation perception, and instance-to-background global localization perception. Our model then ranks confidence scores to select the most relevant candidate. Our approach outperforms previous state-of-the-arts on ScanRefer online benchmark and Nr3D/Sr3D datasets.",1
"Video generation is one of the most challenging tasks in Machine Learning and Computer Vision fields of study. In this paper, we tackle the text to video generation problem, which is a conditional form of video generation. 0s can listen/read natural language sentences, and can imagine or visualize what is being described; therefore, we believe that video generation from natural language sentences will have an important impact on Artificial Intelligence. Video generation is relatively a new field of study in Computer Vision, which is far from being solved. The majority of recent works deal with synthetic datasets or real datasets with very limited types of objects, scenes, and emotions. To the best of our knowledge, this is the very first work on the text (free-form sentences) to video generation on more realistic video datasets like Actor and Action Dataset (A2D) or UCF101. We tackle the complicated problem of video generation by regressing the latent representations of the first and last frames and employing a context-aware interpolation method to build the latent representations of in-between frames. We propose a stacking ``upPooling'' block to sequentially generate RGB frames out of each latent representation and progressively increase the resolution. Moreover, our proposed Discriminator encodes videos based on single and multiple frames. We provide quantitative and qualitative results to support our arguments and show the superiority of our method over well-known baselines like Recurrent Neural Network (RNN) and Deconvolution (as known as Convolutional Transpose) based video generation methods.",0
"Generating videos is a challenging task in the fields of Machine Learning and Computer Vision. This paper addresses the problem of generating videos from text, which is a conditional form of video generation. 0s can understand natural language sentences and visualize what is being described, so generating videos from text could have a significant impact on Artificial Intelligence. Despite being a new field of study in Computer Vision, video generation is far from being solved. Recent works mostly focus on synthetic datasets or real datasets with limited types of objects, scenes, and emotions. This study is the first to address the text to video generation problem on more realistic video datasets such as Actor and Action Dataset (A2D) or UCF101. The study proposes a method that regresses the latent representations of the first and last frames and employs a context-aware interpolation method to build the latent representations of in-between frames. Additionally, a stacking ""upPooling"" block is used to sequentially generate RGB frames and increase the resolution. The proposed Discriminator encodes videos based on single and multiple frames. The study provides quantitative and qualitative results that support the superiority of the proposed method over well-known baselines such as Recurrent Neural Network (RNN) and Deconvolution-based video generation methods.",1
"Softmax classifiers with a very large number of classes naturally occur in many applications such as natural language processing and information retrieval. The calculation of full softmax is costly from the computational and energy perspective. There have been various sampling approaches to overcome this challenge, popularly known as negative sampling (NS). Ideally, NS should sample negative classes from a distribution that is dependent on the input data, the current parameters, and the correct positive class. Unfortunately, due to the dynamically updated parameters and data samples, there is no sampling scheme that is provably adaptive and samples the negative classes efficiently. Therefore, alternative heuristics like random sampling, static frequency-based sampling, or learning-based biased sampling, which primarily trade either the sampling cost or the adaptivity of samples per iteration are adopted. In this paper, we show two classes of distributions where the sampling scheme is truly adaptive and provably generates negative samples in near-constant time. Our implementation in C++ on CPU is significantly superior, both in terms of wall-clock time and accuracy, compared to the most optimized TensorFlow implementations of other popular negative sampling approaches on powerful NVIDIA V100 GPU.",0
"In many applications, such as natural language processing and information retrieval, softmax classifiers may have a large number of classes. However, the computational and energy costs of calculating full softmax can be a challenge. To address this issue, negative sampling (NS) has been proposed as a popular sampling approach. Ideally, NS should sample negative classes based on the input data, current parameters, and correct positive class. Unfortunately, due to the dynamic nature of parameters and data samples, no sampling scheme can efficiently and provably adapt to negative class sampling. Consequently, alternative heuristics are adopted, such as random sampling, static frequency-based sampling, or learning-based biased sampling, which trade off sampling cost or adaptivity per iteration. This paper presents two classes of distributions that generate negative samples in near-constant time and with truly adaptive sampling schemes. Our C++ implementation on CPU is significantly superior in terms of wall-clock time and accuracy compared to other popular negative sampling approaches optimized in TensorFlow on powerful NVIDIA V100 GPU.",1
"Image Captioning is a task that combines computer vision and natural language processing, where it aims to generate descriptive legends for images. It is a two-fold process relying on accurate image understanding and correct language understanding both syntactically and semantically. It is becoming increasingly difficult to keep up with the latest research and findings in the field of image captioning due to the growing amount of knowledge available on the topic. There is not, however, enough coverage of those findings in the available review papers. We perform in this paper a run-through of the current techniques, datasets, benchmarks and evaluation metrics used in image captioning. The current research on the field is mostly focused on deep learning-based methods, where attention mechanisms along with deep reinforcement and adversarial learning appear to be in the forefront of this research topic. In this paper, we review recent methodologies such as UpDown, OSCAR, VIVO, Meta Learning and a model that uses conditional generative adversarial nets. Although the GAN-based model achieves the highest score, UpDown represents an important basis for image captioning and OSCAR and VIVO are more useful as they use novel object captioning. This review paper serves as a roadmap for researchers to keep up to date with the latest contributions made in the field of image caption generation.",0
"The task of Image Captioning involves the combination of computer vision and natural language processing to generate descriptive captions for images. This process requires accurate understanding of both the image and language, both syntactically and semantically. Keeping up with the latest research and findings in this area has become increasingly difficult due to the abundance of knowledge available on the topic, which is not adequately covered in existing review papers. This paper provides an overview of the current techniques, datasets, benchmarks, and evaluation metrics used in Image Captioning. The focus of current research is mainly on deep learning-based methods, particularly those using attention mechanisms, deep reinforcement, and adversarial learning. This review paper discusses recent methodologies such as UpDown, OSCAR, VIVO, Meta Learning, and a model using conditional generative adversarial nets. Although the GAN-based model scores the highest, UpDown is a crucial foundation for Image Captioning, while OSCAR and VIVO are more advantageous in their use of novel object captioning. Researchers can use this review paper as a roadmap to stay up-to-date with the latest developments in Image Captioning.",1
"Image captioning is a task in the field of Artificial Intelligence that merges between computer vision and natural language processing. It is responsible for generating legends that describe images, and has various applications like descriptions used by assistive technology or indexing images (for search engines for instance). This makes it a crucial topic in AI that is undergoing a lot of research. This task however, like many others, is trained on large images labeled via 0 annotation, which can be very cumbersome: it needs manual effort, both financial and temporal costs, it is error-prone and potentially difficult to execute in some cases (e.g. medical images). To mitigate the need for labels, we attempt to use self-supervised learning, a type of learning where models use the data contained within the images themselves as labels. It is challenging to accomplish though, since the task is two-fold: the images and captions come from two different modalities and usually handled by different types of networks. It is thus not obvious what a completely self-supervised solution would look like. How it would achieve captioning in a comparable way to how self-supervision is applied today on image recognition tasks is still an ongoing research topic. In this project, we are using an encoder-decoder architecture where the encoder is a convolutional neural network (CNN) trained on OpenImages dataset and learns image features in a self-supervised fashion using the rotation pretext task. The decoder is a Long Short-Term Memory (LSTM), and it is trained, along within the image captioning model, on MS COCO dataset and is responsible of generating captions. Our GitHub repository can be found: https://github.com/elhagry1/SSL_ImageCaptioning_RotationPrediction",0
"The task of generating captions for images, known as image captioning, is a crucial area of study in Artificial Intelligence that combines computer vision and natural language processing. It has numerous applications, such as assisting technology and image indexing for search engines. However, the traditional method of training involves 0 annotation, which is time-consuming, costly, and prone to errors, making it challenging to execute, especially for medical images. To overcome this, self-supervised learning, where models use the data within the images themselves as labels, is being explored. However, achieving a self-supervised solution for image captioning is a challenge since the images and captions come from different modalities and are typically handled by different networks. In this project, we propose an encoder-decoder architecture, where the encoder is a self-supervised convolutional neural network (CNN) trained on OpenImages dataset, and the decoder is a Long Short-Term Memory (LSTM) trained on MS COCO dataset to generate captions. Our GitHub repository can be accessed via https://github.com/elhagry1/SSL_ImageCaptioning_RotationPrediction.",1
"Recent advances in the areas of multimodal machine learning and artificial intelligence (AI) have led to the development of challenging tasks at the intersection of Computer Vision, Natural Language Processing, and Embodied AI. Whereas many approaches and previous survey pursuits have characterised one or two of these dimensions, there has not been a holistic analysis at the center of all three. Moreover, even when combinations of these topics are considered, more focus is placed on describing, e.g., current architectural methods, as opposed to also illustrating high-level challenges and opportunities for the field. In this survey paper, we discuss Embodied Vision-Language Planning (EVLP) tasks, a family of prominent embodied navigation and manipulation problems that jointly use computer vision and natural language. We propose a taxonomy to unify these tasks and provide an in-depth analysis and comparison of the new and current algorithmic approaches, metrics, simulated environments, as well as the datasets used for EVLP tasks. Finally, we present the core challenges that we believe new EVLP works should seek to address, and we advocate for task construction that enables model generalizability and furthers real-world deployment.",0
"Advancements in multimodal machine learning and AI have resulted in the emergence of complex tasks that intersect Computer Vision, Natural Language Processing, and Embodied AI. While previous studies have focused on one or two of these dimensions, there has been a lack of comprehensive analysis that considers all three. Furthermore, previous research has mainly focused on describing architectural methods, rather than highlighting high-level challenges and opportunities for the field. This survey paper aims to address this gap by discussing Embodied Vision-Language Planning (EVLP) tasks, which are a set of embodied navigation and manipulation problems that use computer vision and natural language. The paper proposes a taxonomy to unify these tasks and provides an in-depth analysis and comparison of current algorithmic approaches, metrics, simulated environments, and datasets used for EVLP tasks. The paper also identifies core challenges that new EVLP works should address and advocates for task construction that promotes model generalizability and facilitates real-world deployment.",1
"Image manipulation with natural language, which aims to manipulate images with the guidance of language descriptions, has been a challenging problem in the fields of computer vision and natural language processing (NLP). Currently, a number of efforts have been made for this task, but their performances are still distant away from generating realistic and text-conformed manipulated images. Therefore, in this paper, we propose a memory-based Image Manipulation Network (MIM-Net), where a set of memories learned from images is introduced to synthesize the texture information with the guidance of the textual description. We propose a two-stage network with an additional reconstruction stage to learn the latent memories efficiently. To avoid the unnecessary background changes, we propose a Target Localization Unit (TLU) to focus on the manipulation of the region mentioned by the text. Moreover, to learn a robust memory, we further propose a novel randomized memory training loss. Experiments on the four popular datasets show the better performance of our method compared to the existing ones.",0
"The manipulation of images using natural language descriptions has proven to be a difficult problem in the fields of computer vision and natural language processing. Despite several attempts to address this challenge, the current methods are unable to generate images that are both realistic and conform to the accompanying text. In this paper, we introduce the Memory-based Image Manipulation Network (MIM-Net), which utilizes a set of learned memories from images to synthesize texture information with the aid of textual descriptions. Our two-stage network, including a reconstruction stage, efficiently learns latent memories. We also propose a Target Localization Unit (TLU) to minimize unnecessary background changes and a novel randomized memory training loss to enhance memory robustness. Our experiments on four popular datasets demonstrate that our approach outperforms existing methods.",1
"We introduce a method that allows to automatically segment images into semantically meaningful regions without 0 supervision. Derived regions are consistent across different images and coincide with 0-defined semantic classes on some datasets. In cases where semantic regions might be hard for 0 to define and consistently label, our method is still able to find meaningful and consistent semantic classes. In our work, we use pretrained StyleGAN2~\cite{karras2020analyzing} generative model: clustering in the feature space of the generative model allows to discover semantic classes. Once classes are discovered, a synthetic dataset with generated images and corresponding segmentation masks can be created. After that a segmentation model is trained on the synthetic dataset and is able to generalize to real images. Additionally, by using CLIP~\cite{radford2021learning} we are able to use prompts defined in a natural language to discover some desired semantic classes. We test our method on publicly available datasets and show state-of-the-art results.",0
"We present a technique that enables automatic segmentation of images into semantically meaningful regions without requiring any 0 input. The resulting regions are consistent across various images and correspond to 0-defined semantic categories in some datasets. Our method is capable of identifying significant and consistent semantic classes in situations where it may be challenging for 0s to define and label them. To achieve this, we utilize a pre-trained generative model called StyleGAN2~\cite{karras2020analyzing} and perform clustering in its feature space. The discovered classes are then used to generate synthetic images along with their respective segmentation masks. Subsequently, a segmentation model can be trained on this synthetic dataset and applied to real images. Additionally, we leverage CLIP~\cite{radford2021learning} to identify desired semantic categories using natural language prompts. We evaluate our approach on publicly available datasets and demonstrate its superior performance.",1
"Self-supervised pre-training of large-scale transformer models on text corpora followed by finetuning has achieved state-of-the-art on a number of natural language processing tasks. Recently, Lu et al. (2021, arXiv:2103.05247) claimed that frozen pretrained transformers (FPTs) match or outperform training from scratch as well as unfrozen (fine-tuned) pretrained transformers in a set of transfer tasks to other modalities. In our work, we find that this result is, in fact, an artifact of not tuning the learning rates. After carefully redesigning the empirical setup, we find that when tuning learning rates properly, pretrained transformers do outperform or match training from scratch in all of our tasks, but only as long as the entire model is finetuned. Thus, while transfer from pretrained language models to other modalities does indeed provide gains and hints at exciting possibilities for future work, properly tuning hyperparameters is important for arriving at robust findings.",0
"The leading natural language processing performance has been achieved through self-supervised pre-training of large transformer models on text corpora, followed by finetuning. Recently, Lu et al. (2021, arXiv:2103.05247) reported that frozen pretrained transformers (FPTs) perform as well as or even better than both training from scratch and unfrozen (fine-tuned) pretrained transformers in transfer tasks to other modalities. However, our study shows that this result is an artifact resulting from improper tuning of learning rates. Through careful redesign of the experimental setup, we demonstrate that pretrained transformers outperform or match training from scratch in all our tasks, but only when the entire model is finetuned and hyperparameters are properly tuned. This highlights the importance of tuning hyperparameters to obtain robust findings and suggests exciting possibilities for future research in transfer learning from pretrained language models to other modalities.",1
"Transformer with self-attention has led to the revolutionizing of natural language processing field, and recently inspires the emergence of Transformer-style architecture design with competitive results in numerous computer vision tasks. Nevertheless, most of existing designs directly employ self-attention over a 2D feature map to obtain the attention matrix based on pairs of isolated queries and keys at each spatial location, but leave the rich contexts among neighbor keys under-exploited. In this work, we design a novel Transformer-style module, i.e., Contextual Transformer (CoT) block, for visual recognition. Such design fully capitalizes on the contextual information among input keys to guide the learning of dynamic attention matrix and thus strengthens the capacity of visual representation. Technically, CoT block first contextually encodes input keys via a $3\times3$ convolution, leading to a static contextual representation of inputs. We further concatenate the encoded keys with input queries to learn the dynamic multi-head attention matrix through two consecutive $1\times1$ convolutions. The learnt attention matrix is multiplied by input values to achieve the dynamic contextual representation of inputs. The fusion of the static and dynamic contextual representations are finally taken as outputs. Our CoT block is appealing in the view that it can readily replace each $3\times3$ convolution in ResNet architectures, yielding a Transformer-style backbone named as Contextual Transformer Networks (CoTNet). Through extensive experiments over a wide range of applications (e.g., image recognition, object detection and instance segmentation), we validate the superiority of CoTNet as a stronger backbone. Source code is available at \url{https://github.com/JDAI-CV/CoTNet}.",0
"The use of self-attention in Transformers has revolutionized natural language processing and inspired the creation of Transformer-style designs for computer vision tasks, which have yielded competitive results. However, most existing designs for visual recognition use self-attention on a 2D feature map to create an attention matrix based on isolated queries and keys, neglecting the contextual information among neighbor keys. To address this issue, we introduce a novel Transformer-style module called the Contextual Transformer (CoT) block, which fully utilizes contextual information among input keys to strengthen visual representation. The CoT block encodes input keys through a $3\times3$ convolution to create a static contextual representation, concatenates the encoded keys with input queries to learn a dynamic multi-head attention matrix through two consecutive $1\times1$ convolutions, and multiplies the learned attention matrix by input values to achieve dynamic contextual representation. The static and dynamic contextual representations are fused and outputted. The CoT block can easily replace each $3\times3$ convolution in ResNet architectures, creating a Transformer-style backbone called the Contextual Transformer Networks (CoTNet). Extensive experiments across various applications validate the superiority of CoTNet as a stronger backbone. The source code is available at \url{https://github.com/JDAI-CV/CoTNet}.",1
"When 0s solve complex problems, they rarely come up with a decision right-away. Instead, they start with an intuitive decision, reflect upon it, spot mistakes, resolve contradictions and jump between different hypotheses. Thus, they create a sequence of ideas and follow a train of thought that ultimately reaches a conclusive decision. Contrary to this, today's neural classification models are mostly trained to map an input to one single and fixed output. In this paper, we investigate how we can give models the opportunity of a second, third and $k$-th thought. We take inspiration from Hegel's dialectics and propose a method that turns an existing classifier's class prediction (such as the image class forest) into a sequence of predictions (such as forest $\rightarrow$ tree $\rightarrow$ mushroom). Concretely, we propose a correction module that is trained to estimate the model's correctness as well as an iterative prediction update based on the prediction's gradient. Our approach results in a dynamic system over class probability distributions $\unicode{x2014}$ the thought flow. We evaluate our method on diverse datasets and tasks from computer vision and natural language processing. We observe surprisingly complex but intuitive behavior and demonstrate that our method (i) can correct misclassifications, (ii) strengthens model performance, (iii) is robust to high levels of adversarial attacks, (iv) can increase accuracy up to 4% in a label-distribution-shift setting and (iv) provides a tool for model interpretability that uncovers model knowledge which otherwise remains invisible in a single distribution prediction.",0
"0s do not typically make immediate decisions when faced with complex problems. Instead, they rely on intuition to make an initial decision and then reflect on it, identify errors, address contradictions, and consider different hypotheses. This process leads to a sequence of ideas that ultimately leads to a conclusive decision. However, current neural classification models are designed to produce a single, fixed output based on an input. In this paper, we explore the possibility of allowing models to have multiple thoughts when making predictions. Our approach is inspired by Hegel's dialectics and involves a correction module that estimates the model's correctness and an iterative prediction update based on the prediction's gradient. This results in a dynamic system that follows a train of thought and produces a sequence of predictions instead of a single output. We evaluate our method on various datasets and tasks and observe that it corrects misclassifications, improves model performance, is resistant to adversarial attacks, increases accuracy in a label-distribution-shift setting, and provides a means for model interpretability by uncovering knowledge that would remain hidden in a single distribution prediction.",1
"We describe an efficient hierarchical method to compute attention in the Transformer architecture. The proposed attention mechanism exploits a matrix structure similar to the Hierarchical Matrix (H-Matrix) developed by the numerical analysis community, and has linear run time and memory complexity. We perform extensive experiments to show that the inductive bias embodied by our hierarchical attention is effective in capturing the hierarchical structure in the sequences typical for natural language and vision tasks. Our method is superior to alternative sub-quadratic proposals by over +6 points on average on the Long Range Arena benchmark. It also sets a new SOTA test perplexity on One-Billion Word dataset with 5x fewer model parameters than that of the previous-best Transformer-based models.",0
"An effective way to calculate attention in the Transformer architecture is presented in this study using a hierarchical method. The proposed mechanism takes advantage of a matrix structure inspired by the Hierarchical Matrix (H-Matrix) used in numerical analysis, resulting in linear run time and memory complexity. To demonstrate the effectiveness of our hierarchical attention approach in capturing the hierarchical patterns found in typical natural language and vision tasks, we conducted extensive experiments. Compared to sub-quadratic alternatives, our method outperforms them by more than +6 points on average on the Long Range Arena benchmark. Moreover, our technique achieves a new state-of-the-art (SOTA) test perplexity on the One-Billion Word dataset, using 5x fewer model parameters than the previous-best Transformer-based models.",1
"This paper is a presentation of a new method for denoising images using Haralick features and further segmenting the characters using artificial neural networks. The image is divided into kernels, each of which is converted to a GLCM (Gray Level Co-Occurrence Matrix) on which a Haralick Feature generation function is called, the result of which is an array with fourteen elements corresponding to fourteen features The Haralick values and the corresponding noise/text classification form a dictionary, which is then used to de-noise the image through kernel comparison. Segmentation is the process of extracting characters from a document and can be used when letters are separated by white space, which is an explicit boundary marker. Segmentation is the first step in many Natural Language Processing problems. This paper explores the process of segmentation using Neural Networks. While there have been numerous methods to segment characters of a document, this paper is only concerned with the accuracy of doing so using neural networks. It is imperative that the characters be segmented correctly, for failing to do so will lead to incorrect recognition by Natural language processing tools. Artificial Neural Networks was used to attain accuracy of upto 89%. This method is suitable for languages where the characters are delimited by white space. However, this method will fail to provide acceptable results when the language heavily uses connected letters. An example would be the Devanagari script, which is predominantly used in northern India.",0
"This paper presents a novel approach to denoising images by utilizing Haralick features and segmenting characters using artificial neural networks. The image is divided into kernels, each of which is transformed into a GLCM. A Haralick feature generation function is applied to the GLCM, producing an array of fourteen elements representing fourteen features. These values, along with corresponding noise/text classifications, are used to create a dictionary for de-noising the image via kernel comparison. Segmentation is crucial for extracting characters from a document, especially when white space is present as a clear boundary marker. This paper explores the use of neural networks for accurate segmentation, which is essential for successful Natural Language Processing outcomes. The proposed method yields up to 89% accuracy and is suitable for languages with white space as character delimiters. This approach may not be effective for languages with connected letters, such as the Devanagari script used primarily in northern India.",1
"Vision and Language Navigation (VLN) requires an agent to navigate to a target location by following natural language instructions. Most of existing works represent a navigation candidate by the feature of the corresponding single view where the candidate lies in. However, an instruction may mention landmarks out of the single view as references, which might lead to failures of textual-visual matching of existing methods. In this work, we propose a multi-module Neighbor-View Enhanced Model (NvEM) to adaptively incorporate visual contexts from neighbor views for better textual-visual matching. Specifically, our NvEM utilizes a subject module and a reference module to collect contexts from neighbor views. The subject module fuses neighbor views at a global level, and the reference module fuses neighbor objects at a local level. Subjects and references are adaptively determined via attention me'chanisms. Our model also includes an action module to utilize the strong orientation guidance (e.g., ""turn left"") in instructions. Each module predicts navigation action separately and their weighted sum is used for predicting the final action. Extensive experimental results demonstrate the effectiveness of the proposed method on the R2R and R4R benchmarks against several state-of-the-art navigators, and NvEM even beats some pre-training ones. Our code is available at https://github.com/MarSaKi/NvEM.",0
"The task of Vision and Language Navigation (VLN) involves an agent navigating to a specified location based on natural language instructions. However, previous works have relied on features from a single view to represent navigation candidates, which may cause issues when instructions reference landmarks outside of that view. To address this, we introduce the Neighbor-View Enhanced Model (NvEM), which incorporates visual context from neighboring views to improve textual-visual matching. The NvEM includes a subject module and a reference module that collect context from neighboring views, with subjects and references determined by attention mechanisms. Additionally, an action module uses orientation guidance in instructions to predict navigation actions, with each module's output being weighted for the final prediction. We demonstrate the effectiveness of the NvEM on the R2R and R4R benchmarks, outperforming several state-of-the-art navigators and even pre-training models. Our code is available at https://github.com/MarSaKi/NvEM.",1
"Language instruction plays an essential role in the natural language grounded navigation tasks. However, navigators trained with limited 0-annotated instructions may have difficulties in accurately capturing key information from the complicated instruction at different timesteps, leading to poor navigation performance. In this paper, we exploit to train a more robust navigator which is capable of dynamically extracting crucial factors from the long instruction, by using an adversarial attacking paradigm. Specifically, we propose a Dynamic Reinforced Instruction Attacker (DR-Attacker), which learns to mislead the navigator to move to the wrong target by destroying the most instructive information in instructions at different timesteps. By formulating the perturbation generation as a Markov Decision Process, DR-Attacker is optimized by the reinforcement learning algorithm to generate perturbed instructions sequentially during the navigation, according to a learnable attack score. Then, the perturbed instructions, which serve as hard samples, are used for improving the robustness of the navigator with an effective adversarial training strategy and an auxiliary self-supervised reasoning task. Experimental results on both Vision-and-Language Navigation (VLN) and Navigation from Dialog History (NDH) tasks show the superiority of our proposed method over state-of-the-art methods. Moreover, the visualization analysis shows the effectiveness of the proposed DR-Attacker, which can successfully attack crucial information in the instructions at different timesteps. Code is available at https://github.com/expectorlin/DR-Attacker.",0
"In natural language based navigation tasks, language instruction is crucial. However, navigators trained with limited 0-annotated instructions may struggle to accurately comprehend the complex instruction at different stages, resulting in poor navigation performance. This paper proposes a Dynamic Reinforced Instruction Attacker (DR-Attacker) to train a more robust navigator that can dynamically extract vital factors from the lengthy instruction. The DR-Attacker uses an adversarial attacking paradigm to mislead the navigator and destroy the most instructive information in instructions at different timesteps. It generates perturbed instructions sequentially during navigation, and these perturbed instructions serve as hard samples for improving the navigator's robustness. The proposed method surpasses state-of-the-art methods in the Vision-and-Language Navigation (VLN) and Navigation from Dialog History (NDH) tasks. The visualization analysis demonstrates the effectiveness of the DR-Attacker in attacking crucial information in the instructions. The code is available at https://github.com/expectorlin/DR-Attacker.",1
"We define disentanglement as how far class-different data points from each other are, relative to the distances among class-similar data points. When maximizing disentanglement during representation learning, we obtain a transformed feature representation where the class memberships of the data points are preserved. If the class memberships of the data points are preserved, we would have a feature representation space in which a nearest neighbour classifier or a clustering algorithm would perform well. We take advantage of this method to learn better natural language representation, and employ it on text classification and text clustering tasks. Through disentanglement, we obtain text representations with better-defined clusters and improve text classification performance. Our approach had a test classification accuracy of as high as 90.11% and test clustering accuracy of 88% on the AG News dataset, outperforming our baseline models -- without any other training tricks or regularization.",0
"Disentanglement is defined as the extent to which dissimilar data points of different classes are separated compared to the distances between similar data points of the same class. By maximizing disentanglement during representation learning, we can create a transformed feature representation that preserves the class memberships of the data points. This results in a feature space where nearest neighbour classifiers or clustering algorithms perform well. We apply this approach to improve natural language representation in text classification and text clustering tasks. Through disentanglement, we obtain text representations with well-defined clusters and achieve better text classification performance. Our method outperforms baseline models without any additional training tricks or regularization, achieving a test classification accuracy of up to 90.11% and a test clustering accuracy of 88% on the AG News dataset.",1
"We present a new four-pronged approach to build firefighter's situational awareness for the first time in the literature. We construct a series of deep learning frameworks built on top of one another to enhance the safety, efficiency, and successful completion of rescue missions conducted by firefighters in emergency first response settings. First, we used a deep Convolutional Neural Network (CNN) system to classify and identify objects of interest from thermal imagery in real-time. Next, we extended this CNN framework for object detection, tracking, segmentation with a Mask RCNN framework, and scene description with a multimodal natural language processing(NLP) framework. Third, we built a deep Q-learning-based agent, immune to stress-induced disorientation and anxiety, capable of making clear navigation decisions based on the observed and stored facts in live-fire environments. Finally, we used a low computational unsupervised learning technique called tensor decomposition to perform meaningful feature extraction for anomaly detection in real-time. With these ad-hoc deep learning structures, we built the artificial intelligence system's backbone for firefighters' situational awareness. To bring the designed system into usage by firefighters, we designed a physical structure where the processed results are used as inputs in the creation of an augmented reality capable of advising firefighters of their location and key features around them, which are vital to the rescue operation at hand, as well as a path planning feature that acts as a virtual guide to assist disoriented first responders in getting back to safety. When combined, these four approaches present a novel approach to information understanding, transfer, and synthesis that could dramatically improve firefighter response and efficacy and reduce life loss.",0
"In this paper, we introduce a novel approach to enhancing firefighters' situational awareness using a four-pronged strategy. Our approach involves the use of a series of deep learning frameworks to enhance safety, efficiency, and successful completion of rescue missions in emergency first response settings. Firstly, we employ a deep Convolutional Neural Network (CNN) system to classify and identify objects of interest from thermal imagery in real-time. Secondly, we extend this CNN framework to include object detection, tracking, segmentation with a Mask RCNN framework, and scene description with a multimodal natural language processing (NLP) framework. Thirdly, we build a deep Q-learning-based agent that can navigate live-fire environments, immune to stress-induced disorientation and anxiety. Finally, we use a low computational unsupervised learning technique called tensor decomposition to perform meaningful feature extraction for anomaly detection in real-time. With these ad-hoc deep learning structures, we develop an artificial intelligence system as a backbone for firefighters' situational awareness. We also design a physical structure to bring the system into usage by firefighters, where the processed results are used to create an augmented reality that advises firefighters of their location and key features around them, as well as a path planning feature that acts as a virtual guide for disoriented first responders. Combining these four approaches can significantly improve information understanding, transfer, and synthesis, leading to a reduction in life loss and an increase in firefighter response and efficacy.",1
"Deep learning (DL) techniques have achieved great success in predictive accuracy in a variety of tasks, but deep neural networks (DNNs) are shown to produce highly overconfident scores for even abnormal samples. Well-defined uncertainty indicates whether a model's output should (or should not) be trusted and thus becomes critical in real-world scenarios which typically involves shifted input distributions due to many factors. Existing uncertainty approaches assume that testing samples from a different data distribution would induce unreliable model predictions thus have higher uncertainty scores. They quantify model uncertainty by calibrating DL model's confidence of a given input and evaluate the effectiveness in computer vision (CV) and natural language processing (NLP)-related tasks. However, their methodologies' reliability may be compromised under programming tasks due to difference in data representations and shift patterns. In this paper, we first define three different types of distribution shift in program data and build a large-scale shifted Java dataset. We implement two common programming language tasks on our dataset to study the effect of each distribution shift on DL model performance. We also propose a large-scale benchmark of existing state-of-the-art predictive uncertainty on programming tasks and investigate their effectiveness under data distribution shift. Experiments show that program distribution shift does degrade the DL model performance to varying degrees and that existing uncertainty methods all present certain limitations in quantifying uncertainty on program dataset.",0
"Predictive accuracy has been significantly improved by deep learning (DL) techniques, but deep neural networks (DNNs) can produce overconfident scores even for abnormal samples. In real-world situations, with input distributions that are typically shifted due to various factors, well-defined uncertainty is crucial to determine whether a model's output should be trusted. Current uncertainty approaches assume that testing samples from a different data distribution would result in unreliable model predictions and, therefore, have higher uncertainty scores. These approaches quantify model uncertainty by calibrating the DL model's confidence of a given input and evaluating its effectiveness in computer vision (CV) and natural language processing (NLP)-related tasks. However, their reliability may be compromised under programming tasks due to differences in data representations and shift patterns. This paper introduces three types of distribution shift in program data and builds a large-scale shifted Java dataset. Two common programming language tasks are implemented on the dataset to study the effect of each distribution shift on DL model performance. Additionally, a large-scale benchmark of existing state-of-the-art predictive uncertainty on programming tasks is proposed, and the effectiveness of these methods under data distribution shift is investigated. The experiments indicate that program distribution shift can degrade DL model performance to varying degrees, and that existing uncertainty methods all present certain limitations in quantifying uncertainty on program datasets.",1
"Faces generated using generative adversarial networks (GANs) have reached unprecedented realism. These faces, also known as ""Deep Fakes"", appear as realistic photographs with very little pixel-level distortions. While some work has enabled the training of models that lead to the generation of specific properties of the subject, generating a facial image based on a natural language description has not been fully explored. For security and criminal identification, the ability to provide a GAN-based system that works like a sketch artist would be incredibly useful. In this paper, we present a novel approach to generate facial images from semantic text descriptions. The learned model is provided with a text description and an outline of the type of face, which the model uses to sketch the features. Our models are trained using an Affine Combination Module (ACM) mechanism to combine the text embedding from BERT and the GAN latent space using a self-attention matrix. This avoids the loss of features due to inadequate ""attention"", which may happen if text embedding and latent vector are simply concatenated. Our approach is capable of generating images that are very accurately aligned to the exhaustive textual descriptions of faces with many fine detail features of the face and helps in generating better images. The proposed method is also capable of making incremental changes to a previously generated image if it is provided with additional textual descriptions or sentences.",0
"Unprecedented levels of realism have been achieved in faces generated using generative adversarial networks (GANs), also known as ""Deep Fakes"". These faces resemble authentic photographs with minimal pixel-level distortions, and while some models can generate specific features of the subject, the ability to create facial images based on natural language descriptions has not been fully explored. A GAN-based system that functions like a sketch artist would be incredibly valuable for security and criminal identification purposes. This paper introduces a new method for generating facial images from semantic text descriptions. Our model utilizes an Affine Combination Module (ACM) mechanism to combine BERT text embeddings and the GAN latent space, resulting in more accurate alignment to exhaustive textual descriptions and finer facial details. Additionally, our approach can make incremental changes to previously generated images with additional textual descriptions.",1
"Deep learning's success has been widely recognized in a variety of machine learning tasks, including image classification, audio recognition, and natural language processing. As an extension of deep learning beyond these domains, graph neural networks (GNNs) are designed to handle the non-Euclidean graph-structure which is intractable to previous deep learning techniques. Existing GNNs are presented using various techniques, making direct comparison and cross-reference more complex. Although existing studies categorize GNNs into spatial-based and spectral-based techniques, there hasn't been a thorough examination of their relationship. To close this gap, this study presents a single framework that systematically incorporates most GNNs. We organize existing GNNs into spatial and spectral domains, as well as expose the connections within each domain. A review of spectral graph theory and approximation theory builds a strong relationship across the spatial and spectral domains in further investigation.",0
"The success of deep learning has been widely acknowledged in several machine learning tasks, such as natural language processing, audio recognition, and image classification. Graph neural networks (GNNs) have been developed to extend deep learning beyond these domains to handle non-Euclidean graph-structures that were previously unmanageable with deep learning techniques. Presently, existing GNNs are categorized using different approaches, making it difficult to compare and cross-reference them directly. Although spatial-based and spectral-based techniques have been used to categorize GNNs, there hasn't been a comprehensive analysis of their relationship. To address this gap, this study introduces a single framework that systematically incorporates most GNNs. We classify existing GNNs into spatial and spectral domains and reveal the connections within each domain. Furthermore, we explore the relationship between the spatial and spectral domains by reviewing spectral graph theory and approximation theory.",1
"Communication between agents in collaborative multi-agent settings is in general implicit or a direct data stream. This paper considers text-based natural language as a novel form of communication between multiple agents trained with reinforcement learning. This could be considered first steps toward a truly autonomous communication without the need to define a limited set of instructions, and natural collaboration between 0s and robots. Inspired by the game of Blind Leads, we propose an environment where one agent uses natural language instructions to guide another through a maze. We test the ability of reinforcement learning agents to effectively communicate through discrete word-level symbols and show that the agents are able to sufficiently communicate through natural language with a limited vocabulary. Although the communication is not always perfect English, the agents are still able to navigate the maze. We achieve a BLEU score of 0.85, which is an improvement of 0.61 over randomly generated sequences while maintaining a 100% maze completion rate. This is a 3.5 times the performance of the random baseline using our reference set.",0
"In collaborative multi-agent settings, communication typically occurs through implicit means or direct data transmission. This paper explores the use of text-based natural language as a new form of communication between multiple agents trained with reinforcement learning. This approach could pave the way for truly autonomous communication without the need for predefined instructions, enabling seamless collaboration between 0s and robots. Taking inspiration from the game of Blind Leads, we propose an environment where one agent guides another through a maze using natural language instructions. Our tests demonstrate that reinforcement learning agents are capable of effective communication using a limited vocabulary of discrete word-level symbols, even if their language is not always perfect. Despite this, the agents are able to navigate the maze with a 100% completion rate, achieving an impressive BLEU score of 0.85, which represents a 0.61 improvement over randomly generated sequences. Our results outperform the random baseline by 3.5 times using our reference set.",1
"Reinforcement learning (RL) is typically concerned with estimating single-step policies or single-step models, leveraging the Markov property to factorize the problem in time. However, we can also view RL as a sequence modeling problem, with the goal being to predict a sequence of actions that leads to a sequence of high rewards. Viewed in this way, it is tempting to consider whether powerful, high-capacity sequence prediction models that work well in other domains, such as natural-language processing, can also provide simple and effective solutions to the RL problem. To this end, we explore how RL can be reframed as ""one big sequence modeling"" problem, using state-of-the-art Transformer architectures to model distributions over sequences of states, actions, and rewards. Addressing RL as a sequence modeling problem significantly simplifies a range of design decisions: we no longer require separate behavior policy constraints, as is common in prior work on offline model-free RL, and we no longer require ensembles or other epistemic uncertainty estimators, as is common in prior work on model-based RL. All of these roles are filled by the same Transformer sequence model. In our experiments, we demonstrate the flexibility of this approach across long-horizon dynamics prediction, imitation learning, goal-conditioned RL, and offline RL.",0
"Reinforcement learning (RL) typically involves estimating single-step policies or models using the Markov property to simplify the problem over time. However, it is also possible to view RL as a sequence modeling problem, where the aim is to predict a sequence of actions leading to high rewards. Researchers are exploring whether sequence prediction models, which are effective in other domains like natural language processing, can also provide simple solutions to the RL problem. To achieve this, they are using state-of-the-art Transformer architectures to model distributions over sequences of states, actions, and rewards. Reframing RL as a sequence modeling problem simplifies design decisions, eliminating the need for separate behavior policy constraints and epistemic uncertainty estimators. The same Transformer sequence model can fulfill all these roles. In experiments, this approach has shown flexibility in long-horizon dynamics prediction, imitation learning, goal-conditioned RL, and offline RL.",1
"Developing video understanding intelligence is quite challenging because it requires holistic integration of images, scripts, and sounds based on natural language processing, temporal dependency, and reasoning. Recently, substantial attempts have been made on several video datasets with associated question answering (QA) on a large scale. However, existing evaluation metrics for video question answering (VideoQA) do not provide meaningful analysis. To make progress, we argue that a well-made framework, established on the way 0s understand, is required to explain and evaluate the performance of understanding in detail. Then we propose a top-down evaluation system for VideoQA, based on the cognitive process of 0s and story elements: Cognitive Modules for Evaluation (CogME). CogME is composed of three cognitive modules: targets, contents, and thinking. The interaction among the modules in the understanding procedure can be expressed in one sentence as follows: ""I understand the CONTENT of the TARGET through a way of THINKING."" Each module has sub-components derived from the story elements. We can specify the required aspects of understanding by annotating the sub-components to individual questions. CogME thus provides a framework for an elaborated specification of VideoQA datasets. To examine the suitability of a VideoQA dataset for validating video understanding intelligence, we evaluated the baseline model of the DramaQA dataset by applying CogME. The evaluation reveals that story elements are unevenly reflected in the existing dataset, and the model based on the dataset may cause biased predictions. Although this study has only been able to grasp a narrow range of stories, we expect that it offers the first step in considering the cognitive process of 0s on the video understanding intelligence of 0s and AI.",0
"The development of video understanding intelligence is a complex process that involves integrating images, scripts, and sounds using natural language processing, temporal dependency, and reasoning. While there have been significant efforts to develop and evaluate video question answering (VideoQA) systems on large-scale datasets, the existing evaluation metrics do not provide meaningful insights. To make progress in this field, we propose a top-down evaluation system called Cognitive Modules for Evaluation (CogME), which is based on the cognitive process of 0s and story elements. CogME comprises three cognitive modules: targets, contents, and thinking, which interact to facilitate understanding. Each module has sub-components that can be annotated to individual questions, providing a detailed specification of VideoQA datasets. Using CogME, we evaluated the baseline model of the DramaQA dataset and found that the story elements were unevenly reflected in the dataset, which may lead to biased predictions by the model. While our study only covers a limited range of stories, we believe that this approach offers a promising first step towards understanding the cognitive process of 0s and AI in video understanding intelligence.",1
"Detecting customized moments and highlights from videos given natural language (NL) user queries is an important but under-studied topic. One of the challenges in pursuing this direction is the lack of annotated data. To address this issue, we present the Query-based Video Highlights (QVHighlights) dataset. It consists of over 10,000 YouTube videos, covering a wide range of topics, from everyday activities and travel in lifestyle vlog videos to social and political activities in news videos. Each video in the dataset is annotated with: (1) a 0-written free-form NL query, (2) relevant moments in the video w.r.t. the query, and (3) five-point scale saliency scores for all query-relevant clips. This comprehensive annotation enables us to develop and evaluate systems that detect relevant moments as well as salient highlights for diverse, flexible user queries. We also present a strong baseline for this task, Moment-DETR, a transformer encoder-decoder model that views moment retrieval as a direct set prediction problem, taking extracted video and query representations as inputs and predicting moment coordinates and saliency scores end-to-end. While our model does not utilize any 0 prior, we show that it performs competitively when compared to well-engineered architectures. With weakly supervised pretraining using ASR captions, Moment-DETR substantially outperforms previous methods. Lastly, we present several ablations and visualizations of Moment-DETR. Data and code is publicly available at https://github.com/jayleicn/moment_detr",0
"The detection of personalized moments and highlights from videos based on natural language user queries is an important yet understudied topic. A major challenge in this area is the lack of annotated data. To address this issue, we introduce the Query-based Video Highlights (QVHighlights) dataset, which comprises over 10,000 YouTube videos covering various topics. Each video is annotated with a 0-written NL query, related moments in the video, and saliency scores for all relevant clips. This extensive annotation allows us to develop and evaluate systems that detect relevant moments and highlights for a wide range of user queries. We present a transformer encoder-decoder model, Moment-DETR, as a strong baseline for this task, which views moment retrieval as a direct set prediction problem. Although Moment-DETR does not use 0 prior, it performs competitively when compared to well-engineered architectures. With weakly supervised pretraining using ASR captions, Moment-DETR outperforms previous methods. Additionally, we provide several ablations and visualizations of Moment-DETR, and our data and code are publicly available on GitHub.",1
"We develop a novel approach to conformal prediction when the target task has limited data available for training. Conformal prediction identifies a small set of promising output candidates in place of a single prediction, with guarantees that the set contains the correct answer with high probability. When training data is limited, however, the predicted set can easily become unusably large. In this work, we obtain substantially tighter prediction sets while maintaining desirable marginal guarantees by casting conformal prediction as a meta-learning paradigm over exchangeable collections of auxiliary tasks. Our conformalization algorithm is simple, fast, and agnostic to the choice of underlying model, learning algorithm, or dataset. We demonstrate the effectiveness of this approach across a number of few-shot classification and regression tasks in natural language processing, computer vision, and computational chemistry for drug discovery.",0
"Our innovative approach to conformal prediction addresses the challenge of limited training data for the target task. Conformal prediction generates a small set of potential output options instead of a single prediction, ensuring a high probability of containing the correct answer. However, when training data is scarce, the predicted set may become impractically large. To overcome this, we propose a meta-learning approach to conformal prediction with exchangeable auxiliary tasks, resulting in significantly tighter prediction sets while maintaining desirable marginal guarantees. Our algorithm is versatile, efficient, and independent of the model, learning algorithm, or dataset used. We validate the effectiveness of this approach in various few-shot classification and regression tasks in natural language processing, computer vision, and computational chemistry for drug discovery.",1
"Transformers have been successful for many natural language processing tasks. However, applying transformers to the video domain for tasks such as long-term video generation and scene understanding has remained elusive due to the high computational complexity and the lack of natural tokenization. In this paper, we propose the Object-Centric Video Transformer (OCVT) which utilizes an object-centric approach for decomposing scenes into tokens suitable for use in a generative video transformer. By factoring the video into objects, our fully unsupervised model is able to learn complex spatio-temporal dynamics of multiple interacting objects in a scene and generate future frames of the video. Our model is also significantly more memory-efficient than pixel-based models and thus able to train on videos of length up to 70 frames with a single 48GB GPU. We compare our model with previous RNN-based approaches as well as other possible video transformer baselines. We demonstrate OCVT performs well when compared to baselines in generating future frames. OCVT also develops useful representations for video reasoning, achieving start-of-the-art performance on the CATER task.",0
"Although transformers have proven to be effective in natural language processing, their application to the video domain for tasks like long-term video generation and scene understanding has been challenging due to computational complexity and the lack of natural tokenization. To address this issue, we introduce the Object-Centric Video Transformer (OCVT), which adopts an object-centric approach to break down scenes into tokens that are suitable for use in a generative video transformer. By breaking down the video into objects, our unsupervised model can learn the complex spatio-temporal dynamics of multiple interacting objects in a scene and generate future frames of the video. Additionally, our model is much more memory-efficient than pixel-based models, allowing it to train on videos up to 70 frames long with a single 48GB GPU. We compare our model to previous RNN-based approaches and other possible video transformer baselines, showing that OCVT performs well in generating future frames and achieving state-of-the-art performance on the CATER task for video reasoning.",1
"Video captioning, i.e. the task of generating captions from video sequences creates a bridge between the Natural Language Processing and Computer Vision domains of computer science. The task of generating a semantically accurate description of a video is quite complex. Considering the complexity, of the problem, the results obtained in recent research works are praiseworthy. However, there is plenty of scope for further investigation. This paper addresses this scope and proposes a novel solution. Most video captioning models comprise two sequential/recurrent layers - one as a video-to-context encoder and the other as a context-to-caption decoder. This paper proposes a novel architecture, namely Semantically Sensible Video Captioning (SSVC) which modifies the context generation mechanism by using two novel approaches - ""stacked attention"" and ""spatial hard pull"". As there are no exclusive metrics for evaluating video captioning models, we emphasize both quantitative and qualitative analysis of our model. Hence, we have used the BLEU scoring metric for quantitative analysis and have proposed a 0 evaluation metric for qualitative analysis, namely the Semantic Sensibility (SS) scoring metric. SS Score overcomes the shortcomings of common automated scoring metrics. This paper reports that the use of the aforementioned novelties improves the performance of state-of-the-art architectures.",0
"The generation of captions from video sequences is a task that connects the domains of Natural Language Processing and Computer Vision in computer science. It is a complex task to create a semantically accurate description of a video, and recent research has produced impressive results. However, there is still much room for further investigation, and this paper proposes a novel solution called Semantically Sensible Video Captioning (SSVC). This architecture modifies the context generation mechanism by using ""stacked attention"" and ""spatial hard pull"" approaches. To evaluate SSVC, both quantitative and qualitative analyses are conducted using the BLEU scoring metric and a 0 evaluation metric called Semantic Sensibility (SS) scoring metric. This paper reports that the use of these novelties improves the performance of state-of-the-art architectures.",1
"Academic advances of AI models in high-precision domains, like healthcare, need to be made explainable in order to enhance real-world adoption. Our past studies and ongoing interactions indicate that medical experts can use AI systems with greater trust if there are ways to connect the model inferences about patients to explanations that are tied back to the context of use. Specifically, risk prediction is a complex problem of diagnostic and interventional importance to clinicians wherein they consult different sources to make decisions. To enable the adoption of the ever improving AI risk prediction models in practice, we have begun to explore techniques to contextualize such models along three dimensions of interest: the patients' clinical state, AI predictions about their risk of complications, and algorithmic explanations supporting the predictions. We validate the importance of these dimensions by implementing a proof-of-concept (POC) in type-2 diabetes (T2DM) use case where we assess the risk of chronic kidney disease (CKD) - a common T2DM comorbidity. Within the POC, we include risk prediction models for CKD, post-hoc explainers of the predictions, and other natural-language modules which operationalize domain knowledge and CPGs to provide context. With primary care physicians (PCP) as our end-users, we present our initial results and clinician feedback in this paper. Our POC approach covers multiple knowledge sources and clinical scenarios, blends knowledge to explain data and predictions to PCPs, and received an enthusiastic response from our medical expert.",0
"In order to increase the practical application of AI models in high-precision domains, such as healthcare, it is necessary to make academic advancements in explainability. Our previous research and ongoing interactions have shown that medical professionals are more inclined to trust AI systems if they can connect the model's patient inferences to explanations that relate back to the context of use. In particular, risk prediction is a complex issue that is crucial for clinicians when making diagnostic and interventional decisions, involving consultation with various sources. To facilitate the integration of increasingly advanced AI risk prediction models into practice, we are exploring techniques to contextualize these models across three dimensions: the patient's clinical state, the AI's predictions regarding their risk of complications, and algorithmic explanations that support these predictions. To validate the importance of these dimensions, we have implemented a proof-of-concept in a type-2 diabetes use case focused on assessing the risk of chronic kidney disease - a common comorbidity. Our POC includes risk prediction models, post-hoc explainers, and natural-language modules that operationalize domain knowledge and clinical practice guidelines to provide context. The results and feedback from primary care physicians who have used our POC approach in multiple clinical scenarios have been positive and encouraging.",1
"Black-box machine learning learning methods are now routinely used in high-risk settings, like medical diagnostics, which demand uncertainty quantification to avoid consequential model failures. Distribution-free uncertainty quantification (distribution-free UQ) is a user-friendly paradigm for creating statistically rigorous confidence intervals/sets for such predictions. Critically, the intervals/sets are valid without distributional assumptions or model assumptions, with explicit guarantees with finitely many datapoints. Moreover, they adapt to the difficulty of the input; when the input example is difficult, the uncertainty intervals/sets are large, signaling that the model might be wrong. Without much work, one can use distribution-free methods on any underlying algorithm, such as a neural network, to produce confidence sets guaranteed to contain the ground truth with a user-specified probability, such as 90%. Indeed, the methods are easy-to-understand and general, applying to many modern prediction problems arising in the fields of computer vision, natural language processing, deep reinforcement learning, and so on. This hands-on introduction is aimed at a reader interested in the practical implementation of distribution-free UQ, including conformal prediction and related methods, who is not necessarily a statistician. We will include many explanatory illustrations, examples, and code samples in Python, with PyTorch syntax. The goal is to provide the reader a working understanding of distribution-free UQ, allowing them to put confidence intervals on their algorithms, with one self-contained document.",0
"Black-box machine learning methods are commonly used in high-risk situations, such as medical diagnostics, where the risk of model failure is high. To prevent such failures, uncertainty quantification is crucial. Distribution-free uncertainty quantification (distribution-free UQ) is a simple method for creating statistically sound confidence intervals/sets for predictions that do not require distributional or model assumptions. These intervals/sets are valid for any number of data points and adjust to input difficulty, with larger intervals/sets indicating potential model errors. Distribution-free methods can be applied to any underlying algorithm, including neural networks, to produce confidence sets with a user-specified probability. These methods are widely applicable to various prediction problems in computer vision, natural language processing, and deep reinforcement learning. This practical introduction is intended for non-statisticians interested in implementing distribution-free UQ and includes detailed explanations, examples, and Python code samples using PyTorch syntax to help readers achieve a working understanding of the topic.",1
"Natural language often exhibits inherent hierarchical structure ingrained with complex syntax and semantics. However, most state-of-the-art deep generative models learn embeddings only in Euclidean vector space, without accounting for this structural property of language. In this paper, we investigate text generation in a hyperbolic latent space to learn continuous hierarchical representations. An Adversarial Poincare Variational Autoencoder (APo-VAE) is presented, where both the prior and variational posterior of latent variables are defined over a Poincare ball via wrapped normal distributions. By adopting the primal-dual formulation of KL divergence, an adversarial learning procedure is introduced to empower robust model training. Extensive experiments in language modeling and dialog-response generation tasks demonstrate the winning effectiveness of the proposed APo-VAE model over VAEs in Euclidean latent space, thanks to its superb capabilities in capturing latent language hierarchies in hyperbolic space.",0
"The structure of natural language is often hierarchical and complex in terms of syntax and semantics. However, many advanced deep generative models only learn embeddings in Euclidean vector space, ignoring this structural property of language. This paper explores the use of a hyperbolic latent space for text generation, which allows for continuous hierarchical representations. The Adversarial Poincare Variational Autoencoder (APo-VAE) is introduced, where both the prior and variational posterior of latent variables are defined over a Poincare ball using wrapped normal distributions. An adversarial learning procedure is implemented using the primal-dual formulation of KL divergence to enhance model training. Extensive experiments in language modeling and dialog-response generation tasks reveal the superior capabilities of the APo-VAE model, compared to VAEs in Euclidean latent space, in capturing latent language hierarchies in hyperbolic space.",1
"The world of empirical machine learning (ML) strongly relies on benchmarks in order to determine the relative effectiveness of different algorithms and methods. This paper proposes the notion of ""a benchmark lottery"" that describes the overall fragility of the ML benchmarking process. The benchmark lottery postulates that many factors, other than fundamental algorithmic superiority, may lead to a method being perceived as superior. On multiple benchmark setups that are prevalent in the ML community, we show that the relative performance of algorithms may be altered significantly simply by choosing different benchmark tasks, highlighting the fragility of the current paradigms and potential fallacious interpretation derived from benchmarking ML methods. Given that every benchmark makes a statement about what it perceives to be important, we argue that this might lead to biased progress in the community. We discuss the implications of the observed phenomena and provide recommendations on mitigating them using multiple machine learning domains and communities as use cases, including natural language processing, computer vision, information retrieval, recommender systems, and reinforcement learning.",0
"Benchmarks play a crucial role in empirical machine learning (ML) to evaluate the effectiveness of various algorithms and methods. This article introduces the concept of a ""benchmark lottery"" to describe the vulnerability of the ML benchmarking process. The benchmark lottery theory suggests that many factors, apart from fundamental algorithmic superiority, could influence the perception of a method as superior. Our research on prevalent benchmark setups in the ML community demonstrates that the relative performance of algorithms can be significantly altered by selecting different benchmark tasks, exposing the fragility of the current paradigms and the potential for erroneous interpretation of benchmarking ML methods. As every benchmark reflects what it deems important, we argue that this may lead to biased progress in the community. We examine the implications of these findings and propose solutions to mitigate them, using various machine learning domains and communities as examples, such as natural language processing, computer vision, information retrieval, recommender systems, and reinforcement learning.",1
"Visual Question Answering (VQA) is concerned with answering free-form questions about an image. Since it requires a deep semantic and linguistic understanding of the question and the ability to associate it with various objects that are present in the image, it is an ambitious task and requires multi-modal reasoning from both computer vision and natural language processing. We propose Graphhopper, a novel method that approaches the task by integrating knowledge graph reasoning, computer vision, and natural language processing techniques. Concretely, our method is based on performing context-driven, sequential reasoning based on the scene entities and their semantic and spatial relationships. As a first step, we derive a scene graph that describes the objects in the image, as well as their attributes and their mutual relationships. Subsequently, a reinforcement learning agent is trained to autonomously navigate in a multi-hop manner over the extracted scene graph to generate reasoning paths, which are the basis for deriving answers. We conduct an experimental study on the challenging dataset GQA, based on both manually curated and automatically generated scene graphs. Our results show that we keep up with a 0 performance on manually curated scene graphs. Moreover, we find that Graphhopper outperforms another state-of-the-art scene graph reasoning model on both manually curated and automatically generated scene graphs by a significant margin.",0
"The task of Visual Question Answering (VQA) involves answering open-ended questions related to an image, which requires a thorough understanding of natural language and objects present in the image. This task requires multi-modal reasoning using computer vision and natural language processing techniques. We introduce a novel approach called Graphhopper that integrates knowledge graph reasoning, computer vision, and natural language processing techniques for this task. Our method involves context-driven, sequential reasoning based on the objects in the image and their relationships. We first derive a scene graph that describes the objects, their attributes, and their relationships, and then use a reinforcement learning agent to generate reasoning paths for answering questions. We evaluated our method on the challenging GQA dataset using both manually curated and automatically generated scene graphs. The results show that Graphhopper performs on par with 0 performance on manually curated scene graphs and outperforms another state-of-the-art scene graph reasoning model on both manually curated and automatically generated scene graphs by a significant margin.",1
"We address the problem of text-guided video temporal grounding, which aims to identify the time interval of certain event based on a natural language description. Different from most existing methods that only consider RGB images as visual features, we propose a multi-modal framework to extract complementary information from videos. Specifically, we adopt RGB images for appearance, optical flow for motion, and depth maps for image structure. While RGB images provide abundant visual cues of certain event, the performance may be affected by background clutters. Therefore, we use optical flow to focus on large motion and depth maps to infer the scene configuration when the action is related to objects recognizable with their shapes. To integrate the three modalities more effectively and enable inter-modal learning, we design a dynamic fusion scheme with transformers to model the interactions between modalities. Furthermore, we apply intra-modal self-supervised learning to enhance feature representations across videos for each modality, which also facilitates multi-modal learning. We conduct extensive experiments on the Charades-STA and ActivityNet Captions datasets, and show that the proposed method performs favorably against state-of-the-art approaches.",0
"Our focus is on solving the issue of text-guided video temporal grounding, where the goal is to identify the time interval of an event based on natural language description. Unlike existing methods that only use RGB images as visual features, our proposed solution is a multi-modal framework that extracts complementary information from videos. We use RGB images for appearance, optical flow for motion, and depth maps for image structure. As RGB images can be affected by background clutter, we utilize optical flow to concentrate on large motion and depth maps to infer the scene configuration when the action involves objects that can be recognized by their shapes. To effectively integrate the three modalities and enable inter-modal learning, we create a dynamic fusion scheme with transformers to model the interaction between modalities. Additionally, we use intra-modal self-supervised learning to improve feature representations across videos for each modality, which also helps with multi-modal learning. Our experiments on the Charades-STA and ActivityNet Captions datasets demonstrate the superior performance of our method compared to state-of-the-art approaches.",1
"With the rise of voice chat rooms, a gigantic resource of data can be exposed to the research community for natural language processing tasks. Moderators in voice chat rooms actively monitor the discussions and remove the participants with offensive language. However, it makes the hate speech detection even more difficult since some participants try to find creative ways to articulate hate speech. This makes the hate speech detection challenging in new social media like Clubhouse. To the best of our knowledge all the hate speech datasets have been collected from text resources like Twitter. In this paper, we take the first step to collect a significant dataset from Clubhouse as the rising star in social media industry. We analyze the collected instances from statistical point of view using the Google Perspective Scores. Our experiments show that, the Perspective Scores can outperform Bag of Words and Word2Vec as high level text features.",0
"The emergence of voice chat rooms presents a vast pool of data for the natural language processing research community. While moderators are active in removing offensive language, some participants attempt to express hate speech in creative ways, making detection challenging in new social media platforms like Clubhouse. Previously, hate speech datasets were collected from text resources like Twitter. This paper takes the first step in gathering a significant dataset from Clubhouse and analyzes instances using the Google Perspective Scores. Results show that Perspective Scores perform better than Bag of Words and Word2Vec as high-level text features.",1
"Compared to consumer lending, Micro, Small and Medium Enterprise (mSME) credit risk modelling is particularly challenging, as, often, the same sources of information are not available. Therefore, it is standard policy for a loan officer to provide a textual loan assessment to mitigate limited data availability. In turn, this statement is analysed by a credit expert alongside any available standard credit data. In our paper, we exploit recent advances from the field of Deep Learning and Natural Language Processing (NLP), including the BERT (Bidirectional Encoder Representations from Transformers) model, to extract information from 60 000 textual assessments provided by a lender. We consider the performance in terms of the AUC (Area Under the receiver operating characteristic Curve) and Brier Score metrics and find that the text alone is surprisingly effective for predicting default. However, when combined with traditional data, it yields no additional predictive capability, with performance dependent on the text's length. Our proposed deep learning model does, however, appear to be robust to the quality of the text and therefore suitable for partly automating the mSME lending process. We also demonstrate how the content of loan assessments influences performance, leading us to a series of recommendations on a new strategy for collecting future mSME loan assessments.",0
"Credit risk modelling for Micro, Small and Medium Enterprises (mSMEs) is more difficult than for consumer lending due to the lack of available information. Loan officers typically provide a textual assessment to compensate for this. Our study utilizes Deep Learning and Natural Language Processing (NLP) techniques, such as the BERT model, to extract information from 60,000 textual assessments. Our findings show that text alone is surprisingly effective in predicting default, but when combined with traditional data, it does not improve predictive capability. The length of the text affects performance, but our deep learning model is robust to text quality, making it suitable for partially automating the mSME lending process. Our research also highlights the impact of loan assessment content on performance and provides recommendations for collecting mSME loan assessments in the future.",1
"Medical Visual Question Answering (VQA) is a multi-modal challenging task widely considered by research communities of the computer vision and natural language processing. Since most current medical VQA models focus on visual content, ignoring the importance of text, this paper proposes a multi-view attention-based model(MuVAM) for medical visual question answering which integrates the high-level semantics of medical images on the basis of text description. Firstly, different methods are utilized to extract the features of the image and the question for the two modalities of vision and text. Secondly, this paper proposes a multi-view attention mechanism that include Image-to-Question (I2Q) attention and Word-to-Text (W2T) attention. Multi-view attention can correlate the question with image and word in order to better analyze the question and get an accurate answer. Thirdly, a composite loss is presented to predict the answer accurately after multi-modal feature fusion and improve the similarity between visual and textual cross-modal features. It consists of classification loss and image-question complementary (IQC) loss. Finally, for data errors and missing labels in the VQA-RAD dataset, we collaborate with medical experts to correct and complete this dataset and then construct an enhanced dataset, VQA-RADPh. The experiments on these two datasets show that the effectiveness of MuVAM surpasses the state-of-the-art method.",0
"The research communities of computer vision and natural language processing widely consider Medical Visual Question Answering (VQA) a challenging multi-modal task. However, most current medical VQA models focus solely on visual content, neglecting the importance of text. This paper proposes a new approach called the multi-view attention-based model (MuVAM) for medical visual question answering. This model integrates the high-level semantics of medical images with text descriptions. To achieve this, the paper utilizes different methods to extract features from the image and question for both modalities of vision and text. Additionally, MuVAM incorporates a multi-view attention mechanism that includes Image-to-Question (I2Q) attention and Word-to-Text (W2T) attention. This attention mechanism allows the model to correlate the question with the image and word to better analyze the question and provide an accurate answer. The paper then presents a composite loss to predict the answer accurately after multi-modal feature fusion and improve the similarity between visual and textual cross-modal features. This loss consists of a classification loss and an image-question complementary (IQC) loss. Finally, the paper addresses data errors and missing labels in the VQA-RAD dataset by collaborating with medical experts to correct and complete the dataset, resulting in an enhanced dataset called VQA-RADPh. The experiments using these two datasets show that MuVAM is more effective than the state-of-the-art methods.",1
"Attribute extrapolation in sample generation is challenging for deep neural networks operating beyond the training distribution. We formulate a new task for extrapolation in sequence generation, focusing on natural language and proteins, and propose GENhance, a generative framework that enhances attributes through a learned latent space. Trained on movie reviews and a computed protein stability dataset, GENhance can generate strongly-positive text reviews and highly stable protein sequences without being exposed to similar data during training. We release our benchmark tasks and models to contribute to the study of generative modeling extrapolation and data-driven design in biology and chemistry.",0
"Generating samples with extrapolated attributes is a difficult task for deep neural networks that operate outside the scope of their training distribution. To address this challenge, we introduce a new task in the realm of sequence generation, specifically in the domains of natural language and proteins. Our proposed solution is GENhance, a generative framework that employs a learned latent space to enhance attributes. By training on movie reviews and a computed protein stability dataset, GENhance is capable of generating highly positive text reviews and stable protein sequences, even in the absence of similar training data. We are publicly releasing our benchmark tasks and models to contribute to research in the areas of generative modeling extrapolation and data-driven design in chemistry and biology.",1
"In this article, we present a Shell Language Preprocessing (SLP) library, which implements tokenization and encoding directed on the parsing of Unix and Linux shell commands. We describe the rationale behind the need for a new approach with specific examples when conventional Natural Language Processing (NLP) pipelines fail. Furthermore, we evaluate our methodology on a security classification task against widely accepted information and communications technology (ICT) tokenization techniques and achieve significant improvement of an F1-score from 0.392 to 0.874.",0
"The Shell Language Preprocessing (SLP) library is introduced in this article. It carries out tokenization and encoding specifically aimed at the parsing of Unix and Linux shell commands. The article explains the reasons why a new approach was required, citing examples where traditional Natural Language Processing (NLP) pipelines were ineffective. The library's effectiveness is then demonstrated by showcasing its successful application in a security classification task. A comparison with conventional information and communications technology (ICT) tokenization techniques reveals a remarkable improvement in the F1-score, from 0.392 to 0.874.",1
