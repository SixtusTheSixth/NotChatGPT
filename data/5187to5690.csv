"This paper shows how a machine, which observes stimuli through an uncharacterized, uncalibrated channel and sensor, can glean machine-independent information (i.e., channel- and sensor-independent information) about the stimuli. First, we demonstrate that a machine defines a specific coordinate system on the stimulus state space, with the nature of that coordinate system depending on the device's channel and sensor. Thus, machines with different channels and sensors ""see"" the same stimulus trajectory through state space, but in different machine-specific coordinate systems. For a large variety of physical stimuli, statistical properties of that trajectory endow the stimulus configuration space with differential geometric structure (a metric and parallel transfer procedure), which can then be used to represent relative stimulus configurations in a coordinate-system-independent manner (and, therefore, in a channel- and sensor-independent manner). The resulting description is an ""inner"" property of the stimulus time series in the sense that it does not depend on extrinsic factors like the observer's choice of a coordinate system in which the stimulus is viewed (i.e., the observer's choice of channel and sensor). This methodology is illustrated with analytic examples and with a numerically simulated experiment. In an intelligent sensory device, this kind of representation ""engine"" could function as a ""front-end"" that passes channel/sensor-independent stimulus representations to a pattern recognition module. After a pattern recognizer has been trained in one of these devices, it could be used without change in other devices having different channels and sensors.",0
"This paper presents a method for a machine to extract information about stimuli independently from the channel and sensor used to observe them. The machine establishes a specific coordinate system for the stimulus state space, which varies depending on the channel and sensor employed. However, the statistical properties of the stimulus trajectory endow the stimulus configuration space with a differential geometric structure, allowing for a channel- and sensor-independent representation of relative stimulus configurations. This representation is an ""inner"" property of the stimulus time series and does not depend on extrinsic factors such as the observer's choice of channel and sensor. The method is demonstrated through analytic examples and a simulated experiment and could be utilized as a ""front-end"" for an intelligent sensory device's pattern recognition module. Once trained, this module could be used across different devices with varying channels and sensors without any modifications.",1
"For the last years, time-series mining has become a challenging issue for researchers. An important application lies in most monitoring purposes, which require analyzing large sets of time-series for learning usual patterns. Any deviation from this learned profile is then considered as an unexpected situation. Moreover, complex applications may involve the temporal study of several heterogeneous parameters. In that paper, we propose a method for mining heterogeneous multivariate time-series for learning meaningful patterns. The proposed approach allows for mixed time-series -- containing both pattern and non-pattern data -- such as for imprecise matches, outliers, stretching and global translating of patterns instances in time. We present the early results of our approach in the context of monitoring the health status of a person at home. The purpose is to build a behavioral profile of a person by analyzing the time variations of several quantitative or qualitative parameters recorded through a provision of sensors installed in the home.",0
"Researchers have faced challenges in time-series mining in recent years, particularly in applications requiring the analysis of large sets of time-series for identifying usual patterns and detecting unexpected situations. Furthermore, complex applications may involve the temporal study of multiple heterogeneous parameters. This paper proposes a method for mining heterogeneous multivariate time-series to identify meaningful patterns, including mixed time-series with both pattern and non-pattern data, imprecise matches, outliers, stretching, and global translating of pattern instances in time. Early results of the approach are presented in the context of monitoring a person's health at home by analyzing time variations of various quantitative or qualitative parameters recorded through sensors installed in the home to create a behavioral profile.",1
"In the computational-mechanics structural analysis of one-dimensional cellular automata the following automata-theoretic analogue of the \emph{change-point problem} from time series analysis arises: \emph{Given a string $\sigma$ and a collection $\{\mc{D}_i\}$ of finite automata, identify the regions of $\sigma$ that belong to each $\mc{D}_i$ and, in particular, the boundaries separating them.} We present two methods for solving this \emph{multi-regular language filtering problem}. The first, although providing the ideal solution, requires a stack, has a worst-case compute time that grows quadratically in $\sigma$'s length and conditions its output at any point on arbitrarily long windows of future input. The second method is to algorithmically construct a transducer that approximates the first algorithm. In contrast to the stack-based algorithm, however, the transducer requires only a finite amount of memory, runs in linear time, and gives immediate output for each letter read; it is, moreover, the best possible finite-state approximation with these three features.",0
"The analysis of one-dimensional cellular automata using computational mechanics involves a problem similar to the change-point problem in time series analysis. This problem asks for the identification of regions belonging to a given collection of finite automata, including the boundaries that separate them, within a given string. We offer two solutions to this multi-regular language filtering problem. The first solution, while optimal, relies on a stack and has a worst-case compute time that grows quadratically with the length of the string. Additionally, it requires knowledge of arbitrarily long windows of future input to produce output. The second solution involves constructing a transducer that approximates the first algorithm. Unlike the stack-based algorithm, the transducer only requires finite memory and runs in linear time, producing immediate output for each letter read. This second solution is the best possible finite-state approximation with these three features.",1
"Emergent behaviors are in the focus of recent research interest. It is then of considerable importance to investigate what optimizations suit the learning and prediction of chaotic systems, the putative candidates for emergence. We have compared L1 and L2 regularizations on predicting chaotic time series using linear recurrent neural networks. The internal representation and the weights of the networks were optimized in a unifying framework. Computational tests on different problems indicate considerable advantages for the L1 regularization: It had considerably better learning time and better interpolating capabilities. We shall argue that optimization viewed as a maximum likelihood estimation justifies our results, because L1 regularization fits heavy-tailed distributions -- an apparently general feature of emergent systems -- better.",0
"Recent research has focused on emergent behaviors, making it important to investigate which optimizations are best suited for learning and predicting chaotic systems, which are considered potential candidates for emergence. Our study compared the effectiveness of L1 and L2 regularizations in predicting chaotic time series using linear recurrent neural networks. We optimized the internal representation and weights of the networks using a unified framework. Our computational tests on various problems demonstrated that L1 regularization had significant advantages, including faster learning time and better interpolating capabilities. We argue that our results are justified by viewing optimization as a maximum likelihood estimation, as L1 regularization is better suited for fitting heavy-tailed distributions - a common characteristic of emergent systems.",1
"We present a new algorithm for discovering patterns in time series and other sequential data. We exhibit a reliable procedure for building the minimal set of hidden, Markovian states that is statistically capable of producing the behavior exhibited in the data -- the underlying process's causal states. Unlike conventional methods for fitting hidden Markov models (HMMs) to data, our algorithm makes no assumptions about the process's causal architecture (the number of hidden states and their transition structure), but rather infers it from the data. It starts with assumptions of minimal structure and introduces complexity only when the data demand it. Moreover, the causal states it infers have important predictive optimality properties that conventional HMM states lack. We introduce the algorithm, review the theory behind it, prove its asymptotic reliability, use large deviation theory to estimate its rate of convergence, and compare it to other algorithms which also construct HMMs from data. We also illustrate its behavior on an example process, and report selected numerical results from an implementation.",0
"A novel algorithm is presented for detecting patterns in sequential data, including time series. The algorithm demonstrates a dependable approach to establishing the smallest set of hidden, Markovian states that can statistically generate the behavior demonstrated in the data, known as the causal states of the underlying process. Unlike conventional methods used for fitting hidden Markov models (HMMs) to data, this algorithm does not make assumptions about the causal architecture of the process, such as the number of hidden states and their transition structure, but instead infers it from the data. It begins with minimal structure assumptions and only introduces complexity when necessary based on the data. Additionally, the causal states inferred by the algorithm possess predictive optimality properties that are absent in conventional HMM states. The algorithm is introduced, and its theory is reviewed, including proof of its asymptotic reliability and estimation of its rate of convergence using large deviation theory. Comparisons are made with other algorithms that construct HMMs from data, and an example process is used to illustrate its behavior. The implementation of the algorithm also includes selected numerical results.",1
"This paper proposes a fusion method of modalities extracted from video through a three-stream network with spatio-temporal and temporal convolutions for fine-grained action classification in sport. It is applied to TTStroke-21 dataset which consists of untrimmed videos of table tennis games. The goal is to detect and classify table tennis strokes in the videos, the first step of a bigger scheme aiming at giving feedback to the players for improving their performance. The three modalities are raw RGB data, the computed optical flow and the estimated pose of the player. The network consists of three branches with attention blocks. Features are fused at the latest stage of the network using bilinear layers. Compared to previous approaches, the use of three modalities allows faster convergence and better performances on both tasks: classification of strokes with known temporal boundaries and joint segmentation and classification. The pose is also further investigated in order to offer richer feedback to the athletes.",0
"In this article, a novel technique for fine-grained action classification in sports is proposed. The approach involves fusing modalities extracted from video data through a three-stream network with spatio-temporal and temporal convolutions. The proposed method is applied to the TTStroke-21 dataset, which comprises of untrimmed table tennis game videos, with the aim of detecting and classifying table tennis strokes. This is the first step towards a larger scheme aimed at providing players with feedback to improve their performance. The three modalities used in the network are raw RGB data, computed optical flow, and estimated player pose. The network comprises of three branches with attention blocks, and features are fused using bilinear layers at the final stage. Compared to previous methods, the use of three modalities results in faster convergence and better performance on both stroke classification and joint segmentation and classification. Moreover, the player's pose is analyzed to provide more comprehensive feedback to athletes.",1
"Establishing robust and accurate correspondences between a pair of images is a long-standing computer vision problem with numerous applications. While classically dominated by sparse methods, emerging dense approaches offer a compelling alternative paradigm that avoids the keypoint detection step. However, dense flow estimation is often inaccurate in the case of large displacements, occlusions, or homogeneous regions. In order to apply dense methods to real-world applications, such as pose estimation, image manipulation, or 3D reconstruction, it is therefore crucial to estimate the confidence of the predicted matches.   We propose the Enhanced Probabilistic Dense Correspondence Network, PDC-Net+, capable of estimating accurate dense correspondences along with a reliable confidence map. We develop a flexible probabilistic approach that jointly learns the flow prediction and its uncertainty. In particular, we parametrize the predictive distribution as a constrained mixture model, ensuring better modelling of both accurate flow predictions and outliers. Moreover, we develop an architecture and an enhanced training strategy tailored for robust and generalizable uncertainty prediction in the context of self-supervised training. Our approach obtains state-of-the-art results on multiple challenging geometric matching and optical flow datasets. We further validate the usefulness of our probabilistic confidence estimation for the tasks of pose estimation, 3D reconstruction, image-based localization, and image retrieval. Code and models are available at https://github.com/PruneTruong/DenseMatching.",0
"The problem of establishing accurate and robust correspondences between two images has been a longstanding issue in computer vision, with many practical applications. Although sparse methods have traditionally dominated this field, newer dense approaches offer an alternative paradigm that eliminates the need for keypoint detection. However, these methods can be unreliable when dealing with large displacements, occlusions, or homogeneous regions. Therefore, in order to utilize dense methods in real-world applications such as image manipulation, pose estimation, or 3D reconstruction, it is crucial to accurately estimate the confidence of the predicted matches. To address this issue, we have developed the PDC-Net+ (Enhanced Probabilistic Dense Correspondence Network), which can estimate precise dense correspondences and a dependable confidence map. Our approach involves a flexible, probabilistic method that learns the flow prediction and its uncertainty jointly. We have employed a constrained mixture model to parametrize the predictive distribution, which provides better modeling of both accurate flow predictions and outliers. Additionally, we have developed an architecture and an improved training strategy tailored for robust and generalizable uncertainty prediction during self-supervised training. Our approach has achieved state-of-the-art results on several challenging optical flow and geometric matching datasets. We have also validated the usefulness of our probabilistic confidence estimation for tasks such as image-based localization, pose estimation, 3D reconstruction, and image retrieval. The code and models for our method are available at https://github.com/PruneTruong/DenseMatching.",1
"In this paper, we propose an end-to-end learning framework for event-based motion deblurring in a self-supervised manner, where real-world events are exploited to alleviate the performance degradation caused by data inconsistency. To achieve this end, optical flows are predicted from events, with which the blurry consistency and photometric consistency are exploited to enable self-supervision on the deblurring network with real-world data. Furthermore, a piece-wise linear motion model is proposed to take into account motion non-linearities and thus leads to an accurate model for the physical formation of motion blurs in the real-world scenario. Extensive evaluation on both synthetic and real motion blur datasets demonstrates that the proposed algorithm bridges the gap between simulated and real-world motion blurs and shows remarkable performance for event-based motion deblurring in real-world scenarios.",0
"Our paper presents a self-supervised approach to event-based motion deblurring using an end-to-end learning framework. Through the exploitation of real-world events, we address the issue of performance degradation caused by data inconsistency. To achieve this, we predict optical flows from events and use blurry consistency and photometric consistency to enable self-supervision on the deblurring network with real-world data. Additionally, we propose a piece-wise linear motion model that accounts for motion non-linearities and accurately models the physical formation of motion blurs in real-world scenarios. Our proposed algorithm is extensively evaluated on both synthetic and real motion blur datasets, showing remarkable performance and bridging the gap between simulated and real-world motion blurs for event-based motion deblurring in real-world scenarios.",1
"Performing a real-time and accurate instrument segmentation from videos is of great significance for improving the performance of robotic-assisted surgery. We identify two important clues for surgical instrument perception, including local temporal dependency from adjacent frames and global semantic correlation in long-range duration. However, most existing works perform segmentation purely using visual cues in a single frame. Optical flow is just used to model the motion between only two frames and brings heavy computational cost. We propose a novel dual-memory network (DMNet) to wisely relate both global and local spatio-temporal knowledge to augment the current features, boosting the segmentation performance and retaining the real-time prediction capability. We propose, on the one hand, an efficient local memory by taking the complementary advantages of convolutional LSTM and non-local mechanisms towards the relating reception field. On the other hand, we develop an active global memory to gather the global semantic correlation in long temporal range to current one, in which we gather the most informative frames derived from model uncertainty and frame similarity. We have extensively validated our method on two public benchmark surgical video datasets. Experimental results demonstrate that our method largely outperforms the state-of-the-art works on segmentation accuracy while maintaining a real-time speed.",0
"Improving the performance of robotic-assisted surgery requires accurately segmenting instruments in real-time from videos. To achieve this, we have identified two crucial factors for surgical instrument perception - local temporal dependency from adjacent frames and global semantic correlation in long-range duration. However, most existing methods use visual cues from a single frame for segmentation, which is insufficient. Optical flow is also not ideal as it models motion between only two frames and requires heavy computational power. To address these limitations, we propose a new approach called Dual-Memory Network (DMNet) that combines both global and local spatio-temporal knowledge for real-time prediction with enhanced segmentation accuracy. Our approach employs an efficient local memory that combines convolutional LSTM and non-local mechanisms to improve reception field and an active global memory that gathers the most informative frames from model uncertainty and frame similarity to augment the current features. Our method has been extensively validated on two public benchmark surgical video datasets, demonstrating superior segmentation accuracy and real-time speed compared to state-of-the-art techniques.",1
"Online action detection (OAD) is a task that receives video segments within a streaming video as inputs and identifies ongoing actions within them. It is important to retain past information associated with a current action. However, long short-term memory (LSTM), a popular recurrent unit for modeling temporal information from videos, accumulates past information from the previous hidden and cell states and the extracted visual features at each timestep without considering the relationships between the past and current information. Consequently, the forget gate of the original LSTM can lose the accumulated information relevant to the current action because it determines which information to forget without considering the current action. We introduce a novel information elevation unit (IEU) that lifts up and accumulate the past information relevant to the current action in order to model the past information that is especially relevant to the current action. To the best of our knowledge, our IEN is the first attempt that considers the computational overhead for the practical use of OAD. Through ablation studies, we design an efficient and effective OAD network using IEUs, called an information elevation network (IEN). Our IEN uses visual features extracted by a fast action recognition network taking only RGB frames because extracting optical flows requires heavy computation overhead. On two OAD benchmark datasets, THUMOS-14 and TVSeries, our IEN outperforms state-of-the-art OAD methods using only RGB frames. Furthermore, on the THUMOS-14 dataset, our IEN outperforms the state-of-the-art OAD methods using two-stream features based on RGB frames and optical flows.",0
"The task of online action detection (OAD) involves identifying ongoing actions within video segments of a streaming video. To effectively accomplish this task, it is important to retain past information associated with a current action. However, the commonly used long short-term memory (LSTM) recurrent unit does not consider the relationships between past and current information, resulting in the forget gate losing accumulated information relevant to the current action. To address this issue, we propose a novel information elevation unit (IEU) that accumulates past information relevant to the current action to model the information most pertinent to the task. Our IEU is the first attempt to consider computational overhead for practical use of OAD. Using visual features extracted by a fast action recognition network from only RGB frames, we design an efficient and effective OAD network called the information elevation network (IEN) using IEUs. Our IEN outperforms state-of-the-art OAD methods on two benchmark datasets, THUMOS-14 and TVSeries, using only RGB frames. Additionally, on the THUMOS-14 dataset, our IEN outperforms state-of-the-art methods using two-stream features based on both RGB frames and optical flows.",1
"Video anomaly detection has gained significant attention due to the increasing requirements of automatic monitoring for surveillance videos. Especially, the prediction based approach is one of the most studied methods to detect anomalies by predicting frames that include abnormal events in the test set after learning with the normal frames of the training set. However, a lot of prediction networks are computationally expensive owing to the use of pre-trained optical flow networks, or fail to detect abnormal situations because of their strong generative ability to predict even the anomalies. To address these shortcomings, we propose spatial rotation transformation (SRT) and temporal mixing transformation (TMT) to generate irregular patch cuboids within normal frame cuboids in order to enhance the learning of normal features. Additionally, the proposed patch transformation is used only during the training phase, allowing our model to detect abnormal frames at fast speed during inference. Our model is evaluated on three anomaly detection benchmarks, achieving competitive accuracy and surpassing all the previous works in terms of speed.",0
"The need for automatic monitoring of surveillance videos has led to a surge in interest in video anomaly detection. Among the various methods studied, the prediction-based approach is one of the most popular ways to detect anomalies. This approach involves predicting frames with abnormal events in the test set after training with normal frames from the training set. However, many prediction networks are expensive in terms of computation due to the use of pre-trained optical flow networks. Additionally, some networks fail to detect abnormal situations because of their strong generative ability to predict even the anomalies. To address these issues, we propose the use of spatial rotation transformation (SRT) and temporal mixing transformation (TMT) to generate irregular patch cuboids within normal frame cuboids to improve the learning of normal features. Our approach only uses patch transformation during the training phase, enabling our model to detect abnormal frames quickly during inference. We evaluated our model on three anomaly detection benchmarks and achieved competitive accuracy, surpassing all previous works in terms of speed.",1
"Dense disparities among multiple views is essential for estimating the 3D architecture of a scene based on the geometrical relationship among the scene and the views or cameras. Scenes with larger extents of heterogeneous textures, differing scene illumination among the multiple views and with occluding objects affect the accuracy of the estimated disparities. Markov random fields (MRF) based methods for disparity estimation address these limitations using spatial dependencies among the observations and among the disparity estimates. These methods, however, are limited by spatially fixed and smaller neighborhood systems or cliques. In this work, we present a new factor graph-based probabilistic graphical model for disparity estimation that allows a larger and a spatially variable neighborhood structure determined based on the local scene characteristics. We evaluated our method using the Middlebury benchmark stereo datasets and the Middlebury evaluation dataset version 3.0 and compared its performance with recent state-of-the-art disparity estimation algorithms. The new factor graph-based method provided disparity estimates with higher accuracy when compared to the recent non-learning- and learning-based disparity estimation algorithms. In addition to disparity estimation, our factor graph formulation can be useful for obtaining maximum a posteriori solution to optimization problems with complex and variable dependency structures as well as for other dense estimation problems such as optical flow estimation.",0
"To estimate the 3D architecture of a scene based on its relationship with multiple views or cameras, it is crucial to have dense disparities among those views. However, scenes with heterogeneous textures, varying illumination, and occluding objects can affect the accuracy of estimated disparities. Markov random fields (MRF) are commonly used to address these limitations by considering spatial dependencies among observations and disparity estimates. However, MRF-based methods are limited by smaller, fixed neighborhood systems or cliques. In this study, we introduce a novel factor graph-based probabilistic graphical model for disparity estimation that allows for a larger and spatially variable neighborhood structure determined by local scene characteristics. Our method was evaluated using the Middlebury benchmark stereo datasets and the Middlebury evaluation dataset version 3.0, and was found to provide more accurate disparity estimates than recent non-learning- and learning-based disparity estimation algorithms. Additionally, our factor graph formulation has the potential to be useful for solving optimization problems with complex and variable dependency structures, as well as for other dense estimation problems like optical flow estimation.",1
"Facial expression spotting is the preliminary step for micro- and macro-expression analysis. The task of reliably spotting such expressions in video sequences is currently unsolved. The current best systems depend upon optical flow methods to extract regional motion features, before categorisation of that motion into a specific class of facial movement. Optical flow is susceptible to drift error, which introduces a serious problem for motions with long-term dependencies, such as high frame-rate macro-expression. We propose a purely deep learning solution which, rather than tracking frame differential motion, compares via a convolutional model, each frame with two temporally local reference frames. Reference frames are sampled according to calculated micro- and macro-expression durations. We show that our solution achieves state-of-the-art performance (F1-score of 0.105) in a dataset of high frame-rate (200 fps) long video sequences (SAMM-LV) and is competitive in a low frame-rate (30 fps) dataset (CAS(ME)2). In this paper, we document our deep learning model and parameters, including how we use local contrast normalisation, which we show is critical for optimal results. We surpass a limitation in existing methods, and advance the state of deep learning in the domain of facial expression spotting.",0
"The initial step for analyzing micro- and macro-expressions is to spot facial expressions, but reliably identifying these expressions in video sequences is currently a problem that has not been solved. The best systems currently rely on optical flow methods to extract regional motion features, which are then categorized into specific classes of facial movement. However, optical flow is susceptible to drift error, making it problematic for analyzing long-term dependencies. To address this issue, we propose a purely deep learning solution that utilizes a convolutional model to compare each frame with two temporally local reference frames, which are sampled based on calculated micro- and macro-expression durations. Our approach achieves state-of-the-art performance on a high frame-rate dataset (SAMM-LV) and competitive performance on a low frame-rate dataset (CAS(ME)2). The paper details our deep learning model and parameters, including the use of local contrast normalization, which we demonstrate is critical for optimal results. Our solution surpasses existing methods and advances the state of deep learning in facial expression spotting.",1
"The temporal and spatial resolution of rainfall data is crucial for climate change modeling studies in which its variability in space and time is considered as a primary factor. Rainfall products from different remote sensing instruments (e.g., radar or satellite) provide different space-time resolutions because of the differences in their sensing capabilities. We developed an approach that augments rainfall data with increased time resolutions to complement relatively lower resolution products. This study proposes a neural network architecture based on Convolutional Neural Networks (CNNs) to improve temporal resolution of radar-based rainfall products and compares the proposed model with an optical flow-based interpolation method.",0
"The accuracy of rainfall data's temporal and spatial resolution is vital in climate change modeling studies, where its variability in space and time is a significant factor. Rainfall data from remote sensing instruments, such as radar or satellite, offers various space-time resolutions due to differing sensing capabilities. To improve rainfall data's temporal resolution, we devised a method to supplement lower resolution products with higher time resolutions. This study introduces a neural network architecture that employs Convolutional Neural Networks (CNNs) to enhance the temporal resolution of radar-based rainfall data and compares it with an optical flow-based interpolation method.",1
"Lines provide the significantly richer geometric structural information about the environment than points, so lines are widely used in recent Visual Odometry (VO) works. Since VO with lines use line tracking results to locate and map, line tracking is a crucial component in VO. Although the state-of-the-art line tracking methods have made great progress, they are still heavily dependent on line detection or the predicted line segments. In order to relieve the dependencies described above to track line segments completely, accurately, and robustly at higher computational efficiency, we propose a structure-aware Line tracking algorithm based entirely on Optical Flow (LOF). Firstly, we propose a gradient-based strategy to sample pixels on lines that are suitable for line optical flow calculation. Then, in order to align the lines by fully using the structural relationship between the sampled points on it and effectively removing the influence of sampled points on it occluded by other objects, we propose a two-step structure-aware line segment alignment method. Furthermore, we propose a line refinement method to refine the orientation, position, and endpoints of the aligned line segments. Extensive experimental results demonstrate that the proposed LOF outperforms the state-of-the-art performance in line tracking accuracy, robustness, and efficiency, which also improves the location accuracy and robustness of VO system with lines.",0
"Recent Visual Odometry (VO) works heavily rely on lines as they offer more geometric structural information about the environment than points. Line tracking is therefore a crucial component in VO. However, current line tracking methods still heavily depend on line detection or predicted line segments. To overcome this, we propose a structure-aware Line tracking algorithm based solely on Optical Flow (LOF). Our algorithm uses a gradient-based strategy to sample pixels on lines suitable for line optical flow calculation. We also propose a two-step structure-aware line segment alignment method to align lines and remove occlusion. Additionally, we introduce a line refinement method to refine line orientation, position, and endpoints. Our experimental results show that LOF outperforms state-of-the-art methods in accuracy, robustness, and efficiency, improving the location accuracy and robustness of VO systems using lines.",1
"Although deep neural networks (DNNs) enable great progress in video abnormal event detection (VAD), existing solutions typically suffer from two issues: (1) The localization of video events cannot be both precious and comprehensive. (2) The semantics and temporal context are under-explored. To tackle those issues, we are motivated by the prevalent cloze test in education and propose a novel approach named Visual Cloze Completion (VCC), which conducts VAD by learning to complete ""visual cloze tests"" (VCTs). Specifically, VCC first localizes each video event and encloses it into a spatio-temporal cube (STC). To achieve both precise and comprehensive localization, appearance and motion are used as complementary cues to mark the object region associated with each event. For each marked region, a normalized patch sequence is extracted from current and adjacent frames and stacked into a STC. With each patch and the patch sequence of a STC compared to a visual ""word"" and ""sentence"" respectively, we deliberately erase a certain ""word"" (patch) to yield a VCT. Then, the VCT is completed by training DNNs to infer the erased patch and its optical flow via video semantics. Meanwhile, VCC fully exploits temporal context by alternatively erasing each patch in temporal context and creating multiple VCTs. Furthermore, we propose localization-level, event-level, model-level and decision-level solutions to enhance VCC, which can further exploit VCC's potential and produce significant performance improvement gain. Extensive experiments demonstrate that VCC achieves state-of-the-art VAD performance. Our codes and results are open at https://github.com/yuguangnudt/VEC_VAD/tree/VCC.",0
"Video abnormal event detection (VAD) using deep neural networks (DNNs) has made great progress, but current solutions have two main issues: (1) The localization of video events is not precise and comprehensive. (2) There is limited exploration of semantics and temporal context. In order to address these issues, we propose a novel approach called Visual Cloze Completion (VCC) inspired by the cloze test in education. VCC localizes each video event and encloses it into a spatio-temporal cube (STC) using appearance and motion as complementary cues. We extract a normalized patch sequence from current and adjacent frames for each marked region and stack them into an STC. We then erase a certain ""word"" (patch) and infer it through video semantics using DNNs to complete a ""visual cloze test"" (VCT). To fully exploit temporal context, we create multiple VCTs by alternatively erasing each patch. We also propose localization-level, event-level, model-level, and decision-level solutions to enhance VCC. Our experiments show that VCC achieves state-of-the-art VAD performance. Our codes and results are open at https://github.com/yuguangnudt/VEC_VAD/tree/VCC.",1
"We introduce RAFT-Stereo, a new deep architecture for rectified stereo based on the optical flow network RAFT. We introduce multi-level convolutional GRUs, which more efficiently propagate information across the image. A modified version of RAFT-Stereo can perform accurate real-time inference. RAFT-stereo ranks first on the Middlebury leaderboard, outperforming the next best method on 1px error by 29% and outperforms all published work on the ETH3D two-view stereo benchmark. Code is available at https://github.com/princeton-vl/RAFT-Stereo.",0
"Introducing RAFT-Stereo, a novel deep architecture for rectified stereo that is based on the optical flow network RAFT. Our approach features multi-level convolutional GRUs that enable more efficient propagation of information across the image. We have also developed a modified version of RAFT-Stereo that is capable of performing accurate real-time inference. Notably, RAFT-Stereo has achieved the top ranking on the Middlebury leaderboard, surpassing the next best method on 1px error by 29% and outperforming all published work on the ETH3D two-view stereo benchmark. Interested parties can access the code at https://github.com/princeton-vl/RAFT-Stereo.",1
"Automatic portrait video matting is an under-constrained problem. Most state-of-the-art methods only exploit the semantic information and process each frame individually. Their performance is compromised due to the lack of temporal information between the frames. To solve this problem, we propose the context motion network to leverage semantic information and motion information. To capture the motion information, we estimate the optical flow and design a context-motion updating operator to integrate features between frames recurrently. Our experiments show that our network outperforms state-of-the-art matting methods significantly on the Video240K SD dataset.",0
"The issue of automatic portrait video matting is not fully resolved, as it is an under-constrained problem. Although many of the latest techniques depend solely on semantic information and tackle one frame at a time, their effectiveness is hindered due to the absence of temporal information across the frames. We suggest a solution to this problem by introducing the context motion network, which utilizes both semantic and motion information. We have devised a context-motion updating operator to combine features recurrently between frames, and to capture motion information, we have estimated the optical flow. Our findings demonstrate that our network surpasses the most advanced matting methods on the Video240K SD dataset.",1
"Video semantic segmentation requires to utilize the complex temporal relations between frames of the video sequence. Previous works usually exploit accurate optical flow to leverage the temporal relations, which suffer much from heavy computational cost. In this paper, we propose a Temporal Memory Attention Network (TMANet) to adaptively integrate the long-range temporal relations over the video sequence based on the self-attention mechanism without exhaustive optical flow prediction. Specially, we construct a memory using several past frames to store the temporal information of the current frame. We then propose a temporal memory attention module to capture the relation between the current frame and the memory to enhance the representation of the current frame. Our method achieves new state-of-the-art performances on two challenging video semantic segmentation datasets, particularly 80.3% mIoU on Cityscapes and 76.5% mIoU on CamVid with ResNet-50.",0
"To perform video semantic segmentation, it is necessary to consider the intricate temporal relationships between the frames in the video sequence. Previous research has often utilized accurate optical flow to exploit these temporal relations, but this method is computationally expensive. In this study, we present the Temporal Memory Attention Network (TMANet), which utilizes a self-attention mechanism to adaptively integrate long-range temporal relations without the need for exhaustive optical flow prediction. Our approach involves constructing a memory that stores temporal information from several past frames to aid in the segmentation of the current frame. We then use a temporal memory attention module to capture the relationship between the current frame and the memory and enhance the current frame's representation. Our method outperforms previous state-of-the-art techniques on two challenging video semantic segmentation datasets, achieving a mIoU of 80.3% on Cityscapes and 76.5% on CamVid with ResNet-50.",1
"One of the most common problems encountered in human-computer interaction is automatic facial expression recognition. Although it is easy for human observer to recognize facial expressions, automatic recognition remains difficult for machines. One of the methods that machines can recognize facial expression is analyzing the changes in face during facial expression presentation. In this paper, optical flow algorithm was used to extract deformation or motion vectors created in the face because of facial expressions. Then, these extracted motion vectors are used to be analyzed. Their positions and directions were exploited for automatic facial expression recognition using different data mining techniques. It means that by employing motion vector features used as our data, facial expressions were recognized. Some of the most state-of-the-art classification algorithms such as C5.0, CRT, QUEST, CHAID, Deep Learning (DL), SVM and Discriminant algorithms were used to classify the extracted motion vectors. Using 10-fold cross validation, their performances were calculated. To compare their performance more precisely, the test was repeated 50 times. Meanwhile, the deformation of face was also analyzed in this research. For example, what exactly happened in each part of face when a person showed fear? Experimental results on Extended Cohen-Kanade (CK+) facial expression dataset demonstrated that the best methods were DL, SVM and C5.0, with the accuracy of 95.3%, 92.8% and 90.2% respectively.",0
"Facial expression recognition is a common challenge in human-computer interaction. While humans can easily identify facial expressions, machines struggle with this task. One approach for machines is to analyze changes in the face during expression presentation. This study used an optical flow algorithm to extract motion vectors from facial deformations caused by expressions. These vectors were analyzed and used for automatic facial expression recognition through various data mining techniques. State-of-the-art algorithms, such as C5.0, CRT, QUEST, CHAID, DL, SVM, and Discriminant, were employed for classification. To evaluate their performance, 10-fold cross-validation was used, and the test was repeated 50 times. Additionally, this research analyzed facial deformation during expressions, such as how fear manifests in different parts of the face. Results from the Extended Cohen-Kanade dataset showed that DL, SVM, and C5.0 were the most accurate methods, achieving 95.3%, 92.8%, and 90.2% accuracy, respectively.",1
"Event camera has offered promising alternative for visual perception, especially in high speed and high dynamic range scenes. Recently, many deep learning methods have shown great success in providing model-free solutions to many event-based problems, such as optical flow estimation. However, existing deep learning methods did not address the importance of temporal information well from the perspective of architecture design and cannot effectively extract spatio-temporal features. Another line of research that utilizes Spiking Neural Network suffers from training issues for deeper architecture. To address these points, a novel input representation is proposed that captures the events temporal distribution for signal enhancement. Moreover, we introduce a spatio-temporal recurrent encoding-decoding neural network architecture for event-based optical flow estimation, which utilizes Convolutional Gated Recurrent Units to extract feature maps from a series of event images. Besides, our architecture allows some traditional frame-based core modules, such as correlation layer and iterative residual refine scheme, to be incorporated. The network is end-to-end trained with self-supervised learning on the Multi-Vehicle Stereo Event Camera dataset. We have shown that it outperforms all the existing state-of-the-art methods by a large margin.",0
"The event camera presents a promising alternative to visual perception, especially for high-speed and high-dynamic-range scenes. Although deep learning methods have shown success in offering model-free solutions to event-based problems like optical flow estimation, they fail to address the importance of temporal information and struggle to extract spatio-temporal features. Spiking Neural Networks also pose training issues for deeper architectures. To tackle these challenges, a novel input representation that captures the temporal distribution of events is proposed for signal enhancement. Additionally, a spatio-temporal recurrent encoding-decoding neural network architecture is introduced for event-based optical flow estimation, utilizing Convolutional Gated Recurrent Units to extract feature maps from event images. Traditional frame-based core modules like correlation layer and iterative residual refine scheme are incorporated into the architecture. The network is self-supervisedly trained end-to-end on the Multi-Vehicle Stereo Event Camera dataset, surpassing all existing state-of-the-art methods by a significant margin.",1
"We propose a novel neural-network-based method to perform matting of videos depicting people that does not require additional user input such as trimaps. Our architecture achieves temporal stability of the resulting alpha mattes by using motion-estimation-based smoothing of image-segmentation algorithm outputs, combined with convolutional-LSTM modules on U-Net skip connections.   We also propose a fake-motion algorithm that generates training clips for the video-matting network given photos with ground-truth alpha mattes and background videos. We apply random motion to photos and their mattes to simulate movement one would find in real videos and composite the result with the background clips. It lets us train a deep neural network operating on videos in an absence of a large annotated video dataset and provides ground-truth training-clip foreground optical flow for use in loss functions.",0
"Our innovative approach utilizes a neural network to conduct video matting of individuals without requiring additional user input such as trimaps. Our design guarantees stable alpha matte outcomes through motion-based smoothing of image segmentation algorithm outputs and convolutional-LSTM modules on U-Net skip connections. In addition, we propose a false motion algorithm that creates training clips for the video-matting network using photos that possess ground-truth alpha mattes and background videos. By applying random motion to both the photos and their mattes, we simulate the movement that exists in actual videos and combine it with the background clips. This approach enables us to train a deep neural network to operate on videos even in the absence of a large annotated video dataset, providing ground-truth training-clip foreground optical flow for use in loss functions.",1
"Self-supervised Multi-view stereo (MVS) with a pretext task of image reconstruction has achieved significant progress recently. However, previous methods are built upon intuitions, lacking comprehensive explanations about the effectiveness of the pretext task in self-supervised MVS. To this end, we propose to estimate epistemic uncertainty in self-supervised MVS, accounting for what the model ignores. Specially, the limitations can be categorized into two types: ambiguious supervision in foreground and invalid supervision in background. To address these issues, we propose a novel Uncertainty reduction Multi-view Stereo (UMVS) framework for self-supervised learning. To alleviate ambiguous supervision in foreground, we involve extra correspondence prior with a flow-depth consistency loss. The dense 2D correspondence of optical flows is used to regularize the 3D stereo correspondence in MVS. To handle the invalid supervision in background, we use Monte-Carlo Dropout to acquire the uncertainty map and further filter the unreliable supervision signals on invalid regions. Extensive experiments on DTU and Tank&Temples benchmark show that our U-MVS framework achieves the best performance among unsupervised MVS methods, with competitive performance with its supervised opponents.",0
"Recently, there has been significant progress in self-supervised Multi-view stereo (MVS) using an image reconstruction pretext task. However, previous methods lack comprehensive explanations for the effectiveness of this approach, as they are based on intuition. To address this issue, we propose an Uncertainty reduction Multi-view Stereo (UMVS) framework for self-supervised learning that estimates epistemic uncertainty in MVS to account for what the model ignores. We identify two limiting factors: ambiguous supervision in the foreground and invalid supervision in the background. To overcome these limitations, we integrate an extra correspondence prior with a flow-depth consistency loss to alleviate ambiguous supervision in the foreground and use Monte-Carlo Dropout to obtain the uncertainty map and filter unreliable supervision signals on invalid regions to handle invalid supervision in the background. Our U-MVS framework achieves the best performance among unsupervised MVS methods and has competitive performance with its supervised counterparts, as demonstrated through extensive experiments on the DTU and Tank&Temples benchmarks.",1
"There hardly exists any large-scale datasets with dense optical flow of non-rigid motion from real-world imagery as of today. The reason lies mainly in the required setup to derive ground truth optical flows: a series of images with known camera poses along its trajectory, and an accurate 3D model from a textured scene. Human annotation is not only too tedious for large databases, it can simply hardly contribute to accurate optical flow. To circumvent the need for manual annotation, we propose a framework to automatically generate optical flow from real-world videos. The method extracts and matches objects from video frames to compute initial constraints, and applies a deformation over the objects of interest to obtain dense optical flow fields. We propose several ways to augment the optical flow variations. Extensive experimental results show that training on our automatically generated optical flow outperforms methods that are trained on rigid synthetic data using FlowNet-S, LiteFlowNet, PWC-Net, and RAFT. Datasets and implementation of our optical flow generation framework are released at https://github.com/lhoangan/arap_flow",0
"At present, there is a scarcity of comprehensive datasets containing dense optical flow for non-rigid motion captured in real-world imagery. The primary factor behind this shortage is the difficulty in obtaining ground truth optical flows, which necessitates a sequence of images with known camera poses and an accurate 3D model of the textured scene. Manual annotation is impractical for large databases and is not very effective in achieving precise optical flow. To address this issue, we propose a framework for automatically generating optical flow from real-world videos. The method involves object extraction and matching in video frames to compute initial constraints, followed by the application of deformation to objects of interest to obtain dense optical flow fields. We offer various ways to enhance optical flow variations. Our extensive experimental results demonstrate that our automatically generated optical flow outperforms methods trained on rigid synthetic data, including FlowNet-S, LiteFlowNet, PWC-Net, and RAFT. We have made available the datasets and implementation of our optical flow generation framework at https://github.com/lhoangan/arap_flow.",1
"As moving objects always draw more attention of human eyes, the temporal motive information is always exploited complementarily with spatial information to detect salient objects in videos. Although efficient tools such as optical flow have been proposed to extract temporal motive information, it often encounters difficulties when used for saliency detection due to the movement of camera or the partial movement of salient objects. In this paper, we investigate the complimentary roles of spatial and temporal information and propose a novel dynamic spatiotemporal network (DS-Net) for more effective fusion of spatiotemporal information. We construct a symmetric two-bypass network to explicitly extract spatial and temporal features. A dynamic weight generator (DWG) is designed to automatically learn the reliability of corresponding saliency branch. And a top-down cross attentive aggregation (CAA) procedure is designed so as to facilitate dynamic complementary aggregation of spatiotemporal features. Finally, the features are modified by spatial attention with the guidance of coarse saliency map and then go through decoder part for final saliency map. Experimental results on five benchmarks VOS, DAVIS, FBMS, SegTrack-v2, and ViSal demonstrate that the proposed method achieves superior performance than state-of-the-art algorithms. The source code is available at https://github.com/TJUMMG/DS-Net.",0
"Temporal motive information is used in conjunction with spatial information to detect salient objects in videos as moving objects tend to attract more attention. Optical flow is an efficient tool for extracting temporal motive information, but its use in saliency detection is challenged by camera movement or partial movement of salient objects. To address this, we introduce a new approach that combines spatial and temporal information more effectively through a dynamic spatiotemporal network (DS-Net). Our approach includes a symmetric two-bypass network that explicitly extracts spatial and temporal features, a dynamic weight generator (DWG) that learns the reliability of the saliency branch, and a top-down cross attentive aggregation (CAA) procedure for dynamic complementary aggregation of spatiotemporal features. Spatial attention, guided by a coarse saliency map, is then used to modify the features, which are finally processed by a decoder for the saliency map. Results on five benchmarks show that our method outperforms state-of-the-art algorithms. The source code is available at https://github.com/TJUMMG/DS-Net.",1
"In this work, we tackle the essential problem of scale inconsistency for self-supervised joint depth-pose learning. Most existing methods assume that a consistent scale of depth and pose can be learned across all input samples, which makes the learning problem harder, resulting in degraded performance and limited generalization in indoor environments and long-sequence visual odometry application. To address this issue, we propose a novel system that explicitly disentangles scale from the network estimation. Instead of relying on PoseNet architecture, our method recovers relative pose by directly solving fundamental matrix from dense optical flow correspondence and makes use of a two-view triangulation module to recover an up-to-scale 3D structure. Then, we align the scale of the depth prediction with the triangulated point cloud and use the transformed depth map for depth error computation and dense reprojection check. Our whole system can be jointly trained end-to-end. Extensive experiments show that our system not only reaches state-of-the-art performance on KITTI depth and flow estimation, but also significantly improves the generalization ability of existing self-supervised depth-pose learning methods under a variety of challenging scenarios, and achieves state-of-the-art results among self-supervised learning-based methods on KITTI Odometry and NYUv2 dataset. Furthermore, we present some interesting findings on the limitation of PoseNet-based relative pose estimation methods in terms of generalization ability. Code is available at https://github.com/B1ueber2y/TrianFlow.",0
"The focus of our work is on addressing scale inconsistency in self-supervised joint depth-pose learning. The current methods assume that a uniform scale of depth and pose can be learned, which results in reduced performance and limited generalization for the long-sequence visual odometry application and indoor environments. We propose a novel system that separates scale from network estimation. Instead of using PoseNet architecture, we recover relative pose by solving fundamental matrix from dense optical flow correspondence and use a two-view triangulation module to determine an up-to-scale 3D structure. We align the scale of the depth prediction with the triangulated point cloud and use the transformed depth map for depth error computation and dense reprojection check. Our entire system can be trained end-to-end. Our experiments indicate that our system not only achieves state-of-the-art performance on KITTI depth and flow estimation but also enhances the generalization ability of existing self-supervised depth-pose learning methods in challenging scenarios. Our method also outperforms other self-supervised learning-based methods on KITTI Odometry and NYUv2 datasets. We also discovered some interesting limitations of PoseNet-based relative pose estimation methods in terms of generalization ability. The code is available at https://github.com/B1ueber2y/TrianFlow.",1
"Recent efforts towards video anomaly detection (VAD) try to learn a deep autoencoder to describe normal event patterns with small reconstruction errors. The video inputs with large reconstruction errors are regarded as anomalies at the test time. However, these methods sometimes reconstruct abnormal inputs well because of the powerful generalization ability of deep autoencoder. To address this problem, we present a novel approach for anomaly detection, which utilizes discriminative prototypes of normal data to reconstruct video frames. In this way, the model will favor the reconstruction of normal events and distort the reconstruction of abnormal events. Specifically, we use a prototype-guided memory module to perform discriminative latent embedding. We introduce a new discriminative criterion for the memory module, as well as a loss function correspondingly, which can encourage memory items to record the representative embeddings of normal data, i.e. prototypes. Besides, we design a novel two-branch autoencoder, which is composed of a future frame prediction network and an RGB difference generation network that share the same encoder. The stacked RGB difference contains motion information just like optical flow, so our model can learn temporal regularity. We evaluate the effectiveness of our method on three benchmark datasets and experimental results demonstrate the proposed method outperforms the state-of-the-art.",0
"The current trend in video anomaly detection (VAD) is to utilize deep autoencoders to learn normal event patterns that have small reconstruction errors. During testing, inputs with large reconstruction errors are considered anomalies. However, the generalization ability of deep autoencoders sometimes allows abnormal inputs to be reconstructed properly, creating a problem. To address this issue, we propose a new approach that employs discriminative prototypes of normal data to reconstruct video frames. This way, the model prioritizes the reconstruction of normal events and distorts the reconstruction of abnormal ones. We use a prototype-guided memory module for discriminative latent embedding, introducing a new discriminative criterion and loss function to encourage memory items to record the representative embeddings of normal data. We also design a novel two-branch autoencoder consisting of a future frame prediction network and an RGB difference generation network sharing the same encoder. The stacked RGB difference contains motion information, allowing our model to learn temporal regularity. We evaluate our method on three benchmark datasets and demonstrate its superiority over the state-of-the-art.",1
"Action anticipation in egocentric videos is a difficult task due to the inherently multi-modal nature of human actions. Additionally, some actions happen faster or slower than others depending on the actor or surrounding context which could vary each time and lead to different predictions. Based on this idea, we build upon RULSTM architecture, which is specifically designed for anticipating human actions, and propose a novel attention-based technique to evaluate, simultaneously, slow and fast features extracted from three different modalities, namely RGB, optical flow, and extracted objects. Two branches process information at different time scales, i.e., frame-rates, and several fusion schemes are considered to improve prediction accuracy. We perform extensive experiments on EpicKitchens-55 and EGTEA Gaze+ datasets, and demonstrate that our technique systematically improves the results of RULSTM architecture for Top-5 accuracy metric at different anticipation times.",0
"Anticipating actions in egocentric videos is a challenging task due to the complex nature of human actions, which involve multiple modalities. Moreover, the speed of actions may vary depending on the actor and context, leading to diverse predictions. To address this issue, we enhance the RULSTM architecture, which is designed for action anticipation, by introducing an innovative attention-based technique that evaluates fast and slow features extracted from three modalities: RGB, optical flow, and object extraction. The technique employs two branches that process information at different time scales and considers several fusion schemes to enhance prediction accuracy. We conduct extensive experiments on the EpicKitchens-55 and EGTEA Gaze+ datasets and demonstrate that our approach significantly improves the Top-5 accuracy metric of the RULSTM architecture across various anticipation times.",1
"We propose FlowReg, a deep learning-based framework for unsupervised image registration for neuroimaging applications. The system is composed of two architectures that are trained sequentially: FlowReg-A which affinely corrects for gross differences between moving and fixed volumes in 3D followed by FlowReg-O which performs pixel-wise deformations on a slice-by-slice basis for fine tuning in 2D. The affine network regresses the 3D affine matrix based on a correlation loss function that enforces global similarity. The deformable network operates on 2D image slices based on the optical flow network FlowNet-Simple but with three loss components. The photometric loss minimizes pixel intensity differences differences, the smoothness loss encourages similar magnitudes between neighbouring vectors, and a correlation loss that is used to maintain the intensity similarity between fixed and moving image slices. The proposed method is compared to four open source registration techniques ANTs, Demons, SE, and Voxelmorph. In total, 4643 FLAIR MR imaging volumes are used from dementia and vascular disease cohorts, acquired from over 60 international centres with varying acquisition parameters. A battery of quantitative novel registration validation metrics are proposed that focus on the structural integrity of tissues, spatial alignment, and intensity similarity. Experimental results show FlowReg (FlowReg-A+O) performs better than iterative-based registration algorithms for intensity and spatial alignment metrics with a Pixelwise Agreement of 0.65, correlation coefficient of 0.80, and Mutual Information of 0.29. Among the deep learning frameworks, FlowReg-A or FlowReg-A+O provided the highest performance over all but one of the metrics. Results show that FlowReg is able to obtain high intensity and spatial similarity while maintaining the shape and structure of anatomy and pathology.",0
"Our proposition is FlowReg, a framework for unsupervised image registration for neuroimaging applications, which is based on deep learning. FlowReg consists of two architectures that are trained sequentially. First, FlowReg-A affinely corrects for gross differences between the moving and fixed volumes in 3D. Then, FlowReg-O performs pixel-wise deformations on a slice-by-slice basis to fine-tune the alignment in 2D. The affine network regresses the 3D affine matrix, using a correlation loss function that enforces global similarity. The deformable network operates on 2D image slices, using three loss components: photometric loss, smoothness loss, and correlation loss. We compare our proposed method to four open source registration techniques (ANTs, Demons, SE, and Voxelmorph), using 4643 FLAIR MR imaging volumes from dementia and vascular disease cohorts, acquired from over 60 international centres with varying acquisition parameters. We propose a battery of quantitative novel registration validation metrics that focus on the structural integrity of tissues, spatial alignment, and intensity similarity. Experimental results show that FlowReg performs better than iterative-based registration algorithms for intensity and spatial alignment metrics, with a Pixelwise Agreement of 0.65, correlation coefficient of 0.80, and Mutual Information of 0.29. Among the deep learning frameworks, FlowReg-A or FlowReg-A+O provided the highest performance over all but one of the metrics. Results show that FlowReg is able to obtain high intensity and spatial similarity while maintaining the shape and structure of anatomy and pathology.",1
"Recent years have seen considerable research activities devoted to video enhancement that simultaneously increases temporal frame rate and spatial resolution. However, the existing methods either fail to explore the intrinsic relationship between temporal and spatial information or lack flexibility in the choice of final temporal/spatial resolution. In this work, we propose an unconstrained space-time video super-resolution network, which can effectively exploit space-time correlation to boost performance. Moreover, it has complete freedom in adjusting the temporal frame rate and spatial resolution through the use of the optical flow technique and a generalized pixelshuffle operation. Our extensive experiments demonstrate that the proposed method not only outperforms the state-of-the-art, but also requires far fewer parameters and less running time.",0
"In recent years, research has focused on enhancing video quality by increasing both temporal frame rate and spatial resolution. However, current methods either do not consider the connection between temporal and spatial information or limit the options for final resolution. Our solution is an unconstrained video super-resolution network that leverages space-time correlation to improve performance. This network can adjust temporal frame rate and spatial resolution using the optical flow technique and a generalized pixelshuffle operation. Our experiments show that this method surpasses the state-of-the-art with fewer parameters and less running time.",1
"We propose to incorporate feature correlation and sequential processing into dense optical flow estimation from event cameras. Modern frame-based optical flow methods heavily rely on matching costs computed from feature correlation. In contrast, there exists no optical flow method for event cameras that explicitly computes matching costs. Instead, learning-based approaches using events usually resort to the U-Net architecture to estimate optical flow sparsely. Our key finding is that the introduction of correlation features significantly improves results compared to previous methods that solely rely on convolution layers. Compared to the state-of-the-art, our proposed approach computes dense optical flow and reduces the end-point error by 23% on MVSEC. Furthermore, we show that all existing optical flow methods developed so far for event cameras have been evaluated on datasets with very small displacement fields with a maximum flow magnitude of 10 pixels. Based on this observation, we introduce a new real-world dataset that exhibits displacement fields with magnitudes up to 210 pixels and 3 times higher camera resolution. Our proposed approach reduces the end-point error on this dataset by 66%.",0
"Our proposal aims to enhance dense optical flow estimation from event cameras by incorporating feature correlation and sequential processing. While traditional frame-based optical flow methods rely on matching costs from feature correlation, no existing method for event cameras explicitly computes matching costs. Instead, learning-based approaches using events typically use the U-Net architecture to estimate optical flow sparsely. Our research reveals that incorporating correlation features significantly improves results compared to previous methods that solely rely on convolution layers. Our proposed approach computes dense optical flow and reduces end-point error by 23% on MVSEC, outperforming state-of-the-art methods. Additionally, we noticed that all existing optical flow methods for event cameras have only been tested on datasets with small displacement fields. To address this, we introduce a new real-world dataset with magnitudes up to 210 pixels and 3 times higher camera resolution. Our approach reduces the end-point error on this dataset by 66%.",1
"Optical flow is inherently a 2D search problem, and thus the computational complexity grows quadratically with respect to the search window, making large displacements matching infeasible for high-resolution images. In this paper, we take inspiration from Transformers and propose a new method for high-resolution optical flow estimation with significantly less computation. Specifically, a 1D attention operation is first applied in the vertical direction of the target image, and then a simple 1D correlation in the horizontal direction of the attended image is able to achieve 2D correspondence modeling effect. The directions of attention and correlation can also be exchanged, resulting in two 3D cost volumes that are concatenated for optical flow estimation. The novel 1D formulation empowers our method to scale to very high-resolution input images while maintaining competitive performance. Extensive experiments on Sintel, KITTI and real-world 4K ($2160 \times 3840$) resolution images demonstrated the effectiveness and superiority of our proposed method. Code and models are available at \url{https://github.com/haofeixu/flow1d}.",0
"The complexity of optical flow increases quadratically as the search window expands, which limits the feasibility of matching large displacements for high-resolution images. To address this issue, we draw inspiration from Transformers and present a novel approach for high-resolution optical flow estimation that requires less computation. Our method involves applying a 1D attention operation vertically on the target image, followed by a simple 1D correlation horizontally on the attended image to achieve 2D correspondence modeling. The direction of attention and correlation can be interchanged, resulting in two 3D cost volumes that are concatenated for optical flow estimation. This 1D formulation enables our method to scale to very high-resolution input images while maintaining competitive performance. Extensive experiments on Sintel, KITTI, and real-world 4K images (2160 x 3840 resolution) demonstrate the effectiveness and superiority of our proposed method. Code and models are available at https://github.com/haofeixu/flow1d.",1
"In this paper, we propose to learn an Unsupervised Single Object Tracker (USOT) from scratch. We identify that three major challenges, i.e., moving object discovery, rich temporal variation exploitation, and online update, are the central causes of the performance bottleneck of existing unsupervised trackers. To narrow the gap between unsupervised trackers and supervised counterparts, we propose an effective unsupervised learning approach composed of three stages. First, we sample sequentially moving objects with unsupervised optical flow and dynamic programming, instead of random cropping. Second, we train a naive Siamese tracker from scratch using single-frame pairs. Third, we continue training the tracker with a novel cycle memory learning scheme, which is conducted in longer temporal spans and also enables our tracker to update online. Extensive experiments show that the proposed USOT learned from unlabeled videos performs well over the state-of-the-art unsupervised trackers by large margins, and on par with recent supervised deep trackers. Code is available at https://github.com/VISION-SJTU/USOT.",0
"This paper presents a proposal for learning an Unsupervised Single Object Tracker (USOT) from scratch. The authors have identified three major challenges that cause performance issues in existing unsupervised trackers: moving object discovery, rich temporal variation exploitation, and online update. To address these challenges, the authors propose an effective unsupervised learning approach that involves three stages. Firstly, the authors use unsupervised optical flow and dynamic programming to sequentially sample moving objects instead of random cropping. Secondly, they train a naive Siamese tracker from scratch using single-frame pairs. Thirdly, they continue training the tracker with a cycle memory learning scheme that allows for longer temporal spans and online updates. Extensive experiments demonstrate that the proposed USOT outperforms existing unsupervised trackers by significant margins and performs comparably to recent supervised deep trackers. The code for this approach is available at https://github.com/VISION-SJTU/USOT.",1
"This paper presents a novel method for pedestrian detection and tracking by fusing camera and LiDAR sensor data. To deal with the challenges associated with the autonomous driving scenarios, an integrated tracking and detection framework is proposed. The detection phase is performed by converting LiDAR streams to computationally tractable depth images, and then, a deep neural network is developed to identify pedestrian candidates both in RGB and depth images. To provide accurate information, the detection phase is further enhanced by fusing multi-modal sensor information using the Kalman filter. The tracking phase is a combination of the Kalman filter prediction and an optical flow algorithm to track multiple pedestrians in a scene. We evaluate our framework on a real public driving dataset. Experimental results demonstrate that the proposed method achieves significant performance improvement over a baseline method that solely uses image-based pedestrian detection.",0
"A new technique for detecting and tracking pedestrians through the combination of camera and LiDAR sensor data is introduced. This method addresses the difficulties that arise in autonomous driving situations through the use of an integrated tracking and detection framework. LiDAR streams are converted into depth images to facilitate the detection process, which is carried out by a deep neural network that identifies pedestrian candidates in both RGB and depth images. To enhance accuracy, multi-modal sensor data is fused by means of a Kalman filter. The tracking phase employs a combination of Kalman filter prediction and an optical flow algorithm to track multiple pedestrians in a given scene. The effectiveness of the proposed approach is demonstrated through experimentation with a real-world driving dataset, with results showing significant performance gains over a baseline method that relies solely on image-based pedestrian detection.",1
"Learning transferable and domain adaptive feature representations from videos is important for video-relevant tasks such as action recognition. Existing video domain adaptation methods mainly rely on adversarial feature alignment, which has been derived from the RGB image space. However, video data is usually associated with multi-modal information, e.g., RGB and optical flow, and thus it remains a challenge to design a better method that considers the cross-modal inputs under the cross-domain adaptation setting. To this end, we propose a unified framework for video domain adaptation, which simultaneously regularizes cross-modal and cross-domain feature representations. Specifically, we treat each modality in a domain as a view and leverage the contrastive learning technique with properly designed sampling strategies. As a result, our objectives regularize feature spaces, which originally lack the connection across modalities or have less alignment across domains. We conduct experiments on domain adaptive action recognition benchmark datasets, i.e., UCF, HMDB, and EPIC-Kitchens, and demonstrate the effectiveness of our components against state-of-the-art algorithms.",0
"Developing transferable and adaptable feature representations from videos is crucial for tasks that involve videos, such as recognizing actions. Existing methods for video domain adaptation primarily depend on adversarial feature alignment, which is based on the RGB image space. However, video data includes multi-modal information, such as RGB and optical flow, making it challenging to design a superior method that considers cross-modal inputs under the cross-domain adaptation setting. To address this issue, we propose a comprehensive framework for video domain adaptation that simultaneously regulates cross-modal and cross-domain feature representations. We consider each modality in a domain as a view and employ contrastive learning with well-designed sampling strategies. Thus, our objectives regulate feature spaces that lack connections across modalities or have less alignment across domains. We evaluate our components on benchmark datasets for domain adaptive action recognition, including UCF, HMDB, and EPIC-Kitchens, and demonstrate their effectiveness compared to state-of-the-art algorithms.",1
"We propose a framework for aligning and fusing multiple images into a single coordinate-based neural representations. Our framework targets burst images that have misalignment due to camera ego motion and small changes in the scene. We describe different strategies for alignment depending on the assumption of the scene motion, namely, perspective planar (i.e., homography), optical flow with minimal scene change, and optical flow with notable occlusion and disocclusion. Our framework effectively combines the multiple inputs into a single neural implicit function without the need for selecting one of the images as a reference frame. We demonstrate how to use this multi-frame fusion framework for various layer separation tasks.",0
"A framework designed to coordinate and merge numerous images into a single neural representation based on the same coordinates is proposed. The framework is intended to fix misalignments in burst images caused by camera ego motion and slight variations in the scenery. Our framework caters to various alignment strategies, contingent on the motion assumptions of the scenery, including perspective planar (homography), optical flow with minimal scene change, and optical flow with notable occlusion and disocclusion. The framework efficiently combines multiple inputs into a single neural implicit function, eliminating the necessity of selecting one image as a reference frame. We demonstrate the use of this multi-frame fusion framework for various layer separation tasks.",1
"A distinctive feature of Doppler radar is the measurement of velocity in the radial direction for radar points. However, the missing tangential velocity component hampers object velocity estimation as well as temporal integration of radar sweeps in dynamic scenes. Recognizing that fusing camera with radar provides complementary information to radar, in this paper we present a closed-form solution for the point-wise, full-velocity estimate of Doppler returns using the corresponding optical flow from camera images. Additionally, we address the association problem between radar returns and camera images with a neural network that is trained to estimate radar-camera correspondences. Experimental results on the nuScenes dataset verify the validity of the method and show significant improvements over the state-of-the-art in velocity estimation and accumulation of radar points.",0
"Doppler radar has the ability to measure velocity in the radial direction for radar points, but the absence of the tangential velocity component poses problems for estimating object velocity and integrating radar sweeps in dynamic scenes. To overcome this limitation, we propose a solution in this paper that combines camera and radar data to provide complementary information. Specifically, we present a closed-form solution that allows for the estimation of full-velocity using optical flow from camera images. Additionally, we tackle the challenge of associating radar returns with camera images by developing a neural network that is trained to estimate radar-camera correspondences. Our experimental results using the nuScenes dataset demonstrate the effectiveness of our approach, which outperforms existing methods in both velocity estimation and accumulation of radar points.",1
"Existing video stabilization methods often generate visible distortion or require aggressive cropping of frame boundaries, resulting in smaller field of views. In this work, we present a frame synthesis algorithm to achieve full-frame video stabilization. We first estimate dense warp fields from neighboring frames and then synthesize the stabilized frame by fusing the warped contents. Our core technical novelty lies in the learning-based hybrid-space fusion that alleviates artifacts caused by optical flow inaccuracy and fast-moving objects. We validate the effectiveness of our method on the NUS, selfie, and DeepStab video datasets. Extensive experiment results demonstrate the merits of our approach over prior video stabilization methods.",0
"Many video stabilization methods currently in use can create noticeable distortions or need to crop frame boundaries severely, which results in a smaller field of view. This study introduces a frame synthesis algorithm that achieves full-frame video stabilization. To accomplish this, we first estimate dense warp fields from neighbouring frames, and then we fuse the warped contents to create the stabilized frame. The primary technical innovation of our method is the learning-based hybrid-space fusion, which mitigates the effects of optical flow inaccuracies and fast-moving objects. We tested our approach on the NUS, selfie, and DeepStab video datasets, and the results of extensive experiments indicate that our method has significant advantages over prior video stabilization methods.",1
"Fingerspelling in sign language has been the means of communicating technical terms and proper nouns when they do not have dedicated sign language gestures. Automatic recognition of fingerspelling can help resolve communication barriers when interacting with deaf people. The main challenges prevalent in fingerspelling recognition are the ambiguity in the gestures and strong articulation of the hands. The automatic recognition model should address high inter-class visual similarity and high intra-class variation in the gestures. Most of the existing research in fingerspelling recognition has focused on the dataset collected in a controlled environment. The recent collection of a large-scale annotated fingerspelling dataset in the wild, from social media and online platforms, captures the challenges in a real-world scenario. In this work, we propose a fine-grained visual attention mechanism using the Transformer model for the sequence-to-sequence prediction task in the wild dataset. The fine-grained attention is achieved by utilizing the change in motion of the video frames (optical flow) in sequential context-based attention along with a Transformer encoder model. The unsegmented continuous video dataset is jointly trained by balancing the Connectionist Temporal Classification (CTC) loss and the maximum-entropy loss. The proposed approach can capture better fine-grained attention in a single iteration. Experiment evaluations show that it outperforms the state-of-the-art approaches.",0
"When sign language lacks specific gestures for technical terms and proper nouns, fingerspelling is used for communication. Automated recognition of fingerspelling can assist in overcoming communication barriers when interacting with the deaf. However, the challenges of ambiguity in gestures and strong articulation of hands must be addressed for successful recognition. Previous research has focused on controlled environments, but a new dataset of fingerspelling in the wild has been collected from social media and online platforms. This presents real-world challenges that require a fine-grained visual attention mechanism using the Transformer model for sequence-to-sequence prediction. By utilizing optical flow and a Transformer encoder model, better fine-grained attention can be achieved. The proposed approach combines Connectionist Temporal Classification (CTC) loss and maximum-entropy loss for joint training of unsegmented continuous video datasets. Experiment evaluations demonstrate that this approach outperforms state-of-the-art methods.",1
"High dynamic range (HDR) video reconstruction from sequences captured with alternating exposures is a very challenging problem. Existing methods often align low dynamic range (LDR) input sequence in the image space using optical flow, and then merge the aligned images to produce HDR output. However, accurate alignment and fusion in the image space are difficult due to the missing details in the over-exposed regions and noise in the under-exposed regions, resulting in unpleasing ghosting artifacts. To enable more accurate alignment and HDR fusion, we introduce a coarse-to-fine deep learning framework for HDR video reconstruction. Firstly, we perform coarse alignment and pixel blending in the image space to estimate the coarse HDR video. Secondly, we conduct more sophisticated alignment and temporal fusion in the feature space of the coarse HDR video to produce better reconstruction. Considering the fact that there is no publicly available dataset for quantitative and comprehensive evaluation of HDR video reconstruction methods, we collect such a benchmark dataset, which contains $97$ sequences of static scenes and 184 testing pairs of dynamic scenes. Extensive experiments show that our method outperforms previous state-of-the-art methods. Our dataset, code and model will be made publicly available.",0
"Reconstructing high dynamic range (HDR) video from sequences of alternating exposures is a difficult task, and current methods struggle to align low dynamic range (LDR) input sequences in the image space using optical flow. This results in unsightly ghosting artifacts due to missing details in over-exposed areas and noise in under-exposed regions. To address this issue, we introduce a deep learning framework for HDR video reconstruction that uses a coarse-to-fine approach. We first perform coarse alignment and pixel blending in the image space to estimate the coarse HDR video. Then, we use a more sophisticated alignment and temporal fusion technique in the feature space of the coarse HDR video to produce better reconstruction. To evaluate our method and others, we have created a benchmark dataset containing 97 sequences of static scenes and 184 testing pairs of dynamic scenes. Our experiments show that our method outperforms previous state-of-the-art methods, and we will make our dataset, code, and model publicly available.",1
"This paper presents a pure transformer-based approach, dubbed the Multi-Modal Video Transformer (MM-ViT), for video action recognition. Different from other schemes which solely utilize the decoded RGB frames, MM-ViT operates exclusively in the compressed video domain and exploits all readily available modalities, i.e., I-frames, motion vectors, residuals and audio waveform. In order to handle the large number of spatiotemporal tokens extracted from multiple modalities, we develop several scalable model variants which factorize self-attention across the space, time and modality dimensions. In addition, to further explore the rich inter-modal interactions and their effects, we develop and compare three distinct cross-modal attention mechanisms that can be seamlessly integrated into the transformer building block. Extensive experiments on three public action recognition benchmarks (UCF-101, Something-Something-v2, Kinetics-600) demonstrate that MM-ViT outperforms the state-of-the-art video transformers in both efficiency and accuracy, and performs better or equally well to the state-of-the-art CNN counterparts with computationally-heavy optical flow.",0
"The Multi-Modal Video Transformer (MM-ViT), a transformer-based method for video action recognition, is presented in this paper. Unlike other methods that only use decoded RGB frames, MM-ViT operates solely in the compressed video domain and utilizes all available modalities, including I-frames, motion vectors, residuals, and audio waveform. To handle the large number of spatiotemporal tokens from multiple modalities, scalable model variants are developed that factorize self-attention across space, time, and modality. Additionally, three cross-modal attention mechanisms are developed and compared to explore the rich inter-modal interactions and their effects. Extensive experiments on three public action recognition benchmarks demonstrate that MM-ViT outperforms state-of-the-art video transformers in both efficiency and accuracy, and performs as well as or better than state-of-the-art CNN counterparts with computationally-heavy optical flow.",1
"Warping-based video stabilizers smooth camera trajectory by constraining each pixel's displacement and warp stabilized frames from unstable ones accordingly. However, since the view outside the boundary is not available during warping, the resulting holes around the boundary of the stabilized frame must be discarded (i.e., cropping) to maintain visual consistency, and thus does leads to a tradeoff between stability and cropping ratio. In this paper, we make a first attempt to address this issue by proposing a new Out-of-boundary View Synthesis (OVS) method. By the nature of spatial coherence between adjacent frames and within each frame, OVS extrapolates the out-of-boundary view by aligning adjacent frames to each reference one. Technically, it first calculates the optical flow and propagates it to the outer boundary region according to the affinity, and then warps pixels accordingly. OVS can be integrated into existing warping-based stabilizers as a plug-and-play module to significantly improve the cropping ratio of the stabilized results. In addition, stability is improved because the jitter amplification effect caused by cropping and resizing is reduced. Experimental results on the NUS benchmark show that OVS can improve the performance of five representative state-of-the-art methods in terms of objective metrics and subjective visual quality. The code is publicly available at https://github.com/Annbless/OVS_Stabilization.",0
"Video stabilizers that rely on warping techniques are effective in smoothing camera trajectory by limiting each pixel's displacement and stabilizing frames that are unstable. However, one downside to this approach is that the view beyond the boundary is not available, resulting in holes around the stabilized frame's boundary that must be cropped. This tradeoff between stability and cropping ratio is addressed in this paper by introducing a new Out-of-boundary View Synthesis (OVS) method. OVS leverages spatial coherence between frames and within each frame to extrapolate the out-of-boundary view by aligning adjacent frames to a reference frame. First, OVS calculates optical flow and propagates it to the outer boundary region based on affinity. Then, it warps pixels accordingly. OVS can be easily integrated into existing warping-based stabilizers as a plug-and-play module, significantly improving the cropping ratio of the stabilized results. Additionally, OVS reduces the jitter amplification effect caused by cropping and resizing, resulting in improved stability. The NUS benchmark shows that OVS can enhance the performance of five state-of-the-art methods in terms of both objective metrics and subjective visual quality. The code for OVS is publicly available at https://github.com/Annbless/OVS_Stabilization.",1
"Learning RAW-to-sRGB mapping has drawn increasing attention in recent years, wherein an input raw image is trained to imitate the target sRGB image captured by another camera. However, the severe color inconsistency makes it very challenging to generate well-aligned training pairs of input raw and target sRGB images. While learning with inaccurately aligned supervision is prone to causing pixel shift and producing blurry results. In this paper, we circumvent such issue by presenting a joint learning model for image alignment and RAW-to-sRGB mapping. To diminish the effect of color inconsistency in image alignment, we introduce to use a global color mapping (GCM) module to generate an initial sRGB image given the input raw image, which can keep the spatial location of the pixels unchanged, and the target sRGB image is utilized to guide GCM for converting the color towards it. Then a pre-trained optical flow estimation network (e.g., PWC-Net) is deployed to warp the target sRGB image to align with the GCM output. To alleviate the effect of inaccurately aligned supervision, the warped target sRGB image is leveraged to learn RAW-to-sRGB mapping. When training is done, the GCM module and optical flow network can be detached, thereby bringing no extra computation cost for inference. Experiments show that our method performs favorably against state-of-the-arts on ZRR and SR-RAW datasets. With our joint learning model, a light-weight backbone can achieve better quantitative and qualitative performance on ZRR dataset. Codes are available at https://github.com/cszhilu1998/RAW-to-sRGB.",0
"In recent years, there has been a growing interest in learning how to map RAW-to-sRGB, which involves training an input raw image to resemble the target sRGB image captured by a different camera. However, due to the significant color inconsistency, it is challenging to generate well-aligned training pairs of input raw and target sRGB images. This can result in pixel shift and blurry outcomes when learning with inaccurately aligned supervision. To address this issue, we propose a joint learning model for image alignment and RAW-to-sRGB mapping. Our approach uses a global color mapping (GCM) module to create an initial sRGB image from the input raw image. The GCM module is guided by the target sRGB image to reduce color inconsistency during image alignment. We then use a pre-trained optical flow estimation network to warp the target sRGB image to align with the GCM output. This warped target sRGB image is then used to learn RAW-to-sRGB mapping. Our experiments show that our method outperforms state-of-the-art approaches on ZRR and SR-RAW datasets. Furthermore, our joint learning model allows a lightweight backbone to achieve superior quantitative and qualitative performance on the ZRR dataset. The code for our method is available at https://github.com/cszhilu1998/RAW-to-sRGB.",1
"Existing optical flow methods are erroneous in challenging scenes, such as fog, rain, and night because the basic optical flow assumptions such as brightness and gradient constancy are broken. To address this problem, we present an unsupervised learning approach that fuses gyroscope into optical flow learning. Specifically, we first convert gyroscope readings into motion fields named gyro field. Second, we design a self-guided fusion module to fuse the background motion extracted from the gyro field with the optical flow and guide the network to focus on motion details. To the best of our knowledge, this is the first deep learning-based framework that fuses gyroscope data and image content for optical flow learning. To validate our method, we propose a new dataset that covers regular and challenging scenes. Experiments show that our method outperforms the state-of-art methods in both regular and challenging scenes. Code and dataset are available at https://github.com/megvii-research/GyroFlow.",0
"In difficult environmental conditions like fog, rain, and darkness, current optical flow techniques are flawed due to the violation of basic assumptions such as brightness and gradient constancy. As a solution, we introduce an unsupervised learning method that integrates gyroscopes into optical flow learning. Our approach involves converting gyroscope data into motion fields named gyro fields and merging background motion extracted from the gyro field with optical flow via a self-guided fusion module to emphasize motion details. This is the first deep learning-based framework that incorporates gyro data and image content for optical flow learning. We have also developed a new dataset that covers a range of scene types to validate our approach. Our experiments demonstrate that our approach surpasses existing methods in both regular and challenging scenes. Code and dataset are accessible at https://github.com/megvii-research/GyroFlow.",1
"We propose a novel framework for video inpainting by adopting an internal learning strategy. Unlike previous methods that use optical flow for cross-frame context propagation to inpaint unknown regions, we show that this can be achieved implicitly by fitting a convolutional neural network to known regions. Moreover, to handle challenging sequences with ambiguous backgrounds or long-term occlusion, we design two regularization terms to preserve high-frequency details and long-term temporal consistency. Extensive experiments on the DAVIS dataset demonstrate that the proposed method achieves state-of-the-art inpainting quality quantitatively and qualitatively. We further extend the proposed method to another challenging task: learning to remove an object from a video giving a single object mask in only one frame in a 4K video.",0
"Our proposition presents a new framework for video inpainting that involves an internal learning approach. Our method differs from previous techniques that utilize optical flow to propagate context across frames in order to fill in unknown areas. Instead, we demonstrate that this can be accomplished implicitly by training a convolutional neural network on known regions. Additionally, we have developed two regularization terms to address problematic sequences that involve ambiguous backgrounds or long-term occlusion. These regularization terms ensure the preservation of high-frequency details and long-term temporal consistency. Our experiments on the DAVIS dataset show that our proposed method achieves superior quantitative and qualitative inpainting quality. We have also extended our approach to address the challenge of removing an object from a 4K video with only one object mask provided in a single frame.",1
"The existing particle image velocimetry (PIV) do not consider the curvature effect of the non-straight particle trajectory, because it seems to be impossible to obtain the curvature information from a pair of particle images. As a result, the computed vector underestimates the real velocity due to the straight-line approximation, that further causes a systematic error for the PIV instrument. In this work, the particle curved trajectory between two recordings is firstly explained with the streamline segment of a steady flow (diffeomorphic transformation) instead of a single vector, and this idea is termed as diffeomorphic PIV. Specifically, a deformation field is introduced to describe the particle displacement, i.e., we try to find the optimal velocity field, of which the corresponding deformation vector field agrees with the particle displacement. Because the variation of the deformation function can be approximated with the variation of the velocity function, the diffeomorphic PIV can be implemented as iterative PIV. That says, the diffeomorphic PIV warps the images with deformation vector field instead of the velocity, and keeps the rest as same as iterative PIVs. Two diffeomorphic deformation schemes -- forward diffeomorphic deformation interrogation (FDDI) and central diffeomorphic deformation interrogation (CDDI) -- are proposed. Tested on synthetic images, the FDDI achieves significant accuracy improvement across different one-pass displacement estimators (cross-correlation, optical flow, deep learning flow). Besides, the results on three real PIV image pairs demonstrate the non-negligible curvature effect for CDI-based PIV, and our FDDI provides larger velocity estimation (more accurate) in the fast curvy streamline areas. The accuracy improvement of the combination of FDDI and accurate dense estimator means that our diffeomorphic PIV paves a new way for complex flow measurement.",0
"The current particle image velocimetry (PIV) methods do not take into account the curvature effect of non-straight particle trajectories. This is because it is difficult to obtain curvature information from a pair of particle images. Consequently, the computed velocity vector is underestimated due to the straight-line approximation, leading to systematic errors in the PIV instrument. In this study, we propose a new approach called diffeomorphic PIV, which explains the curved trajectory between two recordings using the streamline segment of a steady flow. This is achieved by introducing a deformation field to describe particle displacement, with the optimal velocity field being found such that the corresponding deformation vector field matches the particle displacement. Diffeomorphic PIV is implemented as an iterative PIV, with image warping done using the deformation vector field instead of velocity. Two diffeomorphic deformation schemes, forward diffeomorphic deformation interrogation (FDDI) and central diffeomorphic deformation interrogation (CDDI), are proposed. FDDI achieves significant accuracy improvement across different one-pass displacement estimators, as demonstrated through testing on synthetic and real PIV image pairs. The results show that curvature effect is non-negligible for CDI-based PIV, and our FDDI provides more accurate velocity estimation in fast curvy streamline areas. The combination of FDDI and accurate dense estimator paves a new way for complex flow measurement.",1
"We propose a novel framework for finding correspondences in images based on a deep neural network that, given two images and a query point in one of them, finds its correspondence in the other. By doing so, one has the option to query only the points of interest and retrieve sparse correspondences, or to query all points in an image and obtain dense mappings. Importantly, in order to capture both local and global priors, and to let our model relate between image regions using the most relevant among said priors, we realize our network using a transformer. At inference time, we apply our correspondence network by recursively zooming in around the estimates, yielding a multiscale pipeline able to provide highly-accurate correspondences. Our method significantly outperforms the state of the art on both sparse and dense correspondence problems on multiple datasets and tasks, ranging from wide-baseline stereo to optical flow, without any retraining for a specific dataset. We commit to releasing data, code, and all the tools necessary to train from scratch and ensure reproducibility.",0
"Our innovative approach proposes a framework for discovering correspondences in images that utilizes a deep neural network. The network is designed to locate the correspondence of a query point in one image to the corresponding point in the other image. This allows for the retrieval of sparse correspondences when only points of interest are queried, or dense mappings when all points in an image are queried. To capture both local and global priors and enable the model to relate between image regions using the most relevant priors, we utilize a transformer. During inference, our correspondence network is applied by recursively zooming in around the estimates, creating a multiscale pipeline that delivers highly accurate correspondences. Our method surpasses the state of the art in both sparse and dense correspondence problems across various datasets and tasks, such as wide-baseline stereo or optical flow, without requiring retraining for a particular dataset. We pledge to provide data, code, and all necessary tools for training from scratch and ensuring reproducibility.",1
"In this paper, we propose $\text{HF}^2$-VAD, a Hybrid framework that integrates Flow reconstruction and Frame prediction seamlessly to handle Video Anomaly Detection. Firstly, we design the network of ML-MemAE-SC (Multi-Level Memory modules in an Autoencoder with Skip Connections) to memorize normal patterns for optical flow reconstruction so that abnormal events can be sensitively identified with larger flow reconstruction errors. More importantly, conditioned on the reconstructed flows, we then employ a Conditional Variational Autoencoder (CVAE), which captures the high correlation between video frame and optical flow, to predict the next frame given several previous frames. By CVAE, the quality of flow reconstruction essentially influences that of frame prediction. Therefore, poorly reconstructed optical flows of abnormal events further deteriorate the quality of the final predicted future frame, making the anomalies more detectable. Experimental results demonstrate the effectiveness of the proposed method. Code is available at \href{https://github.com/LiUzHiAn/hf2vad}{https://github.com/LiUzHiAn/hf2vad}.",0
"The paper introduces $\text{HF}^2$-VAD, a Hybrid approach for Video Anomaly Detection that seamlessly integrates Flow reconstruction and Frame prediction. The proposed method utilizes ML-MemAE-SC, a network with Multi-Level Memory modules in an Autoencoder with Skip Connections, to memorize normal patterns for optical flow reconstruction, enabling the identification of abnormal events with larger flow reconstruction errors. Additionally, a Conditional Variational Autoencoder (CVAE) is employed to predict the next frame based on several previous frames, conditioned on the reconstructed flows. The quality of flow reconstruction crucially affects that of frame prediction, making anomalies more detectable by deteriorating the quality of the predicted future frame. Experimental results demonstrate the effectiveness of the proposed method, and the code is available at \href{https://github.com/LiUzHiAn/hf2vad}{https://github.com/LiUzHiAn/hf2vad}.",1
"Event camera is an emerging imaging sensor for capturing dynamics of moving objects as events, which motivates our work in estimating 3D human pose and shape from the event signals. Events, on the other hand, have their unique challenges: rather than capturing static body postures, the event signals are best at capturing local motions. This leads us to propose a two-stage deep learning approach, called EventHPE. The first-stage, FlowNet, is trained by unsupervised learning to infer optical flow from events. Both events and optical flow are closely related to human body dynamics, which are fed as input to the ShapeNet in the second stage, to estimate 3D human shapes. To mitigate the discrepancy between image-based flow (optical flow) and shape-based flow (vertices movement of human body shape), a novel flow coherence loss is introduced by exploiting the fact that both flows are originated from the identical human motion. An in-house event-based 3D human dataset is curated that comes with 3D pose and shape annotations, which is by far the largest one to our knowledge. Empirical evaluations on DHP19 dataset and our in-house dataset demonstrate the effectiveness of our approach.",0
"Our research focuses on using the event camera, a novel imaging sensor, to capture the movements of objects and estimate the 3D pose and shape of humans. However, events present unique challenges as they are best suited for capturing local motions rather than static body postures. To address this, we propose a two-stage deep learning approach called EventHPE. The first stage, FlowNet, uses unsupervised learning to infer optical flow from events, which are both related to human body dynamics. The second stage, ShapeNet, estimates 3D human shapes using both events and optical flow as inputs. To overcome the discrepancy between image-based flow and shape-based flow, we introduce a novel flow coherence loss that exploits the fact that both flows originate from the same human motion. We curated an in-house event-based 3D human dataset that is currently the largest of its kind and evaluated our approach on both the DHP19 and our in-house dataset, demonstrating its effectiveness.",1
"Weakly-Supervised Temporal Action Localization (WSTAL) aims to localize actions in untrimmed videos with only video-level labels. Currently, most state-of-the-art WSTAL methods follow a Multi-Instance Learning (MIL) pipeline: producing snippet-level predictions first and then aggregating to the video-level prediction. However, we argue that existing methods have overlooked two important drawbacks: 1) inadequate use of motion information and 2) the incompatibility of prevailing cross-entropy training loss. In this paper, we analyze that the motion cues behind the optical flow features are complementary informative. Inspired by this, we propose to build a context-dependent motion prior, termed as motionness. Specifically, a motion graph is introduced to model motionness based on the local motion carrier (e.g., optical flow). In addition, to highlight more informative video snippets, a motion-guided loss is proposed to modulate the network training conditioned on motionness scores. Extensive ablation studies confirm that motionness efficaciously models action-of-interest, and the motion-guided loss leads to more accurate results. Besides, our motion-guided loss is a plug-and-play loss function and is applicable with existing WSTAL methods. Without loss of generality, based on the standard MIL pipeline, our method achieves new state-of-the-art performance on three challenging benchmarks, including THUMOS'14, ActivityNet v1.2 and v1.3.",0
"The objective of Weakly-Supervised Temporal Action Localization (WSTAL) is to identify actions in untrimmed videos using video-level labels only. The current state-of-the-art WSTAL techniques follow a Multi-Instance Learning (MIL) approach, where they initially make predictions at the snippet-level and then combine them to make a prediction at the video-level. However, this approach has neglected two crucial concerns: 1) insufficient utilization of motion information and 2) the incompatibility of the prevalent cross-entropy training loss. This research examines that the motion cues present in the optical flow features are highly complementary and informative. Therefore, a context-dependent motion prior, referred to as motionness, was created by using a motion graph to model motionness based on the local motion carrier (i.e., optical flow). Furthermore, a motion-guided loss was proposed to train the network based on the motionness scores and to highlight the most informative video snippets. A thorough analysis demonstrates that motionness models the action-of-interest effectively, and the motion-guided loss yields more precise results. Additionally, the motion-guided loss can be utilized with existing WSTAL methods as it is a plug-and-play loss function. Our method, based on the standard MIL pipeline, achieves state-of-the-art performance on three challenging benchmarks, including THUMOS'14, ActivityNet v1.2 and v1.3.",1
"We propose RIFE, a Real-time Intermediate Flow Estimation algorithm for Video Frame Interpolation (VFI). Many recent flow-based VFI methods first estimate the bi-directional optical flows, then scale and reverse them to approximate intermediate flows, leading to artifacts on motion boundaries. RIFE uses a neural network named IFNet that can directly estimate the intermediate flows from coarse-to-fine with much better speed. We design a privileged distillation scheme for training intermediate flow model, which leads to a large performance improvement. Experiments demonstrate that RIFE is flexible and can achieve state-of-the-art performance on several public benchmarks. The code is available at \url{https://github.com/hzwer/arXiv2020-RIFE}",0
"Our proposal is RIFE, an algorithm for Real-time Intermediate Flow Estimation for Video Frame Interpolation (VFI). Current flow-based VFI methods estimate bi-directional optical flows, then approximate intermediate flows by scaling and reversing them, which results in artifacts at motion boundaries. RIFE uses IFNet, a neural network that directly estimates intermediate flows from coarse-to-fine, providing better speed. To train the intermediate flow model, we employ a privileged distillation scheme, which results in a significant improvement in performance. Our experiments show that RIFE is adaptable and can achieve state-of-the-art performance on multiple public benchmarks. The code is available at \url{https://github.com/hzwer/arXiv2020-RIFE}.",1
"Animals have evolved highly functional visual systems to understand motion, assisting perception even under complex environments. In this paper, we work towards developing a computer vision system able to segment objects by exploiting motion cues, i.e. motion segmentation. We make the following contributions: First, we introduce a simple variant of the Transformer to segment optical flow frames into primary objects and the background. Second, we train the architecture in a self-supervised manner, i.e. without using any manual annotations. Third, we analyze several critical components of our method and conduct thorough ablation studies to validate their necessity. Fourth, we evaluate the proposed architecture on public benchmarks (DAVIS2016, SegTrackv2, and FBMS59). Despite using only optical flow as input, our approach achieves superior or comparable results to previous state-of-the-art self-supervised methods, while being an order of magnitude faster. We additionally evaluate on a challenging camouflage dataset (MoCA), significantly outperforming the other self-supervised approaches, and comparing favourably to the top supervised approach, highlighting the importance of motion cues, and the potential bias towards visual appearance in existing video segmentation models.",0
"The visual systems of animals have evolved to understand motion, enabling them to perceive even in complex environments. This study aims to develop a computer vision system that can segment objects by utilizing motion cues, specifically motion segmentation. The study presents several contributions: firstly, a simple variant of the Transformer is introduced to segment optical flow frames into primary objects and the background. Secondly, the architecture is trained in a self-supervised manner without manual annotations. Thirdly, critical components of the method are analyzed, and thorough ablation studies are conducted to confirm their necessity. Finally, the proposed architecture is evaluated on various public benchmarks, including DAVIS2016, SegTrackv2, and FBMS59. Despite using only optical flow as input, the approach achieves superior or comparable results to previous state-of-the-art self-supervised methods and is an order of magnitude faster. The study also evaluates the proposed architecture on a challenging camouflage dataset (MoCA), outperforming other self-supervised approaches and comparing favorably to the top supervised approach, highlighting the importance of motion cues and the potential bias towards visual appearance in existing video segmentation models.",1
"Location and appearance are the key cues for video object segmentation. Many sources such as RGB, depth, optical flow and static saliency can provide useful information about the objects. However, existing approaches only utilize the RGB or RGB and optical flow. In this paper, we propose a novel multi-source fusion network for zero-shot video object segmentation. With the help of interoceptive spatial attention module (ISAM), spatial importance of each source is highlighted. Furthermore, we design a feature purification module (FPM) to filter the inter-source incompatible features. By the ISAM and FPM, the multi-source features are effectively fused. In addition, we put forward an automatic predictor selection network (APS) to select the better prediction of either the static saliency predictor or the moving object predictor in order to prevent over-reliance on the failed results caused by low-quality optical flow maps. Extensive experiments on three challenging public benchmarks (i.e. DAVIS$_{16}$, Youtube-Objects and FBMS) show that the proposed model achieves compelling performance against the state-of-the-arts. The source code will be publicly available at \textcolor{red}{\url{https://github.com/Xiaoqi-Zhao-DLUT/Multi-Source-APS-ZVOS}}.",0
"The primary indicators for video object segmentation are location and appearance. Multiple sources, including RGB, depth, optical flow, and static saliency, can provide valuable insights into objects. However, existing methods only utilize RGB or RGB and optical flow. This study introduces a novel multi-source fusion network for zero-shot video object segmentation that highlights the spatial importance of each source using an interoceptive spatial attention module (ISAM). Additionally, a feature purification module (FPM) is designed to filter inter-source incompatible features. The multi-source features are effectively fused using ISAM and FPM. Furthermore, an automatic predictor selection network (APS) selects the better of the static saliency predictor or the moving object predictor to avoid over-reliance on low-quality optical flow maps that result in failed outcomes. Extensive experiments on three challenging public benchmarks, including DAVIS$_{16}$, Youtube-Objects, and FBMS, demonstrate that the proposed model outperforms existing state-of-the-art methods. The source code will be available for public access at \textcolor{red}{\url{https://github.com/Xiaoqi-Zhao-DLUT/Multi-Source-APS-ZVOS}}.",1
"Self-supervised deep learning-based 3D scene understanding methods can overcome the difficulty of acquiring the densely labeled ground-truth and have made a lot of advances. However, occlusions and moving objects are still some of the major limitations. In this paper, we explore the learnable occlusion aware optical flow guided self-supervised depth and camera pose estimation by an adaptive cross weighted loss to address the above limitations. Firstly, we explore to train the learnable occlusion mask fused optical flow network by an occlusion-aware photometric loss with the temporally supplemental information and backward-forward consistency of adjacent views. And then, we design an adaptive cross-weighted loss between the depth-pose and optical flow loss of the geometric and photometric error to distinguish the moving objects which violate the static scene assumption. Our method shows promising results on KITTI, Make3D, and Cityscapes datasets under multiple tasks. We also show good generalization ability under a variety of challenging scenarios.",0
"Advanced 3D scene understanding methods based on self-supervised deep learning have made significant progress in overcoming the challenge of obtaining densely labeled ground-truth. However, occlusions and moving objects still present major limitations to these methods. This study explores the use of a learnable occlusion-aware optical flow guided self-supervised depth and camera pose estimation method, which utilizes an adaptive cross-weighted loss to address these limitations. The approach involves training a learnable occlusion mask fused optical flow network using an occlusion-aware photometric loss with temporal supplemental information and backward-forward consistency of adjacent views. An adaptive cross-weighted loss is then designed to distinguish moving objects that violate the static scene assumption between the depth-pose and optical flow loss of the geometric and photometric error. Our method demonstrates promising results on various datasets including KITTI, Make3D, and Cityscapes under multiple tasks, and showcases good generalization ability across various challenging scenarios.",1
"This paper strives for action recognition and detection in video modalities like RGB, depth maps or 3D-skeleton sequences when only limited modality-specific labeled examples are available. For the RGB, and derived optical-flow, modality many large-scale labeled datasets have been made available. They have become the de facto pre-training choice when recognizing or detecting new actions from RGB datasets that have limited amounts of labeled examples available. Unfortunately, large-scale labeled action datasets for other modalities are unavailable for pre-training. In this paper, our goal is to recognize actions from limited examples in non-RGB video modalities, by learning from large-scale labeled RGB data. To this end, we propose a two-step training process: (i) we extract action representation knowledge from an RGB-trained teacher network and adapt it to a non-RGB student network. (ii) we then fine-tune the transfer model with available labeled examples of the target modality. For the knowledge transfer we introduce feature-supervision strategies, which rely on unlabeled pairs of two modalities (the RGB and the target modality) to transfer feature level representations from the teacher to the student network. Ablations and generalizations with two RGB source datasets and two non-RGB target datasets demonstrate that an optical-flow teacher provides better action transfer features than RGB for both depth maps and 3D-skeletons, even when evaluated on a different target domain, or for a different task. Compared to alternative cross-modal action transfer methods we show a good improvement in performance especially when labeled non-RGB examples to learn from are scarce",0
"The main objective of this paper is to achieve action recognition and detection in different video modalities such as RGB, depth maps, and 3D-skeleton sequences, even when the availability of modality-specific labeled examples is limited. Although there are many large-scale labeled datasets available for RGB and derived optical-flow modalities, other modalities lack such datasets for pre-training. Therefore, the paper proposes a two-step training process to recognize actions from limited examples in non-RGB video modalities by learning from a large-scale labeled RGB dataset. Firstly, the paper extracts knowledge regarding action representation from an RGB-trained teacher network and then adapts it to a non-RGB student network. Secondly, the transfer model is fine-tuned with available labeled examples of the target modality. To transfer knowledge, the paper introduces feature-supervision strategies based on unlabeled pairs of two modalities to transfer feature level representations from the teacher to the student network. The paper shows that an optical-flow teacher provides better action transfer features than RGB for both depth maps and 3D-skeletons, even when evaluated on a different target domain or for a different task. Compared to other cross-modal action transfer methods, the paper demonstrates a good improvement in performance, particularly when labeled non-RGB examples to learn from are scarce.",1
"Estimating the states of surrounding traffic participants stays at the core of autonomous driving. In this paper, we study a novel setting of this problem: model-free single-object tracking (SOT), which takes the object state in the first frame as input, and jointly solves state estimation and tracking in subsequent frames. The main purpose for this new setting is to break the strong limitation of the popular ""detection and tracking"" scheme in multi-object tracking. Moreover, we notice that shape completion by overlaying the point clouds, which is a by-product of our proposed task, not only improves the performance of state estimation but also has numerous applications. As no benchmark for this task is available so far, we construct a new dataset LiDAR-SOT and corresponding evaluation protocols based on the Waymo Open dataset. We then propose an optimization-based algorithm called SOTracker involving point cloud registration, vehicle shapes, correspondence, and motion priors. Our quantitative and qualitative results prove the effectiveness of our SOTracker and reveal the challenging cases for SOT in point clouds, including the sparsity of LiDAR data, abrupt motion variation, etc. Finally, we also explore how the proposed task and algorithm may benefit other autonomous driving applications, including simulating LiDAR scans, generating motion data, and annotating optical flow. The code and protocols for our benchmark and algorithm are available at https://github.com/TuSimple/LiDAR_SOT/. A video demonstration is at https://www.youtube.com/watch?v=BpHixKs91i8.",0
"The primary focus of autonomous driving is to accurately estimate the states of traffic participants in the surrounding area. This paper examines a new approach to this problem, which involves single-object tracking (SOT) without the use of models. This approach takes the object state in the first frame as input and solves state estimation and tracking in subsequent frames. The aim of this new method is to overcome the limitations of the popular ""detection and tracking"" scheme in multi-object tracking. Additionally, we observe that overlaying point clouds can improve the performance of state estimation and have multiple applications. As there is no benchmark for this task, we construct a new dataset called LiDAR-SOT and corresponding evaluation protocols based on the Waymo Open dataset. We propose an optimization-based algorithm called SOTracker that involves point cloud registration, vehicle shapes, correspondence, and motion priors. Our results show that SOTracker is effective, but SOT in point clouds is challenging due to the sparsity of LiDAR data and abrupt motion variation. Finally, we explore how this task and algorithm can benefit other autonomous driving applications, such as simulating LiDAR scans, generating motion data, and annotating optical flow. The code and protocols for our benchmark and algorithm are available at https://github.com/TuSimple/LiDAR_SOT/. A video demonstration can be viewed at https://www.youtube.com/watch?v=BpHixKs91i8.",1
"In this paper, we firstly present a dataset (X4K1000FPS) of 4K videos of 1000 fps with the extreme motion to the research community for video frame interpolation (VFI), and propose an extreme VFI network, called XVFI-Net, that first handles the VFI for 4K videos with large motion. The XVFI-Net is based on a recursive multi-scale shared structure that consists of two cascaded modules for bidirectional optical flow learning between two input frames (BiOF-I) and for bidirectional optical flow learning from target to input frames (BiOF-T). The optical flows are stably approximated by a complementary flow reversal (CFR) proposed in BiOF-T module. During inference, the BiOF-I module can start at any scale of input while the BiOF-T module only operates at the original input scale so that the inference can be accelerated while maintaining highly accurate VFI performance. Extensive experimental results show that our XVFI-Net can successfully capture the essential information of objects with extremely large motions and complex textures while the state-of-the-art methods exhibit poor performance. Furthermore, our XVFI-Net framework also performs comparably on the previous lower resolution benchmark dataset, which shows a robustness of our algorithm as well. All source codes, pre-trained models, and proposed X4K1000FPS datasets are publicly available at https://github.com/JihyongOh/XVFI.",0
"This paper introduces the X4K1000FPS dataset comprising 4K videos of 1000 fps with extreme motion for video frame interpolation (VFI). We also propose a new VFI network, XVFI-Net, which is specifically designed to handle VFI for 4K videos with large motion. The XVFI-Net is based on a recursive multi-scale shared structure consisting of two cascaded modules for bidirectional optical flow learning. The BiOF-I module can start at any scale of input, while the BiOF-T module only operates at the original input scale to accelerate inference while maintaining high VFI accuracy. Our XVFI-Net successfully captures the essential information of objects with extremely large motions and complex textures while outperforming state-of-the-art methods. We also demonstrate the robustness of our algorithm on a previous lower resolution benchmark dataset. All source codes, pre-trained models, and the proposed X4K1000FPS datasets are publicly available at https://github.com/JihyongOh/XVFI.",1
"Modern methods for counting people in crowded scenes rely on deep networks to estimate people densities in individual images. As such, only very few take advantage of temporal consistency in video sequences, and those that do only impose weak smoothness constraints across consecutive frames. In this paper, we advocate estimating people flows across image locations between consecutive images and inferring the people densities from these flows instead of directly regressing them. This enables us to impose much stronger constraints encoding the conservation of the number of people. As a result, it significantly boosts performance without requiring a more complex architecture. Furthermore, it allows us to exploit the correlation between people flow and optical flow to further improve the results. We also show that leveraging people conservation constraints in both a spatial and temporal manner makes it possible to train a deep crowd counting model in an active learning setting with much fewer annotations. This significantly reduces the annotation cost while still leading to similar performance to the full supervision case.",0
"Counting people in crowded scenes nowadays involves the use of deep networks to determine people densities in individual images. However, only a few methods utilize temporal consistency in video sequences, and those that do only impose weak smoothness constraints across consecutive frames. In this study, we propose estimating people flows across image locations between consecutive images and inferring the people densities from these flows instead of directly regressing them. This approach allows us to impose much stronger constraints that ensure the conservation of the number of people, resulting in a significant improvement in performance without the need for a more complex architecture. Additionally, we can leverage the correlation between people flow and optical flow to further enhance the results. Our approach can also reduce the annotation cost by training a deep crowd counting model in an active learning setting with fewer annotations, while still achieving comparable performance to the full supervision case.",1
"The recently-proposed Perceiver model obtains good results on several domains (images, audio, multimodal, point clouds) while scaling linearly in compute and memory with the input size. While the Perceiver supports many kinds of inputs, it can only produce very simple outputs such as class scores. Perceiver IO overcomes this limitation without sacrificing the original's appealing properties by learning to flexibly query the model's latent space to produce outputs of arbitrary size and semantics. Perceiver IO still decouples model depth from data size and still scales linearly with data size, but now with respect to both input and output sizes. The full Perceiver IO model achieves strong results on tasks with highly structured output spaces, such as natural language and visual understanding, StarCraft II, and multi-task and multi-modal domains. As highlights, Perceiver IO matches a Transformer-based BERT baseline on the GLUE language benchmark without the need for input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation.",0
"The Perceiver model, which has been proposed recently, has shown excellent results across different domains, including images, audio, multimodal, and point clouds. It has the advantage of scaling linearly in compute and memory with the input size, but it can only produce simple outputs like class scores. However, Perceiver IO has been developed to overcome this limitation and can produce outputs of any size and semantics by flexibly querying the model's latent space. Perceiver IO still maintains the original model's desirable properties, such as decoupling model depth from data size and scaling linearly with data size. The full Perceiver IO model has achieved impressive results on tasks with highly structured output spaces, such as natural language and visual understanding, StarCraft II, and multi-task and multi-modal domains. Notably, Perceiver IO has matched a Transformer-based BERT baseline on the GLUE language benchmark without the need for input tokenization and has achieved state-of-the-art performance on Sintel optical flow estimation.",1
"In Computer Vision,object tracking is a very old and complex problem.Though there are several existing algorithms for object tracking, still there are several challenges remain to be solved. For instance, variation of illumination of light, noise, occlusion, sudden start and stop of moving object, shading etc,make the object tracking a complex problem not only for dynamic background but also for static background. In this paper we propose a dual approach for object tracking based on optical flow and swarm Intelligence.The optical flow based KLT(Kanade-Lucas-Tomasi) tracker, tracks the dominant points of the target object from first frame to last frame of a video sequence;whereas swarm Intelligence based PSO (Particle Swarm Optimization) tracker simultaneously tracks the boundary information of the target object from second frame to last frame of the same video sequence.This dual function of tracking makes the trackers very much robust with respect to the above stated problems. The flexibility of our approach is that it can be successfully applicable in variable background as well as static background.We compare the performance of the proposed dual tracking algorithm with several benchmark datasets and obtain very competitive results in general and in most of the cases we obtained superior results using dual tracking algorithm. We also compare the performance of the proposed dual tracker with some existing PSO based algorithms for tracking and achieved better results.",0
"The problem of object tracking in Computer Vision is a complex and long-standing issue, despite the existence of various algorithms. There are several challenges that have yet to be overcome, such as changes in lighting, noise, occlusion, sudden movements, and shading. These factors make object tracking difficult not only for dynamic but also for static backgrounds. To address this, our paper presents a dual approach to object tracking based on optical flow and swarm intelligence. The optical flow-based KLT tracker tracks the main points of the target object from the first to the last frame of the video sequence. Meanwhile, the swarm intelligence-based PSO tracker simultaneously tracks the boundary information of the target object from the second frame to the last frame of the same video sequence. This dual function of tracking makes our trackers more robust to the aforementioned issues. Our approach is flexible and can be applied successfully in variable and static backgrounds. We compared the performance of our proposed dual tracking algorithm with various benchmark datasets and achieved highly competitive results, with superior performance in most cases. Moreover, we compared the performance of our proposed dual tracker with some existing PSO-based algorithms and achieved better results.",1
"Automatic facial action unit (AU) recognition is a challenging task due to the scarcity of manual annotations. To alleviate this problem, a large amount of efforts has been dedicated to exploiting various methods which leverage numerous unlabeled data. However, many aspects with regard to some unique properties of AUs, such as the regional and relational characteristics, are not sufficiently explored in previous works. Motivated by this, we take the AU properties into consideration and propose two auxiliary AU related tasks to bridge the gap between limited annotations and the model performance in a self-supervised manner via the unlabeled data. Specifically, to enhance the discrimination of regional features with AU relation embedding, we design a task of RoI inpainting to recover the randomly cropped AU patches. Meanwhile, a single image based optical flow estimation task is proposed to leverage the dynamic change of facial muscles and encode the motion information into the global feature representation. Based on these two self-supervised auxiliary tasks, local features, mutual relation and motion cues of AUs are better captured in the backbone network with the proposed regional and temporal based auxiliary task learning (RTATL) framework. Extensive experiments on BP4D and DISFA demonstrate the superiority of our method and new state-of-the-art performances are achieved.",0
"Recognizing automatic facial action units (AU) is difficult due to the limited availability of manual annotations. Researchers have tried to overcome this obstacle by using various methods to exploit unlabeled data. However, previous works have not adequately explored the regional and relational characteristics of AUs, leading to suboptimal performance. To address this, we propose two auxiliary tasks that consider AU properties and use self-supervised learning to improve model performance using unlabeled data. The first task is RoI inpainting, which recovers randomly cropped AU patches to enhance the discrimination of regional features with AU relation embedding. The second task is single image-based optical flow estimation, which captures the dynamic change of facial muscles and encodes the motion information into the global feature representation. Our proposed framework, the Regional and Temporal-based Auxiliary Task Learning (RTATL) framework, better captures local features, mutual relations, and motion cues of AUs in the backbone network. Our experiments on BP4D and DISFA datasets demonstrate the superiority of our method, achieving new state-of-the-art performances.",1
"Occlusions pose a significant challenge to optical flow algorithms that rely on local evidences. We consider an occluded point to be one that is imaged in the first frame but not in the next, a slight overloading of the standard definition since it also includes points that move out-of-frame. Estimating the motion of these points is extremely difficult, particularly in the two-frame setting. Previous work relies on CNNs to learn occlusions, without much success, or requires multiple frames to reason about occlusions using temporal smoothness. In this paper, we argue that the occlusion problem can be better solved in the two-frame case by modelling image self-similarities. We introduce a global motion aggregation module, a transformer-based approach to find long-range dependencies between pixels in the first image, and perform global aggregation on the corresponding motion features. We demonstrate that the optical flow estimates in the occluded regions can be significantly improved without damaging the performance in non-occluded regions. This approach obtains new state-of-the-art results on the challenging Sintel dataset, improving the average end-point error by 13.6% on Sintel Final and 13.7% on Sintel Clean. At the time of submission, our method ranks first on these benchmarks among all published and unpublished approaches. Code is available at https://github.com/zacjiang/GMA",0
"Optical flow algorithms that depend on local evidences face a major challenge with occlusions. An occluded point is defined as one that appears in the first frame but not in the next, including points that move out-of-frame. Estimating motion for these points is difficult, especially in the two-frame setting. Previous approaches have relied on CNNs to learn occlusions, with limited success, or require multiple frames to reason about occlusions through temporal smoothness. However, this paper proposes that image self-similarities can be used to solve the occlusion problem better in the two-frame case. The proposed method involves introducing a global motion aggregation module, which uses a transformer-based approach to identify long-range dependencies between pixels in the first image and perform global aggregation on corresponding motion features. This approach significantly improves optical flow estimates in occluded regions without compromising non-occluded regions. The proposed method achieves state-of-the-art results on the challenging Sintel dataset, improving the average end-point error by 13.6% on Sintel Final and 13.7% on Sintel Clean. Code for the proposed method is available at https://github.com/zacjiang/GMA.",1
"While single image shadow detection has been improving rapidly in recent years, video shadow detection remains a challenging task due to data scarcity and the difficulty in modelling temporal consistency. The current video shadow detection method achieves this goal via co-attention, which mostly exploits information that is temporally coherent but is not robust in detecting moving shadows and small shadow regions. In this paper, we propose a simple but powerful method to better aggregate information temporally. We use an optical flow based warping module to align and then combine features between frames. We apply this warping module across multiple deep-network layers to retrieve information from neighboring frames including both local details and high-level semantic information. We train and test our framework on the ViSha dataset. Experimental results show that our model outperforms the state-of-the-art video shadow detection method by 28%, reducing BER from 16.7 to 12.0.",0
"Despite the rapid improvement in single image shadow detection, video shadow detection remains a difficult task due to a shortage of data and the challenge of modeling temporal consistency. The current approach to video shadow detection relies on co-attention, which predominantly utilizes temporally coherent information but is not effective in detecting moving shadows or small shadow regions. This paper proposes a straightforward yet effective method of aggregating information temporally by using an optical flow-based warping module to align and combine features between frames. The warping module is applied across multiple deep-network layers to retrieve information from neighboring frames, including local details and high-level semantic information. The proposed framework is trained and tested on the ViSha dataset, and experimental results demonstrate that it outperforms the state-of-the-art video shadow detection method by 28%, reducing BER from 16.7 to 12.0.",1
"The dominant paradigm in spatiotemporal action detection is to classify actions using spatiotemporal features learned by 2D or 3D Convolutional Networks. We argue that several actions are characterized by their context, such as relevant objects and actors present in the video. To this end, we introduce an architecture based on self-attention and Graph Convolutional Networks in order to model contextual cues, such as actor-actor and actor-object interactions, to improve human action detection in video. We are interested in achieving this in a weakly-supervised setting, i.e. using as less annotations as possible in terms of action bounding boxes. Our model aids explainability by visualizing the learned context as an attention map, even for actions and objects unseen during training. We evaluate how well our model highlights the relevant context by introducing a quantitative metric based on recall of objects retrieved by attention maps. Our model relies on a 3D convolutional RGB stream, and does not require expensive optical flow computation. We evaluate our models on the DALY dataset, which consists of human-object interaction actions. Experimental results show that our contextualized approach outperforms a baseline action detection approach by more than 2 points in Video-mAP. Code is available at \url{https://github.com/micts/acgcn}",0
"In spatiotemporal action detection, the current approach is to use spatiotemporal features learned by 2D or 3D Convolutional Networks to classify actions. However, we believe that the context of certain actions, including relevant objects and actors in the video, should also be taken into account. To address this, we propose a new architecture that utilizes self-attention and Graph Convolutional Networks to model contextual cues, such as actor-actor and actor-object interactions, with the goal of improving human action detection in video. Our focus is on achieving this in a weakly-supervised setting, which means using as few annotations as possible for action bounding boxes. Our model provides explainability by generating an attention map that visualizes the learned context, even for actions and objects that were not present during training. We evaluate the effectiveness of our model using a quantitative metric based on recall of objects retrieved by attention maps. Our approach relies on a 3D convolutional RGB stream and does not require expensive optical flow computation. We test our model on the DALY dataset, which contains human-object interaction actions, and demonstrate that our contextualized approach outperforms a baseline action detection method by more than 2 points in Video-mAP. Code for our model is available at \url{https://github.com/micts/acgcn}.",1
"We present a learning-based approach for removing unwanted obstructions, such as window reflections, fence occlusions, or adherent raindrops, from a short sequence of images captured by a moving camera. Our method leverages motion differences between the background and obstructing elements to recover both layers. Specifically, we alternate between estimating dense optical flow fields of the two layers and reconstructing each layer from the flow-warped images via a deep convolutional neural network. This learning-based layer reconstruction module facilitates accommodating potential errors in the flow estimation and brittle assumptions, such as brightness consistency. We show that the proposed approach learned from synthetically generated data performs well to real images. Experimental results on numerous challenging scenarios of reflection and fence removal demonstrate the effectiveness of the proposed method.",0
"A technique for eliminating unwelcome obstructions from a brief series of images obtained by a mobile camera is introduced. Such obstructions may include window reflections, fence blockages, or raindrops that have adhered to the lens. Our approach relies on the disparities in motion between the background and the obstructing elements to restore both layers. We accomplish this by alternating between computing dense optical flow fields of the two layers and reconstructing each layer from flow-warped images using a deep convolutional neural network. This learning-based module for layer reconstruction helps to account for potential inaccuracies in flow estimation and fragile assumptions, such as brightness consistency. We demonstrate that the proposed strategy, which has been educated on artificially produced data, performs well on actual images. The efficacy of the proposed method is demonstrated by experimental findings on a variety of challenging reflection and fence removal situations.",1
"Highly complex deep learning models are increasingly integrated into modern cyber-physical systems (CPS), many of which have strict safety requirements. One problem arising from this is that deep learning lacks interpretability, operating as a black box. The reliability of deep learning is heavily impacted by how well the model training data represents runtime test data, especially when the input space dimension is high as natural images. In response, we propose a robust out-of-distribution (OOD) detection framework. Our approach detects unusual movements from driving video in real-time by combining classical optic flow operation with representation learning via variational autoencoder (VAE). We also design a method to locate OOD factors in images. Evaluation on a driving simulation data set shows that our approach is statistically more robust than related works.",0
"Modern cyber-physical systems (CPS) increasingly incorporate highly complex deep learning models, which can pose a challenge due to their lack of interpretability and strict safety requirements. The performance of such models heavily depends on how well the training data represents runtime test data, especially in high-dimensional input spaces like natural images. Therefore, we propose an out-of-distribution (OOD) detection framework that utilizes classical optic flow operation and representation learning via variational autoencoder (VAE) to detect unusual movements in driving video in real-time. We also introduce a method for locating OOD factors in images. Our evaluation on a driving simulation data set shows that our approach is statistically more robust than related works.",1
"Feature pyramids and iterative refinement have recently led to great progress in optical flow estimation. However, downsampling in feature pyramids can cause blending of foreground objects with the background, which will mislead subsequent decisions in the iterative processing. The results are missing details especially in the flow of thin and of small structures. We propose a novel Residual Feature Pyramid Module (RFPM) which retains important details in the feature map without changing the overall iterative refinement design of the optical flow estimation. RFPM incorporates a residual structure between multiple feature pyramids into a downsampling module that corrects the blending of objects across boundaries. We demonstrate how to integrate our module with two state-of-the-art iterative refinement architectures. Results show that our RFPM visibly reduces flow errors and improves state-of-art performance in the clean pass of Sintel, and is one of the top-performing methods in KITTI. According to the particular modular structure of RFPM, we introduce a special transfer learning approach that can dramatically decrease the training time compared to a typical full optical flow training schedule on multiple datasets.",0
"Recent advancements in optical flow estimation have been made possible by feature pyramids and iterative refinement. However, downsampling in feature pyramids can lead to the blending of foreground objects with the background, causing subsequent decisions in iterative processing to be misleading. This results in missing details, particularly in the flow of small and thin structures. To address this issue, we propose a Residual Feature Pyramid Module (RFPM) that retains critical details in the feature map while maintaining the overall iterative refinement design of optical flow estimation. RFPM introduces a residual structure between multiple feature pyramids within a downsampling module, which corrects object blending across boundaries. We demonstrate the integration of our module with two state-of-the-art iterative refinement architectures, showing visible reduction in flow errors and improved performance in the clean pass of Sintel, with RFPM being one of the top-performing methods in KITTI. Our particular modular structure also allows for a special transfer learning approach that significantly decreases training time compared to a full optical flow training schedule on multiple datasets.",1
"Learning deformable 3D objects from 2D images is an extremely ill-posed problem. Existing methods rely on explicit supervision to establish multi-view correspondences, such as template shape models and keypoint annotations, which restricts their applicability on objects ""in the wild"". In this paper, we propose to use monocular videos, which naturally provide correspondences across time, allowing us to learn 3D shapes of deformable object categories without explicit keypoints or template shapes. Specifically, we present DOVE, which learns to predict 3D canonical shape, deformation, viewpoint and texture from a single 2D image of a bird, given a bird video collection as well as automatically obtained silhouettes and optical flows as training data. Our method reconstructs temporally consistent 3D shape and deformation, which allows us to animate and re-render the bird from arbitrary viewpoints from a single image.",0
"The task of learning deformable 3D objects from 2D images is problematic due to its ill-posed nature. Current approaches rely on explicit supervision, such as template shape models and keypoint annotations, to establish multi-view correspondences, which limits their practicality for objects found in unpredictable environments. To address this, we suggest using monocular videos that provide natural correspondences across time, which enables us to learn the 3D shapes of deformable object categories without explicit keypoints or template shapes. To demonstrate this, we introduce DOVE, which predicts the 3D canonical shape, deformation, viewpoint, and texture of a bird from a single 2D image using a bird video collection and automatically obtained silhouettes and optical flows as training data. Our approach generates temporally consistent 3D shape and deformation, allowing us to animate and re-render the bird from any viewpoint using just one image.",1
"Deep Learning-based 2D/3D registration methods are highly robust but often lack the necessary registration accuracy for clinical application. A refinement step using the classical optimization-based 2D/3D registration method applied in combination with Deep Learning-based techniques can provide the required accuracy. However, it also increases the runtime. In this work, we propose a novel Deep Learning driven 2D/3D registration framework that can be used end-to-end for iterative registration tasks without relying on any further refinement step. We accomplish this by learning the update step of the 2D/3D registration framework using Point-to-Plane Correspondences. The update step is learned using iterative residual refinement-based optical flow estimation, in combination with the Point-to-Plane correspondence solver embedded as a known operator. Our proposed method achieves an average runtime of around 8s, a mean re-projection distance error of 0.60 $\pm$ 0.40 mm with a success ratio of 97 percent and a capture range of 60 mm. The combination of high registration accuracy, high robustness, and fast runtime makes our solution ideal for clinical applications.",0
"Although Deep Learning-based 2D/3D registration methods are generally robust, they tend to lack the registration accuracy required for clinical purposes. To address this limitation, a refinement step involving classical optimization-based 2D/3D registration methods is often added, but this can result in increased runtime. This study proposes a novel Deep Learning-driven 2D/3D registration framework that eliminates the need for further refinement steps. The proposed framework uses Point-to-Plane Correspondences to learn the update step of the 2D/3D registration process. This is achieved by combining iterative residual refinement-based optical flow estimation with the Point-to-Plane correspondence solver embedded as a known operator. The proposed method delivers an average runtime of approximately 8 seconds, a mean re-projection distance error of 0.60 $\pm$ 0.40 mm, a success ratio of 97 percent, and a capture range of 60 mm. The combination of high registration accuracy, robustness, and fast runtime makes the proposed solution suitable for clinical applications.",1
"Research on group activity recognition mostly leans on the standard two-stream approach (RGB and Optical Flow) as their input features. Few have explored explicit pose information, with none using it directly to reason about the persons interactions. In this paper, we leverage the skeleton information to learn the interactions between the individuals straight from it. With our proposed method GIRN, multiple relationship types are inferred from independent modules, that describe the relations between the body joints pair-by-pair. Additionally to the joints relations, we also experiment with the previously unexplored relationship between individuals and relevant objects (e.g. volleyball). The individuals distinct relations are then merged through an attention mechanism, that gives more importance to those individuals more relevant for distinguishing the group activity. We evaluate our method in the Volleyball dataset, obtaining competitive results to the state-of-the-art. Our experiments demonstrate the potential of skeleton-based approaches for modeling multi-person interactions.",0
"The majority of research on recognizing group activity relies on the standard two-stream approach, which employs RGB and Optical Flow as input features. While a few studies have examined explicit pose information, none have used it directly to understand interactions between people. This paper proposes a method called GIRN, which uses skeleton information to learn about the interactions between individuals. GIRN utilizes multiple modules to infer different types of relationships between body joints, and also examines the relationship between individuals and relevant objects, such as a volleyball. An attention mechanism is used to merge the distinct relationships between individuals, giving more weight to those that are most relevant for distinguishing the group activity. The proposed approach is evaluated using the Volleyball dataset, and achieves competitive results compared to the state-of-the-art. These experiments demonstrate that skeleton-based approaches have great potential for modeling interactions between multiple individuals.",1
"This presentation introduces a self-supervised learning approach to the synthesis of new video clips from old ones, with several new key elements for improved spatial resolution and realism: It conditions the synthesis process on contextual information for temporal continuity and ancillary information for fine control. The prediction model is doubly autoregressive, in the latent space of an autoencoder for forecasting, and in image space for updating contextual information, which is also used to enforce spatio-temporal consistency through a learnable optical flow module. Adversarial training of the autoencoder in the appearance and temporal domains is used to further improve the realism of its output. A quantizer inserted between the encoder and the transformer in charge of forecasting future frames in latent space (and its inverse inserted between the transformer and the decoder) adds even more flexibility by affording simple mechanisms for handling multimodal ancillary information for controlling the synthesis process (eg, a few sample frames, an audio track, a trajectory in image space) and taking into account the intrinsically uncertain nature of the future by allowing multiple predictions. Experiments with an implementation of the proposed approach give very good qualitative and quantitative results on multiple tasks and standard benchmarks.",0
"In this presentation, a novel approach to self-supervised learning is introduced for generating new video clips from old ones. This approach includes several new features to enhance spatial resolution and realism. By incorporating contextual information for temporal continuity and ancillary information for fine control, the synthesis process is conditioned. The prediction model is doubly autoregressive, operating in the latent space of an autoencoder for forecasting and in image space for updating contextual information. Additionally, a learnable optical flow module is used to enforce spatio-temporal consistency. Adversarial training is employed to further improve the realism of the autoencoder's output in both appearance and temporal domains. Inserting a quantizer between the encoder and transformer and its inverse between the transformer and decoder adds even more flexibility. This allows for simple handling of multimodal ancillary information to control the synthesis process and account for the uncertain nature of the future. The proposed approach is experimentally evaluated on multiple tasks and standard benchmarks, yielding excellent qualitative and quantitative results.",1
"To solve the issue of video dehazing, there are two main tasks to attain: how to align adjacent frames to the reference frame; how to restore the reference frame. Some papers adopt explicit approaches (e.g., the Markov random field, optical flow, deformable convolution, 3D convolution) to align neighboring frames with the reference frame in feature space or image space, they then use various restoration methods to achieve the final dehazing results. In this paper, we propose a progressive alignment and restoration method for video dehazing. The alignment process aligns consecutive neighboring frames stage by stage without using the optical flow estimation. The restoration process is not only implemented under the alignment process but also uses a refinement network to improve the dehazing performance of the whole network. The proposed networks include four fusion networks and one refinement network. To decrease the parameters of networks, three fusion networks in the first fusion stage share the same parameters. Extensive experiments demonstrate that the proposed video dehazing method achieves outstanding performance against the-state-of-art methods.",0
"There are two primary objectives to address when resolving the problem of video dehazing: aligning adjacent frames to a reference frame and restoring the reference frame. Certain studies use explicit techniques, such as Markov random field, optical flow, deformable convolution, and 3D convolution, to align neighboring frames with the reference frame in either feature space or image space. They then apply various restoration methods to produce the final dehazing outcomes. In this study, we introduce a gradual alignment and restoration method for video dehazing. The alignment process stages the alignment of consecutive neighboring frames without relying on optical flow estimation. Furthermore, the restoration process not only takes place under the alignment process but also employs a refinement network to boost the dehazing performance of the entire network. Our proposed networks consist of four fusion networks and one refinement network. To reduce network parameters, the first fusion stage's three fusion networks share the same parameters. Through extensive experimentation, we demonstrate that our proposed video dehazing method surpasses current state-of-the-art approaches.",1
"We propose a method for multi-object tracking and segmentation (MOTS) that does not require fine-tuning or per benchmark hyperparameter selection. The proposed method addresses particularly the data association problem. Indeed, the recently introduced HOTA metric, that has a better alignment with the human visual assessment by evenly balancing detections and associations quality, has shown that improvements are still needed for data association. After creating tracklets using instance segmentation and optical flow, the proposed method relies on a space-time memory network (STM) developed for one-shot video object segmentation to improve the association of tracklets with temporal gaps. To the best of our knowledge, our method, named MeNToS, is the first to use the STM network to track object masks for MOTS. We took the 4th place in the RobMOTS challenge. The project page is https://mehdimiah.com/mentos.html.",0
"Our proposed method for multi-object tracking and segmentation (MOTS) eliminates the need for fine-tuning or selecting hyperparameters per benchmark. Our approach specifically targets the data association issue, which is still a challenge despite the introduction of the HOTA metric that attempts to balance the quality of detections and associations. To tackle this problem, we use a space-time memory network (STM) developed for one-shot video object segmentation that improves the association of tracklets with temporal gaps. After creating tracklets using instance segmentation and optical flow, our method, called MeNToS, utilizes the STM network to track object masks for MOTS. To our knowledge, this is the first time that the STM network has been used for this purpose. Our performance in the RobMOTS challenge was fourth place, and more information about our project can be found at https://mehdimiah.com/mentos.html.",1
"We address the problem of text-guided video temporal grounding, which aims to identify the time interval of certain event based on a natural language description. Different from most existing methods that only consider RGB images as visual features, we propose a multi-modal framework to extract complementary information from videos. Specifically, we adopt RGB images for appearance, optical flow for motion, and depth maps for image structure. While RGB images provide abundant visual cues of certain event, the performance may be affected by background clutters. Therefore, we use optical flow to focus on large motion and depth maps to infer the scene configuration when the action is related to objects recognizable with their shapes. To integrate the three modalities more effectively and enable inter-modal learning, we design a dynamic fusion scheme with transformers to model the interactions between modalities. Furthermore, we apply intra-modal self-supervised learning to enhance feature representations across videos for each modality, which also facilitates multi-modal learning. We conduct extensive experiments on the Charades-STA and ActivityNet Captions datasets, and show that the proposed method performs favorably against state-of-the-art approaches.",0
"Our focus is on identifying the time interval of an event based on natural language descriptions, which is known as text-guided video temporal grounding. Most current methods only consider RGB images as visual features, however, we present a multi-modal framework that extracts complementary information from videos. Our approach involves using RGB images for appearance, optical flow for motion, and depth maps for image structure. Although RGB images offer significant visual cues, they can be affected by background clutters. To address this, we use optical flow to concentrate on large motion and depth maps to deduce the scene configuration when the action relates to objects recognizable by their shapes. Our dynamic fusion scheme with transformers enables inter-modal learning by modeling the interactions between modalities. Furthermore, we apply intra-modal self-supervised learning to enhance feature representations across videos for each modality, which also facilitates multi-modal learning. We evaluate our approach on the Charades-STA and ActivityNet Captions datasets and demonstrate that our method outperforms state-of-the-art approaches.",1
"Dense optical flow estimation is challenging when there are large displacements in a scene with heterogeneous motion dynamics, occlusion, and scene homogeneity. Traditional approaches to handle these challenges include hierarchical and multiresolution processing methods. Learning-based optical flow methods typically use a multiresolution approach with image warping when a broad range of flow velocities and heterogeneous motion is present. Accuracy of such coarse-to-fine methods is affected by the ghosting artifacts when images are warped across multiple resolutions and by the vanishing problem in smaller scene extents with higher motion contrast. Previously, we devised strategies for building compact dense prediction networks guided by the effective receptive field (ERF) characteristics of the network (DDCNet). The DDCNet design was intentionally simple and compact allowing it to be used as a building block for designing more complex yet compact networks. In this work, we extend the DDCNet strategies to handle heterogeneous motion dynamics by cascading DDCNet based sub-nets with decreasing extents of their ERF. Our DDCNet with multiresolution capability (DDCNet-Multires) is compact without any specialized network layers. We evaluate the performance of the DDCNet-Multires network using standard optical flow benchmark datasets. Our experiments demonstrate that DDCNet-Multires improves over the DDCNet-B0 and -B1 and provides optical flow estimates with accuracy comparable to similar lightweight learning-based methods.",0
"When a scene has varying motion dynamics, occlusion, and homogeneity, estimating dense optical flow can be difficult. Traditional methods use hierarchical and multiresolution processing to address these challenges. Learning-based approaches typically use multiresolution methods with image warping to account for different flow velocities and motion patterns. However, these methods can result in ghosting artifacts and vanishing problems, affecting their accuracy. Previously, we developed the DDCNet, a simple and compact network for dense prediction. In this study, we extend the DDCNet to handle heterogeneous motion by cascading sub-nets with decreasing effective receptive field (ERF) sizes. This new DDCNet-Multires network is compact, without the need for specialized layers, and achieves a performance comparable to other lightweight learning-based methods on standard optical flow benchmarks.",1
"This technical report presents our solution to the HACS Temporal Action Localization Challenge 2021, Weakly-Supervised Learning Track. The goal of weakly-supervised temporal action localization is to temporally locate and classify action of interest in untrimmed videos given only video-level labels. We adopt the two-stream consensus network (TSCN) as the main framework in this challenge. The TSCN consists of a two-stream base model training procedure and a pseudo ground truth learning procedure. The base model training encourages the model to predict reliable predictions based on single modality (i.e., RGB or optical flow), based on the fusion of which a pseudo ground truth is generated and in turn used as supervision to train the base models. On the HACS v1.1.1 dataset, without fine-tuning the feature-extraction I3D models, our method achieves 22.20% on the validation set and 21.68% on the testing set in terms of average mAP. Our solution ranked the 2rd in this challenge, and we hope our method can serve as a baseline for future academic research.",0
"Our technical report details our approach to the HACS Temporal Action Localization Challenge 2021, specifically the Weakly-Supervised Learning Track. The objective of this challenge is to locate and classify actions of interest in untrimmed videos using only video-level labels. To achieve this, we utilized the two-stream consensus network (TSCN) as our primary framework. The TSCN comprises a two-stream base model training and a pseudo ground truth learning procedure. The base model training encourages the model to make accurate predictions based on a single modality, and a pseudo ground truth is generated from the fusion of these predictions to train the base models. Our method achieved an average mAP of 22.20% on the validation set and 21.68% on the testing set of the HACS v1.1.1 dataset without fine-tuning the feature-extraction I3D models. Our ranking in this challenge was 2nd, and we hope our approach can serve as a foundation for future academic studies.",1
"Detection of moving objects is a very important task in autonomous driving systems. After the perception phase, motion planning is typically performed in Bird's Eye View (BEV) space. This would require projection of objects detected on the image plane to top view BEV plane. Such a projection is prone to errors due to lack of depth information and noisy mapping in far away areas. CNNs can leverage the global context in the scene to project better. In this work, we explore end-to-end Moving Object Detection (MOD) on the BEV map directly using monocular images as input. To the best of our knowledge, such a dataset does not exist and we create an extended KITTI-raw dataset consisting of 12.9k images with annotations of moving object masks in BEV space for five classes. The dataset is intended to be used for class agnostic motion cue based object detection and classes are provided as meta-data for better tuning. We design and implement a two-stream RGB and optical flow fusion architecture which outputs motion segmentation directly in BEV space. We compare it with inverse perspective mapping of state-of-the-art motion segmentation predictions on the image plane. We observe a significant improvement of 13% in mIoU using the simple baseline implementation. This demonstrates the ability to directly learn motion segmentation output in BEV space. Qualitative results of our baseline and the dataset annotations can be found in https://sites.google.com/view/bev-modnet.",0
"The ability to detect moving objects is crucial for autonomous driving systems. Typically, motion planning is performed in Bird's Eye View (BEV) space after the perception phase. However, projecting objects detected on the image plane to the top view BEV plane can lead to errors due to lack of depth information and noisy mapping in distant areas. CNNs can improve this by utilizing the global context in the scene. This study explores end-to-end Moving Object Detection (MOD) on the BEV map using monocular images as input. The authors create an extended KITTI-raw dataset with annotations of moving object masks in BEV space for five classes. The dataset is intended for class agnostic motion cue based object detection and provides classes as meta-data for better tuning. The authors develop a two-stream RGB and optical flow fusion architecture that outputs motion segmentation directly in BEV space. They compare it with inverse perspective mapping of state-of-the-art motion segmentation predictions on the image plane and observe a significant improvement of 13% in mIoU using the simple baseline implementation. This demonstrates the ability to directly learn motion segmentation output in BEV space. The baseline and dataset annotations can be found in https://sites.google.com/view/bev-modnet.",1
"Dense pixel matching problems such as optical flow and disparity estimation are among the most challenging tasks in computer vision. Recently, several deep learning methods designed for these problems have been successful. A sufficiently larger effective receptive field (ERF) and a higher resolution of spatial features within a network are essential for providing higher-resolution dense estimates. In this work, we present a systemic approach to design network architectures that can provide a larger receptive field while maintaining a higher spatial feature resolution. To achieve a larger ERF, we utilized dilated convolutional layers. By aggressively increasing dilation rates in the deeper layers, we were able to achieve a sufficiently larger ERF with a significantly fewer number of trainable parameters. We used optical flow estimation problem as the primary benchmark to illustrate our network design strategy. The benchmark results (Sintel, KITTI, and Middlebury) indicate that our compact networks can achieve comparable performance in the class of lightweight networks.",0
"Computer vision faces significant challenges with dense pixel matching tasks, such as optical flow and disparity estimation. However, recent deep learning methods have shown success in addressing these challenges. To provide higher-resolution dense estimates, a network must have a larger effective receptive field (ERF) and maintain a higher spatial feature resolution. Our study presents a comprehensive approach to designing network architectures that meet these requirements. Our approach utilizes dilated convolutional layers to achieve a larger ERF by aggressively increasing dilation rates in deeper layers. This allows us to achieve a larger ERF with significantly fewer trainable parameters. We demonstrate the effectiveness of our network design strategy using optical flow estimation as the primary benchmark. Our benchmark results (Sintel, KITTI, and Middlebury) show that our compact networks can achieve comparable performance in the lightweight networks class.",1
"State-of-the-art temporal action detectors to date are based on two-stream input including RGB frames and optical flow. Although combining RGB frames and optical flow boosts performance significantly, optical flow is a hand-designed representation which not only requires heavy computation, but also makes it methodologically unsatisfactory that two-stream methods are often not learned end-to-end jointly with the flow. In this paper, we argue that optical flow is dispensable in high-accuracy temporal action detection and image level data augmentation (ILDA) is the key solution to avoid performance degradation when optical flow is removed. To evaluate the effectiveness of ILDA, we design a simple yet efficient one-stage temporal action detector based on single RGB stream named DaoTAD. Our results show that when trained with ILDA, DaoTAD has comparable accuracy with all existing state-of-the-art two-stream detectors while surpassing the inference speed of previous methods by a large margin and the inference speed is astounding 6668 fps on GeForce GTX 1080 Ti. Code is available at \url{https://github.com/Media-Smart/vedatad}.",0
"The current advanced temporal action detectors rely on two-stream input, which includes RGB frames and optical flow. While the combination of these two inputs significantly enhances performance, optical flow is a representation that is manually designed, requires significant computation, and does not enable two-stream methods to be learned end-to-end jointly with the flow, making it methodologically unsatisfactory. This paper contends that high-accuracy temporal action detection does not require optical flow, and instead advocates for image level data augmentation (ILDA) as a key solution to prevent performance degradation when optical flow is removed. To test the effectiveness of ILDA, the authors designed a simple yet effective one-stage temporal action detector based on a single RGB stream, called DaoTAD. The results demonstrate that when trained with ILDA, DaoTAD is as accurate as all existing state-of-the-art two-stream detectors, while significantly outperforming previous methods in terms of inference speed, with an astounding 6668 fps on GeForce GTX 1080 Ti. The code is available at \url{https://github.com/Media-Smart/vedatad}.",1
"Optical flow estimation is a fundamental problem of computer vision and has many applications in the fields of robot learning and autonomous driving. This paper reveals novel geometric laws of optical flow based on the insight and detailed definition of non-occlusion. Then, two novel loss functions are proposed for the unsupervised learning of optical flow based on the geometric laws of non-occlusion. Specifically, after the occlusion part of the images are masked, the flowing process of pixels is carefully considered and geometric constraints are conducted based on the geometric laws of optical flow. First, neighboring pixels in the first frame will not intersect during the pixel displacement to the second frame. Secondly, when the cluster containing adjacent four pixels in the first frame moves to the second frame, no other pixels will flow into the quadrilateral formed by them. According to the two geometrical constraints, the optical flow non-intersection loss and the optical flow non-blocking loss in the non-occlusion regions are proposed. Two loss functions punish the irregular and inexact optical flows in the non-occlusion regions. The experiments on datasets demonstrated that the proposed unsupervised losses of optical flow based on the geometric laws in non-occlusion regions make the estimated optical flow more refined in detail, and improve the performance of unsupervised learning of optical flow. In addition, the experiments training on synthetic data and evaluating on real data show that the generalization ability of optical flow network is improved by our proposed unsupervised approach.",0
"The estimation of optical flow is a crucial task in computer vision, with various applications in fields such as autonomous driving and robot learning. This study presents original geometric principles of optical flow, derived from a comprehensive definition of non-occlusion. The research then proposes two innovative loss functions for unsupervised optical flow learning, based on the geometric laws of non-occlusion. The study carefully considers the flow of pixels after masking the occlusion part of the images and applies geometric constraints based on the laws of optical flow. The first constraint ensures that neighboring pixels in the first frame do not intersect when they move to the second frame. The second constraint states that no other pixels will flow into the quadrilateral formed by the cluster containing adjacent four pixels in the first frame when it moves to the second frame. The proposed optical flow non-intersection loss and optical flow non-blocking loss punish irregular and inexact optical flows in the non-occlusion regions. The experiments on various datasets illustrate that the proposed unsupervised losses of optical flow based on geometric laws in non-occlusion regions refine the estimated optical flow and enhance the performance of unsupervised learning of optical flow. Moreover, the experiments training on synthetic data and evaluating on real data demonstrate that the proposed unsupervised approach improves the generalization ability of the optical flow network.",1
"Different types of spectroscopies, such as X-ray absorption near edge structure (XANES) and Raman spectroscopy, play a very important role in analyzing the characteristics of different materials. In scientific literature, XANES/Raman data are usually plotted in line graphs which is a visually appropriate way to represent the information when the end-user is a human reader. However, such graphs are not conducive to direct programmatic analysis due to the lack of automatic tools. In this paper, we develop a plot digitizer, named Plot2Spectra, to extract data points from spectroscopy graph images in an automatic fashion, which makes it possible for large scale data acquisition and analysis. Specifically, the plot digitizer is a two-stage framework. In the first axis alignment stage, we adopt an anchor-free detector to detect the plot region and then refine the detected bounding boxes with an edge-based constraint to locate the position of two axes. We also apply scene text detector to extract and interpret all tick information below the x-axis. In the second plot data extraction stage, we first employ semantic segmentation to separate pixels belonging to plot lines from the background, and from there, incorporate optical flow constraints to the plot line pixels to assign them to the appropriate line (data instance) they encode. Extensive experiments are conducted to validate the effectiveness of the proposed plot digitizer, which shows that such a tool could help accelerate the discovery and machine learning of materials properties.",0
"Various spectroscopic techniques, including X-ray absorption near edge structure (XANES) and Raman spectroscopy, are crucial in studying the properties of diverse materials. While line graphs are commonly used to represent XANES/Raman data in scientific literature for human readers, they are not ideal for automated analysis. Therefore, we present Plot2Spectra, a plot digitizer that automatically extracts data from spectroscopy graphs, enabling large-scale data acquisition and analysis. The two-stage framework involves axis alignment, where an anchor-free detector and edge-based constraint locate axis positions, and plot data extraction, where semantic segmentation and optical flow constraints assign pixels to corresponding data instances. Extensive experiments confirm the success of Plot2Spectra, which can facilitate the discovery and machine learning of materials properties.",1
"This article presents a novel approach to incorporate visual cues from video-data from a wide-angle stereo camera system mounted at an urban intersection into the forecast of cyclist trajectories. We extract features from image and optical flow (OF) sequences using 3D convolutional neural networks (3D-ConvNet) and combine them with features extracted from the cyclist's past trajectory to forecast future cyclist positions. By the use of additional information, we are able to improve positional accuracy by about 7.5 % for our test dataset and by up to 22 % for specific motion types compared to a method solely based on past trajectories. Furthermore, we compare the use of image sequences to the use of OF sequences as additional information, showing that OF alone leads to significant improvements in positional accuracy. By training and testing our methods using a real-world dataset recorded at a heavily frequented public intersection and evaluating the methods' runtimes, we demonstrate the applicability in real traffic scenarios. Our code and parts of our dataset are made publicly available.",0
"In this article, a new method is presented for predicting the trajectories of cyclists using visual cues obtained from video data captured by a wide-angle stereo camera system installed at an urban intersection. The approach involves extracting features from image and optical flow sequences using 3D convolutional neural networks (3D-ConvNet) and combining them with the cyclist's past trajectory to forecast future positions. By incorporating additional information, the accuracy of the predictions is improved by approximately 7.5% for the test dataset and up to 22% for specific motion types compared to a method that relies solely on past trajectories. The study also compares the effectiveness of using image sequences versus optical flow sequences as additional information, finding that optical flow alone leads to significant improvements in positional accuracy. The method is evaluated using a real-world dataset recorded at a heavily frequented public intersection, and the runtimes are shown to be applicable for use in real traffic scenarios. The code and some of the dataset are available to the public.",1
"Video frame interpolation is the task of creating an interframe between two adjacent frames along the time axis. So, instead of simply averaging two adjacent frames to create an intermediate image, this operation should maintain semantic continuity with the adjacent frames. Most conventional methods use optical flow, and various tools such as occlusion handling and object smoothing are indispensable. Since the use of these various tools leads to complex problems, we tried to tackle the video interframe generation problem without using problematic optical flow . To enable this , we have tried to use a deep neural network with an invertible structure, and developed an U-Net based Generative Flow which is a modified normalizing flow. In addition, we propose a learning method with a new consistency loss in the latent space to maintain semantic temporal consistency between frames. The resolution of the generated image is guaranteed to be identical to that of the original images by using an invertible network. Furthermore, as it is not a random image like the ones by generative models, our network guarantees stable outputs without flicker. Through experiments, we \sam {confirmed the feasibility of the proposed algorithm and would like to suggest the U-Net based Generative Flow as a new possibility for baseline in video frame interpolation. This paper is meaningful in that it is the world's first attempt to use invertible networks instead of optical flows for video interpolation.",0
"The task of video frame interpolation involves creating an intermediate frame between two adjacent frames along the time axis while maintaining semantic continuity with the surrounding frames. Although conventional methods use optical flow and various tools such as occlusion handling and object smoothing, these lead to complex problems. To address this issue, we developed a new approach using a deep neural network with an invertible structure called U-Net based Generative Flow. We also proposed a learning method with a new consistency loss in the latent space to ensure semantic temporal consistency between frames. The invertible network guarantees that the resolution of the generated image is identical to that of the original images, and stable outputs without flicker are ensured. Our experiments have shown that our proposed algorithm is feasible and we suggest it as a new baseline for video frame interpolation. This paper is significant as it is the first attempt in the world to use invertible networks instead of optical flows for video interpolation.",1
"Multiple human tracking is a fundamental problem for scene understanding. Although both accuracy and speed are required in real-world applications, recent tracking methods based on deep learning have focused on accuracy and require substantial running time. This study aims to improve running speed by performing human detection at a certain frame interval because it accounts for most of the running time. The question is how to maintain accuracy while skipping human detection. In this paper, we propose a method that complements the detection results with optical flow, based on the fact that someone's appearance does not change much between adjacent frames. To maintain the tracking accuracy, we introduce robust interest point selection within human regions and a tracking termination metric calculated by the distribution of the interest points. On the MOT20 dataset in the MOTChallenge, the proposed SDOF-Tracker achieved the best performance in terms of the total running speed while maintaining the MOTA metric. Our code is available at https://anonymous.4open.science/r/sdof-tracker-75AE.",0
"The problem of tracking multiple humans is crucial for understanding scenes. However, current deep learning tracking methods prioritize accuracy over speed, which is essential for real-world applications. To address this issue, this study proposes a method that performs human detection at specific frame intervals to improve running speed. The challenge is to maintain accuracy while skipping human detection. To overcome this challenge, the proposed approach combines detection results with optical flow, which is based on the consistency of a person's appearance in adjacent frames. Additionally, robust interest point selection within human regions and a tracking termination metric calculated by the distribution of interest points are introduced to maintain tracking accuracy. The SDOF-Tracker achieved the best performance in terms of running speed and the MOTA metric on the MOT20 dataset in the MOTChallenge. The code for this approach is available at https://anonymous.4open.science/r/sdof-tracker-75AE.",1
"Moving target detection plays an important role in computer vision. However, traditional algorithms such as frame difference and optical flow usually suffer from low accuracy or heavy computation. Recent algorithms such as deep learning-based convolutional neural networks have achieved high accuracy and real-time performance, but they usually need to know the classes of targets in advance, which limits the practical applications. Therefore, we proposed a model free moving target detection algorithm. This algorithm extracts the moving area through the difference of image features. Then, the color and location probability map of the moving area will be calculated through maximum a posteriori probability. And the target probability map can be obtained through the dot multiply between the two maps. Finally, the optimal moving target area can be solved by stochastic gradient descent on the target probability map. Results show that the proposed algorithm achieves the highest accuracy compared with state-of-the-art algorithms, without needing to know the classes of targets. Furthermore, as the existing datasets are not suitable for moving target detection, we proposed a method for producing evaluation dataset. Besides, we also proved the proposed algorithm can be used to assist target tracking.",0
"Computer vision heavily relies on detecting moving targets. However, conventional methods like frame difference and optical flow often yield low accuracy or require extensive computation. While recent deep learning-based convolutional neural networks have successfully achieved high accuracy and real-time performance, they are limited by their requirement for prior knowledge of target classes, thus limiting practical applications. Therefore, we have presented a model-free moving target detection algorithm that utilizes image feature differences to extract moving areas. Subsequently, a color and location probability map is calculated via maximum a posteriori probability. By multiplying the two maps, the target probability map is obtained. Finally, stochastic gradient descent is employed to derive the optimal moving target area from the target probability map. The proposed algorithm outperforms state-of-the-art algorithms without requiring target class identification. Additionally, we have developed an evaluation dataset and demonstrated that the proposed algorithm is useful in target tracking.",1
"Moving Object Detection (MOD) is a crucial task for the Autonomous Driving pipeline. MOD is usually handled via 2-stream convolutional architectures that incorporates both appearance and motion cues, without considering the inter-relations between the spatial or motion features. In this paper, we tackle this problem through multi-head attention mechanisms, both across the spatial and motion streams. We propose MODETR; a Moving Object DEtection TRansformer network, comprised of multi-stream transformer encoders for both spatial and motion modalities, and an object transformer decoder that produces the moving objects bounding boxes using set predictions. The whole architecture is trained end-to-end using bi-partite loss. Several methods of incorporating motion cues with the Transformer model are explored, including two-stream RGB and Optical Flow (OF) methods, and multi-stream architectures that take advantage of sequence information. To incorporate the temporal information, we propose a new Temporal Positional Encoding (TPE) approach to extend the Spatial Positional Encoding(SPE) in DETR. We explore two architectural choices for that, balancing between speed and time. To evaluate the our network, we perform the MOD task on the KITTI MOD [6] data set. Results show significant 5% mAP of the Transformer network for MOD over the state-of-the art methods. Moreover, the proposed TPE encoding provides 10% mAP improvement over the SPE baseline.",0
"Autonomous Driving pipeline relies heavily on Moving Object Detection (MOD). Typically, MOD is tackled using 2-stream convolutional architectures that incorporate appearance and motion cues, but neglect the inter-connections between spatial or motion features. In this study, we address this issue by employing multi-head attention mechanisms across both the spatial and motion streams. Our proposed solution, MODETR, is a Moving Object Detection Transformer network. It comprises multi-stream transformer encoders for both spatial and motion modalities, and an object transformer decoder that predicts moving object bounding boxes using set predictions. The entire architecture is trained end-to-end using bi-partite loss. We explore various methods of integrating motion cues with the Transformer model, including two-stream RGB and Optical Flow (OF) methods, and multi-stream architectures that leverage sequence information. To incorporate temporal information, we suggest a new Temporal Positional Encoding (TPE) approach, in addition to the Spatial Positional Encoding (SPE) in DETR. We examine two architectural options that strike a balance between speed and time. To evaluate our network, we perform the MOD task on the KITTI MOD [6] dataset. The results show a significant 5% mAP of the Transformer network for MOD over state-of-the-art techniques. Furthermore, the proposed TPE encoding results in a 10% mAP improvement over the SPE baseline.",1
"Transferring image-based object detectors to the domain of video remains challenging under resource constraints. Previous efforts utilised optical flow to allow unchanged features to be propagated, however, the overhead is considerable when working with very slowly changing scenes from applications such as surveillance. In this paper, we propose temporal early exits to reduce the computational complexity of per-frame video object detection. Multiple temporal early exit modules with low computational overhead are inserted at early layers of the backbone network to identify the semantic differences between consecutive frames. Full computation is only required if the frame is identified as having a semantic change to previous frames; otherwise, detection results from previous frames are reused. Experiments on CDnet show that our method significantly reduces the computational complexity and execution of per-frame video object detection up to $34 \times$ compared to existing methods with an acceptable reduction of 2.2\% in mAP.",0
"The task of transferring image-based object detectors to the video domain poses a challenge when faced with limited resources. Previous methods have employed optical flow to propagate unchanged features. However, this approach has proven to be impractical for slowly changing scenes, such as those seen in surveillance applications, due to the high overhead involved. To address this issue, this study proposes the use of temporal early exits to decrease the computational complexity of per-frame video object detection. This involves the insertion of multiple temporal early exit modules with low computational overhead at early layers of the backbone network. These modules identify semantic differences between consecutive frames, and full computation is only required if a frame is found to have a semantic change from previous frames. Otherwise, detection results from earlier frames are reused. Experiments conducted on CDnet demonstrate that this method significantly reduces the computational complexity and execution of per-frame video object detection by up to $34 \times$ in comparison to existing techniques, with an acceptable reduction of 2.2\% in mAP.",1
"Instance-level contrastive learning techniques, which rely on data augmentation and a contrastive loss function, have found great success in the domain of visual representation learning. They are not suitable for exploiting the rich dynamical structure of video however, as operations are done on many augmented instances. In this paper we propose ""Video Cross-Stream Prototypical Contrasting"", a novel method which predicts consistent prototype assignments from both RGB and optical flow views, operating on sets of samples. Specifically, we alternate the optimization process; while optimizing one of the streams, all views are mapped to one set of stream prototype vectors. Each of the assignments is predicted with all views except the one matching the prediction, pushing representations closer to their assigned prototypes. As a result, more efficient video embeddings with ingrained motion information are learned, without the explicit need for optical flow computation during inference. We obtain state-of-the-art results on nearest neighbour video retrieval and action recognition, outperforming previous best by +3.2% on UCF101 using the S3D backbone (90.5% Top-1 acc), and by +7.2% on UCF101 and +15.1% on HMDB51 using the R(2+1)D backbone.",0
"In the field of visual representation learning, instance-level contrastive learning techniques have been successful by utilizing data augmentation and a contrastive loss function. However, due to the fact that these operations are performed on numerous augmented instances, they are not ideal for exploiting the complex dynamic structure of video. In our research, we introduce a new method called ""Video Cross-Stream Prototypical Contrasting"", which predicts consistent prototype assignments from both RGB and optical flow views, working on sets of samples. We alternate the optimization process and map all views to one set of stream prototype vectors while optimizing one of the streams. Each of the assignments is predicted with all views except the one that corresponds to the prediction, resulting in more efficient video embeddings with incorporated motion information, eliminating the need for optical flow computation during inference. Our method outperforms previous approaches in nearest neighbour video retrieval and action recognition, achieving state-of-the-art results on UCF101 and HMDB51 using the S3D and R(2+1)D backbones.",1
"Forecasting the formation and development of clouds is a central element of modern weather forecasting systems. Incorrect clouds forecasts can lead to major uncertainty in the overall accuracy of weather forecasts due to their intrinsic role in the Earth's climate system. Few studies have tackled this challenging problem from a machine learning point-of-view due to a shortage of high-resolution datasets with many historical observations globally. In this paper, we present a novel satellite-based dataset called ``CloudCast''. It consists of 70,080 images with 10 different cloud types for multiple layers of the atmosphere annotated on a pixel level. The spatial resolution of the dataset is 928 x 1530 pixels (3x3 km per pixel) with 15-min intervals between frames for the period 2017-01-01 to 2018-12-31. All frames are centered and projected over Europe. To supplement the dataset, we conduct an evaluation study with current state-of-the-art video prediction methods such as convolutional long short-term memory networks, generative adversarial networks, and optical flow-based extrapolation methods. As the evaluation of video prediction is difficult in practice, we aim for a thorough evaluation in the spatial and temporal domain. Our benchmark models show promising results but with ample room for improvement. This is the first publicly available global-scale dataset with high-resolution cloud types on a high temporal granularity to the authors' best knowledge.",0
"Modern weather forecasting systems rely heavily on accurate predictions of cloud formation and development, as incorrect forecasts can result in significant uncertainty in overall weather predictions due to their crucial role in the Earth's climate system. However, few machine learning studies have tackled this challenging problem, primarily due to a lack of high-resolution datasets with a substantial amount of historical observations worldwide. In this paper, we introduce a new satellite-based dataset called ""CloudCast,"" which includes 70,080 images featuring ten distinct cloud types across multiple atmospheric layers annotated at the pixel level. The dataset boasts a spatial resolution of 928 x 1530 pixels (3x3 km per pixel), with frames taken at 15-minute intervals between January 1, 2017, and December 31, 2018. All frames are centered and projected over Europe. To supplement the dataset, we evaluate current state-of-the-art video prediction methods, including convolutional long short-term memory networks, generative adversarial networks, and optical flow-based extrapolation methods. Since evaluating video prediction is challenging, we aim to conduct a thorough evaluation in the spatial and temporal domains. Our benchmark models show promising results, but there is ample room for improvement. This is the first publicly available high-resolution global dataset with cloud types annotated at a high temporal granularity to our knowledge.",1
"State-of-the-art frame interpolation methods generate intermediate frames by inferring object motions in the image from consecutive key-frames. In the absence of additional information, first-order approximations, i.e. optical flow, must be used, but this choice restricts the types of motions that can be modeled, leading to errors in highly dynamic scenarios. Event cameras are novel sensors that address this limitation by providing auxiliary visual information in the blind-time between frames. They asynchronously measure per-pixel brightness changes and do this with high temporal resolution and low latency. Event-based frame interpolation methods typically adopt a synthesis-based approach, where predicted frame residuals are directly applied to the key-frames. However, while these approaches can capture non-linear motions they suffer from ghosting and perform poorly in low-texture regions with few events. Thus, synthesis-based and flow-based approaches are complementary. In this work, we introduce Time Lens, a novel indicates equal contribution method that leverages the advantages of both. We extensively evaluate our method on three synthetic and two real benchmarks where we show an up to 5.21 dB improvement in terms of PSNR over state-of-the-art frame-based and event-based methods. Finally, we release a new large-scale dataset in highly dynamic scenarios, aimed at pushing the limits of existing methods.",0
"Advanced techniques for frame interpolation involve creating intermediate frames by deducing object movements in an image from consecutive key-frames. If additional information is not available, first-order approximations, such as optical flow, must be used. However, this choice restricts the types of movements that can be modeled, leading to inaccuracies in highly dynamic scenarios. Event cameras are a new type of sensor that overcomes this limitation by providing auxiliary visual data during blind-time between frames. They record per-pixel brightness changes with high temporal resolution and low latency. Event-based frame interpolation techniques typically use a synthesis-based approach, where predicted residuals are directly applied to the key-frames. Nevertheless, these approaches can lead to ghosting and perform poorly in low-texture regions with few events, while flow-based approaches can capture nonlinear motions. As a result, synthesis-based and flow-based approaches are complementary. This study introduces Time Lens, a new approach that combines the strengths of both methods. The proposed method is extensively evaluated on three synthetic and two real benchmarks, demonstrating a maximum improvement of 5.21 dB in terms of PSNR compared to the current state-of-the-art frame-based and event-based methods. Lastly, a new large-scale dataset in highly dynamic scenarios is released to push the boundaries of existing techniques.",1
"Despite the continued successes of computationally efficient deep neural network architectures for video object detection, performance continually arrives at the great trilemma of speed versus accuracy versus computational resources (pick two). Current attempts to exploit temporal information in video data to overcome this trilemma are bottlenecked by the state-of-the-art in object detection models. We present, a technique which performs video object detection through the use of off-the-shelf object detectors alongside existing optical flow based motion estimation techniques in parallel. Through a set of experiments on the benchmark MOT20 dataset, we demonstrate that our approach significantly reduces the baseline latency of any given object detector without sacrificing any accuracy. Further latency reduction, up to 25x lower than the original latency, can be achieved with minimal accuracy loss. MOVEX enables low latency video object detection on common CPU based systems, thus allowing for high performance video object detection beyond the domain of GPU computing. The code is available at https://github.com/juliantrue/movex.",0
"Video object detection using computationally efficient deep neural network architectures often faces the challenge of balancing speed, accuracy, and computational resources. Despite attempts to leverage temporal information in video data to overcome this challenge, the current state-of-the-art in object detection models is a bottleneck. Our approach utilizes off-the-shelf object detectors and optical flow based motion estimation techniques in parallel to perform video object detection. Our experiments on the MOT20 dataset demonstrate that our technique significantly reduces the baseline latency of any given object detector without sacrificing accuracy. Additionally, minimal accuracy loss can achieve further latency reduction, up to 25 times lower than the original latency. With MOVEX, high-performance video object detection can be achieved on CPU-based systems, extending beyond the domain of GPU computing. The code is available at https://github.com/juliantrue/movex.",1
"Video super-resolution (VSR), with the aim to restore a high-resolution video from its corresponding low-resolution version, is a spatial-temporal sequence prediction problem. Recently, Transformer has been gaining popularity due to its parallel computing ability for sequence-to-sequence modeling. Thus, it seems to be straightforward to apply the vision Transformer to solve VSR. However, the typical block design of Transformer with a fully connected self-attention layer and a token-wise feed-forward layer does not fit well for VSR due to the following two reasons. First, the fully connected self-attention layer neglects to exploit the data locality because this layer relies on linear layers to compute attention maps. Second, the token-wise feed-forward layer lacks the feature alignment which is important for VSR since this layer independently processes each of the input token embeddings without any interaction among them. In this paper, we make the first attempt to adapt Transformer for VSR. Specifically, to tackle the first issue, we present a spatial-temporal convolutional self-attention layer with a theoretical understanding to exploit the locality information. For the second issue, we design a bidirectional optical flow-based feed-forward layer to discover the correlations across different video frames and also align features. Extensive experiments on several benchmark datasets demonstrate the effectiveness of our proposed method. The code will be available at https://github.com/caojiezhang/VSR-Transformer.",0
"The problem of video super-resolution (VSR), which aims to enhance the resolution of low-resolution videos, is a challenge in spatial-temporal sequence prediction. The Transformer model has gained popularity for its ability to perform parallel computing for sequence-to-sequence modeling. While it may seem like a natural fit for VSR, the typical Transformer block design with a fully connected self-attention layer and a token-wise feed-forward layer is not suitable for VSR due to two reasons. Firstly, the fully connected self-attention layer does not take into account data locality, relying solely on linear layers to compute attention maps. Secondly, the token-wise feed-forward layer lacks feature alignment, which is crucial for VSR since it processes each input token embedding independently without any interaction between them. This paper presents the first attempt to adapt the Transformer model for VSR. To address the aforementioned issues, the authors propose a spatial-temporal convolutional self-attention layer to exploit locality information and a bidirectional optical flow-based feed-forward layer to discover correlations across different video frames and align features. The proposed method is evaluated on multiple benchmark datasets, and the results show its effectiveness. The code for this research will be available on GitHub at https://github.com/caojiezhang/VSR-Transformer.",1
"Facial expressions vary from the visible to the subtle. In recent years, the analysis of micro-expressions $-$ a natural occurrence resulting from the suppression of one's true emotions, has drawn the attention of researchers with a broad range of potential applications. However, spotting microexpressions in long videos becomes increasingly challenging when intertwined with normal or macro-expressions. In this paper, we propose a shallow optical flow three-stream CNN (SOFTNet) model to predict a score that captures the likelihood of a frame being in an expression interval. By fashioning the spotting task as a regression problem, we introduce pseudo-labeling to facilitate the learning process. We demonstrate the efficacy and efficiency of the proposed approach on the recent MEGC 2020 benchmark, where state-of-the-art performance is achieved on CAS(ME)$^{2}$ with equally promising results on SAMM Long Videos.",0
"The range of facial expressions can vary greatly, from noticeable to subtle. Recently, researchers have taken an interest in analyzing micro-expressions, which occur naturally when true emotions are suppressed. This has many potential applications, but becomes more challenging when trying to identify micro-expressions in longer videos that also contain normal and macro-expressions. In this study, we introduce a shallow optical flow three-stream CNN (SOFTNet) model that predicts the likelihood of a frame being part of an expression interval. We approach this task as a regression problem and use pseudo-labeling to aid the learning process. Our results show that this approach is effective and efficient, achieving state-of-the-art performance on the CAS(ME)$^{2}$ benchmark and promising results on SAMM Long Videos.",1
"Geometric alignment appears in a variety of applications, ranging from domain adaptation, optimal transport, and normalizing flows in machine learning; optical flow and learned augmentation in computer vision and deformable registration within biomedical imaging. A recurring challenge is the alignment of domains whose topology is not the same; a problem that is routinely ignored, potentially introducing bias in downstream analysis. As a first step towards solving such alignment problems, we propose an unsupervised topological difference detection algorithm. The model is based on a conditional variational auto-encoder and detects topological anomalies with regards to a reference alongside the registration step. We consider both a) topological changes in the image under spatial variation and b) unexpected transformations. Our approach is validated on a proxy task of unsupervised anomaly detection in images.",0
"Geometric alignment is present in various fields such as machine learning, computer vision, and biomedical imaging. One of the main difficulties encountered is aligning domains with different topologies, which is often overlooked and may lead to biased analysis. To address this issue, we propose an unsupervised algorithm that detects topological differences using a conditional variational auto-encoder during the registration process. We examine both topological changes caused by spatial variation and unexpected transformations. Our approach is validated through unsupervised anomaly detection in images.",1
"We present DistillFlow, a knowledge distillation approach to learning optical flow. DistillFlow trains multiple teacher models and a student model, where challenging transformations are applied to the input of the student model to generate hallucinated occlusions as well as less confident predictions. Then, a self-supervised learning framework is constructed: confident predictions from teacher models are served as annotations to guide the student model to learn optical flow for those less confident predictions. The self-supervised learning framework enables us to effectively learn optical flow from unlabeled data, not only for non-occluded pixels, but also for occluded pixels. DistillFlow achieves state-of-the-art unsupervised learning performance on both KITTI and Sintel datasets. Our self-supervised pre-trained model also provides an excellent initialization for supervised fine-tuning, suggesting an alternate training paradigm in contrast to current supervised learning methods that highly rely on pre-training on synthetic data. At the time of writing, our fine-tuned models ranked 1st among all monocular methods on the KITTI 2015 benchmark, and outperform all published methods on the Sintel Final benchmark. More importantly, we demonstrate the generalization capability of DistillFlow in three aspects: framework generalization, correspondence generalization and cross-dataset generalization.",0
"Our approach, called DistillFlow, is a technique for learning optical flow through knowledge distillation. We utilize multiple teacher models and a student model, with challenging transformations applied to the student model's input to create hallucinated occlusions and less certain predictions. A self-supervised learning framework is established, where the teacher models' confident predictions serve as annotations to guide the student model in learning optical flow for the less certain predictions. This framework enables us to effectively learn optical flow from unlabeled data, including occluded pixels. DistillFlow sets a new benchmark for unsupervised learning on both the KITTI and Sintel datasets, and our self-supervised pre-trained model provides a strong starting point for supervised fine-tuning. We have achieved 1st place on the KITTI 2015 benchmark and outperformed all other methods on the Sintel Final benchmark. Furthermore, we have demonstrated the generalization capabilities of DistillFlow across framework, correspondence, and cross-dataset contexts.",1
"Neuromorphic sensing and computing hold a promise for highly energy-efficient and high-bandwidth-sensor processing. A major challenge for neuromorphic computing is that learning algorithms for traditional artificial neural networks (ANNs) do not transfer directly to spiking neural networks (SNNs) due to the discrete spikes and more complex neuronal dynamics. As a consequence, SNNs have not yet been successfully applied to complex, large-scale tasks. In this article, we focus on the self-supervised learning problem of optical flow estimation from event-based camera inputs, and investigate the changes that are necessary to the state-of-the-art ANN training pipeline in order to successfully tackle it with SNNs. More specifically, we first modify the input event representation to encode a much smaller time slice with minimal explicit temporal information. Consequently, we make the network's neuronal dynamics and recurrent connections responsible for integrating information over time. Moreover, we reformulate the self-supervised loss function for event-based optical flow to improve its convexity. We perform experiments with various types of recurrent ANNs and SNNs using the proposed pipeline. Concerning SNNs, we investigate the effects of elements such as parameter initialization and optimization, surrogate gradient shape, and adaptive neuronal mechanisms. We find that initialization and surrogate gradient width play a crucial part in enabling learning with sparse inputs, while the inclusion of adaptivity and learnable neuronal parameters can improve performance. We show that the performance of the proposed ANNs and SNNs are on par with that of the current state-of-the-art ANNs trained in a self-supervised manner.",0
"Highly energy-efficient and high-bandwidth-sensor processing can be achieved through neuromorphic sensing and computing. However, the learning algorithms used for traditional artificial neural networks (ANNs) cannot be directly applied to spiking neural networks (SNNs) due to the discrete spikes and complex neuronal dynamics. Therefore, SNNs have not yet been successfully applied to complex, large-scale tasks. This article explores the self-supervised learning problem of optical flow estimation from event-based camera inputs and examines the changes required to the state-of-the-art ANN training pipeline to successfully tackle it with SNNs. The input event representation is modified to encode a smaller time slice with minimal explicit temporal information, making the network's neuronal dynamics and recurrent connections responsible for integrating information over time. The self-supervised loss function for event-based optical flow is reformulated to improve its convexity. Experiments are conducted with various types of recurrent ANNs and SNNs using the proposed pipeline, investigating the effects of elements such as parameter initialization and optimization, surrogate gradient shape, and adaptive neuronal mechanisms. It is found that initialization and surrogate gradient width are crucial in enabling learning with sparse inputs, while the inclusion of adaptivity and learnable neuronal parameters can improve performance. The proposed ANNs and SNNs show a performance on par with the current state-of-the-art ANNs trained in a self-supervised manner.",1
"Non-Rigid Structure-from-Motion (NRSfM) reconstructs a deformable 3D object from the correspondences established between monocular 2D images. Current NRSfM methods lack statistical robustness, which is the ability to cope with correspondence errors.This prevents one to use automatically established correspondences, which are prone to errors, thereby strongly limiting the scope of NRSfM. We propose a three-step automatic pipeline to solve NRSfM robustly by exploiting isometry. Step 1 computes the optical flow from correspondences, step 2 reconstructs each 3D point's normal vector using multiple reference images and integrates them to form surfaces with the best reference and step 3 rejects the 3D points that break isometry in their local neighborhood. Importantly, each step is designed to discard or flag erroneous correspondences. Our contributions include the robustification of optical flow by warp estimation, new fast analytic solutions to local normal reconstruction and their robustification, and a new scale-independent measure of 3D local isometric coherence. Experimental results show that our robust NRSfM method consistently outperforms existing methods on both synthetic and real datasets.",0
"The process of Non-Rigid Structure-from-Motion (NRSfM) involves reconstructing a deformable 3D object through correspondences established between monocular 2D images. However, current NRSfM methods lack statistical robustness, meaning that they struggle to handle errors in correspondence. This has limited the use of automatically established correspondences and therefore restricted the potential of NRSfM. In order to address this issue, we propose a three-step automatic pipeline that utilizes isometry to solve NRSfM robustly. Step 1 involves the computation of optical flow from correspondences, while step 2 reconstructs the normal vector of each 3D point using multiple reference images and integrates them to form surfaces with the best reference. Finally, step 3 involves the rejection of 3D points that break isometry in their local neighborhood. Each step is designed to discard or flag erroneous correspondences. Our contributions to this approach include the robustification of optical flow through warp estimation, new fast analytic solutions for local normal reconstruction and their robustification, and the introduction of a new scale-independent measure of 3D local isometric coherence. Our experimental results show that our approach consistently outperforms existing NRSfM methods on both synthetic and real datasets.",1
"End-to-end trained convolutional neural networks have led to a breakthrough in optical flow estimation. The most recent advances focus on improving the optical flow estimation by improving the architecture and setting a new benchmark on the publicly available MPI-Sintel dataset. Instead, in this article, we investigate how deep neural networks estimate optical flow. A better understanding of how these networks function is important for (i) assessing their generalization capabilities to unseen inputs, and (ii) suggesting changes to improve their performance. For our investigation, we focus on FlowNetS, as it is the prototype of an encoder-decoder neural network for optical flow estimation. Furthermore, we use a filter identification method that has played a major role in uncovering the motion filters present in animal brains in neuropsychological research. The method shows that the filters in the deepest layer of FlowNetS are sensitive to a variety of motion patterns. Not only do we find translation filters, as demonstrated in animal brains, but thanks to the easier measurements in artificial neural networks, we even unveil dilation, rotation, and occlusion filters. Furthermore, we find similarities in the refinement part of the network and the perceptual filling-in process which occurs in the mammal primary visual cortex.",0
"Convolutional neural networks trained end-to-end have revolutionized the estimation of optical flow. While recent developments have concentrated on improving the architecture of these networks and setting new benchmarks using the MPI-Sintel dataset, this article delves into the inner workings of deep neural networks used for optical flow estimation. Understanding their function is key to assessing their ability to generalize and suggesting performance-enhancing changes. The study focuses on the prototype encoder-decoder network, FlowNetS, utilizing a filter identification method that has previously been instrumental in revealing motion filters in animal brains. The method exposes the sensitivity of the deepest layer of FlowNetS to various motion patterns, including dilation, rotation, and occlusion filters, in addition to translation filters found in animal brains. The study also draws parallels between the refinement process in the network and the perceptual filling-in process in the primary visual cortex of mammals.",1
"We present an unsupervised learning approach for optical flow estimation by improving the upsampling and learning of pyramid network. We design a self-guided upsample module to tackle the interpolation blur problem caused by bilinear upsampling between pyramid levels. Moreover, we propose a pyramid distillation loss to add supervision for intermediate levels via distilling the finest flow as pseudo labels. By integrating these two components together, our method achieves the best performance for unsupervised optical flow learning on multiple leading benchmarks, including MPI-SIntel, KITTI 2012 and KITTI 2015. In particular, we achieve EPE=1.4 on KITTI 2012 and F1=9.38% on KITTI 2015, which outperform the previous state-of-the-art methods by 22.2% and 15.7%, respectively.",0
"Our approach to optical flow estimation through unsupervised learning involves enhancing the upsampling and learning process of the pyramid network. To resolve the issue of interpolation blur caused by bilinear upsampling between pyramid levels, we have devised a self-guided upsample module. Furthermore, we have introduced a pyramid distillation loss to provide supervision for intermediate levels by distilling the finest flow as pseudo labels. By combining these two elements, we have achieved the most outstanding results for unsupervised optical flow learning on multiple leading benchmarks, including MPI-SIntel, KITTI 2012 and KITTI 2015. Specifically, we have surpassed the previous state-of-the-art methods by 22.2% and 15.7%, respectively, with EPE=1.4 on KITTI 2012 and F1=9.38% on KITTI 2015.",1
"Estimating geometric elements such as depth, camera motion, and optical flow from images is an important part of the robot's visual perception. We use a joint self-supervised method to estimate the three geometric elements. Depth network, optical flow network and camera motion network are independent of each other but are jointly optimized during training phase. Compared with independent training, joint training can make full use of the geometric relationship between geometric elements and provide dynamic and static information of the scene. In this paper, we improve the joint self-supervision method from three aspects: network structure, dynamic object segmentation, and geometric constraints. In terms of network structure, we apply the attention mechanism to the camera motion network, which helps to take advantage of the similarity of camera movement between frames. And according to attention mechanism in Transformer, we propose a plug-and-play convolutional attention module. In terms of dynamic object, according to the different influences of dynamic objects in the optical flow self-supervised framework and the depth-pose self-supervised framework, we propose a threshold algorithm to detect dynamic regions, and mask that in the loss function respectively. In terms of geometric constraints, we use traditional methods to estimate the fundamental matrix from the corresponding points to constrain the camera motion network. We demonstrate the effectiveness of our method on the KITTI dataset. Compared with other joint self-supervised methods, our method achieves state-of-the-art performance in the estimation of pose and optical flow, and the depth estimation has also achieved competitive results. Code will be available https://github.com/jianfenglihg/Unsupervised_geometry.",0
"The robot's visual perception involves estimating important geometric elements such as camera motion, depth, and optical flow from images. To achieve this, we utilize a joint self-supervised method for estimating these elements. Although the depth network, optical flow network, and camera motion network are independent, they are jointly optimized during training to make full use of the geometric relationship between them and provide both dynamic and static scene information. In this study, we enhance the joint self-supervision method by improving the network structure, applying dynamic object segmentation, and incorporating geometric constraints. We employ the attention mechanism in the camera motion network and introduce a plug-and-play convolutional attention module based on the Transformer model. We propose a threshold algorithm to detect dynamic regions and mask them in the loss function. Additionally, to constrain the camera motion network, we use traditional methods to estimate the fundamental matrix from corresponding points. We demonstrate the effectiveness of our method on the KITTI dataset and show that it surpasses other joint self-supervised techniques, achieving state-of-the-art performance in pose and optical flow estimation, and competitive results in depth estimation. The code for our method is available at https://github.com/jianfenglihg/Unsupervised_geometry.",1
"We present a Python-based renderer built on NVIDIA's OptiX ray tracing engine and the OptiX AI denoiser, designed to generate high-quality synthetic images for research in computer vision and deep learning. Our tool enables the description and manipulation of complex dynamic 3D scenes containing object meshes, materials, textures, lighting, volumetric data (e.g., smoke), and backgrounds. Metadata, such as 2D/3D bounding boxes, segmentation masks, depth maps, normal maps, material properties, and optical flow vectors, can also be generated. In this work, we discuss design goals, architecture, and performance. We demonstrate the use of data generated by path tracing for training an object detector and pose estimator, showing improved performance in sim-to-real transfer in situations that are difficult for traditional raster-based renderers. We offer this tool as an easy-to-use, performant, high-quality renderer for advancing research in synthetic data generation and deep learning.",0
"Our Python-based renderer, which utilizes NVIDIA's OptiX ray tracing engine and the OptiX AI denoiser, aims to produce top-notch synthetic images for computer vision and deep learning research. It empowers users to create and edit intricate dynamic 3D scenes with object meshes, materials, textures, lighting, volumetric data, and backgrounds. Additionally, metadata like 2D/3D bounding boxes, segmentation masks, depth maps, normal maps, material properties, and optical flow vectors can be generated. In this paper, we delve into the renderer's design objectives, architecture, and performance. Our experiments demonstrate that training object detection and pose estimation models with data generated through path tracing yields better results in sim-to-real transfer scenarios that challenge traditional raster-based renderers. Our goal is to provide an accessible, efficient, and superior renderer that furthers synthetic data generation and deep learning research.",1
"Semantic segmentation of aerial videos has been extensively used for decision making in monitoring environmental changes, urban planning, and disaster management. The reliability of these decision support systems is dependent on the accuracy of the video semantic segmentation algorithms. The existing CNN based video semantic segmentation methods have enhanced the image semantic segmentation methods by incorporating an additional module such as LSTM or optical flow for computing temporal dynamics of the video which is a computational overhead. The proposed research work modifies the CNN architecture by incorporating temporal information to improve the efficiency of video semantic segmentation.   In this work, an enhanced encoder-decoder based CNN architecture (UVid-Net) is proposed for UAV video semantic segmentation. The encoder of the proposed architecture embeds temporal information for temporally consistent labelling. The decoder is enhanced by introducing the feature-refiner module, which aids in accurate localization of the class labels. The proposed UVid-Net architecture for UAV video semantic segmentation is quantitatively evaluated on extended ManipalUAVid dataset. The performance metric mIoU of 0.79 has been observed which is significantly greater than the other state-of-the-art algorithms. Further, the proposed work produced promising results even for the pre-trained model of UVid-Net on urban street scene with fine tuning the final layer on UAV aerial videos.",0
"The use of semantic segmentation in aerial videos has become increasingly important in decision making for various applications such as environmental monitoring, urban planning, and disaster management. However, the accuracy of video semantic segmentation algorithms is crucial for reliable decision support systems. Current CNN-based methods for video semantic segmentation have improved image segmentation by incorporating additional modules like LSTM or optical flow, leading to increased computational costs. To address this issue, this research proposes modifying the CNN architecture to include temporal information for efficient video semantic segmentation. The proposed UVid-Net architecture incorporates temporal information in the encoder for consistent labeling and introduces a feature-refiner module in the decoder for accurate class label localization. The performance of UVid-Net was evaluated on the ManipalUAVid dataset, showing a significantly higher mIoU of 0.79 compared to state-of-the-art algorithms. Moreover, the proposed architecture demonstrated promising results even when fine-tuning the final layer on UAV aerial videos after pre-training on urban street scenes.",1
"Recently, DETR and Deformable DETR have been proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance as previous complex hand-crafted detectors. However, their performance on Video Object Detection (VOD) has not been well explored. In this paper, we present TransVOD, an end-to-end video object detection model based on a spatial-temporal Transformer architecture. The goal of this paper is to streamline the pipeline of VOD, effectively removing the need for many hand-crafted components for feature aggregation, e.g., optical flow, recurrent neural networks, relation networks. Besides, benefited from the object query design in DETR, our method does not need complicated post-processing methods such as Seq-NMS or Tubelet rescoring, which keeps the pipeline simple and clean. In particular, we present temporal Transformer to aggregate both the spatial object queries and the feature memories of each frame. Our temporal Transformer consists of three components: Temporal Deformable Transformer Encoder (TDTE) to encode the multiple frame spatial details, Temporal Query Encoder (TQE) to fuse object queries, and Temporal Deformable Transformer Decoder to obtain current frame detection results. These designs boost the strong baseline deformable DETR by a significant margin (3%-4% mAP) on the ImageNet VID dataset. TransVOD yields comparable results performance on the benchmark of ImageNet VID. We hope our TransVOD can provide a new perspective for video object detection. Code will be made publicly available at https://github.com/SJTU-LuHe/TransVOD.",0
"DETR and Deformable DETR have been proposed recently to eliminate the need for hand-designed components in object detection, while maintaining good performance as previous complex detectors. However, their performance in Video Object Detection (VOD) has not been thoroughly investigated. To address this gap, this paper introduces TransVOD, an end-to-end video object detection model based on a spatial-temporal Transformer architecture. The aim is to simplify the VOD pipeline by removing the need for hand-crafted components for feature aggregation such as optical flow, recurrent neural networks, and relation networks. With the object query design in DETR, TransVOD does not require complicated post-processing methods like Seq-NMS or Tubelet rescoring, resulting in a clean and straightforward pipeline. The paper presents a temporal Transformer that aggregates both spatial object queries and feature memories of each frame. TransVOD uses three components: a Temporal Deformable Transformer Encoder (TDTE) to encode multiple frame spatial details, a Temporal Query Encoder (TQE) to fuse object queries, and a Temporal Deformable Transformer Decoder to obtain current frame detection results. These designs enhance the performance of deformable DETR by 3-4% mAP on the ImageNet VID dataset. TransVOD achieves comparable results to the benchmark of ImageNet VID, and the code is available at https://github.com/SJTU-LuHe/TransVOD. The paper offers a new perspective on video object detection.",1
"The technology for Visual Odometry (VO) that estimates the position and orientation of the moving object through analyzing the image sequences captured by on-board cameras, has been well investigated with the rising interest in autonomous driving. This paper studies monocular VO from the perspective of Deep Learning (DL). Unlike most current learning-based methods, our approach, called DeepAVO, is established on the intuition that features contribute discriminately to different motion patterns. Specifically, we present a novel four-branch network to learn the rotation and translation by leveraging Convolutional Neural Networks (CNNs) to focus on different quadrants of optical flow input. To enhance the ability of feature selection, we further introduce an effective channel-spatial attention mechanism to force each branch to explicitly distill related information for specific Frame to Frame (F2F) motion estimation. Experiments on various datasets involving outdoor driving and indoor walking scenarios show that the proposed DeepAVO outperforms the state-of-the-art monocular methods by a large margin, demonstrating competitive performance to the stereo VO algorithm and verifying promising potential for generalization.",0
"The rising interest in autonomous driving has led to extensive research on Visual Odometry (VO), which utilizes on-board cameras to estimate the position and orientation of a moving object by analyzing image sequences. This study focuses on monocular VO using Deep Learning (DL) and proposes a new approach called DeepAVO. Unlike current learning-based methods, DeepAVO exploits the idea that features contribute discriminately to different motion patterns. The proposed method uses a four-branch network that leverages Convolutional Neural Networks (CNNs) to focus on different quadrants of optical flow input to learn rotation and translation. To enhance feature selection, an effective channel-spatial attention mechanism is introduced to explicitly distill related information for specific Frame to Frame (F2F) motion estimation. Experiments on various datasets, including outdoor driving and indoor walking scenarios, demonstrate that DeepAVO outperforms state-of-the-art monocular methods and shows promising potential for generalization, with competitive performance to the stereo VO algorithm.",1
"With the advent of neuromorphic vision sensors such as event-based cameras, a paradigm shift is required for most computer vision algorithms. Among these algorithms, optical flow estimation is a prime candidate for this process considering that it is linked to a neuromorphic vision approach. Usage of optical flow is widespread in robotics applications due to its richness and accuracy. We present a Principal Component Analysis (PCA) approach to the problem of event-based optical flow estimation. In this approach, we examine different regularization methods which efficiently enhance the estimation of the optical flow. We show that the best variant of our proposed method, dedicated to the real-time context of visual odometry, is about two times faster compared to state-of-the-art implementations while significantly improves optical flow accuracy.",0
"Most computer vision algorithms need to be reevaluated with the introduction of neuromorphic vision sensors, like event-based cameras. Optical flow estimation is an ideal candidate for this change, as it is linked to a neuromorphic vision approach, and is widely used in robotics applications due to its richness and accuracy. To tackle the problem of event-based optical flow estimation, we propose a Principal Component Analysis (PCA) approach. Our approach examines various regularization methods that improve the estimation of optical flow efficiently. We demonstrate that our proposed method's best version, specifically designed for the real-time context of visual odometry, is about two times faster than current implementations while significantly enhancing the accuracy of optical flow.",1
"In this work we tackle the task of video-based visual emotion recognition in the wild. Standard methodologies that rely solely on the extraction of bodily and facial features often fall short of accurate emotion prediction in cases where the aforementioned sources of affective information are inaccessible due to head/body orientation, low resolution and poor illumination. We aspire to alleviate this problem by leveraging visual context in the form of scene characteristics and attributes, as part of a broader emotion recognition framework. Temporal Segment Networks (TSN) constitute the backbone of our proposed model. Apart from the RGB input modality, we make use of dense Optical Flow, following an intuitive multi-stream approach for a more effective encoding of motion. Furthermore, we shift our attention towards skeleton-based learning and leverage action-centric data as means of pre-training a Spatial-Temporal Graph Convolutional Network (ST-GCN) for the task of emotion recognition. Our extensive experiments on the challenging Body Language Dataset (BoLD) verify the superiority of our methods over existing approaches, while by properly incorporating all of the aforementioned modules in a network ensemble, we manage to surpass the previous best published recognition scores, by a large margin.",0
"The objective of our study is to improve video-based visual emotion recognition in real-life situations. Traditional approaches that rely on facial and bodily features are often unreliable when these sources of information are inaccessible. Such instances can occur due to low resolution, poor lighting, or head/body orientation. To address this challenge, we propose using visual context, such as scene characteristics and attributes, as part of a broader emotion recognition framework. Our model uses Temporal Segment Networks (TSN) as its foundation, along with dense Optical Flow, to better capture motion. Additionally, we incorporate skeleton-based learning and action-centric data to pre-train a Spatial-Temporal Graph Convolutional Network (ST-GCN) for emotion recognition. Our experiments on the Body Language Dataset (BoLD) demonstrate that our approach outperforms existing methods, and by combining all modules in a network ensemble, we significantly surpass the best published recognition scores.",1
"This paper addresses the challenging unsupervised scene flow estimation problem by jointly learning four low-level vision sub-tasks: optical flow $\textbf{F}$, stereo-depth $\textbf{D}$, camera pose $\textbf{P}$ and motion segmentation $\textbf{S}$. Our key insight is that the rigidity of the scene shares the same inherent geometrical structure with object movements and scene depth. Hence, rigidity from $\textbf{S}$ can be inferred by jointly coupling $\textbf{F}$, $\textbf{D}$ and $\textbf{P}$ to achieve more robust estimation. To this end, we propose a novel scene flow framework named EffiScene with efficient joint rigidity learning, going beyond the existing pipeline with independent auxiliary structures. In EffiScene, we first estimate optical flow and depth at the coarse level and then compute camera pose by Perspective-$n$-Points method. To jointly learn local rigidity, we design a novel Rigidity From Motion (RfM) layer with three principal components: \emph{}{(i)} correlation extraction; \emph{}{(ii)} boundary learning; and \emph{}{(iii)} outlier exclusion. Final outputs are fused based on the rigid map $M_R$ from RfM at finer levels. To efficiently train EffiScene, two new losses $\mathcal{L}_{bnd}$ and $\mathcal{L}_{unc}$ are designed to prevent trivial solutions and to regularize the flow boundary discontinuity. Extensive experiments on scene flow benchmark KITTI show that our method is effective and significantly improves the state-of-the-art approaches for all sub-tasks, i.e. optical flow ($5.19 \rightarrow 4.20$), depth estimation ($3.78 \rightarrow 3.46$), visual odometry ($0.012 \rightarrow 0.011$) and motion segmentation ($0.57 \rightarrow 0.62$).",0
"Efficiently estimating unsupervised scene flow is a complex and challenging task. In this paper, we propose a novel framework, named EffiScene, that jointly learns four low-level vision sub-tasks: optical flow, stereo-depth, camera pose, and motion segmentation. Our approach leverages the inherent geometrical structure shared by rigidity of the scene, object movements, and scene depth. By coupling optical flow, stereo-depth, and camera pose, we achieve more robust estimation of rigidity from motion segmentation. EffiScene utilizes a Rigidity From Motion (RfM) layer with correlation extraction, boundary learning, and outlier exclusion to jointly learn local rigidity. Two new losses, designed to prevent trivial solutions and to regularize flow boundary discontinuity, efficiently train EffiScene. Our experiments on the KITTI benchmark show that EffiScene significantly improves the state-of-the-art approaches for all sub-tasks, including optical flow, depth estimation, visual odometry, and motion segmentation.",1
"We present SMURF, a method for unsupervised learning of optical flow that improves state of the art on all benchmarks by $36\%$ to $40\%$ (over the prior best method UFlow) and even outperforms several supervised approaches such as PWC-Net and FlowNet2. Our method integrates architecture improvements from supervised optical flow, i.e. the RAFT model, with new ideas for unsupervised learning that include a sequence-aware self-supervision loss, a technique for handling out-of-frame motion, and an approach for learning effectively from multi-frame video data while still only requiring two frames for inference.",0
"SMURF is a novel technique for unsupervised learning of optical flow that surpasses the previous best method UFlow, as well as several supervised approaches including PWC-Net and FlowNet2. Our method achieves a remarkable improvement of 36% to 40% on all benchmarks. To achieve this, we have incorporated the RAFT model's architecture enhancements from supervised optical flow while introducing innovative unsupervised learning concepts. These include a self-supervision loss that is sequence-aware, a method for handling out-of-frame motion, and an approach for effectively learning from multi-frame video data while still only requiring two frames for inference.",1
"Video salient object detection (VSOD) aims to locate and segment the most attractive object by exploiting both spatial cues and temporal cues hidden in video sequences. However, spatial and temporal cues are often unreliable in real-world scenarios, such as low-contrast foreground, fast motion, and multiple moving objects. To address these problems, we propose a new framework to adaptively capture available information from spatial and temporal cues, which contains Confidence-guided Adaptive Gate (CAG) modules and Dual Differential Enhancement (DDE) modules. For both RGB features and optical flow features, CAG estimates confidence scores supervised by the IoU between predictions and the ground truths to re-calibrate the information with a gate mechanism. DDE captures the differential feature representation to enrich the spatial and temporal information and generate the fused features. Experimental results on four widely used datasets demonstrate the effectiveness of the proposed method against thirteen state-of-the-art methods.",0
"The objective of Video salient object detection (VSOD) is to locate and segment the most captivating object in a video sequence by utilizing both spatial and temporal cues. However, in real-world scenarios, such as low-contrast foreground, fast motion, and multiple moving objects, these cues are frequently unreliable. To address this issue, we have introduced a novel framework that can adaptively capture information from spatial and temporal cues. This framework includes Confidence-guided Adaptive Gate (CAG) modules and Dual Differential Enhancement (DDE) modules. CAG modules estimate confidence scores for both RGB features and optical flow features, utilizing the intersection over union (IoU) between predictions and ground truths to recalibrate the information with a gate mechanism. DDE captures differential feature representation to enhance the spatial and temporal information and generate fused features. Our experimental results on four commonly used datasets demonstrate the effectiveness of our proposed method over thirteen other state-of-the-art approaches.",1
"Most Video Super-Resolution (VSR) methods enhance a video reference frame by aligning its neighboring frames and mining information on these frames. Recently, deformable alignment has drawn extensive attention in VSR community for its remarkable performance, which can adaptively align neighboring frames with the reference one. However, we experimentally find that deformable alignment methods still suffer from fast motion due to locally loss-driven offset prediction and lack explicit motion constraints. Hence, we propose a Matching-based Flow Estimation (MFE) module to conduct global semantic feature matching and estimate optical flow as coarse offset for each location. And a Flow-guided Deformable Module (FDM) is proposed to integrate optical flow into deformable convolution. The FDM uses the optical flow to warp the neighboring frames at first. And then, the warped neighboring frames and the reference one are used to predict a set of fine offsets for each coarse offset. In general, we propose an end-to-end deep network called Flow-guided Deformable Alignment Network (FDAN), which reaches the state-of-the-art performance on two benchmark datasets while is still competitive in computation and memory consumption.",0
"The majority of Video Super-Resolution (VSR) methods enhance a video reference frame by utilizing neighboring frames to extract information and align the frames. Recently, the VSR community has shown great interest in deformable alignment due to its exceptional performance in adaptively aligning neighboring frames with the reference frame. However, our experiments show that deformable alignment methods are still problematic when dealing with fast-moving images due to locally loss-driven offset prediction and the lack of explicit motion constraints. Consequently, we propose the Matching-based Flow Estimation (MFE) module to conduct global semantic feature matching and estimate optical flow as a coarse offset for each location. Furthermore, we propose the Flow-guided Deformable Module (FDM) that integrates optical flow into deformable convolution. The FDM uses the optical flow to warp the neighboring frames, followed by the warped frames and the reference frame being used to predict a set of fine offsets for each coarse offset. Overall, we propose an end-to-end deep network, called the Flow-guided Deformable Alignment Network (FDAN), which achieves state-of-the-art performance on two benchmark datasets while remaining competitive in computation and memory consumption.",1
"Video Frame Interpolation synthesizes non-existent images between adjacent frames, with the aim of providing a smooth and consistent visual experience. Two approaches for solving this challenging task are optical flow based and kernel-based methods. In existing works, optical flow based methods can provide accurate point-to-point motion description, however, they lack constraints on object structure. On the contrary, kernel-based methods focus on structural alignment, which relies on semantic and apparent features, but tends to blur results. Based on these observations, we propose a structure-motion based iterative fusion method. The framework is an end-to-end learnable structure with two stages. First, interpolated frames are synthesized by structure-based and motion-based learning branches respectively, then, an iterative refinement module is established via spatial and temporal feature integration. Inspired by the observation that audiences have different visual preferences on foreground and background objects, we for the first time propose to use saliency masks in the evaluation processes of the task of video frame interpolation. Experimental results on three typical benchmarks show that the proposed method achieves superior performance on all evaluation metrics over the state-of-the-art methods, even when our models are trained with only one-tenth of the data other methods use.",0
"The process of Video Frame Interpolation involves the creation of non-existent images between neighboring frames to ensure a seamless and consistent visual experience. Optical flow based and kernel-based methods are two approaches used to tackle this complex task. Although optical flow based methods provide accurate point-to-point motion description, they lack constraints on object structure. Conversely, kernel-based methods focus on structural alignment, relying on semantic and apparent features, but tend to produce blurry results. Therefore, we introduce a structure-motion based iterative fusion method that is an end-to-end learnable structure with two stages. First, interpolated frames are synthesized using structure-based and motion-based learning branches, followed by an iterative refinement module via spatial and temporal feature integration. Additionally, we propose using saliency masks in the evaluation processes as audiences have different visual preferences on foreground and background objects. Our experimental results on three typical benchmarks demonstrate that our proposed method outperforms state-of-the-art methods on all evaluation metrics, even with just one-tenth of the data used by other methods in training.",1
"Video anomaly detection is a challenging task because of diverse abnormal events. To this task, methods based on reconstruction and prediction are wildly used in recent works, which are built on the assumption that learning on normal data, anomalies cannot be reconstructed or predicated as good as normal patterns, namely the anomaly result with more errors. In this paper, we propose to discriminate anomalies from normal ones by the duality of normality-granted optical flow, which is conducive to predict normal frames but adverse to abnormal frames. The normality-granted optical flow is predicted from a single frame, to keep the motion knowledge focused on normal patterns. Meanwhile, We extend the appearance-motion correspondence scheme from frame reconstruction to prediction, which not only helps to learn the knowledge about object appearances and correlated motion, but also meets the fact that motion is the transformation between appearances. We also introduce a margin loss to enhance the learning of frame prediction. Experiments on standard benchmark datasets demonstrate the impressive performance of our approach.",0
"Detecting anomalies in videos presents a challenge due to the wide range of abnormal events that can occur. Recent studies have primarily relied on reconstruction and prediction methods, assuming that anomalies cannot be reconstructed or predicted accurately compared to normal patterns. In this paper, we propose using the duality of normality-granted optical flow as a means to distinguish between normal and abnormal frames. Our optical flow prediction is based on a single frame to focus on motion knowledge related to normal patterns. Additionally, we extend the appearance-motion correspondence scheme to prediction and introduce a margin loss to improve frame prediction learning. Our experimental results on benchmark datasets show impressive performance.",1
"Intersections where vehicles are permitted to turn and interact with vulnerable road users (VRUs) like pedestrians and cyclists are among some of the most challenging locations for automated and accurate recognition of road users' behavior. In this paper, we propose a deep conditional generative model for interaction detection at such locations. It aims to automatically analyze massive video data about the continuity of road users' behavior. This task is essential for many intelligent transportation systems such as traffic safety control and self-driving cars that depend on the understanding of road users' locomotion. A Conditional Variational Auto-Encoder based model with Gaussian latent variables is trained to encode road users' behavior and perform probabilistic and diverse predictions of interactions. The model takes as input the information of road users' type, position and motion automatically extracted by a deep learning object detector and optical flow from videos, and generates frame-wise probabilities that represent the dynamics of interactions between a turning vehicle and any VRUs involved. The model's efficacy was validated by testing on real--world datasets acquired from two different intersections. It achieved an F1-score above 0.96 at a right--turn intersection in Germany and 0.89 at a left--turn intersection in Japan, both with very busy traffic flows.",0
"Automated recognition of road users' behavior at intersections where vehicles interact with vulnerable road users (VRUs) such as pedestrians and cyclists is a complex task. Our proposed solution is a deep conditional generative model that analyzes extensive video data to detect interactions accurately. This is essential for intelligent transportation systems, including traffic safety control and self-driving cars, that require an understanding of road users' movements. Our model, a Conditional Variational Auto-Encoder based on Gaussian latent variables, encodes road users' behavior to predict probabilistic and diverse interactions. It utilizes deep learning object detectors and optical flow to extract road users' type, position, and motion, generating frame-wise probabilities that represent the dynamics of interactions between turning vehicles and VRUs. Our model was tested on real-world data from two busy intersections, achieving an F1-score of 0.96 at a right-turn intersection in Germany and 0.89 at a left-turn intersection in Japan.",1
"Remarkable progress has been made in 3D reconstruction of rigid structures from a video or a collection of images. However, it is still challenging to reconstruct nonrigid structures from RGB inputs, due to its under-constrained nature. While template-based approaches, such as parametric shape models, have achieved great success in modeling the ""closed world"" of known object categories, they cannot well handle the ""open-world"" of novel object categories or outlier shapes. In this work, we introduce a template-free approach to learn 3D shapes from a single video. It adopts an analysis-by-synthesis strategy that forward-renders object silhouette, optical flow, and pixel values to compare with video observations, which generates gradients to adjust the camera, shape and motion parameters. Without using a category-specific shape template, our method faithfully reconstructs nonrigid 3D structures from videos of human, animals, and objects of unknown classes. Code will be available at lasr-google.github.io .",0
"Significant advancements have been made in the 3D reconstruction of stiff structures from images or videos. However, reconstructing non-rigid structures from RGB inputs remains challenging due to their under-constrained nature. While parametric shape models have been successful in modeling known object categories, they struggle with novel or outlier shapes. This study introduces a template-free approach to 3D shape learning from a single video, utilizing an analysis-by-synthesis strategy to compare object silhouettes, optical flow, and pixel values with video observations, resulting in gradient generation for camera, shape, and motion parameter adjustments. Without category-specific shape templates, the approach effectively reconstructs non-rigid 3D structures from videos of humans, animals, and objects of unknown categories. The code will be available at lasr-google.github.io.",1
"With the goal of predicting the future rainfall intensity in a local region over a relatively short period time, precipitation nowcasting has been a long-time scientific challenge with great social and economic impact. The radar echo extrapolation approaches for precipitation nowcasting take radar echo images as input, aiming to generate future radar echo images by learning from the historical images. To effectively handle complex and high non-stationary evolution of radar echoes, we propose to decompose the movement into optical flow field motion and morphologic deformation. Following this idea, we introduce Flow-Deformation Network (FDNet), a neural network that models flow and deformation in two parallel cross pathways. The flow encoder captures the optical flow field motion between consecutive images and the deformation encoder distinguishes the change of shape from the translational motion of radar echoes. We evaluate the proposed network architecture on two real-world radar echo datasets. Our model achieves state-of-the-art prediction results compared with recent approaches. To the best of our knowledge, this is the first network architecture with flow and deformation separation to model the evolution of radar echoes for precipitation nowcasting. We believe that the general idea of this work could not only inspire much more effective approaches but also be applied to other similar spatiotemporal prediction tasks",0
"Precipitation nowcasting has been a scientific challenge with significant social and economic impact as it aims to predict future rainfall intensity in a local region over a short period. One approach to this is the use of radar echo extrapolation, where historical images are used to generate future radar echo images. To handle the complex and high non-stationary evolution of radar echoes, we suggest decomposing the movement into optical flow field motion and morphologic deformation. This idea is implemented in the Flow-Deformation Network (FDNet), a neural network with two parallel pathways that model flow and deformation. The flow encoder captures the optical flow field motion, while the deformation encoder distinguishes the change of shape from translational motion. We evaluate the network on two real-world radar echo datasets and achieve state-of-the-art results. This is the first network architecture to use flow and deformation separation to model the evolution of radar echoes for precipitation nowcasting. We believe that this approach can be applied to other similar spatiotemporal prediction tasks and inspire more effective approaches.",1
"In this paper, a novel video classification method is presented that aims to recognize different categories of third-person videos efficiently. Our motivation is to achieve a light model that could be trained with insufficient training data. With this intuition, the processing of the 3-dimensional video input is broken to 1D in temporal dimension on top of the 2D in spatial. The processes related to 2D spatial frames are being done by utilizing pre-trained networks with no training phase. The only step which involves training is to classify the 1D time series resulted from the description of the 2D signals. As a matter of fact, optical flow images are first calculated from consecutive frames and described by pre-trained CNN networks. Their dimension is then reduced using PCA. By stacking the description vectors beside each other, a multi-channel time series is created for each video. Each channel of the time series represents a specific feature and follows it over time. The main focus of the proposed method is to classify the obtained time series effectively. Towards this, the idea is to let the machine learn temporal features. This is done by training a multi-channel one dimensional Convolutional Neural Network (1D-CNN). The 1D-CNN learns the features along the only temporal dimension. Hence, the number of training parameters decreases significantly which would result in the trainability of the method on even smaller datasets. It is illustrated that the proposed method could reach the state-of-the-art results on two public datasets UCF11, jHMDB and competitive results on HMDB51.",0
"This paper introduces a new approach for efficiently recognizing various categories of third-person videos. The goal is to develop a lightweight model that can be trained with limited data. To achieve this, the 3D video input is processed by breaking it down into 1D in the temporal dimension and 2D in the spatial dimension. The 2D processes are performed using pre-trained networks without any training phase, while the 1D time series resulting from the description of the 2D signals is the only step that requires training for classification. Optical flow images are first calculated from consecutive frames and described by pre-trained CNN networks, and their dimension is then reduced using PCA. By stacking the description vectors, a multi-channel time series is generated for each video, with each channel representing a specific feature that is tracked over time. The proposed method's primary focus is to effectively classify the obtained time series by allowing the machine to learn temporal features. This is done through training a multi-channel 1D-CNN that learns features along the temporal dimension, resulting in significantly fewer training parameters. The proposed method achieves state-of-the-art results on the UCF11 and jHMDB datasets and competitive results on the HMDB51 dataset.",1
"The goal of this paper is to self-train a 3D convolutional neural network on an unlabeled video collection for deployment on small-scale video collections. As smaller video datasets benefit more from motion than appearance, we strive to train our network using optical flow, but avoid its computation during inference. We propose the first motion-augmented self-training regime, we call MotionFit. We start with supervised training of a motion model on a small, and labeled, video collection. With the motion model we generate pseudo-labels for a large unlabeled video collection, which enables us to transfer knowledge by learning to predict these pseudo-labels with an appearance model. Moreover, we introduce a multi-clip loss as a simple yet efficient way to improve the quality of the pseudo-labeling, even without additional auxiliary tasks. We also take into consideration the temporal granularity of videos during self-training of the appearance model, which was missed in previous works. As a result we obtain a strong motion-augmented representation model suited for video downstream tasks like action recognition and clip retrieval. On small-scale video datasets, MotionFit outperforms alternatives for knowledge transfer by 5%-8%, video-only self-supervision by 1%-7% and semi-supervised learning by 9%-18% using the same amount of class labels.",0
"The objective of this research is to train a 3D convolutional neural network without labeled data by using an unlabeled video collection. We aim to deploy this network on smaller video datasets that rely more on motion than appearance. To achieve this, we propose a novel self-training approach called MotionFit, which incorporates motion information without the need for optical flow computation during inference. Firstly, we train a motion model on a limited labeled video dataset and use it to generate pseudo-labels for a larger unlabeled dataset. These pseudo-labels are then used to train an appearance model to predict them. To improve the quality of the pseudo-labeling, we use a multi-clip loss, and we also consider the temporal granularity of videos during self-training. This results in a robust motion-augmented representation model that performs better in video downstream tasks such as clip retrieval and action recognition. Compared to other methods, MotionFit outperforms video-only self-supervision, semi-supervised learning, and alternatives for knowledge transfer by 1%-7%, 9%-18%, and 5%-8%, respectively, using the same amount of class labels on small-scale video datasets.",1
"Computing optical flow is a fundamental problem in computer vision. However, deep learning-based optical flow techniques do not perform well for non-rigid movements such as those found in faces, primarily due to lack of the training data representing the fine facial motion. We hypothesize that learning optical flow on face motion data will improve the quality of predicted flow on faces. The aim of this work is threefold: (1) exploring self-supervised techniques to generate optical flow ground truth for face images; (2) computing baseline results on the effects of using face data to train Convolutional Neural Networks (CNN) for predicting optical flow; and (3) using the learned optical flow in micro-expression recognition to demonstrate its effectiveness. We generate optical flow ground truth using facial key-points in the BP4D-Spontaneous dataset. The generated optical flow is used to train the FlowNetS architecture to test its performance on the generated dataset. The performance of FlowNetS trained on face images surpassed that of other optical flow CNN architectures, demonstrating its usefulness. Our optical flow features are further compared with other methods using the STSTNet micro-expression classifier, and the results indicate that the optical flow obtained using this work has promising applications in facial expression analysis.",0
"The computation of optical flow is a crucial issue in the field of computer vision. However, optical flow techniques based on deep learning do not work effectively for non-rigid movements, such as those seen in facial expressions, mainly because of the lack of training data that accurately represents fine facial motion. We propose that training optical flow on facial motion data will enhance the accuracy of predicted flow on faces. The objective of this study is threefold: firstly, to investigate self-supervised techniques for creating optical flow ground truth for facial images; secondly, to compute baseline results on the impact of using facial data to train Convolutional Neural Networks (CNN) for predicting optical flow; and finally, to demonstrate the effectiveness of the learned optical flow in micro-expression recognition. We create optical flow ground truth using facial key-points in the BP4D-Spontaneous dataset. The generated optical flow is used to train the FlowNetS architecture to evaluate its performance on the created dataset. FlowNetS trained on facial images outperforms other optical flow CNN architectures, indicating its utility. Furthermore, we compare our optical flow features with other methods using the STSTNet micro-expression classifier, and the results suggest that the optical flow produced by this study has promising applications in facial expression analysis.",1
"Tremor is a key diagnostic feature of Parkinson's Disease (PD), Essential Tremor (ET), and other central nervous system (CNS) disorders. Clinicians or trained raters assess tremor severity with TETRAS scores by observing patients. Lacking quantitative measures, inter- or intra- observer variabilities are almost inevitable as the distinction between adjacent tremor scores is subtle. Moreover, clinician assessments also require patient visits, which limits the frequency of disease progress evaluation. Therefore it is beneficial to develop an automated assessment that can be performed remotely and repeatably at patients' convenience for continuous monitoring. In this work, we proposed to train a deep neural network (DNN) with rank-consistent ordinal regression using 276 clinical videos from 36 essential tremor patients. The videos are coupled with clinician assessed TETRAS scores, which are used as ground truth labels to train the DNN. To tackle the challenge of limited training data, optical flows are used to eliminate irrelevant background and statistic objects from RGB frames. In addition to optical flows, transfer learning is also applied to leverage pre-trained network weights from a related task of tremor frequency estimate. The approach was evaluated by splitting the clinical videos into training (67%) and testing sets (0.33%). The mean absolute error on TETRAS score of the testing results is 0.45, indicating that most of the errors were from the mismatch of adjacent labels, which is expected and acceptable. The model predications also agree well with clinical ratings. This model is further applied to smart phone videos collected from a PD patient who has an implanted device to turn ""On"" or ""Off"" tremor. The model outputs were consistent with the patient tremor states. The results demonstrate that our trained model can be used as a means to assess and track tremor severity.",0
"Parkinson's Disease (PD), Essential Tremor (ET), and other central nervous system (CNS) disorders are characterized by tremors, making it a key diagnostic feature. Tremor severity is typically assessed by clinicians or trained raters using TETRAS scores. However, due to the lack of quantitative measures, inter- or intra-observer variabilities are almost inevitable, and clinician assessments also require patient visits, which limits the frequency of disease progress evaluation. Therefore, an automated assessment that can be performed remotely and repeatedly at patients' convenience for continuous monitoring would be beneficial. In this study, we propose to train a deep neural network (DNN) with rank-consistent ordinal regression using 276 clinical videos from 36 essential tremor patients. These videos are paired with clinician-assessed TETRAS scores, which serve as ground truth labels for the DNN. To address the limited training data, we use optical flows to eliminate irrelevant background and statistical objects from RGB frames. Additionally, transfer learning is applied to leverage pre-trained network weights from a related task of tremor frequency estimation. We evaluate the approach by splitting the clinical videos into training (67%) and testing sets (0.33%). The model's mean absolute error on the TETRAS score for the testing results is 0.45, indicating that most of the errors were from the mismatch of adjacent labels, which is expected and acceptable. The model predictions also agree well with clinical ratings. We further apply the model to smartphone videos collected from a PD patient who has an implanted device to turn ""On"" or ""Off"" tremor. The model outputs are consistent with the patient's tremor states, demonstrating that our trained model can be used as a means to assess and track tremor severity.",1
"Deep-learning-based video processing has yielded transformative results in recent years. However, the video analytics pipeline is energy-intensive due to high data rates and reliance on complex inference algorithms, which limits its adoption in energy-constrained applications. Motivated by the observation of high and variable spatial redundancy and temporal dynamics in video data streams, we design and evaluate an adaptive-resolution optimization framework to minimize the energy use of multi-task video analytics pipelines. Instead of heuristically tuning the input data resolution of individual tasks, our framework utilizes deep reinforcement learning to dynamically govern the input resolution and computation of the entire video analytics pipeline. By monitoring the impact of varying resolution on the quality of high-dimensional video analytics features, hence the accuracy of video analytics results, the proposed end-to-end optimization framework learns the best non-myopic policy for dynamically controlling the resolution of input video streams to globally optimize energy efficiency. Governed by reinforcement learning, optical flow is incorporated into the framework to minimize unnecessary spatio-temporal redundancy that leads to re-computation, while preserving accuracy. The proposed framework is applied to video instance segmentation which is one of the most challenging computer vision tasks, and achieves better energy efficiency than all baseline methods of similar accuracy on the YouTube-VIS dataset.",0
"In recent times, video processing using deep learning has shown remarkable progress. However, the video analytics pipeline consumes a lot of energy due to high data rates and complex inference algorithms, which makes it unsuitable for energy-constrained applications. To address this issue, we have developed an adaptive-resolution optimization framework that minimizes the energy consumption of multi-task video analytics pipelines. Rather than manually adjusting the input data resolution of individual tasks, our framework utilizes deep reinforcement learning to dynamically control the input resolution and computation of the entire video analytics pipeline. By assessing the impact of varying resolution on the quality of high-dimensional video analytics features and accuracy of video analytics results, our end-to-end optimization framework learns the best policy for controlling the resolution of input video streams to optimize energy efficiency. Optical flow is incorporated into the framework to minimize unnecessary spatio-temporal redundancy that leads to re-computation, while preserving accuracy. We apply the proposed framework to video instance segmentation, one of the most challenging computer vision tasks, and achieve better energy efficiency than all baseline methods of similar accuracy on the YouTube-VIS dataset.",1
"Synthetic datasets play a critical role in pre-training CNN models for optical flow, but they are painstaking to generate and hard to adapt to new applications. To automate the process, we present AutoFlow, a simple and effective method to render training data for optical flow that optimizes the performance of a model on a target dataset. AutoFlow takes a layered approach to render synthetic data, where the motion, shape, and appearance of each layer are controlled by learnable hyperparameters. Experimental results show that AutoFlow achieves state-of-the-art accuracy in pre-training both PWC-Net and RAFT. Our code and data are available at https://autoflow-google.github.io .",0
"Generating synthetic datasets for pre-training CNN models for optical flow is crucial, but it can be a time-consuming task and challenging to adapt to new applications. To simplify this process, we introduce AutoFlow, a straightforward and efficient method for producing optical flow training data that enhances a model's performance on a specific dataset. AutoFlow uses a layered approach to generate synthetic data, with learnable hyperparameters controlling each layer's motion, shape, and appearance. Our experimental results demonstrate that AutoFlow achieves the highest level of accuracy in pre-training both PWC-Net and RAFT. To access our code and data, visit https://autoflow-google.github.io.",1
"Recently, several Space-Time Memory based networks have shown that the object cues (e.g. video frames as well as the segmented object masks) from the past frames are useful for segmenting objects in the current frame. However, these methods exploit the information from the memory by global-to-global matching between the current and past frames, which lead to mismatching to similar objects and high computational complexity. To address these problems, we propose a novel local-to-local matching solution for semi-supervised VOS, namely Regional Memory Network (RMNet). In RMNet, the precise regional memory is constructed by memorizing local regions where the target objects appear in the past frames. For the current query frame, the query regions are tracked and predicted based on the optical flow estimated from the previous frame. The proposed local-to-local matching effectively alleviates the ambiguity of similar objects in both memory and query frames, which allows the information to be passed from the regional memory to the query region efficiently and effectively. Experimental results indicate that the proposed RMNet performs favorably against state-of-the-art methods on the DAVIS and YouTube-VOS datasets.",0
"In recent times, Space-Time Memory based networks have demonstrated that object cues from previous frames can aid in object segmentation in the current frame. However, these methods rely on global-to-global matching, which results in mismatches between similar objects and high computational complexity. To tackle this issue, we propose Regional Memory Network (RMNet), a semi-supervised VOS solution that employs local-to-local matching. RMNet constructs precise regional memory by remembering local regions where target objects appeared in the past frames. Optic flow from the previous frame is used to track and predict query regions in the current frame. Our local-to-local matching reduces ambiguity between similar objects, enabling efficient and effective transfer of information from regional memory to the query region. Our experimental results show that RMNet outperforms state-of-the-art methods on DAVIS and YouTube-VOS datasets.",1
"Despite the significant progress made by deep learning in natural image matting, there has been so far no representative work on deep learning for video matting due to the inherent technical challenges in reasoning temporal domain and lack of large-scale video matting datasets. In this paper, we propose a deep learning-based video matting framework which employs a novel and effective spatio-temporal feature aggregation module (ST-FAM). As optical flow estimation can be very unreliable within matting regions, ST-FAM is designed to effectively align and aggregate information across different spatial scales and temporal frames within the network decoder. To eliminate frame-by-frame trimap annotations, a lightweight interactive trimap propagation network is also introduced. The other contribution consists of a large-scale video matting dataset with groundtruth alpha mattes for quantitative evaluation and real-world high-resolution videos with trimaps for qualitative evaluation. Quantitative and qualitative experimental results show that our framework significantly outperforms conventional video matting and deep image matting methods applied to video in presence of multi-frame temporal information.",0
"Up until now, there has been a lack of deep learning research on video matting due to the technical difficulties of reasoning in the temporal domain and the shortage of large-scale video matting datasets. Despite the successes of deep learning in natural image matting, our proposed framework introduces a novel spatio-temporal feature aggregation module (ST-FAM) to address these challenges. As optical flow estimation can be unreliable within matting regions, ST-FAM aligns and aggregates information across different spatial scales and temporal frames within the network decoder. Additionally, we introduce a lightweight interactive trimap propagation network to eliminate the need for frame-by-frame trimap annotations. Our framework is evaluated using a large-scale video matting dataset with groundtruth alpha mattes for quantitative analysis and high-resolution videos with trimaps for qualitative evaluation. Results demonstrate that our framework surpasses conventional video matting and deep image matting methods when presented with multi-frame temporal information.",1
"We propose a self-supervised approach for training multi-frame video denoising networks. These networks predict frame t from a window of frames around t. Our self-supervised approach benefits from the video temporal consistency by penalizing a loss between the predicted frame t and a neighboring target frame, which are aligned using an optical flow. We use the proposed strategy for online internal learning, where a pre-trained network is fine-tuned to denoise a new unknown noise type from a single video. After a few frames, the proposed fine-tuning reaches and sometimes surpasses the performance of a state-of-the-art network trained with supervision. In addition, for a wide range of noise types, it can be applied blindly without knowing the noise distribution. We demonstrate this by showing results on blind denoising of different synthetic and realistic noises.",0
"Our method involves a self-supervised approach to train networks for multi-frame video denoising. These networks are designed to predict frame t based on a window of frames surrounding it. Our approach utilizes the temporal consistency of the video by imposing a loss penalty between the predicted frame t and a neighboring target frame, which are aligned using optical flow. We employ this strategy for online internal learning, where a pre-trained network is fine-tuned to denoise a new unknown noise type from a single video. Our fine-tuning method achieves comparable, and in some cases better, results than a state-of-the-art network trained with supervision after only a few frames. Additionally, our approach can be applied blindly to a wide range of noise types without prior knowledge of the noise distribution. Our results demonstrate the effectiveness of our approach for denoising different types of synthetic and realistic noises.",1
"We propose an architecture and training scheme to predict video frames by explicitly modeling dis-occlusions and capturing the evolution of semantically consistent regions in the video. The scene layout (semantic map) and motion (optical flow) are decomposed into layers, which are predicted and fused with their context to generate future layouts and motions. The appearance of the scene is warped from past frames using the predicted motion in co-visible regions; dis-occluded regions are synthesized with content-aware inpainting utilizing the predicted scene layout. The result is a predictive model that explicitly represents objects and learns their class-specific motion, which we evaluate on video prediction benchmarks.",0
"Our proposal presents an architecture and training approach designed to predict video frames with a focus on modeling dis-occlusions and tracking the evolution of semantically consistent regions within the footage. This is achieved by breaking down the scene layout, or semantic map, and motion, or optical flow, into layers. These layers are then predicted and merged with their respective contexts to generate future layouts and motions. Additionally, we employ a technique that warps the appearance of the scene from past frames using the predicted motion in co-visible areas, while dis-occluded regions are synthesized using content-aware inpainting based on the predicted scene layout. Our approach results in a predictive model that explicitly represents objects and their respective class-specific motion. We evaluate the efficacy of our model on video prediction benchmarks.",1
"Video segmentation for the human head and shoulders is essential in creating elegant media for videoconferencing and virtual reality applications. The main challenge is to process high-quality background subtraction in a real-time manner and address the segmentation issues under motion blurs, e.g., shaking the head or waving hands during conference video. To overcome the motion blur problem in video segmentation, we propose a novel flow-based encoder-decoder network (FUNet) that combines both traditional Horn-Schunck optical-flow estimation technique and convolutional neural networks to perform robust real-time video segmentation. We also introduce a video and image segmentation dataset: ConferenceVideoSegmentationDataset. Code and pre-trained models are available on our GitHub repository: \url{https://github.com/kuangzijian/Flow-Based-Video-Matting}.",0
"Creating polished media for videoconferencing and virtual reality requires video segmentation of the human head and shoulders. The primary obstacle is achieving high-quality background subtraction in real-time and addressing segmentation challenges caused by motion blurs, such as head shaking or hand waving during conferences. Our solution to overcome motion blur problems in video segmentation is a unique flow-based encoder-decoder network (FUNet) that merges the Horn-Schunck optical-flow estimation technique with convolutional neural networks to achieve robust and real-time video segmentation. We also provide a ConferenceVideoSegmentationDataset for video and image segmentation and offer access to code and pre-trained models on our GitHub repository (\url{https://github.com/kuangzijian/Flow-Based-Video-Matting}).",1
"Today's image prediction methods struggle to change the locations of objects in a scene, producing blurry images that average over the many positions they might occupy. In this paper, we propose a simple change to existing image similarity metrics that makes them more robust to positional errors: we match the images using optical flow, then measure the visual similarity of corresponding pixels. This change leads to crisper and more perceptually accurate predictions, and can be used with any image prediction network. We apply our method to predicting future frames of a video, where it obtains strong performance with simple, off-the-shelf architectures.",0
"Current techniques for predicting images encounter difficulties in altering the positions of objects within a given scene. This results in blurry images that do not accurately represent the possible locations of the objects. To address this issue, our paper suggests a minor adjustment to the current image similarity metrics. By utilizing optical flow to match images and assessing the visual similarity of corresponding pixels, our proposed modification improves the precision and perceptual accuracy of predictions. Moreover, this modification can be applied to any image prediction network. We demonstrate the effectiveness of our method in predicting future video frames, where it delivers excellent results with basic, readily available architectures.",1
"3D scene flow estimation is a vital tool in perceiving our environment given depth or range sensors. Unlike optical flow, the data is usually sparse and in most cases partially occluded in between two temporal samplings. Here we propose a new scene flow architecture called OGSF-Net which tightly couples the learning for both flow and occlusions between frames. Their coupled symbiosis results in a more accurate prediction of flow in space. Unlike a traditional multi-action network, our unified approach is fused throughout the network, boosting performances for both occlusion detection and flow estimation. Our architecture is the first to gauge the occlusion in 3D scene flow estimation on point clouds. In key datasets such as Flyingthings3D and KITTI, we achieve the state-of-the-art results.",0
"The estimation of 3D scene flow is a crucial element in our ability to perceive our surroundings when using range or depth sensors. In contrast to optical flow, this data is often incomplete and obscured between two temporal samples. Our proposed solution, known as OGSF-Net, presents a new approach to scene flow architecture that combines the learning of flow and occlusions between frames. By doing so, we achieve a more precise prediction of spatial flow. Our method differs from traditional multi-action networks in that we adopt a unified approach that enhances both occlusion detection and flow estimation throughout the network. We are the first to utilize this approach for 3D scene flow estimation on point clouds, and our results on key datasets such as Flyingthings3D and KITTI are state-of-the-art.",1
"Recently, deep-learning based approaches have achieved impressive performance for autonomous driving. However, end-to-end vision-based methods typically have limited interpretability, making the behaviors of the deep networks difficult to explain. Hence, their potential applications could be limited in practice. To address this problem, we propose an interpretable end-to-end vision-based motion planning approach for autonomous driving, referred to as IVMP. Given a set of past surrounding-view images, our IVMP first predicts future egocentric semantic maps in bird's-eye-view space, which are then employed to plan trajectories for self-driving vehicles. The predicted future semantic maps not only provide useful interpretable information, but also allow our motion planning module to handle objects with low probability, thus improving the safety of autonomous driving. Moreover, we also develop an optical flow distillation paradigm, which can effectively enhance the network while still maintaining its real-time performance. Extensive experiments on the nuScenes dataset and closed-loop simulation show that our IVMP significantly outperforms the state-of-the-art approaches in imitating human drivers with a much higher success rate. Our project page is available at https://sites.google.com/view/ivmp.",0
"Autonomous driving has seen impressive advancements through deep-learning based approaches. However, end-to-end vision-based methods have limited interpretability, hindering the explanation of deep network behaviors and potentially limiting practical applications. To address this, we propose a new approach called IVMP, which is an interpretable end-to-end vision-based motion planning solution for autonomous driving. Our IVMP employs a set of past surrounding-view images to predict future egocentric semantic maps for planning self-driving vehicle trajectories. The predicted future semantic maps provide interpretable information and enable our motion planning module to handle low probability objects, improving autonomous driving safety. Additionally, our optical flow distillation paradigm enhances the network while maintaining real-time performance. Our extensive experiments on the nuScenes dataset and closed-loop simulation demonstrate that IVMP significantly outperforms existing approaches and successfully imitates human drivers. More information can be found on our project page at https://sites.google.com/view/ivmp.",1
"The objective of this paper is to perform audio-visual sound source separation, i.e.~to separate component audios from a mixture based on the videos of sound sources. Moreover, we aim to pinpoint the source location in the input video sequence. Recent works have shown impressive audio-visual separation results when using prior knowledge of the source type (e.g. human playing instrument) and pre-trained motion detectors (e.g. keypoints or optical flows). However, at the same time, the models are limited to a certain application domain. In this paper, we address these limitations and make the following contributions: i) we propose a two-stage architecture, called Appearance and Motion network (AMnet), where the stages specialise to appearance and motion cues, respectively. The entire system is trained in a self-supervised manner; ii) we introduce an Audio-Motion Embedding (AME) framework to explicitly represent the motions that related to sound; iii) we propose an audio-motion transformer architecture for audio and motion feature fusion; iv) we demonstrate state-of-the-art performance on two challenging datasets (MUSIC-21 and AVE) despite the fact that we do not use any pre-trained keypoint detectors or optical flow estimators. Project page: https://ly-zhu.github.io/self-supervised-motion-representations",0
"The purpose of this paper is to achieve audio-visual sound source separation by separating component audios from a mixture based on sound source videos and locating the source in the input video sequence. Some recent studies have shown impressive audio-visual separation results by using prior knowledge of source type and pre-trained motion detectors, but these models have limitations in certain application domains. In this paper, we propose a two-stage architecture called AMnet that specializes in appearance and motion cues, respectively. The entire system is trained in a self-supervised manner. Additionally, we introduce an AME framework to represent the motions related to sound and an audio-motion transformer architecture for audio and motion feature fusion. Despite not using pre-trained keypoint detectors or optical flow estimators, we demonstrate state-of-the-art performance on two challenging datasets (MUSIC-21 and AVE). The project page can be found at https://ly-zhu.github.io/self-supervised-motion-representations.",1
"Visual sound source separation aims at identifying sound components from a given sound mixture with the presence of visual cues. Prior works have demonstrated impressive results, but with the expense of large multi-stage architectures and complex data representations (e.g. optical flow trajectories). In contrast, we study simple yet efficient models for visual sound separation using only a single video frame. Furthermore, our models are able to exploit the information of the sound source category in the separation process. To this end, we propose two models where we assume that i) the category labels are available at the training time, or ii) we know if the training sample pairs are from the same or different category. The experiments with the MUSIC dataset show that our model obtains comparable or better performance compared to several recent baseline methods. The code is available at https://github.com/ly-zhu/Leveraging-Category-Information-for-Single-Frame-Visual-Sound-Source-Separation",0
"The goal of visual sound source separation is to identify individual sound elements in a given mixture by using visual cues. Previous research has shown impressive results, but often relies on complex and multi-stage architectures, as well as intricate data representations such as optical flow trajectories. Conversely, our study focuses on developing simple and efficient models for visual sound separation using only one video frame. Moreover, our models can take advantage of information regarding the sound source category during the separation process. We propose two models, one that assumes category labels are available during training, and another that assumes training sample pairs are either from the same or different categories. We conducted experiments on the MUSIC dataset and found that our models perform comparably or better than several recent baseline methods. Our code is available at https://github.com/ly-zhu/Leveraging-Category-Information-for-Single-Frame-Visual-Sound-Source-Separation.",1
"Optical flow is the motion of a pixel between at least two consecutive video frames and can be estimated through an end-to-end trainable convolutional neural network. To this end, large training datasets are required to improve the accuracy of optical flow estimation. Our paper presents OmniFlow: a new synthetic omnidirectional human optical flow dataset. Based on a rendering engine we create a naturalistic 3D indoor environment with textured rooms, characters, actions, objects, illumination and motion blur where all components of the environment are shuffled during the data capturing process. The simulation has as output rendered images of household activities and the corresponding forward and backward optical flow. To verify the data for training volumetric correspondence networks for optical flow estimation we train different subsets of the data and test on OmniFlow with and without Test-Time-Augmentation. As a result we have generated 23,653 image pairs and corresponding forward and backward optical flow. Our dataset can be downloaded from: https://mytuc.org/byfs",0
"The estimation of optical flow involves analyzing the motion of a pixel in consecutive video frames and can be achieved using a trainable convolutional neural network. To enhance the accuracy of this process, it is necessary to have access to large training datasets. In our study, we introduce OmniFlow, a novel synthetic omnidirectional human optical flow dataset. By using a rendering engine, we created a 3D indoor environment that mimics naturalistic settings, complete with textured rooms, characters, objects, actions, illumination, and motion blur. During the data capturing process, all components of the environment were shuffled to generate a realistic simulation of household activities, with corresponding forward and backward optical flow. We trained different subsets of the data and tested them on OmniFlow, with and without Test-Time-Augmentation, to verify their accuracy for training volumetric correspondence networks for optical flow estimation. Our dataset includes 23,653 image pairs and their respective forward and backward optical flow, which can be downloaded from the following link: https://mytuc.org/byfs.",1
"A majority of methods for video frame interpolation compute bidirectional optical flow between adjacent frames of a video, followed by a suitable warping algorithm to generate the output frames. However, approaches relying on optical flow often fail to model occlusions and complex non-linear motions directly from the video and introduce additional bottlenecks unsuitable for widespread deployment. We address these limitations with FLAVR, a flexible and efficient architecture that uses 3D space-time convolutions to enable end-to-end learning and inference for video frame interpolation. Our method efficiently learns to reason about non-linear motions, complex occlusions and temporal abstractions, resulting in improved performance on video interpolation, while requiring no additional inputs in the form of optical flow or depth maps. Due to its simplicity, FLAVR can deliver 3x faster inference speed compared to the current most accurate method on multi-frame interpolation without losing interpolation accuracy. In addition, we evaluate FLAVR on a wide range of challenging settings and consistently demonstrate superior qualitative and quantitative results compared with prior methods on various popular benchmarks including Vimeo-90K, UCF101, DAVIS, Adobe, and GoPro. Finally, we demonstrate that FLAVR for video frame interpolation can serve as a useful self-supervised pretext task for action recognition, optical flow estimation, and motion magnification.",0
"Several methods for video frame interpolation rely on bidirectional optical flow between adjacent frames, with a warping algorithm to generate output frames. However, this approach can struggle with occlusions and complex, non-linear motions, leading to bottlenecks and limited deployment. Our solution, FLAVR, utilizes 3D space-time convolutions for end-to-end learning and inference, addressing these challenges without requiring additional inputs such as optical flow or depth maps. FLAVR achieves improved performance in video interpolation, with 3x faster inference speed than the current most accurate method, and consistently superior results in various benchmark tests. Additionally, FLAVR can serve as a self-supervised pretext task for other video-related tasks, including action recognition, optical flow estimation, and motion magnification.",1
"A common strategy to video understanding is to incorporate spatial and motion information by fusing features derived from RGB frames and optical flow. In this work, we introduce a new way to leverage semantic segmentation as an intermediate representation for video understanding and use it in a way that requires no additional labeling.   Second, we propose a general framework which learns the intermediate representations (optical flow and semantic segmentation) jointly with the final video understanding task and allows the adaptation of the representations to the end goal. Despite the use of intermediate representations within the network, during inference, no additional data beyond RGB sequences is needed, enabling efficient recognition with a single network.   Finally, we present a way to find the optimal learning configuration by searching the best loss weighting via evolution. We obtain more powerful visual representations for videos which lead to performance gains over the state-of-the-art.",0
"One commonly used approach in video understanding involves merging spatial and motion information obtained from RGB frames and optical flow. However, we propose a novel method that utilizes semantic segmentation as an intermediate representation to improve video understanding without requiring additional labeling. Our framework allows for joint learning of intermediate representations (such as optical flow and semantic segmentation) and the final video understanding task, with the ability to adapt to specific end goals. Moreover, our approach requires no extra data beyond the RGB sequences during inference, which makes recognition more efficient using a single network. Additionally, we introduce a method to discover the optimal learning configuration using evolutionary search to determine the best loss weighting. Our approach results in more powerful visual representations for videos, surpassing state-of-the-art performance.",1
"We present a dense-indirect SLAM system using external dense optical flows as input. We extend the recent probabilistic visual odometry model VOLDOR [Min et al. CVPR'20], by incorporating the use of geometric priors to 1) robustly bootstrap estimation from monocular capture, while 2) seamlessly supporting stereo and/or RGB-D input imagery. Our customized back-end tightly couples our intermediate geometric estimates with an adaptive priority scheme managing the connectivity of an incremental pose graph. We leverage recent advances in dense optical flow methods to achieve accurate and robust camera pose estimates, while constructing fine-grain globally-consistent dense environmental maps. Our open source implementation [https://github.com/htkseason/VOLDOR] operates online at around 15 FPS on a single GTX1080Ti GPU.",0
"An indirect SLAM system that utilizes external dense optical flows as input is introduced in this study. By integrating geometric priors, we enhance the probabilistic visual odometry model VOLDOR [Min et al. CVPR'20] to allow for robust estimation from monocular capture and support stereo and/or RGB-D input imagery. Our customized back-end tightly couples intermediate geometric estimates with an adaptive priority scheme that manages the connectivity of an incremental pose graph. We utilize advanced dense optical flow methods to achieve accurate and robust camera pose estimates while constructing fine-grain globally-consistent dense environmental maps. Our open source implementation [https://github.com/htkseason/VOLDOR] can operate online at approximately 15 FPS on a single GTX1080Ti GPU.",1
"We propose a dense indirect visual odometry method taking as input externally estimated optical flow fields instead of hand-crafted feature correspondences. We define our problem as a probabilistic model and develop a generalized-EM formulation for the joint inference of camera motion, pixel depth, and motion-track confidence. Contrary to traditional methods assuming Gaussian-distributed observation errors, we supervise our inference framework under an (empirically validated) adaptive log-logistic distribution model. Moreover, the log-logistic residual model generalizes well to different state-of-the-art optical flow methods, making our approach modular and agnostic to the choice of optical flow estimators. Our method achieved top-ranking results on both TUM RGB-D and KITTI odometry benchmarks. Our open-sourced implementation is inherently GPU-friendly with only linear computational and storage growth.",0
"We suggest a novel approach for visual odometry that employs dense indirect methods and utilizes externally estimated optical flow fields instead of manually crafted feature correspondences. Our approach utilizes a probabilistic model and a generalized-EM formulation to jointly infer camera motion, pixel depth, and motion-track confidence. Unlike traditional methods that assume Gaussian-distributed observation errors, we supervise our inference framework using an empirically validated adaptive log-logistic distribution model. Our approach is modular and agnostic to the choice of optical flow estimators, as the log-logistic residual model generalizes well to various state-of-the-art optical flow methods. Our method outperformed other techniques on the TUM RGB-D and KITTI odometry benchmarks, and our open-sourced implementation is inherently GPU-friendly, with only linear computational and storage growth.",1
"The optical flow estimation has been assessed in various applications. In this paper, we propose a novel method named motion edge structure difference(MESD) to assess estimation errors of optical flow fields on edge of motion objects. We implement comparison experiments for MESD by evaluating five representative optical flow algorithms on four popular benchmarks: MPI Sintel, Middlebury, KITTI 2012 and KITTI 2015. Our experimental results demonstrate that MESD can reasonably and discriminatively assess estimation errors of optical flow fields on motion edge. The results indicate that MESD could be a supplementary metric to existing general assessment metrics for evaluating optical flow algorithms in related computer vision applications.",0
"Various applications have utilized optical flow estimation. This study introduces a new method called motion edge structure difference (MESD) to determine the errors in the estimation of optical flow fields on the edges of motion objects. To test MESD, we conducted comparative experiments by evaluating five commonly used optical flow algorithms on four popular benchmarks: MPI Sintel, Middlebury, KITTI 2012, and KITTI 2015. The experimental findings suggest that MESD can reliably and effectively determine the estimation errors of optical flow fields on motion edges. This indicates that MESD could serve as a useful supplementary metric to existing general assessment metrics for evaluating optical flow algorithms in computer vision applications.",1
"Event cameras are novel vision sensors that sample, in an asynchronous fashion, brightness increments with low latency and high temporal resolution. The resulting streams of events are of high value by themselves, especially for high speed motion estimation. However, a growing body of work has also focused on the reconstruction of intensity frames from the events, as this allows bridging the gap with the existing literature on appearance- and frame-based computer vision. Recent work has mostly approached this problem using neural networks trained with synthetic, ground-truth data. In this work we approach, for the first time, the intensity reconstruction problem from a self-supervised learning perspective. Our method, which leverages the knowledge of the inner workings of event cameras, combines estimated optical flow and the event-based photometric constancy to train neural networks without the need for any ground-truth or synthetic data. Results across multiple datasets show that the performance of the proposed self-supervised approach is in line with the state-of-the-art. Additionally, we propose a novel, lightweight neural network for optical flow estimation that achieves high speed inference with only a minor drop in performance.",0
"Event cameras are a type of vision sensor that captures brightness changes with high temporal resolution and low latency in an asynchronous manner. The resulting stream of events is highly valuable for motion estimation, but researchers have also focused on reconstructing intensity frames from these events to connect with existing computer vision literature. Recent studies have used neural networks trained on synthetic data to solve this problem. However, this work takes a self-supervised learning approach, leveraging knowledge of event camera operations to train neural networks without relying on ground-truth or synthetic data. The proposed method combines optical flow estimation and event-based photometric constancy to achieve state-of-the-art performance across multiple datasets. A novel, lightweight neural network for optical flow estimation is also proposed, which maintains high speed inference with minimal loss in performance.",1
"This paper deals with the scarcity of data for training optical flow networks, highlighting the limitations of existing sources such as labeled synthetic datasets or unlabeled real videos. Specifically, we introduce a framework to generate accurate ground-truth optical flow annotations quickly and in large amounts from any readily available single real picture. Given an image, we use an off-the-shelf monocular depth estimation network to build a plausible point cloud for the observed scene. Then, we virtually move the camera in the reconstructed environment with known motion vectors and rotation angles, allowing us to synthesize both a novel view and the corresponding optical flow field connecting each pixel in the input image to the one in the new frame. When trained with our data, state-of-the-art optical flow networks achieve superior generalization to unseen real data compared to the same models trained either on annotated synthetic datasets or unlabeled videos, and better specialization if combined with synthetic images.",0
"The main focus of this paper is the lack of available data for optical flow network training. It outlines the limitations of current sources, such as labeled synthetic datasets or unlabeled real videos. The paper presents a framework that can generate accurate optical flow annotations in large amounts quickly from a single real picture. The process involves using a monocular depth estimation network to create a point cloud of the scene and simulating camera movement with known motion vectors and rotation angles. This results in a novel view and corresponding optical flow field connecting each pixel in the input image to the new frame. By training optical flow networks with this data, they demonstrate improved generalization to unseen real data compared to models trained on annotated synthetic datasets or unlabeled videos. Combining synthetic images with this data also leads to better specialization.",1
"We present an unsupervised optical flow estimation method by proposing an adaptive pyramid sampling in the deep pyramid network. Specifically, in the pyramid downsampling, we propose an Content Aware Pooling (CAP) module, which promotes local feature gathering by avoiding cross region pooling, so that the learned features become more representative. In the pyramid upsampling, we propose an Adaptive Flow Upsampling (AFU) module, where cross edge interpolation can be avoided, producing sharp motion boundaries. Equipped with these two modules, our method achieves the best performance for unsupervised optical flow estimation on multiple leading benchmarks, including MPI-SIntel, KITTI 2012 and KITTI 2015. Particuarlly, we achieve EPE=1.5 on KITTI 2012 and F1=9.67% KITTI 2015, which outperform the previous state-of-the-art methods by 16.7% and 13.1%, respectively.",0
"Our unsupervised optical flow estimation method utilizes an adaptive pyramid sampling approach within the deep pyramid network. To improve local feature gathering during pyramid downsampling, we introduce the Content Aware Pooling (CAP) module, which avoids cross region pooling and enhances feature representativeness. Additionally, to produce sharp motion boundaries during pyramid upsampling, we propose the Adaptive Flow Upsampling (AFU) module, which eliminates cross edge interpolation. Our method outperforms previous state-of-the-art methods on several leading benchmarks, including MPI-SIntel, KITTI 2012 and KITTI 2015. Notably, we achieve a EPE of 1.5 on KITTI 2012 and F1 of 9.67% on KITTI 2015, surpassing previous methods by 16.7% and 13.1%, respectively.",1
"Video inpainting aims to fill spatio-temporal ""corrupted"" regions with plausible content. To achieve this goal, it is necessary to find correspondences from neighbouring frames to faithfully hallucinate the unknown content. Current methods achieve this goal through attention, flow-based warping, or 3D temporal convolution. However, flow-based warping can create artifacts when optical flow is not accurate, while temporal convolution may suffer from spatial misalignment. We propose 'Progressive Temporal Feature Alignment Network', which progressively enriches features extracted from the current frame with the feature warped from neighbouring frames using optical flow. Our approach corrects the spatial misalignment in the temporal feature propagation stage, greatly improving visual quality and temporal consistency of the inpainted videos. Using the proposed architecture, we achieve state-of-the-art performance on the DAVIS and FVI datasets compared to existing deep learning approaches. Code is available at https://github.com/MaureenZOU/TSAM.",0
"The objective of video inpainting is to replace corrupt regions in video with believable content. In order to achieve this, it is necessary to locate corresponding regions in adjacent frames to realistically generate the missing content. Different methods such as attention, flow-based warping, and 3D temporal convolution have been used to achieve this goal. However, flow-based warping may produce flaws when the optical flow is inaccurate, while spatial misalignment may be a problem for temporal convolution. To tackle these issues, we have introduced the 'Progressive Temporal Feature Alignment Network'. This method gradually enhances features obtained from the current frame by using features from neighboring frames that are transformed using optical flow. Our approach rectifies the spatial misalignment during the temporal feature propagation stage, leading to a remarkable improvement in the visual quality and temporal consistency of the inpainted videos. The proposed architecture has demonstrated superior performance compared to other deep learning techniques on the DAVIS and FVI datasets. The code is available at https://github.com/MaureenZOU/TSAM.",1
"We propose an unsupervised method for detecting and tracking moving objects in 3D, in unlabelled RGB-D videos. The method begins with classic handcrafted techniques for segmenting objects using motion cues: we estimate optical flow and camera motion, and conservatively segment regions that appear to be moving independently of the background. Treating these initial segments as pseudo-labels, we learn an ensemble of appearance-based 2D and 3D detectors, under heavy data augmentation. We use this ensemble to detect new instances of the ""moving"" type, even if they are not moving, and add these as new pseudo-labels. Our method is an expectation-maximization algorithm, where in the expectation step we fire all modules and look for agreement among them, and in the maximization step we re-train the modules to improve this agreement. The constraint of ensemble agreement helps combat contamination of the generated pseudo-labels (during the E step), and data augmentation helps the modules generalize to yet-unlabelled data (during the M step). We compare against existing unsupervised object discovery and tracking methods, using challenging videos from CATER and KITTI, and show strong improvements over the state-of-the-art.",0
"Our proposed method utilizes unsupervised techniques to detect and track moving objects in unlabelled RGB-D videos. Initially, we apply traditional handcrafted methods to segment objects based on motion cues by estimating optical flow and camera motion and identifying regions that appear to be moving independently from the background. These initial segments serve as pseudo-labels, which are used to train an ensemble of appearance-based 2D and 3D detectors through extensive data augmentation. Our algorithm is an expectation-maximization approach where all modules are activated to identify agreement in the expectation step, and re-training is conducted in the maximization step to improve this agreement. The constraint of ensemble agreement helps counteract contamination of the generated pseudo-labels during the expectation step, while data augmentation assists the modules in generalizing to unlabelled data during the maximization step. We compare our method to existing unsupervised object discovery and tracking techniques using challenging videos from CATER and KITTI and demonstrate significant enhancements over the current state-of-the-art.",1
"We address the problem of scene flow: given a pair of stereo or RGB-D video frames, estimate pixelwise 3D motion. We introduce RAFT-3D, a new deep architecture for scene flow. RAFT-3D is based on the RAFT model developed for optical flow but iteratively updates a dense field of pixelwise SE3 motion instead of 2D motion. A key innovation of RAFT-3D is rigid-motion embeddings, which represent a soft grouping of pixels into rigid objects. Integral to rigid-motion embeddings is Dense-SE3, a differentiable layer that enforces geometric consistency of the embeddings. Experiments show that RAFT-3D achieves state-of-the-art performance. On FlyingThings3D, under the two-view evaluation, we improved the best published accuracy (d < 0.05) from 34.3% to 83.7%. On KITTI, we achieve an error of 5.77, outperforming the best published method (6.31), despite using no object instance supervision. Code is available at https://github.com/princeton-vl/RAFT-3D.",0
"Our focus is on scene flow, specifically, the estimation of 3D motion for each pixel in a pair of stereo or RGB-D video frames. To achieve this, we have developed a new deep architecture called RAFT-3D. While based on the RAFT model for optical flow, RAFT-3D updates a dense field of pixelwise SE3 motion instead of 2D motion. This is made possible through the use of rigid-motion embeddings, which softly group pixels into rigid objects. To ensure geometric consistency of these embeddings, we employ the differentiable Dense-SE3 layer. Our experiments demonstrate that RAFT-3D outperforms existing methods, achieving state-of-the-art accuracy. For instance, on FlyingThings3D, we improved the best published accuracy (d < 0.05) from 34.3% to 83.7%. On KITTI, we achieved an error of 5.77, surpassing the best published method (6.31), despite not using object instance supervision. For those interested, the code is available at https://github.com/princeton-vl/RAFT-3D.",1
"While single-image super-resolution (SISR) has attracted substantial interest in recent years, the proposed approaches are limited to learning image priors in order to add high frequency details. In contrast, multi-frame super-resolution (MFSR) offers the possibility of reconstructing rich details by combining signal information from multiple shifted images. This key advantage, along with the increasing popularity of burst photography, have made MFSR an important problem for real-world applications.   We propose a novel architecture for the burst super-resolution task. Our network takes multiple noisy RAW images as input, and generates a denoised, super-resolved RGB image as output. This is achieved by explicitly aligning deep embeddings of the input frames using pixel-wise optical flow. The information from all frames are then adaptively merged using an attention-based fusion module. In order to enable training and evaluation on real-world data, we additionally introduce the BurstSR dataset, consisting of smartphone bursts and high-resolution DSLR ground-truth. We perform comprehensive experimental analysis, demonstrating the effectiveness of the proposed architecture.",0
"Although single-image super-resolution (SISR) has garnered significant interest, its approaches are limited to learning image priors to add high frequency details. On the other hand, multi-frame super-resolution (MFSR) offers the potential to reconstruct intricate details by combining signal information from multiple shifted images. This advantage, coupled with the growing popularity of burst photography, has made MFSR a crucial issue for practical applications. To address this, we propose a new architecture for burst super-resolution, which takes multiple noisy RAW images as input and generates a denoised, super-resolved RGB image as output. Our approach involves aligning deep embeddings of the input frames using pixel-wise optical flow and merging information from all frames using an attention-based fusion module. To facilitate real-world data training and evaluation, we introduce the BurstSR dataset, which features smartphone bursts and high-resolution DSLR ground-truth. Our comprehensive experimental analysis showcases the effectiveness of our proposed architecture.",1
"Heart beat rhythm and heart rate (HR) are important physiological parameters of the human body. This study presents an efficient multi-hierarchical spatio-temporal convolutional network that can quickly estimate remote physiological (rPPG) signal and HR from face video clips. First, the facial color distribution characteristics are extracted using a low-level face feature Generation (LFFG) module. Then, the three-dimensional (3D) spatio-temporal stack convolution module (STSC) and multi-hierarchical feature fusion module (MHFF) are used to strengthen the spatio-temporal correlation of multi-channel features. In the MHFF, sparse optical flow is used to capture the tiny motion information of faces between frames and generate a self-adaptive region of interest (ROI) skin mask. Finally, the signal prediction module (SP) is used to extract the estimated rPPG signal. The experimental results on the three datasets show that the proposed network outperforms the state-of-the-art methods.",0
"The human body's physiological parameters, including heart beat rhythm and heart rate (HR), are crucial. This research introduces a highly effective multi-hierarchical spatio-temporal convolutional network that rapidly calculates remote physiological (rPPG) signal and HR from facial video clips. Initially, the low-level face feature Generation (LFFG) module extracts facial color distribution characteristics. Then, the spatio-temporal stack convolution module (STSC) and multi-hierarchical feature fusion module (MHFF) enhance the spatio-temporal correlation of multi-channel features. The MHFF employs sparse optical flow to capture slight motion information of faces between frames, creating a self-adaptive region of interest (ROI) skin mask. Lastly, the signal prediction module (SP) extracts the estimated rPPG signal. Experimental results from three datasets demonstrate that the proposed network surpasses state-of-the-art techniques.",1
"State-of-the-art neural network models for optical flow estimation require a dense correlation volume at high resolutions for representing per-pixel displacement. Although the dense correlation volume is informative for accurate estimation, its heavy computation and memory usage hinders the efficient training and deployment of the models. In this paper, we show that the dense correlation volume representation is redundant and accurate flow estimation can be achieved with only a fraction of elements in it. Based on this observation, we propose an alternative displacement representation, named Sparse Correlation Volume, which is constructed directly by computing the k closest matches in one feature map for each feature vector in the other feature map and stored in a sparse data structure. Experiments show that our method can reduce computational cost and memory use significantly, while maintaining high accuracy compared to previous approaches with dense correlation volumes. Code is available at https://github.com/zacjiang/scv .",0
"Advanced neural network models used to estimate optical flow require a dense correlation volume with high resolution in order to accurately represent per-pixel displacement. However, this leads to heavy computation and memory usage, causing the models to be inefficient during training and deployment. This paper demonstrates that accurate flow estimation can be achieved with only a fraction of elements in the dense correlation volume, rendering it redundant. As a result, an alternative displacement representation called Sparse Correlation Volume is proposed, which is created by directly computing the k closest matches in one feature map for each feature vector in the other feature map and stored in a sparse data structure. Experimental results show that this new approach significantly reduces computational cost and memory usage, while maintaining high accuracy compared to previous methods that utilize dense correlation volumes. The source code can be found at https://github.com/zacjiang/scv.",1
"The feature correlation layer serves as a key neural network module in numerous computer vision problems that involve dense correspondences between image pairs. It predicts a correspondence volume by evaluating dense scalar products between feature vectors extracted from pairs of locations in two images. However, this point-to-point feature comparison is insufficient when disambiguating multiple similar regions in an image, severely affecting the performance of the end task. We propose GOCor, a fully differentiable dense matching module, acting as a direct replacement to the feature correlation layer. The correspondence volume generated by our module is the result of an internal optimization procedure that explicitly accounts for similar regions in the scene. Moreover, our approach is capable of effectively learning spatial matching priors to resolve further matching ambiguities. We analyze our GOCor module in extensive ablative experiments. When integrated into state-of-the-art networks, our approach significantly outperforms the feature correlation layer for the tasks of geometric matching, optical flow, and dense semantic matching. The code and trained models will be made available at github.com/PruneTruong/GOCor.",0
"The feature correlation layer is a crucial module in computer vision problems that require dense correspondences between image pairs. It predicts a correspondence volume by evaluating dense scalar products between feature vectors from two images. However, this method fails to differentiate between multiple similar regions in an image, leading to poor performance in the final task. To address this issue, we introduce GOCor, a fully differentiable dense matching module that directly replaces the feature correlation layer. Our module generates a correspondence volume using an internal optimization process that takes similar regions into account. Furthermore, our approach can learn spatial matching priors to resolve additional ambiguities. We extensively analyze our GOCor module through ablative experiments and show that it outperforms the feature correlation layer for tasks such as geometric matching, optical flow, and dense semantic matching when integrated into state-of-the-art networks. The code and trained models are available on github.com/PruneTruong/GOCor.",1
"Establishing dense correspondences between a pair of images is an important and general problem, covering geometric matching, optical flow and semantic correspondences. While these applications share fundamental challenges, such as large displacements, pixel-accuracy, and appearance changes, they are currently addressed with specialized network architectures, designed for only one particular task. This severely limits the generalization capabilities of such networks to new scenarios, where e.g. robustness to larger displacements or higher accuracy is required.   In this work, we propose a universal network architecture that is directly applicable to all the aforementioned dense correspondence problems. We achieve both high accuracy and robustness to large displacements by investigating the combined use of global and local correlation layers. We further propose an adaptive resolution strategy, allowing our network to operate on virtually any input image resolution. The proposed GLU-Net achieves state-of-the-art performance for geometric and semantic matching as well as optical flow, when using the same network and weights. Code and trained models are available at https://github.com/PruneTruong/GLU-Net.",0
"The establishment of dense correspondences between a pair of images is a significant task that encompasses geometric matching, optical flow, and semantic correspondences. Although these applications share common difficulties such as large displacements, pixel accuracy, and appearance changes, they are currently tackled using specialized network architectures that are designed for specific tasks. This limits the networks' ability to generalize to new situations where, for example, robustness to larger displacements or higher accuracy is necessary. This study proposes a universal network architecture that can address all the aforementioned dense correspondence problems. By combining global and local correlation layers, we achieve high accuracy and robustness to large displacements. Additionally, we suggest an adaptive resolution strategy that enables our network to operate on any input image resolution. GLU-Net, the proposed model, achieves state-of-the-art performance in geometric and semantic matching and optical flow, using the same network and weights. We have made the code and trained models available at https://github.com/PruneTruong/GLU-Net.",1
"Semi-supervised video object segmentation (semi-VOS) is widely used in many applications. This task is tracking class-agnostic objects from a given target mask. For doing this, various approaches have been developed based on online-learning, memory networks, and optical flow. These methods show high accuracy but are hard to be utilized in real-world applications due to slow inference time and tremendous complexity. To resolve this problem, template matching methods are devised for fast processing speed but sacrificing lots of performance in previous models. We introduce a novel semi-VOS model based on a template matching method and a temporal consistency loss to reduce the performance gap from heavy models while expediting inference time a lot. Our template matching method consists of short-term and long-term matching. The short-term matching enhances target object localization, while long-term matching improves fine details and handles object shape-changing through the newly proposed adaptive template attention module. However, the long-term matching causes error-propagation due to the inflow of the past estimated results when updating the template. To mitigate this problem, we also propose a temporal consistency loss for better temporal coherence between neighboring frames by adopting the concept of a transition matrix. Our model obtains 79.5% J&F score at the speed of 73.8 FPS on the DAVIS16 benchmark. The code is available in https://github.com/HYOJINPARK/TTVOS.",0
"Semi-VOS is a commonly used technique for video object segmentation. It involves tracking objects that are class-agnostic from a given target mask and has been approached using online-learning, memory networks, and optical flow. However, these methods are complex and slow, making them impractical for real-world applications. To address this issue, template matching methods have been developed. Although they are faster, they lack the performance of previous models. Our team introduces a novel semi-VOS model that combines template matching with a temporal consistency loss to improve performance while maintaining fast inference times. Our template matching method includes short-term and long-term matching, with the latter utilizing an adaptive template attention module to handle object shape-changing. However, long-term matching can cause error-propagation, which we mitigate with a temporal consistency loss based on a transition matrix. Our model achieves a J&F score of 79.5% and runs at 73.8 FPS on the DAVIS16 benchmark. Code for our model is available at https://github.com/HYOJINPARK/TTVOS.",1
"Video interpolation aims to generate a non-existent intermediate frame given the past and future frames. Many state-of-the-art methods achieve promising results by estimating the optical flow between the known frames and then generating the backward flows between the middle frame and the known frames. However, these methods usually suffer from the inaccuracy of estimated optical flows and require additional models or information to compensate for flow estimation errors. Following the recent development in using deformable convolution (DConv) for video interpolation, we propose a light but effective model, called Pyramid Deformable Warping Network (PDWN). PDWN uses a pyramid structure to generate DConv offsets of the unknown middle frame with respect to the known frames through coarse-to-fine successive refinements. Cost volumes between warped features are calculated at every pyramid level to help the offset inference. At the finest scale, the two warped frames are adaptively blended to generate the middle frame. Lastly, a context enhancement network further enhances the contextual detail of the final output. Ablation studies demonstrate the effectiveness of the coarse-to-fine offset refinement, cost volumes, and DConv. Our method achieves better or on-par accuracy compared to state-of-the-art models on multiple datasets while the number of model parameters and the inference time are substantially less than previous models. Moreover, we present an extension of the proposed framework to use four input frames, which can achieve significant improvement over using only two input frames, with only a slight increase in the model size and inference time.",0
"The goal of video interpolation is to create a new frame between two existing frames. While some methods have been successful in estimating the optical flow between frames and generating backward flows, they often suffer from inaccuracies and require additional models to compensate for errors. To address these issues, we propose the Pyramid Deformable Warping Network (PDWN), which uses a pyramid structure to generate Deformable Convolution (DConv) offsets for the middle frame. The PDWN also includes cost volume calculations and adaptive blending to generate the final output. Our approach achieves comparable or better accuracy than previous models while using fewer parameters and less inference time. We also present an extension of our framework using four input frames, which results in significant improvement with only a slight increase in model size and inference time.",1
"We present a deep neural network (DNN) that uses both sensor data (gyroscope) and image content (optical flow) to stabilize videos through unsupervised learning. The network fuses optical flow with real/virtual camera pose histories into a joint motion representation. Next, the LSTM block infers the new virtual camera pose, and this virtual pose is used to generate a warping grid that stabilizes the frame. Novel relative motion representation as well as a multi-stage training process are presented to optimize our model without any supervision. To the best of our knowledge, this is the first DNN solution that adopts both sensor data and image for stabilization. We validate the proposed framework through ablation studies and demonstrated the proposed method outperforms the state-of-art alternative solutions via quantitative evaluations and a user study.",0
"Our study introduces a DNN that applies unsupervised learning to stabilize videos using both gyroscope sensor data and optical flow image content. The DNN combines optical flow with camera pose histories to create a joint motion representation. The LSTM block then predicts a new virtual camera pose, which is used to generate a warping grid that stabilizes the frame. We introduce a novel relative motion representation and a multi-stage training process to optimize the model without supervision. This is the first DNN solution to utilize both sensor data and image for stabilization. Our proposed framework is evaluated through ablation studies, quantitative evaluations, and a user study, all of which demonstrate that our method outperforms existing alternative solutions.",1
"Abnormal event detection is a challenging task that requires effectively handling intricate features of appearance and motion. In this paper, we present an approach of detecting anomalies in videos by learning a novel LSTM based self-contained network on normal dense optical flow. Due to their sigmoid implementations, standard LSTM's forget gate is susceptible to overlooking and dismissing relevant content in long sequence tasks like abnormality detection. The forget gate mitigates participation of previous hidden state for computation of cell state prioritizing current input. In addition, the hyperbolic tangent activation of standard LSTMs sacrifices performance when a network gets deeper. To tackle these two limitations, we introduce a bi-gated, light LSTM cell by discarding the forget gate and introducing sigmoid activation. Specifically, the LSTM architecture we come up with fully sustains content from previous hidden state thereby enabling the trained model to be robust and make context-independent decision during evaluation. Removing the forget gate results in a simplified and undemanding LSTM cell with improved performance effectiveness and computational efficiency. Empirical evaluations show that the proposed bi-gated LSTM based network outperforms various LSTM based models verifying its effectiveness for abnormality detection and generalization tasks on CUHK Avenue and UCSD datasets.",0
"Detecting abnormal events in videos is a complex task that involves managing intricate appearance and motion features. In this article, we introduce a new method for identifying anomalies in videos by teaching a novel LSTM self-contained network to learn from normal dense optical flow. Standard LSTM's forget gate can overlook and disregard relevant content in long sequence tasks like detecting abnormalities because of its sigmoid implementation. The forget gate restricts the previous hidden state's involvement in computing the cell state, prioritizing the current input. Additionally, the hyperbolic tangent activation of standard LSTMs reduces performance as the network gets deeper. To overcome these limitations, we propose a bi-gated, light LSTM cell that eliminates the forget gate and introduces sigmoid activation. The LSTM architecture we develop fully retains content from the previous hidden state, allowing the trained model to be resilient and make context-independent decisions during evaluation. By removing the forget gate, we create a simplified and less demanding LSTM cell with improved performance effectiveness and computational efficiency. Our empirical evaluations show that the proposed bi-gated LSTM-based network surpasses various LSTM-based models, demonstrating its effectiveness for abnormality detection and generalization tasks on CUHK Avenue and UCSD datasets.",1
"The estimation of optical flow is an ambiguous task due to the lack of correspondence at occlusions, shadows, reflections, lack of texture and changes in illumination over time. Thus, unsupervised methods face major challenges as they need to tune complex cost functions with several terms designed to handle each of these sources of ambiguity. In contrast, supervised methods avoid these challenges altogether by relying on explicit ground truth optical flow obtained directly from synthetic or real data. In the case of synthetic data, the ground truth provides an exact and explicit description of what optical flow to assign to a given scene. However, the domain gap between synthetic data and real data often limits the ability of a trained network to generalize. In the case of real data, the ground truth is obtained through multiple sensors and additional data processing, which might introduce persistent errors and contaminate it. As a solution to these issues, we introduce a novel method to build a training set of pseudo-real images that can be used to train optical flow in a supervised manner. Our dataset uses two unpaired frames from real data and creates pairs of frames by simulating random warps, occlusions with super-pixels, shadows and illumination changes, and associates them to their corresponding exact optical flow. We thus obtain the benefit of directly training on real data while having access to an exact ground truth. Training with our datasets on the Sintel and KITTI benchmarks is straightforward and yields models on par or with state of the art performance compared to much more sophisticated training approaches.",0
"The task of estimating optical flow is challenging due to various sources of ambiguity, such as occlusions, shadows, reflections, lack of texture, and changes in illumination over time. Unsupervised methods face significant difficulties in dealing with these complexities since they require tuning complex cost functions with several terms to handle each source of ambiguity individually. On the other hand, supervised methods rely on explicit ground truth optical flow obtained from synthetic or real data to avoid these challenges. Although synthetic data provides an exact description of optical flow, the domain gap between synthetic and real data limits the generalization ability of trained networks. Similarly, obtaining ground truth from real data through multiple sensors and processing can introduce persistent errors. To address these issues, we propose a novel method of constructing a training set of pseudo-real images that can be used to train optical flow in a supervised manner. Our dataset simulates various distortions on unpaired real data frames and associates them with their corresponding ground truth optical flow. This approach allows us to train on real data while having access to exact ground truth. Using our dataset, we achieved state-of-the-art performance on the Sintel and KITTI benchmarks, which is comparable to more complex training approaches.",1
"Establishing dense correspondences between a pair of images is an important and general problem. However, dense flow estimation is often inaccurate in the case of large displacements or homogeneous regions. For most applications and down-stream tasks, such as pose estimation, image manipulation, or 3D reconstruction, it is crucial to know when and where to trust the estimated matches.   In this work, we aim to estimate a dense flow field relating two images, coupled with a robust pixel-wise confidence map indicating the reliability and accuracy of the prediction. We develop a flexible probabilistic approach that jointly learns the flow prediction and its uncertainty. In particular, we parametrize the predictive distribution as a constrained mixture model, ensuring better modelling of both accurate flow predictions and outliers. Moreover, we develop an architecture and training strategy tailored for robust and generalizable uncertainty prediction in the context of self-supervised training. Our approach obtains state-of-the-art results on multiple challenging geometric matching and optical flow datasets. We further validate the usefulness of our probabilistic confidence estimation for the task of pose estimation. Code and models are available at https://github.com/PruneTruong/PDCNet.",0
"The problem of establishing dense correspondences between two images is crucial, but it is often inaccurate in cases of significant displacements or uniform regions. Therefore, it is important to know when and where to rely on estimated matches for downstream tasks like pose estimation, 3D reconstruction, or image manipulation. Our aim is to estimate a dense flow field between two images while also providing a reliable pixel-wise confidence map showing the accuracy and reliability of the prediction. We have developed a flexible probabilistic approach that learns the flow prediction and its uncertainty together. Our approach includes a constrained mixture model that models accurate flow predictions and outliers, and an architecture and training strategy that are tailored for robust and generalizable uncertainty prediction via self-supervised training. Our approach achieves state-of-the-art results on multiple geometric matching and optical flow datasets, and we demonstrate the usefulness of our probabilistic confidence estimation for the task of pose estimation. Our code and models are available at https://github.com/PruneTruong/PDCNet.",1
"Two-view structure-from-motion (SfM) is the cornerstone of 3D reconstruction and visual SLAM. Existing deep learning-based approaches formulate the problem by either recovering absolute pose scales from two consecutive frames or predicting a depth map from a single image, both of which are ill-posed problems. In contrast, we propose to revisit the problem of deep two-view SfM by leveraging the well-posedness of the classic pipeline. Our method consists of 1) an optical flow estimation network that predicts dense correspondences between two frames; 2) a normalized pose estimation module that computes relative camera poses from the 2D optical flow correspondences, and 3) a scale-invariant depth estimation network that leverages epipolar geometry to reduce the search space, refine the dense correspondences, and estimate relative depth maps. Extensive experiments show that our method outperforms all state-of-the-art two-view SfM methods by a clear margin on KITTI depth, KITTI VO, MVS, Scenes11, and SUN3D datasets in both relative pose and depth estimation.",0
"The primary method for achieving 3D reconstruction and visual SLAM is through two-view structure-from-motion (SfM). However, current deep learning-based approaches are flawed, as they attempt to solve ill-posed problems by either recovering absolute pose scales or predicting a depth map from a single image. Our proposal is to revisit deep two-view SfM by using the well-posedness of the classic pipeline. Our approach involves an optical flow estimation network to predict dense correspondences, a normalized pose estimation module to compute relative camera poses, and a scale-invariant depth estimation network to refine the dense correspondences, estimate relative depth maps and reduce the search space by leveraging epipolar geometry. Our method has been extensively tested and has outperformed all state-of-the-art two-view SfM methods on KITTI depth, KITTI VO, MVS, Scenes11, and SUN3D datasets in both relative pose and depth estimation.",1
"The cost volume, capturing the similarity of possible correspondences across two input images, is a key ingredient in state-of-the-art optical flow approaches. When sampling for correspondences to build the cost volume, a large neighborhood radius is required to deal with large displacements, introducing a significant computational burden. To address this, a sequential strategy is usually adopted, where correspondence sampling in a local neighborhood with a small radius suffices. However, such sequential approaches, instantiated by either a pyramid structure over a deep neural network's feature hierarchy or by a recurrent neural network, are slow due to the inherent need for sequential processing of cost volumes. In this paper, we propose dilated cost volumes to capture small and large displacements simultaneously, allowing optical flow estimation without the need for the sequential estimation strategy. To process the cost volume to get pixel-wise optical flow, existing approaches employ 2D or separable 4D convolutions, which we show either suffer from high GPU memory consumption, inferior accuracy, or large model size. Therefore, we propose using 3D convolutions for cost volume filtering to address these issues. By combining the dilated cost volumes and 3D convolutions, our proposed model DCVNet not only exhibits real-time inference (71 fps on a mid-end 1080ti GPU) but is also compact and obtains comparable accuracy to existing approaches.",0
"Optical flow approaches rely on the cost volume to determine possible correspondences between two input images. However, when sampling for correspondences, a large neighborhood radius is necessary to handle large displacements, which can be computationally burdensome. To address this, sequential strategies are typically used, but they are slow due to the need for sequential processing. In this study, we introduce dilated cost volumes that can capture small and large displacements at the same time, eliminating the need for sequential estimation. Existing approaches use 2D or separable 4D convolutions to process the cost volume to obtain pixel-wise optical flow, but we propose using 3D convolutions to address issues such as high GPU memory consumption, inferior accuracy, or large model size. Our proposed model, DCVNet, combines dilated cost volumes and 3D convolutions, resulting in real-time inference and comparable accuracy to existing approaches.",1
"Denoisers trained with synthetic data often fail to cope with the diversity of unknown noises, giving way to methods that can adapt to existing noise without knowing its ground truth. Previous image-based method leads to noise overfitting if directly applied to video denoisers, and has inadequate temporal information management especially in terms of occlusion and lighting variation, which considerably hinders its denoising performance. In this paper, we propose a general framework for video denoising networks that successfully addresses these challenges. A novel twin sampler assembles training data by decoupling inputs from targets without altering semantics, which not only effectively solves the noise overfitting problem, but also generates better occlusion masks efficiently by checking optical flow consistency. An online denoising scheme and a warping loss regularizer are employed for better temporal alignment. Lighting variation is quantified based on the local similarity of aligned frames. Our method consistently outperforms the prior art by 0.6-3.2dB PSNR on multiple noises, datasets and network architectures. State-of-the-art results on reducing model-blind video noises are achieved. Extensive ablation studies are conducted to demonstrate the significance of each technical components.",0
"Denoisers trained using synthetic data often struggle to deal with unknown noises, which has led to the development of methods that can adapt to existing noise without knowledge of its ground truth. Prior image-based methods have proven ineffective when applied directly to video denoisers due to noise overfitting and inadequate management of temporal information, particularly in terms of occlusion and lighting variation. These challenges are successfully addressed in this paper through a general framework for video denoising networks. A novel twin sampler is utilized to decouple inputs from targets and generate better occlusion masks efficiently by checking optical flow consistency. An online denoising scheme and a warping loss regularizer are employed for better temporal alignment, while lighting variation is quantified based on the local similarity of aligned frames. Our method consistently outperforms prior art by 0.6-3.2dB PSNR on multiple noises, datasets, and network architectures, achieving state-of-the-art results on reducing model-blind video noises. Extensive ablation studies are conducted to demonstrate the significance of each technical component.",1
"Most successful self-supervised learning methods are trained to align the representations of two independent views from the data. State-of-the-art methods in video are inspired by image techniques, where these two views are similarly extracted by cropping and augmenting the resulting crop. However, these methods miss a crucial element in the video domain: time. We introduce BraVe, a self-supervised learning framework for video. In BraVe, one of the views has access to a narrow temporal window of the video while the other view has a broad access to the video content. Our models learn to generalise from the narrow view to the general content of the video. Furthermore, BraVe processes the views with different backbones, enabling the use of alternative augmentations or modalities into the broad view such as optical flow, randomly convolved RGB frames, audio or their combinations. We demonstrate that BraVe achieves state-of-the-art results in self-supervised representation learning on standard video and audio classification benchmarks including UCF101, HMDB51, Kinetics, ESC-50 and AudioSet.",0
"The majority of successful self-supervised learning approaches are trained to align the representations of two independent views from the data. In video, state-of-the-art methods are modeled after image techniques, where the two views are obtained by cropping and augmenting the resulting crop in a similar way. However, these methods neglect a crucial aspect of the video domain, namely time. Our proposed framework, BraVe, addresses this issue by incorporating a narrow temporal window of the video in one view and a broad access to the video content in the other. Our models learn to generalize from the narrow view to the overall content of the video. Additionally, BraVe employs distinct backbones for processing the views, allowing for alternative augmentations or modalities to be introduced into the broad view, such as optical flow, randomly convolved RGB frames, audio, or a combination of these. Our results demonstrate that BraVe outperforms previous state-of-the-art methods in self-supervised representation learning on well-known video and audio classification benchmarks, including UCF101, HMDB51, Kinetics, ESC-50, and AudioSet.",1
"Temporal action localization (TAL) is a fundamental yet challenging task in video understanding. Existing TAL methods rely on pre-training a video encoder through action classification supervision. This results in a task discrepancy problem for the video encoder -- trained for action classification, but used for TAL. Intuitively, end-to-end model optimization is a good solution. However, this is not operable for TAL subject to the GPU memory constraints, due to the prohibitive computational cost in processing long untrimmed videos. In this paper, we resolve this challenge by introducing a novel low-fidelity end-to-end (LoFi) video encoder pre-training method. Instead of always using the full training configurations for TAL learning, we propose to reduce the mini-batch composition in terms of temporal, spatial or spatio-temporal resolution so that end-to-end optimization for the video encoder becomes operable under the memory conditions of a mid-range hardware budget. Crucially, this enables the gradient to flow backward through the video encoder from a TAL loss supervision, favourably solving the task discrepancy problem and providing more effective feature representations. Extensive experiments show that the proposed LoFi pre-training approach can significantly enhance the performance of existing TAL methods. Encouragingly, even with a lightweight ResNet18 based video encoder in a single RGB stream, our method surpasses two-stream ResNet50 based alternatives with expensive optical flow, often by a good margin.",0
"Video understanding involves a challenging task known as temporal action localization (TAL). Existing TAL methods pre-train a video encoder for action classification, leading to a task discrepancy problem for the encoder when used for TAL. While end-to-end model optimization seems intuitive, it is not feasible due to GPU memory constraints when processing long untrimmed videos. To overcome this challenge, we introduce a novel low-fidelity end-to-end (LoFi) video encoder pre-training method. By reducing mini-batch composition in terms of temporal, spatial or spatio-temporal resolution, we enable end-to-end optimization for the video encoder under memory constraints. This allows the gradient to flow backward through the video encoder from a TAL loss supervision, solving the task discrepancy problem and providing more effective feature representations. Our experiments demonstrate that our LoFi pre-training approach can enhance the performance of existing TAL methods, surpassing even two-stream ResNet50 based alternatives with expensive optical flow using a lightweight ResNet18 based video encoder in a single RGB stream.",1
"Recent work demonstrated the lack of robustness of optical flow networks to physical, patch-based adversarial attacks. The possibility to physically attack a basic component of automotive systems is a reason for serious concerns. In this paper, we analyze the cause of the problem and show that the lack of robustness is rooted in the classical aperture problem of optical flow estimation in combination with bad choices in the details of the network architecture. We show how these mistakes can be rectified in order to make optical flow networks robust to physical, patch-based attacks.",0
"A study has revealed that optical flow networks are not strong enough to withstand physical, patch-based adversarial attacks. This is a major concern since it means that an essential component of automotive systems can be attacked. The root cause of this vulnerability is attributed to the classical aperture problem of optical flow estimation and poor choices in the network architecture. This paper discusses how these errors can be corrected to enhance the resilience of optical flow networks against physical, patch-based attacks.",1
"In this paper, we focus on designing effective method for fast and accurate scene parsing. A common practice to improve the performance is to attain high resolution feature maps with strong semantic representation. Two strategies are widely used -- atrous convolutions and feature pyramid fusion, are either computation intensive or ineffective. Inspired by the Optical Flow for motion alignment between adjacent video frames, we propose a Flow Alignment Module (FAM) to learn Semantic Flow between feature maps of adjacent levels, and broadcast high-level features to high resolution features effectively and efficiently. Furthermore, integrating our module to a common feature pyramid structure exhibits superior performance over other real-time methods even on light-weight backbone networks, such as ResNet-18. Extensive experiments are conducted on several challenging datasets, including Cityscapes, PASCAL Context, ADE20K and CamVid. Especially, our network is the first to achieve 80.4\% mIoU on Cityscapes with a frame rate of 26 FPS. The code is available at \url{https://github.com/lxtGH/SFSegNets}.",0
"The aim of this research is to develop a fast and precise method for scene parsing. One common technique to enhance performance involves obtaining high resolution feature maps with robust semantic representation. However, the two currently utilized strategies, atrous convolutions and feature pyramid fusion, have notable drawbacks, such as being computationally intensive or ineffective. To address these limitations, we propose a Flow Alignment Module (FAM) inspired by Optical Flow for motion alignment between adjacent video frames. FAM learns Semantic Flow between feature maps of adjacent levels and effectively broadcasts high-level features to high-resolution features in a resource-efficient manner. Integrating our module to a common feature pyramid structure results in superior performance compared to other real-time methods, even on lightweight backbone networks such as ResNet-18. We conducted extensive experiments on several challenging datasets, including Cityscapes, PASCAL Context, ADE20K, and CamVid. Our network achieved 80.4% mIoU on Cityscapes with a frame rate of 26 FPS, making it the first to do so. Our code is available at \url{https://github.com/lxtGH/SFSegNets}.",1
"Despite learning-based visual odometry (VO) has shown impressive results in recent years, the pretrained networks may easily collapse in unseen environments. The large domain gap between training and testing data makes them difficult to generalize to new scenes. In this paper, we propose an online adaptation framework for deep VO with the assistance of scene-agnostic geometric computations and Bayesian inference. In contrast to learning-based pose estimation, our method solves pose from optical flow and depth while the single-view depth estimation is continuously improved with new observations by online learned uncertainties. Meanwhile, an online learned photometric uncertainty is used for further depth and pose optimization by a differentiable Gauss-Newton layer. Our method enables fast adaptation of deep VO networks to unseen environments in a self-supervised manner. Extensive experiments including Cityscapes to KITTI and outdoor KITTI to indoor TUM demonstrate that our method achieves state-of-the-art generalization ability among self-supervised VO methods.",0
"Although learning-based visual odometry (VO) has demonstrated impressive performance in recent years, pretrained networks may easily fail in unfamiliar environments due to the large domain gap between training and testing data. This makes it difficult for them to adapt to new scenes. To address this issue, we propose an online adaptation framework for deep VO that utilizes scene-agnostic geometric computations and Bayesian inference. Our approach differs from learning-based pose estimation as it solves pose from optical flow and depth, and continuously improves single-view depth estimation with new observations using online learned uncertainties. Additionally, a differentiable Gauss-Newton layer optimizes depth and pose through an online learned photometric uncertainty. Our method enables rapid adaptation of deep VO networks to unseen environments in a self-supervised manner. Extensive experiments, including Cityscapes to KITTI and outdoor KITTI to indoor TUM, demonstrate that our method outperforms other self-supervised VO methods and achieves state-of-the-art generalization ability.",1
"We present an approach for high-resolution video frame prediction by conditioning on both past frames and past optical flows. Previous approaches rely on resampling past frames, guided by a learned future optical flow, or on direct generation of pixels. Resampling based on flow is insufficient because it cannot deal with disocclusions. Generative models currently lead to blurry results. Recent approaches synthesis a pixel by convolving input patches with a predicted kernel. However, their memory requirement increases with kernel size. Here, we spatially-displaced convolution (SDC) module for video frame prediction. We learn a motion vector and a kernel for each pixel and synthesize a pixel by applying the kernel at a displaced location in the source image, defined by the predicted motion vector. Our approach inherits the merits of both vector-based and kernel-based approaches, while ameliorating their respective disadvantages. We train our model on 428K unlabelled 1080p video game frames. Our approach produces state-of-the-art results, achieving an SSIM score of 0.904 on high-definition YouTube-8M videos, 0.918 on Caltech Pedestrian videos. Our model handles large motion effectively and synthesizes crisp frames with consistent motion.",0
"We propose a method for predicting high-resolution video frames using a combination of past frames and optical flows. Existing methods either resample past frames based on future optical flows or generate pixels directly, resulting in blurry or insufficient results. Some recent approaches use a predicted kernel to synthesize a pixel by convolving input patches, but this method requires high memory usage. To overcome these limitations, we introduce a spatially-displaced convolution (SDC) module. Our approach learns a motion vector and a kernel for each pixel, which are used to synthesize a pixel by applying the kernel at a displaced location in the source image based on the predicted motion vector. Our method combines the advantages of both vector-based and kernel-based approaches while addressing their drawbacks. We trained our model on 428K unlabelled 1080p video game frames and achieved state-of-the-art results with an SSIM score of 0.904 on high-definition YouTube-8M videos and 0.918 on Caltech Pedestrian videos. Our approach effectively handles large motion and generates crisp frames with consistent motion.",1
"In this paper, we address the problem of estimating dense depth from a sequence of images using deep neural networks. Specifically, we employ a dense-optical-flow network to compute correspondences and then triangulate the point cloud to obtain an initial depth map.Parts of the point cloud, however, may be less accurate than others due to lack of common observations or small parallax. To further increase the triangulation accuracy, we introduce a depth-refinement network (DRN) that optimizes the initial depth map based on the image's contextual cues. In particular, the DRN contains an iterative refinement module (IRM) that improves the depth accuracy over iterations by refining the deep features. Lastly, the DRN also predicts the uncertainty in the refined depths, which is desirable in applications such as measurement selection for scene reconstruction. We show experimentally that our algorithm outperforms state-of-the-art approaches in terms of depth accuracy, and verify that our predicted uncertainty is highly correlated to the actual depth error.",0
"This paper focuses on the challenge of accurately estimating dense depth using deep neural networks from a sequence of images. We utilize a dense-optical-flow network to establish correspondences and generate an initial depth map by triangulating the point cloud. However, some areas of the point cloud may be less precise due to limited observations or minimal parallax. To improve the accuracy of the initial depth map, we introduce a depth-refinement network (DRN) that employs contextual cues from the image. The DRN includes an iterative refinement module (IRM) that enhances depth accuracy by refining deep features over iterations. Additionally, the DRN predicts uncertainty in the refined depths, which is useful in applications like scene reconstruction measurement selection. Our experimental results demonstrate that our algorithm surpasses state-of-the-art methods for depth accuracy, and we confirm that our predicted uncertainty correlates strongly with actual depth error.",1
"Video object detection is a fundamental problem in computer vision and has a wide spectrum of applications. Based on deep networks, video object detection is actively studied for pushing the limits of detection speed and accuracy. To reduce the computation cost, we sparsely sample key frames in video and treat the rest frames are non-key frames; a large and deep network is used to extract features for key frames and a tiny network is used for non-key frames. To enhance the features of non-key frames, we propose a novel short-term feature aggregation method to propagate the rich information in key frame features to non-key frame features in a fast way. The fast feature aggregation is enabled by the freely available motion cues in compressed videos. Further, key frame features are also aggregated based on optical flow. The propagated deep features are then integrated with the directly extracted features for object detection. The feature extraction and feature integration parameters are optimized in an end-to-end manner. The proposed video object detection network is evaluated on the large-scale ImageNet VID benchmark and achieves 77.2\% mAP, which is on-par with state-of-the-art accuracy, at the speed of 30 FPS using a Titan X GPU. The source codes are available at \url{https://github.com/hustvl/LSFA}.",0
"Video object detection is a crucial issue in computer vision with a broad range of practical applications. Researchers are actively exploring deep networks to improve detection speed and precision. To reduce the computation cost, key frames are selected and treated differently from non-key frames, with a large and deep network used to extract features for key frames and a tiny network used for non-key frames. To improve non-key frame features, a new short-term feature aggregation method is proposed to quickly propagate rich information from key frame features to non-key frame features. This is enabled by motion cues in compressed videos, and key frame features are also aggregated based on optical flow. The deep features are then integrated with directly extracted features for object detection, with feature extraction and integration parameters optimized in an end-to-end manner. The proposed network achieves 77.2% mAP on the ImageNet VID benchmark, comparable with state-of-the-art accuracy, at 30 FPS using a Titan X GPU. The source codes are available at \url{https://github.com/hustvl/LSFA}.",1
"Dense optical flow estimation plays a key role in many robotic vision tasks. In the past few years, with the advent of deep learning, we have witnessed great progress in optical flow estimation. However, current networks often consist of a large number of parameters and require heavy computation costs, largely hindering its application on low power-consumption devices such as mobile phones. In this paper, we tackle this challenge and design a lightweight model for fast and accurate optical flow prediction. Our proposed FastFlowNet follows the widely-used coarse-to-fine paradigm with following innovations. First, a new head enhanced pooling pyramid (HEPP) feature extractor is employed to intensify high-resolution pyramid features while reducing parameters. Second, we introduce a new center dense dilated correlation (CDDC) layer for constructing compact cost volume that can keep large search radius with reduced computation burden. Third, an efficient shuffle block decoder (SBD) is implanted into each pyramid level to accelerate flow estimation with marginal drops in accuracy. Experiments on both synthetic Sintel data and real-world KITTI datasets demonstrate the effectiveness of the proposed approach, which needs only 1/10 computation of comparable networks to achieve on par accuracy. In particular, FastFlowNet only contains 1.37M parameters; and can execute at 90 FPS (with a single GTX 1080Ti) or 5.7 FPS (embedded Jetson TX2 GPU) on a pair of Sintel images of resolution 1024x436.",0
"Robotic vision tasks heavily rely on dense optical flow estimation, and recent years have seen significant progress in this area due to deep learning. However, current networks are often complex and require high computational costs, making them unsuitable for low power-consumption devices like mobile phones. To address this issue, this paper proposes a lightweight model called FastFlowNet for fast and accurate optical flow prediction. The model uses a new head enhanced pooling pyramid feature extractor to intensify high-resolution pyramid features while reducing parameters and a new center dense dilated correlation layer to construct a compact cost volume that can keep a large search radius with reduced computation burden. Additionally, an efficient shuffle block decoder is implanted into each pyramid level to accelerate flow estimation with marginal drops in accuracy. Experiments on synthetic Sintel data and real-world KITTI datasets show that FastFlowNet achieves comparable accuracy to other networks but with only 1/10th of the computation needed. FastFlowNet contains 1.37M parameters and can execute at 90 FPS (with a single GTX 1080Ti) or 5.7 FPS (embedded Jetson TX2 GPU) on a pair of Sintel images of resolution 1024x436.",1
"One-sided facial paralysis causes uneven movements of facial muscles on the sides of the face. Physicians currently assess facial asymmetry in a subjective manner based on their clinical experience. This paper proposes a novel method to provide an objective and quantitative asymmetry score for frontal faces. Our metric has the potential to help physicians for diagnosis as well as monitoring the rehabilitation of patients with one-sided facial paralysis. A deep learning based landmark detection technique is used to estimate style invariant facial landmark points and dense optical flow is used to generate motion maps from a short sequence of frames. Six face regions are considered corresponding to the left and right parts of the forehead, eyes, and mouth. Motion is computed and compared between the left and the right parts of each region of interest to estimate the symmetry score. For testing, asymmetric sequences are synthetically generated from a facial expression dataset. A score equation is developed to quantify symmetry in both symmetric and asymmetric face sequences.",0
"The movement of facial muscles on one side of the face causes an imbalanced appearance, known as one-sided facial paralysis. Currently, physicians evaluate facial asymmetry subjectively relying on clinical experience. However, this paper presents a new approach for measuring asymmetry objectively and quantitatively on frontal faces. This method could aid physicians in diagnosing and monitoring the recovery of individuals with one-sided facial paralysis. The proposed technique utilizes deep learning-based landmark detection to estimate invariant facial landmark points and dense optical flow to create motion maps from a brief sequence of frames. The method focuses on six areas of the face, including the left and right portions of the forehead, eyes, and mouth, and calculates and compares motion between each of these regions to determine the symmetry score. To test this method, asymmetric sequences were synthetically generated from a facial expression dataset, and a score equation was created to measure symmetry in both symmetric and asymmetric face sequences.",1
"Surveillance anomaly detection searches for anomalous events, such as crimes or accidents, among normal scenes. Because it occurs rarely, most training data consists of unlabeled, normal videos, which makes the task challenging. Most existing methods use an autoencoder (AE) to learn reconstructing normal videos and detect anomalies by a failure to reconstruct the appearance of abnormal scenes. However, because anomalies are distinguished by appearance or motion, many previous approaches have explicitly separated appearance and motion information--for example, using a pre-trained optical flow model. This explicit separation restricts reciprocal representation capabilities between two information. In contrast, we propose an implicit two-path AE (ITAE), a structure in which two encoders implicitly model appearance and motion features, and a single decoder that combines them to learn normal video patterns. For the complex distribution of normal scenes, we suggest normal density estimation of ITAE features through normalizing flow (NF)-based generative models to learn the tractable likelihoods and find anomalies using out-of-distribution detection. NF models intensify ITAE performance by learning normality through implicitly learned features. Finally, we demonstrate the effectiveness of ITAE and its feature distribution modeling in three benchmarks, especially on the Shanghai Tech Campus (ST) database composed of various anomalies in real-world scenarios.",0
"Surveillance anomaly detection seeks to identify abnormal occurrences, such as accidents or crimes, within normal settings. This task is challenging because the majority of training data is unlabeled, normal video footage. Many existing methods employ autoencoders to learn normal video patterns and detect anomalies by identifying failures to reproduce abnormal scenes. However, previous approaches have separated appearance and motion information, which limits reciprocal representation capabilities. We propose an implicit two-path AE (ITAE) that utilizes two encoders to implicitly model appearance and motion features, and a single decoder to combine them and learn normal video patterns. To learn the tractable likelihoods of ITAE features for complex normal scene distributions, we suggest normal density estimation through normalizing flow (NF)-based generative models for out-of-distribution detection of anomalies. NF models enhance ITAE performance by learning normality through implicitly learned features. Finally, we demonstrate the effectiveness of ITAE and its feature distribution modeling in three benchmarks, particularly on the Shanghai Tech Campus (ST) database, which contains various anomalies in real-world scenarios.",1
"Standard frame-based cameras that sample light intensity frames are heavily impacted by motion blur for high-speed motion and fail to perceive scene accurately when the dynamic range is high. Event-based cameras, on the other hand, overcome these limitations by asynchronously detecting the variation in individual pixel intensities. However, event cameras only provide information about pixels in motion, leading to sparse data. Hence, estimating the overall dense behavior of pixels is difficult. To address such issues associated with the sensors, we present Fusion-FlowNet, a sensor fusion framework for energy-efficient optical flow estimation using both frame- and event-based sensors, leveraging their complementary characteristics. Our proposed network architecture is also a fusion of Spiking Neural Networks (SNNs) and Analog Neural Networks (ANNs) where each network is designed to simultaneously process asynchronous event streams and regular frame-based images, respectively. Our network is end-to-end trained using unsupervised learning to avoid expensive video annotations. The method generalizes well across distinct environments (rapid motion and challenging lighting conditions) and demonstrates state-of-the-art optical flow prediction on the Multi-Vehicle Stereo Event Camera (MVSEC) dataset. Furthermore, our network offers substantial savings in terms of the number of network parameters and computational energy cost.",0
"When motion blur is present in high-speed motion, standard frame-based cameras that sample light intensity frames may not accurately perceive the scene, particularly when there is a high dynamic range. Event-based cameras, on the other hand, detect the variation in individual pixel intensities asynchronously and thus overcome these limitations. However, event cameras only provide information about pixels in motion, making it difficult to estimate the overall dense behavior of pixels. To address these issues, we have developed Fusion-FlowNet, a sensor fusion framework that uses both frame- and event-based sensors to estimate optical flow efficiently. Our proposed network architecture is a fusion of Spiking Neural Networks (SNNs) and Analog Neural Networks (ANNs), each designed to process asynchronous event streams and regular frame-based images, respectively. Our network is end-to-end trained using unsupervised learning to avoid expensive video annotations and demonstrates state-of-the-art optical flow prediction on the Multi-Vehicle Stereo Event Camera (MVSEC) dataset. Furthermore, our network offers substantial savings in terms of the number of network parameters and computational energy cost while generalizing well across different environments with rapid motion and challenging lighting conditions.",1
"Predicting future frames for robotic surgical video is an interesting, important yet extremely challenging problem, given that the operative tasks may have complex dynamics. Existing approaches on future prediction of natural videos were based on either deterministic models or stochastic models, including deep recurrent neural networks, optical flow, and latent space modeling. However, the potential in predicting meaningful movements of robots with dual arms in surgical scenarios has not been tapped so far, which is typically more challenging than forecasting independent motions of one arm robots in natural scenarios. In this paper, we propose a ternary prior guided variational autoencoder (TPG-VAE) model for future frame prediction in robotic surgical video sequences. Besides content distribution, our model learns motion distribution, which is novel to handle the small movements of surgical tools. Furthermore, we add the invariant prior information from the gesture class into the generation process to constrain the latent space of our model. To our best knowledge, this is the first time that the future frames of dual arm robots are predicted considering their unique characteristics relative to general robotic videos. Experiments demonstrate that our model gains more stable and realistic future frame prediction scenes with the suturing task on the public JIGSAWS dataset.",0
"It is a significant challenge to predict future frames for robotic surgical videos due to the complex dynamics of operative tasks. Previous methods for predicting future natural videos have utilized either deterministic models or stochastic models such as deep recurrent neural networks, optical flow, and latent space modeling. However, predicting meaningful movements of robots with dual arms in surgical scenarios has not been explored, which is typically more difficult than forecasting independent motions of one-arm robots in natural scenarios. In this study, we introduce a ternary prior guided variational autoencoder (TPG-VAE) model to predict future frames in robotic surgical video sequences. Our model learns both content and motion distribution, which is novel for handling small movements of surgical tools. Additionally, we incorporate invariant prior information from the gesture class into the generation process to constrain the latent space of our model. To our knowledge, this is the first time that future frames of dual-arm robots have been predicted considering their unique characteristics relative to general robotic videos. Our experiments demonstrate that our model produces more stable and realistic future frame predictions for the suturing task on the public JIGSAWS dataset.",1
"Accurate fall detection for the assistance of older people is crucial to reduce incidents of deaths or injuries due to falls. Meanwhile, a vision-based fall detection system has shown some significant results to detect falls. Still, numerous challenges need to be resolved. The impact of deep learning has changed the landscape of the vision-based system, such as action recognition. The deep learning technique has not been successfully implemented in vision-based fall detection systems due to the requirement of a large amount of computation power and the requirement of a large amount of sample training data. This research aims to propose a vision-based fall detection system that improves the accuracy of fall detection in some complex environments such as the change of light condition in the room. Also, this research aims to increase the performance of the pre-processing of video images. The proposed system consists of the Enhanced Dynamic Optical Flow technique that encodes the temporal data of optical flow videos by the method of rank pooling, which thereby improves the processing time of fall detection and improves the classification accuracy in dynamic lighting conditions. The experimental results showed that the classification accuracy of the fall detection improved by around 3% and the processing time by 40 to 50ms. The proposed system concentrates on decreasing the processing time of fall detection and improving classification accuracy. Meanwhile, it provides a mechanism for summarizing a video into a single image by using a dynamic optical flow technique, which helps to increase the performance of image pre-processing steps.",0
"Reducing incidents of deaths and injuries due to falls in older people is crucial, and accurate fall detection is necessary for this purpose. Although a vision-based fall detection system has shown significant results, there are several challenges that need to be addressed. The deep learning technique has revolutionized the landscape of vision-based systems, but its successful implementation requires a large amount of computation power and training data. This study proposes a vision-based fall detection system that improves accuracy and processing time in complex environments with changing light conditions. The system uses the Enhanced Dynamic Optical Flow technique that encodes temporal data and rank pooling to improve classification accuracy and processing time. Experimental results showed an increase in classification accuracy by 3% and a decrease in processing time by 40 to 50ms. The proposed system aims to improve fall detection accuracy and reduce processing time while also summarizing a video into a single image using a dynamic optical flow technique to enhance image pre-processing.",1
"Since the wide employment of deep learning frameworks in video salient object detection, the accuracy of the recent approaches has made stunning progress. These approaches mainly adopt the sequential modules, based on optical flow or recurrent neural network (RNN), to learn robust spatiotemporal features. These modules are effective but significantly increase the computational burden of the corresponding deep models. In this paper, to simplify the network and maintain the accuracy, we present a lightweight network tailored for video salient object detection through the spatiotemporal knowledge distillation. Specifically, in the spatial aspect, we combine a saliency guidance feature embedding structure and spatial knowledge distillation to refine the spatial features. In the temporal aspect, we propose a temporal knowledge distillation strategy, which allows the network to learn the robust temporal features through the infer-frame feature encoding and distilling information from adjacent frames. The experiments on widely used video datasets (e.g., DAVIS, DAVSOD, SegTrack-V2) prove that our approach achieves competitive performance. Furthermore, without the employment of the complex sequential modules, the proposed network can obtain high efficiency with 0.01s per frame.",0
"The use of deep learning frameworks for video salient object detection has resulted in remarkable progress in the accuracy of recent approaches. These approaches typically rely on sequential modules based on optical flow or RNN to learn robust spatiotemporal features, which are effective but increase the computational load of deep models significantly. In this study, we introduce a lightweight network designed for video salient object detection through spatiotemporal knowledge distillation. To simplify the network while maintaining accuracy, we combine saliency guidance feature embedding and spatial knowledge distillation to refine spatial features. Additionally, we propose a temporal knowledge distillation strategy to enable the network to learn robust temporal features by encoding infer-frame features and distilling information from adjacent frames. Our approach achieves competitive performance on widely used video datasets (e.g., DAVIS, DAVSOD, SegTrack-V2). Moreover, our proposed network can achieve high efficiency without the need for complex sequential modules, with only 0.01s per frame.",1
"In this study, we propose a self-supervised video denoising method called ""restore-from-restored."" This method fine-tunes a pre-trained network by using a pseudo clean video during the test phase. The pseudo clean video is obtained by applying a noisy video to the baseline network. By adopting a fully convolutional neural network (FCN) as the baseline, we can improve video denoising performance without accurate optical flow estimation and registration steps, in contrast to many conventional video restoration methods, due to the translation equivariant property of the FCN. Specifically, the proposed method can take advantage of plentiful similar patches existing across multiple consecutive frames (i.e., patch-recurrence); these patches can boost the performance of the baseline network by a large margin. We analyze the restoration performance of the fine-tuned video denoising networks with the proposed self-supervision-based learning algorithm, and demonstrate that the FCN can utilize recurring patches without requiring accurate registration among adjacent frames. In our experiments, we apply the proposed method to state-of-the-art denoisers and show that our fine-tuned networks achieve a considerable improvement in denoising performance.",0
"The study introduces a video denoising technique named ""restore-from-restored"" that is self-supervised. During the testing phase, the method fine-tunes a pre-trained network using a pseudo clean video obtained by applying a noisy video to the baseline network. Unlike traditional video restoration methods that require accurate optical flow estimation and registration steps, the method employs a fully convolutional neural network (FCN) as the baseline, which has a translation equivariant property, thus improving video denoising performance. The FCN can take advantage of similar patches across consecutive frames (patch-recurrence) to enhance the performance of the baseline network. The study shows that the proposed self-supervision-based learning algorithm can improve restoration performance without requiring accurate registration among adjacent frames. The proposed method is applied to state-of-the-art denoisers, and the results demonstrate a significant improvement in denoising performance.",1
"Abrupt motion of camera or objects in a scene result in a blurry video, and therefore recovering high quality video requires two types of enhancements: visual enhancement and temporal upsampling. A broad range of research attempted to recover clean frames from blurred image sequences or temporally upsample frames by interpolation, yet there are very limited studies handling both problems jointly. In this work, we present a novel framework for deblurring, interpolating and extrapolating sharp frames from a motion-blurred video in an end-to-end manner. We design our framework by first learning the pixel-level motion that caused the blur from the given inputs via optical flow estimation and then predict multiple clean frames by warping the decoded features with the estimated flows. To ensure temporal coherence across predicted frames and address potential temporal ambiguity, we propose a simple, yet effective flow-based rule. The effectiveness and favorability of our approach are highlighted through extensive qualitative and quantitative evaluations on motion-blurred datasets from high speed videos.",0
"Recovering high quality video requires two types of enhancements, visual enhancement and temporal upsampling, as abrupt camera or object motions in a scene can cause blurriness. While previous research has attempted to recover clear frames from blurred image sequences or temporally upsample frames through interpolation, there are limited studies that address both problems simultaneously. This study presents a new framework for deblurring, interpolating, and extrapolating sharp frames from motion-blurred videos in an end-to-end manner. The framework first learns the pixel-level motion that caused the blur through optical flow estimation, then predicts multiple clear frames by warping the decoded features with the estimated flows. To ensure temporal coherence and address potential temporal ambiguity, the study proposes a flow-based rule. The effectiveness of this approach is demonstrated through qualitative and quantitative evaluations on motion-blurred datasets from high speed videos.",1
"In most of computer vision applications, motion blur is regarded as an undesirable artifact. However, it has been shown that motion blur in an image may have practical interests in fundamental computer vision problems. In this work, we propose a novel framework to estimate optical flow from a single motion-blurred image in an end-to-end manner. We design our network with transformer networks to learn globally and locally varying motions from encoded features of a motion-blurred input, and decode left and right frame features without explicit frame supervision. A flow estimator network is then used to estimate optical flow from the decoded features in a coarse-to-fine manner. We qualitatively and quantitatively evaluate our model through a large set of experiments on synthetic and real motion-blur datasets. We also provide in-depth analysis of our model in connection with related approaches to highlight the effectiveness and favorability of our approach. Furthermore, we showcase the applicability of the flow estimated by our method on deblurring and moving object segmentation tasks.",0
"Motion blur is typically considered an unwanted artifact in computer vision applications. However, recent research has revealed that motion blur in an image can actually be beneficial in certain fundamental computer vision problems. This study introduces a new framework for estimating optical flow from a single motion-blurred image in an end-to-end fashion. The network is designed using transformer networks to learn both globally and locally varying motions from the encoded features of a motion-blurred input. The left and right frame features are decoded without explicit frame supervision. The decoded features are then used to estimate optical flow in a coarse-to-fine manner using a flow estimator network. The proposed approach is evaluated qualitatively and quantitatively through a series of experiments on both synthetic and real motion-blur datasets. A comparative analysis with related approaches highlights the effectiveness and favorability of the proposed approach. Additionally, the study demonstrates the applicability of the flow estimated by the proposed method on deblurring and moving object segmentation tasks.",1
"Video deblurring models exploit consecutive frames to remove blurs from camera shakes and object motions. In order to utilize neighboring sharp patches, typical methods rely mainly on homography or optical flows to spatially align neighboring blurry frames. However, such explicit approaches are less effective in the presence of fast motions with large pixel displacements. In this work, we propose a novel implicit method to learn spatial correspondence among blurry frames in the feature space. To construct distant pixel correspondences, our model builds a correlation volume pyramid among all the pixel-pairs between neighboring frames. To enhance the features of the reference frame, we design a correlative aggregation module that maximizes the pixel-pair correlations with its neighbors based on the volume pyramid. Finally, we feed the aggregated features into a reconstruction module to obtain the restored frame. We design a generative adversarial paradigm to optimize the model progressively. Our proposed method is evaluated on the widely-adopted DVD dataset, along with a newly collected High-Frame-Rate (1000 fps) Dataset for Video Deblurring (HFR-DVD). Quantitative and qualitative experiments show that our model performs favorably on both datasets against previous state-of-the-art methods, confirming the benefit of modeling all-range spatial correspondence for video deblurring.",0
"Models for video deblurring utilize consecutive frames to eliminate blurs caused by camera shakes and object motions. To leverage sharp neighboring patches, existing methods primarily rely on homography or optical flows for spatial alignment of blurry frames. However, these explicit approaches prove less effective when dealing with fast motions involving large pixel displacements. In our work, we propose an innovative implicit method that learns spatial correspondence among blurry frames in the feature space. Our model builds a correlation volume pyramid among all neighboring frames' pixel-pairs to establish distant pixel correspondences. To enhance the reference frame's features, we design a correlative aggregation module that maximizes pixel-pair correlations based on the volume pyramid with its neighbors. Finally, we feed the aggregated features into a reconstruction module to obtain the restored frame. We utilize a generative adversarial paradigm to progressively optimize the model. We test our proposed method on the DVD dataset and a newly collected High-Frame-Rate (1000 fps) Dataset for Video Deblurring (HFR-DVD). The quantitative and qualitative experiments show that our model outperforms previous state-of-the-art methods on both datasets, highlighting the benefits of modeling all-range spatial correspondence for video deblurring.",1
"Learning reliable motion representation between consecutive frames, such as optical flow, has proven to have great promotion to video understanding. However, the TV-L1 method, an effective optical flow solver, is time-consuming and expensive in storage for caching the extracted optical flow. To fill the gap, we propose UF-TSN, a novel end-to-end action recognition approach enhanced with an embedded lightweight unsupervised optical flow estimator. UF-TSN estimates motion cues from adjacent frames in a coarse-to-fine manner and focuses on small displacement for each level by extracting pyramid of feature and warping one to the other according to the estimated flow of the last level. Due to the lack of labeled motion for action datasets, we constrain the flow prediction with multi-scale photometric consistency and edge-aware smoothness. Compared with state-of-the-art unsupervised motion representation learning methods, our model achieves better accuracy while maintaining efficiency, which is competitive with some supervised or more complicated approaches.",0
"The use of optical flow as a reliable motion representation between consecutive frames has been shown to significantly aid in video understanding. However, the current method of using the TV-L1 optical flow solver is both time-consuming and requires a large amount of storage for caching extracted optical flow. To address this issue, we have developed UF-TSN, an innovative end-to-end action recognition technique that includes an embedded, lightweight unsupervised optical flow estimator. UF-TSN uses a coarse-to-fine approach to estimate motion cues from adjacent frames and focuses on small displacements by extracting a pyramid of features and warping them based on the estimated flow from the previous level. Since action datasets lack labeled motion, we have constrained the flow prediction using multi-scale photometric consistency and edge-aware smoothness. Compared to other unsupervised motion representation learning techniques, UF-TSN achieves better accuracy while maintaining efficiency, making it competitive with some supervised or more complex approaches.",1
"Multi-view geometry-based methods dominate the last few decades in monocular Visual Odometry for their superior performance, while they have been vulnerable to dynamic and low-texture scenes. More importantly, monocular methods suffer from scale-drift issue, i.e., errors accumulate over time. Recent studies show that deep neural networks can learn scene depths and relative camera in a self-supervised manner without acquiring ground truth labels. More surprisingly, they show that the well-trained networks enable scale-consistent predictions over long videos, while the accuracy is still inferior to traditional methods because of ignoring geometric information. Building on top of recent progress in computer vision, we design a simple yet robust VO system by integrating multi-view geometry and deep learning on Depth and optical Flow, namely DF-VO. In this work, a) we propose a method to carefully sample high-quality correspondences from deep flows and recover accurate camera poses with a geometric module; b) we address the scale-drift issue by aligning geometrically triangulated depths to the scale-consistent deep depths, where the dynamic scenes are taken into account. Comprehensive ablation studies show the effectiveness of the proposed method, and extensive evaluation results show the state-of-the-art performance of our system, e.g., Ours (1.652%) v.s. ORB-SLAM (3.247%}) in terms of translation error in KITTI Odometry benchmark. Source code is publicly available at: \href{https://github.com/Huangying-Zhan/DF-VO}{DF-VO}.",0
"In recent years, multi-view geometry-based methods have been widely used in monocular Visual Odometry due to their superior performance, but they have proven to be vulnerable to dynamic and low-texture scenes. Additionally, monocular methods suffer from the scale-drift issue, where errors accumulate over time. Recent studies have shown that deep neural networks can learn scene depths and relative camera positions in a self-supervised manner, making them a promising solution. However, these networks still struggle with accuracy due to the lack of geometric information. To tackle this issue, we propose a new system called DF-VO that integrates both multi-view geometry and deep learning on Depth and optical Flow. Our method carefully samples high-quality correspondences from deep flows and recovers accurate camera poses using a geometric module. We also address the scale-drift issue by aligning geometrically triangulated depths with scale-consistent deep depths, taking dynamic scenes into account. Our comprehensive ablation studies show the effectiveness of our method, and extensive evaluation results demonstrate the state-of-the-art performance of our system. For example, our system achieved a translation error of 1.652%, compared to ORB-SLAM's 3.247%, in the KITTI Odometry benchmark. The source code for our system is publicly available on GitHub at \href{https://github.com/Huangying-Zhan/DF-VO}{DF-VO}.",1
"Understanding driver activity is vital for in-vehicle systems that aim to reduce the incidence of car accidents rooted in cognitive distraction. Automating real-time behavior recognition while ensuring actions classification with high accuracy is however challenging, given the multitude of circumstances surrounding drivers, the unique traits of individuals, and the computational constraints imposed by in-vehicle embedded platforms. Prior work fails to jointly meet these runtime/accuracy requirements and mostly rely on a single sensing modality, which in turn can be a single point of failure. In this paper, we harness the exceptional feature extraction abilities of deep learning and propose a dedicated Interwoven Deep Convolutional Neural Network (InterCNN) architecture to tackle the problem of accurate classification of driver behaviors in real-time. The proposed solution exploits information from multi-stream inputs, i.e., in-vehicle cameras with different fields of view and optical flows computed based on recorded images, and merges through multiple fusion layers abstract features that it extracts. This builds a tight ensembling system, which significantly improves the robustness of the model. In addition, we introduce a temporal voting scheme based on historical inference instances, to enhance the classification accuracy. Experiments conducted with a dataset that we collect in a mock-up car environment demonstrate that the proposed InterCNN with MobileNet convolutional blocks can classify 9 different behaviors with 73.97% accuracy, and 5 'aggregated' behaviors with 81.66% accuracy. We further show that our architecture is highly computationally efficient, as it performs inferences within 15ms, which satisfies the real-time constraints of intelligent cars. Nevertheless, our InterCNN is robust to lossy input, as the classification remains accurate when two input streams are occluded.",0
"The accurate classification of driver behaviors in real-time is crucial for in-vehicle systems aiming to reduce car accidents caused by cognitive distraction. However, this is a challenging task due to the numerous circumstances surrounding drivers, the unique traits of individuals, and the computational limitations of in-vehicle embedded platforms. Previous attempts at behavior recognition have failed to meet the necessary runtime/accuracy requirements and typically rely on a single sensing modality, which is a single point of failure. To overcome these challenges, we propose a dedicated Interwoven Deep Convolutional Neural Network (InterCNN) architecture that harnesses the feature extraction abilities of deep learning. Our solution utilizes information from multiple inputs, such as in-vehicle cameras with various fields of view and optical flows computed based on recorded images, and merges them through multiple fusion layers to extract abstract features. This creates a robust ensembling system that significantly improves the accuracy of the model. We also introduce a temporal voting scheme based on historical inference instances to further enhance the classification accuracy. Our experiments with a dataset collected in a mock-up car environment demonstrate that our InterCNN with MobileNet convolutional blocks can classify 9 different behaviors with 73.97% accuracy and 5 'aggregated' behaviors with 81.66% accuracy. Additionally, our architecture is highly computationally efficient, performing inferences within 15ms, which satisfies the real-time constraints of intelligent cars. Furthermore, our InterCNN is robust to lossy inputs, as the classification remains accurate even when two input streams are occluded.",1
"Optical flow is a regression task where convolutional neural networks (CNNs) have led to major breakthroughs. However, this comes at major computational demands due to the use of cost-volumes and pyramidal representations. This was mitigated by producing flow predictions at quarter the resolution, which are upsampled using bilinear interpolation during test time. Consequently, fine details are usually lost and post-processing is needed to restore them. We propose the Normalized Convolution UPsampler (NCUP), an efficient joint upsampling approach to produce the full-resolution flow during the training of optical flow CNNs. Our proposed approach formulates the upsampling task as a sparse problem and employs the normalized convolutional neural networks to solve it. We evaluate our upsampler against existing joint upsampling approaches when trained end-to-end with a a coarse-to-fine optical flow CNN (PWCNet) and we show that it outperforms all other approaches on the FlyingChairs dataset while having at least one order fewer parameters. Moreover, we test our upsampler with a recurrent optical flow CNN (RAFT) and we achieve state-of-the-art results on Sintel benchmark with ~6% error reduction, and on-par on the KITTI dataset, while having 7.5% fewer parameters (see Figure 1). Finally, our upsampler shows better generalization capabilities than RAFT when trained and evaluated on different datasets.",0
"Convolutional neural networks (CNNs) have achieved significant progress in optical flow, a regression task. However, the use of cost-volumes and pyramidal representations results in high computational demands. To overcome this issue, flow predictions are made at a quarter of the resolution and upsampled using bilinear interpolation during test time, but this often leads to the loss of fine details that require post-processing. To address this problem, we propose an efficient joint upsampling approach called Normalized Convolution UPsampler (NCUP) that formulates the upsampling task as a sparse problem and employs normalized convolutional neural networks. We evaluate our approach by training it end-to-end with a coarse-to-fine optical flow CNN (PWCNet) and show that it outperforms existing joint upsampling approaches on the FlyingChairs dataset while having significantly fewer parameters. We also test our upsampler with a recurrent optical flow CNN (RAFT) and achieve state-of-the-art results on the Sintel benchmark with a 6% error reduction and on-par results on the KITTI dataset while using 7.5% fewer parameters. Additionally, our upsampler exhibits better generalization capabilities than RAFT when trained and evaluated on different datasets.",1
"Neural style transfer models have been used to stylize an ordinary video to specific styles. To ensure temporal inconsistency between the frames of the stylized video, a common approach is to estimate the optic flow of the pixels in the original video and make the generated pixels match the estimated optical flow. This is achieved by minimizing an optical flow-based (OFB) loss during model training. However, optical flow estimation is itself a challenging task, particularly in complex scenes. In addition, it incurs a high computational cost. We propose a much simpler temporal loss called the frame difference-based (FDB) loss to solve the temporal inconsistency problem. It is defined as the distance between the difference between the stylized frames and the difference between the original frames. The differences between the two frames are measured in both the pixel space and the feature space specified by the convolutional neural networks. A set of human behavior experiments involving 62 subjects with 25,600 votes showed that the performance of the proposed FDB loss matched that of the OFB loss. The performance was measured by subjective evaluation of stability and stylization quality of the generated videos on two typical video stylization models. The results suggest that the proposed FDB loss is a strong alternative to the commonly used OFB loss for video stylization.",0
"One approach to stylizing videos using neural style transfer models involves estimating the optic flow of the original video's pixels and matching the generated pixels to this flow. However, this can be challenging and computationally expensive. To address this issue, we propose a simpler temporal loss called the frame difference-based (FDB) loss. This loss calculates the distance between the differences in stylized and original frames, measured in both pixel and feature space. We conducted human behavior experiments that showed the FDB loss performed as well as the optical flow-based (OFB) loss for subjective evaluation of stability and stylization quality on two video stylization models. Our results suggest that the FDB loss is a strong alternative to the OFB loss for video stylization.",1
"When the input to a deep neural network (DNN) is a video signal, a sequence of feature tensors is produced at the intermediate layers of the model. If neighboring frames of the input video are related through motion, a natural question is, ""what is the relationship between the corresponding feature tensors?"" By analyzing the effect of common DNN operations on optical flow, we show that the motion present in each channel of a feature tensor is approximately equal to the scaled version of the input motion. The analysis is validated through experiments utilizing common motion models. %These results will be useful in collaborative intelligence applications where sequences of feature tensors need to be compressed or further analyzed.",0
"If a video signal is the input for a deep neural network (DNN), the model produces a series of feature tensors at its intermediate layers. If neighboring frames of the video signal are linked through motion, an inquiry arises concerning the correlation between the related feature tensors. By examining the impact of common DNN operations on optical flow, we demonstrate that the motion in each channel of a feature tensor is roughly equivalent to the scaled form of the input motion. Typical motion models were employed in experiments to confirm the analysis. These findings will prove beneficial in collaborative intelligence applications that require the compression or further analysis of sequences of feature tensors.",1
"Recognizing human actions based on videos has became one of the most popular areas of research in computer vision in recent years. This area has many applications such as surveillance, robotics, health care, video search and human-computer interaction. There are many problems associated with recognizing human actions in videos such as cluttered backgrounds, obstructions, viewpoints variation, execution speed and camera movement. A large number of methods have been proposed to solve the problems. This paper focus on spatial and temporal pattern recognition for the classification of videos using Deep Neural Networks. This model takes RGB images and Optical Flow as input data and outputs an action class number. The final recognition accuracy was about 94%.",0
"In recent years, identifying human actions in videos has become a highly researched area in computer vision. This field has numerous applications, including surveillance, robotics, healthcare, video search, and human-computer interaction. However, recognizing human actions in videos poses several challenges, such as cluttered backgrounds, obstructions, viewpoint variations, execution speed, and camera movements. Many techniques have been proposed to overcome these challenges. This study focuses on spatial and temporal pattern recognition using Deep Neural Networks to classify videos. The model utilizes RGB images and Optical Flow as input data and yields an action class number. The accuracy of recognition achieved was approximately 94%.",1
"We present an end-to-end joint training framework that explicitly models 6-DoF motion of multiple dynamic objects, ego-motion and depth in a monocular camera setup without supervision. Our technical contributions are three-fold. First, we highlight the fundamental difference between inverse and forward projection while modeling the individual motion of each rigid object, and propose a geometrically correct projection pipeline using a neural forward projection module. Second, we design a unified instance-aware photometric and geometric consistency loss that holistically imposes self-supervisory signals for every background and object region. Lastly, we introduce a general-purpose auto-annotation scheme using any off-the-shelf instance segmentation and optical flow models to produce video instance segmentation maps that will be utilized as input to our training pipeline. These proposed elements are validated in a detailed ablation study. Through extensive experiments conducted on the KITTI and Cityscapes dataset, our framework is shown to outperform the state-of-the-art depth and motion estimation methods. Our code, dataset, and models are available at https://github.com/SeokjuLee/Insta-DM .",0
"We have developed a training framework that models the 6-DoF motion of multiple dynamic objects, ego-motion, and depth in a monocular camera setup without supervision. Our framework has three main technical contributions. Firstly, we differentiate between inverse and forward projection when modeling the individual motion of each rigid object. We propose a neural forward projection module that uses a geometrically correct projection pipeline. Secondly, we design a unified instance-aware photometric and geometric consistency loss that imposes self-supervisory signals for every background and object region. Finally, we introduce a general-purpose auto-annotation scheme using off-the-shelf instance segmentation and optical flow models to produce video instance segmentation maps. In a detailed ablation study, we validate these proposed elements. Our experiments on the KITTI and Cityscapes dataset show that our framework outperforms state-of-the-art depth and motion estimation methods. We have made our code, dataset, and models available at https://github.com/SeokjuLee/Insta-DM.",1
"Recent constellations of satellites, including the Skysat constellation, are able to acquire bursts of images. This new acquisition mode allows for modern image restoration techniques, including multi-frame super-resolution. As the satellite moves during the acquisition of the burst, elevation changes in the scene translate into noticeable parallax. This parallax hinders the results of the restoration. To cope with this issue, we propose a novel parallax estimation method. The method is composed of a linear Plane+Parallax decomposition of the apparent motion and a multi-frame optical flow algorithm that exploits all frames simultaneously. Using SkySat L1A images, we show that the estimated per-pixel displacements are important for applying multi-frame super-resolution on scenes containing elevation changes and that can also be used to estimate a coarse 3D surface model.",0
"New satellite constellations, such as Skysat, have the capability to capture batches of images, allowing for advanced image restoration techniques like multi-frame super-resolution. However, due to the parallax caused by elevation changes during the image acquisition process, the restoration results may be hindered. To address this issue, we introduce a new method for estimating parallax. This method involves a linear Plane+Parallax decomposition of the apparent motion, along with a multi-frame optical flow algorithm that utilizes all frames at once. By using SkySat L1A images, we demonstrate that this method can accurately estimate per-pixel displacements, which is crucial for applying multi-frame super-resolution to images with elevation changes. Additionally, this method can be used to generate a rudimentary 3D surface model.",1
"Cycling is a promising sustainable mode for commuting and leisure in cities, however, the fear of getting hit or fall reduces its wide expansion as a commuting mode. In this paper, we introduce a novel method called CyclingNet for detecting cycling near misses from video streams generated by a mounted frontal camera on a bike regardless of the camera position, the conditions of the built, the visual conditions and without any restrictions on the riding behaviour. CyclingNet is a deep computer vision model based on convolutional structure embedded with self-attention bidirectional long-short term memory (LSTM) blocks that aim to understand near misses from both sequential images of scenes and their optical flows. The model is trained on scenes of both safe rides and near misses. After 42 hours of training on a single GPU, the model shows high accuracy on the training, testing and validation sets. The model is intended to be used for generating information that can draw significant conclusions regarding cycling behaviour in cities and elsewhere, which could help planners and policy-makers to better understand the requirement of safety measures when designing infrastructure or drawing policies. As for future work, the model can be pipelined with other state-of-the-art classifiers and object detectors simultaneously to understand the causality of near misses based on factors related to interactions of road-users, the built and the natural environments.",0
"Although cycling is a sustainable method of transportation and leisure in urban areas, the fear of accidents and injuries has hindered its widespread adoption as a commuting mode. To address this issue, we present a new technique called CyclingNet, which uses a frontal camera mounted on a bicycle to identify near misses regardless of the camera's location or the visual conditions. CyclingNet is a deep learning model that incorporates a convolutional structure with self-attention bidirectional long-short term memory (LSTM) blocks to detect near misses from sequential images and their optical flows. The model is trained on both safe and near miss scenarios, achieving high accuracy on training, testing, and validation sets after 42 hours of training on one GPU. The model can provide valuable insights into cycling behavior in urban areas, helping city planners and policymakers better understand the need for safety measures in infrastructure design and policy-making. Future work can involve combining CyclingNet with other cutting-edge classifiers and object detectors to examine the factors influencing near misses, such as interactions between road users and the built or natural environments.",1
The goal of this paper is propose a mathematical framework for optical flow refinement with non-quadratic regularization using variational techniques. We demonstrate how the model can be suitably adapted for both rigid and fluid motion estimation. We study the problem as an abstract IVP using an evolutionary PDE approach. We show that for a particular choice of constraint our model approximates the continuity model with non-quadratic regularization using augmented Lagrangian techniques. We subsequently show the results of our algorithm on different datasets.,0
"In this paper, we aim to present a mathematical framework that utilizes variational techniques and non-quadratic regularization to refine optical flow. We showcase the adaptability of our model for estimating both rigid and fluid motion. Our approach involves studying the problem as an abstract initial value problem using an evolutionary PDE approach. By selecting a specific constraint, we demonstrate that our model approximates the continuity model with non-quadratic regularization using augmented Lagrangian techniques. Finally, we present the results of our algorithm on various datasets.",1
"Optical flow estimation is an essential step for many real-world computer vision tasks. Existing deep networks have achieved satisfactory results by mostly employing a pyramidal coarse-to-fine paradigm, where a key process is to adopt warped target feature based on previous flow prediction to correlate with source feature for building 3D matching cost volume. However, the warping operation can lead to troublesome ghosting problem that results in ambiguity. Moreover, occluded areas are treated equally with non occluded regions in most existing works, which may cause performance degradation. To deal with these challenges, we propose a lightweight yet efficient optical flow network, named OAS-Net (occlusion aware sampling network) for accurate optical flow. First, a new sampling based correlation layer is employed without noisy warping operation. Second, a novel occlusion aware module is presented to make raw cost volume conscious of occluded regions. Third, a shared flow and occlusion awareness decoder is adopted for structure compactness. Experiments on Sintel and KITTI datasets demonstrate the effectiveness of proposed approaches.",0
"Many computer vision tasks depend on accurately estimating optical flow, but current deep network approaches can suffer from ghosting issues caused by warping operations and treat occluded areas the same as non-occluded regions, which can lead to reduced performance. To address these challenges, we propose a lightweight and efficient optical flow network called OAS-Net (occlusion aware sampling network). OAS-Net employs a new sampling-based correlation layer instead of warping and includes an occlusion-aware module to handle occluded areas. We also use a shared flow and occlusion awareness decoder to keep the structure compact. Our experiments on Sintel and KITTI datasets demonstrate the effectiveness of our approach.",1
"Generating non-existing frames from a consecutive video sequence has been an interesting and challenging problem in the video processing field. Typical kernel-based interpolation methods predict pixels with a single convolution process that convolves source frames with spatially adaptive local kernels, which circumvents the time-consuming, explicit motion estimation in the form of optical flow. However, when scene motion is larger than the pre-defined kernel size, these methods are prone to yield less plausible results. In addition, they cannot directly generate a frame at an arbitrary temporal position because the learned kernels are tied to the midpoint in time between the input frames. In this paper, we try to solve these problems and propose a novel non-flow kernel-based approach that we refer to as enhanced deformable separable convolution (EDSC) to estimate not only adaptive kernels, but also offsets, masks and biases to make the network obtain information from non-local neighborhood. During the learning process, different intermediate time step can be involved as a control variable by means of an extension of coord-conv trick, allowing the estimated components to vary with different input temporal information. This makes our method capable to produce multiple in-between frames. Furthermore, we investigate the relationships between our method and other typical kernel- and flow-based methods. Experimental results show that our method performs favorably against the state-of-the-art methods across a broad range of datasets. Code will be publicly available on URL: \url{https://github.com/Xianhang/EDSC-pytorch}.",0
"The challenge of generating non-existent frames from a consecutive video sequence has intrigued and tested the capabilities of the video processing field. Traditional interpolation techniques based on kernels use a single convolution process to predict pixels, employing locally adaptive kernels that alleviate the need for explicit motion estimation in the form of optical flow, which can be time-consuming. However, these methods fail to produce convincing results when scene motion exceeds the kernel's predefined size. Additionally, they cannot generate a frame at any arbitrary temporal position since the learned kernels are linked to the midpoint in time between the input frames. To address these limitations, we present a novel non-flow kernel-based method called enhanced deformable separable convolution (EDSC), which estimates adaptive kernels, masks, biases, and offsets to enable the network to gather information from non-local neighborhoods. Our approach permits the involvement of different intermediate time steps as a control variable through an extension of the coord-conv trick during the learning process, allowing the estimated components to vary with various input temporal information, which enables the generation of multiple in-between frames. Furthermore, we explore the connections between our method and other traditional kernel- and flow-based techniques. Our experimental results show that our method outperforms state-of-the-art methods across a wide range of datasets. The code for our method is publicly available at \url{https://github.com/Xianhang/EDSC-pytorch}.",1
"We present Supervision by Registration and Triangulation (SRT), an unsupervised approach that utilizes unlabeled multi-view video to improve the accuracy and precision of landmark detectors. Being able to utilize unlabeled data enables our detectors to learn from massive amounts of unlabeled data freely available and not be limited by the quality and quantity of manual human annotations. To utilize unlabeled data, there are two key observations: (1) the detections of the same landmark in adjacent frames should be coherent with registration, i.e., optical flow. (2) the detections of the same landmark in multiple synchronized and geometrically calibrated views should correspond to a single 3D point, i.e., multi-view consistency. Registration and multi-view consistency are sources of supervision that do not require manual labeling, thus it can be leveraged to augment existing training data during detector training. End-to-end training is made possible by differentiable registration and 3D triangulation modules. Experiments with 11 datasets and a newly proposed metric to measure precision demonstrate accuracy and precision improvements in landmark detection on both images and video. Code is available at https://github.com/D-X-Y/landmark-detection.",0
"We introduce a novel approach called Supervision by Registration and Triangulation (SRT), which employs unlabeled multi-view video to enhance the precision and accuracy of landmark detectors in an unsupervised manner. With the use of unlabeled data, our detectors can learn from vast amounts of freely available data without being restricted by the quality and quantity of human annotations. To leverage unlabeled data, we have made two key observations: (1) the detection of the same landmark in adjacent frames should be coherent with registration, i.e., optical flow, and (2) the detection of the same landmark in multiple synchronized and geometrically calibrated views should correspond to a single 3D point, i.e., multi-view consistency. These sources of supervision do not require manual labeling, making it possible to supplement existing training data during detector training. End-to-end training is feasible due to differentiable registration and 3D triangulation modules. Our experiments on 11 datasets using a novel precision metric show improvements in accuracy and precision in landmark detection on both images and video. The code for this approach is available at https://github.com/D-X-Y/landmark-detection.",1
"Weighted Gaussian Curvature is an important measurement for images. However, its conventional computation scheme has low performance, low accuracy and requires that the input image must be second order differentiable. To tackle these three issues, we propose a novel discrete computation scheme for the weighted Gaussian curvature. Our scheme does not require the second order differentiability. Moreover, our scheme is more accurate, has smaller support region and computationally more efficient than the conventional schemes. Therefore, our scheme holds promise for a large range of applications where the weighted Gaussian curvature is needed, for example, image smoothing, cartoon texture decomposition, optical flow estimation, etc.",0
"The measurement of Weighted Gaussian Curvature is crucial in image analysis, but the conventional method for computing it is flawed in terms of low accuracy, poor performance, and the necessity of having a second order differentiable input image. To overcome these challenges, we have developed a new discrete computation approach for the weighted Gaussian curvature that does not require second order differentiability, is more precise, has a smaller support region, and is more computationally efficient than the conventional method. As a result, our approach is highly promising for a wide range of applications that require weighted Gaussian curvature, such as image smoothing, cartoon texture decomposition, and optical flow estimation.",1
"Self-driving cars and other autonomous vehicles need to detect and track objects in camera images. We present a simple online tracking algorithm that is based on a constant velocity motion model with a Kalman filter, and an assignment heuristic. The assignment heuristic relies on four metrics: An embedding vector that describes the appearance of objects and can be used to re-identify them, a displacement vector that describes the object movement between two consecutive video frames, the Mahalanobis distance between the Kalman filter states and the new detections, and a class distance. These metrics are combined with a linear SVM, and then the assignment problem is solved by the Hungarian algorithm. We also propose an efficient CNN architecture that estimates these metrics. Our multi-frame model accepts two consecutive video frames which are processed individually in the backbone, and then optical flow is estimated on the resulting feature maps. This allows the network heads to estimate the displacement vectors. We evaluate our approach on the challenging BDD100K tracking dataset. Our multi-frame model achieves a good MOTA value of 39.1% with low localization error of 0.206 in MOTP. Our fast single-frame model achieves an even lower localization error of 0.202 in MOTP, and a MOTA value of 36.8%.",0
"A tracking algorithm has been developed for self-driving cars and other autonomous vehicles to detect and follow objects in camera images. The algorithm is based on a constant velocity motion model with a Kalman filter, and an assignment heuristic that uses four metrics: an embedding vector, a displacement vector, the Mahalanobis distance, and a class distance. These metrics are combined with a linear SVM and solved with the Hungarian algorithm. An efficient CNN architecture is proposed to estimate these metrics, which accepts two consecutive video frames that are processed individually in the backbone, and then optical flow is estimated on the resulting feature maps. The approach was evaluated on the BDD100K tracking dataset, where the multi-frame model achieved a MOTA value of 39.1% with low localization error of 0.206 in MOTP, and the fast single-frame model achieved a lower localization error of 0.202 in MOTP and a MOTA value of 36.8%.",1
"Recently, flow-based methods have achieved promising success in video frame interpolation. However, electron microscopic (EM) images suffer from unstable image quality, low PSNR, and disorderly deformation. Existing flow-based interpolation methods cannot precisely compute optical flow for EM images since only predicting each position's unique offset. To overcome these problems, we propose a novel interpolation framework for EM images that progressively synthesizes interpolated features in a coarse-to-fine manner. First, we extract missing intermediate features by the proposed temporal spatial-adaptive (TSA) interpolation module. The TSA interpolation module aggregates temporal contexts and then adaptively samples the spatial-related features with the proposed residual spatial adaptive block. Second, we introduce a stacked deformable refinement block (SDRB) further enhance the reconstruction quality, which is aware of the matching positions and relevant features from input frames with the feedback mechanism. Experimental results demonstrate the superior performance of our approach compared to previous works, both quantitatively and qualitatively.",0
"Video frame interpolation has seen promising success with flow-based methods. However, electron microscopic (EM) images present challenges due to unstable image quality, low PSNR, and disorderly deformation. The current flow-based methods cannot accurately compute optical flow for EM images as they only predict unique offsets for each position. To address these issues, we propose a progressive interpolation framework for EM images that synthesizes features in a coarse-to-fine manner. Our approach uses a temporal spatial-adaptive (TSA) interpolation module to extract missing intermediate features by aggregating temporal contexts and adaptively sampling spatial-related features with a proposed residual spatial adaptive block. We also introduce a stacked deformable refinement block (SDRB) to enhance the reconstruction quality. This block is aware of the matching positions and relevant features from input frames with a feedback mechanism. Our experimental results demonstrate the superior performance of our approach compared to previous works, both quantitatively and qualitatively.",1
"Optical flow estimation with occlusion or large displacement is a problematic challenge due to the lost of corresponding pixels between consecutive frames. In this paper, we discover that the lost information is related to a large quantity of motion features (more than 40%) computed from the popular discriminative cost-volume feature would completely vanish due to invalid sampling, leading to the low efficiency of optical flow learning. We call this phenomenon the Vanishing Cost Volume Problem. Inspired by the fact that local motion tends to be highly consistent within a short temporal window, we propose a novel iterative Motion Feature Recovery (MFR) method to address the vanishing cost volume via modeling motion consistency across multiple frames. In each MFR iteration, invalid entries from original motion features are first determined based on the current flow. Then, an efficient network is designed to adaptively learn the motion correlation to recover invalid features for lost-information restoration. The final optical flow is then decoded from the recovered motion features. Experimental results on Sintel and KITTI show that our method achieves state-of-the-art performances. In fact, MFR currently ranks second on Sintel public website.",0
"The challenge of estimating optical flow in the presence of occlusion or significant displacement is problematic, as it results in the loss of corresponding pixels between consecutive frames. This paper explores the relationship between lost information and motion features, revealing that over 40% of features computed from the common discriminative cost-volume feature disappear due to invalid sampling, reducing optical flow efficiency. This is referred to as the Vanishing Cost Volume Problem. To address this issue, an iterative Motion Feature Recovery (MFR) method is proposed, leveraging the consistency of local motion within a short temporal window to recover lost information. Invalid motion features are first identified based on the current flow, and an efficient network is designed to recover these features via motion correlation. The final optical flow is then decoded from the recovered motion features. Experimental results on Sintel and KITTI datasets demonstrate that MFR achieves state-of-the-art performance, currently ranking second on the Sintel public website.",1
"Micro-Expression Recognition has become challenging, as it is extremely difficult to extract the subtle facial changes of micro-expressions. Recently, several approaches proposed several expression-shared features algorithms for micro-expression recognition. However, they do not reveal the specific discriminative characteristics, which lead to sub-optimal performance. This paper proposes a novel Feature Refinement ({FR}) with expression-specific feature learning and fusion for micro-expression recognition. It aims to obtain salient and discriminative features for specific expressions and also predict expression by fusing the expression-specific features. FR consists of an expression proposal module with attention mechanism and a classification branch. First, an inception module is designed based on optical flow to obtain expression-shared features. Second, in order to extract salient and discriminative features for specific expression, expression-shared features are fed into an expression proposal module with attention factors and proposal loss. Last, in the classification branch, labels of categories are predicted by a fusion of the expression-specific features. Experiments on three publicly available databases validate the effectiveness of FR under different protocol. Results on public benchmarks demonstrate that our FR provides salient and discriminative information for micro-expression recognition. The results also show our FR achieves better or competitive performance with the existing state-of-the-art methods on micro-expression recognition.",0
"Recognizing micro-expressions has become a challenge due to the difficulty in detecting subtle facial changes. Previous approaches have proposed expression-shared feature algorithms, but they fail to identify specific discriminative characteristics, leading to sub-optimal performance. To address this issue, this paper suggests a new approach called Feature Refinement ({FR}) that involves expression-specific feature learning and fusion. The aim is to obtain distinctive features for specific expressions and predict expressions by merging expression-specific features. FR consists of two modules: an inception module for obtaining expression-shared features and an expression proposal module with attention mechanism and proposal loss to extract salient and discriminative features for specific expressions. In the classification branch, a fusion of the expression-specific features predicts category labels. Experiments on three publicly available databases validate the effectiveness of FR under different protocols. The results demonstrate that FR provides valuable information for micro-expression recognition and performs better than or similarly to existing state-of-the-art methods.",1
"The objective of this paper is visual-only self-supervised video representation learning. We make the following contributions: (i) we investigate the benefit of adding semantic-class positives to instance-based Info Noise Contrastive Estimation (InfoNCE) training, showing that this form of supervised contrastive learning leads to a clear improvement in performance; (ii) we propose a novel self-supervised co-training scheme to improve the popular infoNCE loss, exploiting the complementary information from different views, RGB streams and optical flow, of the same data source by using one view to obtain positive class samples for the other; (iii) we thoroughly evaluate the quality of the learnt representation on two different downstream tasks: action recognition and video retrieval. In both cases, the proposed approach demonstrates state-of-the-art or comparable performance with other self-supervised approaches, whilst being significantly more efficient to train, i.e. requiring far less training data to achieve similar performance.",0
"This paper focuses on the self-supervised learning of video representation using only visual cues. Our contributions are threefold: firstly, we examine the benefits of incorporating semantic-class positives into instance-based Info Noise Contrastive Estimation (InfoNCE) training, which demonstrates a significant improvement in performance. Secondly, we propose a new self-supervised co-training approach to enhance the popular infoNCE loss by leveraging information from different views, including RGB streams and optical flow, of the same data source. Thirdly, we conduct a comprehensive evaluation of the learned representation on two different downstream tasks, namely action recognition and video retrieval, where our approach outperforms or performs comparably with other self-supervised methods while being more efficient in training, requiring less training data to achieve similar results.",1
"Temporal information is essential to learning effective policies with Reinforcement Learning (RL). However, current state-of-the-art RL algorithms either assume that such information is given as part of the state space or, when learning from pixels, use the simple heuristic of frame-stacking to implicitly capture temporal information present in the image observations. This heuristic is in contrast to the current paradigm in video classification architectures, which utilize explicit encodings of temporal information through methods such as optical flow and two-stream architectures to achieve state-of-the-art performance. Inspired by leading video classification architectures, we introduce the Flow of Latents for Reinforcement Learning (Flare), a network architecture for RL that explicitly encodes temporal information through latent vector differences. We show that Flare (i) recovers optimal performance in state-based RL without explicit access to the state velocity, solely with positional state information, (ii) achieves state-of-the-art performance on pixel-based challenging continuous control tasks within the DeepMind control benchmark suite, namely quadruped walk, hopper hop, finger turn hard, pendulum swing, and walker run, and is the most sample efficient model-free pixel-based RL algorithm, outperforming the prior model-free state-of-the-art by 1.9X and 1.5X on the 500k and 1M step benchmarks, respectively, and (iv), when augmented over rainbow DQN, outperforms this state-of-the-art level baseline on 5 of 8 challenging Atari games at 100M time step benchmark.",0
"To effectively learn policies with Reinforcement Learning (RL), it is necessary to have access to temporal information. However, current state-of-the-art RL algorithms either assume that this information is included in the state space or, when learning from pixels, use frame-stacking as a simple heuristic to capture the temporal information present in the image observations. This differs from the current trend in video classification architectures, which use explicit encodings of temporal information to achieve optimal performance. To address this, we introduce the Flow of Latents for Reinforcement Learning (Flare), an RL network architecture that encodes temporal information through latent vector differences. Our results show that Flare achieves optimal performance in state-based RL without explicit access to state velocity, and it outperforms prior model-free state-of-the-art by 1.9X and 1.5X on the 500k and 1M step benchmarks, respectively, on challenging continuous control tasks within the DeepMind control benchmark suite. Additionally, when combined with rainbow DQN, Flare outperforms the state-of-the-art level baseline on 5 of 8 challenging Atari games at the 100M time step benchmark.",1
"In this paper, we propose a \textbf{Tr}ansformer-based RGB-D \textbf{e}gocentric \textbf{a}ction \textbf{r}ecognition framework, called Trear. It consists of two modules, inter-frame attention encoder and mutual-attentional fusion block. Instead of using optical flow or recurrent units, we adopt self-attention mechanism to model the temporal structure of the data from different modalities. Input frames are cropped randomly to mitigate the effect of the data redundancy. Features from each modality are interacted through the proposed fusion block and combined through a simple yet effective fusion operation to produce a joint RGB-D representation. Empirical experiments on two large egocentric RGB-D datasets, THU-READ and FPHA, and one small dataset, WCVS, have shown that the proposed method outperforms the state-of-the-art results by a large margin.",0
"The paper presents a new framework called Trear for RGB-D egocentric action recognition based on the Transformer model. The framework consists of two modules: an inter-frame attention encoder and a mutual-attentional fusion block. Instead of using optical flow or recurrent units, the framework adopts a self-attention mechanism to model the temporal structure of data from different modalities. To reduce data redundancy, input frames are randomly cropped. The proposed fusion block combines features from each modality through a simple and effective fusion operation to produce a joint RGB-D representation. Empirical experiments on three datasets, including two large egocentric RGB-D datasets (THU-READ and FPHA) and one small dataset (WCVS), demonstrate that the proposed method outperforms state-of-the-art results by a significant margin.",1
"Optical flow techniques are becoming increasingly performant and robust when estimating motion in a scene, but their performance has yet to be proven in the area of facial expression recognition. In this work, a variety of optical flow approaches are evaluated across multiple facial expression datasets, so as to provide a consistent performance evaluation. The aim of this work is not to propose a new expression recognition technique, but to understand better the adequacy of existing state-of-the art optical flow for encoding facial motion in the context of facial expression recognition. Our evaluations highlight the fact that motion approximation methods used to overcome motion discontinuities have a significant impact when optical flows are used to characterize facial expressions.",0
"Although optical flow techniques are improving and becoming more reliable in estimating motion in a scene, their effectiveness in recognizing facial expressions has not been proven. This study evaluates various optical flow methods on multiple facial expression datasets to provide a consistent evaluation. The goal is not to introduce a new expression recognition technique, but to examine the suitability of current state-of-the-art optical flow for encoding facial motion in the context of facial expression recognition. Our findings reveal that motion approximation methods used to address motion discontinuities have a considerable influence on the characterization of facial expressions with optical flows.",1
"Video facial expression recognition is useful for many applications and received much interest lately. Although some solutions give really good results in a controlled environment (no occlusion), recognition in the presence of partial facial occlusion remains a challenging task. To handle occlusions, solutions based on the reconstruction of the occluded part of the face have been proposed. These solutions are mainly based on the texture or the geometry of the face. However, the similarity of the face movement between different persons doing the same expression seems to be a real asset for the reconstruction. In this paper we exploit this asset and propose a new solution based on an auto-encoder with skip connections to reconstruct the occluded part of the face in the optical flow domain. To the best of our knowledge, this is the first proposition to directly reconstruct the movement for facial expression recognition. We validated our approach in the controlled dataset CK+ on which different occlusions were generated. Our experiments show that the proposed method reduce significantly the gap, in terms of recognition accuracy, between occluded and non-occluded situations. We also compare our approach with existing state-of-the-art solutions. In order to lay the basis of a reproducible and fair comparison in the future, we also propose a new experimental protocol that includes occlusion generation and reconstruction evaluation.",0
"Recently, video facial expression recognition has gained a lot of attention and has proven useful in various applications. Although some solutions have shown promising results in controlled environments without obstructions, recognizing facial expressions in the presence of partial obstruction remains a challenging task. To address this issue, solutions based on reconstructing the obscured parts of the face using texture or geometry have been proposed. However, the similarity in facial movements among different individuals performing the same expression can be advantageous for reconstruction. This paper proposes a novel solution based on an auto-encoder with skip connections to reconstruct the movement of the obscured parts of the face in the optical flow domain. This is the first proposal to directly reconstruct facial movement for expression recognition. The proposed approach is validated on the CK+ dataset, which includes various occlusions. Our experiments demonstrate that our method significantly reduces the gap in recognition accuracy between occluded and non-occluded situations. Additionally, we compare our approach with existing state-of-the-art solutions and propose a new experimental protocol that includes occlusion generation and reconstruction evaluation to facilitate future reproducible and fair comparison.",1
"We study the problem of animating images by transferring spatio-temporal visual effects (such as melting) from a collection of videos. We tackle two primary challenges in visual effect transfer: 1) how to capture the effect we wish to distill; and 2) how to ensure that only the effect, rather than content or artistic style, is transferred from the source videos to the input image. To address the first challenge, we evaluate five loss functions; the most promising one encourages the generated animations to have similar optical flow and texture motions as the source videos. To address the second challenge, we only allow our model to move existing image pixels from the previous frame, rather than predicting unconstrained pixel values. This forces any visual effects to occur using the input image's pixels, preventing unwanted artistic style or content from the source video from appearing in the output. We evaluate our method in objective and subjective settings, and show interesting qualitative results which demonstrate objects undergoing atypical transformations, such as making a face melt or a deer bloom.",0
"Our focus is on animating images by transferring spatio-temporal visual effects, like melting, from a set of videos. We encounter two main challenges in visual effect transfer, namely, how to capture the desired effect and how to ensure that only the effect, not content or artistic style, is transferred from source videos to the input image. To overcome the first challenge, we test five loss functions; the most successful one prompts the generated animations to have comparable optical flow and texture motions as the source videos. To address the second challenge, we only permit our model to shift existing image pixels from the previous frame, instead of predicting unconstrained pixel values. This ensures that visual effects are produced using the input image's pixels, preventing unwanted artistic style or content from the source video from appearing in the output. We assess our approach in objective and subjective settings, showing fascinating qualitative outcomes that demonstrate unusual transformations of objects, such as melting faces or blossoming deer.",1
"Speech-driven facial video generation has been a complex problem due to its multi-modal aspects namely audio and video domain. The audio comprises lots of underlying features such as expression, pitch, loudness, prosody(speaking style) and facial video has lots of variability in terms of head movement, eye blinks, lip synchronization and movements of various facial action units along with temporal smoothness. Synthesizing highly expressive facial videos from the audio input and static image is still a challenging task for generative adversarial networks. In this paper, we propose a multi-modal adaptive normalization(MAN) based architecture to synthesize a talking person video of arbitrary length using as input: an audio signal and a single image of a person. The architecture uses the multi-modal adaptive normalization, keypoint heatmap predictor, optical flow predictor and class activation map[58] based layers to learn movements of expressive facial components and hence generates a highly expressive talking-head video of the given person. The multi-modal adaptive normalization uses the various features of audio and video such as Mel spectrogram, pitch, energy from audio signals and predicted keypoint heatmap/optical flow and a single image to learn the respective affine parameters to generate highly expressive video. Experimental evaluation demonstrates superior performance of the proposed method as compared to Realistic Speech-Driven Facial Animation with GANs(RSDGAN) [53], Speech2Vid [10], and other approaches, on multiple quantitative metrics including: SSIM (structural similarity index), PSNR (peak signal to noise ratio), CPBD (image sharpness), WER(word error rate), blinks/sec and LMD(landmark distance). Further, qualitative evaluation and Online Turing tests demonstrate the efficacy of our approach.",0
"Generating facial videos based on speech has been a challenging task due to the multi-modal nature of the problem, which involves both audio and video domains. Audio has various underlying elements like pitch, expression, loudness, and prosody, while facial videos have head movements, lip synchronization, eye blinks, and movements of various facial action units with temporal smoothness, making it difficult for generative adversarial networks to synthesize highly expressive facial videos from the audio input and static image. To address this issue, we propose a multi-modal adaptive normalization (MAN) based architecture that uses various features of audio and video, such as Mel spectrogram, pitch, energy from audio signals, predicted keypoint heatmap/optical flow, and a single image, to learn the respective affine parameters and generate highly expressive talking-head videos of the given person. The proposed method outperforms other approaches, including RSDGAN and Speech2Vid, on multiple quantitative metrics and shows efficacy in qualitative evaluation and Online Turing tests.",1
"In this paper, we consider the compressed video background subtraction problem that separates the background and foreground of a video from its compressed measurements. The background of a video usually lies in a low dimensional space and the foreground is usually sparse. More importantly, each video frame is a natural image that has textural patterns. By exploiting these properties, we develop a message passing algorithm termed offline denoising-based turbo message passing (DTMP). We show that these structural properties can be efficiently handled by the existing denoising techniques under the turbo message passing framework. We further extend the DTMP algorithm to the online scenario where the video data is collected in an online manner. The extension is based on the similarity/continuity between adjacent video frames. We adopt the optical flow method to refine the estimation of the foreground. We also adopt the sliding window based background estimation to reduce complexity. By exploiting the Gaussianity of messages, we develop the state evolution to characterize the per-iteration performance of offline and online DTMP. Comparing to the existing algorithms, DTMP can work at much lower compression rates, and can subtract the background successfully with a lower mean squared error and better visual quality for both offline and online compressed video background subtraction.",0
"The focus of this paper is the issue of compressed video background subtraction, which aims to distinguish between the foreground and background of a video using its compressed measurements. Generally, the video's background is contained within a low-dimensional space, while the foreground is sparse. Moreover, each video frame is a natural image featuring textural patterns. By leveraging these features, we introduce an algorithm called offline denoising-based turbo message passing (DTMP), which is capable of effectively handling these structural properties using existing denoising techniques within the turbo message passing framework. We also extend DTMP to accommodate online video data collection, using the optical flow method to refine foreground estimation and a sliding window-based background estimation to minimize complexity. To evaluate the performance of offline and online DTMP, we utilize the state evolution method to analyze the per-iteration results. Compared to existing algorithms, DTMP can operate at lower compression rates and achieve a lower mean squared error and better visual quality for both offline and online compressed video background subtraction.",1
"Wearable cameras are becoming more and more popular in several applications, increasing the interest of the research community in developing approaches for recognizing actions from the first-person point of view. An open challenge in egocentric action recognition is that videos lack detailed information about the main actor's pose and thus tend to record only parts of the movement when focusing on manipulation tasks. Thus, the amount of information about the action itself is limited, making crucial the understanding of the manipulated objects and their context. Many previous works addressed this issue with two-stream architectures, where one stream is dedicated to modeling the appearance of objects involved in the action, and another to extracting motion features from optical flow. In this paper, we argue that learning features jointly from these two information channels is beneficial to capture the spatio-temporal correlations between the two better. To this end, we propose a single stream architecture able to do so, thanks to the addition of a self-supervised block that uses a pretext motion prediction task to intertwine motion and appearance knowledge. Experiments on several publicly available databases show the power of our approach.",0
"The popularity of wearable cameras has led to increased interest from the research community in developing methods for recognizing actions from a first-person perspective. However, recognizing actions in egocentric videos is challenging because they lack detailed information about the actor's pose, often capturing only parts of the movement when focusing on manipulation tasks. As a result, understanding the context of the manipulated objects is crucial. Previous works have used two-stream architectures to address this issue, but we propose a single stream architecture that learns features jointly from appearance and motion information. Our approach includes a self-supervised block that uses a pretext motion prediction task to intertwine motion and appearance knowledge, resulting in better spatio-temporal correlations. Experiments on publicly available databases demonstrate the effectiveness of our approach.",1
"We propose an unsupervised vision-based system to estimate the joint configurations of the robot arm from a sequence of RGB or RGB-D images without knowing the model a priori, and then adapt it to the task of category-independent articulated object pose estimation. We combine a classical geometric formulation with deep learning and extend the use of epipolar constraint to multi-rigid-body systems to solve this task. Given a video sequence, the optical flow is estimated to get the pixel-wise dense correspondences. After that, the 6D pose is computed by a modified PnP algorithm. The key idea is to leverage the geometric constraints and the constraint between multiple frames. Furthermore, we build a synthetic dataset with different kinds of robots and multi-joint articulated objects for the research of vision-based robot control and robotic vision. We demonstrate the effectiveness of our method on three benchmark datasets and show that our method achieves higher accuracy than the state-of-the-art supervised methods in estimating joint angles of robot arms and articulated objects.",0
"Our proposal is for an autonomous vision-based system that can determine the joint configurations of a robot arm using a sequence of RGB or RGB-D images, without prior knowledge of the model. We have extended the use of epipolar constraint to multi-rigid-body systems, using a combination of classical geometric formulation and deep learning. Optical flow is estimated to determine pixel-wise dense correspondences in a video sequence, and a modified PnP algorithm is used to calculate the 6D pose. We leverage the geometric constraints and the constraint between multiple frames to achieve this. Additionally, we have created a synthetic dataset comprising various robots and multi-joint articulated objects for research purposes. Our method has been shown to outperform state-of-the-art supervised methods in estimating joint angles of robot arms and articulated objects, as we demonstrate on three benchmark datasets.",1
"Analyzing motion between two consecutive images is one of the fundamental tasks in computer vision. In the lack of labeled data, the loss functions are split into consistency and smoothness, allowing for self-supervised training. This paper focuses on the cost function derivation and presents an unrolling iterative approach, transferring the hard L1 smoothness constraint into a softer multi-layer iterative scheme. More accurate gradients, especially near non-differential positions, improve the network's convergence, providing superior results on tested scenarios. We report state-of-the-art results on both MPI Sintel and KITTI 2015 unsupervised optical flow benchmarks. The provided approach can be used to enhance various architectures and not limited just to the presented pipeline.",0
"One of the fundamental tasks in computer vision is to analyze the motion between two consecutive images. When labeled data is not available, loss functions are divided into consistency and smoothness to enable self-supervised training. This paper focuses on deriving the cost function and introduces an iterative approach that converts the hard L1 smoothness constraint into a multi-layer iterative scheme that is more flexible. This results in more accurate gradients, particularly near non-differential positions, which improves the network's convergence and leads to superior results in tested scenarios. Our approach achieves state-of-the-art results on unsupervised optical flow benchmarks such as MPI Sintel and KITTI 2015. This method can be applied to enhance various architectures and is not limited to the presented pipeline.",1
"Low-quality modalities contain not only a lot of noisy information but also some discriminative features in RGBT tracking. However, the potentials of low-quality modalities are not well explored in existing RGBT tracking algorithms. In this work, we propose a novel duality-gated mutual condition network to fully exploit the discriminative information of all modalities while suppressing the effects of data noise. In specific, we design a mutual condition module, which takes the discriminative information of a modality as the condition to guide feature learning of target appearance in another modality. Such module can effectively enhance target representations of all modalities even in the presence of low-quality modalities. To improve the quality of conditions and further reduce data noise, we propose a duality-gated mechanism and integrate it into the mutual condition module. To deal with the tracking failure caused by sudden camera motion, which often occurs in RGBT tracking, we design a resampling strategy based on optical flow algorithms. It does not increase much computational cost since we perform optical flow calculation only when the model prediction is unreliable and then execute resampling when the sudden camera motion is detected. Extensive experiments on four RGBT tracking benchmark datasets show that our method performs favorably against the state-of-the-art tracking algorithms",0
"The current RGBT tracking algorithms do not fully utilize the potential of low-quality modalities, which contain both noisy information and discriminative features. To address this issue, we propose a novel duality-gated mutual condition network that can effectively exploit the discriminative information of all modalities while suppressing data noise. Our approach incorporates a mutual condition module, which uses the discriminative information from one modality to guide feature learning of target appearance in another modality. This module enhances target representations of all modalities, even when low-quality modalities are present. To further reduce data noise, we introduce a duality-gated mechanism into the mutual condition module. In addition, we address tracking failure caused by sudden camera motion by designing a resampling strategy based on optical flow algorithms. Our method achieves superior performance on four RGBT tracking benchmark datasets compared to state-of-the-art tracking algorithms, while incurring minimal computational cost.",1
"Unsupervised learning of optical flow, which leverages the supervision from view synthesis, has emerged as a promising alternative to supervised methods. However, the objective of unsupervised learning is likely to be unreliable in challenging scenes. In this work, we present a framework to use more reliable supervision from transformations. It simply twists the general unsupervised learning pipeline by running another forward pass with transformed data from augmentation, along with using transformed predictions of original data as the self-supervision signal. Besides, we further introduce a lightweight network with multiple frames by a highly-shared flow decoder. Our method consistently gets a leap of performance on several benchmarks with the best accuracy among deep unsupervised methods. Also, our method achieves competitive results to recent fully supervised methods while with much fewer parameters.",0
"A promising alternative to supervised methods is the unsupervised learning of optical flow, which relies on supervision from view synthesis. However, in challenging scenes, the objective of unsupervised learning may not be reliable. To address this issue, we propose a framework that utilizes more dependable supervision from transformations. This approach involves augmenting the unsupervised learning pipeline by conducting another forward pass with transformed data and using transformed predictions of the original data as the self-supervision signal. Additionally, we introduce a lightweight network with multiple frames through a highly-shared flow decoder. Our method consistently outperforms other deep unsupervised methods on several benchmarks, achieving the highest accuracy. Moreover, our method produces competitive results compared to recent fully supervised methods despite having significantly fewer parameters.",1
"Optical flow estimation is an important yet challenging problem in the field of video analytics. The features of different semantics levels/layers of a convolutional neural network can provide information of different granularity. To exploit such flexible and comprehensive information, we propose a semi-supervised Feature Pyramidal Correlation and Residual Reconstruction Network (FPCR-Net) for optical flow estimation from frame pairs. It consists of two main modules: pyramid correlation mapping and residual reconstruction. The pyramid correlation mapping module takes advantage of the multi-scale correlations of global/local patches by aggregating features of different scales to form a multi-level cost volume. The residual reconstruction module aims to reconstruct the sub-band high-frequency residuals of finer optical flow in each stage. Based on the pyramid correlation mapping, we further propose a correlation-warping-normalization (CWN) module to efficiently exploit the correlation dependency. Experiment results show that the proposed scheme achieves the state-of-the-art performance, with improvement by 0.80, 1.15 and 0.10 in terms of average end-point error (AEE) against competing baseline methods - FlowNet2, LiteFlowNet and PWC-Net on the Final pass of Sintel dataset, respectively.",0
"The estimation of optical flow is a difficult but crucial task in video analytics. Different levels of a convolutional neural network can offer varying degrees of information, and to take advantage of this, we have developed a semi-supervised network called the Feature Pyramidal Correlation and Residual Reconstruction Network (FPCR-Net) for estimating optical flow from pairs of frames. The FPCR-Net comprises two primary modules: pyramid correlation mapping and residual reconstruction. The former combines features of different scales to create a multi-level cost volume and uses multi-scale correlations of global/local patches. The latter aims to reconstruct the high-frequency residuals of finer optical flow in each stage. We have also proposed a correlation-warping-normalization (CWN) module based on the pyramid correlation mapping to make use of correlation dependency. Our proposed method has achieved state-of-the-art performance, with an improvement of 0.80, 1.15, and 0.10 in average end-point error (AEE) as compared to FlowNet2, LiteFlowNet, and PWC-Net respectively, on the Final pass of Sintel dataset.",1
"Independent Sign Language Recognition is a complex visual recognition problem that combines several challenging tasks of Computer Vision due to the necessity to exploit and fuse information from hand gestures, body features and facial expressions. While many state-of-the-art works have managed to deeply elaborate on these features independently, to the best of our knowledge, no work has adequately combined all three information channels to efficiently recognize Sign Language. In this work, we employ SMPL-X, a contemporary parametric model that enables joint extraction of 3D body shape, face and hands information from a single image. We use this holistic 3D reconstruction for SLR, demonstrating that it leads to higher accuracy than recognition from raw RGB images and their optical flow fed into the state-of-the-art I3D-type network for 3D action recognition and from 2D Openpose skeletons fed into a Recurrent Neural Network. Finally, a set of experiments on the body, face and hand features showed that neglecting any of these, significantly reduces the classification accuracy, proving the importance of jointly modeling body shape, facial expression and hand pose for Sign Language Recognition.",0
"Recognizing Sign Language independently poses a complex visual recognition challenge as it involves combining information from hand gestures, body features, and facial expressions. While previous works have independently explored these features, none have effectively fused all three channels to recognize Sign Language efficiently. To address this, we utilize SMPL-X, a modern parametric model that extracts 3D body shape, face, and hand information from a single image. Our holistic 3D reconstruction approach for Sign Language Recognition (SLR) surpasses recognition from raw RGB images and optical flow fed into the state-of-the-art I3D-type network for 3D action recognition and 2D Openpose skeletons fed into a Recurrent Neural Network (RNN). Our experiments confirm that neglecting any of the body, face, or hand features significantly reduces the classification accuracy, emphasizing the importance of jointly modeling all three features in SLR.",1
"The goal of this paper is to formulate a general framework for a constraint-based refinement of the optical flow using variational methods. We demonstrate that for a particular choice of the constraint, formulated as a minimization problem with the quadratic regularization, our results are close to the continuity equation based fluid flow. This closeness to the continuity model is theoretically justified through a modified augmented Lagrangian method and validated numerically. Further, along with the continuity constraint, our model can include geometric constraints as well. The correctness of our process is studied in the Hilbert space setting. Moreover, a special feature of our system is the possibility of a diagonalization by the Cauchy-Riemann operator and transforming it to a diffusion process on the curl and the divergence of the flow. Using the theory of semigroups on the decoupled system, we show that our process preserves the spatial characteristics of the divergence and the vorticities. We perform several numerical experiments and show the results on different datasets.",0
"The aim of this paper is to create a universal framework for refining the optical flow via constraint-based techniques using variational methods. We illustrate that by selecting a specific constraint, which takes the form of a minimization problem with quadratic regularization, our outcomes are in close proximity to the fluid flow based on the continuity equation. This proximity to the continuity model is theoretically justified using a modified augmented Lagrangian method, and verified numerically. Additionally, our model can incorporate geometric constraints in conjunction with the continuity constraint. We assess the accuracy of our process in the Hilbert space setting. Additionally, our system has a unique feature where the Cauchy-Riemann operator can diagonalize it, and it can be transformed into a diffusion process on the curl and divergence of the flow. Through the use of semigroup theory on the decoupled system, we establish that our process conserves the spatial characteristics of the vorticities and divergence. We conduct various numerical experiments and present the results on diverse datasets.",1
"The paper addresses the problem of recognition of actions in video with low inter-class variability such as Table Tennis strokes. Two stream, ""twin"" convolutional neural networks are used with 3D convolutions both on RGB data and optical flow. Actions are recognized by classification of temporal windows. We introduce 3D attention modules and examine their impact on classification efficiency. In the context of the study of sportsmen performances, a corpus of the particular actions of table tennis strokes is considered. The use of attention blocks in the network speeds up the training step and improves the classification scores up to 5% with our twin model. We visualize the impact on the obtained features and notice correlation between attention and player movements and position. Score comparison of state-of-the-art action classification method and proposed approach with attentional blocks is performed on the corpus. Proposed model with attention blocks outperforms previous model without them and our baseline.",0
"The primary focus of this paper is to address the challenge of identifying actions in videos with limited inter-class variability, such as Table Tennis strokes. To this end, we employ two identical convolutional neural networks that use 3D convolutions on both RGB data and optical flow. Our approach relies on temporal window classification to recognize actions. We also integrate 3D attention modules to improve classification efficiency. Specifically, we examine the influence of attention blocks on the efficacy of our model and observe a 5% improvement in classification scores compared to our twin model without attention blocks. We apply our method to a corpus of Table Tennis stroke actions and visualize the impact of attention on player movements and position. Finally, we compare our model's performance with state-of-the-art action classification methods and demonstrate that our proposed approach with attention blocks outperforms previous models and our baseline model.",1
"Learning the necessary high-level reasoning for video stabilization without the help of optical flow has proved to be one of the most challenging tasks in the field of computer vision. In this work, we present an iterative frame interpolation strategy to generate a novel dataset that is diverse enough to formulate video stabilization as a supervised learning problem unassisted by optical flow. A major benefit of treating video stabilization as a pure RGB based generative task over the conventional optical flow assisted approaches is the preservation of content and resolution, which is usually obstructed in the latter approaches. To do so, we provide a new video stabilization dataset and train an efficient network that can produce competitive stabilization results in a fraction of the time taken to do the same with the recent iterative frame interpolation schema. Our method provides qualitatively and quantitatively better results than those generated through state-of-the-art video stabilization methods. To the best of our knowledge, this is the only work that demonstrates the importance of perspective in formulating video stabilization as a deep learning problem instead of replacing it with an inter-frame motion measure",0
"The challenge of acquiring the necessary high-level reasoning for video stabilization without relying on optical flow has proven to be a difficult task in computer vision. This study introduces an iterative frame interpolation technique that generates a varied dataset to formulate video stabilization as a supervised learning problem without needing optical flow. The approach of treating video stabilization as a purely RGB-based generative task rather than relying on conventional optical flow methods has the advantage of preserving content and resolution, which can be obstructed in the latter approach. A new video stabilization dataset is provided, and an efficient network is trained to produce competitive stabilization results in a shorter amount of time than the recent iterative frame interpolation method. Our approach results in better qualitative and quantitative outcomes than state-of-the-art video stabilization methods. This study is the only one to demonstrate the significance of perspective in formulating video stabilization as a deep learning problem rather than replacing it with an inter-frame motion measure.",1
"In optical flow estimation task, coarse-to-fine (C2F) warping strategy is widely used to deal with the large displacement problem and provides efficiency and speed. However, limited by the small search range between the first images and warped second images, current coarse-to-fine optical flow networks fail to capture small and fast-moving objects which disappear at coarse resolution levels. To address this problem, we introduce a lightweight but effective Global Matching Component (GMC) to grab global matching features. We propose a new Hybrid Matching Optical Flow Network (HMFlow) by integrating GMC into existing coarse-to-fine networks seamlessly. Besides keeping in high accuracy and small model size, our proposed HMFlow can apply global matching features to guide the network to discover the small and fast-moving objects mismatched by local matching features. We also build a new dataset, named Small and Fast-Moving Chairs (SFChairs), for evaluation. The experimental results show that our proposed network achieves considerable performance, especially at regions with small and fast-moving objects.",0
"The coarse-to-fine (C2F) warping strategy is commonly used in optical flow estimation tasks to address the challenge of large displacement and improve efficiency. However, current C2F optical flow networks are limited by the small search range between images, resulting in the failure to capture small and fast-moving objects which disappear at coarse resolution levels. In response to this issue, we introduce a Global Matching Component (GMC) that is lightweight yet effective in capturing global matching features. We seamlessly integrate GMC into existing C2F networks to create a Hybrid Matching Optical Flow Network (HMFlow). Our proposed HMFlow maintains high accuracy and a small model size while also using global matching features to guide the network in detecting small and fast-moving objects that may be missed by local matching features. We also present a new dataset, SFChairs, for evaluation purposes. Results from our experiments demonstrate that our proposed network performs well, especially in regions with small and fast-moving objects.",1
"Abnormal event detection (AED) in urban surveillance videos has multiple challenges. Unlike other computer vision problems, the AED is not solely dependent on the content of frames. It also depends on the appearance of the objects and their movements in the scene. Various methods have been proposed to address the AED problem. Among those, deep learning based methods show the best results. This paper is based on deep learning methods and provides an effective way to detect and locate abnormal events in videos by handling spatio temporal data. This paper uses generative adversarial networks (GANs) and performs transfer learning algorithms on pre trained convolutional neural network (CNN) which result in an accurate and efficient model. The efficiency of the model is further improved by processing the optical flow information of the video. This paper runs experiments on two benchmark datasets for AED problem (UCSD Peds1 and UCSD Peds2) and compares the results with other previous methods. The comparisons are based on various criteria such as area under curve (AUC) and true positive rate (TPR). Experimental results show that the proposed method can effectively detect and locate abnormal events in crowd scenes.",0
"Urban surveillance videos pose multiple challenges when it comes to detecting abnormal events. Unlike other computer vision problems, the content of frames alone cannot determine if an event is abnormal. Appearance of objects and their movements in the scene also play a crucial role in AED. Various methods have been proposed to address this problem, with deep learning based techniques showing the most promising results. This paper proposes an effective way to detect and locate abnormal events in videos by leveraging spatio temporal data and using generative adversarial networks (GANs) and transfer learning algorithms on pre-trained convolutional neural networks (CNNs). The accuracy and efficiency of the proposed method is further improved by processing the optical flow information of the video. Experimental results on two benchmark datasets (UCSD Peds1 and UCSD Peds2) demonstrate that the proposed method outperforms previous methods based on criteria such as area under curve (AUC) and true positive rate (TPR) and can effectively detect and locate abnormal events in crowd scenes.",1
"Visual odometry networks commonly use pretrained optical flow networks in order to derive the ego-motion between consecutive frames. The features extracted by these networks represent the motion of all the pixels between frames. However, due to the existence of dynamic objects and texture-less surfaces in the scene, the motion information for every image region might not be reliable for inferring odometry due to the ineffectiveness of dynamic objects in derivation of the incremental changes in position. Recent works in this area lack attention mechanisms in their structures to facilitate dynamic reweighing of the feature maps for extracting more refined egomotion information. In this paper, we explore the effectiveness of self-attention in visual odometry. We report qualitative and quantitative results against the SOTA methods. Furthermore, saliency-based studies alongside specially designed experiments are utilized to investigate the effect of self-attention on VO. Our experiments show that using self-attention allows for the extraction of better features while achieving a better odometry performance compared to networks that lack such structures.",0
"Pretrained optical flow networks are frequently used in visual odometry networks to determine the ego-motion between consecutive frames. These networks extract features that reveal the motion of all pixels between frames. However, dynamic objects and texture-less surfaces in the scene may cause the motion information for every image region to be unreliable for inferring odometry. This is because dynamic objects are ineffective in deriving incremental changes in position. Recent research in this area lacks attention mechanisms to enable dynamic reweighing of feature maps for extracting more refined egomotion information. Our paper examines the effectiveness of self-attention in visual odometry and presents qualitative and quantitative results compared to state-of-the-art methods. We also conduct saliency-based studies and specially designed experiments to investigate the impact of self-attention on VO. Our experimental results demonstrate that self-attention enables the extraction of better features, resulting in superior odometry performance compared to networks that lack this structure.",1
"Skeleton-based action recognition has attracted research attentions in recent years. One common drawback in currently popular skeleton-based human action recognition methods is that the sparse skeleton information alone is not sufficient to fully characterize human motion. This limitation makes several existing methods incapable of correctly classifying action categories which exhibit only subtle motion differences. In this paper, we propose a novel framework for employing human pose skeleton and joint-centered light-weight information jointly in a two-stream graph convolutional network, namely, JOLO-GCN. Specifically, we use Joint-aligned optical Flow Patches (JFP) to capture the local subtle motion around each joint as the pivotal joint-centered visual information. Compared to the pure skeleton-based baseline, this hybrid scheme effectively boosts performance, while keeping the computational and memory overheads low. Experiments on the NTU RGB+D, NTU RGB+D 120, and the Kinetics-Skeleton dataset demonstrate clear accuracy improvements attained by the proposed method over the state-of-the-art skeleton-based methods.",0
"In recent years, there has been a surge of research interest in skeleton-based action recognition. However, current popular methods have a common shortcoming - the sparse skeleton information alone is insufficient for accurately characterizing human motion. This drawback renders several existing approaches incapable of correctly classifying action categories that exhibit subtle motion differences. In this paper, we introduce a novel framework called JOLO-GCN that combines human pose skeleton and joint-centered light-weight information in a two-stream graph convolutional network. Our approach uses Joint-aligned optical Flow Patches (JFP) to capture the local subtle motion around each joint and provide pivotal joint-centered visual information. This hybrid scheme effectively enhances performance while minimizing computational and memory overheads compared to the pure skeleton-based baseline. Our experiments on the NTU RGB+D, NTU RGB+D 120, and the Kinetics-Skeleton dataset show significant accuracy improvements over state-of-the-art skeleton-based methods.",1
"Complex blur such as the mixup of space-variant and space-invariant blur, which is hard to model mathematically, widely exists in real images. In this paper, we propose a novel image deblurring method that does not need to estimate blur kernels. We utilize a pair of images that can be easily acquired in low-light situations: (1) a blurred image taken with low shutter speed and low ISO noise; and (2) a noisy image captured with high shutter speed and high ISO noise. Slicing the blurred image into patches, we extend the Gaussian mixture model (GMM) to model the underlying intensity distribution of each patch using the corresponding patches in the noisy image. We compute patch correspondences by analyzing the optical flow between the two images. The Expectation Maximization (EM) algorithm is utilized to estimate the parameters of GMM. To preserve sharp features, we add an additional bilateral term to the objective function in the M-step. We eventually add a detail layer to the deblurred image for refinement. Extensive experiments on both synthetic and real-world data demonstrate that our method outperforms state-of-the-art techniques, in terms of robustness, visual quality, and quantitative metrics.",0
"Real images often contain complex blurs that are difficult to mathematically model, such as a combination of space-variant and space-invariant blur. This paper presents a new method for deblurring images that does not require the estimation of blur kernels. Instead, we use a pair of images captured in low-light conditions: a blurred image with a low shutter speed and low ISO noise, and a noisy image with a high shutter speed and high ISO noise. We divide the blurred image into patches and use the corresponding patches in the noisy image to extend the Gaussian mixture model (GMM) to model the underlying intensity distribution in each patch. We analyze the optical flow between the two images to compute patch correspondences, and use the Expectation Maximization (EM) algorithm to estimate the GMM parameters. To preserve sharp features, we add a bilateral term to the M-step objective function. Finally, we add a detail layer to refine the deblurred image. Our experiments on synthetic and real-world data show that our method is more robust and produces higher quality results than existing techniques, according to both visual and quantitative metrics.",1
"Facial features deformed according to the intended facial expression. Specific facial features are associated with specific facial expression, i.e. happy means the deformation of mouth. This paper presents the study of facial feature deformation for each facial expression by using an optical flow algorithm and segmented into three different regions of interest. The deformation of facial features shows the relation between facial the and facial expression. Based on the experiments, the deformations of eye and mouth are significant in all expressions except happy. For happy expression, cheeks and mouths are the significant regions. This work also suggests that different facial features' intensity varies in the way that they contribute to the recognition of the different facial expression intensity. The maximum magnitude across all expressions is shown by the mouth for surprise expression which is 9x10-4. While the minimum magnitude is shown by the mouth for angry expression which is 0.4x10-4.",0
"The intended facial expression causes deformities in facial features. Each facial expression corresponds to specific facial features, such as the mouth for a happy expression. This study uses an optical flow algorithm to analyze the deformation of facial features in three regions of interest for each expression. The results demonstrate the correlation between facial features and expressions. The experiments reveal that the eyes and mouth exhibit significant deformation in all expressions except for happy, where the cheeks and mouth are the predominant regions. Additionally, the intensity of different facial features varies and contributes differently to the recognition of facial expressions. The maximum magnitude of deformation is observed in the mouth for the surprise expression (9x10-4), while the minimum magnitude is observed in the mouth for the angry expression (0.4x10-4).",1
"The importance of inference in Machine Learning (ML) has led to an explosive number of different proposals in ML, and particularly in Deep Learning. In an attempt to reduce the complexity of Convolutional Neural Networks, we propose a Volterra filter-inspired Network architecture. This architecture introduces controlled non-linearities in the form of interactions between the delayed input samples of data. We propose a cascaded implementation of Volterra Filtering so as to significantly reduce the number of parameters required to carry out the same classification task as that of a conventional Neural Network. We demonstrate an efficient parallel implementation of this Volterra Neural Network (VNN), along with its remarkable performance while retaining a relatively simpler and potentially more tractable structure. Furthermore, we show a rather sophisticated adaptation of this network to nonlinearly fuse the RGB (spatial) information and the Optical Flow (temporal) information of a video sequence for action recognition. The proposed approach is evaluated on UCF-101 and HMDB-51 datasets for action recognition, and is shown to outperform state of the art CNN approaches.",0
"Machine Learning (ML) has seen a surge in proposals, especially in Deep Learning, due to the significance of inference. With the aim of reducing the complexity of Convolutional Neural Networks, our proposed architecture draws inspiration from Volterra filters. This architecture introduces controlled non-linearities by incorporating interactions between delayed input samples of data. We suggest a cascaded implementation of Volterra Filtering to reduce the parameter count while achieving the same classification task as a typical Neural Network. Our Volterra Neural Network (VNN) demonstrates impressive performance with a simpler and potentially more manageable structure. Additionally, we present a sophisticated adaptation of this network that combines RGB and Optical Flow information to recognize actions in video sequences. The proposed approach surpasses state-of-the-art CNN methods as evaluated on UCF-101 and HMDB-51 datasets.",1
"Contrary to the ongoing trend in automotive applications towards usage of more diverse and more sensors, this work tries to solve the complex scene flow problem under a monocular camera setup, i.e. using a single sensor. Towards this end, we exploit the latest achievements in single image depth estimation, optical flow, and sparse-to-dense interpolation and propose a monocular combination approach (MonoComb) to compute dense scene flow. MonoComb uses optical flow to relate reconstructed 3D positions over time and interpolates occluded areas. This way, existing monocular methods are outperformed in dynamic foreground regions which leads to the second best result among the competitors on the challenging KITTI 2015 scene flow benchmark.",0
"In contrast to the current trend in automotive technology that favors the use of a wider variety of sensors, this study addresses the intricate scene flow problem using only one sensor, namely a monocular camera setup. To tackle this challenge, the researchers utilized recent advancements in single image depth estimation, optical flow, and sparse-to-dense interpolation to propose a monocular combination approach called MonoComb. By using optical flow to relate reconstructed 3D positions over time and interpolating occluded areas, MonoComb outperforms existing monocular methods in dynamic foreground regions, achieving the second-best result among competitors in the challenging KITTI 2015 scene flow benchmark.",1
"Video semantic segmentation is active in recent years benefited from the great progress of image semantic segmentation. For such a task, the per-frame image segmentation is generally unacceptable in practice due to high computation cost. To tackle this issue, many works use the flow-based feature propagation to reuse the features of previous frames. However, the optical flow estimation inevitably suffers inaccuracy and then causes the propagated features distorted. In this paper, we propose distortion-aware feature correction to alleviate the issue, which improves video segmentation performance by correcting distorted propagated features. To be specific, we firstly propose to transfer distortion patterns from feature into image space and conduct effective distortion map prediction. Benefited from the guidance of distortion maps, we proposed Feature Correction Module (FCM) to rectify propagated features in the distorted areas. Our proposed method can significantly boost the accuracy of video semantic segmentation at a low price. The extensive experimental results on Cityscapes and CamVid show that our method outperforms the recent state-of-the-art methods.",0
"In recent years, video semantic segmentation has seen significant advancements due to the progress in image semantic segmentation. However, per-frame image segmentation is impractical because it requires high computation costs. To address this issue, many studies use flow-based feature propagation to reuse features from previous frames. Unfortunately, optical flow estimation often lacks accuracy and causes distorted propagated features. This paper introduces distortion-aware feature correction to improve video segmentation performance by correcting these distorted features. Specifically, the method transfers distortion patterns from features into image space and predicts effective distortion maps. The proposed Feature Correction Module (FCM) rectifies propagated features in the distorted areas with the guidance of these maps. This method is cost-effective and significantly improves accuracy compared to recent state-of-the-art methods. Extensive experiments on Cityscapes and CamVid demonstrate the effectiveness of our proposed method.",1
"Multimodal large-scale datasets for outdoor scenes are mostly designed for urban driving problems. The scenes are highly structured and semantically different from scenarios seen in nature-centered scenes such as gardens or parks. To promote machine learning methods for nature-oriented applications, such as agriculture and gardening, we propose the multimodal synthetic dataset for Enclosed garDEN scenes (EDEN). The dataset features more than 300K images captured from more than 100 garden models. Each image is annotated with various low/high-level vision modalities, including semantic segmentation, depth, surface normals, intrinsic colors, and optical flow. Experimental results on the state-of-the-art methods for semantic segmentation and monocular depth prediction, two important tasks in computer vision, show positive impact of pre-training deep networks on our dataset for unstructured natural scenes. The dataset and related materials will be available at https://lhoangan.github.io/eden.",0
"Multimodal large-scale datasets that focus on outdoor scenes are typically tailored to address urban driving issues. These environments exhibit a high degree of organization and differ semantically from natural surroundings such as parks and gardens. To facilitate the development of machine learning algorithms for agricultural and gardening purposes, we propose the creation of a new multimodal synthetic dataset named Enclosed garDEN scenes (EDEN). The EDEN dataset comprises over 300,000 images derived from more than 100 garden models, each of which is annotated with a range of low/high-level vision modalities, including semantic segmentation, depth, surface normals, intrinsic colors, and optical flow. Our experimental results on state-of-the-art methods for computer vision tasks, including semantic segmentation and monocular depth prediction, demonstrate a positive impact arising from the pre-training of deep networks on our dataset, which is specifically designed to address unstructured natural scenes. The EDEN dataset and associated materials will be made available at https://lhoangan.github.io/eden.",1
"Micro-expression (ME) recognition plays a crucial role in a wide range of applications, particularly in public security and psychotherapy. Recently, traditional methods rely excessively on machine learning design and the recognition rate is not high enough for its practical application because of its short duration and low intensity. On the other hand, some methods based on deep learning also cannot get high accuracy due to problems such as the imbalance of databases. To address these problems, we design a multi-stream convolutional neural network (MSCNN) for ME recognition in this paper. Specifically, we employ EVM and optical flow to magnify and visualize subtle movement changes in MEs and extract the masks from the optical flow images. And then, we add the masks, optical flow images, and grayscale images into the MSCNN. After that, in order to overcome the imbalance of databases, we added a random over-sampler after the Dense Layer of the neural network. Finally, extensive experiments are conducted on two public ME databases: CASME II and SAMM. Compared with many recent state-of-the-art approaches, our method achieves more promising recognition results.",0
"The recognition of micro-expressions (ME) is essential in various fields, including public safety and psychotherapy. However, current techniques rely heavily on machine learning and have low recognition rates due to the brevity and subtlety of MEs. Moreover, deep learning approaches face issues such as database imbalance, which hinders their accuracy. To address these challenges, we present a multi-stream convolutional neural network (MSCNN) for ME recognition. Our MSCNN utilizes EVM and optical flow to enhance and display tiny changes in MEs and extract masks from optical flow images. We then integrate these masks, optical flow images, and grayscale images into the MSCNN. To overcome the database imbalance, we introduce a random over-sampler after the Dense Layer of the neural network. We evaluate our approach on two public ME databases, CASME II and SAMM, and demonstrate better recognition results compared to existing state-of-the-art methods.",1
"Capsule networks (CapsNets) have recently shown promise to excel in most computer vision tasks, especially pertaining to scene understanding. In this paper, we explore CapsNet's capabilities in optical flow estimation, a task at which convolutional neural networks (CNNs) have already outperformed other approaches. We propose a CapsNet-based architecture, termed FlowCaps, which attempts to a) achieve better correspondence matching via finer-grained, motion-specific, and more-interpretable encoding crucial for optical flow estimation, b) perform better-generalizable optical flow estimation, c) utilize lesser ground truth data, and d) significantly reduce the computational complexity in achieving good performance, in comparison to its CNN-counterparts.",0
"Recently, Capsule networks (CapsNets) have demonstrated potential for excelling in computer vision tasks, particularly in relation to scene understanding. This study investigates the capabilities of CapsNets in optical flow estimation, a task that convolutional neural networks (CNNs) have already surpassed other methods. A CapsNet-based architecture named FlowCaps is proposed to achieve a) improved correspondence matching by utilizing more detailed, motion-specific, and understandable encoding essential for optical flow estimation, b) better generalization in optical flow estimation, c) use fewer ground truth data, and d) significantly reduce computational complexity when compared to CNNs while still achieving good performance.",1
"Accurate object segmentation is a crucial task in the context of robotic manipulation. However, creating sufficient annotated training data for neural networks is particularly time consuming and often requires manual labeling. To this end, we propose a simple, yet robust solution for learning to segment unknown objects grasped by a robot. Specifically, we exploit motion and temporal cues in RGB video sequences. Using optical flow estimation we first learn to predict segmentation masks of our given manipulator. Then, these annotations are used in combination with motion cues to automatically distinguish between background, manipulator and unknown, grasped object. In contrast to existing systems our approach is fully self-supervised and independent of precise camera calibration, 3D models or potentially imperfect depth data. We perform a thorough comparison with alternative baselines and approaches from literature. The object masks and views are shown to be suitable training data for segmentation networks that generalize to novel environments and also allow for watertight 3D reconstruction.",0
"Precise object segmentation carries significant importance in the realm of robotic manipulation. Yet, generating ample labeled data for neural networks can be an arduous and time-consuming process, often involving manual labeling. As a solution, we present a robust and straightforward method for learning to segment unknown objects held by a robot. Our approach employs motion and temporal cues in RGB video sequences, utilizing optical flow estimation to anticipate segmentation masks of the manipulator. These annotations, in combination with motion cues, automatically differentiate between the background, manipulator, and the object being held. In contrast to other systems, our approach is entirely self-supervised and not reliant on precise camera calibration, 3D models, or potentially imperfect depth data. We conducted a comprehensive comparison with other baselines and approaches from the literature and found that our object masks and views are ideal training data for segmentation networks that generalize to novel environments and permit watertight 3D reconstruction.",1
"The construction of models for video action classification progresses rapidly. However, the performance of those models can still be easily improved by ensembling with the same models trained on different modalities (e.g. Optical flow). Unfortunately, it is computationally expensive to use several modalities during inference. Recent works examine the ways to integrate advantages of multi-modality into a single RGB-model. Yet, there is still a room for improvement. In this paper, we explore the various methods to embed the ensemble power into a single model. We show that proper initialization, as well as mutual modality learning, enhances single-modality models. As a result, we achieve state-of-the-art results in the Something-Something-v2 benchmark.",0
"The progress in developing models for video action classification has been swift, but their performance can still be enhanced by combining them with models trained on different modalities like Optical flow. Unfortunately, this is computationally expensive during inference. Recent research has focused on integrating the benefits of multi-modality into a single RGB-model, but there is scope for improvement. This paper explores different techniques to incorporate ensemble power into a single model. Our findings demonstrate that proper initialization and mutual modality learning can improve single-modality models and lead to state-of-the-art results in the Something-Something-v2 benchmark.",1
"Interpolation of sparse pixel information towards a dense target resolution finds its application across multiple disciplines in computer vision. State-of-the-art interpolation of motion fields applies model-based interpolation that makes use of edge information extracted from the target image. For depth completion, data-driven learning approaches are widespread. Our work is inspired by latest trends in depth completion that tackle the problem of dense guidance for sparse information. We extend these ideas and create a generic cross-domain architecture that can be applied for a multitude of interpolation problems like optical flow, scene flow, or depth completion. In our experiments, we show that our proposed concept of Sparse Spatial Guided Propagation (SSGP) achieves improvements to robustness, accuracy, or speed compared to specialized algorithms.",0
"The use of interpolation to fill in gaps in pixel information is widely used in various fields of computer vision. The latest advancements in motion field interpolation rely on model-based techniques that utilize edge data from the intended image. Similarly, data-driven learning methods are commonly used in depth completion. Our research is influenced by recent developments in depth completion, which aim to address the challenge of creating a dense guide for sparse data. We have expanded on these ideas to create a versatile architecture that can be applied to a range of interpolation problems such as optical flow, scene flow, and depth completion. Our experiments demonstrate that our Sparse Spatial Guided Propagation (SSGP) approach is more robust, accurate, and faster than specialized algorithms.",1
"The interpretation of ego motion and scene change is a fundamental task for mobile robots. Optical flow information can be employed to estimate motion in the surroundings. Recently, unsupervised optical flow estimation has become a research hotspot. However, unsupervised approaches are often easy to be unreliable on partially occluded or texture-less regions. To deal with this problem, we propose CoT-AMFlow in this paper, an unsupervised optical flow estimation approach. In terms of the network architecture, we develop an adaptive modulation network that employs two novel module types, flow modulation modules (FMMs) and cost volume modulation modules (CMMs), to remove outliers in challenging regions. As for the training paradigm, we adopt a co-teaching strategy, where two networks simultaneously teach each other about challenging regions to further improve accuracy. Experimental results on the MPI Sintel, KITTI Flow and Middlebury Flow benchmarks demonstrate that our CoT-AMFlow outperforms all other state-of-the-art unsupervised approaches, while still running in real time. Our project page is available at https://sites.google.com/view/cot-amflow.",0
"Mobile robots require the ability to interpret ego motion and scene changes, making optical flow information a valuable tool for motion estimation in the surrounding environment. While unsupervised optical flow estimation has recently gained attention, these approaches are often unreliable in regions that are partially occluded or lack texture. To address this issue, our paper proposes CoT-AMFlow, an unsupervised optical flow estimation approach that utilizes an adaptive modulation network with flow modulation modules (FMMs) and cost volume modulation modules (CMMs) to remove outliers in challenging regions. The training paradigm involves a co-teaching strategy where two networks teach each other about challenging regions to improve accuracy. Experimental results demonstrate that CoT-AMFlow outperforms other unsupervised approaches while maintaining real-time performance. To learn more about our approach, please visit our project page at https://sites.google.com/view/cot-amflow.",1
"In this paper, we propose a spatio-temporal contextual network, STC-Flow, for optical flow estimation. Unlike previous optical flow estimation approaches with local pyramid feature extraction and multi-level correlation, we propose a contextual relation exploration architecture by capturing rich long-range dependencies in spatial and temporal dimensions. Specifically, STC-Flow contains three key context modules - pyramidal spatial context module, temporal context correlation module and recurrent residual contextual upsampling module, to build the relationship in each stage of feature extraction, correlation, and flow reconstruction, respectively. Experimental results indicate that the proposed scheme achieves the state-of-the-art performance of two-frame based methods on the Sintel dataset and the KITTI 2012/2015 datasets.",0
"This paper introduces a novel approach to optical flow estimation called STC-Flow, which utilizes a spatio-temporal contextual network. Unlike previous methods that rely on local pyramid feature extraction and multi-level correlation, we propose an architecture that explores contextual relations by capturing long-range dependencies in both spatial and temporal dimensions. STC-Flow comprises three context modules - pyramidal spatial context, temporal context correlation, and recurrent residual contextual upsampling - which are used to establish relationships in feature extraction, correlation, and flow reconstruction. We conducted experiments on the Sintel dataset and the KITTI 2012/2015 datasets, and our results indicate that our proposed approach outperforms current two-frame based methods and achieves state-of-the-art performance.",1
"Learning matching costs has been shown to be critical to the success of the state-of-the-art deep stereo matching methods, in which 3D convolutions are applied on a 4D feature volume to learn a 3D cost volume. However, this mechanism has never been employed for the optical flow task. This is mainly due to the significantly increased search dimension in the case of optical flow computation, ie, a straightforward extension would require dense 4D convolutions in order to process a 5D feature volume, which is computationally prohibitive. This paper proposes a novel solution that is able to bypass the requirement of building a 5D feature volume while still allowing the network to learn suitable matching costs from data. Our key innovation is to decouple the connection between 2D displacements and learn the matching costs at each 2D displacement hypothesis independently, ie, displacement-invariant cost learning. Specifically, we apply the same 2D convolution-based matching net independently on each 2D displacement hypothesis to learn a 4D cost volume. Moreover, we propose a displacement-aware projection layer to scale the learned cost volume, which reconsiders the correlation between different displacement candidates and mitigates the multi-modal problem in the learned cost volume. The cost volume is then projected to optical flow estimation through a 2D soft-argmin layer. Extensive experiments show that our approach achieves state-of-the-art accuracy on various datasets, and outperforms all published optical flow methods on the Sintel benchmark.",0
"The success of advanced deep stereo matching methods relies heavily on understanding matching costs. These methods apply 3D convolutions to a 4D feature volume to learn a 3D cost volume. However, this approach hasn't been applied to the optical flow task because of the increased search dimension, which would require computationally expensive dense 4D convolutions to process a 5D feature volume. In this paper, we introduce a new method that allows the network to learn matching costs without building a 5D feature volume. Our approach is based on displacement-invariant cost learning, which decouples the connection between 2D displacements and learns matching costs at each 2D displacement hypothesis independently. We apply a 2D convolution-based matching net to each hypothesis, which results in a 4D cost volume. Our method also includes a displacement-aware projection layer that mitigates the multi-modal problem by scaling the learned cost volume while considering the correlation between different displacement candidates. Finally, we project the cost volume to optical flow estimation through a 2D soft-argmin layer. Our experiments demonstrate that our approach is highly accurate and outperforms other published optical flow methods on the Sintel benchmark.",1
"Automatic fall detection is a vital technology for ensuring the health and safety of people. Home-based camera systems for fall detection often put people's privacy at risk. Thermal cameras can partially or fully obfuscate facial features, thus preserving the privacy of a person. Another challenge is the less occurrence of falls in comparison to the normal activities of daily living. As fall occurs rarely, it is non-trivial to learn algorithms due to class imbalance. To handle these problems, we formulate fall detection as an anomaly detection within an adversarial framework using thermal imaging. We present a novel adversarial network that comprises of two-channel 3D convolutional autoencoders which reconstructs the thermal data and the optical flow input sequences respectively. We introduce a technique to track the region of interest, a region-based difference constraint, and a joint discriminator to compute the reconstruction error. A larger reconstruction error indicates the occurrence of a fall. The experiments on a publicly available thermal fall dataset show the superior results obtained compared to the standard baseline.",0
"Ensuring the health and safety of individuals is highly dependent on automatic fall detection technology. However, the use of home-based camera systems for this purpose can compromise a person's privacy. To address this issue, thermal cameras have the ability to partially or fully obscure facial features, thereby safeguarding privacy. Moreover, the infrequency of falls compared to daily activities presents a challenge in developing algorithms for fall detection, due to class imbalance. To overcome these obstacles, an adversarial framework is utilized for anomaly detection, employing thermal imaging. This approach involves a novel adversarial network consisting of two-channel 3D convolutional autoencoders that reconstruct both thermal data and optical flow input sequences. Additionally, a region-based difference constraint and joint discriminator are utilized to track the region of interest and calculate the reconstruction error, respectively. A greater reconstruction error signifies the occurrence of a fall. Results of experiments conducted on a publicly available thermal fall dataset demonstrate the superiority of this method compared to the standard baseline.",1
"Applying image processing algorithms independently to each video frame often leads to temporal inconsistency in the resulting video. To address this issue, we present a novel and general approach for blind video temporal consistency. Our method is only trained on a pair of original and processed videos directly instead of a large dataset. Unlike most previous methods that enforce temporal consistency with optical flow, we show that temporal consistency can be achieved by training a convolutional network on a video with the Deep Video Prior. Moreover, a carefully designed iteratively reweighted training strategy is proposed to address the challenging multimodal inconsistency problem. We demonstrate the effectiveness of our approach on 7 computer vision tasks on videos. Extensive quantitative and perceptual experiments show that our approach obtains superior performance than state-of-the-art methods on blind video temporal consistency. Our source codes are publicly available at github.com/ChenyangLEI/deep-video-prior.",0
"The application of image processing algorithms to individual video frames can result in inconsistencies over time. To tackle this issue, we have devised a new and universal method for achieving blind video temporal consistency. Our approach involves training on a pair of original and processed videos directly instead of a large dataset. Rather than relying on optical flow like most previous methods, our method employs a convolutional network trained on a video with the Deep Video Prior to achieve temporal consistency. We also propose an iteratively reweighted training strategy to deal with the challenging multimodal inconsistency problem. Our approach has been tested on 7 computer vision tasks on videos, and extensive quantitative and perceptual experiments have shown that it outperforms state-of-the-art methods in blind video temporal consistency. Our source codes can be accessed on github.com/ChenyangLEI/deep-video-prior.",1
"Video-based human pose estimation in crowded scenes is a challenging problem due to occlusion, motion blur, scale variation and viewpoint change, etc. Prior approaches always fail to deal with this problem because of (1) lacking of usage of temporal information; (2) lacking of training data in crowded scenes. In this paper, we focus on improving human pose estimation in videos of crowded scenes from the perspectives of exploiting temporal context and collecting new data. In particular, we first follow the top-down strategy to detect persons and perform single-person pose estimation for each frame. Then, we refine the frame-based pose estimation with temporal contexts deriving from the optical-flow. Specifically, for one frame, we forward the historical poses from the previous frames and backward the future poses from the subsequent frames to current frame, leading to stable and accurate human pose estimation in videos. In addition, we mine new data of similar scenes to HIE dataset from the Internet for improving the diversity of training set. In this way, our model achieves best performance on 7 out of 13 videos and 56.33 average w\_AP on test dataset of HIE challenge.",0
"The task of detecting human poses in videos of crowded scenes is quite difficult due to several factors such as occlusion, motion blur, scale variation, and changes in viewpoint. Previous approaches have failed to address this problem mainly because of the lack of temporal information usage and inadequate training data for crowded scenes. This research paper aims to enhance human pose estimation in videos of crowded scenes by utilizing temporal context and collecting more data. The proposed method involves detecting individuals and performing single-person pose estimation in each frame using the top-down approach. The frame-based pose estimation is then refined using optical flow-based temporal contexts, which involve propagating the historical poses from the previous frames and future poses from the subsequent frames into the current frame. This approach results in stable and accurate human pose estimation in videos. Additionally, new data collected from similar scenes to the HIE dataset from the internet is utilized to improve the diversity of the training set. The model proposed in this paper achieves the best performance on 7 out of 13 videos and an average w\_AP of 56.33 on the test dataset of the HIE challenge.",1
"This paper presents our solution to ACM MM challenge: Large-scale Human-centric Video Analysis in Complex Events\cite{lin2020human}; specifically, here we focus on Track3: Crowd Pose Tracking in Complex Events. Remarkable progress has been made in multi-pose training in recent years. However, how to track the human pose in crowded and complex environments has not been well addressed. We formulate the problem as several subproblems to be solved. First, we use a multi-object tracking method to assign human ID to each bounding box generated by the detection model. After that, a pose is generated to each bounding box with ID. At last, optical flow is used to take advantage of the temporal information in the videos and generate the final pose tracking result.",0
"Our solution to the ACM MM challenge of Large-scale Human-centric Video Analysis in Complex Events is presented in this paper, with a specific focus on Track3: Crowd Pose Tracking in Complex Events. Although there has been notable progress in multi-pose training, the issue of tracking human poses in crowded and complex environments remains unresolved. To address this, we have formulated the problem into several subproblems. Initially, we employ a multi-object tracking approach to allocate a human ID to each bounding box produced by the detection model. Then, a pose is assigned to each bounding box with an ID. Finally, we use optical flow to leverage the temporal data in the videos and produce the ultimate pose tracking outcome.",1
"In this paper, we introduce a new benchmark dataset named IPN Hand with sufficient size, variety, and real-world elements able to train and evaluate deep neural networks. This dataset contains more than 4,000 gesture samples and 800,000 RGB frames from 50 distinct subjects. We design 13 different static and dynamic gestures focused on interaction with touchless screens. We especially consider the scenario when continuous gestures are performed without transition states, and when subjects perform natural movements with their hands as non-gesture actions. Gestures were collected from about 30 diverse scenes, with real-world variation in background and illumination. With our dataset, the performance of three 3D-CNN models is evaluated on the tasks of isolated and continuous real-time HGR. Furthermore, we analyze the possibility of increasing the recognition accuracy by adding multiple modalities derived from RGB frames, i.e., optical flow and semantic segmentation, while keeping the real-time performance of the 3D-CNN model. Our empirical study also provides a comparison with the publicly available nvGesture (NVIDIA) dataset. The experimental results show that the state-of-the-art ResNext-101 model decreases about 30% accuracy when using our real-world dataset, demonstrating that the IPN Hand dataset can be used as a benchmark, and may help the community to step forward in the continuous HGR. Our dataset and pre-trained models used in the evaluation are publicly available at https://github.com/GibranBenitez/IPN-hand.",0
"A novel benchmark dataset called IPN Hand is presented in this paper, which is of adequate size, diversity, and incorporates real-world characteristics to facilitate the training and assessment of deep neural networks. The dataset comprises more than 4,000 gesture samples and 800,000 RGB frames from 50 distinctive subjects, including 13 static and dynamic gestures designed to interact with touchless screens. The focus is on continuous gestures without transitional states and natural hand movements as non-gesture actions, collected from approximately 30 diverse scenes with real-world variation in background and illumination. We assess the performance of three 3D-CNN models on isolated and continuous real-time hand gesture recognition tasks using our dataset and analyze the possibility of increasing recognition accuracy by incorporating multiple modalities derived from RGB frames such as optical flow and semantic segmentation. Our empirical analysis also includes a comparison with the publicly available nvGesture (NVIDIA) dataset. The experimental results show that the state-of-the-art ResNext-101 model experiences a 30% decrease in accuracy when using our real-world dataset, indicating that the IPN Hand dataset can serve as a benchmark for the continuous hand gesture recognition task, and may aid in the advancement of this field. The dataset and pre-trained models utilized in the evaluation are publicly available at https://github.com/GibranBenitez/IPN-hand.",1
"In recent years, surveillance cameras are widely deployed in public places, and the general crime rate has been reduced significantly due to these ubiquitous devices. Usually, these cameras provide cues and evidence after crimes are conducted, while they are rarely used to prevent or stop criminal activities in time. It is both time and labor consuming to manually monitor a large amount of video data from surveillance cameras. Therefore, automatically recognizing violent behaviors from video signals becomes essential. This paper summarizes several existing video datasets for violence detection and proposes the RWF-2000 database with 2,000 videos captured by surveillance cameras in real-world scenes. Also, we present a new method that utilizes both the merits of 3D-CNNs and optical flow, namely Flow Gated Network. The proposed approach obtains an accuracy of 87.25% on the test set of our proposed database. The database and source codes are currently open to access.",0
"Surveillance cameras have become prevalent in public areas in recent years, leading to a significant reduction in overall crime rates. However, these cameras are typically used for providing post-crime evidence rather than actively preventing criminal activities. The manual monitoring of vast amounts of video data from these cameras is a time-consuming task. As a result, it is crucial to develop automated methods to recognize violent behavior from video signals. This article provides an overview of various video datasets for violence detection and introduces the RWF-2000 database, which comprises 2,000 videos captured by surveillance cameras in real-world scenarios. Furthermore, we present a novel technique called the Flow Gated Network, which combines the strengths of 3D-CNNs and optical flow. The proposed approach achieves an accuracy of 87.25% on the test set of our database. Our database and source codes are accessible to the public.",1
"Visual voice activity detection (V-VAD) uses visual features to predict whether a person is speaking or not. V-VAD is useful whenever audio VAD (A-VAD) is inefficient either because the acoustic signal is difficult to analyze or because it is simply missing. We propose two deep architectures for V-VAD, one based on facial landmarks and one based on optical flow. Moreover, available datasets, used for learning and for testing V-VAD, lack content variability. We introduce a novel methodology to automatically create and annotate very large datasets in-the-wild -- WildVVAD -- based on combining A-VAD with face detection and tracking. A thorough empirical evaluation shows the advantage of training the proposed deep V-VAD models with this dataset.",0
"The method of using visual features to determine if a person is speaking, known as visual voice activity detection (V-VAD), can be helpful when the traditional method of audio VAD (A-VAD) is not effective due to difficult acoustic signals or absence of audio. Two deep architectures for V-VAD, which rely on facial landmarks and optical flow, have been suggested. However, the available datasets used for training and testing V-VAD lack content variability. To address this issue, a novel method called WildVVAD has been introduced which automatically creates and annotates large datasets in-the-wild by combining A-VAD with face detection and tracking. Empirical evaluations demonstrate the benefits of training the proposed deep V-VAD models with the WildVVAD dataset.",1
"Nowadays 360 video analysis has become a significant research topic in the field since the appearance of high-quality and low-cost 360 wearable devices. In this paper, we propose a novel LiteFlowNet360 architecture for 360 videos optical flow estimation. We design LiteFlowNet360 as a domain adaptation framework from perspective video domain to 360 video domain. We adapt it from simple kernel transformation techniques inspired by Kernel Transformer Network (KTN) to cope with inherent distortion in 360 videos caused by the sphere-to-plane projection. First, we apply an incremental transformation of convolution layers in feature pyramid network and show that further transformation in inference and regularization layers are not important, hence reducing the network growth in terms of size and computation cost. Second, we refine the network by training with augmented data in a supervised manner. We perform data augmentation by projecting the images in a sphere and re-projecting to a plane. Third, we train LiteFlowNet360 in a self-supervised manner using target domain 360 videos. Experimental results show the promising results of 360 video optical flow estimation using the proposed novel architecture.",0
"The emergence of high-quality and affordable 360 wearable devices has made 360 video analysis a significant research topic. This paper presents the LiteFlowNet360 architecture, which is designed as a domain adaptation framework for optical flow estimation in 360 videos. To address the distortion caused by the sphere-to-plane projection, we incorporate simple kernel transformation techniques inspired by the Kernel Transformer Network (KTN). We demonstrate that incremental transformation of convolution layers in the feature pyramid network is sufficient, and further transformation in inference and regularization layers is unnecessary, thus reducing network size and computation cost. Additionally, we refine the network via supervised training using augmented data obtained by projecting images onto a sphere and re-projecting them onto a plane. Finally, we train LiteFlowNet360 in a self-supervised manner using target domain 360 videos, and our experimental results show promising outcomes for 360 video optical flow estimation using this novel architecture.",1
"Moving objects in scenes are still a severe challenge for the SLAM system. Many efforts have tried to remove the motion regions in the images by detecting moving objects. In this way, the keypoints belonging to motion regions will be ignored in the later calculations. In this paper, we proposed a novel motion removal method, leveraging semantic information and optical flow to extract motion regions. Different from previous works, we don't predict moving objects or motion regions directly from image sequences. We computed rigid optical flow, synthesized by the depth and pose, and compared it against the estimated optical flow to obtain initial motion regions. Then, we utilized K-means to finetune the motion region masks with instance segmentation masks. The ORB-SLAM2 integrated with the proposed motion removal method achieved the best performance in both indoor and outdoor dynamic environments.",0
"The SLAM system still faces a significant challenge when dealing with moving objects in scenes. To address this, previous efforts have attempted to detect moving objects in images and ignore their corresponding keypoints in subsequent calculations. Our paper proposes a new motion removal approach that utilizes semantic information and optical flow to isolate motion regions. Unlike previous methods, we do not directly predict moving objects or motion regions from image sequences. Instead, we use rigid optical flow synthesized by depth and pose and compare it against estimated optical flow to identify initial motion regions. We then refine these regions using K-means and instance segmentation masks. By integrating our method with ORB-SLAM2, we achieved the best performance in dynamic indoor and outdoor environments.",1
"Drowsiness driving is a major cause of traffic accidents and thus numerous previous researches have focused on driver drowsiness detection. Many drive relevant factors have been taken into consideration for fatigue detection and can lead to high precision, but there are still several serious constraints, such as most existing models are environmentally susceptible. In this paper, fatigue detection is considered as temporal action detection problem instead of image classification. The proposed detection system can be divided into four parts: (1) Localize the key patches of the detected driver picture which are critical for fatigue detection and calculate the corresponding optical flow. (2) Contrast Limited Adaptive Histogram Equalization (CLAHE) is used in our system to reduce the impact of different light conditions. (3) Three individual two-stream networks combined with attention mechanism are designed for each feature to extract temporal information. (4) The outputs of the three sub-networks will be concatenated and sent to the fully-connected network, which judges the status of the driver. The drowsiness detection system is trained and evaluated on the famous Nation Tsing Hua University Driver Drowsiness Detection (NTHU-DDD) dataset and we obtain an accuracy of 94.46%, which outperforms most existing fatigue detection models.",0
"Numerous studies have focused on detecting driver drowsiness, as it is a leading cause of traffic accidents. While many factors have been considered for fatigue detection, such as drive-related variables, current models are still limited by environmental factors. This paper proposes a new approach to fatigue detection, treating it as a temporal action detection problem instead of image classification. The system consists of four parts: identifying critical patches in the driver picture, applying Contrast Limited Adaptive Histogram Equalization to reduce lighting effects, using three two-stream networks with attention mechanisms to extract temporal information, and sending the output to a fully-connected network to determine the driver's status. The system is trained and evaluated on the NTHU-DDD dataset, achieving an accuracy of 94.46%, surpassing most existing fatigue detection models.",1
"Scene flow represents the 3D motion of every point in the dynamic environments. Like the optical flow that represents the motion of pixels in 2D images, 3D motion representation of scene flow benefits many applications, such as autonomous driving and service robot. This paper studies the problem of scene flow estimation from two consecutive 3D point clouds. In this paper, a novel hierarchical neural network with double attention is proposed for learning the correlation of point features in adjacent frames and refining scene flow from coarse to fine layer by layer. The proposed network has a new more-for-less hierarchical architecture. The more-for-less means that the number of input points is greater than the number of output points for scene flow estimation, which brings more input information and balances the precision and resource consumption. In this hierarchical architecture, scene flow of different levels is generated and supervised respectively. A novel attentive embedding module is introduced to aggregate the features of adjacent points using a double attention method in a patch-to-patch manner. The proper layers for flow embedding and flow supervision are carefully considered in our network designment. Experiments show that the proposed network outperforms the state-of-the-art performance of 3D scene flow estimation on the FlyingThings3D and KITTI Scene Flow 2015 datasets. We also apply the proposed network to realistic LiDAR odometry task, which is an key problem in autonomous driving. The experiment results demonstrate that our proposed network can outperform the ICP-based method and shows the good practical application ability.",0
"The motion of every point in dynamic environments can be represented using scene flow in 3D. This representation is useful for various applications, such as autonomous driving and service robots. This study focuses on estimating scene flow from two consecutive 3D point clouds, using a novel hierarchical neural network with double attention. The network has a more-for-less hierarchical architecture that allows for greater input information and balanced precision and resource consumption. Scene flow of different levels is generated and supervised using a novel attentive embedding module that aggregates features of adjacent points using a double attention method. The proposed network outperforms state-of-the-art methods for 3D scene flow estimation on FlyingThings3D and KITTI Scene Flow 2015 datasets. The network is also applied to realistic LiDAR odometry tasks, demonstrating superior performance compared to the ICP-based method and good practical application ability.",1
"Current state-of-the-art trackers often fail due to distractorsand large object appearance changes. In this work, we explore the use ofdense optical flow to improve tracking robustness. Our main insight is that, because flow estimation can also have errors, we need to incorporate an estimate of flow uncertainty for robust tracking. We present a novel tracking framework which combines appearance and flow uncertainty information to track objects in challenging scenarios. We experimentally verify that our framework improves tracking robustness, leading to new state-of-the-art results. Further, our experimental ablations shows the importance of flow uncertainty for robust tracking.",0
"The current advanced trackers tend to falter when faced with distractions and significant changes in the appearance of objects. This study delves into the prospect of enhancing tracking resilience through dense optical flow. The main revelation is that since flow estimation can also be erroneous, it is crucial to integrate an assessment of flow uncertainty to ensure robust tracking. The research introduces a new tracking framework that merges both appearance and flow uncertainty data to trace objects in difficult situations. Our experiments confirm that this framework enhances tracking resilience, yielding novel superior outcomes. Additionally, our experimental analyses underscore the significance of flow uncertainty for stable tracking.",1
"Person Re-Identification (ReID) is a challenging problem in many video analytics and surveillance applications, where a person's identity must be associated across a distributed non-overlapping network of cameras. Video-based person ReID has recently gained much interest because it allows capturing discriminant spatio-temporal information from video clips that is unavailable for image-based ReID. Despite recent advances, deep learning (DL) models for video ReID often fail to leverage this information to improve the robustness of feature representations. In this paper, the motion pattern of a person is explored as an additional cue for ReID. In particular, a flow-guided Mutual Attention network is proposed for fusion of image and optical flow sequences using any 2D-CNN backbone, allowing to encode temporal information along with spatial appearance information. Our Mutual Attention network relies on the joint spatial attention between image and optical flow features maps to activate a common set of salient features across them. In addition to flow-guided attention, we introduce a method to aggregate features from longer input streams for better video sequence-level representation. Our extensive experiments on three challenging video ReID datasets indicate that using the proposed Mutual Attention network allows to improve recognition accuracy considerably with respect to conventional gated-attention networks, and state-of-the-art methods for video-based person ReID.",0
"The identification of individuals across a network of cameras presents a complex challenge in various video analytics and surveillance scenarios. Recently, video-based person ReID has gained significant attention as it can capture spatial-temporal information that cannot be obtained through image-based ReID. However, despite recent advancements in deep learning models, they often fail to exploit this information to improve the representation of features. This study proposes a flow-guided Mutual Attention network that explores the motion pattern of individuals as an additional cue for ReID. The proposed network fuses image and optical flow sequences, allowing the encoding of temporal information with spatial appearance information. The Mutual Attention network uses joint spatial attention between image and optical flow feature maps to activate a common set of salient features. Additionally, the study introduces a method to aggregate features from longer input streams to improve video sequence-level representation. The experiments conducted on three challenging video ReID datasets demonstrate that using the proposed Mutual Attention network leads to a considerable improvement in recognition accuracy compared to conventional gated-attention networks and state-of-the-art methods for video-based person ReID.",1
"Drones shooting can be applied in dynamic traffic monitoring, object detecting and tracking, and other vision tasks. The variability of the shooting location adds some intractable challenges to these missions, such as varying scale, unstable exposure, and scene migration. In this paper, we strive to tackle the above challenges and automatically understand the crowd from the visual data collected from drones. First, to alleviate the background noise generated in cross-scene testing, a double-stream crowd counting model is proposed, which extracts optical flow and frame difference information as an additional branch. Besides, to improve the model's generalization ability at different scales and time, we randomly combine a variety of data transformation methods to simulate some unseen environments. To tackle the crowd density estimation problem under extreme dark environments, we introduce synthetic data generated by game Grand Theft Auto V(GTAV). Experiment results show the effectiveness of the virtual data. Our method wins the challenge with a mean absolute error (MAE) of 12.70. Moreover, a comprehensive ablation study is conducted to explore each component's contribution.",0
"The usage of drones for shooting can be employed for various vision tasks such as dynamic traffic monitoring, object detection, and tracking. However, the challenges posed by variable shooting locations, including varying scale, unstable exposure, and scene migration, make these missions difficult to accomplish. The purpose of this study is to overcome these challenges and automatically comprehend the crowd through the visual data obtained from drones. To achieve this, we propose a double-stream crowd counting model that extracts optical flow and frame difference information to alleviate background noise during cross-scene testing. Furthermore, to enhance the model's generalization ability for different scales and times, we randomly utilize a range of data transformation methods to simulate unseen environments. Additionally, we tackle the issue of crowd density estimation in extreme dark environments by introducing synthetic data generated through Grand Theft Auto V. Our approach proved successful, achieving a mean absolute error (MAE) of 12.70. We also conducted a comprehensive ablation study to determine each component's contribution.",1
"Optical flow, which expresses pixel displacement, is widely used in many computer vision tasks to provide pixel-level motion information. However, with the remarkable progress of the convolutional neural network, recent state-of-the-art approaches are proposed to solve problems directly on feature-level. Since the displacement of feature vector is not consistent to the pixel displacement, a common approach is to:forward optical flow to a neural network and fine-tune this network on the task dataset. With this method,they expect the fine-tuned network to produce tensors encoding feature-level motion information. In this paper, we rethink this de facto paradigm and analyze its drawbacks in the video object detection task. To mitigate these issues, we propose a novel network (IFF-Net) with an \textbf{I}n-network \textbf{F}eature \textbf{F}low estimation module (IFF module) for video object detection. Without resorting pre-training on any additional dataset, our IFF module is able to directly produce \textbf{feature flow} which indicates the feature displacement. Our IFF module consists of a shallow module, which shares the features with the detection branches. This compact design enables our IFF-Net to accurately detect objects, while maintaining a fast inference speed. Furthermore, we propose a transformation residual loss (TRL) based on \textit{self-supervision}, which further improves the performance of our IFF-Net. Our IFF-Net outperforms existing methods and sets a state-of-the-art performance on ImageNet VID.",0
"Pixel displacement is commonly used in computer vision tasks through optical flow to provide motion information. However, recent state-of-the-art approaches utilize convolutional neural networks to solve problems directly on feature-level. Due to feature vector displacement not being consistent with pixel displacement, a common approach is to fine-tune a neural network on the task dataset to produce feature-level motion information. However, this approach has drawbacks in video object detection. To address these issues, we propose a novel network (IFF-Net) with an In-network Feature Flow estimation module (IFF module) that directly produces feature flow to indicate feature displacement. Our IFF module is self-supervised, and our IFF-Net outperforms existing methods on ImageNet VID without the need for pre-training on additional datasets.",1
"Face reenactment aims to animate a source face image to a different pose and expression provided by a driving image. Existing approaches are either designed for a specific identity, or suffer from the identity preservation problem in the one-shot or few-shot scenarios. In this paper, we introduce a method for one-shot face reenactment, which uses the reconstructed 3D meshes (i.e., the source mesh and driving mesh) as guidance to learn the optical flow needed for the reenacted face synthesis. Technically, we explicitly exclude the driving face's identity information in the reconstructed driving mesh. In this way, our network can focus on the motion estimation for the source face without the interference of driving face shape. We propose a motion net to learn the face motion, which is an asymmetric autoencoder. The encoder is a graph convolutional network (GCN) that learns a latent motion vector from the meshes, and the decoder serves to produce an optical flow image from the latent vector with CNNs. Compared to previous methods using sparse keypoints to guide the optical flow learning, our motion net learns the optical flow directly from 3D dense meshes, which provide the detailed shape and pose information for the optical flow, so it can achieve more accurate expression and pose on the reenacted face. Extensive experiments show that our method can generate high-quality results and outperforms state-of-the-art methods in both qualitative and quantitative comparisons.",0
"The objective of face reenactment is to animate a face image to a different pose and expression using a different image as a reference. However, current methods either focus on a specific individual or face identity preservation issues in one-shot or few-shot scenarios. This study proposes a one-shot face reenactment method that employs reconstructed 3D meshes to guide the learning of optical flow for reenacted face synthesis. The driving face's identity information is excluded from the reconstructed driving mesh, allowing the network to concentrate on motion estimation for the source face. An asymmetric autoencoder called a motion net is proposed to learn face motion, with the encoder a graph convolutional network and the decoder a CNN that produces an optical flow image from a latent vector. Our motion net learns the optical flow directly from 3D dense meshes, leading to more accurate expression and pose on the reenacted face compared to previous methods that use sparse keypoints for optical flow learning. Extensive experiments demonstrate that our approach generates high-quality results and outperforms state-of-the-art methods in both qualitative and quantitative comparisons.",1
"Video object detection is a tough task due to the deteriorated quality of video sequences captured under complex environments. Currently, this area is dominated by a series of feature enhancement based methods, which distill beneficial semantic information from multiple frames and generate enhanced features through fusing the distilled information. However, the distillation and fusion operations are usually performed at either frame level or instance level with external guidance using additional information, such as optical flow and feature memory. In this work, we propose a dual semantic fusion network (abbreviated as DSFNet) to fully exploit both frame-level and instance-level semantics in a unified fusion framework without external guidance. Moreover, we introduce a geometric similarity measure into the fusion process to alleviate the influence of information distortion caused by noise. As a result, the proposed DSFNet can generate more robust features through the multi-granularity fusion and avoid being affected by the instability of external guidance. To evaluate the proposed DSFNet, we conduct extensive experiments on the ImageNet VID dataset. Notably, the proposed dual semantic fusion network achieves, to the best of our knowledge, the best performance of 84.1\% mAP among the current state-of-the-art video object detectors with ResNet-101 and 85.4\% mAP with ResNeXt-101 without using any post-processing steps.",0
"The difficulty in video object detection is attributed to the poor quality of video sequences captured in complex environments. Feature enhancement methods currently dominate this area by extracting useful semantic information from multiple frames to create enhanced features. However, these extraction and fusion operations are often carried out at the frame or instance level with additional guidance such as optical flow and feature memory. In this study, we propose a dual semantic fusion network (DSFNet) that combines both frame and instance-level semantics in a unified fusion framework without external guidance. We also introduce a geometric similarity measure to reduce the influence of information distortion caused by noise. As a result, DSFNet generates more robust features and is not affected by the instability of external guidance. We evaluate the performance of DSFNet on the ImageNet VID dataset, and it achieves the best performance of 84.1\% mAP among current state-of-the-art video object detectors with ResNet-101 and 85.4\% mAP with ResNeXt-101 without any post-processing steps.",1
"Deformable convolution, originally proposed for the adaptation to geometric variations of objects, has recently shown compelling performance in aligning multiple frames and is increasingly adopted for video super-resolution. Despite its remarkable performance, its underlying mechanism for alignment remains unclear. In this study, we carefully investigate the relation between deformable alignment and the classic flow-based alignment. We show that deformable convolution can be decomposed into a combination of spatial warping and convolution. This decomposition reveals the commonality of deformable alignment and flow-based alignment in formulation, but with a key difference in their offset diversity. We further demonstrate through experiments that the increased diversity in deformable alignment yields better-aligned features, and hence significantly improves the quality of video super-resolution output. Based on our observations, we propose an offset-fidelity loss that guides the offset learning with optical flow. Experiments show that our loss successfully avoids the overflow of offsets and alleviates the instability problem of deformable alignment. Aside from the contributions to deformable alignment, our formulation inspires a more flexible approach to introduce offset diversity to flow-based alignment, improving its performance.",0
"Deformable convolution is a technique that was initially developed to adapt to the geometric variations of objects. Recently, it has been found to be highly effective in aligning multiple frames and is increasingly being used in video super-resolution. However, its mechanism for alignment remains unclear. This study aims to investigate the relationship between deformable alignment and classic flow-based alignment. It reveals that deformable convolution can be broken down into a combination of spatial warping and convolution. While these two alignments share a similar formulation, they differ in their offset diversity. The study shows that increased diversity in deformable alignment leads to better-aligned features, which significantly improves the quality of video super-resolution output. The findings have led to the proposal of an offset-fidelity loss that guides the offset learning with optical flow, which successfully avoids the overflow of offsets and alleviates the instability problem of deformable alignment. Furthermore, the study inspires a more flexible approach to introduce offset diversity to flow-based alignment, thereby improving its performance.",1
"Optical flow estimation is an important computer vision task, which aims at estimating the dense correspondences between two frames. RAFT (Recurrent All Pairs Field Transforms) currently represents the state-of-the-art in optical flow estimation. It has excellent generalization ability and has obtained outstanding results across several benchmarks. To further improve the robustness and achieve accurate optical flow estimation, we present PRAFlow (Pyramid Recurrent All-Pairs Flow), which builds upon the pyramid network structure. Due to computational limitation, our proposed network structure only uses two pyramid layers. At each layer, the RAFT unit is used to estimate the optical flow at the current resolution. Our model was trained on several simulate and real-image datasets, submitted to multiple leaderboards using the same model and parameters, and won the 2nd place in the optical flow task of ECCV 2020 workshop: Robust Vision Challenge.",0
"The task of optical flow estimation is significant in computer vision. It aims to establish dense correspondences between two frames, with RAFT currently being the most advanced method for this purpose. RAFT is highly adaptable and has achieved exceptional results across various benchmarks. To enhance the precision and robustness of optical flow estimation, we have developed PRAFlow (Pyramid Recurrent All-Pairs Flow) that builds on the pyramid network structure. Our network structure only employs two pyramid layers due to computational constraints. At each layer, the RAFT unit is utilized to estimate optical flow for the current resolution. Our model has been trained on numerous simulated and real-image datasets, and it was submitted to multiple leaderboards using the same model and parameters. We secured the 2nd position in the optical flow task of ECCV 2020 workshop: Robust Vision Challenge.",1
"In the learning based video compression approaches, it is an essential issue to compress pixel-level optical flow maps by developing new motion vector (MV) encoders. In this work, we propose a new framework called Resolution-adaptive Flow Coding (RaFC) to effectively compress the flow maps globally and locally, in which we use multi-resolution representations instead of single-resolution representations for both the input flow maps and the output motion features of the MV encoder. To handle complex or simple motion patterns globally, our frame-level scheme RaFC-frame automatically decides the optimal flow map resolution for each video frame. To cope different types of motion patterns locally, our block-level scheme called RaFC-block can also select the optimal resolution for each local block of motion features. In addition, the rate-distortion criterion is applied to both RaFC-frame and RaFC-block and select the optimal motion coding mode for effective flow coding. Comprehensive experiments on four benchmark datasets HEVC, VTL, UVG and MCL-JCV clearly demonstrate the effectiveness of our overall RaFC framework after combing RaFC-frame and RaFC-block for video compression.",0
"In the realm of learning-based video compression, the compression of pixel-level optical flow maps is a crucial matter that requires the development of new motion vector encoders. This study introduces a fresh approach, named Resolution-adaptive Flow Coding (RaFC), that efficiently compresses flow maps globally and locally by using multi-resolution representations for the input flow maps and the output motion features of the motion vector encoder. To address complex or simple motion patterns globally, RaFC-frame, our frame-level scheme, decides the optimal flow map resolution for each video frame. Additionally, our block-level scheme, RaFC-block, selects the optimal resolution for each local block of motion features to handle different types of motion patterns locally. Furthermore, the rate-distortion criterion is employed to both RaFC-frame and RaFC-block to select the optimal motion coding mode for effective flow coding. Our comprehensive experimentation on four benchmark datasets HEVC, VTL, UVG, and MCL-JCV, clearly demonstrate the overall effectiveness of our RaFC framework after combining RaFC-frame and RaFC-block for video compression.",1
"We propose a lightweight real-time sign language detection model, as we identify the need for such a case in videoconferencing. We extract optical flow features based on human pose estimation and, using a linear classifier, show these features are meaningful with an accuracy of 80%, evaluated on the DGS Corpus. Using a recurrent model directly on the input, we see improvements of up to 91% accuracy, while still working under 4ms. We describe a demo application to sign language detection in the browser in order to demonstrate its usage possibility in videoconferencing applications.",0
"In response to the demand for sign language detection in videoconferencing, we propose a real-time model that is both efficient and accurate. Our approach involves extracting optical flow features through human pose estimation and utilizing a linear classifier, resulting in an 80% accuracy rate as evaluated on the DGS Corpus. By implementing a recurrent model directly on the input, we were able to achieve up to 91% accuracy while maintaining a processing time of under 4ms. To showcase the potential of our model in videoconferencing applications, we also present a demo application for sign language detection in web browsers.",1
"Matching and partitioning problems are fundamentals of computer vision applications with examples in multilabel segmentation, stereo estimation and optical-flow computation. These tasks can be posed as non-convex energy minimization problems and solved near-globally optimal by recent convex lifting approaches. Yet, applying these techniques comes with a significant computational effort, reducing their feasibility in practical applications. We discuss spatial discretization of continuous partitioning problems into a graph structure, generalizing discretization onto a Cartesian grid. This setup allows us to faithfully work on super-pixel graphs constructed by SLIC or Cut-Pursuit, massively decreasing the computational effort for lifted partitioning problems compared to a Cartesian grid, while optimal energy values remain similar: The global matching is still solved near-globally optimal. We discuss this methodology in detail and show examples in multi-label segmentation by minimal partitions and stereo estimation, where we demonstrate that the proposed graph discretization can reduce runtime as well as memory consumption of convex relaxations of matching problems by up to a factor of 10.",0
"Computer vision applications rely on fundamental tasks such as matching and partitioning, which are essential in multilabel segmentation, stereo estimation, and optical-flow computation. These tasks can be formulated as non-convex energy minimization problems and solved using convex lifting approaches, which provide near-globally optimal solutions. However, the computational effort required to apply these techniques is significant, making them impractical for many applications. To address this issue, we propose a spatial discretization method that converts continuous partitioning problems into a graph structure, allowing us to work on super-pixel graphs constructed by SLIC or Cut-Pursuit. This graph discretization significantly reduces the computational effort required for lifted partitioning problems compared to a Cartesian grid, while still providing near-globally optimal solutions. We provide detailed explanations of this methodology and present examples in multi-label segmentation and stereo estimation. Our results demonstrate that the proposed graph discretization can reduce the runtime and memory consumption of convex relaxations of matching problems by up to a factor of 10.",1
"Real-time tool segmentation is an essential component in computer-assisted surgical systems. We propose a novel real-time automatic method based on Fully Convolutional Networks (FCN) and optical flow tracking. Our method exploits the ability of deep neural networks to produce accurate segmentations of highly deformable parts along with the high speed of optical flow. Furthermore, the pre-trained FCN can be fine-tuned on a small amount of medical images without the need to hand-craft features. We validated our method using existing and new benchmark datasets, covering both ex vivo and in vivo real clinical cases where different surgical instruments are employed. Two versions of the method are presented, non-real-time and real-time. The former, using only deep learning, achieves a balanced accuracy of 89.6% on a real clinical dataset, outperforming the (non-real-time) state of the art by 3.8% points. The latter, a combination of deep learning with optical flow tracking, yields an average balanced accuracy of 78.2% across all the validated datasets.",0
"In computer-assisted surgical systems, real-time tool segmentation is crucial. Our proposed method uses Fully Convolutional Networks (FCN) and optical flow tracking to achieve accurate segmentations of deformable parts at high speed. The FCN can be fine-tuned with a small amount of medical images, eliminating the need for hand-crafted features. We tested our method on benchmark datasets covering ex vivo and in vivo clinical cases with various surgical instruments. We present two versions of the method: non-real-time and real-time. The former, using only deep learning, outperforms the state of the art by 3.8% points with a balanced accuracy of 89.6% on a clinical dataset. The latter, combining deep learning with optical flow tracking, achieves an average balanced accuracy of 78.2% across all validated datasets.",1
"Visual odometry (VO) is a prevalent way to deal with the relative localization problem, which is becoming increasingly mature and accurate, but it tends to be fragile under challenging environments. Comparing with classical geometry-based methods, deep learning-based methods can automatically learn effective and robust representations, such as depth, optical flow, feature, ego-motion, etc., from data without explicit computation. Nevertheless, there still lacks a thorough review of the recent advances of deep learning-based VO (Deep VO). Therefore, this paper aims to gain a deep insight on how deep learning can profit and optimize the VO systems. We first screen out a number of qualifications including accuracy, efficiency, scalability, dynamicity, practicability, and extensibility, and employ them as the criteria. Then, using the offered criteria as the uniform measurements, we detailedly evaluate and discuss how deep learning improves the performance of VO from the aspects of depth estimation, feature extraction and matching, pose estimation. We also summarize the complicated and emerging areas of Deep VO, such as mobile robots, medical robots, augmented reality and virtual reality, etc. Through the literature decomposition, analysis, and comparison, we finally put forward a number of open issues and raise some future research directions in this field.",0
"Although visual odometry (VO) is a widely used approach to solve the problem of relative localization, it is often unreliable in challenging environments. In contrast, deep learning-based methods can automatically learn robust representations, such as depth, optical flow, and feature, without explicit computation. However, there is still a lack of comprehensive reviews on recent advances in deep learning-based VO (Deep VO). Therefore, this paper aims to explore how deep learning can optimize VO systems by evaluating qualifications such as accuracy, efficiency, scalability, dynamicity, practicability, and extensibility. Using these criteria, we analyze how deep learning improves the performance of VO in terms of depth estimation, feature extraction and matching, and pose estimation. We also discuss emerging areas of Deep VO, including mobile robots, medical robots, augmented reality, and virtual reality. Finally, we highlight open issues and future research directions in this field.",1
"Objective Semi-supervised video object segmentation refers to segmenting the object in subsequent frames given the object label in the first frame. Existing algorithms are mostly based on the objectives of matching and propagation strategies, which often make use of the previous frame with masking or optical flow. This paper explores a new propagation method, uses short-term matching modules to extract the information of the previous frame and apply it in propagation, and proposes the network of Long-Short-Term similarity matching for video object segmentation (LSMOVS) Method: By conducting pixel-level matching and correlation between long-term matching module and short-term matching module with the first frame and previous frame, global similarity map and local similarity map are obtained, as well as feature pattern of current frame and masking of previous frame. After two refine networks, final results are obtained through segmentation network. Results: According to the experiments on the two data sets DAVIS 2016 and 2017, the method of this paper achieves favorable average of region similarity and contour accuracy without online fine tuning, which achieves 86.5% and 77.4% in terms of single target and multiple targets. Besides, the count of segmented frames per second reached 21. Conclusion: The short-term matching module proposed in this paper is more conducive to extracting the information of the previous frame than only the mask. By combining the long-term matching module with the short-term matching module, the whole network can achieve efficient video object segmentation without online fine tuning",0
"The goal of semi-supervised video object segmentation is to identify the object in subsequent frames based on its label in the first frame. Most current algorithms rely on matching and propagation strategies that utilize previous frames with masking or optical flow. This study introduces a new propagation method called the Long-Short-Term similarity matching for video object segmentation (LSMOVS) method, which uses short-term matching modules to extract information from the previous frame and apply it to propagation. By conducting pixel-level matching and correlation between long-term and short-term matching modules with the first frame and previous frame, global and local similarity maps are obtained, as well as the feature pattern of the current frame and masking of the previous frame. The final results are obtained through a segmentation network after two refine networks. Experimental results on two data sets show that the LSMOVS method achieves favorable region similarity and contour accuracy without online fine-tuning, reaching 86.5% and 77.4% in terms of single and multiple targets. Additionally, the method segments 21 frames per second. This study demonstrates that the short-term matching module is more effective in extracting information from previous frames than masking alone, and that combining long-term and short-term matching modules can result in efficient video object segmentation without online fine-tuning.",1
"In this paper, we propose a panorama stitching algorithm based on asymmetric bidirectional optical flow. This algorithm expects multiple photos captured by fisheye lens cameras as input, and then, through the proposed algorithm, these photos can be merged into a high-quality 360-degree spherical panoramic image. For photos taken from a distant perspective, the parallax among them is relatively small, and the obtained panoramic image can be nearly seamless and undistorted. For photos taken from a close perspective or with a relatively large parallax, a seamless though partially distorted panoramic image can also be obtained. Besides, with the help of Graphics Processing Unit (GPU), this algorithm can complete the whole stitching process at a very fast speed: typically, it only takes less than 30s to obtain a panoramic image of 9000-by-4000 pixels, which means our panorama stitching algorithm is of high value in many real-time applications. Our code is available at https://github.com/MungoMeng/Panorama-OpticalFlow.",0
"The proposed algorithm presented in this paper utilizes asymmetric bidirectional optical flow to stitch together multiple photos taken by fisheye lens cameras. This process results in a high-quality 360-degree spherical panoramic image. When photos are captured from a distant perspective, the parallax is minimal, leading to a nearly seamless and undistorted panoramic image. However, if photos are taken from a close perspective or have a large parallax, the resulting panoramic image may be partially distorted but still seamless. With the assistance of Graphics Processing Unit (GPU), this algorithm can complete the stitching process quickly, taking less than 30 seconds to obtain a panoramic image of 9000-by-4000 pixels. Therefore, this algorithm is highly beneficial for real-time applications. Interested parties can access our code at https://github.com/MungoMeng/Panorama-OpticalFlow.",1
"As a vital topic in media content interpretation, video anomaly detection (VAD) has made fruitful progress via deep neural network (DNN). However, existing methods usually follow a reconstruction or frame prediction routine. They suffer from two gaps: (1) They cannot localize video activities in a both precise and comprehensive manner. (2) They lack sufficient abilities to utilize high-level semantics and temporal context information. Inspired by frequently-used cloze test in language study, we propose a brand-new VAD solution named Video Event Completion (VEC) to bridge gaps above: First, we propose a novel pipeline to achieve both precise and comprehensive enclosure of video activities. Appearance and motion are exploited as mutually complimentary cues to localize regions of interest (RoIs). A normalized spatio-temporal cube (STC) is built from each RoI as a video event, which lays the foundation of VEC and serves as a basic processing unit. Second, we encourage DNN to capture high-level semantics by solving a visual cloze test. To build such a visual cloze test, a certain patch of STC is erased to yield an incomplete event (IE). The DNN learns to restore the original video event from the IE by inferring the missing patch. Third, to incorporate richer motion dynamics, another DNN is trained to infer erased patches' optical flow. Finally, two ensemble strategies using different types of IE and modalities are proposed to boost VAD performance, so as to fully exploit the temporal context and modality information for VAD. VEC can consistently outperform state-of-the-art methods by a notable margin (typically 1.5%-5% AUROC) on commonly-used VAD benchmarks. Our codes and results can be verified at github.com/yuguangnudt/VEC_VAD.",0
"Video anomaly detection (VAD) has made significant progress through the use of deep neural network (DNN) and is an essential topic in media content interpretation. However, current methods have limitations. They typically follow a reconstruction or frame prediction routine, which results in two issues: (1) they cannot accurately and comprehensively localize video activities; (2) they lack the ability to utilize high-level semantics and temporal context information effectively. To address these gaps, we propose a new VAD solution called Video Event Completion (VEC). To achieve precise and comprehensive enclosure of video activities, we propose a novel pipeline that exploits both appearance and motion as mutually complementary cues to localize regions of interest (RoIs). We build a normalized spatio-temporal cube (STC) from each RoI as a video event, which serves as a basic processing unit and the foundation of VEC. To capture high-level semantics, we propose a visual cloze test, where a certain patch of STC is erased to yield an incomplete event (IE), and the DNN learns to restore the original video event from the IE by inferring the missing patch. To incorporate richer motion dynamics, we train another DNN to infer erased patches' optical flow. We propose two ensemble strategies using different types of IE and modalities to boost VAD performance and fully exploit the temporal context and modality information for VAD. VEC outperforms state-of-the-art methods by a significant margin (typically 1.5%-5% AUROC) on commonly-used VAD benchmarks, and our codes and results can be verified at github.com/yuguangnudt/VEC_VAD.",1
"Our previous work classified a taxonomy of suturing gestures during a vesicourethral anastomosis of robotic radical prostatectomy in association with tissue tears and patient outcomes. Herein, we train deep-learning based computer vision (CV) to automate the identification and classification of suturing gestures for needle driving attempts. Using two independent raters, we manually annotated live suturing video clips to label timepoints and gestures. Identification (2395 videos) and classification (511 videos) datasets were compiled to train CV models to produce two- and five-class label predictions, respectively. Networks were trained on inputs of raw RGB pixels as well as optical flow for each frame. Each model was trained on 80/20 train/test splits. In this study, all models were able to reliably predict either the presence of a gesture (identification, AUC: 0.88) as well as the type of gesture (classification, AUC: 0.87) at significantly above chance levels. For both gesture identification and classification datasets, we observed no effect of recurrent classification model choice (LSTM vs. convLSTM) on performance. Our results demonstrate CV's ability to recognize features that not only can identify the action of suturing but also distinguish between different classifications of suturing gestures. This demonstrates the potential to utilize deep learning CV towards future automation of surgical skill assessment.",0
"In our previous work, we created a taxonomy of suturing movements during a specific surgical procedure and linked them to patient outcomes. In this study, we used deep-learning based computer vision to automate the identification and classification of these suturing movements. We manually annotated video clips to create datasets for training the computer vision models, which were able to predict the presence and type of suturing movements with high accuracy. We found that the choice of recurrent classification model did not affect performance. Our results suggest that deep learning computer vision has the potential to automate surgical skill assessment by recognizing and distinguishing between different suturing gestures.",1
"We introduce Recurrent All-Pairs Field Transforms (RAFT), a new deep network architecture for optical flow. RAFT extracts per-pixel features, builds multi-scale 4D correlation volumes for all pairs of pixels, and iteratively updates a flow field through a recurrent unit that performs lookups on the correlation volumes. RAFT achieves state-of-the-art performance. On KITTI, RAFT achieves an F1-all error of 5.10%, a 16% error reduction from the best published result (6.10%). On Sintel (final pass), RAFT obtains an end-point-error of 2.855 pixels, a 30% error reduction from the best published result (4.098 pixels). In addition, RAFT has strong cross-dataset generalization as well as high efficiency in inference time, training speed, and parameter count. Code is available at https://github.com/princeton-vl/RAFT.",0
"The introduction of Recurrent All-Pairs Field Transforms (RAFT) presents a novel deep network architecture for optical flow. RAFT extracts per-pixel features and constructs multi-scale 4D correlation volumes for every pair of pixels. It then updates a flow field through a recurrent unit that performs lookups on the correlation volumes. RAFT achieves outstanding performance, as evidenced by its F1-all error of 5.10% on KITTI, which is a 16% error reduction from the best published result (6.10%). Additionally, RAFT achieves an end-point-error of 2.855 pixels on Sintel (final pass), which is a 30% error reduction from the best published result (4.098 pixels). RAFT has strong cross-dataset generalization and is highly efficient in terms of inference time, training speed, and parameter count. The code is available at https://github.com/princeton-vl/RAFT.",1
"Event cameras are paradigm-shifting novel sensors that report asynchronous, per-pixel brightness changes called 'events' with unparalleled low latency. This makes them ideal for high speed, high dynamic range scenes where conventional cameras would fail. Recent work has demonstrated impressive results using Convolutional Neural Networks (CNNs) for video reconstruction and optic flow with events. We present strategies for improving training data for event based CNNs that result in 20-40% boost in performance of existing state-of-the-art (SOTA) video reconstruction networks retrained with our method, and up to 15% for optic flow networks. A challenge in evaluating event based video reconstruction is lack of quality ground truth images in existing datasets. To address this, we present a new High Quality Frames (HQF) dataset, containing events and ground truth frames from a DAVIS240C that are well-exposed and minimally motion-blurred. We evaluate our method on HQF + several existing major event camera datasets.",0
"Event cameras are innovative sensors that capture per-pixel brightness changes called 'events' with unparalleled low latency, making them ideal for high speed and high dynamic range scenes where traditional cameras are not effective. Recent studies have shown that Convolutional Neural Networks (CNNs) can improve video reconstruction and optic flow with events. We propose techniques to improve the training data for CNNs that result in a 20-40% performance increase for state-of-the-art video reconstruction networks and up to 15% for optic flow networks. However, evaluating event-based video reconstruction is challenging due to a lack of quality ground truth images in existing datasets. To overcome this, we introduce the High Quality Frames (HQF) dataset, which includes events and ground truth frames from a DAVIS240C that are well-exposed and minimally motion-blurred. Our approach is evaluated on HQF and several other major event camera datasets.",1
"Voice Activity Detection (VAD) refers to the task of identification of regions of human speech in digital signals such as audio and video. While VAD is a necessary first step in many speech processing systems, it poses challenges when there are high levels of ambient noise during the audio recording. To improve the performance of VAD in such conditions, several methods utilizing the visual information extracted from the region surrounding the mouth/lip region of the speakers' video recording have been proposed. Even though these provide advantages over audio-only methods, they depend on faithful extraction of lip/mouth regions. Motivated by these, a new paradigm for VAD based on the fact that respiration forms the primary source of energy for speech production is proposed. Specifically, an audio-independent VAD technique using the respiration pattern extracted from the speakers' video is developed. The Respiration Pattern is first extracted from the video focusing on the abdominal-thoracic region of a speaker using an optical flow based method. Subsequently, voice activity is detected from the respiration pattern signal using neural sequence-to-sequence prediction models. The efficacy of the proposed method is demonstrated through experiments on a challenging dataset recorded in real acoustic environments and compared with four previous methods based on audio and visual cues.",0
"Voice Activity Detection (VAD) is the process of identifying human speech regions in digital signals, including audio and video. When there is a high level of ambient noise during audio recording, VAD poses challenges. To improve VAD performance in such conditions, various methods have been proposed that use visual information extracted from the area surrounding the speaker's mouth/lip region. However, these methods rely heavily on accurate extraction of lip/mouth regions. Therefore, a new VAD paradigm based on the fact that respiration is the primary energy source for speech production is suggested. An audio-independent VAD technique is developed that uses respiration patterns extracted from the speaker's video. The optical flow-based method focuses on the abdominal-thoracic region of the speaker to extract the Respiration Pattern. Neural sequence-to-sequence prediction models are then used to detect voice activity from the respiration pattern signal. The proposed method's efficacy is demonstrated through experiments on a challenging dataset recorded in real acoustic environments and compared with four previous methods based on audio and visual cues.",1
"In this paper, we introduce a novel suspect-and-investigate framework, which can be easily embedded in a drone for automated parking violation detection (PVD). Our proposed framework consists of: 1) SwiftFlow, an efficient and accurate convolutional neural network (CNN) for unsupervised optical flow estimation; 2) Flow-RCNN, a flow-guided CNN for car detection and classification; and 3) an illegally parked car (IPC) candidate investigation module developed based on visual SLAM. The proposed framework was successfully embedded in a drone from ATG Robotics. The experimental results demonstrate that, firstly, our proposed SwiftFlow outperforms all other state-of-the-art unsupervised optical flow estimation approaches in terms of both speed and accuracy; secondly, IPC candidates can be effectively and efficiently detected by our proposed Flow-RCNN, with a better performance than our baseline network, Faster-RCNN; finally, the actual IPCs can be successfully verified by our investigation module after drone re-localization.",0
"This paper presents a new framework called suspect-and-investigate, designed for automated parking violation detection (PVD) using a drone. Our framework includes three components: SwiftFlow, a precise and effective convolutional neural network (CNN) for unsupervised optical flow estimation; Flow-RCNN, a CNN guided by flow for car detection and classification; and a module for investigating illegally parked car (IPC) candidates that is developed based on visual SLAM. We integrated this framework into a drone from ATG Robotics and conducted experiments to evaluate its performance. Our results show that SwiftFlow outperforms all other unsupervised optical flow estimation methods in terms of speed and accuracy. Additionally, our proposed Flow-RCNN effectively and efficiently detects IPC candidates, outperforming the Faster-RCNN baseline network. Finally, our investigation module successfully verifies actual IPCs after drone re-localization.",1
"In autonomous driving, monocular sequences contain lots of information. Monocular depth estimation, camera ego-motion estimation and optical flow estimation in consecutive frames are high-profile concerns recently. By analyzing tasks above, pixels in the middle frame are modeled into three parts: the rigid region, the non-rigid region, and the occluded region. In joint unsupervised training of depth and pose, we can segment the occluded region explicitly. The occlusion information is used in unsupervised learning of depth, pose and optical flow, as the image reconstructed by depth-pose and optical flow will be invalid in occluded regions. A less-than-mean mask is designed to further exclude the mismatched pixels interfered with by motion or illumination change in the training of depth and pose networks. This method is also used to exclude some trivial mismatched pixels in the training of the optical flow network. Maximum normalization is proposed for depth smoothness term to restrain depth degradation in textureless regions. In the occluded region, as depth and camera motion can provide more reliable motion estimation, they can be used to instruct unsupervised learning of optical flow. Our experiments in KITTI dataset demonstrate that the model based on three regions, full and explicit segmentation of the occlusion region, the rigid region, and the non-rigid region with corresponding unsupervised losses can improve performance on three tasks significantly. The source code is available at: https://github.com/guangmingw/DOPlearning.",0
"The use of monocular sequences in autonomous driving provides a wealth of information, with recent attention focused on monocular depth estimation, camera ego-motion estimation, and optical flow estimation in consecutive frames. To better understand these tasks, the pixels in the middle frame can be divided into three distinct regions: the rigid, non-rigid, and occluded regions. By conducting joint unsupervised training of depth and pose, the occluded region can be explicitly segmented, allowing for improved understanding of the scene. Additionally, a less-than-mean mask is utilized to exclude mismatched pixels caused by motion or illumination changes during the training of the depth and pose networks. This method is also used to exclude trivial mismatched pixels when training the optical flow network. To improve depth smoothness in textureless regions, maximum normalization is suggested for the depth smoothness term. In the occluded region, depth and camera motion can provide more reliable motion estimation, which can then be used to inform unsupervised learning of optical flow. Experiments conducted on the KITTI dataset demonstrate that this approach, which segments the occlusion region and includes corresponding unsupervised losses, can significantly improve performance across all three tasks. The source code for this approach is available at https://github.com/guangmingw/DOPlearning.",1
"We propose a simply method to generate high quality synthetic dataset based on open-source game Minecraft includes rendered image, Depth map, surface normal map, and 6-dof camera trajectory. This dataset has a perfect ground-truth generated by plug-in program, and thanks for the large game's community, there is an extremely large number of 3D open-world environment, users can find suitable scenes for shooting and build data sets through it and they can also build scenes in-game. as such, We don't need to worry about manual over fitting caused by too small datasets. what's more, there is also a shader community which We can use to minimize data bias between rendered images and real-images as little as possible. Last but not least, we now provide three tools to generate the data for depth prediction ,surface normal prediction and visual odometry, user can also develop the plug-in module for other vision task like segmentation or optical flow prediction.",0
"Our proposal offers a straightforward approach to creating a top-notch artificial dataset using the open-source game Minecraft. This dataset comprises a rendered image, depth map, surface normal map, and a 6-dof camera trajectory, all with accurate ground-truth generated by a plug-in program. Thanks to the vast community of this game, there is an abundant supply of 3D open-world environments, which users can utilize to shoot suitable scenes and create datasets. Additionally, users can build their scenes within the game, eliminating any concerns of manual overfitting due to small datasets. Furthermore, we can leverage the shader community to minimize data bias between rendered and real images. Lastly, we offer three tools for generating data for depth prediction, surface normal prediction, and visual odometry, while users can develop plug-in modules for other vision tasks like segmentation or optical flow prediction.",1
"Activity detection from first-person videos (FPV) captured using a wearable camera is an active research field with potential applications in many sectors, including healthcare, law enforcement, and rehabilitation. State-of-the-art methods use optical flow-based hybrid techniques that rely on features derived from the motion of objects from consecutive frames. In this work, we developed a two-stream network, the \emph{SegCodeNet}, that uses a network branch containing video-streams with color-coded semantic segmentation masks of relevant objects in addition to the original RGB video-stream. We also include a stream-wise attention gating that prioritizes between the two streams and a frame-wise attention module that prioritizes the video frames that contain relevant features. Experiments are conducted on an FPV dataset containing $18$ activity classes in office environments. In comparison to a single-stream network, the proposed two-stream method achieves an absolute improvement of $14.366\%$ and $10.324\%$ for averaged F1 score and accuracy, respectively, when average results are compared for three different frame sizes $224\times224$, $112\times112$, and $64\times64$. The proposed method provides significant performance gains for lower-resolution images with absolute improvements of $17\%$ and $26\%$ in F1 score for input dimensions of $112\times112$ and $64\times64$, respectively. The best performance is achieved for a frame size of $224\times224$ yielding an F1 score and accuracy of $90.176\%$ and $90.799\%$ which outperforms the state-of-the-art Inflated 3D ConvNet (I3D) \cite{carreira2017quo} method by an absolute margin of $4.529\%$ and $2.419\%$, respectively.",0
"The field of activity detection from first-person videos (FPV) captured by wearable cameras has promising applications in various sectors, such as healthcare, law enforcement, and rehabilitation. Current methods rely on optical flow-based hybrid techniques that use features derived from object motion in consecutive frames. In this study, we propose a two-stream network called the \emph{SegCodeNet}, which includes a network branch containing video-streams with color-coded semantic segmentation masks of relevant objects and an original RGB video-stream. The model also comprises a stream-wise attention gating that prioritizes between the two streams and a frame-wise attention module that prioritizes frames with relevant features. We evaluate the proposed method on an FPV dataset with 18 activity classes in office environments. Compared to a single-stream network, the two-stream method shows significant improvement, with an absolute improvement of 14.366% and 10.324% for averaged F1 score and accuracy, respectively. Particularly, the proposed method provides substantial performance gains for lower-resolution images. The best performance was achieved for a frame size of 224x224, with an F1 score and accuracy of 90.176% and 90.799%, respectively, outperforming the state-of-the-art Inflated 3D ConvNet (I3D) method by 4.529% and 2.419%, respectively.",1
"Crowd flow describes the elementary group behavior of crowds. Understanding the dynamics behind these movements can help to identify various abnormalities in crowds. However, developing a crowd model describing these flows is a challenging task. In this paper, a physics-based model is proposed to describe the movements in dense crowds. The crowd model is based on active Langevin equation where the motion points are assumed to be similar to active colloidal particles in fluids. The model is further augmented with computer-vision techniques to segment both linear and non-linear motion flows in a dense crowd. The evaluation of the active Langevin equation-based crowd segmentation has been done on publicly available crowd videos and on our own videos. The proposed method is able to segment the flow with lesser optical flow error and better accuracy in comparison to existing state-of-the-art methods.",0
"The behavior of crowds is referred to as crowd flow and understanding the dynamics of these movements can help detect abnormalities. Creating a model to describe these flows is a challenging task, but this paper proposes a physics-based model that utilizes an active Langevin equation. The motion points in this model are assumed to be similar to active colloidal particles in fluids, and computer-vision techniques are used to segment both linear and non-linear motion flows in dense crowds. The evaluation of this model has been conducted on publicly available crowd videos and the results show that it is able to segment the flow with greater accuracy and lesser optical flow error than existing state-of-the-art methods.",1
"Personal robots and driverless cars need to be able to operate in novel environments and thus quickly and efficiently learn to recognise new object classes. We address this problem by considering the task of video object segmentation. Previous accurate methods for this task finetune a model using the first annotated frame, and/or use additional inputs such as optical flow and complex post-processing. In contrast, we develop a fast, causal algorithm that requires no finetuning, auxiliary inputs or post-processing, and segments a variable number of objects in a single forward-pass. We represent an object with clusters, or ""visual words"", in the embedding space, which correspond to object parts in the image space. This allows us to robustly match to the reference objects throughout the video, because although the global appearance of an object changes as it undergoes occlusions and deformations, the appearance of more local parts may stay consistent. We learn these visual words in an unsupervised manner, using meta-learning to ensure that our training objective matches our inference procedure. We achieve comparable accuracy to finetuning based methods (whilst being 1 to 2 orders of magnitude faster), and state-of-the-art in terms of speed/accuracy trade-offs on four video segmentation datasets. Code is available at https://github.com/harkiratbehl/MetaVOS.",0
"The fast learning of new object classes is crucial for personal robots and driverless cars to operate effectively in novel environments. To address this challenge, we focus on video object segmentation and aim to develop an algorithm that is quick and efficient. Existing methods for this task involve finetuning a model using the first annotated frame and/or utilizing additional inputs such as optical flow and complex post-processing. However, we propose a novel approach that requires no finetuning, auxiliary inputs, or post-processing, and can segment multiple objects in a single forward-pass. We represent an object with clusters or ""visual words"" in the embedding space, which correspond to object parts in the image space. This enables us to match reference objects consistently throughout the video, even as they undergo changes in appearance due to occlusions and deformations. We use unsupervised learning to acquire these visual words, ensuring that our training objective aligns with our inference procedure. Our algorithm achieves accuracy comparable to finetuning-based methods but is significantly faster, and we demonstrate state-of-the-art speed/accuracy trade-offs on four video segmentation datasets. Our code is available at https://github.com/harkiratbehl/MetaVOS.",1
"Nowadays, digital facial content manipulation has become ubiquitous and realistic with the success of generative adversarial networks (GANs), making face recognition (FR) systems suffer from unprecedented security concerns. In this paper, we investigate and introduce a new type of adversarial attack to evade FR systems by manipulating facial content, called \textbf{\underline{a}dversarial \underline{mor}phing \underline{a}ttack} (a.k.a. Amora). In contrast to adversarial noise attack that perturbs pixel intensity values by adding human-imperceptible noise, our proposed adversarial morphing attack works at the semantic level that perturbs pixels spatially in a coherent manner. To tackle the black-box attack problem, we devise a simple yet effective joint dictionary learning pipeline to obtain a proprietary optical flow field for each attack. Our extensive evaluation on two popular FR systems demonstrates the effectiveness of our adversarial morphing attack at various levels of morphing intensity with smiling facial expression manipulations. Both open-set and closed-set experimental results indicate that a novel black-box adversarial attack based on local deformation is possible, and is vastly different from additive noise attacks. The findings of this work potentially pave a new research direction towards a more thorough understanding and investigation of image-based adversarial attacks and defenses.",0
"Digital manipulation of facial content has become highly advanced and widespread due to the success of generative adversarial networks (GANs), which has led to significant security concerns for face recognition (FR) systems. This study presents a new kind of adversarial attack named Adversarial Morphing Attack (Amora) that manipulates facial content to evade FR systems. Unlike adversarial noise attacks that distort pixel intensity values by adding imperceptible noise, our proposed attack works at the semantic level and perturbs pixels spatially in a coherent manner. To address the black-box attack problem, we introduce a simple yet effective joint dictionary learning pipeline to obtain an exclusive optical flow field for each attack. Our extensive evaluation on two popular FR systems shows that our adversarial morphing attack is effective at different levels of morphing intensity, particularly with smiling facial expression manipulations. Our results suggest that a novel black-box adversarial attack based on local deformation is possible, and it differs significantly from additive noise attacks. Overall, our findings open up new avenues for research into image-based adversarial attacks and defenses.",1
"We propose a light-weight variational framework for online tracking of object segmentations in videos based on optical flow and image boundaries. While high-end computer vision methods on this task rely on sequence specific training of dedicated CNN architectures, we show the potential of a variational model, based on generic video information from motion and color. Such cues are usually required for tasks such as robot navigation or grasp estimation. We leverage them directly for video object segmentation and thus provide accurate segmentations at potentially very low extra cost. Our simple method can provide competitive results compared to the costly CNN-based methods with parameter tuning. Furthermore, we show that our approach can be combined with state-of-the-art CNN-based segmentations in order to improve over their respective results. We evaluate our method on the datasets DAVIS 16,17 and SegTrack v2.",0
"Our proposal introduces a simple variational framework for tracking object segmentations in videos using optical flow and image boundaries. Unlike advanced computer vision techniques that require sequence-specific training of dedicated CNN architectures, we rely on generic video information from motion and color, which is commonly used for tasks like robot navigation or grasp estimation. This approach enables us to achieve accurate segmentations at a lower cost. Our method can also produce competitive results compared to expensive CNN-based methods with parameter tuning. Additionally, we demonstrate that our approach can be combined with state-of-the-art CNN-based segmentations to improve results. We evaluate our method on DAVIS 16,17 and SegTrack v2 datasets.",1
"We systematically compare and analyze a set of key components in unsupervised optical flow to identify which photometric loss, occlusion handling, and smoothness regularization is most effective. Alongside this investigation we construct a number of novel improvements to unsupervised flow models, such as cost volume normalization, stopping the gradient at the occlusion mask, encouraging smoothness before upsampling the flow field, and continual self-supervision with image resizing. By combining the results of our investigation with our improved model components, we are able to present a new unsupervised flow technique that significantly outperforms the previous unsupervised state-of-the-art and performs on par with supervised FlowNet2 on the KITTI 2015 dataset, while also being significantly simpler than related approaches.",0
"We conduct a thorough comparison and analysis of key components in unsupervised optical flow in order to determine the most effective photometric loss, occlusion handling, and smoothness regularization methods. In addition, we propose several innovative enhancements to existing unsupervised flow models, including cost volume normalization, stopping gradient at the occlusion mask, promoting smoothness prior to upsampling the flow field, and continual self-supervision through image resizing. Our new unsupervised flow technique, which combines the results of our investigation with our improved model components, surpasses the previous unsupervised state-of-the-art and performs similarly to supervised FlowNet2 on the KITTI 2015 dataset, while also being simpler than related approaches.",1
"This paper presents a novel end-to-end dynamic time-lapse video generation framework, named DTVNet, to generate diversified time-lapse videos from a single landscape image, which are conditioned on normalized motion vectors. The proposed DTVNet consists of two submodules: \emph{Optical Flow Encoder} (OFE) and \emph{Dynamic Video Generator} (DVG). The OFE maps a sequence of optical flow maps to a \emph{normalized motion vector} that encodes the motion information inside the generated video. The DVG contains motion and content streams that learn from the motion vector and the single image respectively, as well as an encoder and a decoder to learn shared content features and construct video frames with corresponding motion respectively. Specifically, the \emph{motion stream} introduces multiple \emph{adaptive instance normalization} (AdaIN) layers to integrate multi-level motion information that are processed by linear layers. In the testing stage, videos with the same content but various motion information can be generated by different \emph{normalized motion vectors} based on only one input image. We further conduct experiments on Sky Time-lapse dataset, and the results demonstrate the superiority of our approach over the state-of-the-art methods for generating high-quality and dynamic videos, as well as the variety for generating videos with various motion information.",0
"The aim of this paper is to introduce a new method for generating time-lapse videos from a single landscape image. The proposed method, called DTVNet, uses normalized motion vectors to produce diverse videos. DTVNet consists of two parts: the Optical Flow Encoder (OFE) and the Dynamic Video Generator (DVG). The OFE converts a sequence of optical flow maps into a normalized motion vector that encodes motion information. The DVG has two streams, one for motion and one for content, which learn from the motion vector and the image respectively. The encoder and decoder work together to construct video frames with corresponding motion. The motion stream employs adaptive instance normalization (AdaIN) layers to integrate multi-level motion information processed by linear layers. By using different normalized motion vectors, videos with the same content but varying motion can be generated from a single image. The proposed method outperforms existing approaches for generating high-quality and dynamic videos with varied motion information, as demonstrated through experiments on the Sky Time-lapse dataset.",1
"Depth map estimation is a crucial task in computer vision, and new approaches have recently emerged taking advantage of light fields, as this new imaging modality captures much more information about the angular direction of light rays compared to common approaches based on stereoscopic images or multi-view. In this paper, we propose a novel depth estimation method from light fields based on existing optical flow estimation methods. The optical flow estimator is applied on a sequence of images taken along an angular dimension of the light field, which produces several disparity map estimates. Considering both accuracy and efficiency, we choose the feature flow method as our optical flow estimator. Thanks to its spatio-temporal edge-aware filtering properties, the different disparity map estimates that we obtain are very consistent, which allows a fast and simple aggregation step to create a single disparity map, which can then converted into a depth map. Since the disparity map estimates are consistent, we can also create a depth map from each disparity estimate, and then aggregate the different depth maps in the 3D space to create a single dense depth map.",0
"Recently, new approaches have emerged in computer vision for estimating depth maps using light fields. Unlike stereoscopic images or multi-view techniques, light fields capture a greater amount of information about the direction of light rays. In this study, we propose a unique method for estimating depth from light fields by utilizing existing optical flow estimation methods. Specifically, we apply an optical flow estimator on a sequence of images taken along the angular dimension of the light field to produce multiple disparity map estimates. To ensure both accuracy and efficiency, we use the feature flow method as our optical flow estimator, which includes spatio-temporal edge-aware filtering properties. The resulting disparity map estimates are highly consistent, allowing for a fast and straightforward aggregation process to create a single disparity map that can be converted into a depth map. Additionally, we can create a depth map from each disparity estimate and aggregate them in 3D space to produce a dense depth map.",1
"Our objective is to transform a video into a set of discrete audio-visual objects using self-supervised learning. To this end, we introduce a model that uses attention to localize and group sound sources, and optical flow to aggregate information over time. We demonstrate the effectiveness of the audio-visual object embeddings that our model learns by using them for four downstream speech-oriented tasks: (a) multi-speaker sound source separation, (b) localizing and tracking speakers, (c) correcting misaligned audio-visual data, and (d) active speaker detection. Using our representation, these tasks can be solved entirely by training on unlabeled video, without the aid of object detectors. We also demonstrate the generality of our method by applying it to non-human speakers, including cartoons and puppets.Our model significantly outperforms other self-supervised approaches, and obtains performance competitive with methods that use supervised face detection.",0
"Our aim is to utilize self-supervised learning to convert a video into a collection of distinct audio-visual objects. To achieve this, we have developed a model that employs attention to locate and group sound sources, as well as optical flow to consolidate information across time. By utilizing the audio-visual object embeddings that our model learns, we have successfully accomplished four speech-oriented tasks: (a) separating multiple sound sources, (b) tracking and localizing speakers, (c) rectifying misaligned audio-visual data, and (d) detecting active speakers. Our approach allows for these tasks to be accomplished solely by training on unlabeled video, without the assistance of object detectors. Moreover, we have demonstrated that our method can be applied to non-human speakers, including cartoons and puppets. Our model surpasses other self-supervised techniques and achieves performance that is comparable to methods that utilize supervised face detection.",1
"To address the problem of training on small datasets for action recognition tasks, most prior works are either based on a large number of training samples or require pre-trained models transferred from other large datasets to tackle overfitting problems. However, it limits the research within organizations that have strong computational abilities. In this work, we try to propose a data-efficient framework that can train the model from scratch on small datasets while achieving promising results. Specifically, by introducing a 3D central difference convolution operation, we proposed a novel C3D neural network-based two-stream (Rank Pooling RGB and Optical Flow) framework for the task. The method is validated on the action recognition track of the ECCV 2020 VIPriors challenges and got the 2nd place (88.31%). It is proved that our method can achieve a promising result even without a pre-trained model on large scale datasets. The code will be released soon.",0
"Most previous works dealing with the problem of training on small datasets for action recognition tasks either rely on a large number of training samples or require pre-trained models from other large datasets to avoid overfitting. However, this approach is limited to organizations with strong computational capabilities. In this study, we aim to propose a data-efficient framework that can train the model from scratch on small datasets while achieving promising results. Our approach involves introducing a 3D central difference convolution operation and creating a novel C3D neural network-based two-stream framework that utilizes Rank Pooling RGB and Optical Flow. We validated our method on the action recognition track of the ECCV 2020 VIPriors challenges and achieved the 2nd place with 88.31% accuracy. Our findings demonstrate that our method can yield promising results without a pre-trained model on large scale datasets. We plan to release our code soon.",1
"Currently, the safety of people has become a very important problem in different places including subway station, universities, colleges, airport, shopping mall and square, city squares. Therefore, considering intelligence event detection systems is more and urgently required. The event detection method is developed to identify abnormal behavior intelligently, so public can take action as soon as possible to prevent unwanted activities. The problem is very challenging due to high crowd density in different areas. One of these issues is occlusion due to which individual tracking and analysis becomes impossible as shown in Fig. 1. Secondly, more challenging is the proper representation of individual behavior in the crowd. We consider a novel method to deal with these challenges. Considering the challenge of tracking, we partition complete frame into smaller patches, and extract motion pattern to demonstrate the motion in each individual patch. For this purpose, our work takes into account KLT corners as consolidated features to describe moving regions and track these features by considering optical flow method. To embed motion patterns, we develop and consider the distribution of all motion information in a patch as Gaussian distribution, and formulate parameters of Gaussian model as our motion pattern descriptor.",0
"The issue of people's safety has become a pressing concern in various public areas such as subway stations, airports, universities, colleges, shopping malls, and city squares. Therefore, it is essential to implement intelligent event detection systems to detect abnormal behavior and take immediate action to prevent unwanted activities. However, this task is challenging due to the high density of crowds in different areas. Two significant difficulties are occlusion, which makes it impossible to track individuals, and representing individual behavior in the crowd. To address these challenges, we propose a novel approach. We partition the complete frame into smaller patches and extract motion patterns in each patch using KLT corners as consolidated features and the optical flow method to track them. We then use the distribution of all motion information in a patch to formulate parameters of a Gaussian model as our motion pattern descriptor.",1
"Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the brightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of microseconds), very high dynamic range (140 dB vs. 60 dB), low power consumption, and high pixel bandwidth (on the order of kHz) resulting in reduced motion blur. Hence, event cameras have a large potential for robotics and computer vision in challenging scenarios for traditional cameras, such as low-latency, high speed, and high dynamic range. However, novel methods are required to process the unconventional output of these sensors in order to unlock their potential. This paper provides a comprehensive overview of the emerging field of event-based vision, with a focus on the applications and the algorithms developed to unlock the outstanding properties of event cameras. We present event cameras from their working principle, the actual sensors that are available and the tasks that they have been used for, from low-level vision (feature detection and tracking, optic flow, etc.) to high-level vision (reconstruction, segmentation, recognition). We also discuss the techniques developed to process events, including learning-based techniques, as well as specialized processors for these novel sensors, such as spiking neural networks. Additionally, we highlight the challenges that remain to be tackled and the opportunities that lie ahead in the search for a more efficient, bio-inspired way for machines to perceive and interact with the world.",0
"Event cameras are sensors that are inspired by biology and are different from traditional frame cameras. They do not capture images at a fixed rate but rather measure changes in brightness asynchronously per pixel. This results in a stream of events that encode information about the time, location, and sign of the brightness changes. Event cameras have several advantages over traditional cameras, including high temporal resolution, very high dynamic range, low power consumption, and reduced motion blur. They are ideal for challenging scenarios such as low-latency, high speed, and high dynamic range, which traditional cameras struggle with. However, processing the output of event cameras requires novel methods to unlock their potential. This paper provides an overview of event-based vision, including the applications and algorithms used to unlock the properties of event cameras. It covers the working principle of event cameras, the available sensors, and the tasks they have been used for. The paper also discusses the techniques developed to process events, including learning-based techniques and specialized processors such as spiking neural networks. Finally, the paper highlights the challenges that remain and the opportunities that lie ahead in the search for a more efficient, bio-inspired way for machines to perceive and interact with the world.",1
"Efficiently modeling dynamic motion information in videos is crucial for action recognition task. Most state-of-the-art methods heavily rely on dense optical flow as motion representation. Although combining optical flow with RGB frames as input can achieve excellent recognition performance, the optical flow extraction is very time-consuming. This undoubtably will count against real-time action recognition. In this paper, we shed light on fast action recognition by lifting the reliance on optical flow. Our motivation lies in the observation that small displacements of motion boundaries are the most critical ingredients for distinguishing actions, so we design a novel motion cue called Persistence of Appearance (PA). In contrast to optical flow, our PA focuses more on distilling the motion information at boundaries. Also, it is more efficient by only accumulating pixel-wise differences in feature space, instead of using exhaustive patch-wise search of all the possible motion vectors. Our PA is over 1000x faster (8196fps vs. 8fps) than conventional optical flow in terms of motion modeling speed. To further aggregate the short-term dynamics in PA to long-term dynamics, we also devise a global temporal fusion strategy called Various-timescale Aggregation Pooling (VAP) that can adaptively model long-range temporal relationships across various timescales. We finally incorporate the proposed PA and VAP to form a unified framework called Persistent Appearance Network (PAN) with strong temporal modeling ability. Extensive experiments on six challenging action recognition benchmarks verify that our PAN outperforms recent state-of-the-art methods at low FLOPs. Codes and models are available at: https://github.com/zhang-can/PAN-PyTorch.",0
"It is crucial to efficiently model dynamic motion information in videos for action recognition. However, most current methods heavily rely on dense optical flow, which can be time-consuming. While combining optical flow with RGB frames can achieve excellent recognition performance, this approach is not suitable for real-time action recognition. In this study, we propose a novel motion cue called Persistence of Appearance (PA), which focuses on distilling motion information at boundaries and is more efficient than optical flow. Our PA is over 1000x faster than conventional optical flow in terms of motion modeling speed. To aggregate short-term dynamics in PA to long-term dynamics, we devise a global temporal fusion strategy called Various-timescale Aggregation Pooling (VAP), which can adaptively model long-range temporal relationships across various timescales. We incorporate PA and VAP into a unified framework called Persistent Appearance Network (PAN) with strong temporal modeling ability. Experiments on six challenging action recognition benchmarks demonstrate that our PAN outperforms recent state-of-the-art methods at low FLOPs. Codes and models are available at: https://github.com/zhang-can/PAN-PyTorch.",1
"Situational awareness and Indoor location tracking for firefighters is one of the tasks with paramount importance in search and rescue operations. For Indoor Positioning systems (IPS), GPS is not the best possible solution. There are few other techniques like dead reckoning, Wifi and bluetooth based triangulation, Structure from Motion (SFM) based scene reconstruction for Indoor positioning system. However due to high temperatures, the rapidly changing environment of fires, and low parallax in the thermal images, these techniques are not suitable for relaying the necessary information in a fire fighting environment needed to increase situational awareness in real time. In fire fighting environments, thermal imaging cameras are used due to smoke and low visibility hence obtaining relative orientation from the vanishing point estimation is very difficult. The following technique that is the content of this research implements a novel optical flow based video compass for orientation estimation and fused IMU data based activity recognition for IPS. This technique helps first responders to go into unprepared, unknown environments and still maintain situational awareness like the orientation and, position of the victim fire fighters.",0
"In search and rescue operations, situational awareness and indoor location tracking are of utmost importance for firefighters. Global Positioning System (GPS) is not the most effective solution for Indoor Positioning Systems (IPS). Alternative techniques such as dead reckoning, Wifi and Bluetooth-based triangulation, and Structure from Motion (SFM) based scene reconstruction are available for IPS. However, these techniques are unsuitable for fire-fighting environments due to the high temperatures, rapidly changing conditions, and low parallax in thermal images. Thermal imaging cameras are used in such environments, but obtaining relative orientation from vanishing point estimation is challenging because of smoke and low visibility. This research proposes a novel technique that uses optical flow-based video compass for orientation estimation and fused Inertial Measurement Unit (IMU) data-based activity recognition for IPS. This technique enables first responders to navigate unprepared environments and maintain situational awareness, including the orientation and position of victim firefighters.",1
"The objective of this paper is self-supervised learning from video, in particular for representations for action recognition. We make the following contributions: (i) We propose a new architecture and learning framework Memory-augmented Dense Predictive Coding (MemDPC) for the task. It is trained with a predictive attention mechanism over the set of compressed memories, such that any future states can always be constructed by a convex combination of the condense representations, allowing to make multiple hypotheses efficiently. (ii) We investigate visual-only self-supervised video representation learning from RGB frames, or from unsupervised optical flow, or both. (iii) We thoroughly evaluate the quality of learnt representation on four different downstream tasks: action recognition, video retrieval, learning with scarce annotations, and unintentional action classification. In all cases, we demonstrate state-of-the-art or comparable performance over other approaches with orders of magnitude fewer training data.",0
"The aim of this article is to explore self-supervised learning from video, with a focus on creating representations for action recognition. Our contributions are as follows: Firstly, we introduce a novel architecture and learning framework called Memory-augmented Dense Predictive Coding (MemDPC) for this task. This approach uses a predictive attention mechanism over compressed memories, enabling the efficient construction of multiple hypotheses for future states through a convex combination of condensed representations. Secondly, we examine visual-only self-supervised video representation learning from RGB frames, unsupervised optical flow, or both. Lastly, we conduct a comprehensive evaluation of the quality of the learned representations across four different downstream tasks, including action recognition, video retrieval, learning with limited annotations, and unintentional action classification. Our results demonstrate state-of-the-art or comparable performance compared to other approaches, with significantly less training data required.",1
"Human action recognition is regarded as a key cornerstone in domains such as surveillance or video understanding. Despite recent progress in the development of end-to-end solutions for video-based action recognition, achieving state-of-the-art performance still requires using auxiliary hand-crafted motion representations, e.g., optical flow, which are usually computationally demanding. In this work, we propose to use residual frames (i.e., differences between adjacent RGB frames) as an alternative ""lightweight"" motion representation, which carries salient motion information and is computationally efficient. In addition, we develop a new pseudo-3D convolution module which decouples 3D convolution into 2D and 1D convolution. The proposed module exploits residual information in the feature space to better structure motions, and is equipped with a self-attention mechanism that assists to recalibrate the appearance and motion features. Empirical results confirm the efficiency and effectiveness of residual frames as well as the proposed pseudo-3D convolution module.",0
"The recognition of human actions is considered crucial in fields such as video comprehension and surveillance. Although recent advancements have been made in creating end-to-end solutions for identifying actions in videos, achieving optimal performance often requires the use of manually crafted motion representations, like optical flow, which can be computationally intensive. This study proposes the use of residual frames (the contrast between adjacent RGB frames) as a more lightweight alternative for motion representation. These frames contain significant motion information and are less computationally demanding. Additionally, a new pseudo-3D convolution module is developed, which splits 3D convolution into 2D and 1D convolution. This module utilizes residual information in the feature space to better organize motions and is equipped with a self-attention mechanism to recalibrate appearance and motion features. Experimental results demonstrate the efficiency and effectiveness of residual frames and the proposed pseudo-3D convolution module.",1
"Dynamic Vision Sensors (DVSs) asynchronously stream events in correspondence of pixels subject to brightness changes. Differently from classic vision devices, they produce a sparse representation of the scene. Therefore, to apply standard computer vision algorithms, events need to be integrated into a frame or event-surface. This is usually attained through hand-crafted grids that reconstruct the frame using ad-hoc heuristics. In this paper, we propose Matrix-LSTM, a grid of Long Short-Term Memory (LSTM) cells that efficiently process events and learn end-to-end task-dependent event-surfaces. Compared to existing reconstruction approaches, our learned event-surface shows good flexibility and expressiveness on optical flow estimation on the MVSEC benchmark and it improves the state-of-the-art of event-based object classification on the N-Cars dataset.",0
"Dynamic Vision Sensors (DVSs) detect changes in pixel brightness and transmit events asynchronously. Unlike traditional vision devices, DVSs produce a sparse representation of the scene. To utilize standard computer vision algorithms, events must be integrated into a frame or event-surface. Typically, this involves using hand-crafted grids and ad-hoc heuristics to reconstruct the frame. In this paper, we propose Matrix-LSTM, a grid of Long Short-Term Memory (LSTM) cells that efficiently process events and learn task-dependent event-surfaces in an end-to-end manner. Our approach outperforms existing reconstruction methods on optical flow estimation in the MVSEC benchmark and improves the state-of-the-art of event-based object classification on the N-Cars dataset, while also providing greater flexibility and expressiveness.",1
"Particle Image Velocimetry (PIV) is a classical flow estimation problem which is widely considered and utilised, especially as a diagnostic tool in experimental fluid dynamics and the remote sensing of environmental flows. Recently, the development of deep learning based methods has inspired new approaches to tackle the PIV problem. These supervised learning based methods are driven by large volumes of data with ground truth training information. However, it is difficult to collect reliable ground truth data in large-scale, real-world scenarios. Although synthetic datasets can be used as alternatives, the gap between the training set-ups and real-world scenarios limits applicability. We present here what we believe to be the first work which takes an unsupervised learning based approach to tackle PIV problems. The proposed approach is inspired by classic optical flow methods. Instead of using ground truth data, we make use of photometric loss between two consecutive image frames, consistency loss in bidirectional flow estimates and spatial smoothness loss to construct the total unsupervised loss function. The approach shows significant potential and advantages for fluid flow estimation. Results presented here demonstrate that our method outputs competitive results compared with classical PIV methods as well as supervised learning based methods for a broad PIV dataset, and even outperforms these existing approaches in some difficult flow cases. Codes and trained models are available at https://github.com/erizmr/UnLiteFlowNet-PIV.",0
"PIV, a well-known flow estimation problem, is commonly used in experimental fluid dynamics and environmental flow remote sensing as a diagnostic tool. Recently, deep learning has led to new methods for tackling this problem. However, these supervised learning methods rely on large data volumes with ground truth training information, which is difficult to collect in real-world scenarios. Synthetic datasets can be used, but their limited applicability due to differences with real-world set-ups restricts their use. We present an unsupervised learning based approach, inspired by classic optical flow methods, which does not rely on ground truth data. Our approach uses photometric loss, consistency loss in bidirectional flow estimates, and spatial smoothness loss to construct the total unsupervised loss function. This approach shows significant potential and advantages for fluid flow estimation, as demonstrated by our competitive results with classical PIV methods and supervised learning based methods for a broad PIV dataset. In some difficult flow cases, our method even outperforms existing approaches. We have made our codes and trained models available at https://github.com/erizmr/UnLiteFlowNet-PIV.",1
"Many researches have been carried out for change detection using temporal SAR images. In this paper an algorithm for change detection using SAR videos has been proposed. There are various challenges related to SAR videos such as high level of speckle noise, rotation of SAR image frames of the video around a particular axis due to the circular movement of airborne vehicle, non-uniform back scattering of SAR pulses. Hence conventional change detection algorithms used for optical videos and SAR temporal images cannot be directly utilized for SAR videos. We propose an algorithm which is a combination of optical flow calculation using Lucas Kanade (LK) method and blob detection. The developed method follows a four steps approach: image filtering and enhancement, applying LK method, blob analysis and combining LK method with blob analysis. The performance of the developed approach was tested on SAR videos available on Sandia National Laboratories website and SAR videos generated by a SAR simulator.",0
"Numerous studies have been conducted to detect changes through the use of temporal SAR images. This paper introduces a new algorithm for change detection using SAR videos. However, SAR videos present several challenges, including a high level of speckle noise, rotation of SAR image frames due to the circular movement of the airborne vehicle, and non-uniform backscattering of SAR pulses. As a result, conventional change detection algorithms for optical videos and SAR temporal images cannot be directly applied to SAR videos. To address this, we propose a four-step approach combining optical flow calculation using the Lucas Kanade (LK) method with blob detection. The approach involves image filtering and enhancement, application of the LK method, blob analysis, and combining the LK method with blob analysis. We tested the performance of this approach using SAR videos from the Sandia National Laboratories website and SAR videos generated by a SAR simulator.",1
"Video super-resolution (VSR) aims to utilize multiple low-resolution frames to generate a high-resolution prediction for each frame. In this process, inter- and intra-frames are the key sources for exploiting temporal and spatial information. However, there are a couple of limitations for existing VSR methods. First, optical flow is often used to establish temporal correspondence. But flow estimation itself is error-prone and affects recovery results. Second, similar patterns existing in natural images are rarely exploited for the VSR task. Motivated by these findings, we propose a temporal multi-correspondence aggregation strategy to leverage similar patches across frames, and a cross-scale nonlocal-correspondence aggregation scheme to explore self-similarity of images across scales. Based on these two new modules, we build an effective multi-correspondence aggregation network (MuCAN) for VSR. Our method achieves state-of-the-art results on multiple benchmark datasets. Extensive experiments justify the effectiveness of our method.",0
"The objective of Video super-resolution (VSR) is to create high-resolution predictions for each frame by utilizing multiple low-resolution frames. To achieve this, temporal and spatial information from inter- and intra-frames is vital. However, current VSR techniques have some limitations. Firstly, optical flow is used to establish temporal correspondence, but its estimation is unreliable and can negatively impact the recovery outcome. Secondly, natural image patterns that could be beneficial for VSR are not often exploited. In light of these challenges, we propose a new method called the temporal multi-correspondence aggregation strategy, which employs similar patches across frames. Additionally, we use a cross-scale nonlocal-correspondence aggregation scheme to explore self-similarity of images across scales. Our approach, the multi-correspondence aggregation network (MuCAN), is built on these two new modules and has shown to produce superior results compared to existing methods when tested on various benchmark datasets. We have conducted extensive experiments that validate the effectiveness of our approach.",1
"Video style transfer techniques inspire many exciting applications on mobile devices. However, their efficiency and stability are still far from satisfactory. To boost the transfer stability across frames, optical flow is widely adopted, despite its high computational complexity, e.g. occupying over 97% inference time. This paper proposes to learn a lightweight video style transfer network via knowledge distillation paradigm. We adopt two teacher networks, one of which takes optical flow during inference while the other does not. The output difference between these two teacher networks highlights the improvements made by optical flow, which is then adopted to distill the target student network. Furthermore, a low-rank distillation loss is employed to stabilize the output of student network by mimicking the rank of input videos. Extensive experiments demonstrate that our student network without an optical flow module is still able to generate stable video and runs much faster than the teacher network.",0
"The use of video style transfer techniques has sparked interest in many potential mobile device applications. However, their effectiveness and reliability are currently unsatisfactory. To enhance the transfer stability between frames, optical flow is commonly used, even though it requires a high amount of computational complexity, taking up over 97% inference time. In this study, we propose a lightweight video style transfer network that is learned through the knowledge distillation approach. Two teacher networks are adopted, one of which uses optical flow during inference while the other does not. By comparing the output of these two teacher networks, we can identify the improvements made by optical flow, which are then used to teach the target student network. Additionally, we employ a low-rank distillation loss to increase the stability of the student network output by mimicking the rank of input videos. Our extensive experiments demonstrate that, even without the optical flow module, our student network can generate stable video and runs much faster than the teacher network.",1
"Novel view synthesis often needs the paired data from both the source and target views. This paper proposes a view translation model under cVAE-GAN framework without requiring the paired data. We design a conditional deformable module (CDM) which uses the view condition vectors as the filters to convolve the feature maps of the main branch in VAE. It generates several pairs of displacement maps to deform the features, like the 2D optical flows. The results are fed into the deformed feature based normalization module (DFNM), which scales and offsets the main branch feature, given its deformed one as the input from the side branch. Taking the advantage of the CDM and DFNM, the encoder outputs a view-irrelevant posterior, while the decoder takes the code drawn from it to synthesize the reconstructed and the viewtranslated images. To further ensure the disentanglement between the views and other factors, we add adversarial training on the code. The results and ablation studies on MultiPIE and 3D chair datasets validate the effectiveness of the framework in cVAE and the designed module.",0
"This article introduces a new method for view synthesis that does not require paired data from both the source and target views. The proposed approach is based on a view translation model within the cVAE-GAN framework. The model includes a conditional deformable module (CDM) that uses view condition vectors as filters to convolve feature maps, producing displacement maps that deform features like 2D optical flows. These results are then fed into the deformed feature based normalization module (DFNM), which scales and offsets the main branch feature. By taking advantage of CDM and DFNM, the encoder outputs a view-irrelevant posterior, while the decoder uses the code drawn from it to synthesize both the reconstructed and view-translated images. To ensure disentanglement between views and other factors, adversarial training is added to the code. Experiments and ablation studies on MultiPIE and 3D chair datasets confirm the effectiveness of the proposed approach within the cVAE framework and the designed module.",1
"Cost volume is an essential component of recent deep models for optical flow estimation and is usually constructed by calculating the inner product between two feature vectors. However, the standard inner product in the commonly-used cost volume may limit the representation capacity of flow models because it neglects the correlation among different channel dimensions and weighs each dimension equally. To address this issue, we propose a learnable cost volume (LCV) using an elliptical inner product, which generalizes the standard inner product by a positive definite kernel matrix. To guarantee its positive definiteness, we perform spectral decomposition on the kernel matrix and re-parameterize it via the Cayley representation. The proposed LCV is a lightweight module and can be easily plugged into existing models to replace the vanilla cost volume. Experimental results show that the LCV module not only improves the accuracy of state-of-the-art models on standard benchmarks, but also promotes their robustness against illumination change, noises, and adversarial perturbations of the input signals.",0
"In current deep models for optical flow estimation, cost volume is a crucial aspect that is often developed by computing the inner product of two feature vectors. However, this standard inner product in the prevalent cost volume can limit the flow model's representation capacity as it disregards the correlation among different channel dimensions and treats each dimension equally. To tackle this problem, we present a learnable cost volume (LCV) that employs an elliptical inner product, which extends the standard inner product with a positive definite kernel matrix. To ensure its positive definiteness, we perform spectral decomposition on the kernel matrix and re-parameterize it via the Cayley representation. The LCV module is lightweight and can be easily integrated into existing models to replace the vanilla cost volume. Our experimental findings demonstrate that the LCV module enhances the accuracy of cutting-edge models on standard benchmarks and enhances their resilience against changes in illumination, noises, and adversarial perturbations in the input signals.",1
"Recently, researchers in Machine Learning algorithms, Computer Vision scientists, engineers and others, showed a growing interest in 3D simulators as a mean to artificially create experimental settings that are very close to those in the real world. However, most of the existing platforms to interface algorithms with 3D environments are often designed to setup navigation-related experiments, to study physical interactions, or to handle ad-hoc cases that are not thought to be customized, sometimes lacking a strong photorealistic appearance and an easy-to-use software interface. In this paper, we present a novel platform, SAILenv, that is specifically designed to be simple and customizable, and that allows researchers to experiment visual recognition in virtual 3D scenes. A few lines of code are needed to interface every algorithm with the virtual world, and non-3D-graphics experts can easily customize the 3D environment itself, exploiting a collection of photorealistic objects. Our framework yields pixel-level semantic and instance labeling, depth, and, to the best of our knowledge, it is the only one that provides motion-related information directly inherited from the 3D engine. The client-server communication operates at a low level, avoiding the overhead of HTTP-based data exchanges. We perform experiments using a state-of-the-art object detector trained on real-world images, showing that it is able to recognize the photorealistic 3D objects of our environment. The computational burden of the optical flow compares favourably with the estimation performed using modern GPU-based convolutional networks or more classic implementations. We believe that the scientific community will benefit from the easiness and high-quality of our framework to evaluate newly proposed algorithms in their own customized realistic conditions.",0
"In recent times, there has been a growing interest among Machine Learning algorithm researchers, Computer Vision scientists, engineers, and others in the use of 3D simulators to artificially create experimental settings that closely resemble real-world scenarios. However, the majority of existing platforms for interfacing algorithms with 3D environments are typically designed for navigation-related experiments, physical interaction studies, or ad-hoc cases without customization options, and often lack photorealistic appearance and user-friendly software interfaces. This paper presents a new platform, SAILenv, specifically designed to be customizable and easy to use, allowing researchers to experiment with visual recognition in virtual 3D scenes. With just a few lines of code, algorithms can be easily interfaced with the virtual world, and non-experts in 3D graphics can easily customize the 3D environment using a variety of photorealistic objects. Our framework provides pixel-level semantic and instance labeling, depth, and motion-related information inherited directly from the 3D engine. The client-server communication operates at a low level, avoiding the overhead of HTTP-based data exchanges. We conducted experiments using a state-of-the-art object detector trained on real-world images, demonstrating its ability to recognize photorealistic 3D objects in our environment. The computational burden of the optical flow is favorable compared to modern GPU-based convolutional networks or classic implementations. We believe that the scientific community will benefit from the ease and high quality of our framework to evaluate newly proposed algorithms in their own customized realistic conditions.",1
"Motion plays a crucial role in understanding videos and most state-of-the-art neural models for video classification incorporate motion information typically using optical flows extracted by a separate off-the-shelf method. As the frame-by-frame optical flows require heavy computation, incorporating motion information has remained a major computational bottleneck for video understanding. In this work, we replace external and heavy computation of optical flows with internal and light-weight learning of motion features. We propose a trainable neural module, dubbed MotionSqueeze, for effective motion feature extraction. Inserted in the middle of any neural network, it learns to establish correspondences across frames and convert them into motion features, which are readily fed to the next downstream layer for better prediction. We demonstrate that the proposed method provides a significant gain on four standard benchmarks for action recognition with only a small amount of additional cost, outperforming the state of the art on Something-Something-V1&V2 datasets.",0
"Understanding videos relies heavily on motion, and modern neural models for video classification incorporate motion information using optical flows extracted through a separate, computationally-intensive method. However, processing optical flows frame-by-frame remains a significant computational bottleneck. This study proposes MotionSqueeze, a trainable neural module that extracts motion features internally and efficiently, without relying on external optical flow computation. MotionSqueeze establishes correspondences across frames and converts them into motion features, which are then fed to the next downstream layer for improved prediction. The proposed method outperforms the state of the art on Something-Something-V1&V2 datasets and provides significant gains on four standard benchmarks for action recognition, with only a small increase in cost.",1
"Supervised learning in large discriminative models is a mainstay for modern computer vision. Such an approach necessitates investing in large-scale human-annotated datasets for achieving state-of-the-art results. In turn, the efficacy of supervised learning may be limited by the size of the human annotated dataset. This limitation is particularly notable for image segmentation tasks, where the expense of human annotation is especially large, yet large amounts of unlabeled data may exist. In this work, we ask if we may leverage semi-supervised learning in unlabeled video sequences and extra images to improve the performance on urban scene segmentation, simultaneously tackling semantic, instance, and panoptic segmentation. The goal of this work is to avoid the construction of sophisticated, learned architectures specific to label propagation (e.g., patch matching and optical flow). Instead, we simply predict pseudo-labels for the unlabeled data and train subsequent models with both human-annotated and pseudo-labeled data. The procedure is iterated for several times. As a result, our Naive-Student model, trained with such simple yet effective iterative semi-supervised learning, attains state-of-the-art results at all three Cityscapes benchmarks, reaching the performance of 67.8% PQ, 42.6% AP, and 85.2% mIOU on the test set. We view this work as a notable step towards building a simple procedure to harness unlabeled video sequences and extra images to surpass state-of-the-art performance on core computer vision tasks.",0
"Modern computer vision heavily relies on supervised learning for large discriminative models, which requires investing in human-annotated datasets to achieve the best possible results. However, the efficacy of this approach is limited by the size of the annotated dataset, especially for image segmentation tasks that are expensive to annotate. To address this, we explore the potential of semi-supervised learning using unlabeled video sequences and extra images to improve urban scene segmentation performance for semantic, instance, and panoptic segmentation. Instead of using complex architectures for label propagation, we predict pseudo-labels for the unlabeled data and train subsequent models with both human-annotated and pseudo-labeled data. This iterative process using simple yet effective semi-supervised learning resulted in our Naive-Student model achieving state-of-the-art results in all three Cityscapes benchmarks, with a performance of 67.8% PQ, 42.6% AP, and 85.2% mIOU on the test set. This work demonstrates the potential of a straightforward approach to leverage unlabeled video sequences and extra images to surpass state-of-the-art performance in core computer vision tasks.",1
"Single encoder-decoder methodologies for semantic segmentation are reaching their peak in terms of segmentation quality and efficiency per number of layers. To address these limitations, we propose a new architecture based on a decoder which uses a set of shallow networks for capturing more information content. The new decoder has a new topology of skip connections, namely backward and stacked residual connections. In order to further improve the architecture we introduce a weight function which aims to re-balance classes to increase the attention of the networks to under-represented objects. We carried out an extensive set of experiments that yielded state-of-the-art results for the CamVid, Gatech and Freiburg Forest datasets. Moreover, to further prove the effectiveness of our decoder, we conducted a set of experiments studying the impact of our decoder to state-of-the-art segmentation techniques. Additionally, we present a set of experiments augmenting semantic segmentation with optical flow information, showing that motion clues can boost pure image based semantic segmentation approaches.",0
"Semantic segmentation using single encoder-decoder methodologies has reached a peak in terms of segmentation quality and efficiency per number of layers. To overcome these limitations, we propose a novel architecture that uses a decoder with a set of shallow networks to capture more information content. Our new decoder has a unique topology of skip connections, including backward and stacked residual connections. To improve the architecture further, we introduce a weight function that rebalances classes to increase the networks' attention to under-represented objects. We conducted extensive experiments that yielded state-of-the-art results for the CamVid, Gatech, and Freiburg Forest datasets. Furthermore, we studied the impact of our decoder on state-of-the-art segmentation techniques and presented experiments that integrate optical flow information to augment semantic segmentation, showing that motion clues can enhance image-based semantic segmentation approaches.",1
"In this work we review the coarse-to-fine spatial feature pyramid concept, which is used in state-of-the-art optical flow estimation networks to make exploration of the pixel flow search space computationally tractable and efficient. Within an individual pyramid level, we improve the cost volume construction process by departing from a warping- to a sampling-based strategy, which avoids ghosting and hence enables us to better preserve fine flow details. We further amplify the positive effects through a level-specific, loss max-pooling strategy that adaptively shifts the focus of the learning process on under-performing predictions. Our second contribution revises the gradient flow across pyramid levels. The typical operations performed at each pyramid level can lead to noisy, or even contradicting gradients across levels. We show and discuss how properly blocking some of these gradient components leads to improved convergence and ultimately better performance. Finally, we introduce a distillation concept to counteract the issue of catastrophic forgetting and thus preserving knowledge over models sequentially trained on multiple datasets. Our findings are conceptually simple and easy to implement, yet result in compelling improvements on relevant error measures that we demonstrate via exhaustive ablations on datasets like Flying Chairs2, Flying Things, Sintel and KITTI. We establish new state-of-the-art results on the challenging Sintel and KITTI 2012 test datasets, and even show the portability of our findings to different optical flow and depth from stereo approaches.",0
"This work examines the use of coarse-to-fine spatial feature pyramids in optical flow estimation networks. We aim to make the pixel flow search space more manageable by implementing a sampling-based strategy for constructing cost volumes, rather than a warping-based approach. Our approach avoids ghosting and preserves fine flow details, which is further enhanced by a level-specific, loss max-pooling strategy that focuses on underperforming predictions. We also address the issue of noisy and contradicting gradients across pyramid levels by blocking some gradient components, resulting in improved convergence and better performance. To counteract the problem of catastrophic forgetting, we introduce a distillation concept that preserves knowledge over models sequentially trained on multiple datasets. Our approach yields significant improvements on relevant error measures, as demonstrated through exhaustive ablations on datasets like Flying Chairs2, Flying Things, Sintel, and KITTI. We establish new state-of-the-art results on the challenging Sintel and KITTI 2012 test datasets, and our findings are applicable to different optical flow and depth from stereo approaches.",1
"Deep learning approaches have achieved great success in addressing the problem of optical flow estimation. The keys to success lie in the use of cost volume and coarse-to-fine flow inference. However, the matching problem becomes ill-posed when partially occluded or homogeneous regions exist in images. This causes a cost volume to contain outliers and affects the flow decoding from it. Besides, the coarse-to-fine flow inference demands an accurate flow initialization. Ambiguous correspondence yields erroneous flow fields and affects the flow inferences in subsequent levels. In this paper, we introduce LiteFlowNet3, a deep network consisting of two specialized modules, to address the above challenges. (1) We ameliorate the issue of outliers in the cost volume by amending each cost vector through an adaptive modulation prior to the flow decoding. (2) We further improve the flow accuracy by exploring local flow consistency. To this end, each inaccurate optical flow is replaced with an accurate one from a nearby position through a novel warping of the flow field. LiteFlowNet3 not only achieves promising results on public benchmarks but also has a small model size and a fast runtime.",0
"The problem of optical flow estimation has been successfully addressed by deep learning methods which employ cost volume and coarse-to-fine flow inference. However, the presence of partially occluded or homogeneous regions in images can cause matching problems leading to outliers in the cost volume and inaccurate flow decoding. Additionally, accurate flow initialization is required for coarse-to-fine flow inference, as ambiguous correspondence can result in erroneous flow fields and negatively impact subsequent flow inferences. To alleviate these challenges, we propose LiteFlowNet3, a deep network with two specialized modules. Firstly, we address the issue of outliers in the cost volume by using adaptive modulation to amend each cost vector prior to flow decoding. Secondly, we enhance flow accuracy by exploring local flow consistency through a novel warping of the flow field. LiteFlowNet3 achieves promising results on public benchmarks, with the added benefits of a small model size and fast runtime.",1
"This paper considers the generic problem of dense alignment between two images, whether they be two frames of a video, two widely different views of a scene, two paintings depicting similar content, etc. Whereas each such task is typically addressed with a domain-specific solution, we show that a simple unsupervised approach performs surprisingly well across a range of tasks. Our main insight is that parametric and non-parametric alignment methods have complementary strengths. We propose a two-stage process: first, a feature-based parametric coarse alignment using one or more homographies, followed by non-parametric fine pixel-wise alignment. Coarse alignment is performed using RANSAC on off-the-shelf deep features. Fine alignment is learned in an unsupervised way by a deep network which optimizes a standard structural similarity metric (SSIM) between the two images, plus cycle-consistency. Despite its simplicity, our method shows competitive results on a range of tasks and datasets, including unsupervised optical flow on KITTI, dense correspondences on Hpatches, two-view geometry estimation on YFCC100M, localization on Aachen Day-Night, and, for the first time, fine alignment of artworks on the Brughel dataset. Our code and data are available at http://imagine.enpc.fr/~shenx/RANSAC-Flow/",0
"This article explores the general issue of dense alignment between two images, regardless of whether they are two frames in a video, two vastly different views of a scene, or two paintings depicting similar content. Although these tasks are usually tackled with specialized solutions, the authors demonstrate that a straightforward unsupervised approach can perform surprisingly well across a range of tasks. The authors' primary insight is that parametric and non-parametric alignment methods have complementary strengths, and they propose a two-stage process that involves feature-based parametric coarse alignment and non-parametric fine pixel-wise alignment. The coarse alignment employs RANSAC on pre-existing deep features, while the fine alignment is learned in an unsupervised manner by a deep network that optimizes a standard structural similarity metric between the two images, as well as cycle-consistency. Despite its simplicity, this approach shows promising results on a variety of tasks and datasets, including unsupervised optical flow, dense correspondences, two-view geometry estimation, localization, and artwork alignment. The authors offer their code and data at http://imagine.enpc.fr/~shenx/RANSAC-Flow/.",1
"For semantic segmentation, most existing real-time deep models trained with each frame independently may produce inconsistent results for a video sequence. Advanced methods take into considerations the correlations in the video sequence, e.g., by propagating the results to the neighboring frames using optical flow, or extracting the frame representations with other frames, which may lead to inaccurate results or unbalanced latency. In this work, we process efficient semantic video segmentation in a per-frame fashion during the inference process. Different from previous per-frame models, we explicitly consider the temporal consistency among frames as extra constraints during the training process and embed the temporal consistency into the segmentation network. Therefore, in the inference process, we can process each frame independently with no latency, and improve the temporal consistency with no extra computational cost and post-processing. We employ compact models for real-time execution. To narrow the performance gap between compact models and large models, new knowledge distillation methods are designed. Our results outperform previous keyframe based methods with a better trade-off between the accuracy and the inference speed on popular benchmarks, including the Cityscapes and Camvid. The temporal consistency is also improved compared with corresponding baselines which are trained with each frame independently. Code is available at: https://tinyurl.com/segment-video",0
"Many real-time deep models trained for semantic segmentation produce inconsistent results when applied to a video sequence due to the lack of consideration for correlations between frames. Methods that do account for these correlations, such as those that use optical flow or extract frame representations with other frames, often result in inaccurate results or unbalanced latency. In this study, we introduce an approach that performs efficient semantic video segmentation on a per-frame basis during the inference process while explicitly considering temporal consistency between frames as an extra constraint during training. This embedding of temporal consistency into the segmentation network enables us to process each frame independently without additional computational cost or post-processing and improve temporal consistency. Additionally, we use compact models for real-time execution and have designed new knowledge distillation methods to narrow the performance gap between compact and large models. Our approach outperforms previous keyframe-based methods in benchmarks, including Cityscapes and Camvid, with a better trade-off between accuracy and inference speed while also improving temporal consistency compared to corresponding baselines that are trained independently. Our code is available at https://tinyurl.com/segment-video.",1
"We present a novel deep learning architecture for probabilistic future prediction from video. We predict the future semantics, geometry and motion of complex real-world urban scenes and use this representation to control an autonomous vehicle. This work is the first to jointly predict ego-motion, static scene, and the motion of dynamic agents in a probabilistic manner, which allows sampling consistent, highly probable futures from a compact latent space. Our model learns a representation from RGB video with a spatio-temporal convolutional module. The learned representation can be explicitly decoded to future semantic segmentation, depth, and optical flow, in addition to being an input to a learnt driving policy. To model the stochasticity of the future, we introduce a conditional variational approach which minimises the divergence between the present distribution (what could happen given what we have seen) and the future distribution (what we observe actually happens). During inference, diverse futures are generated by sampling from the present distribution.",0
"Our innovative deep learning architecture is designed to predict the future of complex real-world urban scenes in a probabilistic manner using video. We can anticipate the future semantics, geometry, and motion, which is valuable for controlling autonomous vehicles. Our approach is unique in that it is the first to predict ego-motion, static scene, and dynamic agents' motion concurrently using a compact latent space to produce consistent, highly likely futures. Our model uses spatio-temporal convolutional modules to learn a representation from RGB video, which can then be explicitly decoded to future semantic segmentation, depth, and optical flow. This representation is also utilized as an input for a learnt driving policy. To account for the stochastic nature of the future, we introduce a conditional variational technique that minimizes the gap between the present and future distributions. During inference, we generate diverse futures by sampling from the present distribution.",1
"The use of hand gestures can be a useful tool for many applications in the human-computer interaction community. In a broad range of areas hand gesture techniques can be applied specifically in sign language recognition, robotic surgery, etc. In the process of hand gesture recognition, proper detection, and tracking of the moving hand become challenging due to the varied shape and size of the hand. Here the objective is to track the movement of the hand irrespective of the shape, size, and color of the hand. And, for this, a motion template guided by optical flow (OFMT) is proposed. OFMT is a compact representation of the motion information of a gesture encoded into a single image. In the experimentation, different datasets using bare hand with an open palm, and folded palm wearing green-glove are used, and in both cases, we could generate the OFMT images with equal precision. Recently, deep network-based techniques have shown impressive improvements as compared to conventional hand-crafted feature-based techniques. Moreover, in the literature, it is seen that the use of different streams with informative input data helps to increase the performance in the recognition accuracy. This work basically proposes a two-stream fusion model for hand gesture recognition and a compact yet efficient motion template based on optical flow. Specifically, the two-stream network consists of two layers: a 3D convolutional neural network (C3D) that takes gesture videos as input and a 2D-CNN that takes OFMT images as input. C3D has shown its efficiency in capturing spatio-temporal information of a video. Whereas OFMT helps to eliminate irrelevant gestures providing additional motion information. Though each stream can work independently, they are combined with a fusion scheme to boost the recognition results. We have shown the efficiency of the proposed two-stream network on two databases.",0
"Hand gestures have many applications in human-computer interaction. These techniques can be used in various areas such as sign language recognition and robotic surgery. However, detecting and tracking the moving hand can be challenging due to the hand's varying shape and size. The objective is to track the hand's movement regardless of its shape, size, and color. To achieve this, a motion template guided by optical flow (OFMT) has been proposed. OFMT is a compact representation of the motion information of a gesture encoded into a single image. In experiments, different datasets using bare hands with an open or folded palm and a green-gloved hand were used, and OFMT images were generated with equal precision. Recent studies have shown that deep network-based techniques outperform conventional hand-crafted feature-based techniques. Using different streams with informative input data can improve recognition accuracy. This work proposes a two-stream fusion model for hand gesture recognition, including a compact yet efficient motion template based on optical flow. The two-stream network consists of a 3D convolutional neural network (C3D) that takes gesture videos as input and a 2D-CNN that takes OFMT images as input. C3D efficiently captures spatio-temporal information of a video, while OFMT eliminates irrelevant gestures and provides additional motion information. Although each stream can work independently, they are combined with a fusion scheme to boost the recognition results. The efficiency of the proposed two-stream network has been demonstrated on two databases.",1
"Optical flow is a crucial component of the feature space for early visual processing of dynamic scenes especially in new applications such as self-driving vehicles, drones and autonomous robots. The dynamic vision sensors are well suited for such applications because of their asynchronous, sparse and temporally precise representation of the visual dynamics. Many algorithms proposed for computing visual flow for these sensors suffer from the aperture problem as the direction of the estimated flow is governed by the curvature of the object rather than the true motion direction. Some methods that do overcome this problem by temporal windowing under-utilize the true precise temporal nature of the dynamic sensors. In this paper, we propose a novel multi-scale plane fitting based visual flow algorithm that is robust to the aperture problem and also computationally fast and efficient. Our algorithm performs well in many scenarios ranging from fixed camera recording simple geometric shapes to real world scenarios such as camera mounted on a moving car and can successfully perform event-by-event motion estimation of objects in the scene to allow for predictions of upto 500 ms i.e. equivalent to 10 to 25 frames with traditional cameras.",0
"Optical flow is an essential feature space component for early visual processing of dynamic scenes, particularly in innovative applications like autonomous robots, drones, and self-driving vehicles. Dynamic vision sensors are well-suited for these applications due to their asynchronous, sparse, and temporally precise visual dynamics representation. However, many algorithms for computing visual flow for these sensors face the aperture problem, where the estimated flow direction is determined by the object's curvature rather than the true motion direction. Some methods that overcome this problem underutilize the sensor's true temporal nature. This paper proposes a novel multi-scale plane fitting-based visual flow algorithm that is both robust to the aperture problem and computationally efficient. Our algorithm excels in several scenarios, including fixed camera recording of simple geometric shapes and real-world situations such as a camera mounted on a moving vehicle. It can successfully estimate event-by-event motion in the scene, allowing for predictions of up to 500 ms, equivalent to 10 to 25 frames with traditional cameras.",1
"Transferring existing image-based detectors to the video is non-trivial since the quality of frames is always deteriorated by part occlusion, rare pose, and motion blur. Previous approaches exploit to propagate and aggregate features across video frames by using optical flow-warping. However, directly applying image-level optical flow onto the high-level features might not establish accurate spatial correspondences. Therefore, a novel module called Learnable Spatio-Temporal Sampling (LSTS) has been proposed to learn semantic-level correspondences among adjacent frame features accurately. The sampled locations are first randomly initialized, then updated iteratively to find better spatial correspondences guided by detection supervision progressively. Besides, Sparsely Recursive Feature Updating (SRFU) module and Dense Feature Aggregation (DFA) module are also introduced to model temporal relations and enhance per-frame features, respectively. Without bells and whistles, the proposed method achieves state-of-the-art performance on the ImageNet VID dataset with less computational complexity and real-time speed. Code will be made available at https://github.com/jiangzhengkai/LSTS.",0
"It is difficult to transfer image-based detectors to video due to the degradation of frame quality caused by factors such as part occlusion, rare pose, and motion blur. Previous methods have used optical flow-warping to propagate and aggregate features across frames, but applying this directly to high-level features may not establish accurate spatial correspondences. To address this issue, a new module called Learnable Spatio-Temporal Sampling (LSTS) has been proposed to accurately learn semantic-level correspondences among adjacent frame features. The LSTS module utilizes detection supervision to iteratively update sampled locations for better spatial correspondences. Additionally, the proposed method includes the Sparsely Recursive Feature Updating (SRFU) module and Dense Feature Aggregation (DFA) module to model temporal relations and improve per-frame features, respectively. The method achieves state-of-the-art performance on the ImageNet VID dataset with real-time speed and less computational complexity. The code for this method will be available at https://github.com/jiangzhengkai/LSTS.",1
"Modern methods for counting people in crowded scenes rely on deep networks to estimate people densities in individual images. As such, only very few take advantage of temporal consistency in video sequences, and those that do only impose weak smoothness constraints across consecutive frames.   In this paper, we advocate estimating people flows across image locations between consecutive images and inferring the people densities from these flows instead of directly regressing. This enables us to impose much stronger constraints encoding the conservation of the number of people. As a result, it significantly boosts performance without requiring a more complex architecture. Furthermore, it also enables us to exploit the correlation between people flow and optical flow to further improve the results.   We will demonstrate that we consistently outperform state-of-the-art methods on five benchmark datasets.",0
"Crowd counting methods nowadays use deep networks to estimate people densities in individual images, but only a small number of them utilize the temporal consistency in video sequences. The few that do, only impose weak smoothness constraints across consecutive frames. This paper proposes estimating people flows between consecutive images and inferring people densities from these flows instead of direct regression. This approach enables stronger constraints encoding the conservation of the number of people, leading to improved performance without requiring a more complex architecture. Moreover, it leverages the correlation between people flow and optical flow to further enhance the results. The study shows consistent outperformance of this method over state-of-the-art approaches across five benchmark datasets.",1
"In this paper, we focus on a prediction-based novelty estimation strategy upon the deep reinforcement learning (DRL) framework, and present a flow-based intrinsic curiosity module (FICM) to exploit the prediction errors from optical flow estimation as exploration bonuses. We propose the concept of leveraging motion features captured between consecutive observations to evaluate the novelty of observations in an environment. FICM encourages a DRL agent to explore observations with unfamiliar motion features, and requires only two consecutive frames to obtain sufficient information when estimating the novelty. We evaluate our method and compare it with a number of existing methods on multiple benchmark environments, including Atari games, Super Mario Bros., and ViZDoom. We demonstrate that FICM is favorable to tasks or environments featuring moving objects, which allow FICM to utilize the motion features between consecutive observations. We further ablatively analyze the encoding efficiency of FICM, and discuss its applicable domains comprehensively.",0
"The focus of this paper is on a unique method for estimating novelty in deep reinforcement learning (DRL) called prediction-based novelty estimation. To accomplish this, a flow-based intrinsic curiosity module (FICM) is introduced, which uses prediction errors from optical flow estimation as exploration bonuses. The authors propose using motion features between consecutive observations to assess the novelty of observations in an environment. With FICM, a DRL agent is encouraged to explore observations with unfamiliar motion features, and only requires two consecutive frames to estimate the novelty adequately. The authors evaluate their method on various benchmark environments, including Atari games, Super Mario Bros., and ViZDoom, and demonstrate that FICM works best in environments with moving objects. They also analyze the encoding efficiency of FICM and examine its applicable domains in detail.",1
"The objective of this paper is to recover the original component signals from a mixture audio with the aid of visual cues of the sound sources. Such task is usually referred as visually guided sound source separation. The proposed Cascaded Opponent Filter (COF) framework consists of multiple stages, which recursively refine the source separation. A key element in COF is a novel opponent filter module that identifies and relocates residual components between sources. The system is guided by the appearance and motion of the source, and, for this purpose, we study different representations based on video frames, optical flows, dynamic images, and their combinations. Finally, we propose a Sound Source Location Masking (SSLM) technique, which, together with COF, produces a pixel level mask of the source location. The entire system is trained end-to-end using a large set of unlabelled videos. We compare COF with recent baselines and obtain the state-of-the-art performance in three challenging datasets (MUSIC, A-MUSIC, and A-NATURAL). Project page: https://ly-zhu.github.io/cof-net.",0
"The aim of this article is to use visual cues of sound sources to separate the original component signals from a mixed audio. This process is known as visually guided sound source separation. The proposed framework, Cascaded Opponent Filter (COF), is composed of several stages that progressively refine the source separation. The COF system relies on an innovative opponent filter module that identifies and relocates residual components between sources. To guide the system, we explore different representations based on video frames, optical flows, dynamic images, and their combinations. Additionally, we introduce a Sound Source Location Masking (SSLM) technique, which, in conjunction with COF, generates a pixel level mask of the source location. We train the entire system end-to-end using a substantial amount of unlabelled videos. We compare COF with recent baselines and achieve the best performance in three challenging datasets (MUSIC, A-MUSIC, and A-NATURAL). For further information, visit our project page at https://ly-zhu.github.io/cof-net.",1
"Many semantic events in team sport activities e.g. basketball often involve both group activities and the outcome (score or not). Motion patterns can be an effective means to identify different activities. Global and local motions have their respective emphasis on different activities, which are difficult to capture from the optical flow due to the mixture of global and local motions. Hence it calls for a more effective way to separate the global and local motions. When it comes to the specific case for basketball game analysis, the successful score for each round can be reliably detected by the appearance variation around the basket. Based on the observations, we propose a scheme to fuse global and local motion patterns (MPs) and key visual information (KVI) for semantic event recognition in basketball videos. Firstly, an algorithm is proposed to estimate the global motions from the mixed motions based on the intrinsic property of camera adjustments. And the local motions could be obtained from the mixed and global motions. Secondly, a two-stream 3D CNN framework is utilized for group activity recognition over the separated global and local motion patterns. Thirdly, the basket is detected and its appearance features are extracted through a CNN structure. The features are utilized to predict the success or failure. Finally, the group activity recognition and success/failure prediction results are integrated using the kronecker product for event recognition. Experiments on NCAA dataset demonstrate that the proposed method obtains state-of-the-art performance.",0
"Team sports like basketball involve various semantic events that combine group activities and outcome, often measured by the score. Motion patterns are useful for identifying these events, but it's challenging to distinguish between global and local motions using optical flow. To address this issue, we propose a scheme that combines global and local motion patterns with key visual information for semantic event recognition in basketball videos. First, we use an algorithm to estimate global motions from mixed motions, followed by extracting local motions. We then employ a two-stream 3D CNN framework for group activity recognition and detect the basket to predict success or failure using CNN features. Finally, we integrate group activity recognition and success/failure prediction results using the kronecker product for event recognition. Our experiments on the NCAA dataset show that our method outperforms other approaches.",1
"We present a new lightweight CNN-based algorithm for multi-frame optical flow estimation. Our solution introduces a double recurrence over spatial scale and time through repeated use of a generic ""STaR"" (SpatioTemporal Recurrent) cell. It includes (i) a temporal recurrence based on conveying learned features rather than optical flow estimates; (ii) an occlusion detection process which is coupled with optical flow estimation and therefore uses a very limited number of extra parameters. The resulting STaRFlow algorithm gives state-of-the-art performances on MPI Sintel and Kitti2015 and involves significantly less parameters than all other methods with comparable results.",0
"A novel CNN-based algorithm for estimating multi-frame optical flow is presented, which is lightweight and efficient. Our approach adopts a double recurrence strategy that operates over both spatial and temporal scales. This is achieved by utilizing a SpatioTemporal Recurrent (STaR) cell in a repetitive manner. The STaRFlow algorithm incorporates a temporal recurrence mechanism that leverages learned features instead of optical flow estimates. Furthermore, an occlusion detection process is integrated with optical flow estimation, requiring only a small number of additional parameters. The resulting approach achieves state-of-the-art performance on the MPI Sintel and Kitti2015 datasets, and requires significantly fewer parameters than other comparable methods.",1
"Shopping behaviour analysis through counting and tracking of people in shop-like environments offers valuable information for store operators and provides key insights in the stores layout (e.g. frequently visited spots). Instead of using extra staff for this, automated on-premise solutions are preferred. These automated systems should be cost-effective, preferably on lightweight embedded hardware, work in very challenging situations (e.g. handling occlusions) and preferably work real-time. We solve this challenge by implementing a real-time TensorRT optimized YOLOv3-based pedestrian detector, on a Jetson TX2 hardware platform. By combining the detector with a sparse optical flow tracker we assign a unique ID to each customer and tackle the problem of loosing partially occluded customers. Our detector-tracker based solution achieves an average precision of 81.59% at a processing speed of 10 FPS. Besides valuable statistics, heat maps of frequently visited spots are extracted and used as an overlay on the video stream.",0
"By utilizing people counting and tracking in shop-like environments, store operators can obtain valuable insights into their customers' shopping behavior and the store's layout, such as frequently visited areas. To avoid hiring additional staff, automated on-premise systems are preferred. These systems should be cost-effective, lightweight, work in challenging situations (such as dealing with occlusions), and operate in real-time. Our approach to this challenge involves implementing a real-time TensorRT-optimized YOLOv3-based pedestrian detector on a Jetson TX2 hardware platform. By combining the detector with a sparse optical flow tracker, we can assign a unique ID to each customer and address the issue of partially occluded customers. Our detector-tracker solution achieves an average precision of 81.59% at a processing speed of 10 FPS. In addition to providing valuable statistics, we also extract heat maps of frequently visited spots, which are overlaid on the video stream.",1
"Fast motion feedback is crucial in computer-aided surgery (CAS) on moving tissue. Image-assistance in safety-critical vision applications requires a dense tracking of tissue motion. This can be done using optical flow (OF). Accurate motion predictions at high processing rates lead to higher patient safety. Current deep learning OF models show the common speed vs. accuracy trade-off. To achieve high accuracy at high processing rates, we propose patient-specific fine-tuning of a fast model. This minimizes the domain gap between training and application data, while reducing the target domain to the capability of the lower complex, fast model. We propose to obtain training sequences pre-operatively in the operation room. We handle missing ground truth, by employing teacher-student learning. Using flow estimations from teacher model FlowNet2 we specialize a fast student model FlowNet2S on the patient-specific domain. Evaluation is performed on sequences from the Hamlyn dataset. Our student model shows very good performance after fine-tuning. Tracking accuracy is comparable to the teacher model at a speed up of factor six. Fine-tuning can be performed within minutes, making it feasible for the operation room. Our method allows to use a real-time capable model that was previously not suited for this task. This method is laying the path for improved patient-specific motion estimation in CAS.",0
"In computer-aided surgery, quick feedback on motion is essential, especially when dealing with moving tissue. To ensure safety in critical vision applications, precise tracking of tissue motion is necessary, and optical flow (OF) is commonly used. However, there is often a trade-off between speed and accuracy in current deep learning OF models. To address this issue, we propose fine-tuning a fast model to achieve high accuracy at high processing rates, using patient-specific training data obtained pre-operatively in the operation room. To handle missing ground truth data, we employ teacher-student learning by using flow estimations from a teacher model (FlowNet2) to specialize a fast student model (FlowNet2S) on the patient-specific domain. Our approach yields excellent tracking accuracy comparable to the teacher model but at a six-times faster speed. Fine-tuning can be done quickly, making it feasible for the operation room. Our method paves the way for improved patient-specific motion estimation in computer-aided surgery.",1
"We design and implement an end-to-end system for real-time crime detection in low-light environments. Unlike Closed-Circuit Television, which performs reactively, the Low-Light Environment Neural Surveillance provides real time crime alerts. The system uses a low-light video feed processed in real-time by an optical-flow network, spatial and temporal networks, and a Support Vector Machine to identify shootings, assaults, and thefts. We create a low-light action-recognition dataset, LENS-4, which will be publicly available. An IoT infrastructure set up via Amazon Web Services interprets messages from the local board hosting the camera for action recognition and parses the results in the cloud to relay messages. The system achieves 71.5% accuracy at 20 FPS. The user interface is a mobile app which allows local authorities to receive notifications and to view a video of the crime scene. Citizens have a public app which enables law enforcement to push crime alerts based on user proximity.",0
"Our team has developed and implemented a comprehensive system to detect crimes in low-light environments in real-time. Unlike Closed-Circuit Television, which responds to incidents after they occur, our Low-Light Environment Neural Surveillance system provides immediate crime alerts. By processing a low-light video feed in real-time using an optical-flow network, spatial and temporal networks, and a Support Vector Machine, our system can identify incidents such as shootings, assaults, and thefts. We have also created a publicly available low-light action-recognition dataset called LENS-4. An IoT infrastructure is set up via Amazon Web Services to interpret messages from the local camera board and relay the results to the cloud for further analysis. The system boasts an accuracy rate of 71.5% at 20 FPS. The user interface is a mobile app that provides local authorities with notifications and video footage of the crime scene. Additionally, a public app is available to citizens, allowing law enforcement to send crime alerts based on user proximity.",1
"An object's geocentric pose, defined as the height above ground and orientation with respect to gravity, is a powerful representation of real-world structure for object detection, segmentation, and localization tasks using RGBD images. For close-range vision tasks, height and orientation have been derived directly from stereo-computed depth and more recently from monocular depth predicted by deep networks. For long-range vision tasks such as Earth observation, depth cannot be reliably estimated with monocular images. Inspired by recent work in monocular height above ground prediction and optical flow prediction from static images, we develop an encoding of geocentric pose to address this challenge and train a deep network to compute the representation densely, supervised by publicly available airborne lidar. We exploit these attributes to rectify oblique images and remove observed object parallax to dramatically improve the accuracy of localization and to enable accurate alignment of multiple images taken from very different oblique viewpoints. We demonstrate the value of our approach by extending two large-scale public datasets for semantic segmentation in oblique satellite images. All of our data and code are publicly available.",0
"The geocentric pose of an object, which refers to its height above the ground and its orientation in relation to gravity, is a useful tool for detecting, segmenting, and locating objects in RGBD images. While stereo-computed depth and monocular depth predicted by deep networks can determine height and orientation for close-range vision tasks, they are unreliable for long-range vision tasks such as Earth observation. To address this issue, we have developed a geocentric pose encoding and trained a deep network to compute it using publicly available airborne lidar data. This approach can rectify oblique images, remove object parallax, and improve localization accuracy, enabling precise alignment of multiple images from different viewpoints. Our method has been applied to extend two large-scale public datasets for semantic segmentation in oblique satellite images, and all data and code are available to the public.",1
"Occlusion is an inevitable and critical problem in unsupervised optical flow learning. Existing methods either treat occlusions equally as non-occluded regions or simply remove them to avoid incorrectness. However, the occlusion regions can provide effective information for optical flow learning. In this paper, we present OccInpFlow, an occlusion-inpainting framework to make full use of occlusion regions. Specifically, a new appearance-flow network is proposed to inpaint occluded flows based on the image content. Moreover, a boundary warp is proposed to deal with occlusions caused by displacement beyond image border. We conduct experiments on multiple leading flow benchmark data sets such as Flying Chairs, KITTI and MPI-Sintel, which demonstrate that the performance is significantly improved by our proposed occlusion handling framework.",0
"Unsupervised optical flow learning is plagued by the inevitable and vital problem of occlusion. Current methods either treat occlusions the same as non-occluded regions or eliminate them to avoid errors. Nonetheless, occlusion regions contain valuable information for optical flow learning. This article introduces OccInpFlow, an occlusion-inpainting framework that maximizes the use of occlusion regions. The framework proposes a new appearance-flow network to complete occluded flows based on image content and a boundary warp to address occlusions caused by displacement beyond the image border. Experiments on several renowned flow benchmark data sets including Flying Chairs, KITTI, and MPI-Sintel demonstrate significant improvement in performance due to our proposed occlusion handling framework.",1
"Special cameras that provide useful features for face anti-spoofing are desirable, but not always an option. In this work we propose a method to utilize the difference in dynamic appearance between bona fide and spoof samples by creating artificial modalities from RGB videos. We introduce two types of artificial transforms: rank pooling and optical flow, combined in end-to-end pipeline for spoof detection. We demonstrate that using intermediate representations that contain less identity and fine-grained features increase model robustness to unseen attacks as well as to unseen ethnicities. The proposed method achieves state-of-the-art on the largest cross-ethnicity face anti-spoofing dataset CASIA-SURF CeFA (RGB).",0
"While special cameras that have useful features for face anti-spoofing may be desirable, they are not always available. This study presents an alternative method that utilizes the differences in the dynamic appearance of bona fide and spoof samples. The proposed method involves creating artificial modalities from RGB videos using two types of artificial transforms: rank pooling and optical flow. These transforms are combined in an end-to-end pipeline for more effective spoof detection. The study shows that using intermediate representations that contain fewer identity and fine-grained features can increase the model's robustness to unseen attacks and ethnicities. The proposed method achieves state-of-the-art results on the largest cross-ethnicity face anti-spoofing dataset CASIA-SURF CeFA (RGB).",1
"One of the most relevant tasks in an intelligent vehicle navigation system is the detection of obstacles. It is important that a visual perception system for navigation purposes identifies obstacles, and it is also important that this system can extract essential information that may influence the vehicle's behavior, whether it will be generating an alert for a human driver or guide an autonomous vehicle in order to be able to make its driving decisions. In this paper we present an approach for the identification of obstacles and extraction of class, position, depth and motion information from these objects that employs data gained exclusively from passive vision. We performed our experiments on two different data-sets and the results obtained shown a good efficacy from the use of depth and motion patterns to assess the obstacles' potential threat status.",0
"Detecting obstacles is a crucial task for an intelligent vehicle navigation system. A visual perception system must identify obstacles and extract critical information that can affect the vehicle's behavior, whether it is alerting a human driver or guiding an autonomous vehicle to make driving decisions. This paper proposes an approach that leverages passive vision data to identify obstacles and extract information on their class, position, depth, and motion. We conducted experiments on two data-sets, and our results demonstrate that using depth and motion patterns effectively assesses the potential threat status of obstacles.",1
"Robust and accurate six degree-of-freedom tracking on portable devices remains a challenging problem, especially on small hand-held devices such as smartphones. For improved robustness and accuracy, complementary movement information from an IMU and a camera is often fused. Conventional visual-inertial methods fuse information from IMUs with a sparse cloud of feature points tracked by the device camera. We consider a visually dense approach, where the IMU data is fused with the dense optical flow field estimated from the camera data. Learning-based methods applied to the full image frames can leverage visual cues and global consistency of the flow field to improve the flow estimates. We show how a learning-based optical flow model can be combined with conventional inertial navigation, and how ideas from probabilistic deep learning can aid the robustness of the measurement updates. The practical applicability is demonstrated on real-world data acquired by an iPad in a challenging low-texture environment.",0
"The problem of achieving accurate six degree-of-freedom tracking on portable devices, particularly small devices like smartphones, remains challenging. To enhance accuracy and robustness, movement information from both an IMU and camera is often combined. Traditional visual-inertial approaches fuse data from IMUs with a sparse set of feature points tracked by the camera. In contrast, we propose a visually dense method that fuses IMU data with the dense optical flow field obtained from the camera. By leveraging learning-based techniques applied to the entire image frames, visual cues and global flow field consistency can improve the flow estimates. We demonstrate how a learning-based optical flow model can be combined with conventional inertial navigation, and how probabilistic deep learning can enhance measurement update robustness. Our approach is tested in a low-texture environment using real-world data acquired by an iPad, showing promising practical applicability.",1
"Visual Odometry (VO) accumulates a positional drift in long-term robot navigation tasks. Although Convolutional Neural Networks (CNNs) improve VO in various aspects, VO still suffers from moving obstacles, discontinuous observation of features, and poor textures or visual information. While recent approaches estimate a 6DoF pose either directly from (a series of) images or by merging depth maps with optical flow (OF), research that combines absolute pose regression with OF is limited. We propose ViPR, a novel modular architecture for long-term 6DoF VO that leverages temporal information and synergies between absolute pose estimates (from PoseNet-like modules) and relative pose estimates (from FlowNet-based modules) by combining both through recurrent layers. Experiments on known datasets and on our own Industry dataset show that our modular design outperforms state of the art in long-term navigation tasks.",0
"Long-term robot navigation tasks are impacted by positional drift in Visual Odometry (VO), despite the enhancements made by Convolutional Neural Networks (CNNs). However, VO still struggles with challenges such as obstacles in motion, inconsistent observation of features, and limited visual information. Although some recent techniques estimate a 6DoF pose using images or by merging depth maps with optical flow (OF), there is limited research that combines absolute pose regression with OF. Our proposed solution, ViPR, is a novel modular architecture that incorporates both absolute pose estimates (from PoseNet-like modules) and relative pose estimates (from FlowNet-based modules) through recurrent layers, taking advantage of temporal information and synergies between the two. Results from experiments on well-known datasets and our own Industry dataset demonstrate that our modular design is superior to state-of-the-art methods in long-term navigation tasks.",1
"Significant progress has been made for estimating optical flow using deep neural networks. Advanced deep models achieve accurate flow estimation often with a considerable computation complexity and time-consuming training processes. In this work, we present a lightweight yet effective model for real-time optical flow estimation, termed FDFlowNet (fast deep flownet). We achieve better or similar accuracy on the challenging KITTI and Sintel benchmarks while being about 2 times faster than PWC-Net. This is achieved by a carefully-designed structure and newly proposed components. We first introduce an U-shape network for constructing multi-scale feature which benefits upper levels with global receptive field compared with pyramid network. In each scale, a partial fully connected structure with dilated convolution is proposed for flow estimation that obtains a good balance among speed, accuracy and number of parameters compared with sequential connected and dense connected structures. Experiments demonstrate that our model achieves state-of-the-art performance while being fast and lightweight.",0
"Deep neural networks have made significant strides in estimating optical flow, although the advanced models can be computationally complex and require time-consuming training processes. Our work introduces a lightweight and effective model for real-time optical flow estimation called FDFlowNet (fast deep flownet). We achieve comparable or better accuracy than PWC-Net on challenging benchmarks such as KITTI and Sintel, while being twice as fast. This is accomplished through a well-designed structure and newly proposed components. We use a U-shape network for constructing multi-scale features, which benefits upper levels with a global receptive field over a pyramid network. For flow estimation, we propose a partial fully connected structure with dilated convolution in each scale, which balances speed, accuracy, and number of parameters compared to sequential and dense connected structures. Our model achieves state-of-the-art performance while remaining fast and lightweight, as demonstrated through experiments.",1
"Dense pixel matching is required for many computer vision algorithms such as disparity, optical flow or scene flow estimation. Feature Pyramid Networks (FPN) have proven to be a suitable feature extractor for CNN-based dense matching tasks. FPN generates well localized and semantically strong features at multiple scales. However, the generic FPN is not utilizing its full potential, due to its reasonable but limited localization accuracy. Thus, we present ResFPN -- a multi-resolution feature pyramid network with multiple residual skip connections, where at any scale, we leverage the information from higher resolution maps for stronger and better localized features. In our ablation study, we demonstrate the effectiveness of our novel architecture with clearly higher accuracy than FPN. In addition, we verify the superior accuracy of ResFPN in many different pixel matching applications on established datasets like KITTI, Sintel, and FlyingThings3D.",0
"Many computer vision algorithms require dense pixel matching, including disparity, optical flow, and scene flow estimation. Feature Pyramid Networks (FPN) have been successful in extracting suitable features for CNN-based dense matching tasks. However, FPN has limited localization accuracy, which limits its potential. To address this, we present ResFPN, a multi-resolution feature pyramid network with multiple residual skip connections that leverages information from higher resolution maps for stronger and better localized features. Our ablation study shows that ResFPN has higher accuracy than FPN, and we demonstrate its superior accuracy in many different pixel matching applications on established datasets such as KITTI, Sintel, and FlyingThings3D.",1
"Recently, 3D convolutional networks (3D ConvNets) yield good performance in action recognition. However, optical flow stream is still needed to ensure better performance, the cost of which is very high. In this paper, we propose a fast but effective way to extract motion features from videos utilizing residual frames as the input data in 3D ConvNets. By replacing traditional stacked RGB frames with residual ones, 35.6% and 26.6% points improvements over top-1 accuracy can be obtained on the UCF101 and HMDB51 datasets when ResNet-18 models are trained from scratch. And we achieved the state-of-the-art results in this training mode. Analysis shows that better motion features can be extracted using residual frames compared to RGB counterpart. By combining with a simple appearance path, our proposal can be even better than some methods using optical flow streams.",0
"Action recognition has seen significant advancements with the use of 3D convolutional networks (3D ConvNets). However, to achieve optimal performance, optical flow stream is still required, which is costly. We present a novel approach to extract motion features from videos using residual frames as input data in 3D ConvNets. By replacing stacked RGB frames with residual frames, we observed a noteworthy improvement of 35.6% and 26.6% in top-1 accuracy on UCF101 and HMDB51 datasets, respectively, when ResNet-18 models are trained from scratch. Our approach outperforms existing methods, demonstrating that residual frames are better at extracting motion features than their RGB counterparts. Combining our approach with a simple appearance path can surpass some methods using optical flow streams.",1
"Pseudo-LiDAR point cloud interpolation is a novel and challenging task in the field of autonomous driving, which aims to address the frequency mismatching problem between camera and LiDAR. Previous works represent the 3D spatial motion relationship induced by a coarse 2D optical flow, and the quality of interpolated point clouds only depends on the supervision of depth maps. As a result, the generated point clouds suffer from inferior global distributions and local appearances. To solve the above problems, we propose a Pseudo-LiDAR point cloud interpolation network to generates temporally and spatially high-quality point cloud sequences. By exploiting the scene flow between point clouds, the proposed network is able to learn a more accurate representation of the 3D spatial motion relationship. For the more comprehensive perception of the distribution of point cloud, we design a novel reconstruction loss function that implements the chamfer distance to supervise the generation of Pseudo-LiDAR point clouds in 3D space. In addition, we introduce a multi-modal deep aggregation module to facilitate the efficient fusion of texture and depth features. As the benefits of the improved motion representation, training loss function, and model structure, our approach gains significant improvements on the Pseudo-LiDAR point cloud interpolation task. The experimental results evaluated on KITTI dataset demonstrate the state-of-the-art performance of the proposed network, quantitatively and qualitatively.",0
"The task of Pseudo-LiDAR point cloud interpolation is a new and difficult challenge in the realm of self-driving technology. Its main goal is to overcome the problem of camera and LiDAR frequency mismatch. Previous methods have used a 2D optical flow to represent the 3D spatial motion relationship, resulting in low-quality point clouds with uneven global distribution and local appearance. To address these issues, we present a Pseudo-LiDAR point cloud interpolation network that generates high-quality point cloud sequences with accurate 3D spatial motion representation. To achieve this, we use a scene flow between point clouds and a novel reconstruction loss function that implements the chamfer distance to supervise the generation of Pseudo-LiDAR point clouds in 3D space. Moreover, we introduce a multi-modal deep aggregation module to efficiently combine texture and depth features. Our approach significantly improves the Pseudo-LiDAR point cloud interpolation task, as demonstrated by state-of-the-art results on the KITTI dataset.",1
"Visual attention serves as a means of feature selection mechanism in the perceptual system. Motivated by Broadbent's leaky filter model of selective attention, we evaluate how such mechanism could be implemented and affect the learning process of deep reinforcement learning. We visualize and analyze the feature maps of DQN on a toy problem Catch, and propose an approach to combine visual selective attention with deep reinforcement learning. We experiment with optical flow-based attention and A2C on Atari games. Experiment results show that visual selective attention could lead to improvements in terms of sample efficiency on tested games. An intriguing relation between attention and batch normalization is also discovered.",0
"The perceptual system uses visual attention as a way to select features. Based on Broadbent's leaky filter model of selective attention, we assess how this mechanism can impact the learning process of deep reinforcement learning. Our study involves analyzing the feature maps of DQN on a simple problem called Catch, and proposing a method to integrate visual selective attention with deep reinforcement learning. We conduct experiments using optical flow-based attention and A2C on Atari games, and observe that visual selective attention can enhance sample efficiency on tested games. Moreover, we uncover a fascinating correlation between attention and batch normalization.",1
"Self-supervised learning allows for better utilization of unlabelled data. The feature representation obtained by self-supervision can be used in downstream tasks such as classification, object detection, segmentation, and anomaly detection. While classification, object detection, and segmentation have been investigated with self-supervised learning, anomaly detection needs more attention. We consider the problem of anomaly detection in images and videos, and present a new visual anomaly detection technique for videos. Numerous seminal and state-of-the-art self-supervised methods are evaluated for anomaly detection on a variety of image datasets. The best performing image-based self-supervised representation learning method is then used for video anomaly detection to see the importance of spatial features in visual anomaly detection in videos. We also propose a simple self-supervision approach for learning temporal coherence across video frames without the use of any optical flow information. At its core, our method identifies the frame indices of a jumbled video sequence allowing it to learn the spatiotemporal features of the video. This intuitive approach shows superior performance of visual anomaly detection compared to numerous methods for images and videos on UCF101 and ILSVRC2015 video datasets.",0
"Utilizing unlabelled data is made more effective through self-supervised learning, which can generate feature representations applicable in downstream tasks like classification, anomaly detection, segmentation, and object detection. While self-supervised learning has been examined for classification, segmentation, and object detection, there is a need to focus more on anomaly detection. To address this, we propose a new visual anomaly detection technique for videos by evaluating various self-supervised methods on different image datasets. With the best performing image-based self-supervised representation learning method, we assess the significance of spatial features in visual anomaly detection in videos. We also introduce a simple self-supervision strategy for learning temporal coherence across video frames without relying on optical flow information. Our approach identifies frame indices in a scrambled video sequence, enabling it to learn the spatiotemporal features of the video. Results show that our method outperforms other image and video methods on UCF101 and ILSVRC2015 video datasets in terms of visual anomaly detection.",1
"Recently, several studies proposed methods to utilize some classes of optimization problems in designing deep neural networks to encode constraints that conventional layers cannot capture. However, these methods are still in their infancy and require special treatments, such as analyzing the KKT condition, for deriving the backpropagation formula. In this paper, we propose a new layer formulation called the fixed-point iteration (FPI) layer that facilitates the use of more complicated operations in deep networks. The backward FPI layer is also proposed for backpropagation, which is motivated by the recurrent back-propagation (RBP) algorithm. But in contrast to RBP, the backward FPI layer yields the gradient by a small network module without an explicit calculation of the Jacobian. In actual applications, both the forward and backward FPI layers can be treated as nodes in the computational graphs. All components in the proposed method are implemented at a high level of abstraction, which allows efficient higher-order differentiations on the nodes. In addition, we present two practical methods of the FPI layer, FPI_NN and FPI_GD, where the update operations of FPI are a small neural network module and a single gradient descent step based on a learnable cost function, respectively. FPI\_NN is intuitive, simple, and fast to train, while FPI_GD can be used for efficient training of energy networks that have been recently studied. While RBP and its related studies have not been applied to practical examples, our experiments show the FPI layer can be successfully applied to real-world problems such as image denoising, optical flow, and multi-label classification.",0
"Recently, several studies have proposed using optimization problems to design deep neural networks that can encode constraints that traditional layers are unable to capture. Although these methods are still in their early stages and require special treatment, such as analyzing the KKT condition, for deriving the backpropagation formula. This paper introduces a new layer formulation called the fixed-point iteration (FPI) layer, which allows for the use of more complex operations in deep networks. The backward FPI layer is also proposed for backpropagation, which is motivated by the recurrent back-propagation (RBP) algorithm. However, unlike RBP, the backward FPI layer produces the gradient by a small network module without explicitly calculating the Jacobian. Both the forward and backward FPI layers can be treated as nodes in computational graphs in practical applications. The proposed method's components are implemented at a high level of abstraction, allowing for efficient higher-order differentiations on the nodes. Additionally, the paper presents two practical methods of the FPI layer, FPI_NN and FPI_GD, which use a small neural network module and a single gradient descent step based on a learnable cost function, respectively, for update operations of FPI. FPI_NN is intuitive, simple, and quick to train, while FPI_GD can be used to efficiently train energy networks that have recently been studied. While RBP and related studies have not yet been applied to practical examples, the experiments show that the FPI layer can be successfully applied to real-world problems such as image denoising, optical flow, and multi-label classification.",1
"Humans are very good at directing their visual attention toward relevant areas when they search for different types of objects. For instance, when we search for cars, we will look at the streets, not at the top of buildings. The motivation of this paper is to train a network to do the same via a multi-task learning approach. To train visual attention, we produce foreground/background segmentation labels in a semi-supervised way, using background subtraction or optical flow. Using these labels, we train an object detection model to produce foreground/background segmentation maps as well as bounding boxes while sharing most model parameters. We use those segmentation maps inside the network as a self-attention mechanism to weight the feature map used to produce the bounding boxes, decreasing the signal of non-relevant areas. We show that by using this method, we obtain a significant mAP improvement on two traffic surveillance datasets, with state-of-the-art results on both UA-DETRAC and UAVDT.",0
"The ability of humans to focus their visual attention on relevant areas when searching for objects is noteworthy. For instance, when searching for cars, people tend to look at the streets rather than the tops of buildings. The purpose of this research is to train a network to do the same using a multi-task learning approach. Our method involves producing foreground/background segmentation labels through a semi-supervised process using background subtraction or optical flow. We then train an object detection model to generate both foreground/background segmentation maps and bounding boxes, sharing most model parameters. We utilize these segmentation maps as a self-attention mechanism within the network, weighting the feature map used to generate bounding boxes and reducing the impact of non-relevant areas. Our results demonstrate a significant improvement in mAP on two traffic surveillance datasets, achieving state-of-the-art performance on both UA-DETRAC and UAVDT.",1
"Fall detection in specialized homes for the elderly is challenging. Vision-based fall detection solutions have a significant advantage over sensor-based ones as they do not instrument the resident who can suffer from mental diseases. This work is part of a project intended to deploy fall detection solutions in nursing homes. The proposed solution, based on Deep Learning, is built on a Convolutional Neural Network (CNN) trained to maximize a sensitivity-based metric. This work presents the requirements from the medical side and how it impacts the tuning of a CNN. Results highlight the importance of the temporal aspect of a fall. Therefore, a custom metric adapted to this use case and an implementation of a decision-making process are proposed in order to best meet the medical teams requirements. Clinical relevance This work presents a fall detection solution enabled to detect 86.2% of falls while producing only 11.6% of false alarms in average on the considered databases.",0
"Detecting falls in specialized homes for the elderly poses a challenge, particularly with regards to sensor-based solutions that require residents to be fitted with instruments, which may prove difficult for those with mental health issues. As part of a project aimed at deploying fall detection solutions in nursing homes, this study proposes a Deep Learning-based approach utilizing a Convolutional Neural Network (CNN) trained to optimize a sensitivity-based metric. The study outlines medical requirements and their impact on CNN tuning, highlighting the temporal aspect of falls as a crucial factor. To meet the needs of medical teams, the study presents a custom metric and decision-making process, resulting in a fall detection solution that detected 86.2% of falls while producing only 11.6% of false alarms on average across the databases considered. The clinical relevance of this approach is significant.",1
"Inter-vehicle distance and relative velocity estimations are two basic functions for any ADAS (Advanced driver-assistance systems). In this paper, we propose a monocular camera-based inter-vehicle distance and relative velocity estimation method based on end-to-end training of a deep neural network. The key novelty of our method is the integration of multiple visual clues provided by any two time-consecutive monocular frames, which include deep feature clue, scene geometry clue, as well as temporal optical flow clue. We also propose a vehicle-centric sampling mechanism to alleviate the effect of perspective distortion in the motion field (i.e. optical flow). We implement the method by a light-weight deep neural network. Extensive experiments are conducted which confirm the superior performance of our method over other state-of-the-art methods, in terms of estimation accuracy, computational speed, and memory footprint.",0
"In any Advanced driver-assistance system (ADAS), two fundamental features are inter-vehicle distance and relative velocity estimations. This article introduces a novel approach to estimate these features using a monocular camera-based method. The approach employs an end-to-end training of a deep neural network, integrating multiple visual clues from two consecutive frames. These clues include deep feature, scene geometry, and temporal optical flow. The method also proposes a vehicle-centric sampling mechanism to mitigate perspective distortion in the motion field. A lightweight deep neural network is used to implement the method, and extensive experiments confirm its superior performance over other state-of-the-art methods. The performance metrics include estimation accuracy, computational speed, and memory footprint.",1
"In this work, we demonstrate that receptive fields in 3D pose estimation can be effectively specified using optical flow. We introduce adaptive receptive fields, a simple and effective method to aid receptive field selection in pose estimation models based on optical flow inference. We contrast the performance of a benchmark state-of-the-art model running on fixed receptive fields with their adaptive field counterparts. By using a reduced receptive field, our model can process slow-motion sequences (10x longer) 23% faster than the benchmark model running at regular speed. The reduction in computational cost is achieved while producing a pose prediction accuracy to within 0.36% of the benchmark model.",0
"This study showcases the successful use of optical flow for determining receptive fields in 3D pose estimation. The introduction of adaptive receptive fields serves as a helpful and uncomplicated approach in selecting receptive fields for pose estimation models that utilize optical flow inference. To compare the performance of fixed receptive fields and adaptive fields, we utilized a state-of-the-art benchmark model. Our results show that our model, which uses reduced receptive fields, is capable of processing slow-motion sequences that are 10 times longer and is 23% faster than the benchmark model running at a regular speed. This reduction in computational costs does not compromise the accuracy of our pose estimation as our model produced pose predictions within 0.36% of the benchmark model's predictions.",1
"Learning to represent videos is a very challenging task both algorithmically and computationally. Standard video CNN architectures have been designed by directly extending architectures devised for image understanding to include the time dimension, using modules such as 3D convolutions, or by using two-stream design to capture both appearance and motion in videos. We interpret a video CNN as a collection of multi-stream convolutional blocks connected to each other, and propose the approach of automatically finding neural architectures with better connectivity and spatio-temporal interactions for video understanding. This is done by evolving a population of overly-connected architectures guided by connection weight learning. Architectures combining representations that abstract different input types (i.e., RGB and optical flow) at multiple temporal resolutions are searched for, allowing different types or sources of information to interact with each other. Our method, referred to as AssembleNet, outperforms prior approaches on public video datasets, in some cases by a great margin. We obtain 58.6% mAP on Charades and 34.27% accuracy on Moments-in-Time.",0
"Representing videos is a complex task that poses challenges in both algorithmic and computational aspects. The traditional approach of extending image-based architectures to include time dimension via 3D convolutions or two-stream design has limitations. We propose a new method called AssembleNet that automatically discovers neural architectures with better connectivity and spatio-temporal interactions for video understanding. This is achieved by evolving a population of overly-connected architectures guided by connection weight learning. AssembleNet combines representations that abstract multiple input types at various temporal resolutions, enabling different sources of information to interact with each other. Our approach outperforms prior methods on public video datasets, with 58.6% mAP on Charades and 34.27% accuracy on Moments-in-Time, demonstrating its effectiveness.",1
"Motion is a salient cue to recognize actions in video. Modern action recognition models leverage motion information either explicitly by using optical flow as input or implicitly by means of 3D convolutional filters that simultaneously capture appearance and motion information. This paper proposes an alternative approach based on a learnable correlation operator that can be used to establish frame-toframe matches over convolutional feature maps in the different layers of the network. The proposed architecture enables the fusion of this explicit temporal matching information with traditional appearance cues captured by 2D convolution. Our correlation network compares favorably with widely-used 3D CNNs for video modeling, and achieves competitive results over the prominent two-stream network while being much faster to train. We empirically demonstrate that correlation networks produce strong results on a variety of video datasets, and outperform the state of the art on four popular benchmarks for action recognition: Kinetics, Something-Something, Diving48 and Sports1M.",0
"To identify actions in videos, motion is a significant indicator. Contemporary models for action recognition utilize motion information either in an explicit manner by utilizing optical flow as input or implicitly through 3D convolutional filters that capture both appearance and motion information. This article introduces a different approach that relies on a learnable correlation operator to establish matches between frames over convolutional feature maps in different layers of the network. This architecture allows for the combination of explicit temporal matching data with traditional 2D convolutional appearance cues. Our correlation network is comparable to widely-used 3D CNNs for video modeling and produces competitive results compared to the prominent two-stream network while being much quicker to train. We have demonstrated through empirical evidence that correlation networks generate strong results across various video datasets and surpass the state of the art across four popular benchmarks for action recognition: Kinetics, Something-Something, Diving48, and Sports1M.",1
"Learning based approaches have not yet achieved their full potential in optical flow estimation, where their performance still trails heuristic approaches. In this paper, we present a CNN based patch matching approach for optical flow estimation. An important contribution of our approach is a novel thresholded loss for Siamese networks. We demonstrate that our loss performs clearly better than existing losses. It also allows to speed up training by a factor of 2 in our tests. Furthermore, we present a novel way for calculating CNN based features for different image scales, which performs better than existing methods. We also discuss new ways of evaluating the robustness of trained features for the application of patch matching for optical flow. An interesting discovery in our paper is that low-pass filtering of feature maps can increase the robustness of features created by CNNs. We proved the competitive performance of our approach by submitting it to the KITTI 2012, KITTI 2015 and MPI-Sintel evaluation portals where we obtained state-of-the-art results on all three datasets.",0
"Despite efforts, learning based methods have not reached their maximum potential for optical flow estimation, as heuristic approaches still outperform them. This paper introduces a CNN patch matching approach for optical flow estimation, with a key contribution being a new thresholded loss for Siamese networks that outperforms existing losses and speeds up training by 2x. Additionally, the paper presents an innovative method for calculating CNN based features for different image scales that surpasses current methods, as well as new techniques for evaluating the robustness of trained features for patch matching in optical flow. Interestingly, the authors discovered that low-pass filtering of feature maps can increase the robustness of CNN-created features. The approach was evaluated on KITTI 2012, KITTI 2015, and MPI-Sintel evaluation portals, where it achieved state-of-the-art results on all three datasets.",1
"Face spoofing causes severe security threats in face recognition systems. Previous anti-spoofing works focused on supervised techniques, typically with either binary or auxiliary supervision. Most of them suffer from limited robustness and generalization, especially in the cross-dataset setting. In this paper, we propose a semi-supervised adversarial learning framework for spoof face detection, which largely relaxes the supervision condition. To capture the underlying structure of live faces data in latent representation space, we propose to train the live face data only, with a convolutional Encoder-Decoder network acting as a Generator. Meanwhile, we add a second convolutional network serving as a Discriminator. The generator and discriminator are trained by competing with each other while collaborating to understand the underlying concept in the normal class(live faces). Since the spoof face detection is video based (i.e., temporal information), we intuitively take the optical flow maps converted from consecutive video frames as input. Our approach is free of the spoof faces, thus being robust and general to different types of spoof, even unknown spoof. Extensive experiments on intra- and cross-dataset tests show that our semi-supervised method achieves better or comparable results to state-of-the-art supervised techniques.",0
"The use of fake faces in face recognition systems poses a significant security risk. Previous attempts to prevent this have focused on supervised techniques, which have limitations in their robustness and generalization, particularly when used across different datasets. To address these issues, we propose a semi-supervised adversarial learning framework for detecting spoof faces. This approach requires less supervision and trains a convolutional Encoder-Decoder network to recognize live faces. A second convolutional network serves as a Discriminator, and the generator and discriminator compete and collaborate to better understand the underlying concept of live faces. To account for the temporal aspect of spoof face detection, we use optical flow maps converted from successive video frames. Our method is robust and can detect different types of spoof, including unknown ones. Experimental results demonstrate that our semi-supervised approach is comparable to or better than current supervised methods in both intra- and cross-dataset settings.",1
"Instance segmentation of unknown objects from images is regarded as relevant for several robot skills including grasping, tracking and object sorting. Recent results in computer vision have shown that large hand-labeled datasets enable high segmentation performance. To overcome the time-consuming process of manually labeling data for new environments, we present a transfer learning approach for robots that learn to segment objects by interacting with their environment in a self-supervised manner. Our robot pushes unknown objects on a table and uses information from optical flow to create training labels in the form of object masks. To achieve this, we fine-tune an existing DeepMask network for instance segmentation on the self-labeled training data acquired by the robot. We evaluate our trained network (SelfDeepMask) on a set of real images showing challenging and cluttered scenes with novel objects. Here, SelfDeepMask outperforms the DeepMask network trained on the COCO dataset by 9.5% in average precision. Furthermore, we combine our approach with recent approaches for training with noisy labels in order to better cope with induced label noise.",0
"The relevance of instance segmentation for various robot skills, such as grasping, tracking, and object sorting, is widely acknowledged, and recent advances in computer vision have demonstrated that large, hand-labeled datasets improve segmentation performance. However, the process of manually labeling data for new environments is time-consuming. To address this issue, we propose a transfer learning method for robots to learn to segment objects by interacting with their environment in a self-supervised manner. Our approach involves pushing unknown objects on a table and using optical flow information to generate object masks for training. We fine-tune an existing DeepMask network for instance segmentation using the self-labeled training data. The resulting network, SelfDeepMask, is evaluated on real images of challenging and cluttered scenes with novel objects, and it outperforms the DeepMask network trained on the COCO dataset by 9.5% in average precision. We also incorporate recent methods for training with noisy labels to improve our approach's ability to handle induced label noise.",1
"We reveal that the Analytic Signal phase, and its gradient have a hitherto unstudied discontinuity in $2-D $ and higher dimensions. The shortcoming can result in severe artifacts whereas the problem does not exist in $1-D $ signals. Direct use of Gabor phase, or its gradient, in computer vision and biometric recognition e.g., as done in influential studies \cite{fleet90,wiskott1997face}, may produce undesired results that will go unnoticed unless special images similar to ours reveal them. Instead of the Analytic Signal phase, we suggest the use of Linear Symmetry phase, relying on more than one set of Gabor filters, but with a negligible computational add-on, as a remedy. Gradient magnitudes of this phase are continuous in contrast to that of the analytic signal whereas continuity of the gradient direction of the phase is guaranteed if Linear Symmetry Tensor replaces gradient vector. The suggested phase has also a built-in automatic scale estimator, useful for robust detection of patterns by multi-scale processing. We show crucial concepts on synthesized fingerprint images, where ground truth regarding instantaneous frequency, (scale \& direction), and phase are known with favorable results. A comparison to a baseline alternative is also reported. To that end, a novel multi-scale minutia model where location, direction, and scale of minutia parameters are steerable, without the creation of uncontrollable minutia is also presented. This is a useful tool, to reduce development times of minutia detection methods with explainable behavior. A revealed consequence is that minutia directions are not determined by the linear phase alone, but also by each other and the influence must be corrected to obtain steerability and accurate ground truths. Essential conclusions are readily transferable to $N-D $, and unrelated applications, e.g. optical flow or disparity estimation in stereo.",0
"In this study, we have identified a previously unexamined discontinuity in the Analytic Signal phase and its gradient in 2-D and higher dimensions, which can lead to significant artifacts in computer vision and biometric recognition. Although this issue does not exist in 1-D signals, it can cause problems when using Gabor phase or its gradient. To avoid these issues, we propose using Linear Symmetry phase, which relies on multiple sets of Gabor filters and has a negligible computational cost. The gradient magnitudes of this phase are continuous, and the gradient direction is guaranteed to be continuous if the Linear Symmetry Tensor replaces the gradient vector. Additionally, this phase includes an automatic scale estimator, making it useful for robust pattern detection through multi-scale processing. We demonstrate the efficacy of this approach on synthesized fingerprint images, where ground truth regarding instantaneous frequency, scale, direction, and phase is known. Our approach also includes a multi-scale minutia model that enables steerable minutia detection methods with explainable behavior. We highlight that minutia directions are influenced by each other and must be corrected to obtain accurate ground truths. These findings are applicable beyond biometric recognition and computer vision, including optical flow or disparity estimation in stereo, and can be extended to N-D analysis.",1
"In this paper, we address the open research problem of surgical gesture recognition using motion cues from video data only. We adapt Optical flow ConvNets initially proposed by Simonyan et al.. While Simonyan uses both RGB frames and dense optical flow, we use only dense optical flow representations as input to emphasize the role of motion in surgical gesture recognition, and present it as a robust alternative to kinematic data. We also overcome one of the limitations of Optical flow ConvNets by initializing our model with cross modality pre-training. A large number of promising studies that address surgical gesture recognition highly rely on kinematic data which requires additional recording devices. To our knowledge, this is the first paper that addresses surgical gesture recognition using dense optical flow information only. We achieve competitive results on JIGSAWS dataset, moreover, our model achieves more robust results with less standard deviation, which suggests optical flow information can be used as an alternative to kinematic data for the recognition of surgical gestures.",0
"The objective of this research paper is to tackle the unresolved issue of recognizing surgical gestures using solely motion cues from video data. Our approach involves modifying Optical flow ConvNets proposed by Simonyan et al. by using only dense optical flow representations as input to highlight the importance of motion in surgical gesture recognition. We present this as a reliable alternative to kinematic data and address a limitation of Optical flow ConvNets by initializing our model with cross modality pre-training. Many studies on surgical gesture recognition rely heavily on kinematic data which requires additional recording equipment. To our knowledge, this is the first paper to focus on surgical gesture recognition using dense optical flow information exclusively. We obtain competitive results on JIGSAWS dataset, and our model is more dependable with less standard deviation, indicating that optical flow information can be used as a substitute for kinematic data in recognizing surgical gestures.",1
"We present a self-supervised learning framework to estimate the individual object motion and monocular depth from video. We model the object motion as a 6 degree-of-freedom rigid-body transformation. The instance segmentation mask is leveraged to introduce the information of object. Compared with methods which predict dense optical flow map to model the motion, our approach significantly reduces the number of values to be estimated. Our system eliminates the scale ambiguity of motion prediction through imposing a novel geometric constraint loss term. Experiments on KITTI driving dataset demonstrate our system is capable to capture the object motion without external annotation. Our system outperforms previous self-supervised approaches in terms of 3D scene flow prediction, and contribute to the disparity prediction in dynamic area.",0
"A framework for self-supervised learning is proposed in this study for estimating individual object motion and monocular depth from video. The object motion is modeled as a 6 degree-of-freedom rigid-body transformation and the instance segmentation mask is utilized to incorporate object information. Compared to methods that use dense optical flow maps to model motion, our approach significantly reduces the number of values to be estimated. Our system solves the scale ambiguity issue of motion prediction by enforcing a novel geometric constraint loss term. The KITTI driving dataset is used for experiments and our system is found to be capable of capturing object motion without external annotation. Our system is superior to previous self-supervised approaches in terms of 3D scene flow prediction and contributes to disparity prediction in dynamic areas.",1
"Micro-expressions are brief and subtle facial expressions that go on and off the face in a fraction of a second. This kind of facial expressions usually occurs in high stake situations and is considered to reflect a human's real intent. There has been some interest in micro-expression analysis, however, a great majority of the methods are based on classically established computer vision methods such as local binary patterns, histogram of gradients and optical flow. A novel methodology for micro-expression recognition using the Riesz pyramid, a multi-scale steerable Hilbert transform is presented. In fact, an image sequence is transformed with this tool, then the image phase variations are extracted and filtered as proxies for motion. Furthermore, the dominant orientation constancy from the Riesz transform is exploited to average the micro-expression sequence into an image pair. Based on that, the Mean Oriented Riesz Feature description is introduced. Finally the performance of our methods are tested in two spontaneous micro-expressions databases and compared to state-of-the-art methods.",0
"Micro-expressions are subtle facial expressions that occur briefly and quickly, typically in high-pressure situations, and are believed to reveal a person's true intentions. While there is interest in analyzing micro-expressions, most methods rely on traditional computer vision techniques such as local binary patterns, histogram of gradients, and optical flow. However, a new approach to micro-expression recognition is presented using the Riesz pyramid, a multi-scale steerable Hilbert transform. This method transforms an image sequence and extracts the phase variations to filter as a proxy for motion. Additionally, the Riesz transform's dominant orientation constancy is used to average the micro-expression sequence into an image pair, resulting in the introduction of the Mean Oriented Riesz Feature description. The performance of this method is tested on two spontaneous micro-expression databases and compared to state-of-the-art techniques.",1
"Understanding on-road vehicle behaviour from a temporal sequence of sensor data is gaining in popularity. In this paper, we propose a pipeline for understanding vehicle behaviour from a monocular image sequence or video. A monocular sequence along with scene semantics, optical flow and object labels are used to get spatial information about the object (vehicle) of interest and other objects (semantically contiguous set of locations) in the scene. This spatial information is encoded by a Multi-Relational Graph Convolutional Network (MR-GCN), and a temporal sequence of such encodings is fed to a recurrent network to label vehicle behaviours. The proposed framework can classify a variety of vehicle behaviours to high fidelity on datasets that are diverse and include European, Chinese and Indian on-road scenes. The framework also provides for seamless transfer of models across datasets without entailing re-annotation, retraining and even fine-tuning. We show comparative performance gain over baseline Spatio-temporal classifiers and detail a variety of ablations to showcase the efficacy of the framework.",0
"The popularity of understanding on-road vehicle behavior through a sequence of sensor data is increasing. This paper presents a method for comprehending vehicle behavior using a series of monocular images or videos. The technique involves using a monocular sequence, scene semantics, optical flow, and object labels to obtain spatial information for the object of interest and other objects in the scene. This spatial information is then encoded using a Multi-Relational Graph Convolutional Network (MR-GCN), and the resulting temporal sequence of encodings is input into a recurrent network for labeling vehicle behaviors. The proposed framework is effective in classifying various vehicle behaviors accurately using diverse datasets that include European, Chinese, and Indian on-road scenes. Moreover, the framework allows for seamless model transfer across datasets without requiring re-annotation, retraining, or even fine-tuning. Our experiments indicate improved performance over baseline Spatio-temporal classifiers and highlight the effectiveness of the proposed framework through various ablations.",1
"In this paper, we tackle the problem of egocentric action anticipation, i.e., predicting what actions the camera wearer will perform in the near future and which objects they will interact with. Specifically, we contribute Rolling-Unrolling LSTM, a learning architecture to anticipate actions from egocentric videos. The method is based on three components: 1) an architecture comprised of two LSTMs to model the sub-tasks of summarizing the past and inferring the future, 2) a Sequence Completion Pre-Training technique which encourages the LSTMs to focus on the different sub-tasks, and 3) a Modality ATTention (MATT) mechanism to efficiently fuse multi-modal predictions performed by processing RGB frames, optical flow fields and object-based features. The proposed approach is validated on EPIC-Kitchens, EGTEA Gaze+ and ActivityNet. The experiments show that the proposed architecture is state-of-the-art in the domain of egocentric videos, achieving top performances in the 2019 EPIC-Kitchens egocentric action anticipation challenge. The approach also achieves competitive performance on ActivityNet with respect to methods not based on unsupervised pre-training and generalizes to the tasks of early action recognition and action recognition. To encourage research on this challenging topic, we made our code, trained models, and pre-extracted features available at our web page: http://iplab.dmi.unict.it/rulstm.",0
"This paper addresses the challenge of predicting the actions and object interactions of camera wearers in egocentric videos, a task known as egocentric action anticipation. To achieve this, we propose a new learning architecture called Rolling-Unrolling LSTM. This method consists of three components: 1) a two-LSTM architecture to model past events and future predictions, 2) a Sequence Completion Pre-Training technique to encourage the LSTMs to focus on different sub-tasks, and 3) a Modality ATTention (MATT) mechanism to fuse multi-modal predictions from RGB frames, optical flow fields, and object-based features. Our approach is validated on three datasets and achieves top performance in the 2019 EPIC-Kitchens egocentric action anticipation challenge. Additionally, it performs well on ActivityNet and generalizes to early action recognition and action recognition tasks. To encourage further research, our code, trained models, and pre-extracted features are available on our website: http://iplab.dmi.unict.it/rulstm.",1
"Temporal feature extraction is an important issue in video-based action recognition. Optical flow is a popular method to extract temporal feature, which produces excellent performance thanks to its capacity of capturing pixel-level correlation information between consecutive frames. However, such a pixel-level correlation is extracted at the cost of high computational complexity and large storage resource. In this paper, we propose a novel temporal feature extraction method, named Attentive Correlated Temporal Feature (ACTF), by exploring inter-frame correlation within a certain region. The proposed ACTF exploits both bilinear and linear correlation between successive frames on the regional level. Our method has the advantage of achieving performance comparable to or better than optical flow-based methods while avoiding the introduction of optical flow. Experimental results demonstrate our proposed method achieves the state-of-the-art performances of 96.3% on UCF101 and 76.3% on HMDB51 benchmark datasets.",0
"The extraction of temporal features is crucial for recognizing actions in video-based content. Optical flow is a commonly used approach for temporal feature extraction, as it effectively captures correlations between adjacent frames at the pixel level. However, this method is computationally complex and requires significant storage resources. To address this issue, we introduce a new technique called Attentive Correlated Temporal Feature (ACTF), which leverages inter-frame correlations within specific regions. Our method combines both bilinear and linear correlations between frames at the regional level, achieving comparable or superior performance to optical flow-based methods without the need for optical flow. Our experiments demonstrate that ACTF achieves state-of-the-art results of 96.3% on UCF101 and 76.3% on HMDB51 benchmark datasets.",1
"Understanding ego-motion and surrounding vehicle state is essential to enable automated driving and advanced driving assistance technologies. Typical approaches to solve this problem use fusion of multiple sensors such as LiDAR, camera, and radar to recognize surrounding vehicle state, including position, velocity, and orientation. Such sensing modalities are overly complex and costly for production of personal use vehicles. In this paper, we propose a novel machine learning method to estimate ego-motion and surrounding vehicle state using a single monocular camera. Our approach is based on a combination of three deep neural networks to estimate the 3D vehicle bounding box, depth, and optical flow from a sequence of images. The main contribution of this paper is a new framework and algorithm that integrates these three networks in order to estimate the ego-motion and surrounding vehicle state. To realize more accurate 3D position estimation, we address ground plane correction in real-time. The efficacy of the proposed method is demonstrated through experimental evaluations that compare our results to ground truth data available from other sensors including Can-Bus and LiDAR.",0
"To enable automated driving and advanced driving assistance technologies, it is crucial to comprehend ego-motion and surrounding vehicle state. Typically, fusion of multiple sensors like LiDAR, camera, and radar is employed to identify surrounding vehicle state, encompassing position, velocity, and orientation. However, this approach is excessively complicated and expensive for personal use vehicle production. Hence, in this study, we introduce a new machine learning technique that uses a single monocular camera to estimate ego-motion and surrounding vehicle state. Our method involves three deep neural networks that estimate 3D vehicle bounding box, depth, and optical flow from a sequence of images. We integrate these networks to estimate the ego-motion and surrounding vehicle state and address ground plane correction in real-time to achieve more precise 3D position estimation. We demonstrate the effectiveness of our approach through experimental evaluations that compare our results with ground truth data from other sensors like Can-Bus and LiDAR.",1
"We propose to modify the common training protocols of optical flow, leading to sizable accuracy improvements without adding to the computational complexity of the training process. The improvement is based on observing the bias in sampling challenging data that exists in the current training protocol, and improving the sampling process. In addition, we find that both regularization and augmentation should decrease during the training protocol.   Using an existing low parameters architecture, the method is ranked first on the MPI Sintel benchmark among all other methods, improving the best two frames method accuracy by more than 10%. The method also surpasses all similar architecture variants by more than 12% and 19.7% on the KITTI benchmarks, achieving the lowest Average End-Point Error on KITTI2012 among two-frame methods, without using extra datasets.",0
"Our proposal aims to enhance the accuracy of optical flow training protocols by modifying them without increasing computational complexity. We have identified a bias in the sampling of challenging data in the current training protocol, which we address by improving the sampling process. Furthermore, we have found that regularization and augmentation should decrease during the training protocol. By leveraging an existing low parameters architecture, our method has outperformed all other methods on the MPI Sintel benchmark, delivering an accuracy improvement of over 10% for the best two frames method. Additionally, our approach has surpassed all similar architecture variants by more than 12% and 19.7% on the KITTI benchmarks, achieving the lowest Average End-Point Error on KITTI2012 among two-frame methods without utilizing additional datasets.",1
"Egocentric activity recognition in first-person videos has an increasing importance with a variety of applications such as lifelogging, summarization, assisted-living and activity tracking. Existing methods for this task are based on interpretation of various sensor information using pre-determined weights for each feature. In this work, we propose a new framework for egocentric activity recognition problem based on combining audio-visual features with multi-kernel learning (MKL) and multi-kernel boosting (MKBoost). For that purpose, firstly grid optical-flow, virtual-inertia feature, log-covariance, cuboid are extracted from the video. The audio signal is characterized using a ""supervector"", obtained based on Gaussian mixture modelling of frame-level features, followed by a maximum a-posteriori adaptation. Then, the extracted multi-modal features are adaptively fused by MKL classifiers in which both the feature and kernel selection/weighing and recognition tasks are performed together. The proposed framework was evaluated on a number of egocentric datasets. The results showed that using multi-modal features with MKL outperforms the existing methods.",0
"The recognition of egocentric activity in first-person videos is becoming increasingly important for various applications, such as activity tracking, assisted-living, lifelogging, and summarization. Current methods for this task utilize pre-determined weights for interpreting sensor information from various sources. This study introduces a novel framework for egocentric activity recognition that combines audio-visual features with multi-kernel learning (MKL) and multi-kernel boosting (MKBoost). To achieve this, the video's grid optical-flow, virtual-inertia feature, log-covariance, and cuboid are extracted, while the audio signal is characterized using a supervector obtained through Gaussian mixture modelling of frame-level features followed by maximum a-posteriori adaptation. The extracted multi-modal features are then fused adaptively by MKL classifiers, which perform both the feature and kernel selection/weighing and recognition tasks simultaneously. The proposed framework was evaluated on various egocentric datasets, and the results indicate that using multi-modal features with MKL produces superior results compared to existing methods.",1
"Current benchmarks for optical flow algorithms evaluate the estimation either directly by comparing the predicted flow fields with the ground truth or indirectly by using the predicted flow fields for frame interpolation and then comparing the interpolated frames with the actual frames. In the latter case, objective quality measures such as the mean squared error are typically employed. However, it is well known that for image quality assessment, the actual quality experienced by the user cannot be fully deduced from such simple measures. Hence, we conducted a subjective quality assessment crowdscouring study for the interpolated frames provided by one of the optical flow benchmarks, the Middlebury benchmark. We collected forced-choice paired comparisons between interpolated images and corresponding ground truth. To increase the sensitivity of observers when judging minute difference in paired comparisons we introduced a new method to the field of full-reference quality assessment, called artefact amplification. From the crowdsourcing data, we reconstructed absolute quality scale values according to Thurstone's model. As a result, we obtained a re-ranking of the 155 participating algorithms w.r.t. the visual quality of the interpolated frames. This re-ranking not only shows the necessity of visual quality assessment as another evaluation metric for optical flow and frame interpolation benchmarks, the results also provide the ground truth for designing novel image quality assessment (IQA) methods dedicated to perceptual quality of interpolated images. As a first step, we proposed such a new full-reference method, called WAE-IQA. By weighing the local differences between an interpolated image and its ground truth WAE-IQA performed slightly better than the currently best FR-IQA approach from the literature.",0
"Optical flow algorithms are currently evaluated using two methods: direct comparison between predicted and actual flow fields, or indirect comparison by interpolating frames using predicted flow fields and comparing them with the actual frames using objective measures like mean squared error. However, such measures do not fully capture the user's actual experience of image quality. Therefore, we conducted a subjective quality assessment study using crowd-sourcing for the Middlebury benchmark's interpolated frames. We introduced a new method called artefact amplification to increase sensitivity in observers, and reconstructed absolute quality scale values according to Thurstone's model. We obtained a re-ranking of the algorithms based on visual quality of the interpolated frames, which highlights the need for visual quality assessment as another evaluation metric for optical flow and frame interpolation benchmarks. We proposed a new full-reference method called WAE-IQA, which outperformed the current best FR-IQA approach from the literature. This study provides ground truth for designing novel IQA methods dedicated to the perceptual quality of interpolated images.",1
"Modeling hand-object manipulations is essential for understanding how humans interact with their environment. While of practical importance, estimating the pose of hands and objects during interactions is challenging due to the large mutual occlusions that occur during manipulation. Recent efforts have been directed towards fully-supervised methods that require large amounts of labeled training samples. Collecting 3D ground-truth data for hand-object interactions, however, is costly, tedious, and error-prone. To overcome this challenge we present a method to leverage photometric consistency across time when annotations are only available for a sparse subset of frames in a video. Our model is trained end-to-end on color images to jointly reconstruct hands and objects in 3D by inferring their poses. Given our estimated reconstructions, we differentiably render the optical flow between pairs of adjacent images and use it within the network to warp one frame to another. We then apply a self-supervised photometric loss that relies on the visual consistency between nearby images. We achieve state-of-the-art results on 3D hand-object reconstruction benchmarks and demonstrate that our approach allows us to improve the pose estimation accuracy by leveraging information from neighboring frames in low-data regimes.",0
"Understanding how humans interact with their environment requires modeling hand-object manipulations. However, estimating the pose of hands and objects during interactions is challenging due to mutual occlusions. Fully-supervised methods have been used to address this issue, but collecting 3D ground-truth data is costly, tedious, and error-prone. To overcome this challenge, we propose leveraging photometric consistency across time when annotations are only available for a sparse subset of frames in a video. Our end-to-end model jointly reconstructs hands and objects in 3D by inferring their poses from color images. We use differentiable rendering of optical flow between adjacent images to warp one frame to another and apply a self-supervised photometric loss that relies on visual consistency between nearby images. Our approach achieves state-of-the-art results on 3D hand-object reconstruction benchmarks and improves pose estimation accuracy by leveraging information from neighboring frames in low-data regimes.",1
"Recent deep learning approaches have achieved impressive performance on visual sound separation tasks. However, these approaches are mostly built on appearance and optical flow like motion feature representations, which exhibit limited abilities to find the correlations between audio signals and visual points, especially when separating multiple instruments of the same types, such as multiple violins in a scene. To address this, we propose ""Music Gesture,"" a keypoint-based structured representation to explicitly model the body and finger movements of musicians when they perform music. We first adopt a context-aware graph network to integrate visual semantic context with body dynamics, and then apply an audio-visual fusion model to associate body movements with the corresponding audio signals. Experimental results on three music performance datasets show: 1) strong improvements upon benchmark metrics for hetero-musical separation tasks (i.e. different instruments); 2) new ability for effective homo-musical separation for piano, flute, and trumpet duets, which to our best knowledge has never been achieved with alternative methods. Project page: http://music-gesture.csail.mit.edu.",0
"Impressive results have been achieved by recent deep learning approaches in visual sound separation tasks. However, their reliance on appearance and optical flow like motion feature representations has limited their ability to identify the correlations between audio signals and visual points, particularly when separating multiple instruments of the same type. In response, we propose ""Music Gesture,"" which employs a structured representation based on keypoints to model the body and finger movements of musicians when performing music. We use a context-aware graph network to incorporate visual semantic context with body dynamics, and an audio-visual fusion model to connect body movements with corresponding audio signals. Experiments conducted on three music performance datasets demonstrate that our approach leads to significant improvements in hetero-musical separation tasks and enables effective homo-musical separation for piano, flute, and trumpet duets, which has not been achieved by other methods. Additional information can be found on our project page at http://music-gesture.csail.mit.edu.",1
"We study the energy minimization problem in low-level vision tasks from a novel perspective. We replace the heuristic regularization term with a learnable subspace constraint, and preserve the data term to exploit domain knowledge derived from the first principle of a task. This learning subspace minimization (LSM) framework unifies the network structures and the parameters for many low-level vision tasks, which allows us to train a single network for multiple tasks simultaneously with completely shared parameters, and even generalizes the trained network to an unseen task as long as its data term can be formulated. We demonstrate our LSM framework on four low-level tasks including interactive image segmentation, video segmentation, stereo matching, and optical flow, and validate the network on various datasets. The experiments show that the proposed LSM generates state-of-the-art results with smaller model size, faster training convergence, and real-time inference.",0
"A new perspective is taken on the energy minimization problem in low-level vision tasks. Instead of using a heuristic regularization term, a learnable subspace constraint is utilized while maintaining the data term to benefit from domain knowledge. This approach, called the learning subspace minimization (LSM) framework, unites network structures and parameters for various low-level vision tasks. This enables a single network to be trained for multiple tasks with shared parameters, and even generalizes to new tasks as long as the data term can be defined. The LSM framework is demonstrated on four low-level tasks and validated on different datasets. Results show that LSM generates superior outcomes with a smaller model size, faster training convergence, and real-time inference.",1
"Object tracking and 3D reconstruction are often performed together, with tracking used as input for reconstruction. However, the obtained reconstructions also provide useful information for improving tracking. We propose a novel method that closes this loop, first tracking to reconstruct, and then reconstructing to track. Our approach, MOTSFusion (Multi-Object Tracking, Segmentation and dynamic object Fusion), exploits the 3D motion extracted from dynamic object reconstructions to track objects through long periods of complete occlusion and to recover missing detections. Our approach first builds up short tracklets using 2D optical flow, and then fuses these into dynamic 3D object reconstructions. The precise 3D object motion of these reconstructions is used to merge tracklets through occlusion into long-term tracks, and to locate objects when detections are missing. On KITTI, our reconstruction-based tracking reduces the number of ID switches of the initial tracklets by more than 50%, and outperforms all previous approaches for both bounding box and segmentation tracking.",0
"Typically, object tracking and 3D reconstruction are done simultaneously, where tracking serves as input for reconstruction. However, the reconstructions obtained from this process also offer valuable insights for improving tracking. Our new method, MOTSFusion, closes this feedback loop by first using tracking to reconstruct, and then reconstructing to track. By leveraging the 3D motion extracted from dynamic object reconstructions, our approach can track objects through extended periods of total occlusion and recover missing detections. We begin by creating short tracklets using 2D optical flow and then combining them into dynamic 3D object reconstructions. We use the accurate 3D object motion from these reconstructions to merge tracklets through occlusion into long-term tracks and locate objects when detections are missing. Our reconstruction-based tracking on KITTI has reduced the number of ID switches of the initial tracklets by over 50% and outperforms all previous methods for both bounding box and segmentation tracking.",1
"Semi-supervised video object segmentation aims to separate a target object from a video sequence, given the mask in the first frame. Most of current prevailing methods utilize information from additional modules trained in other domains like optical flow and instance segmentation, and as a result they do not compete with other methods on common ground. To address this issue, we propose a simple yet strong transductive method, in which additional modules, datasets, and dedicated architectural designs are not needed. Our method takes a label propagation approach where pixel labels are passed forward based on feature similarity in an embedding space. Different from other propagation methods, ours diffuses temporal information in a holistic manner which take accounts of long-term object appearance. In addition, our method requires few additional computational overhead, and runs at a fast $\sim$37 fps speed. Our single model with a vanilla ResNet50 backbone achieves an overall score of 72.3 on the DAVIS 2017 validation set and 63.1 on the test set. This simple yet high performing and efficient method can serve as a solid baseline that facilitates future research. Code and models are available at \url{https://github.com/microsoft/transductive-vos.pytorch}.",0
"The objective of semi-supervised video object segmentation is to separate a target object from a video sequence by using the mask in the first frame. Presently, most of the methods use information from other domains such as optical flow and instance segmentation, which makes it hard to compare their performance with other techniques. To tackle this problem, we suggest a transductive method that is simple but robust and does not require additional modules, datasets, or special architectural designs. Our approach uses label propagation, where pixel labels move forward based on feature similarity in an embedding space. Unlike other propagation techniques, our method diffuses temporal information holistically, taking into account long-term object appearance. Furthermore, our method has low computational overhead, running at a fast speed of approximately 37 fps. With a vanilla ResNet50 backbone, our single model achieves an overall score of 72.3 on the DAVIS 2017 validation set and 63.1 on the test set. This efficient and high-performing method can serve as a solid baseline for future research. Our code and models are available at \url{https://github.com/microsoft/transductive-vos.pytorch}.",1
"Scene flow estimation has been receiving increasing attention for 3D environment perception. Monocular scene flow estimation -- obtaining 3D structure and 3D motion from two temporally consecutive images -- is a highly ill-posed problem, and practical solutions are lacking to date. We propose a novel monocular scene flow method that yields competitive accuracy and real-time performance. By taking an inverse problem view, we design a single convolutional neural network (CNN) that successfully estimates depth and 3D motion simultaneously from a classical optical flow cost volume. We adopt self-supervised learning with 3D loss functions and occlusion reasoning to leverage unlabeled data. We validate our design choices, including the proxy loss and augmentation setup. Our model achieves state-of-the-art accuracy among unsupervised/self-supervised learning approaches to monocular scene flow, and yields competitive results for the optical flow and monocular depth estimation sub-tasks. Semi-supervised fine-tuning further improves the accuracy and yields promising results in real-time.",0
"The estimation of scene flow has become increasingly important for understanding 3D environments. However, obtaining 3D structure and motion from two consecutive images using monocular scene flow estimation is a difficult problem that currently lacks practical solutions. In this study, we propose a novel method that achieves high accuracy and real-time performance. By approaching the problem from an inverse perspective, we developed a single convolutional neural network that can estimate depth and 3D motion simultaneously. We utilized self-supervised learning with 3D loss functions and occlusion reasoning to leverage unlabeled data. We also validated our choices, including the proxy loss and augmentation setup. Our method outperformed other unsupervised/self-supervised learning approaches for monocular scene flow and achieved competitive results for optical flow and monocular depth estimation. We further improved accuracy through semi-supervised fine-tuning and achieved promising real-time results.",1
"Correspondence estimation is one of the most widely researched and yet only partially solved area of computer vision with many applications in tracking, mapping, recognition of objects and environment. In this paper, we propose a novel way to estimate dense correspondence on an RGB image where visual descriptors are learned from video examples by training a fully convolutional network. Most deep learning methods solve this by training the network with a large set of expensive labeled data or perform labeling through strong 3D generative models using RGB-D videos. Our method learns from RGB videos using contrastive loss, where relative labeling is estimated from optical flow. We demonstrate the functionality in a quantitative analysis on rendered videos, where ground truth information is available. Not only does the method perform well on test data with the same background, it also generalizes to situations with a new background. The descriptors learned are unique and the representations determined by the network are global. We further show the applicability of the method to real-world videos.",0
"The field of computer vision has extensively researched correspondence estimation, which has various practical applications such as object recognition, mapping, and tracking. In this article, we propose a new method to estimate dense correspondence in an RGB image using a fully convolutional network to learn visual descriptors from video examples. Unlike other deep learning approaches that rely on expensive labeled data or RGB-D videos, our method uses contrastive loss and optical flow to estimate relative labeling from RGB videos. We demonstrate the effectiveness of our method through quantitative analysis on rendered videos with known ground truth, showing that it performs well in both familiar and unfamiliar backgrounds. The learned descriptors are unique and have global representations. Additionally, we validate the method's usefulness in real-world videos.",1
"In classic video action recognition, labels may not contain enough information about the diverse video appearance and dynamics, thus, existing models that are trained under the standard supervised learning paradigm may extract less generalizable features. We evaluate these models under a cross-dataset experiment setting, as the above label bias problem in video analysis is even more prominent across different data sources. We find that using the optical flows as model inputs harms the generalization ability of most video recognition models.   Based on these findings, we present a multi-task learning paradigm for video classification. Our key idea is to avoid label bias and improve the generalization ability by taking data as its own supervision or supervising constraints on the data. First, we take the optical flows and the RGB frames by taking them as auxiliary supervisions, and thus naming our model as Reversed Two-Stream Networks (Rev2Net). Further, we collaborate the auxiliary flow prediction task and the frame reconstruction task by introducing a new training objective to Rev2Net, named Decoding Discrepancy Penalty (DDP), which constraints the discrepancy of the multi-task features in a self-supervised manner. Rev2Net is shown to be effective on the classic action recognition task. It specifically shows a strong generalization ability in the cross-dataset experiments.",0
"The conventional approach to recognizing action in videos may be insufficient in providing the necessary information about the varied appearance and movements of videos. This leads to models trained through supervised learning being less effective at extracting features that are generalizable. To test these models, we conducted experiments across different datasets. We discovered that utilizing optical flows as model inputs can hinder the models' ability to generalize. To address this, we propose a multi-task learning technique for video classification that avoids label bias and improves generalization by supervising the data itself. We use the optical flows and RGB frames as auxiliary supervisions and name our model Rev2Net. We also introduce a new training objective, DDP, which collaborates the auxiliary flow prediction task and the frame reconstruction task, and constrains the discrepancy of multi-task features in a self-supervised manner. Rev2Net proves effective in action recognition and shows strong generalization ability in cross-dataset experiments.",1
"Feature warping is a core technique in optical flow estimation; however, the ambiguity caused by occluded areas during warping is a major problem that remains unsolved. In this paper, we propose an asymmetric occlusion-aware feature matching module, which can learn a rough occlusion mask that filters useless (occluded) areas immediately after feature warping without any explicit supervision. The proposed module can be easily integrated into end-to-end network architectures and enjoys performance gains while introducing negligible computational cost. The learned occlusion mask can be further fed into a subsequent network cascade with dual feature pyramids with which we achieve state-of-the-art performance. At the time of submission, our method, called MaskFlownet, surpasses all published optical flow methods on the MPI Sintel, KITTI 2012 and 2015 benchmarks. Code is available at https://github.com/microsoft/MaskFlownet.",0
"Optical flow estimation relies heavily on feature warping, but occluded areas create ambiguity that poses a significant challenge. This paper introduces an innovative solution - an occlusion-aware feature matching module that can identify and filter out useless (occluded) areas. The module can be easily integrated into end-to-end network architectures and offers substantial performance gains with minimal computational cost. The learned occlusion mask can be further used in a subsequent network cascade with dual feature pyramids, resulting in state-of-the-art performance. This approach, named MaskFlownet, currently outperforms all published optical flow methods on the MPI Sintel, KITTI 2012 and 2015 benchmarks. The code is available at https://github.com/microsoft/MaskFlownet.",1
"Akin to many subareas of computer vision, the recent advances in deep learning have also significantly influenced the literature on optical flow. Previously, the literature had been dominated by classical energy-based models, which formulate optical flow estimation as an energy minimization problem. However, as the practical benefits of Convolutional Neural Networks (CNNs) over conventional methods have become apparent in numerous areas of computer vision and beyond, they have also seen increased adoption in the context of motion estimation to the point where the current state of the art in terms of accuracy is set by CNN approaches. We first review this transition as well as the developments from early work to the current state of CNNs for optical flow estimation. Alongside, we discuss some of their technical details and compare them to recapitulate which technical contribution led to the most significant accuracy improvements. Then we provide an overview of the various optical flow approaches introduced in the deep learning age, including those based on alternative learning paradigms (e.g., unsupervised and semi-supervised methods) as well as the extension to the multi-frame case, which is able to yield further accuracy improvements.",0
"The recent advancements in deep learning have greatly impacted the literature on optical flow, much like other subareas of computer vision. Previously, classical energy-based models dominated the literature on optical flow estimation, which posed the problem as an energy minimization task. However, with the practical benefits of Convolutional Neural Networks (CNNs) becoming evident in various areas of computer vision, they have also garnered increased adoption in the field of motion estimation, surpassing conventional methods and setting the current standard for accuracy. In this article, we examine the evolution of optical flow estimation from early work to CNN approaches, highlighting their technical details and comparing them to identify the most significant accuracy improvements. We also provide an overview of the deep learning-based optical flow approaches, including those based on alternative learning paradigms (such as unsupervised and semi-supervised methods) and the extension to the multi-frame case, which further enhances accuracy.",1
"We present a simple and effective deep convolutional neural network (CNN) model for video deblurring. The proposed algorithm mainly consists of optical flow estimation from intermediate latent frames and latent frame restoration steps. It first develops a deep CNN model to estimate optical flow from intermediate latent frames and then restores the latent frames based on the estimated optical flow. To better explore the temporal information from videos, we develop a temporal sharpness prior to constrain the deep CNN model to help the latent frame restoration. We develop an effective cascaded training approach and jointly train the proposed CNN model in an end-to-end manner. We show that exploring the domain knowledge of video deblurring is able to make the deep CNN model more compact and efficient. Extensive experimental results show that the proposed algorithm performs favorably against state-of-the-art methods on the benchmark datasets as well as real-world videos.",0
"Our study introduces a deep convolutional neural network (CNN) model for video deblurring that is both simple and effective. The algorithm primarily involves optical flow estimation from intermediate latent frames and restoration of the latent frames. To achieve this, we first establish a deep CNN model that estimates the optical flow from intermediate latent frames, followed by restoring the latent frames based on the estimated optical flow. To enhance the exploration of temporal information from videos, we create a temporal sharpness prior that constrains the deep CNN model to assist in the restoration of the latent frame. We also develop a cascaded training approach to jointly train the CNN model in an end-to-end manner. Our results demonstrate that incorporating domain knowledge of video deblurring enhances the efficiency and compactness of the deep CNN model. Extensive experiments reveal that our proposed algorithm outperforms state-of-the-art methods on both benchmark datasets and real-world videos.",1
"In this paper, we propose a unified method to jointly learn optical flow and stereo matching. Our first intuition is stereo matching can be modeled as a special case of optical flow, and we can leverage 3D geometry behind stereoscopic videos to guide the learning of these two forms of correspondences. We then enroll this knowledge into the state-of-the-art self-supervised learning framework, and train one single network to estimate both flow and stereo. Second, we unveil the bottlenecks in prior self-supervised learning approaches, and propose to create a new set of challenging proxy tasks to boost performance. These two insights yield a single model that achieves the highest accuracy among all existing unsupervised flow and stereo methods on KITTI 2012 and 2015 benchmarks. More remarkably, our self-supervised method even outperforms several state-of-the-art fully supervised methods, including PWC-Net and FlowNet2 on KITTI 2012.",0
"The aim of this paper is to present a method that can simultaneously learn optical flow and stereo matching. Our initial idea is that stereo matching can be seen as a specific example of optical flow. By utilizing the 3D geometry present in stereoscopic videos, we can guide the learning process for both forms of correspondences. We integrate this knowledge into a self-supervised learning framework, training a single network to estimate both flow and stereo. Additionally, we identify shortcomings in previous self-supervised methods and propose a new set of challenging tasks to improve performance. These two insights combine to create a single model that achieves the highest accuracy among all existing unsupervised flow and stereo methods on KITTI 2012 and 2015 benchmarks. Notably, our self-supervised approach even surpasses several state-of-the-art fully supervised methods, such as PWC-Net and FlowNet2 on KITTI 2012.",1
"In dense foggy scenes, existing optical flow methods are erroneous. This is due to the degradation caused by dense fog particles that break the optical flow basic assumptions such as brightness and gradient constancy. To address the problem, we introduce a semi-supervised deep learning technique that employs real fog images without optical flow ground-truths in the training process. Our network integrates the domain transformation and optical flow networks in one framework. Initially, given a pair of synthetic fog images, its corresponding clean images and optical flow ground-truths, in one training batch we train our network in a supervised manner. Subsequently, given a pair of real fog images and a pair of clean images that are not corresponding to each other (unpaired), in the next training batch, we train our network in an unsupervised manner. We then alternate the training of synthetic and real data iteratively. We use real data without ground-truths, since to have ground-truths in such conditions is intractable, and also to avoid the overfitting problem of synthetic data training, where the knowledge learned on synthetic data cannot be generalized to real data testing. Together with the network architecture design, we propose a new training strategy that combines supervised synthetic-data training and unsupervised real-data training. Experimental results show that our method is effective and outperforms the state-of-the-art methods in estimating optical flow in dense foggy scenes.",0
"Optical flow methods are inaccurate in dense fog due to the degradation caused by fog particles breaking basic assumptions such as brightness and gradient constancy. To overcome this issue, we introduce a semi-supervised deep learning technique that utilizes real fog images without optical flow ground-truths for training. Our network integrates domain transformation and optical flow networks in one framework. We begin training in a supervised manner with a batch of synthetic fog images and their corresponding clean images and optical flow ground-truths. In the next training batch, we train our network in an unsupervised manner with unpaired real fog images and clean images. We alternate training with synthetic and real data iteratively, using real data without ground-truths to avoid overfitting and intractable ground-truth conditions. Our proposed training strategy combines supervised synthetic-data training and unsupervised real-data training. Experimental results demonstrate the effectiveness of our method, which outperforms state-of-the-art methods in estimating optical flow in dense foggy scenes.",1
"The widespread adoption of deep learning models places demands on their robustness. In this paper, we consider the robustness of deep neural networks on videos, which comprise both the spatial features of individual frames extracted by a convolutional neural network and the temporal dynamics between adjacent frames captured by a recurrent neural network. To measure robustness, we study the maximum safe radius problem, which computes the minimum distance from the optical flow sequence obtained from a given input to that of an adversarial example in the neighbourhood of the input. We demonstrate that, under the assumption of Lipschitz continuity, the problem can be approximated using finite optimisation via discretising the optical flow space, and the approximation has provable guarantees. We then show that the finite optimisation problem can be solved by utilising a two-player turn-based game in a cooperative setting, where the first player selects the optical flows and the second player determines the dimensions to be manipulated in the chosen flow. We employ an anytime approach to solve the game, in the sense of approximating the value of the game by monotonically improving its upper and lower bounds. We exploit a gradient-based search algorithm to compute the upper bounds, and the admissible A* algorithm to update the lower bounds. Finally, we evaluate our framework on the UCF101 video dataset.",0
"The robustness of deep learning models is essential for their widespread adoption. This paper examines the robustness of deep neural networks on videos, which involves both the spatial features of individual frames and the temporal dynamics between adjacent frames. The maximum safe radius problem is studied to measure the robustness, which calculates the minimum distance between the optical flow sequence of a given input and that of an adversarial example in the input's neighbourhood. The paper demonstrates that the problem can be approximated using finite optimization by discretizing the optical flow space under the assumption of Lipschitz continuity, and the approximation has provable guarantees. A cooperative two-player turn-based game is used to solve the finite optimization problem, where the first player selects the optical flows and the second player determines the dimensions to be manipulated in the chosen flow. An anytime approach is employed to solve the game, approximating the value of the game by monotonically improving its upper and lower bounds. Gradient-based search algorithms and the admissible A* algorithm are used to compute the upper and lower bounds, respectively. Finally, the framework is evaluated on the UCF101 video dataset.",1
"In this paper, we propose Two-Stream AMTnet, which leverages recent advances in video-based action representation[1] and incremental action tube generation[2]. Majority of the present action detectors follow a frame-based representation, a late-fusion followed by an offline action tube building steps. These are sub-optimal as: frame-based features barely encode the temporal relations; late-fusion restricts the network to learn robust spatiotemporal features; and finally, an offline action tube generation is not suitable for many real-world problems such as autonomous driving, human-robot interaction to name a few. The key contributions of this work are: (1) combining AMTnet's 3D proposal architecture with an online action tube generation technique which allows the model to learn stronger temporal features needed for accurate action detection and facilitates running inference online; (2) an efficient fusion technique allowing the deep network to learn strong spatiotemporal action representations. This is achieved by augmenting the previous Action Micro-Tube (AMTnet) action detection framework in three distinct ways: by adding a parallel motion stIn this paper, we propose a new deep neural network architecture for online action detection, termed ream to the original appearance one in AMTnet; (2) in opposition to state-of-the-art action detectors which train appearance and motion streams separately, and use a test time late fusion scheme to fuse RGB and flow cues, by jointly training both streams in an end-to-end fashion and merging RGB and optical flow features at training time; (3) by introducing an online action tube generation algorithm which works at video-level, and in real-time (when exploiting only appearance features). Two-Stream AMTnet exhibits superior action detection performance over state-of-the-art approaches on the standard action detection benchmarks.",0
"The article introduces Two-Stream AMTnet as a new deep neural network architecture for online action detection. This model incorporates recent advancements in video-based action representation and incremental action tube generation. Existing action detectors typically use a frame-based representation, which limits their ability to encode temporal relations. They also rely on offline action tube building, which is not practical in real-world scenarios such as autonomous driving or human-robot interaction. Two-Stream AMTnet addresses these limitations by combining 3D proposal architecture with online action tube generation and an efficient fusion technique. The network learns strong spatiotemporal action representations by joint training and merging RGB and optical flow features. Additionally, the model uses an online action tube generation algorithm that works at the video-level and in real-time. The results demonstrate that Two-Stream AMTnet outperforms state-of-the-art approaches on standard action detection benchmarks.",1
"We present a learning-based approach for removing unwanted obstructions, such as window reflections, fence occlusions or raindrops, from a short sequence of images captured by a moving camera. Our method leverages the motion differences between the background and the obstructing elements to recover both layers. Specifically, we alternate between estimating dense optical flow fields of the two layers and reconstructing each layer from the flow-warped images via a deep convolutional neural network. The learning-based layer reconstruction allows us to accommodate potential errors in the flow estimation and brittle assumptions such as brightness consistency. We show that training on synthetically generated data transfers well to real images. Our results on numerous challenging scenarios of reflection and fence removal demonstrate the effectiveness of the proposed method.",0
"A technique that employs learning has been introduced to eliminate undesired obstructions, like reflections from windows, raindrops, or occlusions by fences, in a brief series of images taken by a moving camera. Our approach utilizes the differences in movement between the background and obstructing components to restore both layers. We achieve this by alternately estimating dense optical flow fields for the two layers and reconstructing each layer from the flow-warped images using a deep convolutional neural network. The learning-based layer reconstruction enables us to account for errors in the flow estimation and fragile assumptions, such as brightness consistency. We have shown that training on artificially produced data can be applied to genuine images effectively. Our method's effectiveness is demonstrated by the results obtained from numerous challenging scenarios of reflection and fence removal.",1
"Event cameras are bio-inspired sensors that asynchronously report intensity changes in microsecond resolution. DAVIS can capture high dynamics of a scene and simultaneously output high temporal resolution events and low frame-rate intensity images. In this paper, we propose a single image (potentially blurred) and events based optical flow estimation approach. First, we demonstrate how events can be used to improve flow estimates. To this end, we encode the relation between flow and events effectively by presenting an event-based photometric consistency formulation. Then, we consider the special case of image blur caused by high dynamics in the visual environments and show that including the blur formation in our model further constrains flow estimation. This is in sharp contrast to existing works that ignore the blurred images while our formulation can naturally handle either blurred or sharp images to achieve accurate flow estimation. Finally, we reduce flow estimation, as well as image deblurring, to an alternative optimization problem of an objective function using the primal-dual algorithm. Experimental results on both synthetic and real data (with blurred and non-blurred images) show the superiority of our model in comparison to state-of-the-art approaches.",0
"The sensors inspired by biology, known as event cameras, report changes in intensity in microseconds. DAVIS, one such sensor, can capture a scene's high dynamics and output high temporal resolution events and low frame-rate intensity images. This paper presents an approach for optical flow estimation based on a single image and events, with the potential for the image to be blurred. The paper first demonstrates how events can enhance flow estimates by presenting an event-based photometric consistency formulation. The paper then considers the case of blurred images caused by high dynamics in the visual environments and shows that including the blur formation in the model further improves flow estimation. The proposed formulation can handle both blurred and sharp images, unlike existing works that only handle sharp images. Finally, the paper reduces flow estimation and image deblurring to an alternative optimization problem of an objective function using the primal-dual algorithm. Experimental results on both synthetic and real data, including blurred and non-blurred images, demonstrate the superiority of the proposed model compared to state-of-the-art approaches.",1
"Detecting and segmenting individual objects, regardless of their category, is crucial for many applications such as action detection or robotic interaction. While this problem has been well-studied under the classic formulation of spatio-temporal grouping, state-of-the-art approaches do not make use of learning-based methods. To bridge this gap, we propose a simple learning-based approach for spatio-temporal grouping. Our approach leverages motion cues from optical flow as a bottom-up signal for separating objects from each other. Motion cues are then combined with appearance cues that provide a generic objectness prior for capturing the full extent of objects. We show that our approach outperforms all prior work on the benchmark FBMS dataset. One potential worry with learning-based methods is that they might overfit to the particular type of objects that they have been trained on. To address this concern, we propose two new benchmarks for generic, moving object detection, and show that our model matches top-down methods on common categories, while significantly out-performing both top-down and bottom-up methods on never-before-seen categories.",0
"The ability to identify and isolate individual objects, regardless of their category, is crucial in various fields, including action detection and robotic interaction. While the issue has been thoroughly investigated using the traditional spatio-temporal grouping approach, current state-of-the-art methods do not utilize learning-based techniques. To address this gap, we suggest a straightforward learning-based approach for spatio-temporal grouping. Our approach employs motion cues from optical flow as a bottom-up signal to separate objects from each other. We then combine motion cues with appearance cues to capture the entire scope of objects, providing a generic objectness prior. Our approach performs better than all prior work on the FBMS dataset. One possible concern with learning-based techniques is that they may overfit to specific object types. To address this issue, we introduce two new benchmarks for detecting generic, moving objects. Our model matches top-down methods on common categories and outperforms both top-down and bottom-up methods on never-before-seen categories.",1
"Encoder-decoder networks have found widespread use in various dense prediction tasks. However, the strong reduction of spatial resolution in the encoder leads to a loss of location information as well as boundary artifacts. To address this, image-adaptive post-processing methods have shown beneficial by leveraging the high-resolution input image(s) as guidance data. We extend such approaches by considering an important orthogonal source of information: the network's confidence in its own predictions. We introduce probabilistic pixel-adaptive convolutions (PPACs), which not only depend on image guidance data for filtering, but also respect the reliability of per-pixel predictions. As such, PPACs allow for image-adaptive smoothing and simultaneously propagating pixels of high confidence into less reliable regions, while respecting object boundaries. We demonstrate their utility in refinement networks for optical flow and semantic segmentation, where PPACs lead to a clear reduction in boundary artifacts. Moreover, our proposed refinement step is able to substantially improve the accuracy on various widely used benchmarks.",0
"Various dense prediction tasks have widely adopted encoder-decoder networks. However, the encoder's strong reduction of spatial resolution results in a loss of location information and boundary artifacts. To combat this, image-adaptive post-processing methods have been successful in utilizing high-resolution input images as guidance data. Our approach extends this by taking into account the network's confidence in its predictions. We introduce probabilistic pixel-adaptive convolutions (PPACs), which utilize both image guidance data and per-pixel prediction reliability to enable image-adaptive smoothing while preserving object boundaries. PPACs are effective in refinement networks for optical flow and semantic segmentation, reducing boundary artifacts and improving accuracy on commonly used benchmarks.",1
"Whole understanding of the surroundings is paramount to autonomous systems. Recent works have shown that deep neural networks can learn geometry (depth) and motion (optical flow) from a monocular video without any explicit supervision from ground truth annotations, particularly hard to source for these two tasks. In this paper, we take an additional step toward holistic scene understanding with monocular cameras by learning depth and motion alongside with semantics, with supervision for the latter provided by a pre-trained network distilling proxy ground truth images. We address the three tasks jointly by a) a novel training protocol based on knowledge distillation and self-supervision and b) a compact network architecture which enables efficient scene understanding on both power hungry GPUs and low-power embedded platforms. We thoroughly assess the performance of our framework and show that it yields state-of-the-art results for monocular depth estimation, optical flow and motion segmentation.",0
"The comprehensive comprehension of the environment is crucial for independent systems. Recent research has revealed that deep neural networks can acquire knowledge of geometric properties (depth) and motion (optical flow) from a single video without any explicit instruction from ground truth annotations, which are particularly difficult to obtain for these two tasks. In this article, we have taken a further step towards a holistic understanding of the scene captured by monocular cameras by teaching the network depth, motion, and semantics all at once, with supervision for the latter provided by a pre-trained network distilling proxy ground truth images. We have accomplished this through a) a new training method based on knowledge distillation and self-supervision, and b) a compact network architecture that allows for efficient scene comprehension on both power-hungry GPUs and low-power embedded platforms. We have thoroughly evaluated the performance of our framework and demonstrated that it produces the best outcomes for monocular depth estimation, optical flow, and motion segmentation.",1
"In this work we contribute a novel pipeline to automatically generate training data, and to improve over state-of-the-art multi-object tracking and segmentation (MOTS) methods. Our proposed track mining algorithm turns raw street-level videos into high-fidelity MOTS training data, is scalable and overcomes the need of expensive and time-consuming manual annotation approaches. We leverage state-of-the-art instance segmentation results in combination with optical flow predictions, also trained on automatically harvested training data. Our second major contribution is MOTSNet - a deep learning, tracking-by-detection architecture for MOTS - deploying a novel mask-pooling layer for improved object association over time. Training MOTSNet with our automatically extracted data leads to significantly improved sMOTSA scores on the novel KITTI MOTS dataset (+1.9%/+7.5% on cars/pedestrians), and MOTSNet improves by +4.1% over previously best methods on the MOTSChallenge dataset. Our most impressive finding is that we can improve over previous best-performing works, even in complete absence of manually annotated MOTS training data.",0
"Our work presents a new pipeline that can generate training data automatically and enhance state-of-the-art multi-object tracking and segmentation (MOTS) approaches. Our innovative track mining algorithm transforms raw street-level videos into high-quality MOTS training data that is scalable and eliminates the need for costly and time-consuming manual annotation techniques. We utilize cutting-edge instance segmentation outcomes coupled with optical flow predictions, which are also acquired from automatically obtained training data. Our second significant contribution is MOTSNet - a deep learning, tracking-by-detection framework for MOTS that employs an original mask-pooling layer to enhance object association over time. Our use of our automatically extracted data to train MOTSNet leads to a substantial increase in sMOTSA scores on the new KITTI MOTS dataset. (+1.9%/+7.5% on cars/pedestrians). Moreover, MOTSNet surpasses previous best methods on the MOTSChallenge dataset by +4.1%. The most impressive outcome of our research is that we can improve upon previous best-performing works, even in the absence of manually annotated MOTS training data.",1
"High-quality 3D reconstructions from endoscopy video play an important role in many clinical applications, including surgical navigation where they enable direct video-CT registration. While many methods exist for general multi-view 3D reconstruction, these methods often fail to deliver satisfactory performance on endoscopic video. Part of the reason is that local descriptors that establish pair-wise point correspondences, and thus drive reconstruction, struggle when confronted with the texture-scarce surface of anatomy. Learning-based dense descriptors usually have larger receptive fields enabling the encoding of global information, which can be used to disambiguate matches. In this work, we present an effective self-supervised training scheme and novel loss design for dense descriptor learning. In direct comparison to recent local and dense descriptors on an in-house sinus endoscopy dataset, we demonstrate that our proposed dense descriptor can generalize to unseen patients and scopes, thereby largely improving the performance of Structure from Motion (SfM) in terms of model density and completeness. We also evaluate our method on a public dense optical flow dataset and a small-scale SfM public dataset to further demonstrate the effectiveness and generality of our method. The source code is available at https://github.com/lppllppl920/DenseDescriptorLearning-Pytorch.",0
"The creation of high-quality 3D reconstructions through endoscopy video is crucial in various clinical applications, such as surgical navigation. However, existing methods for general multi-view 3D reconstruction are insufficient in producing satisfactory results for endoscopic video. This is mainly due to the difficulty local descriptors face in establishing point correspondences because of the anatomy's texture-scarce surface. Therefore, our work proposes a self-supervised training scheme and loss design for dense descriptor learning, which can encode global information to disambiguate matches. Our proposed dense descriptor outperforms recent local and dense descriptors in terms of Structure from Motion (SfM) model density and completeness on an in-house sinus endoscopy dataset. Moreover, our method can generalize to unseen patients and scopes. We also evaluate the effectiveness and generality of our method on a public dense optical flow dataset and a small-scale SfM public dataset. Our source code is available at https://github.com/lppllppl920/DenseDescriptorLearning-Pytorch.",1
"Particle Imaging Velocimetry (PIV) estimates the flow of fluid by analyzing the motion of injected particles. The problem is challenging as the particles lie at different depths but have similar appearance and tracking a large number of particles is particularly difficult. In this paper, we present a PIV solution that uses densely sampled light field to reconstruct and track 3D particles. We exploit the refocusing capability and focal symmetry constraint of the light field for reliable particle depth estimation. We further propose a new motion-constrained optical flow estimation scheme by enforcing local motion rigidity and the Navier-Stoke constraint. Comprehensive experiments on synthetic and real experiments show that using a single light field camera, our technique can recover dense and accurate 3D fluid flows in small to medium volumes.",0
"The estimation of fluid flow through Particle Imaging Velocimetry (PIV) is accomplished by analyzing the movement of injected particles. However, this is a difficult task as the particles are located at various depths and have similar appearances, making tracking a large number of particles particularly challenging. This paper introduces a PIV solution that employs a densely sampled light field to reconstruct and track 3D particles. By leveraging the refocusing ability and focal symmetry limitation of the light field, we ensure reliable estimation of particle depth. We also propose a new motion-constrained optical flow estimation method by enforcing both local motion rigidity and the Navier-Stoke constraint. Comprehensive testing on both synthetic and actual data demonstrates that our technique, utilizing a single light field camera, can accurately recover dense 3D fluid flows in small and medium volumes.",1
"Deep neural networks have been successfully applied to solving the video-based person re-identification problem with impressive results reported. The existing networks for person re-id are designed to extract discriminative features that preserve the identity information. Usually, whole video frames are fed into the neural networks and all the regions in a frame are equally treated. This may be a suboptimal choice because many regions, e.g., background regions in the video, are not related to the person. Furthermore, the person of interest may be occluded by another person or something else. These unrelated regions may hinder person re-identification. In this paper, we introduce a novel gating mechanism to deep neural networks. Our gating mechanism will learn which regions are helpful for person re-identification and let these regions pass the gate. The unrelated background regions or occluding regions are filtered out by the gate. In each frame, the color channels and optical flow channels provide quite different information. To better leverage such information, we generate one gate using the color channels and another gate using the optical flow channels. These two gates are combined to provide a more reliable gate with a novel fusion method. Experimental results on two major datasets demonstrate the performance improvements due to the proposed gating mechanism.",0
"Impressive results have been achieved through the successful application of deep neural networks to solve the video-based person re-identification problem. Currently, person re-identification networks are designed to extract identity-preserving features from whole video frames, treating all regions equally. However, this approach may not be optimal as many regions, such as background regions or occluded areas, are irrelevant to the person of interest and may impede successful re-identification. To solve this issue, our paper proposes a new gating mechanism for deep neural networks. This mechanism learns which regions are useful for person re-identification and allows only those regions to pass through the gate. We create two separate gates, one for color channels and one for optical flow channels, to better leverage the unique information provided by each channel. Our experimental results demonstrate the effectiveness of our proposed gating mechanism on two major datasets.",1
"This paper presents baseline results for the Third Facial Micro-Expression Grand Challenge (MEGC 2020). Both macro- and micro-expression intervals in CAS(ME)$^2$ and SAMM Long Videos are spotted by employing the method of Main Directional Maximal Difference Analysis (MDMD). The MDMD method uses the magnitude maximal difference in the main direction of optical flow features to spot facial movements. The single-frame prediction results of the original MDMD method are post-processed into reasonable video intervals. The metric F1-scores of baseline results are evaluated: for CAS(ME)$^2$, the F1-scores are 0.1196 and 0.0082 for macro- and micro-expressions respectively, and the overall F1-score is 0.0376; for SAMM Long Videos, the F1-scores are 0.0629 and 0.0364 for macro- and micro-expressions respectively, and the overall F1-score is 0.0445. The baseline project codes are publicly available at https://github.com/HeyingGithub/Baseline-project-for-MEGC2020_spotting.",0
"In this article, the baseline outcomes for the Third Facial Micro-Expression Grand Challenge (MEGC 2020) are presented. The Main Directional Maximal Difference Analysis (MDMD) approach is employed to detect both macro- and micro-expression intervals in CAS(ME)$^2$ and SAMM Long Videos. This method identifies facial movements by utilizing the maximal difference in the magnitude of optical flow features in the main direction. The video intervals are generated by post-processing the single-frame prediction results of the original MDMD method. The baseline results are evaluated using F1-scores, which are 0.1196 and 0.0082 for macro- and micro-expressions, respectively, with an overall F1-score of 0.0376 for CAS(ME)$^2$. For SAMM Long Videos, the F1-scores are 0.0629 and 0.0364 for macro- and micro-expressions, respectively, with an overall F1-score of 0.0445. The baseline project codes are available to the public at https://github.com/HeyingGithub/Baseline-project-for-MEGC2020_spotting.",1
"Hand hygiene is one of the most significant factors in preventing hospital acquired infections (HAI) which often be transmitted by medical staffs in contact with patients in the operating room (OR). Hand hygiene monitoring could be important to investigate and reduce the outbreak of infections within the OR. However, an effective monitoring tool for hand hygiene compliance is difficult to develop due to the visual complexity of the OR scene. Recent progress in video understanding with convolutional neural net (CNN) has increased the application of recognition and detection of human actions. Leveraging this progress, we proposed a fully automated hand hygiene monitoring tool of the alcohol-based hand rubbing action of anesthesiologists on OR video using spatio-temporal features with 3D CNN. First, the region of interest (ROI) of anesthesiologists' upper body were detected and cropped. A temporal smoothing filter was applied to the ROIs. Then, the ROIs were given to a 3D CNN and classified into two classes: rubbing hands or other actions. We observed that a transfer learning from Kinetics-400 is beneficial and the optical flow stream was not helpful in our dataset. The final accuracy, precision, recall and F1 score in testing is 0.76, 0.85, 0.65 and 0.74, respectively.",0
"Preventing hospital acquired infections (HAI) is crucial, and one of the most effective ways to achieve this is through hand hygiene. Unfortunately, medical staff in the operating room (OR) may unintentionally transmit infections to patients, making it even more important to monitor hand hygiene in this setting. However, developing a monitoring tool that can effectively capture compliance with hand hygiene protocols is challenging, primarily due to the visual complexity of the OR. Fortunately, recent advancements in video understanding using convolutional neural nets (CNN) have increased the potential for recognizing and detecting human actions. Building on this progress, we have proposed a fully automated tool for monitoring the alcohol-based hand rubbing action of anesthesiologists in OR videos. We achieved this by utilizing spatio-temporal features with 3D CNN, which enabled us to detect and crop the region of interest (ROI) of the anesthesiologists' upper body. We then applied a temporal smoothing filter to the ROIs, before classifying them into two categories: rubbing hands or other actions. Our findings suggest that transfer learning from Kinetics-400 was helpful, while the optical flow stream was not beneficial for our dataset. Finally, our testing results show that our tool achieved an accuracy, precision, recall and F1 score of 0.76, 0.85, 0.65 and 0.74, respectively.",1
"Fine-grained action recognition datasets exhibit environmental bias, where multiple video sequences are captured from a limited number of environments. Training a model in one environment and deploying in another results in a drop in performance due to an unavoidable domain shift. Unsupervised Domain Adaptation (UDA) approaches have frequently utilised adversarial training between the source and target domains. However, these approaches have not explored the multi-modal nature of video within each domain. In this work we exploit the correspondence of modalities as a self-supervised alignment approach for UDA in addition to adversarial alignment.   We test our approach on three kitchens from our large-scale dataset, EPIC-Kitchens, using two modalities commonly employed for action recognition: RGB and Optical Flow. We show that multi-modal self-supervision alone improves the performance over source-only training by 2.4% on average. We then combine adversarial training with multi-modal self-supervision, showing that our approach outperforms other UDA methods by 3%.",0
"Action recognition datasets with fine-grained details tend to have environmental bias, as they are limited to a few environments where multiple video sequences are captured. This results in reduced performance when a model is trained in one environment and deployed in another due to domain shift. Adversarial training has been commonly used in Unsupervised Domain Adaptation (UDA) approaches, but they do not consider the multi-modal nature of video within each domain. This study proposes a self-supervised alignment approach for UDA that exploits the correspondence of modalities, in addition to adversarial alignment. The approach was tested on three kitchens from the EPIC-Kitchens dataset using RGB and Optical Flow modalities for action recognition. Results showed that multi-modal self-supervision alone improved performance over source-only training by an average of 2.4%. Combining adversarial training with multi-modal self-supervision resulted in an approach that outperformed other UDA methods by 3%.",1
"Over four decades, the majority addresses the problem of optical flow estimation using variational methods. With the advance of machine learning, some recent works have attempted to address the problem using convolutional neural network (CNN) and have showed promising results. FlowNet2, the state-of-the-art CNN, requires over 160M parameters to achieve accurate flow estimation. Our LiteFlowNet2 outperforms FlowNet2 on Sintel and KITTI benchmarks, while being 25.3 times smaller in the model size and 3.1 times faster in the running speed. LiteFlowNet2 is built on the foundation laid by conventional methods and resembles the corresponding roles as data fidelity and regularization in variational methods. We compute optical flow in a spatial-pyramid formulation as SPyNet but through a novel lightweight cascaded flow inference. It provides high flow estimation accuracy through early correction with seamless incorporation of descriptor matching. Flow regularization is used to ameliorate the issue of outliers and vague flow boundaries through feature-driven local convolutions. Our network also owns an effective structure for pyramidal feature extraction and embraces feature warping rather than image warping as practiced in FlowNet2 and SPyNet. Comparing to LiteFlowNet, LiteFlowNet2 improves the optical flow accuracy on Sintel Clean by 23.3%, Sintel Final by 12.8%, KITTI 2012 by 19.6%, and KITTI 2015 by 18.8%, while being 2.2 times faster. Our network protocol and trained models are made publicly available on https://github.com/twhui/LiteFlowNet2.",0
"For more than forty years, the majority of studies have focused on using variational methods to address the issue of optical flow estimation. Recently, some researchers have attempted to tackle this problem using convolutional neural network (CNN) and have demonstrated encouraging results. FlowNet2 is currently the most advanced CNN and is composed of over 160M parameters to achieve precise flow estimation. Our LiteFlowNet2 surpasses FlowNet2 in terms of performance on Sintel and KITTI benchmarks, while also being 25.3 times smaller in model size and 3.1 times faster in running speed. Our approach builds on conventional methods and plays a similar role to data fidelity and regularization in variational methods. We compute optical flow using a spatial-pyramid formulation similar to SPyNet, but with a novel lightweight cascaded flow inference approach. Our network provides high flow estimation accuracy through early correction and seamless incorporation of descriptor matching. Flow regularization is used to address the issue of outliers and vague flow boundaries through feature-driven local convolutions. Our network also possesses an effective structure for pyramidal feature extraction and embraces feature warping instead of image warping as utilized in FlowNet2 and SPyNet. Compared to LiteFlowNet, LiteFlowNet2 improves optical flow accuracy on Sintel Clean by 23.3%, Sintel Final by 12.8%, KITTI 2012 by 19.6%, and KITTI 2015 by 18.8%, while being 2.2 times faster. Our network protocol and trained models are publicly available on https://github.com/twhui/LiteFlowNet2.",1
"Pose tracking is an important problem that requires identifying unique human pose-instances and matching them temporally across different frames of a video. However, existing pose tracking methods are unable to accurately model temporal relationships and require significant computation, often computing the tracks offline. We present an efficient Multi-person Pose Tracking method, KeyTrack, that only relies on keypoint information without using any RGB or optical flow information to track human keypoints in real-time. Keypoints are tracked using our Pose Entailment method, in which, first, a pair of pose estimates is sampled from different frames in a video and tokenized. Then, a Transformer-based network makes a binary classification as to whether one pose temporally follows another. Furthermore, we improve our top-down pose estimation method with a novel, parameter-free, keypoint refinement technique that improves the keypoint estimates used during the Pose Entailment step. We achieve state-of-the-art results on the PoseTrack'17 and the PoseTrack'18 benchmarks while using only a fraction of the computation required by most other methods for computing the tracking information.",0
"Identifying unique human poses and matching them across different frames of a video is a crucial task known as pose tracking. Although existing methods for pose tracking are available, they have limited accuracy in modeling temporal relationships and require substantial computation, often computed offline. In this study, we introduce KeyTrack, an efficient Multi-person Pose Tracking method that only uses keypoint information without relying on RGB or optical flow information to track human keypoints in real-time. Our approach employs the Pose Entailment method to track keypoints, where a pair of pose estimates is tokenized from different frames in a video and subjected to binary classification by a Transformer-based network to determine if one pose follows another. We also enhance our top-down pose estimation method with a novel keypoint refinement technique that improves keypoint estimates used in the Pose Entailment step. Our approach outperforms most other methods in computing tracking information while using only a fraction of the required computation, as demonstrated by state-of-the-art results on the PoseTrack'17 and the PoseTrack'18 benchmarks.",1
"It has been proposed by many researchers that combining deep neural networks with graphical models can create more efficient and better regularized composite models. The main difficulties in implementing this in practice are associated with a discrepancy in suitable learning objectives as well as with the necessity of approximations for the inference. In this work we take one of the simplest inference methods, a truncated max-product Belief Propagation, and add what is necessary to make it a proper component of a deep learning model: We connect it to learning formulations with losses on marginals and compute the backprop operation. This BP-Layer can be used as the final or an intermediate block in convolutional neural networks (CNNs), allowing us to design a hierarchical model composing BP inference and CNNs at different scale levels. The model is applicable to a range of dense prediction problems, is well-trainable and provides parameter-efficient and robust solutions in stereo, optical flow and semantic segmentation.",0
"Many researchers suggest that deep neural networks combined with graphical models can create composite models that are more efficient and better regulated. However, implementing this idea poses challenges, such as a discrepancy in suitable learning objectives and the need for approximations in inference. In this study, we address these challenges by using a truncated max-product Belief Propagation as a learning component. By connecting it to learning formulations with losses on marginals and computing the backprop operation, we create a BP-Layer that can serve as the final or intermediate block in convolutional neural networks (CNNs). This hierarchical model is ideal for dense prediction problems and can provide parameter-efficient and robust solutions in stereo, optical flow, and semantic segmentation.",1
"Differentiable image sampling in the form of backward warping has seen broad adoption in tasks like depth estimation and optical flow prediction. In contrast, how to perform forward warping has seen less attention, partly due to additional challenges such as resolving the conflict of mapping multiple pixels to the same target location in a differentiable way. We propose softmax splatting to address this paradigm shift and show its effectiveness on the application of frame interpolation. Specifically, given two input frames, we forward-warp the frames and their feature pyramid representations based on an optical flow estimate using softmax splatting. In doing so, the softmax splatting seamlessly handles cases where multiple source pixels map to the same target location. We then use a synthesis network to predict the interpolation result from the warped representations. Our softmax splatting allows us to not only interpolate frames at an arbitrary time but also to fine tune the feature pyramid and the optical flow. We show that our synthesis approach, empowered by softmax splatting, achieves new state-of-the-art results for video frame interpolation.",0
"Backward warping, which involves differentiable image sampling, has been widely used in tasks like depth estimation and optical flow prediction. However, forward warping has received less attention, partly due to challenges in mapping multiple pixels to the same target location in a differentiable manner. To address this issue, we propose using softmax splatting and demonstrate its effectiveness in the context of frame interpolation. In our approach, we use optical flow to forward-warp two input frames and their feature pyramid representations, while ensuring that multiple source pixels are mapped to the same target location in a seamless manner. We then use a synthesis network to predict the interpolated result. Our approach is not only capable of interpolating frames at any time, but also fine-tuning the feature pyramid and optical flow. We show that our synthesis approach, powered by softmax splatting, achieves state-of-the-art results for video frame interpolation.",1
"In this paper, we proposed an unsupervised learning method for estimating the optical flow between video frames, especially to solve the occlusion problem. Occlusion is caused by the movement of an object or the movement of the camera, defined as when certain pixels are visible in one video frame but not in adjacent frames. Due to the lack of pixel correspondence between frames in the occluded area, incorrect photometric loss calculation can mislead the optical flow training process. In the video sequence, we found that the occlusion in the forward ($t\rightarrow t+1$) and backward ($t\rightarrow t-1$) frame pairs are usually complementary. That is, pixels that are occluded in subsequent frames are often not occluded in the previous frame and vice versa. Therefore, by using this complementarity, a new weighted loss is proposed to solve the occlusion problem. In addition, we calculate gradients in multiple directions to provide richer supervision information. Our method achieves competitive optical flow accuracy compared to the baseline and some supervised methods on KITTI 2012 and 2015 benchmarks. This source code has been released at https://github.com/jianfenglihg/UnOpticalFlow.git.",0
"A method for unsupervised learning was proposed in this paper to estimate optical flow between video frames with a focus on addressing the occlusion issue. This problem arises when certain pixels are not visible in adjacent frames due to object or camera movement, which leads to incorrect photometric loss calculation and misleading optical flow training process. The study discovered that occlusion in forward and backward frame pairs tends to be complementary, meaning pixels that are occluded in subsequent frames are often not occluded in the previous frame and vice versa. To tackle this, a new weighted loss was suggested, utilizing this complementarity. Moreover, gradients were calculated in multiple directions to provide more comprehensive supervision information. The method showed competitive optical flow accuracy compared to the baseline and some supervised methods on KITTI 2012 and 2015 benchmarks. The source code is available at https://github.com/jianfenglihg/UnOpticalFlow.git.",1
"Depth from a monocular video can enable billions of devices and robots with a single camera to see the world in 3D. In this paper, we present an approach with a differentiable flow-to-depth layer for video depth estimation. The model consists of a flow-to-depth layer, a camera pose refinement module, and a depth fusion network. Given optical flow and camera pose, our flow-to-depth layer generates depth proposals and the corresponding confidence maps by explicitly solving an epipolar geometry optimization problem. Our flow-to-depth layer is differentiable, and thus we can refine camera poses by maximizing the aggregated confidence in the camera pose refinement module. Our depth fusion network can utilize depth proposals and their confidence maps inferred from different adjacent frames to produce the final depth map. Furthermore, the depth fusion network can additionally take the depth proposals generated by other methods to improve the results further. The experiments on three public datasets show that our approach outperforms state-of-the-art depth estimation methods, and has reasonable cross dataset generalization capability: our model trained on KITTI still performs well on the unseen Waymo dataset.",0
"This paper proposes a method for estimating depth from monocular videos, which has the potential to allow devices and robots with only one camera to perceive the world in 3D. The proposed model consists of a flow-to-depth layer, a camera pose refinement module, and a depth fusion network. The flow-to-depth layer generates depth proposals and confidence maps by solving an epipolar geometry optimization problem. The camera pose refinement module refines the camera poses by maximizing the confidence scores, and the depth fusion network produces the final depth map by utilizing the depth proposals and their confidence maps from adjacent frames. The results of the experiments on three public datasets demonstrate that our approach outperforms the state-of-the-art depth estimation methods and has reasonable cross-dataset generalization capability. The model trained on KITTI also performs well on the unseen Waymo dataset.",1
"Applications of satellite data in areas such as weather tracking and modeling, ecosystem monitoring, wildfire detection, and land-cover change are heavily dependent on the trade-offs to spatial, spectral and temporal resolutions of observations. In weather tracking, high-frequency temporal observations are critical and used to improve forecasts, study severe events, and extract atmospheric motion, among others. However, while the current generation of geostationary satellites have hemispheric coverage at 10-15 minute intervals, higher temporal frequency observations are ideal for studying mesoscale severe weather events. In this work, we apply a task specific optical flow approach to temporal up-sampling using deep convolutional neural networks. We apply this technique to 16-bands of GOES-R/Advanced Baseline Imager mesoscale dataset to temporally enhance full disk hemispheric snapshots of different spatial resolutions from 15 minutes to 1 minute. Experiments show the effectiveness of task specific optical flow and multi-scale blocks for interpolating high-frequency severe weather events relative to bilinear and global optical flow baselines. Lastly, we demonstrate strong performance in capturing variability during a convective precipitation events.",0
"The utilization of satellite data in various fields, including weather tracking, ecosystem monitoring, wildfire detection, and land-cover change, relies heavily on the compromises made in terms of spatial, spectral, and temporal resolutions of observations. High-frequency temporal observations are crucial in weather tracking to enhance forecasting accuracy, study severe events, and extract atmospheric motion. However, geostationary satellites with hemispheric coverage at 10-15 minute intervals cannot provide higher temporal frequency observations, which are more suitable for studying mesoscale severe weather events. In this study, we applied a task-specific optical flow technique to temporally upscale 16-bands of GOES-R/Advanced Baseline Imager mesoscale dataset using deep convolutional neural networks. With this approach, we were able to enhance full disk hemispheric snapshots of different spatial resolutions from 15 minutes to 1 minute. Our experiments revealed the effectiveness of task-specific optical flow and multi-scale blocks in interpolating high-frequency severe weather events compared to bilinear and global optical flow baselines. Lastly, we demonstrated the strong performance of our approach in capturing variability during convective precipitation events.",1
"We address the problem of joint optical flow and camera motion estimation in rigid scenes by incorporating geometric constraints into an unsupervised deep learning framework. Unlike existing approaches which rely on brightness constancy and local smoothness for optical flow estimation, we exploit the global relationship between optical flow and camera motion using epipolar geometry. In particular, we formulate the prediction of optical flow and camera motion as a bi-level optimization problem, consisting of an upper-level problem to estimate the flow that conforms to the predicted camera motion, and a lower-level problem to estimate the camera motion given the predicted optical flow. We use implicit differentiation to enable back-propagation through the lower-level geometric optimization layer independent of its implementation, allowing end-to-end training of the network. With globally-enforced geometric constraints, we are able to improve the quality of the estimated optical flow in challenging scenarios and obtain better camera motion estimates compared to other unsupervised learning methods.",0
"Our objective is to tackle the issue of determining joint optical flow and camera motion in rigid scenes by integrating geometric constraints into an unsupervised deep learning framework. Unlike existing techniques that depend on brightness constancy and local smoothness for optical flow estimation, we utilize epipolar geometry to explore the global connection between optical flow and camera motion. Specifically, we formulate predicting optical flow and camera motion as a bi-level optimization problem, with an upper-level problem that estimates flow that conforms to predicted camera motion and a lower-level problem that estimates camera motion with predicted optical flow. We use implicit differentiation to allow back-propagation through the lower-level geometric optimization layer, regardless of its implementation, enabling end-to-end network training. By enforcing global geometric constraints, we enhance the quality of estimated optical flow in challenging situations and obtain superior camera motion estimates compared to other unsupervised learning methods.",1
"People identification in video based on the way they walk (i.e. gait) is a relevant task in computer vision using a non-invasive approach. Standard and current approaches typically derive gait signatures from sequences of binary energy maps of subjects extracted from images, but this process introduces a large amount of non-stationary noise, thus, conditioning their efficacy. In contrast, in this paper we focus on the raw pixels, or simple functions derived from them, letting advanced learning techniques to extract relevant features. Therefore, we present a comparative study of different Convolutional Neural Network (CNN) architectures by using three different modalities (i.e. gray pixels, optical flow channels and depth maps) on two widely-adopted and challenging datasets: TUM-GAID and CASIA-B. In addition, we perform a comparative study between different early and late fusion methods used to combine the information obtained from each kind of modalities. Our experimental results suggest that (i) the raw pixel values represent a competitive input modality, compared to the traditional state-of-the-art silhouette-based features (e.g. GEI), since equivalent or better results are obtained; (ii) the fusion of the raw pixel information with information from optical flow and depth maps allows to obtain state-of-the-art results on the gait recognition task with an image resolution several times smaller than the previously reported results; and, (iii) the selection and the design of the CNN architecture are critical points that can make a difference between state-of-the-art results or poor ones.",0
"In computer vision, identifying individuals through their gait (walking pattern) is a significant task that can be accomplished non-invasively. However, the usual approach of deriving gait signatures from binary energy maps of subjects extracted from images introduces non-stationary noise, reducing effectiveness. This paper seeks to improve the process by using raw pixels or simple functions derived from them and letting advanced learning techniques extract relevant features. The study involves a comparison of various Convolutional Neural Network (CNN) architectures using three modalities (gray pixels, optical flow channels, and depth maps) on two datasets (TUM-GAID and CASIA-B), as well as the comparison of early and late fusion methods for combining modalities. The results indicate that using raw pixel values as input produces comparable or better results than traditional silhouette-based features, and fusing raw pixel information with optical flow and depth maps leads to state-of-the-art results. The study also highlights the importance of CNN architecture design in achieving optimal results.",1
"This paper proposes a simple yet effective method for human action recognition in video. The proposed method separately extracts local appearance and motion features using state-of-the-art three-dimensional convolutional neural networks from sampled snippets of a video. These local features are then concatenated to form global representations which are then used to train a linear SVM to perform the action classification using full context of the video, as partial context as used in previous works. The videos undergo two simple proposed preprocessing techniques, optical flow scaling and crop filling. We perform an extensive evaluation on three common benchmark dataset to empirically show the benefit of the SVM, and the two preprocessing steps.",0
"In this paper, a straightforward and efficient approach is presented for recognizing human actions in videos. The approach involves extracting local appearance and motion features from selected snippets of a video using advanced three-dimensional convolutional neural networks. Subsequently, these local features are combined to create global representations that are utilized to train a linear SVM for action classification, taking into account the entire context of the video rather than just a portion as previous methods have done. Additionally, two uncomplicated preprocessing techniques, optical flow scaling and crop filling, are proposed and applied to the videos. We conduct a comprehensive evaluation on three standard benchmark datasets to demonstrate the benefits of the SVM and the two preprocessing steps.",1
"While the satellite-based Global Positioning System (GPS) is adequate for some outdoor applications, many other applications are held back by its multi-meter positioning errors and poor indoor coverage. In this paper, we study the feasibility of real-time video-based localization on resource-constrained platforms. Before commencing a localization task, a video-based localization system downloads an offline model of a restricted target environment, such as a set of city streets, or an indoor shopping mall. The system is then able to localize the user within the model, using only video as input.   To enable such a system to run on resource-constrained embedded systems or smartphones, we (a) propose techniques for efficiently building a 3D model of a surveyed path, through frame selection and efficient feature matching, (b) substantially reduce model size by multiple compression techniques, without sacrificing localization accuracy, (c) propose efficient and concurrent techniques for feature extraction and matching to enable online localization, (d) propose a method with interleaved feature matching and optical flow based tracking to reduce the feature extraction and matching time in online localization.   Based on an extensive set of both indoor and outdoor videos, manually annotated with location ground truth, we demonstrate that sub-meter accuracy, at real-time rates, is achievable on smart-phone type platforms, despite challenging video conditions.",0
"Although the Global Positioning System (GPS) that uses satellites is sufficient for some outdoor applications, it has limitations such as multi-meter positioning errors and inadequate indoor coverage, which restricts many other applications. This research explores the possibility of video-based real-time localization on platforms with limited resources. To initiate a localization task, the system downloads an offline model of a specific restricted environment, such as a city street or indoor shopping mall, before using video input to determine the user's location within the model. To make the system functional on embedded systems or smartphones, we propose techniques to efficiently construct a 3D model of a surveyed path, reduce model size without compromising localization accuracy, and suggest efficient feature extraction and matching techniques for online localization. Additionally, we recommend interleaved feature matching and tracking with optical flow to reduce feature extraction and matching time in online localization. Based on a comprehensive set of both indoor and outdoor videos manually annotated with location ground truth, we demonstrate that sub-meter accuracy can be achieved on smart-phone type platforms in real-time, even with challenging video conditions.",1
"In this work we present a monocular visual odometry (VO) algorithm which leverages geometry-based methods and deep learning. Most existing VO/SLAM systems with superior performance are based on geometry and have to be carefully designed for different application scenarios. Moreover, most monocular systems suffer from scale-drift issue.Some recent deep learning works learn VO in an end-to-end manner but the performance of these deep systems is still not comparable to geometry-based methods. In this work, we revisit the basics of VO and explore the right way for integrating deep learning with epipolar geometry and Perspective-n-Point (PnP) method. Specifically, we train two convolutional neural networks (CNNs) for estimating single-view depths and two-view optical flows as intermediate outputs. With the deep predictions, we design a simple but robust frame-to-frame VO algorithm (DF-VO) which outperforms pure deep learning-based and geometry-based methods. More importantly, our system does not suffer from the scale-drift issue being aided by a scale consistent single-view depth CNN. Extensive experiments on KITTI dataset shows the robustness of our system and a detailed ablation study shows the effect of different factors in our system.",0
"This study introduces a monocular visual odometry (VO) algorithm that combines geometry-based methods with deep learning. Current VO/SLAM systems that offer high performance rely on geometry and must be tailored for specific application scenarios. Additionally, most monocular systems experience scale-drift problems. Although recent deep learning approaches attempt to solve VO in an end-to-end manner, they still cannot match the performance of geometry-based methods. This study revisits the fundamentals of VO and explores the most effective way to integrate deep learning with epipolar geometry and the Perspective-n-Point (PnP) technique. Specifically, the researchers train two convolutional neural networks (CNNs) for estimating single-view depths and two-view optical flows as intermediate outputs. Using these deep predictions, they devise a simple but robust frame-to-frame VO algorithm (DF-VO) that surpasses pure deep learning-based and geometry-based methods. Furthermore, their system overcomes scale-drift issues with the aid of a scale-consistent single-view depth CNN. Their extensive experiments on the KITTI dataset demonstrate the robustness of their system, and their detailed ablation study reveals the impact of various factors within their system.",1
"Motion blurry images challenge many computer vision algorithms, e.g, feature detection, motion estimation, or object recognition. Deep convolutional neural networks are state-of-the-art for image deblurring. However, obtaining training data with corresponding sharp and blurry image pairs can be difficult. In this paper, we present a differentiable reblur model for self-supervised motion deblurring, which enables the network to learn from real-world blurry image sequences without relying on sharp images for supervision. Our key insight is that motion cues obtained from consecutive images yield sufficient information to inform the deblurring task. We therefore formulate deblurring as an inverse rendering problem, taking into account the physical image formation process: we first predict two deblurred images from which we estimate the corresponding optical flow. Using these predictions, we re-render the blurred images and minimize the difference with respect to the original blurry inputs. We use both synthetic and real dataset for experimental evaluations. Our experiments demonstrate that self-supervised single image deblurring is really feasible and leads to visually compelling results.",0
"Many computer vision algorithms struggle with motion blurry images, including feature detection, motion estimation, and object recognition. While deep convolutional neural networks are currently the best option for image deblurring, gathering the necessary training data with corresponding sharp and blurry image pairs can be challenging. This paper proposes a differentiable reblur model for self-supervised motion deblurring that enables the network to learn from real-world blurry image sequences without relying on sharp images for supervision. The approach takes advantage of motion cues obtained from consecutive images to inform the deblurring task, formulating it as an inverse rendering problem that considers the physical image formation process. The method predicts two deblurred images from which it estimates corresponding optical flow, and then re-renders the blurred images to minimize the difference with respect to the original blurry inputs. Both synthetic and real datasets are used for experimental evaluations, and results demonstrate that self-supervised single image deblurring is feasible and produces visually compelling outcomes.",1
"We introduce the first very large detection dataset for event cameras. The dataset is composed of more than 39 hours of automotive recordings acquired with a 304x240 ATIS sensor. It contains open roads and very diverse driving scenarios, ranging from urban, highway, suburbs and countryside scenes, as well as different weather and illumination conditions. Manual bounding box annotations of cars and pedestrians contained in the recordings are also provided at a frequency between 1 and 4Hz, yielding more than 255,000 labels in total. We believe that the availability of a labeled dataset of this size will contribute to major advances in event-based vision tasks such as object detection and classification. We also expect benefits in other tasks such as optical flow, structure from motion and tracking, where for example, the large amount of data can be leveraged by self-supervised learning methods.",0
"We are presenting the initial detection dataset for event cameras with a large scale. The dataset includes over 39 hours of automotive recordings obtained with a 304x240 ATIS sensor, which encompasses various driving scenarios like urban, highway, suburbs, and countryside scenes, along with different weather and illumination conditions. Furthermore, manual bounding box annotations of cars and pedestrians present in the recordings are given at a frequency between 1 and 4Hz, resulting in more than 255,000 labels overall. We are confident that the availability of such a labeled dataset will facilitate significant developments in event-based vision tasks such as object detection and classification. Additionally, we anticipate advantages in optical flow, structure from motion, and tracking, where the vast amount of data can be utilized by self-supervised learning methods.",1
"With the prevalence of RGB-D cameras, multi-modal video data have become more available for human action recognition. One main challenge for this task lies in how to effectively leverage their complementary information. In this work, we propose a Modality Compensation Network (MCN) to explore the relationships of different modalities, and boost the representations for human action recognition. We regard RGB/optical flow videos as source modalities, skeletons as auxiliary modality. Our goal is to extract more discriminative features from source modalities, with the help of auxiliary modality. Built on deep Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM) networks, our model bridges data from source and auxiliary modalities by a modality adaptation block to achieve adaptive representation learning, that the network learns to compensate for the loss of skeletons at test time and even at training time. We explore multiple adaptation schemes to narrow the distance between source and auxiliary modal distributions from different levels, according to the alignment of source and auxiliary data in training. In addition, skeletons are only required in the training phase. Our model is able to improve the recognition performance with source data when testing. Experimental results reveal that MCN outperforms state-of-the-art approaches on four widely-used action recognition benchmarks.",0
"The availability of multi-modal video data has increased with the prevalence of RGB-D cameras, making human action recognition more feasible. However, effectively utilizing the complementary information presented by different modalities remains a major challenge. To address this issue, we propose the Modality Compensation Network (MCN), which enhances representations for human action recognition by exploring the relationships among different modalities. Specifically, we treat RGB/optical flow videos as source modalities and skeletons as an auxiliary modality. Our objective is to extract more discriminative features from the source modalities with the aid of the auxiliary modality. To achieve adaptive representation learning, our model, which combines deep Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM) networks, uses a modality adaptation block to bridge data from the source and auxiliary modalities. The network learns to compensate for the loss of skeletons during both training and testing by exploring multiple adaptation schemes that narrow the distance between source and auxiliary modal distributions from different levels. Importantly, skeletons are only needed during training. Our model improves recognition performance when testing with source data and outperforms state-of-the-art approaches on four widely-used action recognition benchmarks.",1
"Most of Multiple Object Tracking (MOT) approaches compute individual target features for two subtasks: estimating target-wise motions and conducting pair-wise Re-Identification (Re-ID). Because of the indefinite number of targets among video frames, both subtasks are very difficult to scale up efficiently in end-to-end Deep Neural Networks (DNNs). In this paper, we design an end-to-end DNN tracking approach, Flow-Fuse-Tracker (FFT), that addresses the above issues with two efficient techniques: target flowing and target fusing. Specifically, in target flowing, a FlowTracker DNN module learns the indefinite number of target-wise motions jointly from pixel-level optical flows. In target fusing, a FuseTracker DNN module refines and fuses targets proposed by FlowTracker and frame-wise object detection, instead of trusting either of the two inaccurate sources of target proposal. Because FlowTracker can explore complex target-wise motion patterns and FuseTracker can refine and fuse targets from FlowTracker and detectors, our approach can achieve the state-of-the-art results on several MOT benchmarks. As an online MOT approach, FFT produced the top MOTA of 46.3 on the 2DMOT15, 56.5 on the MOT16, and 56.5 on the MOT17 tracking benchmarks, surpassing all the online and offline methods in existing publications.",0
"The majority of Multiple Object Tracking (MOT) methods calculate individual target characteristics for two tasks: determining target-specific movements and performing pair-wise Re-Identification (Re-ID). Since the number of targets in video frames is uncertain, both tasks present challenges for efficiently scaling up end-to-end Deep Neural Networks (DNNs). This article introduces an end-to-end DNN tracking approach, Flow-Fuse-Tracker (FFT), which addresses these problems using two effective techniques: target flowing and target fusing. Target flowing involves learning the indefinite number of target-specific movements jointly from pixel-level optical flows using a FlowTracker DNN module. Target fusing, on the other hand, refines and combines targets proposed by FlowTracker and frame-wise object detection through a FuseTracker DNN module, rather than relying on either of the two inaccurate sources of target proposal. Our approach achieves state-of-the-art results on several MOT benchmarks because FlowTracker can explore complex target-specific motion patterns and FuseTracker can refine and fuse targets from FlowTracker and detectors. As a real-time MOT approach, FFT outperforms all online and offline methods in existing publications, producing the top MOTA of 46.3 on the 2DMOT15, 56.5 on the MOT16, and 56.5 on the MOT17 tracking benchmarks.",1
"Video denoising is to remove noise from noise-corrupted data, thus recovering true signals via spatiotemporal processing. Existing approaches for spatiotemporal video denoising tend to suffer from motion blur artifacts, that is, the boundary of a moving object tends to appear blurry especially when the object undergoes a fast motion, causing optical flow calculation to break down. In this paper, we address this challenge by designing a first-image-then-video two-stage denoising neural network, consisting of an image denoising module for spatially reducing intra-frame noise followed by a regular spatiotemporal video denoising module. The intuition is simple yet powerful and effective: the first stage of image denoising effectively reduces the noise level and, therefore, allows the second stage of spatiotemporal denoising for better modeling and learning everywhere, including along the moving object boundaries. This two-stage network, when trained in an end-to-end fashion, yields the state-of-the-art performances on the video denoising benchmark Vimeo90K dataset in terms of both denoising quality and computation. It also enables an unsupervised approach that achieves comparable performance to existing supervised approaches.",0
"The goal of video denoising is to eliminate noise from data that has been corrupted by noise, thereby recovering authentic signals through spatiotemporal processing. However, current techniques for spatiotemporal video denoising often encounter issues with motion blur artifacts, whereby the border of a moving object appears fuzzy, particularly during rapid motion, which can disrupt optical flow calculations. To overcome this challenge, we propose a two-stage denoising neural network that first employs an image denoising module to decrease intra-frame noise and then utilizes a regular spatiotemporal video denoising module. This method is effective because the initial stage of image denoising reduces the noise level, allowing the second stage to model and learn more accurately, including along the boundaries of moving objects. This two-stage network, when trained end-to-end, achieves state-of-the-art performance on the Vimeo90K video denoising benchmark dataset in terms of both denoising quality and computation. Additionally, it enables an unsupervised approach that achieves performance comparable to existing supervised methods.",1
"In this paper, we study the value of using synthetically produced videos as training data for neural networks used for action categorization. Motivated by the fact that texture and background of a video play little to no significant roles in optical flow, we generated simplified texture-less and background-less videos and utilized the synthetic data to train a Temporal Segment Network (TSN). The results demonstrated that augmenting TSN with simplified synthetic data improved the original network accuracy (68.5%), achieving 71.8% on HMDB-51 when adding 4,000 videos and 72.4% when adding 8,000 videos. Also, training using simplified synthetic videos alone on 25 classes of UCF-101 achieved 30.71% when trained on 2500 videos and 52.7% when trained on 5000 videos. Finally, results showed that when reducing the number of real videos of UCF-25 to 10% and combining them with synthetic videos, the accuracy drops to only 85.41%, compared to a drop to 77.4% when no synthetic data is added.",0
"The objective of this research is to explore the effectiveness of synthetic videos as training data for neural networks utilized in action categorization. Our approach is motivated by the fact that optical flow is not significantly influenced by texture and background. To this end, we generated texture-less and background-less videos and incorporated them into the training of a Temporal Segment Network (TSN). The findings of our study indicate that the inclusion of synthetic data enhances the performance of the TSN, as evidenced by an improvement from 68.5% to 71.8% accuracy on HMDB-51 when 4,000 videos were added, and 72.4% accuracy when 8,000 videos were added. Moreover, training solely on simplified synthetic videos resulted in 30.71% accuracy on 25 classes of UCF-101 when 2,500 videos were used, and 52.7% accuracy when 5,000 videos were used. Lastly, our results demonstrate that when real videos of UCF-25 are reduced to 10% and combined with synthetic videos, the accuracy only drops to 85.41%, compared to 77.4% when no synthetic data is included.",1
"The existing approaches for salient motion segmentation are unable to explicitly learn geometric cues and often give false detections on prominent static objects. We exploit multiview geometric constraints to avoid such shortcomings. To handle the nonrigid background like a sea, we also propose a robust fusion mechanism between motion and appearance-based features. We find dense trajectories, covering every pixel in the video, and propose trajectory-based epipolar distances to distinguish between background and foreground regions. Trajectory epipolar distances are data-independent and can be readily computed given a few features' correspondences between the images. We show that by combining epipolar distances with optical flow, a powerful motion network can be learned. Enabling the network to leverage both of these features, we propose a simple mechanism, we call input-dropout. Comparing the motion-only networks, we outperform the previous state of the art on DAVIS-2016 dataset by 5.2% in the mean IoU score. By robustly fusing our motion network with an appearance network using the input-dropout mechanism, we also outperform the previous methods on DAVIS-2016, 2017 and Segtrackv2 dataset.",0
"The current methods for identifying important motion in a video are inadequate because they do not consider geometric cues and often falsely identify stationary objects. To overcome these limitations, we utilize multiview geometric constraints. We also propose a fusion mechanism that combines motion and appearance-based features to handle nonrigid backgrounds like the ocean. We use dense trajectories that cover every pixel in the video and introduce trajectory-based epipolar distances to distinguish between foreground and background regions. These distances are data-independent and can be easily computed with a few features' correspondences between the images. By combining epipolar distances with optical flow, we can create a powerful motion network. We introduce a simple mechanism called input-dropout, which enables the network to utilize both features. By comparing motion-only networks, we outperform previous state-of-the-art methods by 5.2% in the mean IoU score on the DAVIS-2016 dataset. By robustly fusing our motion network with an appearance network using the input-dropout mechanism, we also outperform previous methods on the DAVIS-2016, 2017, and Segtrackv2 datasets.",1
"For a long time, the vision community tries to learn the spatio-temporal representation by combining convolutional neural network together with various temporal models, such as the families of Markov chain, optical flow, RNN and temporal convolution. However, these pipelines consume enormous computing resources due to the alternately learning process for spatial and temporal information. One natural question is whether we can embed the temporal information into the spatial one so the information in the two domains can be jointly learned once-only. In this work, we answer this question by presenting a simple yet powerful operator -- temporal interlacing network (TIN). Instead of learning the temporal features, TIN fuses the two kinds of information by interlacing spatial representations from the past to the future, and vice versa. A differentiable interlacing target can be learned to control the interlacing process. In this way, a heavy temporal model is replaced by a simple interlacing operator. We theoretically prove that with a learnable interlacing target, TIN performs equivalently to the regularized temporal convolution network (r-TCN), but gains 4% more accuracy with 6x less latency on 6 challenging benchmarks. These results push the state-of-the-art performances of video understanding by a considerable margin. Not surprising, the ensemble model of the proposed TIN won the $1^{st}$ place in the ICCV19 - Multi Moments in Time challenge. Code is made available to facilitate further research at https://github.com/deepcs233/TIN",0
"The vision community has been attempting to learn how to represent spatio-temporal data by combining convolutional neural networks with various temporal models. However, these methods require a large amount of computing resources due to the alternating learning process for spatial and temporal information. Therefore, a natural question arises as to whether temporal information can be embedded into spatial information to enable joint learning. This work proposes a simple yet effective operator called the temporal interlacing network (TIN) that fuses spatial representations from the past and future by interlacing them. The interlacing process can be controlled by a differentiable interlacing target. TIN replaces the need for a heavy temporal model with a simple interlacing operator. Theoretical analysis shows that TIN performs equivalently to the regularized temporal convolution network (r-TCN), but with 4% more accuracy and 6x less latency on 6 challenging benchmarks. The TIN ensemble model won the $1^{st}$ place in the ICCV19 - Multi Moments in Time challenge. The code is available at https://github.com/deepcs233/TIN to facilitate further research.",1
"We consider the problem of unsupervised camera pose estimation. Given an input video sequence, our goal is to estimate the camera pose (i.e. the camera motion) between consecutive frames. Traditionally, this problem is tackled by placing strict constraints on the transformation vector or by incorporating optical flow through a complex pipeline. We propose an alternative approach that utilizes a compositional re-estimation process for camera pose estimation. Given an input, we first estimate a depth map. Our method then iteratively estimates the camera motion based on the estimated depth map. Our approach significantly improves the predicted camera motion both quantitatively and visually. Furthermore, the re-estimation resolves the problem of out-of-boundaries pixels in a novel and simple way. Another advantage of our approach is that it is adaptable to other camera pose estimation approaches. Experimental analysis on KITTI benchmark dataset demonstrates that our method outperforms existing state-of-the-art approaches in unsupervised camera ego-motion estimation.",0
"The problem of determining the camera pose without supervision is being considered. The aim is to estimate the camera's movement between consecutive frames when given a video sequence. Traditionally, this issue is addressed by imposing strict constraints on the transformation vector or by incorporating optical flow through a complex pipeline. We suggest an alternative method that employs a compositional re-estimation process for camera pose estimation. Initially, we estimate a depth map from the input. Our method then iteratively determines the camera motion based on the estimated depth map. Our approach enhances the predicted camera motion in terms of both quantitative and visual aspects. Additionally, the re-estimation technique solves the problem of out-of-boundary pixels in a unique and straightforward manner. Our approach is also adaptable to other camera pose estimation methods. The KITTI benchmark dataset was experimentally analyzed, and our technique outperformed current state-of-the-art unsupervised camera ego-motion estimation techniques.",1
"Recently, 3D convolutional networks yield good performance in action recognition. However, optical flow stream is still needed to ensure better performance, the cost of which is very high. In this paper, we propose a fast but effective way to extract motion features from videos utilizing residual frames as the input data in 3D ConvNets. By replacing traditional stacked RGB frames with residual ones, 20.5% and 12.5% points improvements over top-1 accuracy can be achieved on the UCF101 and HMDB51 datasets when trained from scratch. Because residual frames contain little information of object appearance, we further use a 2D convolutional network to extract appearance features and combine them with the results from residual frames to form a two-path solution. In three benchmark datasets, our two-path solution achieved better or comparable performances than those using additional optical flow methods, especially outperformed the state-of-the-art models on Mini-kinetics dataset. Further analysis indicates that better motion features can be extracted using residual frames with 3D ConvNets, and our residual-frame-input path is a good supplement for existing RGB-frame-input models.",0
"Recently, 3D convolutional networks have shown promising results in action recognition. However, incorporating optical flow stream is essential for better performance, despite its high cost. This paper proposes an efficient method to extract motion features from videos using residual frames as input data in 3D ConvNets. By replacing stacked RGB frames with residual ones, a significant increase in top-1 accuracy of 20.5% and 12.5% on the UCF101 and HMDB51 datasets, respectively, can be obtained when trained from scratch. As residual frames contain minimal object appearance information, a 2D convolutional network is used to extract appearance features, which are then combined with the results from residual frames to create a two-path solution. Our two-path solution outperforms existing models on Mini-kinetics dataset and achieves comparable or better performances than those using additional optical flow methods on three benchmark datasets. Further analysis reveals that residual frames with 3D ConvNets can extract better motion features, making our residual-frame-input path an excellent complement to existing RGB-frame-input models.",1
"Event-based vision sensors, such as the Dynamic Vision Sensor (DVS), are ideally suited for real-time motion analysis. The unique properties encompassed in the readings of such sensors provide high temporal resolution, superior sensitivity to light and low latency. These properties provide the grounds to estimate motion extremely reliably in the most sophisticated scenarios but they come at a price - modern event-based vision sensors have extremely low resolution and produce a lot of noise. Moreover, the asynchronous nature of the event stream calls for novel algorithms.   This paper presents a new, efficient approach to object tracking with asynchronous cameras. We present a novel event stream representation which enables us to utilize information about the dynamic (temporal) component of the event stream, and not only the spatial component, at every moment of time. This is done by approximating the 3D geometry of the event stream with a parametric model; as a result, the algorithm is capable of producing the motion-compensated event stream (effectively approximating egomotion), and without using any form of external sensors in extremely low-light and noisy conditions without any form of feature tracking or explicit optical flow computation. We demonstrate our framework on the task of independent motion detection and tracking, where we use the temporal model inconsistencies to locate differently moving objects in challenging situations of very fast motion.",0
"Dynamic Vision Sensor (DVS) and other event-based vision sensors are well-suited for real-time motion analysis due to their unique features, including high temporal resolution, superior light sensitivity, and low latency. However, these advantages come with a drawback - these sensors have low resolution and produce a lot of noise. Additionally, the asynchronous nature of the event stream requires new algorithms. In this paper, we introduce a novel approach to object tracking with asynchronous cameras. Our method utilizes a parametric model to approximate the 3D geometry of the event stream, allowing us to incorporate information about the dynamic component of the event stream at every moment of time. With this approach, we can produce a motion-compensated event stream without external sensors, even in low-light and noisy conditions. We demonstrate the effectiveness of our framework on the task of independent motion detection and tracking, where we use the temporal model inconsistencies to identify objects with different movements, even in situations of fast motion.",1
"Visual odometry is an essential key for a localization module in SLAM systems. However, previous methods require tuning the system to adapt environment changes. In this paper, we propose a learning-based approach for frame-to-frame monocular visual odometry estimation. The proposed network is only learned by disparity maps for not only covering the environment changes but also solving the scale problem. Furthermore, attention block and skip-ordering scheme are introduced to achieve robust performance in various driving environment. Our network is compared with the conventional methods which use common domain such as color or optical flow. Experimental results confirm that the proposed network shows better performance than other approaches with higher and more stable results.",0
"Visual odometry is crucial for localizing SLAM systems, but previous methods required system tuning to adapt to environmental changes. In this study, we present a learning-based approach for estimating frame-to-frame monocular visual odometry. The proposed network utilizes only disparity maps, addressing both environment changes and the scale problem. To achieve robust performance in diverse driving environments, we introduce an attention block and skip-ordering scheme. Our network is compared to conventional methods using color or optical flow. Experimental results demonstrate that our approach outperforms other methods with higher and more consistent results.",1
"VBM3D is an extension to video of the well known image denoising algorithm BM3D, which takes advantage of the sparse representation of stacks of similar patches in a transform domain. The extension is rather straightforward: the similar 2D patches are taken from a spatio-temporal neighborhood which includes neighboring frames. In spite of its simplicity, the algorithm offers a good trade-off between denoising performance and computational complexity. In this work we revisit this method, providing an open-source C++ implementation reproducing the results. A detailed description is given and the choice of parameters is thoroughly discussed. Furthermore, we discuss several extensions of the original algorithm: (1) a multi-scale implementation, (2) the use of 3D patches, (3) the use of optical flow to guide the patch search. These extensions allow to obtain results which are competitive with even the most recent state of the art.",0
"VBM3D is a video denoising algorithm that builds upon BM3D, a well-known image denoising technique utilizing the sparse representation of stacks of similar patches in a transform domain. VBM3D uses a spatio-temporal neighborhood to extract similar 2D patches, including neighboring frames. Despite its straightforward implementation, VBM3D strikes a balance between denoising performance and computational complexity. In this study, we present an open-source C++ implementation of the algorithm, providing a detailed description and discussing parameter choices. Additionally, we explore several extensions of the original algorithm, including a multi-scale implementation, the use of 3D patches, and optical flow-guided patch search. These extensions yield results that are comparable to the most recent state-of-the-art methods.",1
"Video super-resolution (SR) aims at generating a sequence of high-resolution (HR) frames with plausible and temporally consistent details from their low-resolution (LR) counterparts. The key challenge for video SR lies in the effective exploitation of temporal dependency between consecutive frames. Existing deep learning based methods commonly estimate optical flows between LR frames to provide temporal dependency. However, the resolution conflict between LR optical flows and HR outputs hinders the recovery of fine details. In this paper, we propose an end-to-end video SR network to super-resolve both optical flows and images. Optical flow SR from LR frames provides accurate temporal dependency and ultimately improves video SR performance. Specifically, we first propose an optical flow reconstruction network (OFRnet) to infer HR optical flows in a coarse-to-fine manner. Then, motion compensation is performed using HR optical flows to encode temporal dependency. Finally, compensated LR inputs are fed to a super-resolution network (SRnet) to generate SR results. Extensive experiments have been conducted to demonstrate the effectiveness of HR optical flows for SR performance improvement. Comparative results on the Vid4 and DAVIS-10 datasets show that our network achieves the state-of-the-art performance.",0
"The goal of video super-resolution (SR) is to create a series of high-resolution (HR) frames that accurately reflect the details of their low-resolution (LR) counterparts in a consistent and believable manner. However, the primary challenge of video SR lies in effectively utilizing the temporal dependency between consecutive frames. Current deep learning methods rely on estimating optical flows between LR frames to provide this temporal dependency, but the resolution discrepancy between LR optical flows and HR outputs can impede the recovery of fine details. In this study, we propose an end-to-end video SR network that can super-resolve both optical flows and images. By using HR optical flows to encode temporal dependency, we can improve the accuracy of video SR. Our approach involves using an optical flow reconstruction network (OFRnet) to generate HR optical flows and motion compensation to account for temporal dependency. The compensated LR inputs are then fed into a super-resolution network (SRnet) to produce the final SR results. Our experiments demonstrate that the use of HR optical flows significantly improves SR performance. Our network achieves state-of-the-art results on the Vid4 and DAVIS-10 datasets.",1
"This paper tackles the problem of real-time semantic segmentation of high definition videos using a hybrid GPU / CPU approach. We propose an Efficient Video Segmentation(EVS) pipeline that combines:   (i) On the CPU, a very fast optical flow method, that is used to exploit the temporal aspect of the video and propagate semantic information from one frame to the next. It runs in parallel with the GPU.   (ii) On the GPU, two Convolutional Neural Networks: A main segmentation network that is used to predict dense semantic labels from scratch, and a Refiner that is designed to improve predictions from previous frames with the help of a fast Inconsistencies Attention Module (IAM). The latter can identify regions that cannot be propagated accurately.   We suggest several operating points depending on the desired frame rate and accuracy. Our pipeline achieves accuracy levels competitive to the existing real-time methods for semantic image segmentation(mIoU above 60%), while achieving much higher frame rates. On the popular Cityscapes dataset with high resolution frames (2048 x 1024), the proposed operating points range from 80 to 1000 Hz on a single GPU and CPU.",0
"The aim of this paper is to address the challenge of real-time semantic segmentation of high definition videos through a hybrid GPU / CPU method. The authors present an Efficient Video Segmentation (EVS) pipeline which involves two elements. Firstly, on the CPU, a rapid optical flow method is employed to exploit the temporal aspect of the video and propagate semantic data from one frame to another, while running in parallel with the GPU. Secondly, two Convolutional Neural Networks are used on the GPU: a primary segmentation network that predicts dense semantic labels from scratch, and a Refiner that enhances predictions from previous frames using a fast Inconsistencies Attention Module (IAM) to identify regions that cannot be propagated accurately. The authors offer a variety of operational points based on desired accuracy and frame rate, with their pipeline achieving competitive accuracy levels for real-time semantic image segmentation (mIoU above 60%), and significantly higher frame rates. The suggested operating points for the Cityscapes dataset with high resolution frames (2048 x 1024) range from 80 to 1000 Hz on a single GPU and CPU.",1
"We describe a technique that automatically generates plausible depth maps from videos using non-parametric depth sampling. We demonstrate our technique in cases where past methods fail (non-translating cameras and dynamic scenes). Our technique is applicable to single images as well as videos. For videos, we use local motion cues to improve the inferred depth maps, while optical flow is used to ensure temporal depth consistency. For training and evaluation, we use a Kinect-based system to collect a large dataset containing stereoscopic videos with known depths. We show that our depth estimation technique outperforms the state-of-the-art on benchmark databases. Our technique can be used to automatically convert a monoscopic video into stereo for 3D visualization, and we demonstrate this through a variety of visually pleasing results for indoor and outdoor scenes, including results from the feature film Charade.",0
"The technique we present involves the automatic generation of believable depth maps from videos, using non-parametric depth sampling. We showcase how our technique succeeds in scenarios where previous methods have failed, such as with dynamic scenes and non-translating cameras. Our approach is not only applicable to single images but also to videos. When working with videos, we employ local motion cues to enhance the inferred depth maps and ensure temporal depth consistency using optical flow. To train and assess our technique, we use a Kinect-based system to collect a vast dataset that comprises stereoscopic videos with known depths. Our results indicate that our depth estimation technique surpasses the state-of-the-art on benchmark databases. Moreover, our approach can convert a monoscopic video into stereo for 3D visualization automatically. We provide a range of visually appealing outcomes for indoor and outdoor scenes, including examples from the feature film Charade.",1
"We describe a technique that automatically generates plausible depth maps from videos using non-parametric depth sampling. We demonstrate our technique in cases where past methods fail (non-translating cameras and dynamic scenes). Our technique is applicable to single images as well as videos. For videos, we use local motion cues to improve the inferred depth maps, while optical flow is used to ensure temporal depth consistency. For training and evaluation, we use a Kinect-based system to collect a large dataset containing stereoscopic videos with known depths. We show that our depth estimation technique outperforms the state-of-the-art on benchmark databases. Our technique can be used to automatically convert a monoscopic video into stereo for 3D visualization, and we demonstrate this through a variety of visually pleasing results for indoor and outdoor scenes, including results from the feature film Charade.",0
"We have devised a method that can generate believable depth maps from videos by utilizing non-parametric depth sampling. Our approach can overcome the limitations of previous techniques when dealing with non-translating cameras and dynamic scenes. It can be applied to both individual images and videos, with the latter benefiting from improved depth maps due to local motion cues and temporal depth consistency through optical flow. Our method was evaluated using a large dataset of stereoscopic videos with known depths collected through a Kinect-based system. We have demonstrated that our technique surpasses the current state-of-the-art performance on benchmark databases. Our approach can also automatically transform a monoscopic video into a 3D stereo format for visualization. We have showcased the effectiveness of our method with various indoor and outdoor scenes, including a feature film called Charade.",1
"Event cameras provide a number of benefits over traditional cameras, such as the ability to track incredibly fast motions, high dynamic range, and low power consumption. However, their application into computer vision problems, many of which are primarily dominated by deep learning solutions, has been limited by the lack of labeled training data for events. In this work, we propose a method which leverages the existing labeled data for images by simulating events from a pair of temporal image frames, using a convolutional neural network. We train this network on pairs of images and events, using an adversarial discriminator loss and a pair of cycle consistency losses. The cycle consistency losses utilize a pair of pre-trained self-supervised networks which perform optical flow estimation and image reconstruction from events, and constrain our network to generate events which result in accurate outputs from both of these networks. Trained fully end to end, our network learns a generative model for events from images without the need for accurate modeling of the motion in the scene, exhibited by modeling based methods, while also implicitly modeling event noise. Using this simulator, we train a pair of downstream networks on object detection and 2D human pose estimation from events, using simulated data from large scale image datasets, and demonstrate the networks' abilities to generalize to datasets with real events.",0
"Traditional cameras have limitations that event cameras do not, such as the ability to track fast movements, low power consumption, and high dynamic range. However, event cameras have been limited in their application to computer vision problems dominated by deep learning solutions due to the lack of labeled training data for events. This study proposes a method to solve this issue by simulating events from a pair of temporal image frames, using a convolutional neural network. The network is trained on pairs of images and events, with an adversarial discriminator loss and a pair of cycle consistency losses. The cycle consistency losses use pre-trained self-supervised networks to constrain the network to generate events that result in accurate outputs from the optical flow estimation and image reconstruction networks. The network learns a generative model for events from images, without the need for accurate modeling of motion in the scene or explicitly modeling event noise. Using this simulator, the study demonstrates the network's ability to generalize to datasets with real events by training downstream networks on object detection and 2D human pose estimation from events, using simulated data from large scale image datasets.",1
"Two-stream networks have achieved great success in video recognition. A two-stream network combines a spatial stream of RGB frames and a temporal stream of Optical Flow to make predictions. However, the temporal redundancy of RGB frames as well as the high-cost of optical flow computation creates challenges for both the performance and efficiency. Recent works instead use modern compressed video modalities as an alternative to the RGB spatial stream and improve the inference speed by orders of magnitudes. Previous works create one stream for each modality which are combined with an additional temporal stream through late fusion. This is redundant since some modalities like motion vectors already contain temporal information. Based on this observation, we propose a compressed domain two-stream network IP TSN for compressed video recognition, where the two streams are represented by the two types of frames (I and P frames) in compressed videos, without needing a separate temporal stream. With this goal, we propose to fully exploit the motion information of P-stream through generalized distillation from optical flow, which largely improves the efficiency and accuracy. Our P-stream runs 60 times faster than using optical flow while achieving higher accuracy. Our full IP TSN, evaluated over public action recognition benchmarks (UCF101, HMDB51 and a subset of Kinetics), outperforms other compressed domain methods by large margins while improving the total inference speed by 20%.",0
"Video recognition has seen great success with the use of two-stream networks, which combine RGB frames and Optical Flow to make predictions. However, RGB frames have temporal redundancy, and optical flow computation is costly, making it difficult to achieve high performance and efficiency. To address this, recent research has explored using compressed video modalities instead of the RGB spatial stream. Previous approaches use one stream for each modality and combine them through late fusion, but this is redundant as some modalities already contain temporal information. To overcome this, we propose the compressed domain two-stream network IP TSN for compressed video recognition, using I and P frames without a separate temporal stream. We also use generalized distillation from optical flow to fully exploit the motion information of P-stream, which significantly improves efficiency and accuracy. Our P-stream runs 60 times faster than using optical flow, with higher accuracy. Our IP TSN outperforms other compressed domain methods on public action recognition benchmarks (UCF101, HMDB51, and a subset of Kinetics), improving total inference speed by 20%.",1
"High-resolution nowcasting is an essential tool needed for effective adaptation to climate change, particularly for extreme weather. As Deep Learning (DL) techniques have shown dramatic promise in many domains, including the geosciences, we present an application of DL to the problem of precipitation nowcasting, i.e., high-resolution (1 km x 1 km) short-term (1 hour) predictions of precipitation. We treat forecasting as an image-to-image translation problem and leverage the power of the ubiquitous UNET convolutional neural network. We find this performs favorably when compared to three commonly used models: optical flow, persistence and NOAA's numerical one-hour HRRR nowcasting prediction.",0
"To adapt to climate change effectively, high-resolution nowcasting is crucial, especially for extreme weather. As Deep Learning (DL) techniques have been successful in various fields, including geosciences, we demonstrate the use of DL in precipitation nowcasting. This involves predicting high-resolution (1 km x 1 km) short-term (1 hour) precipitation. We approach forecasting as an image-to-image translation problem and use the UNET convolutional neural network. Our results show that this outperforms three commonly used models: optical flow, persistence, and NOAA's numerical one-hour HRRR nowcasting prediction.",1
"Learning-based visual odometry and SLAM methods demonstrate a steady improvement over past years. However, collecting ground truth poses to train these methods is difficult and expensive. This could be resolved by training in an unsupervised mode, but there is still a large gap between performance of unsupervised and supervised methods. In this work, we focus on generating synthetic data for deep learning-based visual odometry and SLAM methods that take optical flow as an input. We produce training data in a form of optical flow that corresponds to arbitrary camera movement between a real frame and a virtual frame. For synthesizing data we use depth maps either produced by a depth sensor or estimated from stereo pair. We train visual odometry model on synthetic data and do not use ground truth poses hence this model can be considered unsupervised. Also it can be classified as monocular as we do not use depth maps on inference. We also propose a simple way to convert any visual odometry model into a SLAM method based on frame matching and graph optimization. We demonstrate that both the synthetically-trained visual odometry model and the proposed SLAM method build upon this model yields state-of-the-art results among unsupervised methods on KITTI dataset and shows promising results on a challenging EuRoC dataset.",0
"Visual odometry and SLAM techniques that rely on machine learning have shown gradual improvement over the years. However, obtaining accurate and reliable ground truth poses for training these methods is a difficult and costly process. One solution to this issue is to train models in an unsupervised manner, but this approach still has a significant performance gap compared to supervised methods. In this study, the focus is on generating synthetic training data for deep learning-based visual odometry and SLAM techniques that use optical flow as an input. The training data is produced in the form of optical flow, which corresponds to the camera's movement between a real and a virtual frame. Synthetic data is generated using depth maps obtained from a depth sensor or estimated from a stereo pair. The visual odometry model is trained on synthetic data without relying on ground truth poses, making it an unsupervised and monocular approach as depth maps are not used during inference. Additionally, a simple method is proposed to convert any visual odometry model into a SLAM technique based on frame matching and graph optimization. The results demonstrate that the synthetically-trained visual odometry model and the proposed SLAM method built on top of this model outperform other unsupervised methods on the KITTI dataset. The method also shows promising results on the challenging EuRoC dataset.",1
"Modern approaches for multi-person pose estimation in video require large amounts of dense annotations. However, labeling every frame in a video is costly and labor intensive. To reduce the need for dense annotations, we propose a PoseWarper network that leverages training videos with sparse annotations (every k frames) to learn to perform dense temporal pose propagation and estimation. Given a pair of video frames---a labeled Frame A and an unlabeled Frame B---we train our model to predict human pose in Frame A using the features from Frame B by means of deformable convolutions to implicitly learn the pose warping between A and B. We demonstrate that we can leverage our trained PoseWarper for several applications. First, at inference time we can reverse the application direction of our network in order to propagate pose information from manually annotated frames to unlabeled frames. This makes it possible to generate pose annotations for the entire video given only a few manually-labeled frames. Compared to modern label propagation methods based on optical flow, our warping mechanism is much more compact (6M vs 39M parameters), and also more accurate (88.7% mAP vs 83.8% mAP). We also show that we can improve the accuracy of a pose estimator by training it on an augmented dataset obtained by adding our propagated poses to the original manual labels. Lastly, we can use our PoseWarper to aggregate temporal pose information from neighboring frames during inference. This allows our system to achieve state-of-the-art pose detection results on the PoseTrack2017 and PoseTrack2018 datasets. Code has been made available at: https://github.com/facebookresearch/PoseWarper.",0
"Current methods for multi-person pose estimation in videos rely heavily on dense annotations, which can be time-consuming and costly to obtain. To address this issue, our proposed PoseWarper network uses training videos with sparse annotations to learn dense temporal pose propagation and estimation. By training our model to predict human pose in a labeled frame using the features from an unlabeled frame, we can leverage our trained PoseWarper for various applications. We can propagate pose information from manually annotated frames to unlabeled frames, generate pose annotations for the entire video with only a few labeled frames, improve the accuracy of a pose estimator, and aggregate temporal pose information from neighboring frames during inference. Our warping mechanism is more compact and accurate than modern label propagation methods based on optical flow. We have made our code available at https://github.com/facebookresearch/PoseWarper.",1
"The optical flow of humans is well known to be useful for the analysis of human action. Recent optical flow methods focus on training deep networks to approach the problem. However, the training data used by them does not cover the domain of human motion. Therefore, we develop a dataset of multi-human optical flow and train optical flow networks on this dataset. We use a 3D model of the human body and motion capture data to synthesize realistic flow fields in both single- and multi-person images. We then train optical flow networks to estimate human flow fields from pairs of images. We demonstrate that our trained networks are more accurate than a wide range of top methods on held-out test data and that they can generalize well to real image sequences. The code, trained models and the dataset are available for research.",0
"Human optical flow is widely recognized as a valuable tool for analyzing human movement. Recent methods for optical flow have focused on using deep networks to address this issue. However, the training data employed by these methods fails to encompass the human motion domain. To tackle this challenge, we have created a multi-human optical flow dataset and trained optical flow networks on it. We have generated realistic flow fields in single- and multi-person images using motion capture data and a 3D model of the human body. Subsequently, we have trained optical flow networks to estimate human flow fields from image pairs. We have shown that our trained networks outperform a variety of top methods on test data and can adapt well to real image sequences. The dataset, trained models, and code are available for research purposes.",1
"Moving Object Detection (MOD) is a critical task for autonomous vehicles as moving objects represent higher collision risk than static ones. The trajectory of the ego-vehicle is planned based on the future states of detected moving objects. It is quite challenging as the ego-motion has to be modelled and compensated to be able to understand the motion of the surrounding objects. In this work, we propose a real-time end-to-end CNN architecture for MOD utilizing spatio-temporal context to improve robustness. We construct a novel time-aware architecture exploiting temporal motion information embedded within sequential images in addition to explicit motion maps using optical flow images.We demonstrate the impact of our algorithm on KITTI dataset where we obtain an improvement of 8% relative to the baselines. We compare our algorithm with state-of-the-art methods and achieve competitive results on KITTI-Motion dataset in terms of accuracy at three times better run-time. The proposed algorithm runs at 23 fps on a standard desktop GPU targeting deployment on embedded platforms.",0
"Detecting moving objects is a crucial task for autonomous vehicles since they pose a higher risk of collision compared to stationary objects. To plan the trajectory of the ego-vehicle, it is necessary to consider the future states of detected moving objects. However, this is a challenging task as the motion of the ego-vehicle must be modeled and accounted for to understand the motion of surrounding objects. In this study, we introduce a real-time end-to-end CNN architecture for Moving Object Detection (MOD) that utilizes spatio-temporal context to enhance robustness. Our innovative time-aware architecture incorporates temporal motion information and explicit motion maps using optical flow images. Our algorithm improves the MOD performance by 8% relative to existing methods on the KITTI dataset. Furthermore, our algorithm achieves competitive results on the KITTI-Motion dataset with three times better run-time efficiency. Our proposed algorithm runs at 23 fps on a standard desktop GPU, making it suitable for deployment on embedded platforms.",1
"Deep learning-based video salient object detection has recently achieved great success with its performance significantly outperforming any other unsupervised methods. However, existing data-driven approaches heavily rely on a large quantity of pixel-wise annotated video frames to deliver such promising results. In this paper, we address the semi-supervised video salient object detection task using pseudo-labels. Specifically, we present an effective video saliency detector that consists of a spatial refinement network and a spatiotemporal module. Based on the same refinement network and motion information in terms of optical flow, we further propose a novel method for generating pixel-level pseudo-labels from sparsely annotated frames. By utilizing the generated pseudo-labels together with a part of manual annotations, our video saliency detector learns spatial and temporal cues for both contrast inference and coherence enhancement, thus producing accurate saliency maps. Experimental results demonstrate that our proposed semi-supervised method even greatly outperforms all the state-of-the-art fully supervised methods across three public benchmarks of VOS, DAVIS, and FBMS.",0
"Recently, deep learning-based video salient object detection has proven to be highly successful, surpassing other unsupervised techniques in terms of performance. However, current data-driven methods rely heavily on a substantial amount of pixel-wise annotated video frames to achieve such impressive outcomes. This study introduces a semi-supervised approach to video salient object detection using pseudo-labels. A spatial refinement network and a spatiotemporal module comprise the proposed effective video saliency detector. Additionally, a novel method for generating pixel-level pseudo-labels from sparsely annotated frames is presented, which uses the same refinement network and motion information in terms of optical flow. By incorporating the generated pseudo-labels with some manual annotations, the video saliency detector can learn spatial and temporal cues for both contrast inference and coherence enhancement, resulting in precise saliency maps. Experimental results show that the proposed semi-supervised method significantly outperforms all the state-of-the-art fully supervised methods in three public benchmarks of VOS, DAVIS, and FBMS.",1
"A major challenge for video semantic segmentation is the lack of labeled data. In most benchmark datasets, only one frame of a video clip is annotated, which makes most supervised methods fail to utilize information from the rest of the frames. To exploit the spatio-temporal information in videos, many previous works use pre-computed optical flows, which encode the temporal consistency to improve the video segmentation. However, the video segmentation and optical flow estimation are still considered as two separate tasks. In this paper, we propose a novel framework for joint video semantic segmentation and optical flow estimation. Semantic segmentation brings semantic information to handle occlusion for more robust optical flow estimation, while the non-occluded optical flow provides accurate pixel-level temporal correspondences to guarantee the temporal consistency of the segmentation. Moreover, our framework is able to utilize both labeled and unlabeled frames in the video through joint training, while no additional calculation is required in inference. Extensive experiments show that the proposed model makes the video semantic segmentation and optical flow estimation benefit from each other and outperforms existing methods under the same settings in both tasks.",0
"Video semantic segmentation faces a significant hurdle due to the scarcity of labeled data, with only one frame of a video clip being annotated in most benchmark datasets. This limitation prevents supervised methods from utilizing information from the remaining frames. To tackle this issue, previous works have used pre-computed optical flows to leverage spatio-temporal information. However, video segmentation and optical flow estimation are still viewed as separate tasks. To address this, we present a new approach for the joint estimation of optical flow and video semantic segmentation, which allows semantic information to handle occlusions for more robust optical flow estimation. The non-occluded optical flow, in turn, provides accurate pixel-level temporal correspondences to ensure the segmentation's temporal consistency. Our framework can use both labeled and unlabeled frames in the video during joint training, without any additional computation required during inference. Our experimental results demonstrate that our proposed model benefits from the combination of video semantic segmentation and optical flow estimation, surpassing current methods in both tasks under the same settings.",1
"Majority of state-of-the-art monocular depth estimation methods are supervised learning approaches. The success of such approaches heavily depends on the high-quality depth labels which are expensive to obtain. Some recent methods try to learn depth networks by leveraging unsupervised cues from monocular videos which are easier to acquire but less reliable. In this paper, we propose to resolve this dilemma by transferring knowledge from synthetic videos with easily obtainable ground-truth depth labels. Due to the stylish difference between synthetic and real images, we propose a temporally-consistent domain adaptation (TCDA) approach that simultaneously explores labels in the synthetic domain and temporal constraints in the videos to improve style transfer and depth prediction. Furthermore, we make use of the ground-truth optical flow and pose information in the synthetic data to learn moving mask and pose prediction networks. The learned moving masks can filter out moving regions that produces erroneous temporal constraints and the estimated poses provide better initializations for estimating temporal constraints. Experimental results demonstrate the effectiveness of our method and comparable performance against state-of-the-art.",0
"The majority of modern techniques for estimating monocular depth involve supervised learning methods. However, these approaches rely heavily on high-quality depth labels, which can be expensive to obtain. Some recent methods have attempted to leverage unsupervised cues from monocular videos, which are easier to acquire but less reliable. In this paper, we propose a solution to this dilemma by transferring knowledge from synthetic videos that have readily available ground-truth depth labels. Because synthetic and real images differ stylistically, we introduce a temporally-consistent domain adaptation (TCDA) approach that simultaneously explores labels in the synthetic domain and temporal constraints in the videos to improve depth prediction and style transfer. Additionally, we utilize ground-truth optical flow and pose information in the synthetic data to develop moving mask and pose prediction networks. The learned moving masks can filter out erroneous temporal constraints produced by moving regions, and the estimated poses provide better initializations for estimating temporal constraints. Our experimental results demonstrate the effectiveness of our method and comparable performance to state-of-the-art techniques.",1
"We propose a torus model for high-contrast patches of optical flow. Our model is derived from a database of ground-truth optical flow from the computer-generated video \emph{Sintel}, collected by Butler et al.\ in \emph{A naturalistic open source movie for optical flow evaluation}. Using persistent homology and zigzag persistence, popular tools from the field of computational topology, we show that the high-contrast $3\times 3$ patches from this video are well-modeled by a \emph{torus}, a nonlinear 2-dimensional manifold. Furthermore, we show that the optical flow torus model is naturally equipped with the structure of a fiber bundle, related to the statistics of range image patches.",0
"A torus model is suggested for optical flow's high-contrast patches. Our model is derived from a ground-truth optical flow database from the computer-generated video, ""Sintel"", gathered by Butler et al. in ""A naturalistic open source movie for optical flow evaluation."" We utilize popular tools from computational topology, persistent homology and zigzag persistence, to demonstrate that the high-contrast $3 \times 3$ patches from the video can be appropriately modeled by a nonlinear 2-dimensional manifold, the torus. Additionally, we demonstrate that the optical flow torus model is naturally structured as a fiber bundle that relates to the range image patch statistics.",1
"Scene flow is a challenging task aimed at jointly estimating the 3D structure and motion of the sensed environment. Although deep learning solutions achieve outstanding performance in terms of accuracy, these approaches divide the whole problem into standalone tasks (stereo and optical flow) addressing them with independent networks. Such a strategy dramatically increases the complexity of the training procedure and requires power-hungry GPUs to infer scene flow barely at 1 FPS. Conversely, we propose DWARF, a novel and lightweight architecture able to infer full scene flow jointly reasoning about depth and optical flow easily and elegantly trainable end-to-end from scratch. Moreover, since ground truth images for full scene flow are scarce, we propose to leverage on the knowledge learned by networks specialized in stereo or flow, for which much more data are available, to distill proxy annotations. Exhaustive experiments show that i) DWARF runs at about 10 FPS on a single high-end GPU and about 1 FPS on NVIDIA Jetson TX2 embedded at KITTI resolution, with moderate drop in accuracy compared to 10x deeper models, ii) learning from many distilled samples is more effective than from the few, annotated ones available. Code available at: https://github.com/FilippoAleotti/Dwarf-Tensorflow",0
"The task of scene flow involves estimating the 3D structure and motion of the environment, which is challenging. Although deep learning approaches are accurate, they typically use separate networks for stereo and optical flow, making the training process complex and requiring powerful GPUs for inference. To address this, we propose a lightweight architecture called DWARF that can easily and elegantly infer full scene flow by jointly reasoning about depth and optical flow. We also suggest leveraging the knowledge learned by specialized networks in stereo or flow to distill proxy annotations since ground truth images are scarce. Our experiments show that DWARF runs at 10 FPS on a high-end GPU and 1 FPS on NVIDIA Jetson TX2 embedded at KITTI resolution, with a moderate drop in accuracy compared to deeper models. Additionally, learning from many distilled samples is more effective than from a few annotated ones. The code for DWARF is available at: https://github.com/FilippoAleotti/Dwarf-Tensorflow.",1
"Spatial-temporal feature learning is of vital importance for video emotion recognition. Previous deep network structures often focused on macro-motion which extends over long time scales, e.g., on the order of seconds. We believe integrating structures capturing information about both micro- and macro-motion will benefit emotion prediction, because human perceive both micro- and macro-expressions. In this paper, we propose to combine micro- and macro-motion features to improve video emotion recognition with a two-stream recurrent network, named MIMAMO (Micro-Macro-Motion) Net. Specifically, smaller and shorter micro-motions are analyzed by a two-stream network, while larger and more sustained macro-motions can be well captured by a subsequent recurrent network. Assigning specific interpretations to the roles of different parts of the network enables us to make choice of parameters based on prior knowledge: choices that turn out to be optimal. One of the important innovations in our model is the use of interframe phase differences rather than optical flow as input to the temporal stream. Compared with the optical flow, phase differences require less computation and are more robust to illumination changes. Our proposed network achieves state of the art performance on two video emotion datasets, the OMG emotion dataset and the Aff-Wild dataset. The most significant gains are for arousal prediction, for which motion information is intuitively more informative. Source code is available at https://github.com/wtomin/MIMAMO-Net.",0
"The ability to learn spatial-temporal features is crucial for accurately recognizing emotions in videos. Traditional deep network structures have primarily focused on capturing macro-motion, which occurs over long time periods, such as several seconds. However, as humans perceive both micro- and macro-expressions, we believe that incorporating features that capture information about both types of motion will enhance emotion prediction. In this study, we introduce the MIMAMO (Micro-Macro-Motion) Net, a two-stream recurrent network that combines micro- and macro-motion features to improve video emotion recognition. The two-stream network analyzes smaller and shorter micro-motions, while a subsequent recurrent network captures larger and more sustained macro-motions. By assigning specific roles to different parts of the network, we can select optimal parameters based on prior knowledge. One innovative aspect of our model is the use of interframe phase differences instead of optical flow as input to the temporal stream, which requires less computation and is more robust to illumination changes. Our proposed network achieves state-of-the-art performance on two video emotion datasets, the OMG emotion dataset and the Aff-Wild dataset, with the most significant improvements observed in arousal prediction. The source code for our model is available at https://github.com/wtomin/MIMAMO-Net.",1
"Moving object detection is a critical task for autonomous vehicles. As dynamic objects represent higher collision risk than static ones, our own ego-trajectories have to be planned attending to the future states of the moving elements of the scene. Motion can be perceived using temporal information such as optical flow. Conventional optical flow computation is based on camera sensors only, which makes it prone to failure in conditions with low illumination. On the other hand, LiDAR sensors are independent of illumination, as they measure the time-of-flight of their own emitted lasers. In this work, we propose a robust and real-time CNN architecture for Moving Object Detection (MOD) under low-light conditions by capturing motion information from both camera and LiDAR sensors. We demonstrate the impact of our algorithm on KITTI dataset where we simulate a low-light environment creating a novel dataset ""Dark KITTI"". We obtain a 10.1% relative improvement on Dark-KITTI, and a 4.25% improvement on standard KITTI relative to our baselines. The proposed algorithm runs at 18 fps on a standard desktop GPU using $256\times1224$ resolution images.",0
"Detecting moving objects is a crucial task for autonomous vehicles, as dynamic objects pose a higher risk of collision than static ones. Therefore, when planning our own path, we must take into account the future states of the moving elements in the scene. To perceive motion, we can use temporal information such as optical flow. However, conventional optical flow computation based on camera sensors is susceptible to failure in low-light conditions. In contrast, LiDAR sensors are not affected by illumination as they measure the time-of-flight of their own emitted lasers. This study proposes a real-time CNN architecture for Moving Object Detection (MOD) that captures motion information from both camera and LiDAR sensors to achieve robustness under low-light conditions. The proposed algorithm was tested on the KITTI dataset, including a new dataset called ""Dark KITTI,"" which simulates a low-light environment. Results showed a 10.1% relative improvement on Dark-KITTI and a 4.25% improvement on standard KITTI compared to the baselines. The algorithm runs at 18 fps on a standard desktop GPU with $256\times1224$ resolution images.",1
"It is expensive to generate real-life image labels and there is a domain gap between real-life and simulated images, hence a model trained on the latter cannot adapt to the former. Solving this can totally eliminate the need for labeling real-life datasets completely. Class balanced self-training is one of the existing techniques that attempt to reduce the domain gap. Moreover, augmenting RGB with flow maps has improved performance in simple semantic segmentation and geometry is preserved across domains. Hence, by augmenting images with dense optical flow map, domain adaptation in semantic segmentation can be improved.",0
"Generating labels for real-life images is costly and poses a challenge due to the domain gap between real-life and simulated images. Consequently, models trained on simulated images cannot adapt well to real-life images. Addressing this challenge can eliminate the need for labeling real-life datasets altogether. One technique that aims to reduce the domain gap is class balanced self-training. Additionally, incorporating flow maps with RGB has shown to enhance the performance of simple semantic segmentation while preserving geometry across domains. Therefore, improving domain adaptation in semantic segmentation can be achieved by augmenting images with dense optical flow maps.",1
"In this paper we present a novel approach for depth map enhancement from an RGB-D video sequence. The basic idea is to exploit the shading information in the color image. Instead of making assumption about surface albedo or controlled object motion and lighting, we use the lighting variations introduced by casual object movement. We are effectively calculating photometric stereo from a moving object under natural illuminations. The key technical challenge is to establish correspondences over the entire image set. We therefore develop a lighting insensitive robust pixel matching technique that out-performs optical flow method in presence of lighting variations. In addition we present an expectation-maximization framework to recover the surface normal and albedo simultaneously, without any regularization term. We have validated our method on both synthetic and real datasets to show its superior performance on both surface details recovery and intrinsic decomposition.",0
"A new method for improving depth maps from RGB-D video sequences is introduced in this paper. The approach utilizes shading information from the color image, instead of relying on assumptions about surface albedo or controlled object motion and lighting. Lighting variations introduced by natural object movement are used to effectively calculate photometric stereo. The main technical hurdle involves establishing correspondences across the entire image set, which is accomplished through a lighting-insensitive, robust pixel matching technique that outperforms optical flow methods in the presence of lighting variations. Additionally, an expectation-maximization framework is presented for recovering surface normal and albedo simultaneously, without the need for regularization terms. Results from both synthetic and real datasets demonstrate the method's superior performance in recovering surface details and intrinsic decomposition.",1
"Synthetic visual data can provide practically infinite diversity and rich labels, while avoiding ethical issues with privacy and bias. However, for many tasks, current models trained on synthetic data generalize poorly to real data. The task of 3D human pose estimation is a particularly interesting example of this sim2real problem, because learning-based approaches perform reasonably well given real training data, yet labeled 3D poses are extremely difficult to obtain in the wild, limiting scalability. In this paper, we show that standard neural-network approaches, which perform poorly when trained on synthetic RGB images, can perform well when the data is pre-processed to extract cues about the person's motion, notably as optical flow and the motion of 2D keypoints. Therefore, our results suggest that motion can be a simple way to bridge a sim2real gap when video is available. We evaluate on the 3D Poses in the Wild dataset, the most challenging modern benchmark for 3D pose estimation, where we show full 3D mesh recovery that is on par with state-of-the-art methods trained on real 3D sequences, despite training only on synthetic humans from the SURREAL dataset.",0
"Using synthetic visual data can offer a wide range of options and accurate labels without raising concerns about privacy or bias. However, current models trained on synthetic data often fall short when it comes to real-world applications. The challenge of estimating 3D human pose is a prime example of this problem, as it is difficult to obtain labeled 3D poses outside of a controlled environment. Despite this limitation, we have found that standard neural-network approaches can perform well when pre-processed to include cues about a person's motion, such as optical flow and the motion of 2D keypoints. By leveraging motion, we can bridge the gap between synthetic and real data when video is available. Our evaluation on the challenging 3D Poses in the Wild dataset reveals that our method produces full 3D mesh recovery that is comparable to state-of-the-art methods trained on real 3D sequences, despite only using synthetic humans from the SURREAL dataset for training.",1
"Architecture optimization, which is a technique for finding an efficient neural network that meets certain requirements, generally reduces to a set of multiple-choice selection problems among alternative sub-structures or parameters. The discrete nature of the selection problem, however, makes this optimization difficult. To tackle this problem we introduce a novel concept of a trainable gate function. The trainable gate function, which confers a differentiable property to discretevalued variables, allows us to directly optimize loss functions that include non-differentiable discrete values such as 0-1 selection. The proposed trainable gate can be applied to pruning. Pruning can be carried out simply by appending the proposed trainable gate functions to each intermediate output tensor followed by fine-tuning the overall model, using any gradient-based training methods. So the proposed method can jointly optimize the selection of the pruned channels while fine-tuning the weights of the pruned model at the same time. Our experimental results demonstrate that the proposed method efficiently optimizes arbitrary neural networks in various tasks such as image classification, style transfer, optical flow estimation, and neural machine translation.",0
"The process of architecture optimization involves finding an efficient neural network that meets specific requirements by selecting from various sub-structures or parameters. However, due to the discrete nature of this selection problem, optimization can be challenging. To address this issue, we propose a new concept called a trainable gate function that makes discrete-valued variables differentiable, enabling us to optimize loss functions that include non-differentiable discrete values like 0-1 selection. This trainable gate function can be applied to pruning, which involves appending the proposed function to each intermediate output tensor and fine-tuning the overall model using gradient-based training methods. This approach allows for the joint optimization of pruned channel selection and weight fine-tuning. Our experimental results demonstrate that this method is effective in optimizing various neural networks for tasks such as image classification, style transfer, optical flow estimation, and neural machine translation.",1
"Generating temporal action proposals remains a very challenging problem, where the main issue lies in predicting precise temporal proposal boundaries and reliable action confidence in long and untrimmed real-world videos. In this paper, we propose an efficient and unified framework to generate temporal action proposals named Dense Boundary Generator (DBG), which draws inspiration from boundary-sensitive methods and implements boundary classification and action completeness regression for densely distributed proposals. In particular, the DBG consists of two modules: Temporal boundary classification (TBC) and Action-aware completeness regression (ACR). The TBC aims to provide two temporal boundary confidence maps by low-level two-stream features, while the ACR is designed to generate an action completeness score map by high-level action-aware features. Moreover, we introduce a dual stream BaseNet (DSB) to encode RGB and optical flow information, which helps to capture discriminative boundary and actionness features. Extensive experiments on popular benchmarks ActivityNet-1.3 and THUMOS14 demonstrate the superiority of DBG over the state-of-the-art proposal generator (e.g., MGG and BMN). Our code will be made available upon publication.",0
"The task of generating temporal action proposals is quite difficult as it involves predicting precise boundaries and reliable confidence in long and untrimmed real-world videos. This paper presents a novel and efficient framework called Dense Boundary Generator (DBG) that draws inspiration from boundary-sensitive methods. The DBG consists of two modules, namely Temporal Boundary Classification (TBC) and Action-Aware Completeness Regression (ACR), which respectively aim to provide temporal boundary confidence maps and action completeness score maps. Additionally, a dual stream BaseNet (DSB) is introduced to encode RGB and optical flow information, enabling the capture of discriminative boundary and actionness features. Extensive experiments conducted on popular benchmarks ActivityNet-1.3 and THUMOS14 reveal that DBG outperforms state-of-the-art proposal generators such as MGG and BMN. The code will be made available upon publication.",1
"Many video enhancement algorithms rely on optical flow to register frames in a video sequence. Precise flow estimation is however intractable; and optical flow itself is often a sub-optimal representation for particular video processing tasks. In this paper, we propose task-oriented flow (TOFlow), a motion representation learned in a self-supervised, task-specific manner. We design a neural network with a trainable motion estimation component and a video processing component, and train them jointly to learn the task-oriented flow. For evaluation, we build Vimeo-90K, a large-scale, high-quality video dataset for low-level video processing. TOFlow outperforms traditional optical flow on standard benchmarks as well as our Vimeo-90K dataset in three video processing tasks: frame interpolation, video denoising/deblocking, and video super-resolution.",0
"Video enhancement algorithms often use optical flow to align frames in a video sequence, but precise flow estimation is difficult and optical flow may not always be the best option for certain video processing tasks. This paper introduces task-oriented flow (TOFlow), a motion representation that is learned in a self-supervised, task-specific manner. A neural network is developed with a motion estimation component and a video processing component, which are trained together to learn TOFlow. A large, high-quality video dataset called Vimeo-90K is created to test TOFlow's performance in low-level video processing tasks. Results show that TOFlow outperforms traditional optical flow on standard benchmarks and in three video processing tasks: frame interpolation, video denoising/deblocking, and video super-resolution.",1
"In recent years, artificial intelligence (AI) based on deep learning (DL) has sparked tremendous global interest. DL is widely used today and has expanded into various interesting areas. It is becoming more popular in cross-subject research, such as studies of smart city systems, which combine computer science with engineering applications. Human action detection is one of these areas. Human action detection is an interesting challenge due to its stringent requirements in terms of computing speed and accuracy. High-accuracy real-time object tracking is also considered a significant challenge. This paper integrates the YOLO detection network, which is considered a state-of-the-art tool for real-time object detection, with motion vectors and the Coyote Optimization Algorithm (COA) to construct a real-time human action localization and tracking system. The proposed system starts with the extraction of motion information from a compressed video stream and the extraction of appearance information from RGB frames using an object detector. Then, a fusion step between the two streams is performed, and the results are fed into the proposed action tracking model. The COA is used in object tracking due to its accuracy and fast convergence. The basic foundation of the proposed model is the utilization of motion vectors, which already exist in a compressed video bit stream and provide sufficient information to improve the localization of the target action without requiring high consumption of computational resources compared with other popular methods of extracting motion information, such as optical flows. This advantage allows the proposed approach to be implemented in challenging environments where the computational resources are limited, such as Internet of Things (IoT) systems.",0
"The global interest in artificial intelligence (AI) based on deep learning (DL) has recently increased significantly. DL has expanded into various areas, including cross-subject research, such as smart city systems that combine computer science with engineering applications. Human action detection is a challenging area that requires both high computing speed and accuracy. Real-time object tracking is also a significant challenge. To address these challenges, this paper proposes a real-time human action localization and tracking system that integrates the YOLO detection network, motion vectors, and the Coyote Optimization Algorithm (COA). The system starts by extracting motion and appearance information from a compressed video stream using an object detector. The two streams are then fused, and the results are fed into the proposed action tracking model. The COA is used for object tracking because of its accuracy and fast convergence. The proposed model utilizes motion vectors, which already exist in a compressed video bit stream and provide sufficient information to improve target action localization without requiring high computational resources. This feature makes the proposed approach suitable for challenging environments with limited computational resources, such as Internet of Things (IoT) systems.",1
"Fine-grained action detection is an important task with numerous applications in robotics and human-computer interaction. Existing methods typically utilize a two-stage approach including extraction of local spatio-temporal features followed by temporal modeling to capture long-term dependencies. While most recent papers have focused on the latter (long-temporal modeling), here, we focus on producing features capable of modeling fine-grained motion more efficiently. We propose a novel locally-consistent deformable convolution, which utilizes the change in receptive fields and enforces a local coherency constraint to capture motion information effectively. Our model jointly learns spatio-temporal features (instead of using independent spatial and temporal streams). The temporal component is learned from the feature space instead of pixel space, e.g. optical flow. The produced features can be flexibly used in conjunction with other long-temporal modeling networks, e.g. ST-CNN, DilatedTCN, and ED-TCN. Overall, our proposed approach robustly outperforms the original long-temporal models on two fine-grained action datasets: 50 Salads and GTEA, achieving F1 scores of 80.22% and 75.39% respectively.",0
"The detection of fine-grained actions is a significant task for various fields such as robotics and human-computer interaction. Currently, the common approach involves a two-stage process comprising the extraction of local spatio-temporal features, followed by temporal modeling to capture long-term dependencies. While recent research has focused on long-term modeling, this study aims to improve the efficiency of feature production for fine-grained motion. To achieve this goal, a novel locally-consistent deformable convolution approach is proposed, which enforces a local coherency constraint and utilizes the change in receptive fields to capture motion information effectively. Our model learns spatio-temporal features jointly, instead of using spatial and temporal streams separately. The temporal component is learned from the feature space instead of pixel space such as optical flow. The produced features can be used flexibly with other long-temporal modeling networks like ST-CNN, DilatedTCN, and ED-TCN. Our proposed approach outperforms the original long-temporal models on two fine-grained action datasets, 50 Salads and GTEA, achieving F1 scores of 80.22% and 75.39%, respectively.",1
"Inspired by the cognitive process of humans and animals, Curriculum Learning (CL) trains a model by gradually increasing the difficulty of the training data. In this paper, we study whether CL can be applied to complex geometry problems like estimating monocular Visual Odometry (VO). Unlike existing CL approaches, we present a novel CL strategy for learning the geometry of monocular VO by gradually making the learning objective more difficult during training. To this end, we propose a novel geometry-aware objective function by jointly optimizing relative and composite transformations over small windows via bounded pose regression loss. A cascade optical flow network followed by recurrent network with a differentiable windowed composition layer, termed CL-VO, is devised to learn the proposed objective. Evaluation on three real-world datasets shows superior performance of CL-VO over state-of-the-art feature-based and learning-based VO.",0
"The concept behind Curriculum Learning (CL) is based on the way humans and animals learn, where the difficulty of the training data is gradually increased. In this study, we examine whether CL can be used to solve complex geometry problems such as monocular Visual Odometry (VO). Unlike traditional CL approaches, we introduce a new CL method for learning monocular VO geometry by progressively increasing the difficulty of the learning objective during training. For this purpose, we suggest a novel geometry-aware objective function that optimizes relative and composite transformations using bounded pose regression loss over small windows. To learn this objective, we develop CL-VO, a model consisting of a cascade optical flow network followed by a recurrent network with a differentiable windowed composition layer. Our experiments on three real-world datasets show that CL-VO outperforms existing feature-based and learning-based VO methods.",1
"The deep learning-based visual tracking algorithms such as MDNet achieve high performance leveraging to the feature extraction ability of a deep neural network. However, the tracking efficiency of these trackers is not very high due to the slow feature extraction for each frame in a video. In this paper, we propose an effective tracking algorithm to alleviate the time-consuming problem. Specifically, we design a deep flow collaborative network, which executes the expensive feature network only on sparse keyframes and transfers the feature maps to other frames via optical flow. Moreover, we raise an effective adaptive keyframe scheduling mechanism to select the most appropriate keyframe. We evaluate the proposed approach on large-scale datasets: OTB2013 and OTB2015. The experiment results show that our algorithm achieves considerable speedup and high precision as well.",0
"MDNet and other deep learning-based visual tracking algorithms rely on the feature extraction capability of deep neural networks to achieve high performance. However, these trackers suffer from low efficiency due to the slow feature extraction process for each video frame. To address this issue, we propose an efficient tracking algorithm in this paper. Our solution is based on a deep flow collaborative network that utilizes the feature network only on sparse keyframes and transfers the feature maps to other frames using optical flow. Additionally, we introduce an adaptive keyframe scheduling mechanism to select the optimal keyframe. Our proposed approach is evaluated on large-scale datasets: OTB2013 and OTB2015, and the results demonstrate a significant improvement in both speed and accuracy.",1
"Predicting future video frames is extremely challenging, as there are many factors of variation that make up the dynamics of how frames change through time. Previously proposed solutions require complex inductive biases inside network architectures with highly specialized computation, including segmentation masks, optical flow, and foreground and background separation. In this work, we question if such handcrafted architectures are necessary and instead propose a different approach: finding minimal inductive bias for video prediction while maximizing network capacity. We investigate this question by performing the first large-scale empirical study and demonstrate state-of-the-art performance by learning large models on three different datasets: one for modeling object interactions, one for modeling human motion, and one for modeling car driving.",0
"It is extremely difficult to predict future video frames due to the various factors that contribute to how frames change over time. Previous solutions have required complex inductive biases in network architectures, such as segmentation masks, optical flow, and foreground and background separation. However, we challenge the necessity of such handcrafted architectures and propose a different approach: maximizing network capacity while finding minimal inductive bias for video prediction. To investigate this, we conducted a large-scale empirical study and obtained state-of-the-art performance by training large models on three different datasets, each for modeling different scenarios.",1
"The paper addresses the problem of motion saliency in videos, that is, identifying regions that undergo motion departing from its context. We propose a new unsupervised paradigm to compute motion saliency maps. The key ingredient is the flow inpainting stage. Candidate regions are determined from the optical flow boundaries. The residual flow in these regions is given by the difference between the optical flow and the flow inpainted from the surrounding areas. It provides the cue for motion saliency. The method is flexible and general by relying on motion information only. Experimental results on the DAVIS 2016 benchmark demonstrate that the method compares favourably with state-of-the-art video saliency methods.",0
"The paper deals with the issue of motion saliency in videos, which involves identifying regions that exhibit motion that is different from their surroundings. We present a novel unsupervised approach for computing motion saliency maps, which relies on the flow inpainting phase. The initial step involves identifying potential regions based on the optical flow boundaries. The residual flow in these areas is determined by comparing the optical flow with the flow that is inpainted from the surrounding regions, providing a cue for motion saliency. The method is versatile and general, as it solely relies on motion information. The experimental outcomes on the DAVIS 2016 benchmark display that the method performs well compared to current video saliency techniques.",1
"Robust and computationally efficient anomaly detection in videos is a problem in video surveillance systems. We propose a technique to increase robustness and reduce computational complexity in a Convolutional Neural Network (CNN) based anomaly detector that utilizes the optical flow information of video data. We reduce the complexity of the network by denoising the intermediate layer outputs of the CNN and by using powers-of-two weights, which replaces the computationally expensive multiplication operations with bit-shift operations. Denoising operation during inference forces small valued intermediate layer outputs to zero. The number of zeros in the network significantly increases as a result of denoising, we can implement the CNN about 10% faster than a comparable network while detecting all the anomalies in the testing set. It turns out that denoising operation also provides robustness because the contribution of small intermediate values to the final result is negligible. During training we also generate motion vector images by a Generative Adversarial Network (GAN) to improve the robustness of the overall system. We experimentally observe that the resulting system is robust to background motion.",0
"Video surveillance systems face the challenge of detecting anomalies in videos efficiently and with reliability. To address this issue, we present a method that enhances the Convolutional Neural Network (CNN) based anomaly detector's robustness and reduces its computational complexity by incorporating optical flow information. We simplify the CNN by denoising intermediate layer outputs and utilizing powers-of-two weights instead of multiplication operations. This denoising operation during inference removes small valued intermediate layer outputs, resulting in a considerable increase in zeros within the network. By implementing the CNN with this approach, we can detect all anomalies in the testing set while achieving a 10% faster processing speed than a comparable network. Furthermore, the denoising operation also contributes to the system's robustness by reducing the impact of small intermediate values on the final result. To further improve the system's resilience to background motion, we generate motion vector images using a Generative Adversarial Network (GAN) during training. Through experimentation, we observed that the resulting system is highly resilient to background motion.",1
"In this research, Piano performances have been analyzed only based on visual information. Computer vision algorithms, e.g., Hough transform and binary thresholding, have been applied to find where the keyboard and specific keys are located. At the same time, Convolutional Neural Networks(CNNs) has been also utilized to find whether specific keys are pressed or not, and how much intensity the keys are pressed only based on visual information. Especially for detecting intensity, a new method of utilizing spatial, temporal CNNs model is devised. Early fusion technique is especially applied in temporal CNNs architecture to analyze hand movement. We also make a new dataset for training each model. Especially when finding an intensity of a pressed key, both of video frames and their optical flow images are used to train models to find effectiveness.",0
"The study focused on analyzing piano performances using visual information only. Computer vision techniques, such as Hough transform and binary thresholding, were employed to identify the location of the keyboard and certain keys. Additionally, Convolutional Neural Networks (CNNs) were utilized to determine if specific keys were pressed and the amount of pressure applied, solely based on visual data. To detect the intensity of key presses, a novel spatial-temporal CNN model was developed. The temporal CNNs architecture employed the early fusion technique to analyze hand movements. A new dataset was also created to train each model, with video frames and their optical flow images used to enhance the effectiveness of identifying key intensity.",1
"We introduce a compact network for holistic scene flow estimation, called SENSE, which shares common encoder features among four closely-related tasks: optical flow estimation, disparity estimation from stereo, occlusion estimation, and semantic segmentation. Our key insight is that sharing features makes the network more compact, induces better feature representations, and can better exploit interactions among these tasks to handle partially labeled data. With a shared encoder, we can flexibly add decoders for different tasks during training. This modular design leads to a compact and efficient model at inference time. Exploiting the interactions among these tasks allows us to introduce distillation and self-supervised losses in addition to supervised losses, which can better handle partially labeled real-world data. SENSE achieves state-of-the-art results on several optical flow benchmarks and runs as fast as networks specifically designed for optical flow. It also compares favorably against the state of the art on stereo and scene flow, while consuming much less memory.",0
"SENSE is a compact network designed for holistic scene flow estimation. It shares encoder features among four closely-related tasks - optical flow estimation, disparity estimation from stereo, occlusion estimation, and semantic segmentation. Our approach is based on the idea that sharing features can reduce network size, improve feature representation, and optimize interactions among tasks for handling partially labeled data. During training, we can add decoders for different tasks using a shared encoder, resulting in a modular design that leads to an efficient model at inference time. By exploiting interactions among the tasks, we can introduce distillation and self-supervised losses in addition to supervised losses, which can improve performance on real-world data. SENSE outperforms state-of-the-art models on optical flow benchmarks and runs as fast as networks designed specifically for optical flow. It also compares favorably against the state of the art on stereo and scene flow while using much less memory.",1
"Unsupervised video object segmentation has often been tackled by methods based on recurrent neural networks and optical flow. Despite their complexity, these kinds of approaches tend to favour short-term temporal dependencies and are thus prone to accumulating inaccuracies, which cause drift over time. Moreover, simple (static) image segmentation models, alone, can perform competitively against these methods, which further suggests that the way temporal dependencies are modelled should be reconsidered. Motivated by these observations, in this paper we explore simple yet effective strategies to model long-term temporal dependencies. Inspired by the non-local operators of [70], we introduce a technique to establish dense correspondences between pixel embeddings of a reference ""anchor"" frame and the current one. This allows the learning of pairwise dependencies at arbitrarily long distances without conditioning on intermediate frames. Without online supervision, our approach can suppress the background and precisely segment the foreground object even in challenging scenarios, while maintaining consistent performance over time. With a mean IoU of $81.7\%$, our method ranks first on the DAVIS-2016 leaderboard of unsupervised methods, while still being competitive against state-of-the-art online semi-supervised approaches. We further evaluate our method on the FBMS dataset and the ViSal video saliency dataset, showing results competitive with the state of the art.",0
"Typically, unsupervised video object segmentation is tackled using recurrent neural networks and optical flow methods. While these approaches are complex, they tend to prioritize short-term temporal dependencies, leading to inaccuracies and drift over time. Additionally, simple image segmentation models can perform competitively against these methods. Therefore, we propose exploring simple yet effective strategies to model long-term temporal dependencies. Our approach involves establishing dense correspondences between pixel embeddings of a reference anchor frame and the current frame to learn pairwise dependencies at arbitrarily long distances without conditioning on intermediate frames. Our method achieves a mean IoU of 81.7% on the DAVIS-2016 leaderboard of unsupervised methods, ranking first, and remains competitive against state-of-the-art online semi-supervised approaches, without online supervision. We also demonstrate competitive results on the FBMS dataset and the ViSal video saliency dataset.",1
"Action recognition is a key problem in computer vision that labels videos with a set of predefined actions. Capturing both, semantic content and motion, along the video frames is key to achieve high accuracy performance on this task. Most of the state-of-the-art methods rely on RGB frames for extracting the semantics and pre-computed optical flow fields as a motion cue. Then, both are combined using deep neural networks. Yet, it has been argued that such models are not able to leverage the motion information extracted from the optical flow, but instead the optical flow allows for better recognition of people and objects in the video. This urges the need to explore different cues or models that can extract motion in a more informative fashion. To tackle this issue, we propose to explore the predictive coding network, so called PredNet, a recurrent neural network that propagates predictive coding errors across layers and time steps. We analyze whether PredNet can better capture motions in videos by estimating over time the representations extracted from pre-trained networks for action recognition. In this way, the model only relies on the video frames, and does not need pre-processed optical flows as input. We report the effectiveness of our proposed model on UCF101 and HMDB51 datasets.",0
"The task of recognizing actions in videos is a crucial problem in computer vision. To achieve high accuracy performance, it is important to capture both the semantic content and motion across video frames. Most current methods utilize RGB frames to extract semantics and pre-calculated optical flow fields for motion cues, which are then combined using deep neural networks. However, there is a debate on whether these models effectively use motion information from optical flow or if it simply enhances recognition of people and objects in the video. This highlights the need to explore alternative cues or models that can extract motion in a more informative manner. To address this, we propose using the PredNet, a recurrent neural network that propagates predictive coding errors across layers and time steps. We investigate whether the PredNet can better capture motion in videos by estimating the representations extracted from pre-trained networks for action recognition over time. Our proposed model only relies on video frames and does not require pre-processed optical flows as input. We demonstrate the effectiveness of our model on UCF101 and HMDB51 datasets.",1
"Deep neural nets achieve state-of-the-art performance on the problem of optical flow estimation. Since optical flow is used in several safety-critical applications like self-driving cars, it is important to gain insights into the robustness of those techniques. Recently, it has been shown that adversarial attacks easily fool deep neural networks to misclassify objects. The robustness of optical flow networks to adversarial attacks, however, has not been studied so far. In this paper, we extend adversarial patch attacks to optical flow networks and show that such attacks can compromise their performance. We show that corrupting a small patch of less than 1% of the image size can significantly affect optical flow estimates. Our attacks lead to noisy flow estimates that extend significantly beyond the region of the attack, in many cases even completely erasing the motion of objects in the scene. While networks using an encoder-decoder architecture are very sensitive to these attacks, we found that networks using a spatial pyramid architecture are less affected. We analyse the success and failure of attacking both architectures by visualizing their feature maps and comparing them to classical optical flow techniques which are robust to these attacks. We also demonstrate that such attacks are practical by placing a printed pattern into real scenes.",0
"State-of-the-art performance in optical flow estimation is accomplished by deep neural nets. Given that optical flow is used in safety-critical applications like self-driving cars, it is essential to understand the robustness of these techniques. Recent studies have demonstrated that deep neural networks are easily fooled by adversarial attacks causing misclassification of objects. However, the vulnerability of optical flow networks to adversarial attacks has yet to be explored. This research extends adversarial patch attacks to optical flow networks and reveals that such attacks can compromise their performance. The study shows that corrupting a small patch of less than 1% of the image size can significantly affect optical flow estimates. These attacks result in noisy flow estimates that extend beyond the region of the attack, potentially erasing the motion of objects in the scene. While encoder-decoder architecture networks are highly vulnerable to these attacks, spatial pyramid architecture networks are less affected. The research analyses the success and failure of attacking both architectures by visualizing their feature maps and comparing them to classical optical flow techniques that are resilient to these attacks. Additionally, the study demonstrates the practicality of these attacks by placing a printed pattern into real scenes.",1
"Though machine learning has achieved notable success in modeling sequential and spatial data for speech recognition and in computer vision, applications to remote sensing and climate science problems are seldom considered. In this paper, we demonstrate techniques from unsupervised learning of future video frame prediction, to increase the accuracy of ice flow tracking in multi-spectral satellite images. As the volume of cryosphere data increases in coming years, this is an interesting and important opportunity for machine learning to address a global challenge for climate change, risk management from floods, and conserving freshwater resources. Future frame prediction of ice melt and tracking the optical flow of ice dynamics presents modeling difficulties, due to uncertainties in global temperature increase, changing precipitation patterns, occlusion from cloud cover, rapid melting and glacier retreat due to black carbon aerosol deposition, from wildfires or human fossil emissions. We show the adversarial learning method helps improve the accuracy of tracking the optical flow of ice dynamics compared to existing methods in climate science. We present a dataset, IceNet, to encourage machine learning research and to help facilitate further applications in the areas of cryospheric science and climate change.",0
"While machine learning has been successful in modeling sequential and spatial data for speech recognition and computer vision, it has not been widely utilized in remote sensing and climate science. This paper showcases how unsupervised learning techniques used in future video frame prediction can increase the accuracy of ice flow tracking in multi-spectral satellite images. With the increasing amount of cryosphere data, this presents a crucial opportunity for machine learning to address global challenges such as climate change, flood risk management, and freshwater conservation. However, predicting ice melt and tracking ice dynamics optical flow is challenging due to uncertainties in global temperature increase, changing precipitation patterns, occlusion from cloud cover, and rapid melting caused by black carbon aerosol deposition from wildfires or human fossil emissions. Through the use of adversarial learning methods, we demonstrate improved accuracy in tracking ice dynamics optical flow compared to current methods in climate science. Additionally, we introduce the IceNet dataset to encourage further machine learning research and applications in cryospheric science and climate change.",1
"Recently unsupervised learning of depth from videos has made remarkable progress and the results are comparable to fully supervised methods in outdoor scenes like KITTI. However, there still exist great challenges when directly applying this technology in indoor environments, e.g., large areas of non-texture regions like white wall, more complex ego-motion of handheld camera, transparent glasses and shiny objects. To overcome these problems, we propose a new optical-flow based training paradigm which reduces the difficulty of unsupervised learning by providing a clearer training target and handles the non-texture regions. Our experimental evaluation demonstrates that the result of our method is comparable to fully supervised methods on the NYU Depth V2 benchmark. To the best of our knowledge, this is the first quantitative result of purely unsupervised learning method reported on indoor datasets.",0
"The progress of unsupervised learning for depth extraction from videos has been impressive lately, and it has produced results that are similar to those obtained by fully supervised methods in outdoor environments such as KITTI. Nonetheless, using this technology directly in indoor settings poses significant challenges, such as large areas of non-textured regions, more complicated ego-motion of handheld cameras, and transparent glasses and shiny objects. To address these issues, we have introduced a novel training paradigm based on optical flow that reduces the difficulty of unsupervised learning by providing a clearer training target and handling non-texture regions. Our experimental analysis has shown that our approach's output is comparable to fully supervised methods on the NYU Depth V2 benchmark. To our knowledge, this is the first quantitative outcome of a purely unsupervised learning method on indoor datasets.",1
"We address the challenging task of video-based person re-identification. Recent works have shown that splitting the video sequences into clips and then aggregating clip based similarity is appropriate for the task. We show that using a learned clip similarity aggregation function allows filtering out hard clip pairs, e.g. where the person is not clearly visible, is in a challenging pose, or where the poses in the two clips are too different to be informative. This allows the method to focus on clip-pairs which are more informative for the task. We also introduce the use of 3D CNNs for video-based re-identification and show their effectiveness by performing equivalent to previous works, which use optical flow in addition to RGB, while using RGB inputs only. We give quantitative results on three challenging public benchmarks and show better or competitive performance. We also validate our method qualitatively.",0
"Our focus is on the difficult task of person re-identification using video. Previous studies have found that dividing video sequences into clips and combining the similarities between these clips is a suitable approach. We demonstrate that employing a learned function to aggregate clip similarities can eliminate challenging clip pairs, such as those where people are not easily seen, are in awkward positions, or where the poses in the two clips differ too much to be informative. This enables the method to concentrate on clip-pairs that offer more useful information for the task. Additionally, we introduce the use of 3D CNNs for video-based re-identification and show that they are effective, achieving results equivalent to previous works that use both optical flow and RGB inputs, while using only RGB inputs. We present quantitative results using three challenging public benchmarks and demonstrate better or comparative performance. Furthermore, we validate our method qualitatively.",1
"Driver drowsiness increases crash risk, leading to substantial road trauma each year. Drowsiness detection methods have received considerable attention, but few studies have investigated the implementation of a detection approach on a mobile phone. Phone applications reduce the need for specialised hardware and hence, enable a cost-effective roll-out of the technology across the driving population. While it has been shown that three-dimensional (3D) operations are more suitable for spatiotemporal feature learning, current methods for drowsiness detection commonly use frame-based, multi-step approaches. However, computationally expensive techniques that achieve superior results on action recognition benchmarks (e.g. 3D convolutions, optical flow extraction) create bottlenecks for real-time, safety-critical applications on mobile devices. Here, we show how depthwise separable 3D convolutions, combined with an early fusion of spatial and temporal information, can achieve a balance between high prediction accuracy and real-time inference requirements. In particular, increased accuracy is achieved when assessment requires motion information, for example, when sunglasses conceal the eyes. Further, a custom TensorFlow-based smartphone application shows the true impact of various approaches on inference times and demonstrates the effectiveness of real-time monitoring based on out-of-sample data to alert a drowsy driver. Our model is pre-trained on ImageNet and Kinetics and fine-tuned on a publicly available Driver Drowsiness Detection dataset. Fine-tuning on large naturalistic driving datasets could further improve accuracy to obtain robust in-vehicle performance. Overall, our research is a step towards practical deep learning applications, potentially preventing micro-sleeps and reducing road trauma.",0
"Every year, road accidents are caused by drivers who become drowsy while driving, leading to significant road injuries. Although there has been considerable attention given to detecting driver drowsiness, few studies have explored the implementation of a detection approach on mobile phones. The use of phone applications reduces the need for specialized hardware and is therefore a cost-effective way to deploy this technology to all drivers. While three-dimensional (3D) operations have been shown to be more suitable for spatiotemporal feature learning, current approaches for drowsiness detection commonly use frame-based, multi-step methods. However, these computationally expensive techniques create bottlenecks for real-time, safety-critical applications on mobile devices. This study demonstrates that depthwise separable 3D convolutions, combined with early fusion of spatial and temporal information, can achieve a balance between high prediction accuracy and real-time inference requirements. Furthermore, a custom TensorFlow-based smartphone application shows the effectiveness of real-time monitoring based on out-of-sample data to alert a drowsy driver. The model is pre-trained on ImageNet and Kinetics and fine-tuned on a publicly available Driver Drowsiness Detection dataset. Fine-tuning on large naturalistic driving datasets could further improve accuracy to obtain robust in-vehicle performance. This research is a significant step towards practical deep learning applications that could prevent micro-sleeps and reduce road trauma.",1
"The automatic detection and tracking of general objects (like persons, animals or cars), text and logos in a video is crucial for many video understanding tasks, and usually real-time processing as required. We propose OmniTrack, an efficient and robust algorithm which is able to automatically detect and track objects, text as well as brand logos in real-time. It combines a powerful deep learning based object detector (YoloV3) with high-quality optical flow methods. Based on the reference YoloV3 C++ implementation, we did some important performance optimizations which will be described. The major steps in the training procedure for the combined detector for text and logo will be presented. We will describe then the OmniTrack algorithm, consisting of the phases preprocessing, feature calculation, prediction, matching and update. Several performance optimizations have been implemented there as well, like doing the object detection and optical flow calculation asynchronously. Experiments show that the proposed algorithm runs in real-time for standard definition ($720x576$) video on a PC with a Quadro RTX 5000 GPU.",0
"Detecting and tracking objects, text, and logos in a video is essential for video understanding tasks, and real-time processing is often necessary. To address this need, we introduce the OmniTrack algorithm, which efficiently and robustly detects and tracks these elements in real-time. The algorithm combines a deep learning-based object detector (YoloV3) with high-quality optical flow methods. We optimized the algorithm's performance based on the YoloV3 C++ implementation and present the key steps of the training procedure for the combined detector. The OmniTrack algorithm consists of preprocessing, feature calculation, prediction, matching, and update phases, and we implemented several performance optimizations, such as asynchronous object detection and optical flow calculation. Our experiments demonstrate that the proposed algorithm can run in real-time for standard definition ($720x576$) video on a Quadro RTX 5000 GPU-equipped PC.",1
"Deep video action recognition models have been highly successful in recent years but require large quantities of manually annotated data, which are expensive and laborious to obtain. In this work, we investigate the generation of synthetic training data for video action recognition, as synthetic data have been successfully used to supervise models for a variety of other computer vision tasks. We propose an interpretable parametric generative model of human action videos that relies on procedural generation, physics models and other components of modern game engines. With this model we generate a diverse, realistic, and physically plausible dataset of human action videos, called PHAV for ""Procedural Human Action Videos"". PHAV contains a total of 39,982 videos, with more than 1,000 examples for each of 35 action categories. Our video generation approach is not limited to existing motion capture sequences: 14 of these 35 categories are procedurally defined synthetic actions. In addition, each video is represented with 6 different data modalities, including RGB, optical flow and pixel-level semantic labels. These modalities are generated almost simultaneously using the Multiple Render Targets feature of modern GPUs. In order to leverage PHAV, we introduce a deep multi-task (i.e. that considers action classes from multiple datasets) representation learning architecture that is able to simultaneously learn from synthetic and real video datasets, even when their action categories differ. Our experiments on the UCF-101 and HMDB-51 benchmarks suggest that combining our large set of synthetic videos with small real-world datasets can boost recognition performance. Our approach also significantly outperforms video representations produced by fine-tuning state-of-the-art unsupervised generative models of videos.",0
"Recent years have seen great success in deep video action recognition models, but acquiring the large amounts of manually annotated data required for these models is expensive and labor-intensive. This study explores the use of synthetic data for video action recognition, as synthetic data has proven effective for other computer vision tasks. The authors propose an interpretable parametric generative model for human action videos that employs procedural generation, physics models, and other components of modern game engines. Using this model, they generate a diverse, realistic, and physically plausible dataset of human action videos (PHAV). PHAV includes 39,982 videos, with over 1,000 examples for each of 35 action categories. The video generation approach is not limited to existing motion capture sequences, with 14 of the categories being procedurally defined synthetic actions. Additionally, each video is represented with 6 different data modalities, generated almost simultaneously using modern GPU technology. To utilize PHAV, the authors introduce a deep multi-task representation learning architecture that can learn from both synthetic and real video datasets, even when their action categories differ. Experimental results on the UCF-101 and HMDB-51 benchmarks show that combining synthetic videos with small real-world datasets can improve recognition performance, and that the proposed approach outperforms video representations produced by fine-tuning state-of-the-art unsupervised generative models of videos.",1
"Video salient object detection aims at discovering the most visually distinctive objects in a video. How to effectively take object motion into consideration during video salient object detection is a critical issue. Existing state-of-the-art methods either do not explicitly model and harvest motion cues or ignore spatial contexts within optical flow images. In this paper, we develop a multi-task motion guided video salient object detection network, which learns to accomplish two sub-tasks using two sub-networks, one sub-network for salient object detection in still images and the other for motion saliency detection in optical flow images. We further introduce a series of novel motion guided attention modules, which utilize the motion saliency sub-network to attend and enhance the sub-network for still images. These two sub-networks learn to adapt to each other by end-to-end training. Experimental results demonstrate that the proposed method significantly outperforms existing state-of-the-art algorithms on a wide range of benchmarks. We hope our simple and effective approach will serve as a solid baseline and help ease future research in video salient object detection. Code and models will be made available.",0
"The objective of video salient object detection is to identify the most visually distinct objects in a video. One crucial aspect is how to consider object motion effectively during this process. However, current methods either do not take motion cues into account or neglect spatial contexts within optical flow images. To address this issue, we have developed a motion-guided video salient object detection network that performs two sub-tasks using two sub-networks. One sub-network identifies salient objects in still images, while the other detects motion saliency in optical flow images. Additionally, we have introduced motion-guided attention modules that use the motion saliency sub-network to enhance the still image sub-network. Both sub-networks are adapted to each other through end-to-end training. Our approach significantly outperforms existing state-of-the-art algorithms on various benchmarks. We expect that our straightforward and efficient approach will serve as a reliable starting point and facilitate future research in video salient object detection. Code and models will be provided.",1
"In this work we propose a capsule-based approach for semi-supervised video object segmentation. Current video object segmentation methods are frame-based and often require optical flow to capture temporal consistency across frames which can be difficult to compute. To this end, we propose a video based capsule network, CapsuleVOS, which can segment several frames at once conditioned on a reference frame and segmentation mask. This conditioning is performed through a novel routing algorithm for attention-based efficient capsule selection. We address two challenging issues in video object segmentation: 1) segmentation of small objects and 2) occlusion of objects across time. The issue of segmenting small objects is addressed with a zooming module which allows the network to process small spatial regions of the video. Apart from this, the framework utilizes a novel memory module based on recurrent networks which helps in tracking objects when they move out of frame or are occluded. The network is trained end-to-end and we demonstrate its effectiveness on two benchmark video object segmentation datasets; it outperforms current offline approaches on the Youtube-VOS dataset while having a run-time that is almost twice as fast as competing methods. The code is publicly available at https://github.com/KevinDuarte/CapsuleVOS.",0
"Our proposed approach for semi-supervised video object segmentation involves using capsules instead of the current frame-based methods. These methods often require optical flow to ensure temporal consistency across frames, which can be challenging to calculate. Our CapsuleVOS network can segment multiple frames simultaneously, depending on a reference frame and segmentation mask, thanks to a unique routing algorithm for attention-based efficient capsule selection. Our approach addresses two common challenges in video object segmentation: the segmentation of small objects and occlusion of objects over time. We combat the former with our zooming module that processes small spatial regions of the video, while the latter is addressed through our novel memory module based on recurrent networks that tracks objects when they go out of frame or become occluded. Our network is trained end-to-end and is effective on two benchmark video object segmentation datasets. It outperforms current offline approaches on the Youtube-VOS dataset and is almost twice as fast as competing methods. The code we used is available to the public at https://github.com/KevinDuarte/CapsuleVOS.",1
"Synthetic data is an increasingly popular tool for training deep learning models, especially in computer vision but also in other areas. In this work, we attempt to provide a comprehensive survey of the various directions in the development and application of synthetic data. First, we discuss synthetic datasets for basic computer vision problems, both low-level (e.g., optical flow estimation) and high-level (e.g., semantic segmentation), synthetic environments and datasets for outdoor and urban scenes (autonomous driving), indoor scenes (indoor navigation), aerial navigation, simulation environments for robotics, applications of synthetic data outside computer vision (in neural programming, bioinformatics, NLP, and more); we also survey the work on improving synthetic data development and alternative ways to produce it such as GANs. Second, we discuss in detail the synthetic-to-real domain adaptation problem that inevitably arises in applications of synthetic data, including synthetic-to-real refinement with GAN-based models and domain adaptation at the feature/model level without explicit data transformations. Third, we turn to privacy-related applications of synthetic data and review the work on generating synthetic datasets with differential privacy guarantees. We conclude by highlighting the most promising directions for further work in synthetic data studies.",0
"The use of synthetic data has become increasingly popular for training deep learning models, specifically in computer vision and other fields. This study aims to provide a comprehensive overview of the various developments and applications of synthetic data. The first section covers synthetic datasets used for low-level and high-level computer vision problems, as well as synthetic environments for autonomous driving, indoor navigation, aerial navigation, and robotics. The study also delves into how synthetic data can be used outside of computer vision, including in neural programming, bioinformatics, and NLP. Furthermore, the study explores techniques for improving synthetic data development and alternative methods such as GANs. The second section concentrates on the synthetic-to-real domain adaptation problem, including synthetic-to-real refinement and domain adaptation at the feature/model level. The third section looks at privacy-related applications of synthetic data, discussing the work on generating synthetic datasets with differential privacy guarantees. Finally, the study concludes by identifying the most promising areas for future research in synthetic data studies.",1
"We propose a novel conditional GAN (cGAN) model for continuous fine-grained human action segmentation, that utilises multi-modal data and learned scene context information. The proposed approach utilises two GANs: termed Action GAN and Auxiliary GAN, where the Action GAN is trained to operate over the current RGB frame while the Auxiliary GAN utilises supplementary information such as depth or optical flow. The goal of both GANs is to generate similar `action codes', a vector representation of the current action. To facilitate this process a context extractor that incorporates data and recent outputs from both modes is used to extract context information to aid recognition. The result is a recurrent GAN architecture which learns a task specific loss function from multiple feature modalities. Extensive evaluations on variants of the proposed model to show the importance of utilising different information streams such as context and auxiliary information in the proposed network; and show that our model is capable of outperforming state-of-the-art methods for three widely used datasets: 50 Salads, MERL Shopping and Georgia Tech Egocentric Activities, comprising both static and dynamic camera settings.",0
"Our study presents a new approach to human action segmentation using a conditional GAN (cGAN) model that combines multi-modal data and scene context information. The model consists of two GANs, namely Action GAN and Auxiliary GAN, which operate on RGB frames and supplementary information such as depth or optical flow, respectively. The ultimate objective of both GANs is to generate similar action codes, which are vector representations of the current action. To aid recognition, a context extractor that considers data and recent outputs from both modes is used to extract context information. This leads to a recurrent GAN architecture that learns a task-specific loss function from multiple feature modalities. Our evaluations demonstrate the importance of utilizing different information streams such as context and auxiliary information in the proposed network. Moreover, our model outperforms state-of-the-art methods for three widely used datasets, namely 50 Salads, MERL Shopping, and Georgia Tech Egocentric Activities, which comprise both static and dynamic camera settings.",1
"This paper proposes a vision-based fire and smoke segmentation system which use spatial, temporal and motion information to extract the desired regions from the video frames. The fusion of information is done using multiple features such as optical flow, divergence and intensity values. These features extracted from the images are used to segment the pixels into different classes in an unsupervised way. A comparative analysis is done by using multiple clustering algorithms for segmentation. Here the Markov Random Field performs more accurately than other segmentation algorithms since it characterizes the spatial interactions of pixels using a finite number of parameters. It builds a probabilistic image model that selects the most likely labeling using the maximum a posteriori (MAP) estimation. This unsupervised approach is tested on various images and achieves a frame-wise fire detection rate of 95.39%. Hence this method can be used for early detection of fire in real-time and it can be incorporated into an indoor or outdoor surveillance system.",0
"A system for fire and smoke segmentation using vision-based technology is proposed in this paper. The system utilizes spatial, temporal and motion information to extract desired regions from video frames. Multiple features such as optical flow, divergence and intensity values are fused to extract information from the images, and unsupervised segmentation is utilized to classify pixels into different classes. A comparative analysis is conducted using multiple clustering algorithms for segmentation, with Markov Random Field performing most accurately due to its ability to characterize spatial interactions of pixels with a finite number of parameters. The system achieves a frame-wise fire detection rate of 95.39% in various image tests, making it suitable for real-time early detection of fires and integration into indoor or outdoor surveillance systems.",1
"Infrared human action recognition has many advantages, i.e., it is insensitive to illumination change, appearance variability, and shadows. Existing methods for infrared action recognition are either based on spatial or local temporal information, however, the global temporal information, which can better describe the movements of body parts across the whole video, is not considered. In this letter, we propose a novel global temporal representation named optical-flow stacked difference image (OFSDI) and extract robust and discriminative feature from the infrared action data by considering the local, global, and spatial temporal information together. Due to the small size of the infrared action dataset, we first apply convolutional neural networks on local, spatial, and global temporal stream respectively to obtain efficient convolutional feature maps from the raw data rather than train a classifier directly. Then these convolutional feature maps are aggregated into effective descriptors named three-stream trajectory-pooled deep-convolutional descriptors by trajectory-constrained pooling. Furthermore, we improve the robustness of these features by using the locality-constrained linear coding (LLC) method. With these features, a linear support vector machine (SVM) is adopted to classify the action data in our scheme. We conduct the experiments on infrared action recognition datasets InfAR and NTU RGB+D. The experimental results show that the proposed approach outperforms the representative state-of-the-art handcrafted features and deep learning features based methods for the infrared action recognition.",0
"The benefits of infrared human action recognition include insensitivity to changes in illumination, variations in appearance, and shadows. However, existing methods only rely on spatial or local temporal information, neglecting global temporal information that can better describe whole-body movements. To address this issue, we introduce the optical-flow stacked difference image (OFSDI) as a new global temporal representation and combine it with local, global, and spatial temporal information to extract robust and discriminative features from infrared action data. To deal with the small size of the dataset, we apply convolutional neural networks separately to obtain efficient convolutional feature maps, which are then combined into three-stream trajectory-pooled deep-convolutional descriptors using trajectory-constrained pooling. We further enhance the robustness of these features by using the locality-constrained linear coding (LLC) method and classify the action data using a linear support vector machine (SVM). Our approach outperforms previous handcrafted and deep learning features-based methods on two datasets, InfAR and NTU RGB+D, demonstrating its effectiveness.",1
"We propose a novel video inpainting algorithm that simultaneously hallucinates missing appearance and motion (optical flow) information, building upon the recent 'Deep Image Prior' (DIP) that exploits convolutional network architectures to enforce plausible texture in static images. In extending DIP to video we make two important contributions. First, we show that coherent video inpainting is possible without a priori training. We take a generative approach to inpainting based on internal (within-video) learning without reliance upon an external corpus of visual data to train a one-size-fits-all model for the large space of general videos. Second, we show that such a framework can jointly generate both appearance and flow, whilst exploiting these complementary modalities to ensure mutual consistency. We show that leveraging appearance statistics specific to each video achieves visually plausible results whilst handling the challenging problem of long-term consistency.",0
"Our proposed video inpainting algorithm is a fresh approach that addresses the simultaneous reconstruction of missing appearance and motion data (optical flow) by utilizing the 'Deep Image Prior' (DIP) methodology. DIP is a convolutional network architecture that ensures realistic texture in still images. Our innovation builds upon this by extending DIP to video and making two crucial contributions. Firstly, we demonstrate that coherent video inpainting is achievable without relying on pre-training. Our approach is generative, where we use internal learning within each video to fill in the gaps, instead of using a one-size-fits-all model trained on a vast dataset of visual data. Secondly, we show that our framework can produce both appearance and flow data simultaneously, utilizing these two modalities to ensure accuracy and consistency. By analyzing the appearance statistics of each video, our approach achieves credible results while overcoming the challenge of long-term consistency.",1
"Recently, it is increasingly popular to equip mobile RGB cameras with Time-of-Flight (ToF) sensors for active depth sensing. However, for off-the-shelf ToF sensors, one must tackle two problems in order to obtain high-quality depth with respect to the RGB camera, namely 1) online calibration and alignment; and 2) complicated error correction for ToF depth sensing. In this work, we propose a framework for jointly alignment and refinement via deep learning. First, a cross-modal optical flow between the RGB image and the ToF amplitude image is estimated for alignment. The aligned depth is then refined via an improved kernel predicting network that performs kernel normalization and applies the bias prior to the dynamic convolution. To enrich our data for end-to-end training, we have also synthesized a dataset using tools from computer graphics. Experimental results demonstrate the effectiveness of our approach, achieving state-of-the-art for ToF refinement.",0
"In recent times, it has become increasingly popular to equip mobile RGB cameras with Time-of-Flight (ToF) sensors for active depth sensing. However, when using off-the-shelf ToF sensors, obtaining high-quality depth with respect to the RGB camera requires addressing two problems - online calibration and alignment, and complicated error correction for ToF depth sensing. In this study, we present a framework that uses deep learning to jointly align and refine the data. The first step involves estimating a cross-modal optical flow between the RGB image and the ToF amplitude image for alignment. The aligned depth is then refined using an improved kernel predicting network that performs kernel normalization and applies the bias prior to the dynamic convolution. To enrich the data for end-to-end training, we have synthesized a dataset using computer graphics tools. Our experimental results demonstrate the effectiveness of our approach, which achieves state-of-the-art performance for ToF refinement.",1
"The goal of this study is to develop and analyze multimodal models for predicting experienced affective responses of viewers watching movie clips. We develop hybrid multimodal prediction models based on both the video and audio of the clips. For the video content, we hypothesize that both image content and motion are crucial features for evoked emotion prediction. To capture such information, we extract features from RGB frames and optical flow using pre-trained neural networks. For the audio model, we compute an enhanced set of low-level descriptors including intensity, loudness, cepstrum, linear predictor coefficients, pitch and voice quality. Both visual and audio features are then concatenated to create audio-visual features, which are used to predict the evoked emotion. To classify the movie clips into the corresponding affective response categories, we propose two approaches based on deep neural network models. The first one is based on fully connected layers without memory on the time component, the second incorporates the sequential dependency with a long short-term memory recurrent neural network (LSTM). We perform a thorough analysis of the importance of each feature set. Our experiments reveal that in our set-up, predicting emotions at each time step independently gives slightly better accuracy performance than with the LSTM. Interestingly, we also observe that the optical flow is more informative than the RGB in videos, and overall, models using audio features are more accurate than those based on video features when making the final prediction of evoked emotions.",0
"The aim of this research is to create and assess models that can predict the emotional responses of viewers while watching movie clips using multiple modes of input. We have developed hybrid models that utilize both audio and visual data from the clips. To achieve this, we have extracted features from RGB frames and optical flow using pre-trained neural networks to capture image content and motion for the video model. For the audio model, we have computed a set of low-level descriptors consisting of intensity, loudness, cepstrum, linear predictor coefficients, pitch, and voice quality. We have then combined these audio and visual features to generate audio-visual features that can predict the evoked emotions. To classify the movie clips into the corresponding affective response categories, we have proposed two approaches using deep neural network models. One approach is based on fully connected layers without memory on the time component, and the other incorporates the sequential dependency with a long short-term memory recurrent neural network (LSTM). In our experiments, we have analyzed the importance of each feature set and found that predicting emotions independently at each time step gives slightly better accuracy performance than with the LSTM. Furthermore, we have observed that optical flow is more informative than RGB in videos, and models that use audio features are more accurate than those based on video features when making the final prediction of evoked emotions.",1
"We focus on the word-level visual lipreading, which requires to decode the word from the speaker's video. Recently, many state-of-the-art visual lipreading methods explore the end-to-end trainable deep models, involving the use of 2D convolutional networks (e.g., ResNet) as the front-end visual feature extractor and the sequential model (e.g., Bi-LSTM or Bi-GRU) as the back-end. Although a deep 2D convolution neural network can provide informative image-based features, it ignores the temporal motion existing between the adjacent frames. In this work, we investigate the spatial-temporal capacity power of I3D (Inflated 3D ConvNet) for visual lipreading. We demonstrate that, after pre-trained on the large-scale video action recognition dataset (e.g., Kinetics), our models show a considerable improvement of performance on the task of lipreading. A comparison between a set of video model architectures and input data representation is also reported. Our extensive experiments on LRW shows that a two-stream I3D model with RGB video and optical flow as the inputs achieves the state-of-the-art performance.",0
"Our focus is on visual lipreading at the word level, which involves decoding the speaker's video to identify the spoken word. Recently, state-of-the-art visual lipreading techniques have used end-to-end trainable deep models, utilizing 2D convolutional networks (such as ResNet) as the visual feature extractor at the front-end and sequential models (such as Bi-LSTM or Bi-GRU) at the back-end. However, deep 2D convolution neural networks neglect the temporal motion between adjacent frames, even though they provide informative image-based features. In this study, we explore the spatial-temporal capacity of I3D (Inflated 3D ConvNet) for visual lipreading. Our pre-trained models on the large-scale video action recognition dataset (Kinetics) show significant improvements in lipreading performance. We also compare various video model architectures and input data representations. Our extensive experiments on LRW demonstrate that a two-stream I3D model with RGB video and optical flow inputs achieves superior performance, making it the new state-of-the-art.",1
"Predicting depth from a monocular video sequence is an important task for autonomous driving. Although it has advanced considerably in the past few years, recent methods based on convolutional neural networks (CNNs) discard temporal coherence in the video sequence and estimate depth independently for each frame, which often leads to undesired inconsistent results over time. To address this problem, we propose to memorize temporal consistency in the video sequence, and leverage it for the task of depth prediction. To this end, we introduce a two-stream CNN with a flow-guided memory module, where each stream encodes visual and temporal features, respectively. The memory module, implemented using convolutional gated recurrent units (ConvGRUs), inputs visual and temporal features sequentially together with optical flow tailored to our task. It memorizes trajectories of individual features selectively and propagates spatial information over time, enforcing a long-term temporal consistency to prediction results. We evaluate our method on the KITTI benchmark dataset in terms of depth prediction accuracy, temporal consistency and runtime, and achieve a new state of the art. We also provide an extensive experimental analysis, clearly demonstrating the effectiveness of our approach to memorizing temporal consistency for depth prediction.",0
"Autonomous driving heavily relies on accurately predicting depth from a monocular video sequence. Despite significant advancements in recent years, current methods utilizing convolutional neural networks (CNNs) discard temporal coherence and estimate depth independently for each frame, leading to inconsistent results over time. To overcome this issue, we propose a two-stream CNN with a flow-guided memory module that memorizes temporal consistency in the video sequence to aid in depth prediction. The memory module, comprising convolutional gated recurrent units (ConvGRUs), selectively memorizes trajectories of individual features and propagates spatial information over time, promoting long-term temporal consistency. Our approach achieves state-of-the-art results in depth prediction accuracy, temporal consistency, and runtime on the KITTI benchmark dataset. Our extensive experimental analysis confirms the effectiveness of our method in memorizing temporal consistency for depth prediction.",1
"We present a method for decomposing the 3D scene flow observed from a moving stereo rig into stationary scene elements and dynamic object motion. Our unsupervised learning framework jointly reasons about the camera motion, optical flow, and 3D motion of moving objects. Three cooperating networks predict stereo matching, camera motion, and residual flow, which represents the flow component due to object motion and not from camera motion. Based on rigid projective geometry, the estimated stereo depth is used to guide the camera motion estimation, and the depth and camera motion are used to guide the residual flow estimation. We also explicitly estimate the 3D scene flow of dynamic objects based on the residual flow and scene depth. Experiments on the KITTI dataset demonstrate the effectiveness of our approach and show that our method outperforms other state-of-the-art algorithms on the optical flow and visual odometry tasks.",0
"Our approach involves breaking down the 3D scene flow captured by a moving stereo rig into two components: stationary scene elements and dynamic object motion. This is achieved through an unsupervised learning framework that considers camera motion, optical flow, and 3D motion of moving objects. Three networks collaborate to predict stereo matching, camera motion, and residual flow, which captures flow due to object motion and not camera motion. Our method uses rigid projective geometry to guide the camera motion estimation based on the estimated stereo depth, while the residual flow estimation is guided by depth and camera motion. Additionally, we explicitly estimate the 3D scene flow of dynamic objects using residual flow and scene depth. We conducted experiments on the KITTI dataset, which demonstrate the superiority of our approach over other state-of-the-art algorithms on optical flow and visual odometry tasks.",1
"Dashboard cameras capture a tremendous amount of driving scene video each day. These videos are purposefully coupled with vehicle sensing data, such as from the speedometer and inertial sensors, providing an additional sensing modality for free. In this work, we leverage the large-scale unlabeled yet naturally paired data for visual representation learning in the driving scenario. A representation is learned in an end-to-end self-supervised framework for predicting dense optical flow from a single frame with paired sensing data. We postulate that success on this task requires the network to learn semantic and geometric knowledge in the ego-centric view. For example, forecasting a future view to be seen from a moving vehicle requires an understanding of scene depth, scale, and movement of objects. We demonstrate that our learned representation can benefit other tasks that require detailed scene understanding and outperforms competing unsupervised representations on semantic segmentation.",0
"Every day, dashboard cameras capture an enormous amount of driving scene footage, which is intentionally combined with vehicle sensing data, such as from the speedometer and inertial sensors, to create an additional sensing method for free. Our research takes advantage of this vast amount of unlabeled yet naturally paired data for visual representation learning in driving scenarios. We teach the network to predict dense optical flow from a single frame with paired sensing data by using an end-to-end self-supervised framework. In order to succeed in this task, we assume that the network must learn semantic and geometric knowledge in the ego-centric view. For instance, predicting a future view from a moving vehicle necessitates an understanding of scene depth, scale, and object movement. Our study demonstrates that our learned representation can benefit other tasks that require detailed scene comprehension and outperforms competing unsupervised representations on semantic segmentation.",1
"We propose a learning-based method that solves monocular stereo and can be extended to fuse depth information from multiple target frames. Given two unconstrained images from a monocular camera with known intrinsic calibration, our network estimates relative camera poses and the depth map of the source image. The core contribution of the proposed method is threefold. First, a network is tailored for static scenes that jointly estimates the optical flow and camera motion. By the joint estimation, the optical flow search space is gradually reduced resulting in an efficient and accurate flow estimation. Second, a novel triangulation layer is proposed to encode the estimated optical flow and camera motion while avoiding common numerical issues caused by epipolar. Third, beyond two-view depth estimation, we further extend the above networks to fuse depth information from multiple target images and estimate the depth map of the source image. To further benefit the research community, we introduce tools to generate photorealistic structure-from-motion datasets such that deep networks can be well trained and evaluated. The proposed method is compared with previous methods and achieves state-of-the-art results within less time. Images from real-world applications and Google Earth are used to demonstrate the generalization ability of the method.",0
"Our proposal introduces a learning-based method for solving monocular stereo that can be expanded to incorporate depth information from multiple target frames. The method involves using two unconstrained images from a monocular camera with known intrinsic calibration, where a network estimates the relative camera poses and depth map of the source image. The proposed method has three core contributions. Firstly, a network specifically designed for static scenes jointly estimates optical flow and camera motion, reducing the search space and improving flow estimation. Secondly, a novel triangulation layer encodes the estimated optical flow and camera motion while avoiding numerical issues caused by epipolar. Thirdly, the network is extended beyond two-view depth estimation to fuse depth information from multiple target images and estimate the depth map of the source image. Additionally, we provide tools to generate photorealistic structure-from-motion datasets for training and evaluation purposes. Our method outperforms previous methods and achieves state-of-the-art results in less time. The generalization ability of the method is demonstrated using images from real-world applications and Google Earth.",1
"We present GLNet, a self-supervised framework for learning depth, optical flow, camera pose and intrinsic parameters from monocular video - addressing the difficulty of acquiring realistic ground-truth for such tasks. We propose three contributions: 1) we design new loss functions that capture multiple geometric constraints (eg. epipolar geometry) as well as an adaptive photometric loss that supports multiple moving objects, rigid and non-rigid, 2) we extend the model such that it predicts camera intrinsics, making it applicable to uncalibrated video, and 3) we propose several online refinement strategies that rely on the symmetry of our self-supervised loss in training and testing, in particular optimizing model parameters and/or the output of different tasks, thus leveraging their mutual interactions. The idea of jointly optimizing the system output, under all geometric and photometric constraints can be viewed as a dense generalization of classical bundle adjustment. We demonstrate the effectiveness of our method on KITTI and Cityscapes, where we outperform previous self-supervised approaches on multiple tasks. We also show good generalization for transfer learning in YouTube videos.",0
"GLNet is a framework that uses self-supervision to learn depth, optical flow, camera pose, and intrinsic parameters from monocular video. This approach addresses the challenge of obtaining realistic ground-truth data for these tasks. Our framework has three key contributions. Firstly, we introduce new loss functions that capture multiple geometric constraints, such as epipolar geometry, and an adaptive photometric loss that accommodates multiple moving objects, rigid and non-rigid. Secondly, we extend the model to predict camera intrinsics, making it suitable for uncalibrated video. Thirdly, we propose several online refinement strategies that optimize model parameters and/or the output of different tasks by leveraging their mutual interactions. This dense optimization approach can be seen as a generalization of classical bundle adjustment. We demonstrate the effectiveness of GLNet on KITTI and Cityscapes datasets, outperforming previous self-supervised methods on multiple tasks. Our framework also shows good generalization for transfer learning in YouTube videos.",1
"Dense prediction tasks typically employ encoder-decoder architectures, but the prevalent convolutions in the decoder are not image-adaptive and can lead to boundary artifacts. Different generalized convolution operations have been introduced to counteract this. We go beyond these by leveraging guidance data to redefine their inherent notion of proximity. Our proposed network layer builds on the permutohedral lattice, which performs sparse convolutions in a high-dimensional space allowing for powerful non-local operations despite small filters. Multiple features with different characteristics span this permutohedral space. In contrast to prior work, we learn these features in a task-specific manner by generalizing the basic permutohedral operations to learnt feature representations. As the resulting objective is complex, a carefully designed framework and learning procedure are introduced, yielding rich feature embeddings in practice. We demonstrate the general applicability of our approach in different joint upsampling tasks. When adding our network layer to state-of-the-art networks for optical flow and semantic segmentation, boundary artifacts are removed and the accuracy is improved.",0
"Tasks requiring dense prediction often use encoder-decoder architectures, but the common convolutions in the decoder may not adapt to images and can create boundary artifacts. To address this, various generalized convolution operations have been introduced. However, we propose a novel approach that uses guidance data to redefine the inherent proximity concept. Our network layer builds on the permutohedral lattice, which performs sparse convolutions in a high-dimensional space and enables non-local operations despite using small filters. Multiple features with distinct characteristics are present in this permutohedral space. Unlike earlier work, we learn these features in a task-specific manner by generalizing the basic permutohedral operations to learned feature representations. Since the resulting objective is complex, we introduce a well-designed framework and learning procedure to produce rich feature embeddings in practice. Our approach is generally applicable in various joint upsampling tasks, and when added to state-of-the-art networks for optical flow and semantic segmentation, it removes boundary artifacts and improves accuracy.",1
"Prediction and interpolation for long-range video data involves the complex task of modeling motion trajectories for each visible object, occlusions and dis-occlusions, as well as appearance changes due to viewpoint and lighting. Optical flow based techniques generalize but are suitable only for short temporal ranges. Many methods opt to project the video frames to a low dimensional latent space, achieving long-range predictions. However, these latent representations are often non-interpretable, and therefore difficult to manipulate. This work poses video prediction and interpolation as unsupervised latent structure inference followed by a temporal prediction in this latent space. The latent representations capture foreground semantics without explicit supervision such as keypoints or poses. Further, as each landmark can be mapped to a coordinate indicating where a semantic part is positioned, we can reliably interpolate within the coordinate domain to achieve predictable motion interpolation. Given an image decoder capable of mapping these landmarks back to the image domain, we are able to achieve high-quality long-range video interpolation and extrapolation by operating on the landmark representation space.",0
"The complex task of predicting and interpolating long-range video data involves modeling motion trajectories for visible objects, accounting for occlusions, dis-occlusions, and changes in appearance due to lighting and viewpoint. While optical flow techniques can generalize, they are only suitable for short temporal ranges. Some methods use a low dimensional latent space to achieve long-range predictions, but these representations are often difficult to interpret and manipulate. This work suggests an approach that involves unsupervised latent structure inference followed by temporal prediction in the latent space. The latent representations capture foreground semantics without explicit supervision, and mapping landmarks to coordinates allows for reliable interpolation in the coordinate domain. By using an image decoder to map landmarks back to the image domain, high-quality long-range video interpolation and extrapolation can be achieved by operating in the landmark representation space.",1
"Motion estimation (ME) and motion compensation (MC) have been widely used for classical video frame interpolation systems over the past decades. Recently, a number of data-driven frame interpolation methods based on convolutional neural networks have been proposed. However, existing learning based methods typically estimate either flow or compensation kernels, thereby limiting performance on both computational efficiency and interpolation accuracy. In this work, we propose a motion estimation and compensation driven neural network for video frame interpolation. A novel adaptive warping layer is developed to integrate both optical flow and interpolation kernels to synthesize target frame pixels. This layer is fully differentiable such that both the flow and kernel estimation networks can be optimized jointly. The proposed model benefits from the advantages of motion estimation and compensation methods without using hand-crafted features. Compared to existing methods, our approach is computationally efficient and able to generate more visually appealing results. Furthermore, the proposed MEMC-Net can be seamlessly adapted to several video enhancement tasks, e.g., super-resolution, denoising, and deblocking. Extensive quantitative and qualitative evaluations demonstrate that the proposed method performs favorably against the state-of-the-art video frame interpolation and enhancement algorithms on a wide range of datasets.",0
"Classical video frame interpolation systems have relied on motion estimation (ME) and motion compensation (MC) for many years. Recently, convolutional neural networks have been used for data-driven frame interpolation methods. However, these methods usually estimate either flow or compensation kernels, which reduces both computational efficiency and interpolation accuracy. To address this issue, we propose a motion estimation and compensation driven neural network for video frame interpolation. Our approach utilizes an adaptive warping layer that integrates optical flow and interpolation kernels to synthesize target frame pixels. This layer is fully differentiable, allowing the flow and kernel estimation networks to be optimized jointly. Our model does not require hand-crafted features, making it computationally efficient and capable of producing visually appealing results. Additionally, the proposed MEMC-Net can be easily adapted to other video enhancement tasks such as super-resolution, denoising, and deblocking. Extensive evaluations show that our method outperforms state-of-the-art video frame interpolation and enhancement algorithms on various datasets.",1
"Pedestrian action recognition and intention prediction is one of the core issues in the field of autonomous driving. In this research field, action recognition is one of the key technologies. A large number of scholars have done a lot of work to im-prove the accuracy of the algorithm for the task. However, there are relatively few studies and improvements in the computational complexity of algorithms and sys-tem real-time. In the autonomous driving application scenario, the real-time per-formance and ultra-low latency of the algorithm are extremely important evalua-tion indicators, which are directly related to the availability and safety of the au-tonomous driving system. To this end, we construct a bypass enhanced RGB flow model, which combines the previous two-branch algorithm to extract RGB feature information and optical flow feature information respectively. In the train-ing phase, the two branches are merged by distillation method, and the bypass enhancement is combined in the inference phase to ensure accuracy. The real-time behavior of the behavior recognition algorithm is significantly improved on the premise that the accuracy does not decrease. Experiments confirm the superiority and effectiveness of our algorithm.",0
"Recognizing and predicting pedestrian actions is a crucial aspect of autonomous driving. Action recognition is a key technology in this research field, with many scholars striving to enhance the accuracy of algorithms. However, there is limited focus on improving the computational complexity of algorithms and achieving real-time system performance. In autonomous driving scenarios, the real-time performance and ultra-low latency of algorithms are crucial indicators for evaluating availability and safety. To address this, we developed a bypass enhanced RGB flow model that combines two-branch algorithms for extracting RGB and optical flow feature information. By merging the branches using a distillation method during training and implementing bypass enhancement during inference, we ensured accuracy while significantly improving real-time behavior recognition. Our experiments confirmed the effectiveness and superiority of our algorithm.",1
"Due to better video quality and higher frame rate, the performance of multiple object tracking issues has been greatly improved in recent years. However, in real application scenarios, camera motion and noisy per frame detection results degrade the performance of trackers significantly. High-speed and high-quality multiple object trackers are still in urgent demand. In this paper, we propose a new multiple object tracker following the popular tracking-by-detection scheme. We tackle the camera motion problem with an optical flow network and utilize an auxiliary tracker to deal with the missing detection problem. Besides, we use both the appearance and motion information to improve the matching quality. The experimental results on the VisDrone-MOT dataset show that our approach can improve the performance of multiple object tracking significantly while achieving a high efficiency.",0
"Recent years have witnessed significant improvements in the performance of multiple object tracking issues, thanks to better video quality and higher frame rate. However, the real application scenarios are plagued with camera motion and noisy per frame detection results, which considerably degrade the performance of trackers. Consequently, there is a pressing need for high-speed and high-quality multiple object trackers. To address this challenge, the present paper proposes a new multiple object tracker that adopts the popular tracking-by-detection scheme. The proposed tracker employs an optical flow network to tackle camera motion and an auxiliary tracker to handle missing detection cases. Furthermore, both appearance and motion information are utilized to enhance the matching quality. The experimental evaluation on the VisDrone-MOT dataset demonstrates that our approach significantly improves the performance of multiple object tracking while ensuring high efficiency.",1
"This paper presents a novel obstacle avoidance system for road robots equipped with RGB-D sensor that captures scenes of its way forward. The purpose of the system is to have road robots move around autonomously and constantly without any collision even with small obstacles, which are often missed by existing solutions. For each input RGB-D image, the system uses a new two-stage semantic segmentation network followed by the morphological processing to generate the accurate semantic map containing road and obstacles. Based on the map, the local path planning is applied to avoid possible collision. Additionally, optical flow supervision and motion blurring augmented training scheme is applied to improve temporal consistency between adjacent frames and overcome the disturbance caused by camera shake. Various experiments are conducted to show that the proposed architecture obtains high performance both in indoor and outdoor scenarios.",0
"In this article, a unique system for avoiding obstacles is presented for road robots that utilize an RGB-D sensor to capture images of their surroundings ahead. The aim of this system is to enable autonomous movement of road robots, ensuring that they can move continuously without any collisions, even with small obstacles that may be missed by existing solutions. To achieve this, the system utilizes a new two-stage semantic segmentation network and morphological processing to generate an accurate semantic map containing information about the road and obstacles. This map is then used to plan a local path and avoid potential collisions. To improve temporal consistency between frames and overcome the effects of camera shake, an optical flow supervision and motion blurring augmented training scheme is also applied. The proposed architecture is shown to perform well in both indoor and outdoor scenarios through various experiments.",1
"Detecting action units (AUs) on human faces is challenging because various AUs make subtle facial appearance change over various regions at different scales. Current works have attempted to recognize AUs by emphasizing important regions. However, the incorporation of expert prior knowledge into region definition remains under-exploited, and current AU detection approaches do not use regional convolutional neural networks (R-CNN) with expert prior knowledge to directly focus on AU-related regions adaptively. By incorporating expert prior knowledge, we propose a novel R-CNN based model named AU R-CNN. The proposed solution offers two main contributions: (1) AU R-CNN directly observes different facial regions, where various AUs are located. Specifically, we define an AU partition rule which encodes the expert prior knowledge into the region definition and RoI-level label definition. This design produces considerably better detection performance than existing approaches. (2) We integrate various dynamic models (including convolutional long short-term memory, two stream network, conditional random field, and temporal action localization network) into AU R-CNN and then investigate and analyze the reason behind the performance of dynamic models. Experiment results demonstrate that \textit{only} static RGB image information and no optical flow-based AU R-CNN surpasses the one fused with dynamic models. AU R-CNN is also superior to traditional CNNs that use the same backbone on varying image resolutions. State-of-the-art recognition performance of AU detection is achieved. The complete network is end-to-end trainable. Experiments on BP4D and DISFA datasets show the effectiveness of our approach. The implementation code is available online.",0
"Detecting action units (AUs) on human faces poses a challenge due to the subtle changes that occur over different regions and scales. Previous attempts at AU recognition have focused on identifying important regions, but incorporating expert prior knowledge into region definition has not been fully explored. Current AU detection methods also fail to utilize regional convolutional neural networks (R-CNN) to adaptively target AU-related regions. To address these limitations, we introduce a novel R-CNN model called AU R-CNN that incorporates expert prior knowledge and offers two key contributions. Firstly, AU R-CNN directly observes facial regions where AUs are located using an AU partition rule that encodes expert prior knowledge into region and RoI-level label definitions. This design achieves better detection performance than existing approaches. Secondly, we integrate various dynamic models into AU R-CNN and analyze the reason behind their performance. Experiments show that AU R-CNN with only static RGB image information surpasses the one fused with dynamic models, and achieves state-of-the-art recognition performance. Our approach is end-to-end trainable and effective on BP4D and DISFA datasets. Implementation code is available online.",1
"We present a 3D Convolutional Neural Networks (CNNs) based single shot detector for spatial-temporal action detection tasks. Our model includes: (1) two short-term appearance and motion streams, with single RGB and optical flow image input separately, in order to capture the spatial and temporal information for the current frame; (2) two long-term 3D ConvNet based stream, working on sequences of continuous RGB and optical flow images to capture the context from past frames. Our model achieves strong performance for action detection in video and can be easily integrated into any current two-stream action detection methods. We report a frame-mAP of 71.30% on the challenging UCF101-24 actions dataset, achieving the state-of-the-art result of the one-stage methods. To the best of our knowledge, our work is the first system that combined 3D CNN and SSD in action detection tasks.",0
"We have developed a single shot detector for spatial-temporal action detection tasks using 3D Convolutional Neural Networks (CNNs). Our model comprises two short-term appearance and motion streams, which capture spatial and temporal information for the current frame, and two long-term 3D ConvNet based streams that operate on sequences of continuous RGB and optical flow images to capture context from previous frames. Our model performs well in action detection in videos and can be easily integrated into existing two-stream action detection approaches. We achieved a frame-mAP of 71.30% on the challenging UCF101-24 actions dataset, which is the state-of-the-art result for one-stage methods. Our work is the first to combine 3D CNN and SSD in action detection tasks.",1
"In the recent year, state-of-the-art for facial micro-expression recognition have been significantly advanced by deep neural networks. The robustness of deep learning has yielded promising performance beyond that of traditional handcrafted approaches. Most works in literature emphasized on increasing the depth of networks and employing highly complex objective functions to learn more features. In this paper, we design a Shallow Triple Stream Three-dimensional CNN (STSTNet) that is computationally light whilst capable of extracting discriminative high level features and details of micro-expressions. The network learns from three optical flow features (i.e., optical strain, horizontal and vertical optical flow fields) computed based on the onset and apex frames of each video. Our experimental results demonstrate the effectiveness of the proposed STSTNet, which obtained an unweighted average recall rate of 0.7605 and unweighted F1-score of 0.7353 on the composite database consisting of 442 samples from the SMIC, CASME II and SAMM databases.",0
"Deep neural networks have significantly advanced the state-of-the-art for facial micro-expression recognition in recent years, surpassing traditional handcrafted approaches in terms of performance. Previous studies have focused on increasing network depth and using complex objective functions to learn more features. However, we propose a computationally light Shallow Triple Stream Three-dimensional CNN (STSTNet) that can extract discriminative high-level features and micro-expression details. Our network utilizes three optical flow features (optical strain, horizontal and vertical optical flow fields) based on onset and apex video frames. Our experimental results demonstrate the effectiveness of STSTNet, achieving an unweighted average recall rate of 0.7605 and unweighted F1-score of 0.7353 on a composite database of 442 samples from SMIC, CASME II, and SAMM databases.",1
"Avoiding bottleneck situations in crowds is critical for the safety and comfort of people at large events or in public transportation. Based on the work of Lagrangian motion analysis we propose a novel video-based bottleneckdetector by identifying characteristic stowage patterns in crowd-movements captured by optical flow fields. The Lagrangian framework allows to assess complex timedependent crowd-motion dynamics at large temporal scales near the bottleneck by two dimensional Lagrangian fields. In particular we propose long-term temporal filtered Finite Time Lyapunov Exponents (FTLE) fields that provide towards a more global segmentation of the crowd movements and allows to capture its deformations when a crowd is passing a bottleneck. Finally, these deformations are used for an automatic spatio-temporal detection of such situations. The performance of the proposed approach is shown in extensive evaluations on the existing J\""ulich and AGORASET datasets, that we have updated with ground truth data for spatio-temporal bottleneck analysis.",0
"Ensuring the safety and comfort of individuals in large gatherings or public transportation requires the avoidance of bottleneck scenarios. To address this, we suggest a new bottleneck detection method that utilizes Lagrangian motion analysis to identify distinct stowage patterns in crowd movements captured by optical flow fields. This approach enables the assessment of complex time-dependent crowd-motion dynamics at large temporal scales near the bottleneck through two-dimensional Lagrangian fields. We suggest using long-term temporal filtered Finite Time Lyapunov Exponents (FTLE) fields to achieve a more comprehensive segmentation of the crowd's movements and to capture its deformations as it passes through a bottleneck. These deformations are then used for an automatic spatio-temporal detection of such situations. We demonstrate the effectiveness of this method by conducting extensive evaluations on the existing J""ulich and AGORASET datasets, which we supplemented with ground truth data for spatio-temporal bottleneck analysis.",1
"Event cameras are vision sensors that record asynchronous streams of per-pixel brightness changes, referred to as ""events"". They have appealing advantages over frame-based cameras for computer vision, including high temporal resolution, high dynamic range, and no motion blur. Due to the sparse, non-uniform spatiotemporal layout of the event signal, pattern recognition algorithms typically aggregate events into a grid-based representation and subsequently process it by a standard vision pipeline, e.g., Convolutional Neural Network (CNN). In this work, we introduce a general framework to convert event streams into grid-based representations through a sequence of differentiable operations. Our framework comes with two main advantages: (i) allows learning the input event representation together with the task dedicated network in an end to end manner, and (ii) lays out a taxonomy that unifies the majority of extant event representations in the literature and identifies novel ones. Empirically, we show that our approach to learning the event representation end-to-end yields an improvement of approximately 12% on optical flow estimation and object recognition over state-of-the-art methods.",0
"Event cameras are sensors that capture asynchronous streams of changes in brightness on a per-pixel basis, which are called ""events"". They offer several advantages over traditional frame-based cameras for computer vision, such as a high dynamic range, high temporal resolution, and no motion blur. However, due to the sparse and non-uniform nature of the event signal, pattern recognition algorithms usually group events into a grid-based format and process them using a standard vision pipeline, such as a Convolutional Neural Network (CNN). This study presents a general framework that can convert event streams into grid-based representations using a sequence of differentiable operations. The framework has two main benefits: (i) it enables learning the input event representation and dedicated network for a task in an end-to-end manner, and (ii) it provides a taxonomy that unifies existing event representations in the literature and identifies new ones. The study demonstrates that the proposed approach for learning the event representation end-to-end yields a 12% improvement in optical flow estimation and object recognition over state-of-the-art methods.",1
"Video objection detection (VID) has been a rising research direction in recent years. A central issue of VID is the appearance degradation of video frames caused by fast motion. This problem is essentially ill-posed for a single frame. Therefore, aggregating features from other frames becomes a natural choice. Existing methods rely heavily on optical flow or recurrent neural networks for feature aggregation. However, these methods emphasize more on the temporally nearby frames. In this work, we argue that aggregating features in the full-sequence level will lead to more discriminative and robust features for video object detection. To achieve this goal, we devise a novel Sequence Level Semantics Aggregation (SELSA) module. We further demonstrate the close relationship between the proposed method and the classic spectral clustering method, providing a novel view for understanding the VID problem. We test the proposed method on the ImageNet VID and the EPIC KITCHENS dataset and achieve new state-of-the-art results. Our method does not need complicated postprocessing methods such as Seq-NMS or Tubelet rescoring, which keeps the pipeline simple and clean.",0
"Recently, Video objection detection (VID) has become a popular research area. One of the main challenges in VID is the deterioration of video frames due to fast motion, which is a complex problem for a single frame. Therefore, it is natural to aggregate features from other frames. Existing methods rely heavily on optical flow or recurrent neural networks, which focus on nearby frames. However, in this study, we argue that aggregating features at the full-sequence level will result in more distinctive and durable features for video object detection. To achieve this, we introduce a novel Sequence Level Semantics Aggregation (SELSA) module. We also show a new perspective for understanding the VID problem by highlighting the relationship between our approach and the classic spectral clustering method. Our proposed method achieves state-of-the-art results on the ImageNet VID and the EPIC KITCHENS dataset without the need for complicated postprocessing techniques such as Seq-NMS or Tubelet rescoring, keeping the pipeline straightforward and concise.",1
"As a milestone for video object segmentation, one-shot video object segmentation (OSVOS) has achieved a large margin compared to the conventional optical-flow based methods regarding to the segmentation accuracy. Its excellent performance mainly benefit from the three-step training mechanism, that are: (1) acquiring object features on the base dataset (i.e. ImageNet), (2) training the parent network on the training set of the target dataset (i.e. DAVIS-2016) to be capable of differentiating the object of interest from the background. (3) online fine-tuning the interested object on the first frame of the target test set to overfit its appearance, then the model can be utilized to segment the same object in the rest frames of that video. In this paper, we argue that for the step (2), OSVOS has the limitation to 'overemphasize' the generic semantic object information while 'dilute' the instance cues of the object(s), which largely block the whole training process. Through adding a common module, video loss, which we formulate with various forms of constraints (including weighted BCE loss, high-dimensional triplet loss, as well as a novel mixed instance-aware video loss), to train the parent network in the step (2), the network is then better prepared for the step (3), i.e. online fine-tuning on the target instance. Through extensive experiments using different network structures as the backbone, we show that the proposed video loss module can improve the segmentation performance significantly, compared to that of OSVOS. Meanwhile, since video loss is a common module, it can be generalized to other fine-tuning based methods and similar vision tasks such as depth estimation and saliency detection.",0
"The one-shot video object segmentation (OSVOS) method has surpassed conventional optical-flow based techniques in segmentation accuracy. This is mainly due to its three-step training process: (1) acquiring object features from the base dataset ImageNet, (2) training the parent network on the target dataset DAVIS-2016 to differentiate the object of interest from the background, and (3) online fine-tuning of the interested object on the first frame of the target test set to overfit its appearance. However, we argue that step (2) of OSVOS overemphasizes generic semantic object information, diluting the instance cues of the object(s), which hinders the training process. To overcome this, we propose adding a video loss module with various constraints, such as weighted BCE loss, high-dimensional triplet loss, and a novel mixed instance-aware video loss, to train the parent network in step (2). This prepares the network for step (3) and significantly improves segmentation performance compared to OSVOS. Moreover, video loss is a common module that can be applied to other fine-tuning based methods and similar vision tasks like depth estimation and saliency detection.",1
"This draft summarizes some basics about geometric computer vision needed to implement efficient computer vision algorithms for applications that use measurements from at least one digital camera mounted on a moving platform with a special focus on automotive applications processing image streams taken from cameras mounted on a car. Our intention is twofold: On the one hand, we would like to introduce well-known basic geometric relations in a compact way that can also be found in lecture books about geometric computer vision like [1, 2]. On the other hand, we would like to share some experience about subtleties that should be taken into account in order to set up quite simple but robust and fast vision algorithms that are able to run in real time. We added a conglomeration of literature, we found to be relevant when implementing basic algorithms like optical flow, visual odometry and structure from motion. The reader should get some feeling about how the estimates of these algorithms are interrelated, which parts of the algorithms are critical in terms of robustness and what kind of additional assumptions can be useful to constrain the solution space of the underlying usually non-convex optimization problems.",0
"The aim of this document is to provide an overview of the fundamentals of geometric computer vision necessary for creating efficient computer vision algorithms for applications utilizing measurements from one or more digital cameras mounted on a moving platform, with a particular emphasis on processing image streams captured by cameras installed on vehicles. Our objective is two-fold: firstly, we seek to present an abridged version of commonly known geometric relationships that are also featured in textbooks on geometric computer vision, such as [1, 2]. Secondly, we aim to share our insights on the intricacies that must be taken into consideration when devising simple yet robust and quick vision algorithms that can operate in real-time. We have included a collection of relevant literature that we came across while implementing elementary algorithms like optical flow, visual odometry, and structure from motion. Our readers will gain an understanding of how these algorithms' estimations are interconnected, the critical aspects of the algorithms' robustness, and the additional suppositions that can be advantageous in limiting the solution space of the typically non-convex optimization problems at their core.",1
"In this paper, we revive the use of old-fashioned handcrafted video representations for action recognition and put new life into these techniques via a CNN-based hallucination step. Despite of the use of RGB and optical flow frames, the I3D model (amongst others) thrives on combining its output with the Improved Dense Trajectory (IDT) and extracted with its low-level video descriptors encoded via Bag-of-Words (BoW) and Fisher Vectors (FV). Such a fusion of CNNs and handcrafted representations is time-consuming due to pre-processing, descriptor extraction, encoding and tuning parameters. Thus, we propose an end-to-end trainable network with streams which learn the IDT-based BoW/FV representations at the training stage and are simple to integrate with the I3D model. Specifically, each stream takes I3D feature maps ahead of the last 1D conv. layer and learns to `translate' these maps to BoW/FV representations. Thus, our model can hallucinate and use such synthesized BoW/FV representations at the testing stage. We show that even features of the entire I3D optical flow stream can be hallucinated thus simplifying the pipeline. Our model saves 20-55h of computations and yields state-of-the-art results on four publicly available datasets.",0
"This paper proposes the revival of old-fashioned handcrafted video representations for action recognition, utilizing a CNN-based hallucination step to enhance these techniques. The I3D model, which combines RGB and optical flow frames, integrates its output with Improved Dense Trajectory (IDT) and low-level video descriptors encoded via Bag-of-Words (BoW) and Fisher Vectors (FV) to achieve success. However, this fusion of CNNs and handcrafted representations is time-consuming due to pre-processing, descriptor extraction, encoding, and tuning parameters. To address this issue, the authors suggest an end-to-end trainable network with streams that learn the IDT-based BoW/FV representations during the training stage, making it simple to integrate with the I3D model. The proposed model can hallucinate and use synthesized BoW/FV representations during the testing stage, saving 20-55h of computations and achieving state-of-the-art results on four publicly available datasets. Additionally, the model can hallucinate even the features of the entire I3D optical flow stream, simplifying the pipeline.",1
"Transferring image-based object detectors to the domain of videos remains a challenging problem. Previous efforts mostly exploit optical flow to propagate features across frames, aiming to achieve a good trade-off between accuracy and efficiency. However, introducing an extra model to estimate optical flow can significantly increase the overall model size. The gap between optical flow and high-level features can also hinder it from establishing spatial correspondence accurately. Instead of relying on optical flow, this paper proposes a novel module called Progressive Sparse Local Attention (PSLA), which establishes the spatial correspondence between features across frames in a local region with progressively sparser stride and uses the correspondence to propagate features. Based on PSLA, Recursive Feature Updating (RFU) and Dense Feature Transforming (DenseFT) are proposed to model temporal appearance and enrich feature representation respectively in a novel video object detection framework. Experiments on ImageNet VID show that our method achieves the best accuracy compared to existing methods with smaller model size and acceptable runtime speed.",0
"The task of transferring object detectors from images to videos remains a challenging endeavor. Previous attempts have primarily relied on optical flow to disseminate features across frames, with the aim of striking a balance between accuracy and efficiency. However, including an additional model to estimate optical flow can significantly augment the overall model size, and the disparity between optical flow and high-level features may impede accurate establishment of spatial correspondence. Rather than relying on optical flow, this study proposes a fresh module called Progressive Sparse Local Attention (PSLA), which establishes spatial correspondence between features across frames in a local area with progressively sparser stride and utilizes the correspondence to disseminate features. Recursive Feature Updating (RFU) and Dense Feature Transforming (DenseFT) are introduced based on PSLA to model temporal appearance and enhance feature representation, respectively, in a new video object detection framework. Experiments conducted on ImageNet VID demonstrate that our method achieves the best accuracy compared to existing methods with a smaller model size and acceptable runtime speed.",1
"We study the video super-resolution (SR) problem for facilitating video analytics tasks, e.g. action recognition, instead of for visual quality. The popular action recognition methods based on convolutional networks, exemplified by two-stream networks, are not directly applicable on video of low spatial resolution. This can be remedied by performing video SR prior to recognition, which motivates us to improve the SR procedure for recognition accuracy. Tailored for two-stream action recognition networks, we propose two video SR methods for the spatial and temporal streams respectively. On the one hand, we observe that regions with action are more important to recognition, and we propose an optical-flow guided weighted mean-squared-error loss for our spatial-oriented SR (SoSR) network to emphasize the reconstruction of moving objects. On the other hand, we observe that existing video SR methods incur temporal discontinuity between frames, which also worsens the recognition accuracy, and we propose a siamese network for our temporal-oriented SR (ToSR) training that emphasizes the temporal continuity between consecutive frames. We perform experiments using two state-of-the-art action recognition networks and two well-known datasets--UCF101 and HMDB51. Results demonstrate the effectiveness of our proposed SoSR and ToSR in improving recognition accuracy.",0
"Our focus is on studying the video super-resolution (SR) problem to aid video analytics tasks such as action recognition, rather than solely for visual quality. Existing action recognition methods that depend on convolutional networks, such as two-stream networks, cannot be used directly on low spatial resolution videos. To address this, we propose two video SR methods tailored for two-stream action recognition networks, one for spatial-oriented SR (SoSR) and the other for temporal-oriented SR (ToSR). Our SoSR network emphasizes the reconstruction of moving objects by using an optical-flow guided weighted mean-squared-error loss, recognizing the importance of action regions. Meanwhile, our ToSR network uses a siamese network to emphasize the temporal continuity between consecutive frames, which is often disrupted by existing video SR methods. We evaluated our proposed methods on two well-known datasets, UCF101 and HMDB51, using two state-of-the-art action recognition networks, and the results demonstrate the effectiveness of our proposed SoSR and ToSR in improving recognition accuracy.",1
"When a deep neural network is trained on data with only image-level labeling, the regions activated in each image tend to identify only a small region of the target object. We propose a method of using videos automatically harvested from the web to identify a larger region of the target object by using temporal information, which is not present in the static image. The temporal variations in a video allow different regions of the target object to be activated. We obtain an activated region in each frame of a video, and then aggregate the regions from successive frames into a single image, using a warping technique based on optical flow. The resulting localization maps cover more of the target object, and can then be used as proxy ground-truth to train a segmentation network. This simple approach outperforms existing methods under the same level of supervision, and even approaches relying on extra annotations. Based on VGG-16 and ResNet 101 backbones, our method achieves the mIoU of 65.0 and 67.4, respectively, on PASCAL VOC 2012 test images, which represents a new state-of-the-art.",0
"When data with only image-level labeling is used to train a deep neural network, the activated regions in each image are limited to a small part of the target object. To address this, we propose a solution that involves using videos sourced from the internet, which contain temporal information that is absent in static images. The temporal variations in the videos enable different regions of the target object to be activated. We extract an activated region from every frame of the video, and then combine the regions from successive frames into a single image using a warping technique based on optical flow. This results in localization maps that cover a larger portion of the target object, which can then be utilized as a substitute ground-truth to train a segmentation network. Our approach surpasses existing methods under the same level of supervision and even approaches that rely on additional annotations. Our method, based on VGG-16 and ResNet 101 backbones, achieves an mIoU of 65.0 and 67.4 on PASCAL VOC 2012 test images, respectively, which sets a new state-of-the-art.",1
"Anomaly detection plays in many fields of research, along with the strongly related task of outlier detection, a very important role. Especially within the context of the automated analysis of video material recorded by surveillance cameras, abnormal situations can be of very different nature. For this purpose this work investigates Generative-Adversarial-Network-based methods (GAN) for anomaly detection related to surveillance applications. The focus is on the usage of static camera setups, since this kind of camera is one of the most often used and belongs to the lower price segment. In order to address this task, multiple subtasks are evaluated, including the influence of existing optical flow methods for the incorporation of short-term temporal information, different forms of network setups and losses for GANs, and the use of morphological operations for further performance improvement. With these extension we achieved up to 2.4% better results. Furthermore, the final method reduced the anomaly detection error for GAN-based methods by about 42.8%.",0
"In various research fields, the significance of anomaly detection and outlier detection is highly notable. In particular, when analyzing automated surveillance camera footage, anomalous situations can vary greatly. This study focuses on the use of Generative-Adversarial-Network-based (GAN) techniques for anomaly detection in surveillance applications, with an emphasis on static camera setups, which are common and affordable. The study evaluates multiple subtasks, such as the impact of optical flow techniques for short-term temporal information integration, network setups and losses for GANs, and the use of morphological operations for performance enhancement. By incorporating these extensions, the study achieved a 2.4% improvement in results. Additionally, the final method successfully reduced the anomaly detection error for GAN-based methods by approximately 42.8%.",1
"In this paper we present mono-stixels, a compact environment representation specially designed for dynamic street scenes. Mono-stixels are a novel approach to estimate stixels from a monocular camera sequence instead of the traditionally used stereo depth measurements. Our approach jointly infers the depth, motion and semantic information of the dynamic scene as a 1D energy minimization problem based on optical flow estimates, pixel-wise semantic segmentation and camera motion. The optical flow of a stixel is described by a homography. By applying the mono-stixel model the degrees of freedom of a stixel-homography are reduced to only up to two degrees of freedom. Furthermore, we exploit a scene model and semantic information to handle moving objects. In our experiments we use the public available DeepFlow for optical flow estimation and FCN8s for the semantic information as inputs and show on the KITTI 2015 dataset that mono-stixels provide a compact and reliable depth reconstruction of both the static and moving parts of the scene. Thereby, mono-stixels overcome the limitation to static scenes of previous structure-from-motion approaches.",0
"The focus of this paper is the presentation of mono-stixels, a novel method to represent dynamic street scenes in a compact form. The uniqueness of mono-stixels lies in their estimation from a monocular camera sequence, unlike traditional approaches that use stereo depth measurements. Our approach involves the use of optical flow, pixel-wise semantic segmentation, and camera motion to jointly infer depth, motion, and semantic information as a 1D energy minimization problem. The optical flow of a stixel is defined by a homography, and by applying the mono-stixel model, the degrees of freedom are reduced to a maximum of two. We also use a scene model and semantic information to handle moving objects. In our experiments, we use DeepFlow for optical flow estimation and FCN8s for semantic information as inputs, and demonstrate on the KITTI 2015 dataset that mono-stixels provide a reliable reconstruction of both static and moving elements of the scene. This overcomes the limitations of previous structure-from-motion approaches that were confined to static scenes.",1
"Images of static scenes submerged beneath a wavy water surface exhibit severe non-rigid distortions. The physics of water flow suggests that water surfaces possess spatio-temporal smoothness and temporal periodicity. Hence they possess a sparse representation in the 3D discrete Fourier (DFT) basis. Motivated by this, we pose the task of restoration of such video sequences as a compressed sensing (CS) problem. We begin by tracking a few salient feature points across the frames of a video sequence of the submerged scene. Using these point trajectories, we show that the motion fields at all other (non-tracked) points can be effectively estimated using a typical CS solver. This by itself is a novel contribution in the field of non-rigid motion estimation. We show that this method outperforms state of the art algorithms for underwater image restoration. We further consider a simple optical flow algorithm based on local polynomial expansion of the image frames (PEOF). Surprisingly, we demonstrate that PEOF is more efficient and often outperforms all the state of the art methods in terms of numerical measures. Finally, we demonstrate that a two-stage approach consisting of the CS step followed by PEOF much more accurately preserves the image structure and improves the (visual as well as numerical) video quality as compared to just the PEOF stage.",0
"Severely non-rigid distortions are present in images of static scenes beneath wavy water surfaces. The spatio-temporal smoothness and temporal periodicity of water flow indicate that water surfaces have a sparse representation in the 3D discrete Fourier (DFT) basis. Therefore, we propose to restore such video sequences by treating the task as a compressed sensing (CS) problem. Initially, we track a few prominent feature points in the submerged scene's video sequence. By using these point trajectories, we can effectively estimate the motion fields at all other non-tracked points through a standard CS solver, which is a novel development in non-rigid motion estimation. Our method outperforms the current state-of-the-art algorithms for underwater image restoration. We also assess a straightforward optical flow algorithm based on local polynomial expansion of the image frames (PEOF) and find that it is often more effective than the state-of-the-art methods in terms of numerical measures. Finally, we demonstrate that a two-stage approach, including the CS step followed by PEOF, results in better preservation of image structure and improved visual and numerical video quality compared to just the PEOF stage.",1
"Egocentric action anticipation consists in understanding which objects the camera wearer will interact with in the near future and which actions they will perform. We tackle the problem proposing an architecture able to anticipate actions at multiple temporal scales using two LSTMs to 1) summarize the past, and 2) formulate predictions about the future. The input video is processed considering three complimentary modalities: appearance (RGB), motion (optical flow) and objects (object-based features). Modality-specific predictions are fused using a novel Modality ATTention (MATT) mechanism which learns to weigh modalities in an adaptive fashion. Extensive evaluations on two large-scale benchmark datasets show that our method outperforms prior art by up to +7% on the challenging EPIC-Kitchens dataset including more than 2500 actions, and generalizes to EGTEA Gaze+. Our approach is also shown to generalize to the tasks of early action recognition and action recognition. Our method is ranked first in the public leaderboard of the EPIC-Kitchens egocentric action anticipation challenge 2019. Please see our web pages for code and examples: http://iplab.dmi.unict.it/rulstm - https://github.com/fpv-iplab/rulstm.",0
"The concept of egocentric action anticipation involves predicting which objects a camera wearer will interact with and the actions they will perform in the near future. To solve this problem, we propose an architecture that uses two LSTMs to summarize past events and formulate predictions about the future at multiple temporal scales. We process the input video using three modalities, including appearance, motion, and objects. Our approach fuses modality-specific predictions using a novel Modality ATTention mechanism that learns to weigh modalities in an adaptive way. Our method outperforms prior art by up to +7% on the challenging EPIC-Kitchens dataset containing more than 2500 actions and generalizes to EGTEA Gaze+. We demonstrate that our method also works for early action recognition and action recognition tasks. Our approach is ranked first in the public leaderboard of the 2019 EPIC-Kitchens egocentric action anticipation challenge. Examples and code can be found on our web pages: http://iplab.dmi.unict.it/rulstm - https://github.com/fpv-iplab/rulstm.",1
"In this paper, we develop a modified differential Structure from Motion (SfM) algorithm that can estimate relative pose from two consecutive frames despite of Rolling Shutter (RS) artifacts. In particular, we show that under constant velocity assumption, the errors induced by the rolling shutter effect can be easily rectified by a linear scaling operation on each optical flow. We further propose a 9-point algorithm to recover the relative pose of a rolling shutter camera that undergoes constant acceleration motion. We demonstrate that the dense depth maps recovered from the relative pose of the RS camera can be used in a RS-aware warping for image rectification to recover high-quality Global Shutter (GS) images. Experiments on both synthetic and real RS images show that our RS-aware differential SfM algorithm produces more accurate results on relative pose estimation and 3D reconstruction from images distorted by RS effect compared to standard SfM algorithms that assume a GS camera model. We also demonstrate that our RS-aware warping for image rectification method outperforms state-of-the-art commercial software products, i.e. Adobe After Effects and Apple Imovie, at removing RS artifacts.",0
"The objective of this study is to enhance the Structure from Motion (SfM) algorithm to estimate relative pose from two consecutive frames, while considering Rolling Shutter (RS) artifacts. A modified differential SfM algorithm is proposed that rectifies the errors caused by the rolling shutter effect through a linear scaling operation on each optical flow under the assumption of constant velocity. Additionally, a 9-point algorithm is presented to recover the relative pose of a rolling shutter camera that undergoes constant acceleration motion. The dense depth maps obtained from the relative pose of the RS camera are used for RS-aware warping to rectify images and recover high-quality Global Shutter (GS) images. Experimental results indicate that the RS-aware differential SfM algorithm is more accurate in relative pose estimation and 3D reconstruction from images distorted by RS effect than standard SfM algorithms that assume a GS camera model. The RS-aware warping for image rectification method proposed in this study performs better than the state-of-the-art commercial software products, such as Adobe After Effects and Apple Imovie, in removing RS artifacts.",1
"We address the challenging task of foreground object discovery and segmentation in video. We introduce an efficient solution, suitable for both unsupervised and supervised scenarios, based on a spacetime graph representation of the video sequence. We ensure a fine grained representation with one-to-one correspondences between graph nodes and video pixels. We formulate the task as a spectral clustering problem by exploiting the spatio-temporal consistency between the scene elements in terms of motion and appearance. Graph nodes that belong to the main object of interest should form a strong cluster, as they are linked through long range optical flow chains and have similar motion and appearance features along those chains. On one hand, the optimization problem aims to maximize the segmentation clustering score based on the motion structure through space and time. On the other hand, the segmentation should be consistent with respect to node features. Our approach leads to a graph formulation in which the segmentation solution becomes the principal eigenvector of a novel Feature-Motion matrix. While the actual matrix is not computed explicitly, the proposed algorithm efficiently computes, in a few iteration steps, the principal eigenvector that captures the segmentation of the main object in the video. The proposed algorithm, GO-VOS, produces a global optimum solution and, consequently, it does not depend on initialization. In practice, GO-VOS achieves state of the art results on three challenging datasets used in current literature: DAVIS, SegTrack and YouTube-Objects.",0
"Our aim is to discover and segment foreground objects in video, a task that poses significant challenges. We present a solution that is efficient for both supervised and unsupervised scenarios, using a spacetime graph representation of the video sequence. Our approach ensures a precise representation with a one-to-one correspondence between graph nodes and video pixels. We approach the task by formulating it as a spectral clustering problem, taking advantage of spatio-temporal consistency between scene elements in terms of appearance and motion. Our algorithm aims to form a strong cluster of graph nodes that belong to the primary object of interest, linked through long-range optical flow chains and possessing similar motion and appearance features. The objective is to maximize the segmentation clustering score based on motion structure through space and time while also ensuring consistency with respect to node features. Our approach leads to a graph formulation where the segmentation solution becomes the principal eigenvector of a novel Feature-Motion matrix that is efficiently computed in a few iterations. Our algorithm, GO-VOS, produces a global optimum solution that does not depend on initialization. In practice, GO-VOS achieves state-of-the-art results on three challenging datasets: DAVIS, SegTrack, and YouTube-Objects.",1
"In this paper, we propose a convolutional layer inspired by optical flow algorithms to learn motion representations. Our representation flow layer is a fully-differentiable layer designed to capture the `flow' of any representation channel within a convolutional neural network for action recognition. Its parameters for iterative flow optimization are learned in an end-to-end fashion together with the other CNN model parameters, maximizing the action recognition performance. Furthermore, we newly introduce the concept of learning `flow of flow' representations by stacking multiple representation flow layers. We conducted extensive experimental evaluations, confirming its advantages over previous recognition models using traditional optical flows in both computational speed and performance. Code/models available here: https://piergiaj.github.io/rep-flow-site/",0
"The paper proposes a convolutional layer that takes inspiration from optical flow algorithms to acquire motion representations. The representation flow layer, which is fully-differentiable, is intended to capture the flow of any representation channel in a convolutional neural network for action recognition. Its iterative flow optimization parameters are learned alongside the other CNN model parameters in an end-to-end manner to maximize action recognition performance. Additionally, the paper introduces the idea of learning 'flow of flow' representations by stacking multiple representation flow layers. Extensive experimental evaluations were conducted, demonstrating its superiority over previous recognition models that employ traditional optical flows in terms of both computational speed and performance. Code/models are accessible at https://piergiaj.github.io/rep-flow-site/.",1
"Video deblurring is a challenging task due to the spatially variant blur caused by camera shake, object motions, and depth variations, etc. Existing methods usually estimate optical flow in the blurry video to align consecutive frames or approximate blur kernels. However, they tend to generate artifacts or cannot effectively remove blur when the estimated optical flow is not accurate. To overcome the limitation of separate optical flow estimation, we propose a Spatio-Temporal Filter Adaptive Network (STFAN) for the alignment and deblurring in a unified framework. The proposed STFAN takes both blurry and restored images of the previous frame as well as blurry image of the current frame as input, and dynamically generates the spatially adaptive filters for the alignment and deblurring. We then propose the new Filter Adaptive Convolutional (FAC) layer to align the deblurred features of the previous frame with the current frame and remove the spatially variant blur from the features of the current frame. Finally, we develop a reconstruction network which takes the fusion of two transformed features to restore the clear frames. Both quantitative and qualitative evaluation results on the benchmark datasets and real-world videos demonstrate that the proposed algorithm performs favorably against state-of-the-art methods in terms of accuracy, speed as well as model size.",0
"The removal of blur from videos is a difficult task as the blur is not uniform due to various factors such as camera shake, object movements, and variations in depth. Current methods use optical flow estimation to align frames or approximate blur kernels, but these methods often generate artifacts or fail to remove blur when the optical flow estimation is inaccurate. To address this issue, we propose a Spatio-Temporal Filter Adaptive Network (STFAN) that combines alignment and deblurring in a single framework. The STFAN takes blurry and restored images of the previous frame, as well as the blurry image of the current frame, as inputs, and generates adaptive filters for alignment and deblurring. We also introduce the Filter Adaptive Convolutional (FAC) layer to align deblurred features of the previous frame with the current frame and remove the spatially variant blur from the current frame's features. Finally, we develop a reconstruction network that restores clear frames by fusing the transformed features. Evaluations show that our method outperforms state-of-the-art approaches in terms of accuracy, speed, and model size on benchmark datasets and real-world videos.",1
"This paper presents novel techniques for recovering 3D dense scene flow, based on differential analysis of 4D light fields. The key enabling result is a per-ray linear equation, called the ray flow equation, that relates 3D scene flow to 4D light field gradients. The ray flow equation is invariant to 3D scene structure and applicable to a general class of scenes, but is under-constrained (3 unknowns per equation). Thus, additional constraints must be imposed to recover motion. We develop two families of scene flow algorithms by leveraging the structural similarity between ray flow and optical flow equations: local 'Lucas-Kanade' ray flow and global 'Horn-Schunck' ray flow, inspired by corresponding optical flow methods. We also develop a combined local-global method by utilizing the correspondence structure in the light fields. We demonstrate high precision 3D scene flow recovery for a wide range of scenarios, including rotation and non-rigid motion. We analyze the theoretical and practical performance limits of the proposed techniques via the light field structure tensor, a 3x3 matrix that encodes the local structure of light fields. We envision that the proposed analysis and algorithms will lead to design of future light-field cameras that are optimized for motion sensing, in addition to depth sensing.",0
"In this article, innovative methods for retrieving 3D dense scene flow are presented. The techniques rely on differential analysis of 4D light fields, resulting in a per-ray linear equation known as the ray flow equation. This equation is invariant to 3D scene structure and applicable to a broad range of scenes, but it is under-constrained, with three unknowns per equation. To recover motion, additional restrictions are necessary. Two scene flow algorithms are introduced, using the structural similarity between ray flow and optical flow equations: local 'Lucas-Kanade' ray flow and global 'Horn-Schunck' ray flow, as well as a combined local-global method that utilizes the correspondence structure in light fields. The techniques show high precision 3D scene flow recovery for different scenarios, including rotation and non-rigid motion. The light field structure tensor is analyzed to determine the theoretical and practical performance limits of the proposed techniques. These methods are expected to lead to future light-field cameras optimized for motion sensing, in addition to depth sensing.",1
"Most of current Convolution Neural Network (CNN) based methods for optical flow estimation focus on learning optical flow on synthetic datasets with groundtruth, which is not practical. In this paper, we propose an unsupervised optical flow estimation framework named PCLNet. It uses pyramid Convolution LSTM (ConvLSTM) with the constraint of adjacent frame reconstruction, which allows flexibly estimating multi-frame optical flows from any video clip. Besides, by decoupling motion feature learning and optical flow representation, our method avoids complex short-cut connections used in existing frameworks while improving accuracy of optical flow estimation. Moreover, different from those methods using specialized CNN architectures for capturing motion, our framework directly learns optical flow from the features of generic CNNs and thus can be easily embedded in any CNN based frameworks for other tasks. Extensive experiments have verified that our method not only estimates optical flow effectively and accurately, but also obtains comparable performance on action recognition.",0
"The majority of Convolution Neural Network (CNN) based methods for estimating optical flow focus on synthetic datasets with groundtruth, which is not feasible. This article proposes an unsupervised optical flow estimation framework, PCLNet, which utilizes pyramid Convolution LSTM (ConvLSTM) with the limitation of adjacent frame reconstruction. This allows for flexible estimation of multi-frame optical flows from any video clip. Additionally, by separating motion feature learning and optical flow representation, our method eliminates the need for complex short-cut connections used in other frameworks, while improving the accuracy of optical flow estimation. Furthermore, unlike other methods that use specialized CNN architectures to capture motion, our framework directly learns optical flow from the features of generic CNNs, making it simple to incorporate into any CNN-based frameworks for other tasks. Through numerous experiments, we have confirmed that our method is effective and accurate in estimating optical flow, while also achieving comparable performance in action recognition.",1
"Video stabilization algorithms are of greater importance nowadays with the prevalence of hand-held devices which unavoidably produce videos with undesirable shaky motions. In this paper we propose a data-driven online video stabilization method along with a paired dataset for deep learning. The network processes each unsteady frame progressively in a multi-scale manner, from low resolution to high resolution, and then outputs an affine transformation to stabilize the frame. Different from conventional methods which require explicit feature tracking or optical flow estimation, the underlying stabilization process is learned implicitly from the training data, and the stabilization process can be done online. Since there are limited public video stabilization datasets available, we synthesized unstable videos with different extent of shake that simulate real-life camera movement. Experiments show that our method is able to outperform other stabilization methods in several unstable samples while remaining comparable in general. Also, our method is tested on complex contents and found robust enough to dampen these samples to some extent even it was not explicitly trained in the contents.",0
"Due to the prevalence of hand-held devices that produce videos with unwanted shaky motions, video stabilization algorithms have become increasingly important. This paper presents a data-driven online video stabilization method, including a paired dataset for deep learning. The network progressively processes each unstable frame in a multi-scale manner, from low to high resolution, and produces an affine transformation to stabilize the frame. Unlike traditional methods that require explicit feature tracking or optical flow estimation, our approach implicitly learns the underlying stabilization process from the training data, enabling online stabilization. Since few public video stabilization datasets exist, we synthesized unstable videos with varying degrees of shake to simulate real-life camera movement. Our experiments show that our method outperforms other stabilization methods in several unstable samples while maintaining comparable performance overall. Additionally, our method was tested on complex content and found to be sufficiently robust to reduce shaking, even though it was not specifically trained on that content.",1
"We focus on the word-level visual lipreading, which requires recognizing the word being spoken, given only the video but not the audio. State-of-the-art methods explore the use of end-to-end neural networks, including a shallow (up to three layers) 3D convolutional neural network (CNN) + a deep 2D CNN (e.g., ResNet) as the front-end to extract visual features, and a recurrent neural network (e.g., bidirectional LSTM) as the back-end for classification. In this work, we propose to replace the shallow 3D CNNs + deep 2D CNNs front-end with recent successful deep 3D CNNs --- two-stream (i.e., grayscale video and optical flow streams) I3D. We evaluate different combinations of front-end and back-end modules with the grayscale video and optical flow inputs on the LRW dataset. The experiments show that, compared to the shallow 3D CNNs + deep 2D CNNs front-end, the deep 3D CNNs front-end with pre-training on the large-scale image and video datasets (e.g., ImageNet and Kinetics) can improve the classification accuracy. Also, we demonstrate that using the optical flow input alone can achieve comparable performance as using the grayscale video as input. Moreover, the two-stream network using both the grayscale video and optical flow inputs can further improve the performance. Overall, our two-stream I3D front-end with a Bi-LSTM back-end results in an absolute improvement of 5.3% over the previous art on the LRW dataset.",0
"Our focus is on visual lipreading at the word level, where the aim is to recognize spoken words solely from their video, without audio. State-of-the-art techniques use neural networks, with a 3D CNN paired with a deep 2D CNN to extract visual features, and a recurrent neural network for classification. We propose replacing the 3D CNN + deep 2D CNN front-end with a newer, more successful deep 3D CNN - the two-stream I3D, which uses both grayscale video and optical flow streams. We test various combinations of front-end and back-end modules using both types of inputs on the LRW dataset. The results show that the deep 3D CNNs front-end, pre-trained on large-scale image and video datasets, improves classification accuracy over the shallow 3D CNNs + deep 2D CNNs front-end. We also show that using optical flow alone achieves comparable performance to grayscale video input, while using both further enhances the network's performance. Our two-stream I3D front-end with a Bi-LSTM back-end achieves a 5.3% improvement over the previous art on the LRW dataset.",1
"Many road accidents occur due to distracted drivers. Today, driver monitoring is essential even for the latest autonomous vehicles to alert distracted drivers in order to take over control of the vehicle in case of emergency. In this paper, a spatio-temporal approach is applied to classify drivers' distraction level and movement decisions using convolutional neural networks (CNNs). We approach this problem as action recognition to benefit from temporal information in addition to spatial information. Our approach relies on features extracted from sparsely selected frames of an action using a pre-trained BN-Inception network. Experiments show that our approach outperforms the state-of-the art results on the Distracted Driver Dataset (96.31%), with an accuracy of 99.10% for 10-class classification while providing real-time performance. We also analyzed the impact of fusion using RGB and optical flow modalities with a very recent data level fusion strategy. The results on the Distracted Driver and Brain4Cars datasets show that fusion of these modalities further increases the accuracy.",0
"Distracted drivers are a major contributor to road accidents, even in the latest autonomous vehicles. Therefore, it is crucial to monitor drivers to alert them in case of emergency. This paper proposes a spatio-temporal approach using convolutional neural networks (CNNs) to classify drivers' level of distraction and movement decisions. The problem is approached as action recognition, allowing for the utilization of both spatial and temporal information. The approach uses features extracted from sparsely selected frames of an action using a pre-trained BN-Inception network. Results show that this approach outperforms the state-of-the-art results on the Distracted Driver Dataset with 99.10% accuracy for 10-class classification while also providing real-time performance. The study also analyzes the impact of fusion using RGB and optical flow modalities with recent data level fusion strategy, showing an increase in accuracy on both the Distracted Driver and Brain4Cars datasets.",1
"Optical Flow (OF) and depth are commonly used for visual odometry since they provide sufficient information about camera ego-motion in a rigid scene. We reformulate the problem of ego-motion estimation as a problem of motion estimation of a 3D-scene with respect to a static camera. The entire scene motion can be represented as a combination of motions of its visible points. Using OF and depth we estimate a motion of each point in terms of 6DoF and represent results in the form of motion maps, each one addressing single degree of freedom. In this work we provide motion maps as inputs to a deep neural network that predicts 6DoF of scene motion. Through our evaluation on outdoor and indoor datasets we show that utilizing motion maps leads to accuracy improvement in comparison with naive stacking of depth and OF. Another contribution of our work is a novel network architecture that efficiently exploits motion maps and outperforms learnable RGB/RGB-D baselines.",0
"Visual odometry commonly relies on Optical Flow (OF) and depth to gather enough information about camera ego-motion in a rigid scene. To transform the problem of ego-motion estimation, we view it as a problem of motion estimation of a 3D-scene in relation to a static camera. We can represent the entire scene motion through a combination of the motions of its visible points. By using OF and depth, we estimate the motion of each point in terms of 6DoF and present the results as motion maps, where each map addresses a single degree of freedom. Our approach involves providing motion maps as inputs to a deep neural network that predicts the 6DoF of scene motion. Our evaluation using outdoor and indoor datasets demonstrates that using motion maps results in improved accuracy compared to the naive stacking of depth and OF. Furthermore, our work introduces a new network architecture that efficiently utilizes motion maps and outperforms learnable RGB/RGB-D baselines.",1
"In this technical report we investigate speed estimation of the ego-vehicle on the KITTI benchmark using state-of-the-art deep neural network based optical flow and single-view depth prediction methods. Using a straightforward intuitive approach and approximating a single scale factor, we evaluate several application schemes of the deep networks and formulate meaningful conclusions such as: combining depth information with optical flow improves speed estimation accuracy as opposed to using optical flow alone; the quality of the deep neural network methods influences speed estimation performance; using the depth and optical flow results from smaller crops of wide images degrades performance. With these observations in mind, we achieve a RMSE of less than 1 m/s for vehicle speed estimation using monocular images as input from recordings of the KITTI benchmark. Limitations and possible future directions are discussed as well.",0
"This technical report delves into the estimation of the ego-vehicle's speed on the KITTI benchmark. To accomplish this, we utilize advanced deep neural network-based optical flow and single-view depth prediction techniques. By employing a straightforward and intuitive approach, we assess various deep network application schemes and reach significant conclusions. For example, we discover that combining depth information with optical flow leads to more accurate speed estimation than using optical flow alone. Additionally, the quality of the deep neural network methods has an impact on speed estimation performance. Furthermore, utilizing the depth and optical flow results from smaller image crops reduces performance. With these insights, we achieve an RMSE of under 1 m/s in vehicle speed estimation using monocular images as input from KITTI benchmark recordings. We also discuss limitations and possible future directions.",1
"Learning to estimate 3D geometry in a single frame and optical flow from consecutive frames by watching unlabeled videos via deep convolutional network has made significant progress recently. Current state-of-the-art (SoTA) methods treat the two tasks independently. One typical assumption of the existing depth estimation methods is that the scenes contain no independent moving objects. while object moving could be easily modeled using optical flow. In this paper, we propose to address the two tasks as a whole, i.e. to jointly understand per-pixel 3D geometry and motion. This eliminates the need of static scene assumption and enforces the inherent geometrical consistency during the learning process, yielding significantly improved results for both tasks. We call our method as ""Every Pixel Counts++"" or ""EPC++"". Specifically, during training, given two consecutive frames from a video, we adopt three parallel networks to predict the camera motion (MotionNet), dense depth map (DepthNet), and per-pixel optical flow between two frames (OptFlowNet) respectively. The three types of information are fed into a holistic 3D motion parser (HMP), and per-pixel 3D motion of both rigid background and moving objects are disentangled and recovered. Comprehensive experiments were conducted on datasets with different scenes, including driving scenario (KITTI 2012 and KITTI 2015 datasets), mixed outdoor/indoor scenes (Make3D) and synthetic animation (MPI Sintel dataset). Performance on the five tasks of depth estimation, optical flow estimation, odometry, moving object segmentation and scene flow estimation shows that our approach outperforms other SoTA methods. Code will be available at: https://github.com/chenxuluo/EPC.",0
"Recently, there has been significant progress in using deep convolutional networks to estimate 3D geometry in a single frame and optical flow from consecutive frames by watching unlabeled videos. However, current state-of-the-art methods treat these two tasks independently, assuming that scenes contain no independent moving objects. This paper proposes a new approach that jointly understands per-pixel 3D geometry and motion, eliminating the need for static scene assumptions and enforcing geometrical consistency during the learning process. The proposed method, called ""Every Pixel Counts++"" or ""EPC++"", uses three parallel networks to predict camera motion, dense depth map, and per-pixel optical flow, which are then fed into a holistic 3D motion parser to disentangle and recover per-pixel 3D motion of both rigid background and moving objects. The proposed approach outperforms other state-of-the-art methods on various datasets, including driving scenarios, mixed outdoor/indoor scenes, and synthetic animation. Code will be available at https://github.com/chenxuluo/EPC.",1
"We present Accel, a novel semantic video segmentation system that achieves high accuracy at low inference cost by combining the predictions of two network branches: (1) a reference branch that extracts high-detail features on a reference keyframe, and warps these features forward using frame-to-frame optical flow estimates, and (2) an update branch that computes features of adjustable quality on the current frame, performing a temporal update at each video frame. The modularity of the update branch, where feature subnetworks of varying layer depth can be inserted (e.g. ResNet-18 to ResNet-101), enables operation over a new, state-of-the-art accuracy-throughput trade-off spectrum. Over this curve, Accel models achieve both higher accuracy and faster inference times than the closest comparable single-frame segmentation networks. In general, Accel significantly outperforms previous work on efficient semantic video segmentation, correcting warping-related error that compounds on datasets with complex dynamics. Accel is end-to-end trainable and highly modular: the reference network, the optical flow network, and the update network can each be selected independently, depending on application requirements, and then jointly fine-tuned. The result is a robust, general system for fast, high-accuracy semantic segmentation on video.",0
"Introducing Accel, a new semantic video segmentation system that combines the predictions of two network branches to achieve high accuracy at low inference cost. The first branch, called the reference branch, extracts high-detail features on a reference keyframe and warps them forward using frame-to-frame optical flow estimates. The second branch, known as the update branch, computes features of adjustable quality on the current frame and performs a temporal update at each video frame. The update branch's modularity allows for the insertion of feature subnetworks of varying layer depth, resulting in a new accuracy-throughput trade-off spectrum. Accel models achieve higher accuracy and faster inference times than the closest comparable single-frame segmentation networks. Furthermore, Accel outperforms previous work on efficient semantic video segmentation by correcting warping-related error that compounds on datasets with complex dynamics. Accel is end-to-end trainable and highly modular, making it a robust, general system for fast, high-accuracy semantic segmentation on video that can be tailored to specific application requirements.",1
"Appearance and motion are two key components to depict and characterize the video content. Currently, the two-stream models have achieved state-of-the-art performances on video classification. However, extracting motion information, specifically in the form of optical flow features, is extremely computationally expensive, especially for large-scale video classification. In this paper, we propose a motion hallucination network, namely MoNet, to imagine the optical flow features from the appearance features, with no reliance on the optical flow computation. Specifically, MoNet models the temporal relationships of the appearance features and exploits the contextual relationships of the optical flow features with concurrent connections. Extensive experimental results demonstrate that the proposed MoNet can effectively and efficiently hallucinate the optical flow features, which together with the appearance features consistently improve the video classification performances. Moreover, MoNet can help cutting down almost a half of computational and data-storage burdens for the two-stream video classification. Our code is available at: https://github.com/YongyiTang92/MoNet-Features.",0
"To depict and characterize video content, appearance and motion are essential components. While the two-stream models have achieved exceptional performance in video classification, extracting motion information in the form of optical flow features is computationally expensive, particularly for large-scale video classification. This paper introduces MoNet, a motion hallucination network that can imagine optical flow features from appearance features without relying on optical flow computation. The proposed model models the temporal relationships of the appearance features and exploits the contextual relationships of the optical flow features with concurrent connections. Experimental results demonstrate that MoNet can effectively and efficiently hallucinate optical flow features, which, when combined with appearance features, consistently improve video classification performance. Furthermore, MoNet reduces almost half of the computational and data-storage burdens for two-stream video classification. The code is available at https://github.com/YongyiTang92/MoNet-Features.",1
"The goal of this paper is to detect the spatio-temporal extent of an action. The two-stream detection network based on RGB and flow provides state-of-the-art accuracy at the expense of a large model-size and heavy computation. We propose to embed RGB and optical-flow into a single two-in-one stream network with new layers. A motion condition layer extracts motion information from flow images, which is leveraged by the motion modulation layer to generate transformation parameters for modulating the low-level RGB features. The method is easily embedded in existing appearance- or two-stream action detection networks, and trained end-to-end. Experiments demonstrate that leveraging the motion condition to modulate RGB features improves detection accuracy. With only half the computation and parameters of the state-of-the-art two-stream methods, our two-in-one stream still achieves impressive results on UCF101-24, UCFSports and J-HMDB.",0
"The aim of this article is to identify the spatial and temporal scope of an action. Although the two-stream detection network based on RGB and flow is highly accurate, it requires a large model-size and heavy computation. To address this issue, we propose a two-in-one stream network that combines RGB and optical-flow with new layers. The motion condition layer extracts motion information from flow images, which is utilized by the motion modulation layer to generate transformation parameters for modulating the low-level RGB features. This approach can be easily integrated into existing appearance- or two-stream action detection networks and trained end-to-end. Results show that using the motion condition to modulate RGB features enhances detection accuracy. Despite having only half the computation and parameters of the state-of-the-art two-stream methods, our two-in-one stream still achieves impressive results on UCF101-24, UCFSports and J-HMDB.",1
"Convolutional neural networks (CNNs) can model complicated non-linear relations between images. However, they are notoriously sensitive to small changes in the input. Most CNNs trained to describe image-to-image mappings generate temporally unstable results when applied to video sequences, leading to flickering artifacts and other inconsistencies over time. In order to use CNNs for video material, previous methods have relied on estimating dense frame-to-frame motion information (optical flow) in the training and/or the inference phase, or by exploring recurrent learning structures. We take a different approach to the problem, posing temporal stability as a regularization of the cost function. The regularization is formulated to account for different types of motion that can occur between frames, so that temporally stable CNNs can be trained without the need for video material or expensive motion estimation. The training can be performed as a fine-tuning operation, without architectural modifications of the CNN. Our evaluation shows that the training strategy leads to large improvements in temporal smoothness. Moreover, for small datasets the regularization can help in boosting the generalization performance to a much larger extent than what is possible with na\""ive augmentation strategies.",0
"CNNs have the ability to represent complex non-linear relationships between images; however, they are highly sensitive to minor changes in the input. When applied to video sequences, CNNs designed for image-to-image mappings result in unstable outcomes, leading to inconsistencies and flickering artifacts over time. Prior methods have relied on estimating dense frame-to-frame motion information or using recurrent learning structures to address this problem. We propose an alternative approach by incorporating temporal stability as a regularization term in the cost function. This regularization factor accounts for various types of motion between frames, enabling the training of temporally stable CNNs without requiring video materials or costly motion estimation. The CNN architecture does not require any modifications, and the training can be performed through fine-tuning. Our results demonstrate that this training strategy significantly improves temporal smoothness. Additionally, for small datasets, the regularization can enhance generalization performance more effectively than naive augmentation strategies.",1
"We present a new method to learn video representations from unlabeled data. Given large-scale unlabeled video data, the objective is to benefit from such data by learning a generic and transferable representation space that can be directly used for a new task such as zero/few-shot learning. We formulate our unsupervised representation learning as a multi-modal, multi-task learning problem, where the representations are also shared across different modalities via distillation. Further, we also introduce the concept of finding a better loss function to train such multi-task multi-modal representation space using an evolutionary algorithm; our method automatically searches over different combinations of loss functions capturing multiple (self-supervised) tasks and modalities. Our formulation allows for the distillation of audio, optical flow and temporal information into a single, RGB-based convolutional neural network. We also compare the effects of using additional unlabeled video data and evaluate our representation learning on standard public video datasets.",0
"A novel approach to acquiring video representations from unlabelled data is introduced. The aim is to leverage this data to acquire a versatile and adaptable representation space that can be used directly for tasks like zero/few-shot learning. Our unsupervised learning process is framed as a multi-modal, multi-task learning challenge, where distillation is used to share representations across different modalities. To improve the training of this multi-modal, multi-task representation space, we propose a new loss function search method using evolutionary algorithms. Our framework can incorporate audio, optical flow and temporal information into a single, RGB-based convolutional neural network. The impact of using additional unlabelled video data is also discussed, and our representation learning approach is evaluated on established public video datasets.",1
"We address the problem of temporal activity detection in continuous, untrimmed video streams. This is a difficult task that requires extracting meaningful spatio-temporal features to capture activities, accurately localizing the start and end times of each activity. We introduce a new model, Region Convolutional 3D Network (R-C3D), which encodes the video streams using a three-dimensional fully convolutional network, then generates candidate temporal regions containing activities and finally classifies selected regions into specific activities. Computation is saved due to the sharing of convolutional features between the proposal and the classification pipelines. We further improve the detection performance by efficiently integrating an optical flow based motion stream with the original RGB stream. The two-stream network is jointly optimized by fusing the flow and RGB feature maps at different levels. Additionally, the training stage incorporates an online hard example mining strategy to address the extreme foreground-background imbalance typically observed in any detection pipeline. Instead of heuristically sampling the candidate segments for the final activity classification stage, we rank them according to their performance and only select the worst performers to update the model. This improves the model without heavy hyper-parameter tuning. Extensive experiments on three benchmark datasets are carried out to show superior performance over existing temporal activity detection methods. Our model achieves state-of-the-art results on the THUMOS'14 and Charades datasets. We further demonstrate that our model is a general temporal activity detection framework that does not rely on assumptions about particular dataset properties by evaluating our approach on the ActivityNet dataset.",0
"Our focus is on detecting temporal activity in continuous, untrimmed video streams. This is a challenging task that requires identifying meaningful spatio-temporal features to accurately locate the start and end times of each activity. We propose a novel model, the Region Convolutional 3D Network (R-C3D), which encodes the video streams using a three-dimensional fully convolutional network. The model generates temporal regions containing activities and classifies them into specific activities. Sharing convolutional features between the proposal and classification pipelines reduces computation. To improve detection performance, we integrate an optical flow-based motion stream with the original RGB stream and jointly optimize the two-stream network by fusing the flow and RGB feature maps. The training stage includes an online hard example mining strategy to address foreground-background imbalance. We rank candidate segments according to their performance and only select the worst performers to update the model, improving the model without requiring heavy hyper-parameter tuning. We conduct experiments on three benchmark datasets and achieve state-of-the-art results on the THUMOS'14 and Charades datasets. Our model is a general temporal activity detection framework that can be applied without assumptions about dataset properties, as demonstrated on the ActivityNet dataset.",1
"Anomaly detection in crowd videos has become a popular area of research for the computer vision community. Several existing methods generally perform a prior training about the scene with or without the use of labeled data. However, it is difficult to always guarantee the availability of prior data, especially, for scenarios like remote area surveillance. To address such challenge, we propose an adaptive training-less system capable of detecting anomaly on-the-fly while dynamically estimating and adjusting response based on certain parameters. This makes our system both training-less and adaptive in nature. Our pipeline consists of three main components, namely, adaptive 3D-DCT model for multi-object detection-based association, local motion structure description through saliency modulated optic flow, and anomaly detection based on earth movers distance (EMD). The proposed model, despite being training-free, is found to achieve comparable performance with several state-of-the-art methods on the publicly available UCSD, UMN, CHUK-Avenue and ShanghaiTech datasets.",0
"The detection of anomalies in crowd videos has become a popular research topic within the computer vision community. Existing methods typically involve prior training of the scene, with or without labeled data. However, it is often difficult to ensure the availability of prior data, particularly in remote surveillance scenarios. In response to this challenge, we have developed an adaptive training-less system that can detect anomalies on-the-fly, while dynamically estimating and adjusting the response based on specific parameters. This makes our system both adaptive and training-free. Our pipeline comprises three main components: an adaptive 3D-DCT model for multi-object detection-based association, local motion structure description via saliency modulated optic flow, and anomaly detection using earth movers distance (EMD). Despite its lack of training, our proposed model achieves comparable performance to several state-of-the-art methods on publicly available datasets such as UCSD, UMN, CHUK-Avenue, and ShanghaiTech.",1
"In this dissertation, I present my work towards exploring temporal information for better video understanding. Specifically, I have worked on two problems: action recognition and semantic segmentation. For action recognition, I have proposed a framework, termed hidden two-stream networks, to learn an optimal motion representation that does not require the computation of optical flow. My framework alleviates several challenges faced in video classification, such as learning motion representations, real-time inference, multi-framerate handling, generalizability to unseen actions, etc. For semantic segmentation, I have introduced a general framework that uses video prediction models to synthesize new training samples. By scaling up the training dataset, my trained models are more accurate and robust than previous models even without modifications to the network architectures or objective functions. I believe videos have much more potential to be mined, and temporal information is one of the most important cues for machines to perceive the visual world better.",0
"My dissertation focuses on enhancing video understanding by exploring temporal information. I have addressed two problems, namely action recognition and semantic segmentation. To tackle the former, I have devised a novel framework called hidden two-stream networks that facilitates optimal motion representation without the need for optical flow computation. This approach overcomes several video classification challenges such as multi-framerate handling, real-time inference, learning motion representations, and generalizability to unseen actions. In terms of semantic segmentation, I have proposed a comprehensive framework that utilizes video prediction models to generate new training samples. This approach has helped me to augment the size of the training dataset, resulting in more accurate and robust trained models that do not require any modifications to the network architectures or objective functions. I am convinced that there is enormous potential in mining videos, and temporal information is an invaluable cue that can significantly enhance machines' ability to perceive the visual world.",1
"Mathematical optimization is widely used in various research fields. With a carefully-designed objective function, mathematical optimization can be quite helpful in solving many problems. However, objective functions are usually hand-crafted and designing a good one can be quite challenging. In this paper, we propose a novel framework to learn the objective function based on a neural net-work. The basic idea is to consider the neural network as an objective function, and the input as an optimization variable. For the learning of objective function from the training data, two processes are conducted: In the inner process, the optimization variable (the input of the network) are optimized to minimize the objective function (the network output), while fixing the network weights. In the outer process, on the other hand, the weights are optimized based on how close the final solution of the inner process is to the desired solution. After learning the objective function, the solution for the test set is obtained in the same manner of the inner process. The potential and applicability of our approach are demonstrated by the experiments on toy examples and a computer vision task, optical flow.",0
"Mathematical optimization is a widely used tool in various research fields, and can be especially helpful in solving problems when a well-designed objective function is used. However, creating a good objective function can be difficult as they are typically crafted by hand. In this study, we introduce a unique framework for learning the objective function using a neural network. The neural network acts as the objective function, with the input serving as the optimization variable. Our process involves two steps: firstly, the optimization variable is optimized to minimize the network output while the network weights remain fixed. Secondly, the weights are optimized based on how closely the final solution of the first step matches the desired solution. After learning the objective function, the solution for the test set is obtained using the same process as the first step. We demonstrate the effectiveness and versatility of our approach through experiments on toy examples and a computer vision task, optical flow.",1
"Precipitation nowcasting is a short-range forecast of rain/snow (up to 2 hours), often displayed on top of the geographical map by the weather service. Modern precipitation nowcasting algorithms rely on the extrapolation of observations by ground-based radars via optical flow techniques or neural network models. Dependent on these radars, typical nowcasting is limited to the regions around their locations. We have developed a method for precipitation nowcasting based on geostationary satellite imagery and incorporated the resulting data into the Yandex.Weather precipitation map (including an alerting service with push notifications for products in the Yandex ecosystem), thus expanding its coverage and paving the way to a truly global nowcasting service.",0
"The prediction of rain or snow in the near future, known as precipitation nowcasting, is typically provided by weather services for up to two hours. This information is often displayed on top of a map to show the areas affected. Current methods rely on ground-based radars and use optical flow techniques or neural network models to extrapolate observations. However, the coverage of such systems is limited to the areas around the radar. To solve this issue, we have developed a new approach that uses geostationary satellite imagery. By incorporating this data into the Yandex.Weather precipitation map, we have expanded the coverage of the service, and created a truly global nowcasting service. This includes an alerting service with push notifications for products in the Yandex ecosystem.",1
"Stereo matching and flow estimation are two essential tasks for scene understanding, spatially in 3D and temporally in motion. Existing approaches have been focused on the unsupervised setting due to the limited resource to obtain the large-scale ground truth data. To construct a self-learnable objective, co-related tasks are often linked together to form a joint framework. However, the prior work usually utilizes independent networks for each task, thus not allowing to learn shared feature representations across models. In this paper, we propose a single and principled network to jointly learn spatiotemporal correspondence for stereo matching and flow estimation, with a newly designed geometric connection as the unsupervised signal for temporally adjacent stereo pairs. We show that our method performs favorably against several state-of-the-art baselines for both unsupervised depth and flow estimation on the KITTI benchmark dataset.",0
"Stereo matching and flow estimation are crucial for comprehending scenes in both 3D space and motion. Due to limited resources for obtaining large-scale ground truth data, previous approaches have concentrated on the unsupervised setting. To create a self-learnable objective, related tasks are often combined to form a joint framework. However, prior work has used separate networks for each task, preventing shared feature representations across models. This paper proposes a single and organized network that simultaneously learns spatiotemporal correspondence for stereo matching and flow estimation. A newly designed geometric connection serves as the unsupervised signal for temporally adjacent stereo pairs. The results show that our method outperforms several state-of-the-art baselines for both unsupervised depth and flow estimation on the KITTI benchmark dataset.",1
"Modern optical flow methods make use of salient scene feature points detected and matched within the scene as a basis for sparse-to-dense optical flow estimation. Current feature detectors however either give sparse, non uniform point clouds (resulting in flow inaccuracies) or lack the efficiency for frame-rate real-time applications. In this work we use the novel Dense Gradient Based Features (DeGraF) as the input to a sparse-to-dense optical flow scheme. This consists of three stages: 1) efficient detection of uniformly distributed Dense Gradient Based Features (DeGraF); 2) feature tracking via robust local optical flow; and 3) edge preserving flow interpolation to recover overall dense optical flow. The tunable density and uniformity of DeGraF features yield superior dense optical flow estimation compared to other popular feature detectors within this three stage pipeline. Furthermore, the comparable speed of feature detection also lends itself well to the aim of real-time optical flow recovery. Evaluation on established real-world benchmark datasets show test performance in an autonomous vehicle setting where DeGraF-Flow shows promising results in terms of accuracy with competitive computational efficiency among non-GPU based methods, including a marked increase in speed over the conceptually similar EpicFlow approach.",0
"Sparse-to-dense optical flow estimation in modern methods uses detected and matched salient scene feature points. However, the current feature detectors are either inefficient for real-time applications or result in flow inaccuracies due to non-uniform point clouds. This study introduces Dense Gradient Based Features (DeGraF) as a novel input to the sparse-to-dense optical flow scheme, consisting of three stages: efficient detection of uniformly distributed DeGraF features, feature tracking via robust local optical flow, and edge preserving flow interpolation for overall dense optical flow recovery. DeGraF features yield superior dense optical flow estimation with tunable density and uniformity, making it suitable for real-time optical flow recovery. Evaluation on established real-world benchmark datasets shows promising results in an autonomous vehicle setting, with DeGraF-Flow's accuracy and computational efficiency being comparable to non-GPU based methods, and faster than the similar EpicFlow approach.",1
"Video representation is a key challenge in many computer vision applications such as video classification, video captioning, and video surveillance. In this paper, we propose a novel approach for video representation that captures meaningful information including motion and appearance from a sequence of video frames and compacts it into a single image. To this end, we compute the optical flow and use it in a least squares optimization to find a new image, the so-called Flow Profile Image (FPI). This image encodes motions as well as foreground appearance information while background information is removed. The quality of this image is validated in activity recognition experiments and the results are compared with other video representation techniques such as dynamic images [1] and eigen images [2]. The experimental results as well as visual quality confirm that FPIs can be successfully used in video processing applications.",0
"Many computer vision applications, like video classification, video captioning, and video surveillance, face the challenge of video representation. This paper introduces a new method for video representation that captures essential information, such as motion and appearance, from a series of video frames and condenses it into one image called the Flow Profile Image (FPI). Using optical flow and least squares optimization, the FPI encodes motion and foreground appearance details while removing background information. We compared the FPIs to other video representation techniques, like dynamic images and eigen images, in activity recognition experiments. The results confirm that FPIs are a valuable tool in video processing applications, as they provide excellent visual quality and experimental results.",1
"Video inpainting, which aims at filling in missing regions of a video, remains challenging due to the difficulty of preserving the precise spatial and temporal coherence of video contents. In this work we propose a novel flow-guided video inpainting approach. Rather than filling in the RGB pixels of each frame directly, we consider video inpainting as a pixel propagation problem. We first synthesize a spatially and temporally coherent optical flow field across video frames using a newly designed Deep Flow Completion network. Then the synthesized flow field is used to guide the propagation of pixels to fill up the missing regions in the video. Specifically, the Deep Flow Completion network follows a coarse-to-fine refinement to complete the flow fields, while their quality is further improved by hard flow example mining. Following the guide of the completed flow, the missing video regions can be filled up precisely. Our method is evaluated on DAVIS and YouTube-VOS datasets qualitatively and quantitatively, achieving the state-of-the-art performance in terms of inpainting quality and speed.",0
"The task of video inpainting, which involves filling in gaps in a video, is still challenging because it is difficult to maintain the precise spatial and temporal coherence of the video content. To address this issue, we have proposed a new flow-guided video inpainting approach. Rather than filling in the RGB pixels of each frame directly, our method treats video inpainting as a pixel propagation problem. We first create a coherent optical flow field across video frames using a newly developed Deep Flow Completion network. We then use this synthesized flow field to guide the propagation of pixels to fill in the missing regions of the video. We refine the flow fields in a coarse-to-fine manner using the Deep Flow Completion network, and further enhance their quality through hard flow example mining. By following the guide of the completed flow, our method can accurately fill in the missing video regions. We have evaluated our approach on the DAVIS and YouTube-VOS datasets and achieved state-of-the-art performance in terms of both inpainting quality and speed.",1
"In this paper, we present a new inpainting framework for recovering missing regions of video frames. Compared with image inpainting, performing this task on video presents new challenges such as how to preserving temporal consistency and spatial details, as well as how to handle arbitrary input video size and length fast and efficiently. Towards this end, we propose a novel deep learning architecture which incorporates ConvLSTM and optical flow for modeling the spatial-temporal consistency in videos. It also saves much computational resource such that our method can handle videos with larger frame size and arbitrary length streamingly in real-time. Furthermore, to generate an accurate optical flow from corrupted frames, we propose a robust flow generation module, where two sources of flows are fed and a flow blending network is trained to fuse them. We conduct extensive experiments to evaluate our method in various scenarios and different datasets, both qualitatively and quantitatively. The experimental results demonstrate the superior of our method compared with the state-of-the-art inpainting approaches.",0
"A new framework for filling in missing regions of video frames is presented in this paper. The challenges of video inpainting are discussed, including preserving temporal consistency and spatial details, and handling arbitrary video size and length quickly and effectively. To address these challenges, a novel deep learning architecture is proposed that incorporates ConvLSTM and optical flow to model spatial-temporal consistency in videos. This architecture is computationally efficient, allowing for real-time processing of videos with larger frame sizes and arbitrary lengths. Additionally, a robust flow generation module is proposed to generate accurate optical flow from corrupted frames. Extensive experiments are conducted to evaluate the proposed method, demonstrating its superiority over state-of-the-art inpainting approaches.",1
"Motion has shown to be useful for video understanding, where motion is typically represented by optical flow. However, computing flow from video frames is very time-consuming. Recent works directly leverage the motion vectors and residuals readily available in the compressed video to represent motion at no cost. While this avoids flow computation, it also hurts accuracy since the motion vector is noisy and has substantially reduced resolution, which makes it a less discriminative motion representation. To remedy these issues, we propose a lightweight generator network, which reduces noises in motion vectors and captures fine motion details, achieving a more Discriminative Motion Cue (DMC) representation. Since optical flow is a more accurate motion representation, we train the DMC generator to approximate flow using a reconstruction loss and a generative adversarial loss, jointly with the downstream action classification task. Extensive evaluations on three action recognition benchmarks (HMDB-51, UCF-101, and a subset of Kinetics) confirm the effectiveness of our method. Our full system, consisting of the generator and the classifier, is coined as DMC-Net which obtains high accuracy close to that of using flow and runs two orders of magnitude faster than using optical flow at inference time.",0
"The use of motion has proven beneficial for comprehending videos, with optical flow being the typical representation. However, determining flow from video frames is a time-intensive process. Recent research has utilized readily available motion vectors and residuals from compressed videos as a cost-free substitute for flow computation. This approach sacrifices accuracy due to the noisy and low-resolution motion vector, resulting in a less effective motion representation. To address this, we suggest a lightweight generator network that minimizes noise in motion vectors and captures intricate motion details, producing a more Discriminative Motion Cue (DMC) representation. The DMC generator is trained to approximate flow using a reconstruction loss and a generative adversarial loss alongside the downstream action classification objective since optical flow is a more precise representation of motion. Our method is extensively evaluated on three action recognition benchmarks (HMDB-51, UCF-101, and a subset of Kinetics), confirming its effectiveness. Our complete system, comprising the generator and classifier, is named DMC-Net and achieves high accuracy similar to that of using flow while running two orders of magnitude faster at inference time.",1
"In this paper, we adapt the geodesic distance-based recursive filter to the sparse data interpolation problem. The proposed technique is general and can be easily applied to any kind of sparse data. We demonstrate the superiority over other interpolation techniques in three experiments for qualitative and quantitative evaluation.   In addition, we compare our method with the popular interpolation algorithm presented in the EpicFlow optical flow paper that is intuitively motivated by a similar geodesic distance principle. The comparison shows that our algorithm is more accurate and considerably faster than the EpicFlow interpolation technique.",0
"The geodesic distance-based recursive filter has been modified to address the sparse data interpolation issue in this study. This technique is versatile and can be utilized for any type of sparse data. Through three qualitative and quantitative experiments, we prove its superiority over other interpolation methods. Furthermore, we compare our approach with the geodesic distance-inspired EpicFlow optical flow paper's popular interpolation algorithm and reveal that our method is not only more precise but also faster.",1
"Multiview stereo aims to reconstruct scene depth from images acquired by a camera under arbitrary motion. Recent methods address this problem through deep learning, which can utilize semantic cues to deal with challenges such as textureless and reflective regions. In this paper, we present a convolutional neural network called DPSNet (Deep Plane Sweep Network) whose design is inspired by best practices of traditional geometry-based approaches for dense depth reconstruction. Rather than directly estimating depth and/or optical flow correspondence from image pairs as done in many previous deep learning methods, DPSNet takes a plane sweep approach that involves building a cost volume from deep features using the plane sweep algorithm, regularizing the cost volume via a context-aware cost aggregation, and regressing the dense depth map from the cost volume. The cost volume is constructed using a differentiable warping process that allows for end-to-end training of the network. Through the effective incorporation of conventional multiview stereo concepts within a deep learning framework, DPSNet achieves state-of-the-art reconstruction results on a variety of challenging datasets.",0
"The goal of multiview stereo is to determine the depth of a scene based on photos taken by a moving camera. Recent methods have utilized deep learning to overcome difficulties such as reflective and textureless areas by incorporating semantic cues. This article introduces DPSNet, a convolutional neural network that incorporates traditional geometry-based techniques for dense depth reconstruction. Instead of estimating depth and/or optical flow correspondence directly from image pairs like previous deep learning methods, DPSNet utilizes the plane sweep algorithm to create a cost volume from deep features. The cost volume is then regularized via context-aware cost aggregation and used to regress a dense depth map. The network is trained end-to-end using a differentiable warping process. By incorporating conventional multiview stereo concepts within a deep learning framework, DPSNet achieves exceptional results on challenging datasets.",1
"Processing and fusing information among multi-modal is a very useful technique for achieving high performance in many computer vision problems. In order to tackle multi-modal information more effectively, we introduce a novel framework for multi-modal fusion: Cross-modal Message Passing (CMMP). Specifically, we propose a cross-modal message passing mechanism to fuse two-stream network for action recognition, which composes of an appearance modal network (RGB image) and a motion modal (optical flow image) network. The objectives of individual networks in this framework are two-fold: a standard classification objective and a competing objective. The classification object ensures that each modal network predicts the true action category while the competing objective encourages each modal network to outperform the other one. We quantitatively show that the proposed CMMP fuses the traditional two-stream network more effectively, and outperforms all existing two-stream fusion method on UCF-101 and HMDB-51 datasets.",0
"The fusion of information across multiple modes is a highly effective technique for achieving superior performance in various computer vision tasks. To enhance the handling of multi-modal information, we present a new framework for multi-modal fusion called Cross-modal Message Passing (CMMP). Our approach includes a cross-modal message passing mechanism that fuses two-stream networks for action recognition, comprising an appearance modal network (RGB image) and a motion modal (optical flow image) network. The objectives of the individual networks in this framework are twofold: a standard classification objective and a competing objective. The classification objective ensures that each modal network accurately predicts the action category, while the competing objective incentivizes each modal network to outperform the other. Our quantitative results demonstrate that the proposed CMMP is more effective at fusing the traditional two-stream network and outperforms all existing two-stream fusion methods on UCF-101 and HMDB-51 datasets.",1
In this paper we present a self-supervised method for representation learning utilizing two different modalities. Based on the observation that cross-modal information has a high semantic meaning we propose a method to effectively exploit this signal. For our approach we utilize video data since it is available on a large scale and provides easily accessible modalities given by RGB and optical flow. We demonstrate state-of-the-art performance on highly contested action recognition datasets in the context of self-supervised learning. We show that our feature representation also transfers to other tasks and conduct extensive ablation studies to validate our core contributions. Code and model can be found at https://github.com/nawidsayed/Cross-and-Learn.,0
"This paper introduces a technique for self-supervised representation learning that involves utilizing two different modalities. The method is based on the idea that cross-modal information carries significant semantic meaning and proposes a strategy for effectively exploiting this signal. The approach makes use of video data as it is widely available and offers easily accessible modalities in the form of RGB and optical flow. The study demonstrates exceptional performance on competitive action recognition datasets within the self-supervised learning context. Additionally, the research shows that the feature representation can be applied to other tasks, and extensive ablation studies confirm the core contributions. Interested individuals can access the code and model at https://github.com/nawidsayed/Cross-and-Learn.",1
