"Localized Narratives is a dataset with detailed natural language descriptions of images paired with mouse traces that provide a sparse, fine-grained visual grounding for phrases. We propose TReCS, a sequential model that exploits this grounding to generate images. TReCS uses descriptions to retrieve segmentation masks and predict object labels aligned with mouse traces. These alignments are used to select and position masks to generate a fully covered segmentation canvas; the final image is produced by a segmentation-to-image generator using this canvas. This multi-step, retrieval-based approach outperforms existing direct text-to-image generation models on both automatic metrics and human evaluations: overall, its generated images are more photo-realistic and better match descriptions.",0
"Recent advances in deep learning have enabled the development of models that can generate images from textual descriptions, opening up new possibilities in areas such as computer vision and virtual reality. However, these models often struggle with generating high-quality images that accurately capture the user's intended meaning. In this work, we propose a novel approach based on fine-grained attention mechanisms that allow users to provide explicit feedback at different levels of detail, from overall scene composition down to individual object features. Our method leverages this user input to guide the image generation process, resulting in more accurate and relevant outputs. We demonstrate the effectiveness of our approach through extensive experiments and evaluations, showing significant improvements over state-of-the-art methods in terms of visual fidelity, diversity, and alignment with user intent. Overall, our results highlight the potential of grounded text-to-image generation techniques to enable richer human-AI interaction in creative applications.",1
"Vision-and-language navigation (VLN) is a multimodal task where an agent follows natural language instructions and navigates in visual environments. Multiple setups have been proposed, and researchers apply new model architectures or training techniques to boost navigation performance. However, recent studies witness a slow-down in the performance improvements in both indoor and outdoor VLN tasks, and the agents' inner mechanisms for making navigation decisions remain unclear. To the best of our knowledge, the way the agents perceive the multimodal input is under-studied and clearly needs investigations. In this work, we conduct a series of diagnostic experiments to unveil agents' focus during navigation. Results show that indoor navigation agents refer to both object tokens and direction tokens in the instruction when making decisions. In contrast, outdoor navigation agents heavily rely on direction tokens and have a poor understanding of the object tokens. Furthermore, instead of merely staring at surrounding objects, indoor navigation agents can set their sights on objects further from the current viewpoint. When it comes to vision-and-language alignments, many models claim that they are able to align object tokens with certain visual targets, but we cast doubt on the reliability of such alignments.",0
"Title: ""What Truly Counts in Vision-and-Language Navigation Research""  Researchers have made tremendous strides in developing advanced algorithms capable of navigating environments through natural language instructions, yet there remains a lack of consensus on how to evaluate these systems effectively. This paper seeks to address this gap by identifying key aspects that should be considered when assessing vision-and-language navigation models. Through a comprehensive literature review and analysis of state-of-the-art approaches, we argue that current evaluation methods often fall short in measuring crucial qualities such as interpretability, adaptability, explainability, robustness, and human-likeness. Instead, we propose a holistic framework that emphasizes understanding, generalization, trustworthiness, human interaction, and collaboration between humans and machines. Our findings provide valuable insights into the future direction of research, with implications for both academia and industry. By shifting our focus towards meaningful metrics, we can ensure that vision-and-language navigation technology becomes truly beneficial and accessible to all users.",1
"Recently, the Transformer module has been transplanted from natural language processing to computer vision. This paper applies the Transformer to video-based person re-identification, where the key issue is to extract the discriminative information from a tracklet. We show that, despite the strong learning ability, the vanilla Transformer suffers from an increased risk of over-fitting, arguably due to a large number of attention parameters and insufficient training data. To solve this problem, we propose a novel pipeline where the model is pre-trained on a set of synthesized video data and then transferred to the downstream domains with the perception-constrained Spatiotemporal Transformer (STT) module and Global Transformer (GT) module. The derived algorithm achieves significant accuracy gain on three popular video-based person re-identification benchmarks, MARS, DukeMTMC-VideoReID, and LS-VID, especially when the training and testing data are from different domains. More importantly, our research sheds light on the application of the Transformer on highly-structured visual data.",0
"Spatiotemporal Transformers have been recently proposed as an alternative architecture to deep convolutional neural networks (CNNs) that has achieved state-of-the-art results on several computer vision tasks such as image classification, object detection, and semantic segmentation. In contrast to CNNs which rely heavily on learned kernels/filters and pooling operations, Spatiotemporal Transformers leverage self attention mechanisms allowing them to capture global dependencies without relying on explicit spatial pyramidal structures. However, these models often struggle at capturing long-range temporal dependencies since their self attention mechanism only focuses on the local context within each clip in a sliding window fashion rather than reasoning across longer video segments. Inspired by recent work that showed pretraining using masked action recognition can improve performance on video understanding problems, we explore whether leveraging a similar transformer trained for video action recognition could enable spatiotemporal representations better suited for person reidentification given a surveillance camera setup where multiple cameras observe different parts of a scene simultaneously. Given an unsegmented set of videos containing humans captured from disjoint camera views, our method uses a pairwise discriminative feature learning formulation in order to model appearance differences caused by viewpoint changes, occlusions, illumination variations and other nuisance factors like clothing style or hair color changes, while minimizing differences due to true identity switches. We evaluate our approach over three commonly used benchmark datasets and achieve competitive mAP scores on all of them showing the effectiveness of utilizing a large language model pretrained for visual representation learning. Our findings indicate that",1
"The advent of larger machine learning (ML) models have improved state-of-the-art (SOTA) performance in various modeling tasks, ranging from computer vision to natural language. As ML models continue increasing in size, so does their respective energy consumption and computational requirements. However, the methods for tracking, reporting, and comparing energy consumption remain limited. We presentEnergyVis, an interactive energy consumption tracker for ML models. Consisting of multiple coordinated views, EnergyVis enables researchers to interactively track, visualize and compare model energy consumption across key energy consumption and carbon footprint metrics (kWh and CO2), helping users explore alternative deployment locations and hardware that may reduce carbon footprints. EnergyVis aims to raise awareness concerning computational sustainability by interactively highlighting excessive energy usage during model training; and by providing alternative training options to reduce energy usage.",0
"This paper presents EnergyVis: An interactive tool that helps users track and explore energy consumption for machine learning models in real time. In recent years, there has been growing interest in understanding the environmental impacts of training and deploying machine learning (ML) systems due to their high computational demands and increasingly widespread use. To this end, we have developed EnergyVis as part of our research into making sustainability considerations more tractable in ML workflows.  EnergyVis provides a clear overview of the relationships among datasets, algorithms, hyperparameters, and their associated energy usage, allowing users to quickly identify trends and correlations related to model performance and power consumption. Users can interactively adjust parameters such as batch size, model architecture, and dataset type using sliders and drop-down menus, immediately observing corresponding changes in accuracy metrics and energy consumption estimates on both the GPU and CPU. These estimates employ detailed calculations based on hardware configurations, power utilization, and other system characteristics obtained from standard sensors available on modern personal computers and cloud platforms. EnergyVis further offers a history view of past runs, enabling comparative analysis across different settings and machine types for improved model tuning and decision support. Additionally, users may export visualizations of their simulations to share insights derived from EnergyVis, fostering collaboration and transparency within project teams or organizations adopting greener technologies.",1
"Referring image segmentation aims to segment the objects referred by a natural language expression. Previous methods usually focus on designing an implicit and recurrent feature interaction mechanism to fuse the visual-linguistic features to directly generate the final segmentation mask without explicitly modeling the localization information of the referent instances. To tackle these problems, we view this task from another perspective by decoupling it into a ""Locate-Then-Segment"" (LTS) scheme. Given a language expression, people generally first perform attention to the corresponding target image regions, then generate a fine segmentation mask about the object based on its context. The LTS first extracts and fuses both visual and textual features to get a cross-modal representation, then applies a cross-model interaction on the visual-textual features to locate the referred object with position prior, and finally generates the segmentation result with a light-weight segmentation network. Our LTS is simple but surprisingly effective. On three popular benchmark datasets, the LTS outperforms all the previous state-of-the-art methods by a large margin (e.g., +3.2% on RefCOCO+ and +3.4% on RefCOCOg). In addition, our model is more interpretable with explicitly locating the object, which is also proved by visualization experiments. We believe this framework is promising to serve as a strong baseline for referring image segmentation.",0
"Our approach to referring image segmentation addresses two key challenges that arise due to incomplete object boundaries and ambiguity within images caused by occlusion. We present a novel pipeline that utilizes multi-level representation learning with human annotations to locate objects and guide their segmentations through interactive inputs. By focusing on shape consistency within our active contours model, we achieve accurate and efficient object localization as well as precise instance-specific boundary delineation. Our system has been validated against competing methods on benchmark datasets such as Pascal VOC 2012 and Cityscapes, surpassing state-of-the-art performance under certain metrics. As annotating ground truth data can become overwhelming for larger scale applications, our method offers a practical alternative capable of obtaining high quality results without excessive human intervention. Ultimately, our ""Locate then Segment"" strategy provides both accuracy and efficiency for users seeking detailed segmentations in complex scenes.",1
"Knowledge is captured in the form of entities and their relationships and stored in knowledge graphs. Knowledge graphs enhance the capabilities of applications in many different areas including Web search, recommendation, and natural language understanding. This is mainly because, entities enable machines to understand things that go beyond simple tokens. Many modern algorithms use learned entity embeddings from these structured representations. However, building a knowledge graph takes time and effort, hence very costly and nontrivial. On the other hand, many Web sources describe entities in some structured format and therefore, finding ways to get them into useful entity knowledge is advantageous. We propose an approach that processes entity centric textual knowledge sources to learn entity embeddings and in turn avoids the need for a traditional knowledge graph. We first extract triples into the new representation format that does not use traditional complex triple extraction methods defined by pre-determined relationship labels. Then we learn entity embeddings through this new type of triples. We show that the embeddings learned from our approach are: (i) high quality and comparable to a known knowledge graph-based embeddings and can be used to improve them further, (ii) better than a contextual language model-based entity embeddings, and (iii) easy to compute and versatile in domain-specific applications where a knowledge graph is not readily available",0
"This should describe what kinds of tasks can be done using entity context graphs without ever explaining explicitly what they are (ie no sentence starting ""an EC graph is"") nor providing examples of entities that might appear in one.  Title: Semi-Structured Knowledge Extraction from Unstructured Data  With the vast amount of text data available online, extracting relevant knowledge from unstructured sources remains a challenging task. Traditional methods such as Named Entity Recognition (NER) have limited capabilities in identifying relationships among entities and understanding their contexts within semi-structured documents like tables and news articles. However, advanced techniques like entity context graphs offer promising solutions to these problems by enabling accurate identification and representation of entities and their connections across multiple levels. By leveraging cutting-edge natural language processing algorithms, we propose a methodology for constructing high-quality entity context graphs from diverse web resources. These representations facilitate efficient querying, reasoning, clustering, ranking, classification, validation, filtering, recommendation, summarization, and exploration - empowering developers, researchers, organizations, governments, and individuals alike to harness the full potential of big data insights. With significant applications spanning domains including science, healthcare, finance, social media, politics, entertainment, education, business intelligence, public safety, and more, the use cases for our approach showcase the tremendous impact possible through innovative technologies for making sense out of massive collections of messy digital content. Our experiments demonstrate promising results on benchmark datasets, indicating the robustness and effectiveness of this novel framework for realizing the exciting possibilities of open domain question answering systems, fact extraction services, semantic search engines, personal assistants, chatbots, expert advisors, conversational agents, intelligent dashboards, and smart decision support tools. We hope this work inspires further advancements towards achieving the ultimate goal of creating intelligent machines capable of interacting effectively with humans in complex environments while remaining adaptive, explainable, interpretable, transparent, reliable, secure, scalable, maintainable, modular, flexible, customizable, usable, human-centric, and inclusive.",1
"Following the success in advancing natural language processing and understanding, transformers are expected to bring revolutionary changes to computer vision. This work provides the first and comprehensive study on the robustness of vision transformers (ViTs) against adversarial perturbations. Tested on various white-box and transfer attack settings, we find that ViTs possess better adversarial robustness when compared with convolutional neural networks (CNNs). We summarize the following main observations contributing to the improved robustness of ViTs:   1) Features learned by ViTs contain less low-level information and are more generalizable, which contributes to superior robustness against adversarial perturbations.   2) Introducing convolutional or tokens-to-token blocks for learning low-level features in ViTs can improve classification accuracy but at the cost of adversarial robustness.   3) Increasing the proportion of transformers in the model structure (when the model consists of both transformer and CNN blocks) leads to better robustness. But for a pure transformer model, simply increasing the size or adding layers cannot guarantee a similar effect.   4) Pre-training on larger datasets does not significantly improve adversarial robustness though it is critical for training ViTs.   5) Adversarial training is also applicable to ViT for training robust models.   Furthermore, feature visualization and frequency analysis are conducted for explanation. The results show that ViTs are less sensitive to high-frequency perturbations than CNNs and there is a high correlation between how well the model learns low-level features and its robustness against different frequency-based perturbations.",0
"While transformer models have achieved great success in natural language processing tasks, their adversarial robustness remains unexplored. In this paper, we investigate the adversarial robustness of visual transformers using several state-of-the-art techniques from NLP attacks on vision systems. Our experiments show that while some attack methods are effective at fooling these models, others are largely ineffective due to the unique characteristics of image data. We find that preprocessing steps such as normalization can greatly improve model robustness, and propose a new training method based on adversarial examples to further enhance performance. Finally, we provide detailed analysis of model behavior during both successful and failed attacks, shedding light on potential strategies for improving model robustness moving forward. Overall, our work represents an important step towards better understanding the limitations of current generative adversarial networks, and highlights promising directions for future research.",1
"Research in the area of Vision and Language encompasses challenging topics that seek to connect visual and textual information. The video-to-text problem is one of these topics, in which the goal is to connect an input video with its textual description. This connection can be mainly made by retrieving the most significant descriptions from a corpus or generating a new one given a context video. These two ways represent essential tasks for Computer Vision and Natural Language Processing communities, called text retrieval from video task and video captioning/description task. These two tasks are substantially more complex than predicting or retrieving a single sentence from an image. The spatiotemporal information present in videos introduces diversity and complexity regarding the visual content and the structure of associated language descriptions. This review categorizes and describes the state-of-the-art techniques for the video-to-text problem. It covers the main video-to-text methods and the ways to evaluate their performance. We analyze how the most reported benchmark datasets have been created, showing their drawbacks and strengths for the problem requirements. We also show the impressive progress that researchers have made on each dataset, and we analyze why, despite this progress, the video-to-text conversion is still unsolved. State-of-the-art techniques are still a long way from achieving human-like performance in generating or retrieving video descriptions. We cover several significant challenges in the field and discuss future research directions.",0
"This comprehensive review provides a thorough overview of vision and language research from a video-to-text perspective. It covers existing techniques, their limitations, and future directions aimed at bridging the gap between these two domains. The authors examine recent advances in deep learning architectures such as generative models, attention mechanisms, and transformers that have enabled significant progress in text generation from visual input. In addition, they discuss challenges related to data availability, annotation costs, evaluation metrics, and generalization ability that need further investigation. Ultimately, the goal of this paper is to inspire researchers to continue exploring new approaches toward seamless integration of vision and language. With real-world applications ranging from image/video captioning and retrieval to question answering and machine translation, this field holds immense potential for breakthroughs in artificial intelligence.",1
"Neural networks training on edge terminals is essential for edge AI computing, which needs to be adaptive to evolving environment. Quantised models can efficiently run on edge devices, but existing training methods for these compact models are designed to run on powerful servers with abundant memory and energy budget. For example, quantisation-aware training (QAT) method involves two copies of model parameters, which is usually beyond the capacity of on-chip memory in edge devices. Data movement between off-chip and on-chip memory is energy demanding as well. The resource requirements are trivial for powerful servers, but critical for edge devices. To mitigate these issues, We propose Resource Constrained Training (RCT). RCT only keeps a quantised model throughout the training, so that the memory requirements for model parameters in training is reduced. It adjusts per-layer bitwidth dynamically in order to save energy when a model can learn effectively with lower precision. We carry out experiments with representative models and tasks in image application and natural language processing. Experiments show that RCT saves more than 86\% energy for General Matrix Multiply (GEMM) and saves more than 46\% memory for model parameters, with limited accuracy loss. Comparing with QAT-based method, RCT saves about half of energy on moving model parameters.",0
"Introduction: In recent years, there has been significant interest in developing Artificial Intelligence (AI) systems that can operate on resource constrained devices such as smartphones, wearables, and other edge computing platforms. However, training these models requires large amounts of data and computational resources which may not always be available at the edge. This work proposes a novel approach called ""Resource Constrained Training"" (RCT) for optimizing AI model performance under limited training resources. Methods: We use several popular machine learning algorithms including deep neural networks (DNNs), support vector machines (SVMs), random decision trees (RDTs), and k-nearest neighbors (KNN) as case studies in our experiments. Our approach involves two main steps: i) pruning the models to reduce their complexity while maintaining accuracy, ii) using transfer learning techniques such as fine-tuning and distillation to improve generalization performance. Results: Experiments conducted across various datasets show that our method significantly reduces the number of parameters without compromising model performance. Furthermore, we demonstrate that by combining both methods, we achieve better results than simply applying either one independently. Discussion: Our findings suggest that RCT provides a promising framework for building efficient and effective AI models on resource constrained devices. Future research directions include exploring more advanced pruning techniques and evaluating different types of regularizations in combination with RCT. Overall, RCT presents opportunities for enabling real-time inference on AI applications in various domains like computer vision, natural language processing, and recommendation systems.",1
"Deep learning model (primarily convolutional networks and LSTM) for time series classification has been studied broadly by the community with the wide applications in different domains like healthcare, finance, industrial engineering and IoT. Meanwhile, Transformer Networks recently achieved frontier performance on various natural language processing and computer vision tasks. In this work, we explored a simple extension of the current Transformer Networks with gating, named Gated Transformer Networks (GTN) for the multivariate time series classification problem. With the gating that merges two towers of Transformer which model the channel-wise and step-wise correlations respectively, we show how GTN is naturally and effectively suitable for the multivariate time series classification task. We conduct comprehensive experiments on thirteen dataset with full ablation study. Our results show that GTN is able to achieve competing results with current state-of-the-art deep learning models. We also explored the attention map for the natural interpretability of GTN on time series modeling. Our preliminary results provide a strong baseline for the Transformer Networks on multivariate time series classification task and grounds the foundation for future research.",0
"Gated transformer networks have recently emerged as powerful models for natural language processing tasks, but their application in time series classification has been limited due to the inherent sequential structure of the data. In this work, we propose using gated transformers for multivariate time series classification by incorporating temporal dependencies through attention mechanisms. We evaluate our model on several benchmark datasets and compare its performance against state-of-the-art methods. Our results show that the proposed approach achieves superior accuracy while providing interpretable features that capture complex patterns in the data. This study highlights the potential of integrating transformers into time series analysis, opening up new opportunities for research in this domain.",1
"Although transfer learning is proven to be effective in computer vision and natural language processing applications, it is rarely investigated in forecasting financial time series. Majority of existing works on transfer learning are based on single-source transfer learning due to the availability of open-access large-scale datasets. However, in financial domain, the lengths of individual time series are relatively short and single-source transfer learning models are less effective. Therefore, in this paper, we investigate multi-source deep transfer learning for financial time series. We propose two multi-source transfer learning methods namely Weighted Average Ensemble for Transfer Learning (WAETL) and Tree-structured Parzen Estimator Ensemble Selection (TPEES). The effectiveness of our approach is evaluated on financial time series extracted from stock markets. Experiment results reveal that TPEES outperforms other baseline methods on majority of multi-source transfer tasks.",0
"Incorporating multiple data sources into a machine learning model can improve forecast accuracy. However, effectively utilizing heterogeneous datasets from diverse domains remains challenging due to differences in scale, distribution, quality, and correlation structure. To address these difficulties, we propose a novel multi-source transfer learning framework that leverages knowledge transferred from previous tasks via pre-trained models, ensembles learned across different sources, and domain adaptation techniques. Our proposed methodology is applied to financial time series prediction problems involving mixed frequency, seasonality, trend, and cross-sectional dependencies. Empirical results demonstrate the superiority of our approach over single-source benchmarks as well as other state-of-the-art methods based on deep neural networks (DNN) with gradient boosting machines (XGBoost), attention mechanisms (Attention-LSTM), adversarial training (DAT), generative adversarial imitation (GAIN), transfer reinforcement learning (TRL). We showcase how each component of our framework contributes incrementally towards better performance under varying degrees of complexity introduced by multiple source configurations. Lastly, we provide interpretability analysis and outlier detection during inference time, highlighting the benefits of using self-supervised representation learning for financial signal processing tasks.",1
"The dominant approaches to text representation in natural language rely on learning embeddings on massive corpora which have convenient properties such as compositionality and distance preservation. In this paper, we develop a novel method to learn a heavy-tailed embedding with desirable regularity properties regarding the distributional tails, which allows to analyze the points far away from the distribution bulk using the framework of multivariate extreme value theory. In particular, a classifier dedicated to the tails of the proposed embedding is obtained which performance outperforms the baseline. This classifier exhibits a scale invariance property which we leverage by introducing a novel text generation method for label preserving dataset augmentation. Numerical experiments on synthetic and real text data demonstrate the relevance of the proposed framework and confirm that this method generates meaningful sentences with controllable attribute, e.g. positive or negative sentiment.",0
"Natural language processing (NLP) has been at the forefront of many research fields due to advances in deep learning and artificial intelligence. One of these fields is text classification which deals with categorizing pieces of text into predefined classes based on their content. With the growth in social media platforms such as Twitter and Reddit comes more unstructured data that may contain offensive or polarized texts. In order to effectively classify this type of data, heavy-tail representations have proven effective by allowing the model to capture both skewness and excess kurtosis present in the data. We propose two different types of representations: Pareto Smooth Skewed Gaussians (PSSGs) which capture tail heaviness, skewness, and light tails; and Inverse Gaussian (IG) distributions that can account for the entire range of the distribution including the tail region. The proposed models outperform traditional normal mixture models commonly used in NLP. Furthermore, we demonstrate how data augmentation techniques can improve performance even further with limited additional training data. Our results show that using heavy-tailed representations paired with data augmentation yield better accuracy than previous state-of-the-art methods. The implications of our work are far reaching and could lead to improved models for sentiment analysis, opinion mining, topic labeling and other applications where unstructured data plays a key role.",1
"Curriculum learning (CL) is a training strategy that trains a machine learning model from easier data to harder data, which imitates the meaningful learning order in human curricula. As an easy-to-use plug-in, the CL strategy has demonstrated its power in improving the generalization capacity and convergence rate of various models in a wide range of scenarios such as computer vision and natural language processing etc. In this survey article, we comprehensively review CL from various aspects including motivations, definitions, theories, and applications. We discuss works on curriculum learning within a general CL framework, elaborating on how to design a manually predefined curriculum or an automatic curriculum. In particular, we summarize existing CL designs based on the general framework of Difficulty Measurer+Training Scheduler and further categorize the methodologies for automatic CL into four groups, i.e., Self-paced Learning, Transfer Teacher, RL Teacher, and Other Automatic CL. We also analyze principles to select different CL designs that may benefit practical applications. Finally, we present our insights on the relationships connecting CL and other machine learning concepts including transfer learning, meta-learning, continual learning and active learning, etc., then point out challenges in CL as well as potential future research directions deserving further investigations.",0
"Here are some drafts to choose from:  * This survey article investigates current approaches and techniques used in artificial intelligence (AI) related to curriculum learning, which refers to training deep neural networks through task selection that maximizes improvement. We provide an overview of recent work in this area along with their advantages and disadvantages. Our goal was to give an up-to-date picture of cutting-edge advancements within this specific branch of AI research.  To create this survey we conducted a thorough search of academic publications using digital libraries such as WebOfScience. An initial list of papers was gathered by searching key phrases like “curriculum learning”, “progressive network” and others. From there we carefully evaluated each publication based upon relevance to our focus and included them within this review. In total we present nineteen papers that cover state-of-the art methods found in the literature, which have been grouped into five categories: methods without memory-related constraints, those with regularization mechanisms applied during every iteration, and ones that enforce upper bounds on computational resources. For every approach discussed we analyzed its contribution as well as any possible drawbacks within experiments carried out by the authors themselves. As a result readers can now make informed decisions on whether these models may apply to their own projects while gaining insights into open challenges currently faced by the field at large. Overall, the aim is to serve both practitioners interested in machine learning problems as well as theoreticians looking for future research opportunities.* AI agents need to learn new tasks quickly in order to keep up with fast changes in the real world. One method to achieve this is via curriculum learning. This involves selecting tasks which maximize improvement of a model. Despite the interest in this topic, little has been done to gather together all existing research regarding different types of curricula for continuous control learning and how they compare. To address thi",1
"In video object tracking, there exist rich temporal contexts among successive frames, which have been largely overlooked in existing trackers. In this work, we bridge the individual video frames and explore the temporal contexts across them via a transformer architecture for robust object tracking. Different from classic usage of the transformer in natural language processing tasks, we separate its encoder and decoder into two parallel branches and carefully design them within the Siamese-like tracking pipelines. The transformer encoder promotes the target templates via attention-based feature reinforcement, which benefits the high-quality tracking model generation. The transformer decoder propagates the tracking cues from previous templates to the current frame, which facilitates the object searching process. Our transformer-assisted tracking framework is neat and trained in an end-to-end manner. With the proposed transformer, a simple Siamese matching approach is able to outperform the current top-performing trackers. By combining our transformer with the recent discriminative tracking pipeline, our method sets several new state-of-the-art records on prevalent tracking benchmarks.",0
"In recent years, deep learning has revolutionized computer vision tasks such as image classification, object detection, and segmentation. However, visual tracking remains challenging due to factors like occlusion, motion blur, illumination changes, and camera movements. To address these issues, we propose a novel framework that combines a transformer network and a tracker.  The transformer provides global temporal context by modeling interactions among patches throughout the video sequence, enabling it to capture patterns that traditional trackers miss. Meanwhile, the tracker serves two purposes: (i) providing accurate initial bounding boxes, which reduces drift caused by incorrect starting positions; and (ii) enforcing spatial constraints during inference, ensuring that predictions stay within the ground truth bounding box.  Our experiments on popular benchmarks demonstrate that our method outperforms state-of-the-art trackers across all evaluation metrics, including precision, recall, and area under curve. Additionally, ablation studies show that both components contribute significantly to our success. Finally, our approach generalizes well to unseen domains and achieves competitive performance compared against previous methods trained on much larger datasets.  In summary, we introduce a novel hybrid architecture that leverages the strengths of both transformers and trackers to address challenges in visual tracking. Our results set a new bar for accuracy and robustness in this task, highlighting the potential of deep learning techniques for computer vision problems beyond image understanding.",1
"It is encouraged to see that progress has been made to bridge videos and natural language. However, mainstream video captioning methods suffer from slow inference speed due to the sequential manner of autoregressive decoding, and prefer generating generic descriptions due to the insufficient training of visual words (e.g., nouns and verbs) and inadequate decoding paradigm. In this paper, we propose a non-autoregressive decoding based model with a coarse-to-fine captioning procedure to alleviate these defects. In implementations, we employ a bi-directional self-attention based network as our language model for achieving inference speedup, based on which we decompose the captioning procedure into two stages, where the model has different focuses. Specifically, given that visual words determine the semantic correctness of captions, we design a mechanism of generating visual words to not only promote the training of scene-related words but also capture relevant details from videos to construct a coarse-grained sentence ""template"". Thereafter, we devise dedicated decoding algorithms that fill in the ""template"" with suitable words and modify inappropriate phrasing via iterative refinement to obtain a fine-grained description. Extensive experiments on two mainstream video captioning benchmarks, i.e., MSVD and MSR-VTT, demonstrate that our approach achieves state-of-the-art performance, generates diverse descriptions, and obtains high inference efficiency. Our code is available at https://github.com/yangbang18/Non-Autoregressive-Video-Captioning.",0
"Here is a sample abstract for your consideration: ``` High quality video caption generation has proven to be a challenging task due to several factors such as computational cost, training data scarcity, and difficulty in model understanding. In recent years, autoregressive models have been widely used for generating accurate and diverse descriptions of videos. However, these models tend to generate lower quality outputs at early time steps, which leads to inferior overall results compared to non-autoregressive approaches that can generate high quality descriptions from the outset. This research proposes a novel coarse-to-fine approach using a pretrained convolutional neural network (CNN) for efficient feature extraction followed by recurrent layers and attention mechanisms for fine-grained caption generation. Experimental evaluation shows significant improvements over previous state-of-the art methods on benchmark datasets. Additionally, we demonstrate qualitative analysis showing the effectiveness of our proposed method in capturing important visual elements in the video frames. Overall, our work presents a new perspective in video captioning that combines efficiency and accuracy, paving the way for improved applications in computer vision and natural language processing fields. ``` Please note that you should review and modify any text generated by me, just like this response, before use for publication or submission.  Also, plagiarism detection systems may still find portions of my responses unoriginal even if heavily modified, so always double check and cite any external sources used herein properly.",1
"Cognitive grammar suggests that the acquisition of language grammar is grounded within visual structures. While grammar is an essential representation of natural language, it also exists ubiquitously in vision to represent the hierarchical part-whole structure. In this work, we study grounded grammar induction of vision and language in a joint learning framework. Specifically, we present VLGrammar, a method that uses compound probabilistic context-free grammars (compound PCFGs) to induce the language grammar and the image grammar simultaneously. We propose a novel contrastive learning framework to guide the joint learning of both modules. To provide a benchmark for the grounded grammar induction task, we collect a large-scale dataset, \textsc{PartIt}, which contains human-written sentences that describe part-level semantics for 3D objects. Experiments on the \textsc{PartIt} dataset show that VLGrammar outperforms all baselines in image grammar induction and language grammar induction. The learned VLGrammar naturally benefits related downstream tasks. Specifically, it improves the image unsupervised clustering accuracy by 30\%, and performs well in image retrieval and text retrieval. Notably, the induced grammar shows superior generalizability by easily generalizing to unseen categories.",0
"""VLGrammar"" presents a novel approach for inducing joint grammar models that unify visual representations and natural language descriptions. By leveraging grounded corpora in which images and sentences are paired based on human annotations, our method learns shared linguistic structures across vision and language modalities. We validate our model on challenging tasks such as zero-shot image generation, scene understanding, and program synthesis from human instructions, demonstrating significant improvements over state-of-the-art alternatives. Our work highlights the importance of holistic multimodal modeling for advancing artificial intelligence and bridging the gap between high-level semantics and low-level perception.",1
"Humans learn from life events to form intuitions towards the understanding of visual environments and languages. Envision that you are instructed by a high-level instruction, ""Go to the bathroom in the master bedroom and replace the blue towel on the left wall"", what would you possibly do to carry out the task? Intuitively, we comprehend the semantics of the instruction to form an overview of where a bathroom is and what a blue towel is in mind; then, we navigate to the target location by consistently matching the bathroom appearance in mind with the current scene. In this paper, we present an agent that mimics such human behaviors. Specifically, we focus on the Remote Embodied Visual Referring Expression in Real Indoor Environments task, called REVERIE, where an agent is asked to correctly localize a remote target object specified by a concise high-level natural language instruction, and propose a two-stage training pipeline. In the first stage, we pretrain the agent with two cross-modal alignment sub-tasks, namely the Scene Grounding task and the Object Grounding task. The agent learns where to stop in the Scene Grounding task and what to attend to in the Object Grounding task respectively. Then, to generate action sequences, we propose a memory-augmented attentive action decoder to smoothly fuse the pre-trained vision and language representations with the agent's past memory experiences. Without bells and whistles, experimental results show that our method outperforms previous state-of-the-art(SOTA) significantly, demonstrating the effectiveness of our method.",0
"A new approach has been developed for remote embodied visual grounding using scene-intuitive agents. This method involves training agents on large amounts of data from diverse environments, allowing them to learn concepts that are relevant across domains. These agents then use their knowledge to reason about scenes, identify objects, and generate descriptions in natural language. The results demonstrate significant improvements over previous methods in terms of accuracy, efficiency, and scalability. Additionally, the proposed approach can handle complex and dynamic scenarios, making it well suited for real-world applications such as robotics, virtual reality, and augmented reality. Overall, our work represents a major step forward in the field of computer vision and artificial intelligence.",1
"Deep models trained in supervised mode have achieved remarkable success on a variety of tasks. When labeled samples are limited, self-supervised learning (SSL) is emerging as a new paradigm for making use of large amounts of unlabeled samples. SSL has achieved promising performance on natural language and image learning tasks. Recently, there is a trend to extend such success to graph data using graph neural networks (GNNs). In this survey, we provide a unified review of different ways of training GNNs using SSL. Specifically, we categorize SSL methods into contrastive and predictive models. In either category, we provide a unified framework for methods as well as how these methods differ in each component under the framework. Our unified treatment of SSL methods for GNNs sheds light on the similarities and differences of various methods, setting the stage for developing new methods and algorithms. We also summarize different SSL settings and the corresponding datasets used in each setting. To facilitate methodological development and empirical comparison, we develop a standardized testbed for SSL in GNNs, including implementations of common baseline methods, datasets, and evaluation metrics.",0
"Abstract We present a comprehensive review of self-supervised learning techniques applied to graph neural networks (GNNs). Motivated by recent advances towards unifying supervision in GNNs using graph-level pretext tasks on node/edge features and graphs themselves, we provide a survey across three areas: feature extraction on graphs, contrastive approaches and generative methods, as well as a discussion on evaluation metrics specific to these settings. By exploring both types of pretext tasks applied to various benchmark datasets, our study provides insights into which design choices impact performance across different data modalities. For future research directions, we discuss promising opportunities arising from scalability concerns, generalization beyond static graphs, and the potential integration of meta-learning techniques. Our work serves practitioners seeking guidance on model selection and implementation details, as well as researchers looking to gain a broader understanding of current progress and open challenges in self-supervised learning with GNNs. Keywords: Graph neural networks; Self-supervised learning; Pretext task; Contrastive learning; Generative models; Evaluation metrics",1
"Decompilation is the procedure of transforming binary programs into a high-level representation, such as source code, for human analysts to examine. While modern decompilers can reconstruct and recover much information that is discarded during compilation, inferring variable names is still extremely difficult. Inspired by recent advances in natural language processing, we propose a novel solution to infer variable names in decompiled code based on Masked Language Modeling, Byte-Pair Encoding, and neural architectures such as Transformers and BERT. Our solution takes \textit{raw} decompiler output, the less semantically meaningful code, as input, and enriches it using our proposed \textit{finetuning} technique, Constrained Masked Language Modeling. Using Constrained Masked Language Modeling introduces the challenge of predicting the number of masked tokens for the original variable name. We address this \textit{count of token prediction} challenge with our post-processing algorithm. Compared to the state-of-the-art approaches, our trained VarBERT model is simpler and of much better performance. We evaluated our model on an existing large-scale data set with 164,632 binaries and showed that it can predict variable names identical to the ones present in the original source code up to 84.15\% of the time.",0
"Abstract: As software becomes increasingly ubiquitous and complex, reverse engineering has gained importance as an effective tool for understanding how code works, identifying security vulnerabilities, debugging errors, improving performance, and recovering lost source code by inferring variable names from compiled binary files using deobfuscation techniques like control flow graph (CFG) recovery, stack trace analysis, dynamic taint analysis, and symbolic execution. In recent years, machine learning methods have been applied successfully to improve several steps in the reverse engineering pipeline. However, existing approaches largely ignore the semantic relationship between tokens in natural language programs and program variables. In this paper, we present a novel method based on constrained masked language modeling that explicitly models the correlation between natural language-like code sequences and corresponding variable names. Our approach significantly outperforms state-of-the-art deobfuscating tools and demonstrates superior accuracy compared to previous ML-based solutions without any need for external resources, such as internet access during inference time or pre-training on large datasets. We evaluate our proposed model across multiple benchmarks and use cases, highlighting its effectiveness at recovering accurate, high-entropy, and contextually relevant variable names even in heavily obfuscated binaries. Additionally, we demonstrate that incorporating simple syntax constraints can further boost performance. Overall, our work advances the field of variable name recovery and underscores the viability and potential benefits of integrating NLP techniques into modern DE practices.",1
"This paper describes an approach to solving the next destination city recommendation problem for a travel reservation system. We propose a two stages approach: a heuristic approach for candidates selection and an attention neural network model for candidates re-ranking. Our method was inspired by listwise learning-to-rank methods and recent developments in natural language processing and the transformer architecture in particular. We used this approach to solve the Booking.com recommendations challenge Our team achieved 5th place on the challenge using this method, with 0.555 accuracy@4 value on the closed part of the dataset.",0
"""This paper presents a novel approach to recommending cities for travelers to visit based on their past trips and preferences. We propose using an attention-based neural network model that takes into account both the user's historical destinations and their feedback on those trips to rank potential new cities to visit. Our method uses the attention mechanism to weight the importance of each previous city visited by the user, allowing our model to focus on the most relevant experiences when making predictions. We evaluate our proposed approach against several baseline methods and show significant improvement in terms of accuracy and diversity of recommendations. Overall, our results suggest that our attentional neural ranking system can effectively capture users' unique travel patterns and preferences, providing them with personalized suggestions for future trips.""",1
"Among image classification, skip and densely-connection-based networks have dominated most leaderboards. Recently, from the successful development of multi-head attention in natural language processing, it is sure that now is a time of either using a Transformer-like model or hybrid CNNs with attention. However, the former need a tremendous resource to train, and the latter is in the perfect balance in this direction. In this work, to make CNNs handle global and local information, we proposed UPANets, which equips channel-wise attention with a hybrid skip-densely-connection structure. Also, the extreme-connection structure makes UPANets robust with a smoother loss landscape. In experiments, UPANets surpassed most well-known and widely-used SOTAs with an accuracy of 96.47% in Cifar-10, 80.29% in Cifar-100, and 67.67% in Tiny Imagenet. Most importantly, these performances have high parameters efficiency and only trained in one customer-based GPU. We share implementing code of UPANets in https://github.com/hanktseng131415go/UPANets.",0
"Title: ""Learning from the Universal Pixel Attention Network""  Universal Pixel Attention Network (UPANet) is a novel deep learning architecture that outperforms state-of-the-art models on image classification tasks using a fraction of the parameters. By applying the UPA block to convolutional layers, our method learns a non-local dependence structure that captures relationships between all pairs of pixels in an input image. Our model achieves better performance than prior works while reducing computational complexity by up to two orders of magnitude. Experiments show that our method consistently improves over previous architectures across datasets including ImageNet. This work advances our understanding of the role played by attention mechanisms in vision tasks and has implications for efficient deployment in resource-limited settings such as edge computing.",1
"Convolutional video models have an order of magnitude larger computational complexity than their counterpart image-level models. Constrained by computational resources, there is no model or training method that can train long video sequences end-to-end. Currently, the main-stream method is to split a raw video into clips, leading to incomplete fragmentary temporal information flow. Inspired by natural language processing techniques dealing with long sentences, we propose to treat videos as serial fragments satisfying Markov property, and train it as a whole by progressively propagating information through the temporal dimension in multiple steps. This progressive training (PGT) method is able to train long videos end-to-end with limited resources and ensures the effective transmission of information. As a general and robust training method, we empirically demonstrate that it yields significant performance improvements on different models and datasets. As an illustrative example, the proposed method improves SlowOnly network by 3.7 mAP on Charades and 1.9 top-1 accuracy on Kinetics with negligible parameter and computation overhead. Code is available at https://github.com/BoPang1996/PGT.",0
"This should summarize the entire paper and highlight the key contributions. Please use technical language. Thank you. | I am asking for assistance writing an abstract around 150 to 300 words long for a paper titled ""PGT: A Progressive Method for Training Models on Long Videos"". Please provide me the Abstract text.  Also could you please give me the following details so that i can write it. The objective of your research paper The methodology used in your paper. Major findings/results. Implications of these results.",1
"Deep supervised learning has achieved great success in the last decade. However, its deficiencies of dependence on manual labels and vulnerability to attacks have driven people to explore a better solution. As an alternative, self-supervised learning attracts many researchers for its soaring performance on representation learning in the last several years. Self-supervised representation learning leverages input data itself as supervision and benefits almost all types of downstream tasks. In this survey, we take a look into new self-supervised learning methods for representation in computer vision, natural language processing, and graph learning. We comprehensively review the existing empirical methods and summarize them into three main categories according to their objectives: generative, contrastive, and generative-contrastive (adversarial). We further investigate related theoretical analysis work to provide deeper thoughts on how self-supervised learning works. Finally, we briefly discuss open problems and future directions for self-supervised learning. An outline slide for the survey is provided.",0
"Title: ""Self-Supervised Learning: An Analysis of Generative versus Contrastive Approaches""  Abstract:  The field of machine learning has seen significant advancements due to the rise of self-supervised learning (SSL), which enables algorithms to learn from unlabeled data without explicit guidance. There are two main approaches within SSL: generative models that aim to generate new instances resembling those found in the training set, and contrastive models that learn representations by maximizing agreement between augmentations of similar inputs. This study aims to compare these two methods in terms of their effectiveness and applicability across different datasets and domains. Our analysis shows that both generative and contrastive approaches have their own strengths and weaknesses depending on the task at hand, making it important to carefully consider the choice of model for any given problem. We hope our findings provide insights into the design of future SSL systems and spur further research in this exciting area.",1
"Concentration inequalities are widely used for analyzing machine learning algorithms. However, current concentration inequalities cannot be applied to some of the most popular deep neural networks, notably in natural language processing. This is mostly due to the non-causal nature of such involved data, in the sense that each data point depends on other neighbor data points. In this paper, a framework for modeling non-causal random fields is provided and a Hoeffding-type concentration inequality is obtained for this framework. The proof of this result relies on a local approximation of the non-causal random field by a function of a finite number of i.i.d. random variables.",0
"Machine learning algorithms have proven to be highly effective at solving complex problems across various domains. However, most modern approaches rely on causality assumptions that may not hold in practice. Non-causal models have been proposed as alternatives but often suffer from poor generalization performance. In this work, we propose a new approach called deviation bounds which provides tight guarantees on the error of non-causal models while ensuring their stability under small perturbations of the underlying data distribution. We show that our method can significantly improve the accuracy of non-causal models compared to state-of-the-art methods. Our experimental results demonstrate the effectiveness of our approach on multiple benchmark datasets and showcase its advantage over competing baselines. Furthermore, we provide insights into the behavior of deviation bounds by analyzing its theoretical properties, including the connection between deviation bounds and adversarial training, leading to novel improvements in robustness against input variations. Overall, this work represents a significant advancement towards achieving high predictive accuracy in non-causal settings, laying the groundwork for future research in this rapidly developing field.",1
"The attention mechanism has been widely used in deep neural networks as a model component. By now, it has become a critical building block in many state-of-the-art natural language models. Despite its great success established empirically, the working mechanism of attention has not been investigated at a sufficient theoretical depth to date. In this paper, we set up a simple text classification task and study the dynamics of training a simple attention-based classification model using gradient descent. In this setting, we show that, for the discriminative words that the model should attend to, a persisting identity exists relating its embedding and the inner product of its key and the query. This allows us to prove that training must converge to attending to the discriminative words when the attention output is classified by a linear classifier. Experiments are performed, which validate our theoretical analysis and provide further insights.",0
"Recent research has focused on improving attention models through novel architectures and training methods. One key component of these improvements is increasing model capacity, which allows for more accurate representations of complex tasks. However, simply adding computational resources without careful consideration can lead to overfitting and poor generalization performance. In this work, we explore the dynamics of training attention models and propose methods that balance computation and regularization. Our approach shows promising results across multiple domains, demonstrating robustness against various challenges faced during training. By providing insights into how attention models operate, our findings aim to inform future advances in deep learning. Keywords: attention models; dynamics; training; regularization; capacity (hide)",1
"Language model pre-training (LMPT) has achieved remarkable results in natural language understanding. However, LMPT is much less successful in non-natural language domains like protein sequences, revealing a crucial discrepancy between the various sequential domains. Here, we posit that while LMPT can effectively model per-token relations, it fails at modeling per-sequence relations in non-natural language domains. To this end, we develop a framework that couples LMPT with deep structure-preserving metric learning to produce richer embeddings than can be obtained from LMPT alone. We examine new and existing pre-training models in this framework and theoretically analyze the framework overall. We also design experiments on a variety of synthetic datasets and new graph-augmented datasets of proteins and scientific abstracts. Our approach offers notable performance improvements on downstream tasks, including prediction of protein remote homology and classification of citation intent.",0
This could be either the original abstract you provided earlier or something else I wrote but please choose one as the best overall representation of your work!,1
"Multi-Task Learning (MTL) networks have emerged as a promising method for transferring learned knowledge across different tasks. However, MTL must deal with challenges such as: overfitting to low resource tasks, catastrophic forgetting, and negative task transfer, or learning interference. Often, in Natural Language Processing (NLP), a separate model per task is needed to obtain the best performance. However, many fine-tuning approaches are both parameter inefficient, i.e., potentially involving one new model per task, and highly susceptible to losing knowledge acquired during pretraining. We propose a novel Transformer architecture consisting of a new conditional attention mechanism as well as a set of task-conditioned modules that facilitate weight sharing. Through this construction, we achieve more efficient parameter sharing and mitigate forgetting by keeping half of the weights of a pretrained model fixed. We also use a new multi-task data sampling strategy to mitigate the negative effects of data imbalance across tasks. Using this approach, we are able to surpass single task fine-tuning methods while being parameter and data efficient (using around 66% of the data for weight updates). Compared to other BERT Large methods on GLUE, our 8-task model surpasses other Adapter methods by 2.8% and our 24-task model outperforms by 0.7-1.0% models that use MTL and single task fine-tuning. We show that a larger variant of our single multi-task model approach performs competitively across 26 NLP tasks and yields state-of-the-art results on a number of test and development sets. Our code is publicly available at https://github.com/CAMTL/CA-MTL.",0
"A novel approach has been developed that improves transfer learning in natural language processing (NLP) using fewer parameters and less data. This method, known as conditionally adaptive multi-task learning, allows multiple tasks to share the same neural network architecture while allowing each task to learn unique representations that are optimized specifically for their individual goals. By doing so, we demonstrate significant improvements in performance on low-resource benchmarks while maintaining strong generalization capabilities across all tasks. Our method achieves this balance by dynamically selecting which layers in the shared model receive gradient updates during training, based on the current task at hand. With this simple change, our model can now perform more effectively with far fewer parameters than traditional methods would require. We evaluate the effectiveness of our system through rigorous testing and provide a detailed analysis of its strengths and limitations compared to existing approaches in the field. Overall, we believe this work provides valuable insights into how to improve the efficiency and efficacy of large-scale NLP models, particularly in challenging environments where resources may be scarce. Our findings have important implications for future research into the design and development of modern deep learning systems and reinforces the importance of exploring new ways to optimize these complex architectures.",1
"Machine Learning in general and Deep Learning in particular has gained much interest in the recent decade and has shown significant performance improvements for many Computer Vision or Natural Language Processing tasks. In order to deal with databases which have just a small amount of training samples or to deal with models which have large amount of parameters, the regularization is indispensable. In this paper, we enforce the manifold preservation (manifold learning) from the original data into latent presentation by using ""manifold attack"". The later is inspired in a fashion of adversarial learning : finding virtual points that distort mostly the manifold preservation then using these points as supplementary samples to train the model. We show that our approach of regularization provides improvements for the accuracy rate and for the robustness to adversarial examples.",0
"In recent years, there has been a growing concern over adversarial attacks against machine learning models. Adversarial examples, deliberately generated inputs designed to fool the model into making incorrect predictions, pose a serious challenge to the reliability and security of these systems. This study proposes a new approach called ""Manifold Attack"" that exploits the geometric properties of high dimensional spaces and can generate highly effective adversarial examples with little prior knowledge. The key insight behind our method is that the decision boundary of many machine learning algorithms takes on a complex nonlinear form even in high dimensions, leading to multiple ""manifolds"" in the feature space along which successful attacks can be constructed. We demonstrate the effectiveness of Manifold Attack through extensive experiments across different types of deep neural networks and datasets, showing that it outperforms several state-of-the-art methods by significant margins. Our work highlights the importance of understanding the intrinsic geometry of data distributions for both developing better machine learning algorithms and defending against potential attacks.",1
"Data augmentation is an effective technique to improve the generalization of deep neural networks. However, previous data augmentation methods usually treat the augmented samples equally without considering their individual impacts on the model. To address this, for the augmented samples from the same training example, we propose to assign different weights to them. We construct the maximal expected loss which is the supremum over any reweighted loss on augmented samples. Inspired by adversarial training, we minimize this maximal expected loss (MMEL) and obtain a simple and interpretable closed-form solution: more attention should be paid to augmented samples with large loss values (i.e., harder examples). Minimizing this maximal expected loss enables the model to perform well under any reweighting strategy. The proposed method can generally be applied on top of any data augmentation methods. Experiments are conducted on both natural language understanding tasks with token-level data augmentation, and image classification tasks with commonly-used image augmentation techniques like random crop and horizontal flip. Empirical results show that the proposed method improves the generalization performance of the model.",0
"In this paper we present the problem of reweighting augmented samples minimizing the maximal expected loss, which provides a principled approach for ensuring that the generated samples have desirable properties such as class balance and inclusion of rare events. To address this problem we propose a novel framework based on importance sampling techniques from probability theory, and develop algorithms that can efficiently implement this approach for both continuous and discrete distributions. We demonstrate the effectiveness of our method through experiments on several benchmark datasets, showing that our algorithm can effectively improve model performance while reducing training time compared to alternative approaches.",1
"We aim to address the problem of Natural Language Video Localization (NLVL)-localizing the video segment corresponding to a natural language description in a long and untrimmed video. State-of-the-art NLVL methods are almost in one-stage fashion, which can be typically grouped into two categories: 1) anchor-based approach: it first pre-defines a series of video segment candidates (e.g., by sliding window), and then does classification for each candidate; 2) anchor-free approach: it directly predicts the probabilities for each video frame as a boundary or intermediate frame inside the positive segment. However, both kinds of one-stage approaches have inherent drawbacks: the anchor-based approach is susceptible to the heuristic rules, further limiting the capability of handling videos with variant length. While the anchor-free approach fails to exploit the segment-level interaction thus achieving inferior results. In this paper, we propose a novel Boundary Proposal Network (BPNet), a universal two-stage framework that gets rid of the issues mentioned above. Specifically, in the first stage, BPNet utilizes an anchor-free model to generate a group of high-quality candidate video segments with their boundaries. In the second stage, a visual-language fusion layer is proposed to jointly model the multi-modal interaction between the candidate and the language query, followed by a matching score rating layer that outputs the alignment score for each candidate. We evaluate our BPNet on three challenging NLVL benchmarks (i.e., Charades-STA, TACoS and ActivityNet-Captions). Extensive experiments and ablative studies on these datasets demonstrate that the BPNet outperforms the state-of-the-art methods.",0
"Abstract: This paper presents a novel approach for natural language video localization based on boundary proposals. Our method addresses two key challenges in current state-of-the-art approaches for NLVL: (i) limited representation power of individual bounding boxes due to their rigid nature, and (ii) computational inefficiency caused by exhaustive search over all possible combinations of object detections. To overcome these limitations, we propose using boundary proposal networks that generate flexible and accurate boundaries around objects. We show that our network can effectively capture the rich semantic information present in natural language queries, leading to more precise localizations. Additionally, we introduce a new two-stage architecture where the first stage generates coarse, but efficient, localizations followed by refinement at the second stage. Experiments demonstrate significant improvements over baseline methods across multiple datasets. Our approach sets a new standard for efficiency and accuracy in natural language video localization research.",1
"Negotiation, as an essential and complicated aspect of online shopping, is still challenging for an intelligent agent. To that end, we propose the Price Negotiator, a modular deep neural network that addresses the unsolved problems in recent studies by (1) considering images of the items as a crucial, though neglected, source of information in a negotiation, (2) heuristically finding the most similar items from an external online source to predict the potential value and an acceptable agreement price, (3) predicting a general price-based action at each turn which is fed into the language generator to output the supporting natural language, and (4) adjusting the prices based on the predicted actions. Empirically, we show that our model, that is trained in both supervised and reinforcement learning setting, significantly improves negotiation on the CraigslistBargain dataset, in terms of the agreement price, price consistency, and dialogue quality.",0
"This paper presents a new approach to negotiation that leverages online value look-ahead (OVL) techniques to improve the negotiating process. OVL involves using available information to predict future values and make better decisions based on those predictions. In the context of negotiation, OVL can be used to identify areas where parties may have different beliefs about the value of goods or services, allowing them to find common ground and reach agreements more efficiently.  The proposed method uses game theory to model negotiation scenarios and guide decision making. By looking ahead at potential outcomes, negotiators can evaluate the consequences of their actions and adjust their strategies accordingly. This allows both sides to arrive at mutually beneficial agreements while minimizing risk and uncertainty.  Experimental results demonstrate the effectiveness of the approach, showing significant improvements over traditional negotiation methods. The paper concludes by discussing future directions for research in this area, including expanding the scope of OVL applications and exploring new ways to incorporate machine learning into negotiation processes. Overall, the work presented here offers valuable insights into the use of technology in negotiations and has important implications for businesses, policymakers, and society as a whole.",1
"We present a locality preserving loss (LPL) that improves the alignment between vector space embeddings while separating uncorrelated representations. Given two pretrained embedding manifolds, LPL optimizes a model to project an embedding and maintain its local neighborhood while aligning one manifold to another. This reduces the overall size of the dataset required to align the two in tasks such as cross-lingual word alignment. We show that the LPL-based alignment between input vector spaces acts as a regularizer, leading to better and consistent accuracy than the baseline, especially when the size of the training set is small. We demonstrate the effectiveness of LPL optimized alignment on semantic text similarity (STS), natural language inference (SNLI), multi-genre language inference (MNLI) and cross-lingual word alignment(CLA) showing consistent improvements, finding up to 16% improvement over our baseline in lower resource settings.",0
"This paper presents a new approach for preserving locality in deep learning models, which is important for applications where spatial relationships among features matter. We introduce a novel loss function we call ""locality preserving loss"" (LPL) that encourages neighboring points in the data space to have similar representations in the feature space. Our method works by penalizing large distances between neighbors in both data space and feature space simultaneously, effectively aligning them close together. By minimizing our LPL during training, we show significant improvements over baseline methods on several challenging tasks such as image classification and semantic segmentation. Importantly, our method requires no changes to existing neural network architectures, making it easy to adopt in practice. In summary, our work demonstrates the effectiveness of our LPL in producing more locally coherent embeddings while achieving state-of-the-art results across multiple domains.",1
"Integration is indispensable, not only in mathematics, but also in a wide range of other fields. A deep learning method has recently been developed and shown to be capable of integrating mathematical functions that could not previously be integrated on a computer. However, that method treats integration as equivalent to natural language translation and does not reflect mathematical information. In this study, we adjusted the learning model to take mathematical information into account and developed a wide range of learning models that learn the order of numerical operations more robustly. In this way, we achieved a 98.80% correct answer rate with symbolic integration, a higher rate than that of any existing method. We judged the correctness of the integration based on whether the derivative of the primitive function was consistent with the integrand. By building an integrated model based on this strategy, we achieved a 99.79% rate of correct answers with symbolic integration.",0
"This paper presents a novel approach to symbolic integration that combines multiple learning models with complementary strengths and weaknesses. Our method leverages recent advances in deep neural networks (DNNs), which have achieved state-of-the-art performance on many complex integration problems. However, DNNs can struggle when faced with input functions outside their training distribution or when asked to perform highly accurate numerical computations. To address these limitations, we propose augmenting our DNN integration model with traditional symbolic calculus methods and other machine learning techniques that excel at handling special function classes, Diophantine approximations, and analytical continuation. We evaluate our hybrid system using benchmark tests from diverse mathematical areas such as number theory, physics, signal processing, and computer graphics. Our experiments demonstrate substantial improvements over standalone integration models in terms of speed, accuracy, and reliability. We conclude that combining learning models with different capabilities offers significant benefits for tackling hard integration challenges beyond human comprehension and tractability.",1
"Transformer is a powerful tool for many natural language tasks which is based on self-attention, a mechanism that encodes the dependence of other tokens on each specific token, but the computation of self-attention is a bottleneck due to its quadratic time complexity. There are various approaches to reduce the time complexity and approximation of matrix is one such. In Nystr\""omformer, the authors used Nystr\""om based method for approximation of softmax. The Nystr\""om method generates a fast approximation to any large-scale symmetric positive semidefinite (SPSD) matrix using only a few columns of the SPSD matrix. However, since the Nystr\""om approximation is low-rank when the spectrum of the SPSD matrix decays slowly, the Nystr\""om approximation is of low accuracy. Here an alternative method is proposed for approximation which has a much stronger error bound than the Nystr\""om method. The time complexity of this same as Nystr\""omformer which is $O\left({n}\right)$.",0
In this paper we propose an alternative to the widely popular Nyströmformer architecture which approximates global attention through local attention. Our method introduces spectral shifting to reduce complexity while maintaining accuracy. We evaluate our approach on four NLP benchmark datasets and achieve state-of-the-art results without requiring large models. This work demonstrates that attention approximations can indeed improve performance beyond previous methods.,1
"Due to the rapid emergence of short videos and the requirement for content understanding and creation, the video captioning task has received increasing attention in recent years. In this paper, we convert traditional video captioning task into a new paradigm, \ie, Open-book Video Captioning, which generates natural language under the prompts of video-content-relevant sentences, not limited to the video itself. To address the open-book video captioning problem, we propose a novel Retrieve-Copy-Generate network, where a pluggable video-to-text retriever is constructed to retrieve sentences as hints from the training corpus effectively, and a copy-mechanism generator is introduced to extract expressions from multi-retrieved sentences dynamically. The two modules can be trained end-to-end or separately, which is flexible and extensible. Our framework coordinates the conventional retrieval-based methods with orthodox encoder-decoder methods, which can not only draw on the diverse expressions in the retrieved sentences but also generate natural and accurate content of the video. Extensive experiments on several benchmark datasets show that our proposed approach surpasses the state-of-the-art performance, indicating the effectiveness and promising of the proposed paradigm in the task of video captioning.",0
"This research presents an approach to open-book video captioning using a retrieval system combined with deep learning models based on Generative Adversarial Networks (GAN). We introduce a novel architecture that utilizes both textual context from external sources such as subtitles or closed-captioned transcripts and visual content from the input video. Our model leverages state-of-the-art machine translation techniques to retrieve relevant sections of text from books related to the topic of the video, which can then be used along with the generated captions to improve accuracy and coherence. Evaluations demonstrate significant improvements over previous approaches, achieving impressive results comparable to human performance. Additionally, we showcase our method’s ability to generate accurate descriptions for challenging scenarios involving unconventional events, minority languages, and complex concepts.",1
"In meta-learning, the knowledge learned from previous tasks is transferred to new ones, but this transfer only works if tasks are related. Sharing information between unrelated tasks might hurt performance, and it is unclear how to transfer knowledge across tasks with a hierarchical structure. Our research extends a model agnostic meta-learning model, MAML, by exploiting hierarchical task relationships. Our algorithm, TreeMAML, adapts the model to each task with a few gradient steps, but the adaptation follows the hierarchical tree structure: in each step, gradients are pooled across tasks clusters, and subsequent steps follow down the tree. We also implement a clustering algorithm that generates the tasks tree without previous knowledge of the task structure, allowing us to make use of implicit relationships between the tasks. We show that the new algorithm, which we term TreeMAML, performs better than MAML when the task structure is hierarchical for synthetic experiments. To study the performance of the method in real-world data, we apply this method to Natural Language Understanding, we use our algorithm to finetune Language Models taking advantage of the language phylogenetic tree. We show that TreeMAML improves the state of the art results for cross-lingual Natural Language Inference. This result is useful, since most languages in the world are under-resourced and the improvement on cross-lingual transfer allows the internationalization of NLP models. This results open the window to use this algorithm in other real-world hierarchical datasets.",0
"In recent years, meta learning has emerged as a powerful tool for improving the performance of machine learning models by enabling them to learn from few data points or even one example. Model Agnostic Meta Learning (MAML) is a popular method that uses gradients computed over multiple tasks to achieve fast adaptation to new tasks. However, most prior work on MAML has focused on problems where the input space is continuous and high dimensional. This paper proposes a novel approach called ""Meta-learning with MAML on Trees"" which extends the use of MAML to tree structured domains such as natural language processing and computer vision. Our method leverages the structure present in these problems to design more effective gradient updates that significantly improve performance compared to existing methods. We demonstrate the effectiveness of our approach on several benchmark datasets and show that it can consistently outperform state-of-the art baselines across a wide range of tasks. Overall, our results highlight the potential of using meta-learning with MAML on trees for solving complex real world problems with minimal training data.",1
"The classification accuracy of deep learning models depends not only on the size of their training sets, but also on the quality of their labels. In medical image classification, large-scale datasets are becoming abundant, but their labels will be noisy when they are automatically extracted from radiology reports using natural language processing tools. Given that deep learning models can easily overfit these noisy-label samples, it is important to study training approaches that can handle label noise. In this paper, we adapt a state-of-the-art (SOTA) noisy-label multi-class training approach to learn a multi-label classifier for the dataset Chest X-ray14, which is a large scale dataset known to contain label noise in the training set. Given that this dataset also has label noise in the testing set, we propose a new theoretically sound method to estimate the performance of the model on a hidden clean testing data, given the result on the noisy testing data. Using our clean data performance estimation, we notice that the majority of label noise on Chest X-ray14 is present in the class 'No Finding', which is intuitively correct because this is the most likely class to contain one or more of the 14 diseases due to labelling mistakes.",0
"In recent years, deep learning has revolutionized medical image analysis by providing unprecedented performance across many tasks such as classification, segmentation, and detection. A key bottleneck in training these models is obtaining high quality labeled data, which can be prohibitively expensive and time consuming to obtain. One approach to mitigate the label scarcity problem is through weakly supervised methods that leverage additional sources of information beyond fully annotated images to train classifiers. However, these approaches often assume the availability of large amounts of partially labeled data, which may not always be realistic. In this work, we propose NoisyLabelLearning (NLL), a new algorithm for large scale medical image classification in the setting where only small numbers of examples have been manually annotated. NLL combines semi-supervised learning techniques, including noise contrastive estimation and pseudo-label generation, allowing us to generate synthetic annotations from other modalities like CT scans, MRIs, or pathology reports. We then use existing deep learning architectures to learn a joint embedding space between these modality views using a triplet loss. Finally, we use our generated labels to fine tune classifiers on large datasets with very few manual annotations. By evaluating our method on multiple benchmarks including breast cancer histopathological image classification and chest xray pneumonia classification, we show state-of-the art results significantly outperforming previous semi-supervised methods. Our codebase is open source and freely available online.",1
"The Traveling Salesman Problem (TSP) is the most popular and most studied combinatorial problem, starting with von Neumann in 1951. It has driven the discovery of several optimization techniques such as cutting planes, branch-and-bound, local search, Lagrangian relaxation, and simulated annealing. The last five years have seen the emergence of promising techniques where (graph) neural networks have been capable to learn new combinatorial algorithms. The main question is whether deep learning can learn better heuristics from data, i.e. replacing human-engineered heuristics? This is appealing because developing algorithms to tackle efficiently NP-hard problems may require years of research, and many industry problems are combinatorial by nature. In this work, we propose to adapt the recent successful Transformer architecture originally developed for natural language processing to the combinatorial TSP. Training is done by reinforcement learning, hence without TSP training solutions, and decoding uses beam search. We report improved performances over recent learned heuristics with an optimal gap of 0.004% for TSP50 and 0.39% for TSP100.",0
"In recent years, there has been a growing interest in using machine learning algorithms to solve optimization problems traditionally tackled by exact methods, such as the traveling salesman problem (TSP). This work presents a new approach that utilizes the transformer network architecture, which has achieved state-of-the-art results on natural language processing tasks, to find near-optimal solutions for TSP instances. We demonstrate the effectiveness of our method through extensive experiments on benchmark datasets, showing that it can outperform existing machine learning based approaches while competitively matching the performance of proven exact algorithms. Our study shows promise for future research into applying neural networks to solve complex combinatorial optimizations tasks.",1
"Visual Question Answering (VQA) is an extremely stimulating and challenging research area where Computer Vision (CV) and Natural Language Processig (NLP) have recently met. In image captioning and video summarization, the semantic information is completely contained in still images or video dynamics, and it has only to be mined and expressed in a human-consistent way. Differently from this, in VQA semantic information in the same media must be compared with the semantics implied by a question expressed in natural language, doubling the artificial intelligence-related effort. Some recent surveys about VQA approaches have focused on methods underlying either the image-related processing or the verbal-related one, or on the way to consistently fuse the conveyed information. Possible applications are only suggested, and, in fact, most cited works rely on general-purpose datasets that are used to assess the building blocks of a VQA system. This paper rather considers the proposals that focus on real-world applications, possibly using as benchmarks suitable data bound to the application domain. The paper also reports about some recent challenges in VQA research.",0
"In this study, we examine how visual question answering (VQA) can be used across different domains. We present multiple experiments that demonstrate VQA’s effectiveness in tasks such as image retrieval, caption generation, and machine translation. Our results show that using VQA consistently leads to improved performance over baseline models. Additionally, we find that VQA achieves state-of-the art accuracy on several challenging datasets. Finally, we conclude by discussing potential future directions for research in VQA. Overall, our work highlights the promise of using VQA for natural language processing and computer vision tasks.",1
"We tackle the problem of producing compact models, maximizing their accuracy for a given model size. A standard solution is to train networks with Quantization Aware Training, where the weights are quantized during training and the gradients approximated with the Straight-Through Estimator. In this paper, we extend this approach to work beyond int8 fixed-point quantization with extreme compression methods where the approximations introduced by STE are severe, such as Product Quantization. Our proposal is to only quantize a different random subset of weights during each forward, allowing for unbiased gradients to flow through the other weights. Controlling the amount of noise and its form allows for extreme compression rates while maintaining the performance of the original model. As a result we establish new state-of-the-art compromises between accuracy and model size both in natural language processing and image classification. For example, applying our method to state-of-the-art Transformer and ConvNet architectures, we can achieve 82.5% accuracy on MNLI by compressing RoBERTa to 14MB and 80.0 top-1 accuracy on ImageNet by compressing an EfficientNet-B3 to 3.3MB.",0
"In recent years, deep learning has achieved significant successes across many fields due to advances in computing power and data availability. Despite these achievements, deep learning models often require large amounts of computational resources and memory, making them difficult to deploy on resource-constrained devices such as smartphones, drones, and embedded systems. To address this issue, model compression techniques have been developed to reduce the size of deep learning models while maintaining their accuracy. One approach to model compression is quantization, which replaces floating point weights with integers during training and inference. However, using quantized gradients can lead to severe degradation in model performance due to noise introduced by the quantization process. This study explores methods to mitigate the negative effects of quantization noise on the training of neural networks, allowing for more extreme levels of model compression without sacrificing accuracy. Our results show that incorporating regularization into the training process and adjusting hyperparameters such as batch size can significantly improve the stability and generalization ability of quantized models, even under high levels of compression. These findings suggest new directions for developing efficient machine learning algorithms that can operate effectively on limited hardware resources.",1
"State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",0
"This study presents a new approach for learning transferable visual models from natural language supervision. By leveraging large amounts of textual data that describe images, we can train artificial intelligence systems to generate high-quality image predictions based on verbal descriptions alone. Our method uses deep learning techniques and can be applied to a wide range of tasks related to computer vision. We demonstrate the effectiveness of our system through comprehensive experiments and show that our model outperforms state-of-the-art methods in several benchmark datasets. Overall, this work represents a significant step forward in enabling computers to learn about the world through natural language, paving the way for exciting applications such as image generation and question answering.",1
"MLPerf Mobile is the first industry-standard open-source mobile benchmark developed by industry members and academic researchers to allow performance/accuracy evaluation of mobile devices with different AI chips and software stacks. The benchmark draws from the expertise of leading mobile-SoC vendors, ML-framework providers, and model producers. In this paper, we motivate the drive to demystify mobile-AI performance and present MLPerf Mobile's design considerations, architecture, and implementation. The benchmark comprises a suite of models that operate under standard models, data sets, quality metrics, and run rules. For the first iteration, we developed an app to provide an ""out-of-the-box"" inference-performance benchmark for computer vision and natural-language processing on mobile devices. MLPerf Mobile can serve as a framework for integrating future models, for customizing quality-target thresholds to evaluate system performance, for comparing software frameworks, and for assessing heterogeneous-hardware capabilities for machine learning, all fairly and faithfully with fully reproducible results.",0
"Here's a possible abstract:  The performance of machine learning models on mobile devices has become increasingly important as more users rely on their smartphones for tasks such as image recognition, natural language processing, and other data-driven applications. However, measuring the performance of these models can be challenging due to differences in hardware, software configurations, and use cases. To address this need, we introduce MLPerf Mobile Inference Benchmark, which provides standardized benchmarks for evaluating the speed and accuracy of machine learning models running on Android phones and tablets. Our benchmark includes diverse workloads covering vision, speech, translation, and retrieval tasks. We evaluate multiple popular mobile platforms and frameworks including TensorFlow Lite, PyTorch Mobile, Core ML, and Android NNAPI. By providing industry-standard metrics, we aim to drive innovation and competition among vendors, making it easier for developers to select the most appropriate platform for their needs. Ultimately, our goal is to enable better experiences for end-users by empowering developers to create high-quality machine learning apps that run efficiently on mobile devices.",1
"Sign language is the primary language for people with a hearing loss. Sign language recognition (SLR) is the automatic recognition of sign language, which represents a challenging problem for computers, though some progress has been made recently using deep learning. Huge amounts of data are generally required to train deep learning models. However, corresponding datasets are missing for the majority of sign languages. Transfer learning is a technique to utilize a related task with an abundance of data available to help solve a target task lacking sufficient data. Transfer learning has been applied highly successfully in computer vision and natural language processing. However, much less research has been conducted in the field of SLR. This paper investigates how effectively transfer learning can be applied to isolated SLR using an inflated 3D convolutional neural network as the deep learning architecture. Transfer learning is implemented by pre-training a network on the American Sign Language dataset MS-ASL and subsequently fine-tuning it separately on three different sizes of the German Sign Language dataset SIGNUM. The results of the experiments give clear empirical evidence that transfer learning can be effectively applied to isolated SLR. The accuracy performances of the networks applying transfer learning increased substantially by up to 21% as compared to the baseline models that were not pre-trained on the MS-ASL dataset.",0
"This study proposes to use transfer learning on a pretrained deep convolutional neural network (CNN) model for sign language recognition by inflating the bottleneck layer features into a larger representation space. We aim to leverage the knowledge from large datasets such as ImageNet and fine tune our model to recognize American Sign Language signs accurately. Our method uses two sets of data: one set to extract feature representations and another smaller set to finetune the weights of the fully connected layers. Our proposed approach achieves state-of-the-art accuracy on the RWTH-PHOENIX-Weather dataset. Additionally, we compare the performance of different CNN models that have been trained on either 2D images or depth maps extracted from videos, showing that using depth maps leads to better results. Finally, we visualize the activation maps of the inflated CNN model to show that it learns meaningful features relevant to sign language. Overall, our work demonstrates the effectiveness of using transfer learning and inflation techniques in enhancing CNNs for sign language recognition tasks.",1
"Large scale recommender models find most relevant items from huge catalogs, and they play a critical role in modern search and recommendation systems. To model the input space with large-vocab categorical features, a typical recommender model learns a joint embedding space through neural networks for both queries and items from user feedback data. However, with millions to billions of items in the corpus, users tend to provide feedback for a very small set of them, causing a power-law distribution. This makes the feedback data for long-tail items extremely sparse.   Inspired by the recent success in self-supervised representation learning research in both computer vision and natural language understanding, we propose a multi-task self-supervised learning (SSL) framework for large-scale item recommendations. The framework is designed to tackle the label sparsity problem by learning better latent relationship of item features. Specifically, SSL improves item representation learning as well as serving as additional regularization to improve generalization. Furthermore, we propose a novel data augmentation method that utilizes feature correlations within the proposed framework.   We evaluate our framework using two real-world datasets with 500M and 1B training examples respectively. Our results demonstrate the effectiveness of SSL regularization and show its superior performance over the state-of-the-art regularization techniques. We also have already launched the proposed techniques to a web-scale commercial app-to-app recommendation system, with significant improvements top-tier business metrics demonstrated in A/B experiments on live traffic. Our online results also verify our hypothesis that our framework indeed improves model performance even more on slices that lack supervision.",0
"This paper presents a self-supervised learning method called Pairwise Dependency Embedding (PDE) which can learn item representations without explicit feedback signals by leveraging pairwise relationships among items. By casting the task as a dimensionality reduction problem on item pairs, we propose PDE that jointly models item co-occurrences with user behavior data under a unified framework. Comprehensive experiments on large scale real-world datasets demonstrate the effectiveness and efficiency of our approach against several strong baselines. In addition, we present extensive analyses over different dimensions and discuss implications on recommender systems design beyond accuracy gain. Finally, we believe our work opens up exciting future directions for efficient and accurate recommendations under limited supervision scenarios in emerging applications such as conversational recommendation and personalized search.",1
"Product embeddings have been heavily investigated in the past few years, serving as the cornerstone for a broad range of machine learning applications in e-commerce. Despite the empirical success of product embeddings, little is known on how and why they work from the theoretical standpoint. Analogous results from the natural language processing (NLP) often rely on domain-specific properties that are not transferable to the e-commerce setting, and the downstream tasks often focus on different aspects of the embeddings. We take an e-commerce-oriented view of the product embeddings and reveal a complete theoretical view from both the representation learning and the learning theory perspective. We prove that product embeddings trained by the widely-adopted skip-gram negative sampling algorithm and its variants are sufficient dimension reduction regarding a critical product relatedness measure. The generalization performance in the downstream machine learning task is controlled by the alignment between the embeddings and the product relatedness measure. Following the theoretical discoveries, we conduct exploratory experiments that supports our theoretical insights for the product embeddings.",0
"Abstract This is a sample of a theoretical understanding of product embedding for e-commerce machine learning that could serve as the foundation for developing better product recommendations, search results, product discovery tools, visual design elements, or even chatbots for customer service. With advances in natural language processing (NLP) and deep learning techniques such as latent semantic analysis (LSA), researchers can uncover important relationships among products without relying on explicit keywords. By utilizing distributed representations, these algorithms capture fine-grained connections across diverse dimensions like text descriptions, categories, images, brands, prices, etc., allowing for accurate predictive modeling and personalized suggestions. Incorporating knowledge graph embeddings further enhances performance by providing structured contexts around entities and their attributes. Ultimately, identifying the optimal combination of features and models requires experimentation, which can benefit from domain-specific expertise from data engineers who collaborate with experts in ML development. To enable reproducibility and facilitate collaboration, open source libraries and pipelines become essential components of successful product embedding workflows. Overall, building effective recommendation systems warrants a synergy between NLP, computer vision, knowledge graphs, and machine learning, with continuous refinement through empirical evaluation against user feedback metrics. Keywords: product embedding; e-commerce; machine learning; feature engineering; recommender system; NLP; LSA; knowledge graph; collaborative filtering; matrix factorization; latent semantic analysis; CCA; Canonical Correlation Analysis; UMAP; Universal Maps",1
"Weaknesses in computer systems such as faults, bugs and errors in the architecture, design or implementation of software provide vulnerabilities that can be exploited by attackers to compromise the security of a system. Common Weakness Enumerations (CWE) are a hierarchically designed dictionary of software weaknesses that provide a means to understand software flaws, potential impact of their exploitation, and means to mitigate these flaws. Common Vulnerabilities and Exposures (CVE) are brief low-level descriptions that uniquely identify vulnerabilities in a specific product or protocol. Classifying or mapping of CVEs to CWEs provides a means to understand the impact and mitigate the vulnerabilities. Since manual mapping of CVEs is not a viable option, automated approaches are desirable but challenging.   We present a novel Transformer-based learning framework (V2W-BERT) in this paper. By using ideas from natural language processing, link prediction and transfer learning, our method outperforms previous approaches not only for CWE instances with abundant data to train, but also rare CWE classes with little or no data to train. Our approach also shows significant improvements in using historical data to predict links for future instances of CVEs, and therefore, provides a viable approach for practical applications. Using data from MITRE and National Vulnerability Database, we achieve up to 97% prediction accuracy for randomly partitioned data and up to 94% prediction accuracy in temporally partitioned data. We believe that our work will influence the design of better methods and training models, as well as applications to solve increasingly harder problems in cybersecurity.",0
"Title: An Abstract for ""V2W-BERT: A Framework for Effective Hierarchical Multiclass Classification of Software Vulnerabilities""  Effectively classifying software vulnerabilities is crucial for maintaining secure systems. However, traditional approaches have limitations that hinder their effectiveness. To address these issues, we propose V2W-BERT, a framework designed for hierarchical multiclass classification of software vulnerabilities. We present evidence from our experiments using standard datasets, demonstrating the superiority of V2W-BERT over state-of-the-art methods. Our novel approach utilizes pre-trained transformer models to encode textual representations of vulnerability reports, enabling better understanding of complex dependencies among classes. Furthermore, we incorporate a hierarchy into the model by predicting higher-level labels first, reducing the search space for lower-level predictions. In summary, V2W-BERT offers significant improvements to the field of software vulnerability classification, with promising applications in real-world settings.",1
"Semantic embeddings have advanced the state of the art for countless natural language processing tasks, and various extensions to multimodal domains, such as visual-semantic embeddings, have been proposed. While the power of visual-semantic embeddings comes from the distillation and enrichment of information through machine learning, their inner workings are poorly understood and there is a shortage of analysis tools. To address this problem, we generalize the notion of probing tasks to the visual-semantic case. To this end, we (i) discuss the formalization of probing tasks for embeddings of image-caption pairs, (ii) define three concrete probing tasks within our general framework, (iii) train classifiers to probe for those properties, and (iv) compare various state-of-the-art embeddings under the lens of the proposed probing tasks. Our experiments reveal an up to 12% increase in accuracy on visual-semantic embeddings compared to the corresponding unimodal embeddings, which suggest that the text and image dimensions represented in the former do complement each other.",0
"""Probing Multimodal Embeddings for Linguistic Properties"" investigates the effectiveness of multimodal embeddings in capturing linguistic properties such as syntax, semantics, sentiment analysis, and other related tasks. In particular, we focus on visual-semantic representations which have gained popularity due to their ability to encode both textual and visual features into a single vector space. Our work presents empirical studies that assess the performance of these models on a range of NLP (Natural Language Processing) tasks and showcases how they can provide significant improvements over traditional techniques. We demonstrate the utility of our approach by evaluating the quality of generated semantic outputs using human annotations and automated metrics. By providing insights into the strengths and weaknesses of existing methods, we aim to further push research towards developing more accurate and efficient ways of combining multiple modalities to enhance language understanding.",1
"Transformer is a ubiquitous model for natural language processing and has attracted wide attentions in computer vision. The attention maps are indispensable for a transformer model to encode the dependencies among input tokens. However, they are learned independently in each layer and sometimes fail to capture precise patterns. In this paper, we propose a novel and generic mechanism based on evolving attention to improve the performance of transformers. On one hand, the attention maps in different layers share common knowledge, thus the ones in preceding layers can instruct the attention in succeeding layers through residual connections. On the other hand, low-level and high-level attentions vary in the level of abstraction, so we adopt convolutional layers to model the evolutionary process of attention maps. The proposed evolving attention mechanism achieves significant performance improvement over various state-of-the-art models for multiple tasks, including image classification, natural language understanding and machine translation.",0
"Recent advances in computer vision have led to significant improvements in image classification accuracy through convolutional neural networks (CNN). However, these models typically process images sequentially, from left to right and top to bottom, which can lead to suboptimal feature extraction and slow inference speeds. In order to address these issues, we propose a new architecture that combines residual connections with attention mechanisms, allowing the network to selectively focus on important regions of the input image. Our approach is based on the idea that certain features may be more informative than others, and by learning to attend to those features the model can improve its performance without increasing complexity. We demonstrate the effectiveness of our method on several challenging benchmark datasets, outperforming previous state-of-the-art methods in terms of both accuracy and speed. Overall, our work represents an important step towards developing more efficient and effective visual recognition systems.",1
"This paper attacks the challenging problem of video retrieval by text. In such a retrieval paradigm, an end user searches for unlabeled videos by ad-hoc queries described exclusively in the form of a natural-language sentence, with no visual example provided. Given videos as sequences of frames and queries as sequences of words, an effective sequence-to-sequence cross-modal matching is crucial. To that end, the two modalities need to be first encoded into real-valued vectors and then projected into a common space. In this paper we achieve this by proposing a dual deep encoding network that encodes videos and queries into powerful dense representations of their own. Our novelty is two-fold. First, different from prior art that resorts to a specific single-level encoder, the proposed network performs multi-level encoding that represents the rich content of both modalities in a coarse-to-fine fashion. Second, different from a conventional common space learning algorithm which is either concept based or latent space based, we introduce hybrid space learning which combines the high performance of the latent space and the good interpretability of the concept space. Dual encoding is conceptually simple, practically effective and end-to-end trained with hybrid space learning. Extensive experiments on four challenging video datasets show the viability of the new method.",0
"This paper introduces dual encoding video retrieval by text, which leverages both audio-visual features (AV) and image-text embedding (ITE). Dual encoding combines AV-features derived from video frames and ITE-embeddings obtained via Vision-Language models trained on vast amounts of Internet data. We argue that using two distinct modalities improves search quality over prior state-of-the-art methods. Our approach enhances performance compared to previous work across all metrics: mAP@Rank20 increases by more than 7 points; nDCG@Rank20 increases more than 0.4; while Recall@Rank20 and MAP@Rank10 increase roughly 6% points each. On average, dual encoding shows improvement of at least 9% compared to single modality baselines. These improvements demonstrate our method's efficacy as an innovative solution towards robust video retrieval using text queries.",1
"Statistical learning theory provides the foundation to applied machine learning, and its various successful applications in computer vision, natural language processing and other scientific domains. The theory, however, does not take into account the unique challenges of performing statistical learning in geospatial settings. For instance, it is well known that model errors cannot be assumed to be independent and identically distributed in geospatial (a.k.a. regionalized) variables due to spatial correlation; and trends caused by geophysical processes lead to covariate shifts between the domain where the model was trained and the domain where it will be applied, which in turn harm the use of classical learning methodologies that rely on random samples of the data. In this work, we introduce the geostatistical (transfer) learning problem, and illustrate the challenges of learning from geospatial data by assessing widely-used methods for estimating generalization error of learning models, under covariate shift and spatial correlation. Experiments with synthetic Gaussian process data as well as with real data from geophysical surveys in New Zealand indicate that none of the methods are adequate for model selection in a geospatial context. We provide general guidelines regarding the choice of these methods in practice while new methods are being actively researched.",0
"Abstract: In recent years, geostatistics has become increasingly important due to the exponential growth of location-based data generated by modern technologies such as GPS, satellite imagery, and remote sensing. This abundance of spatial data provides opportunities for advancing our understanding of natural phenomena and human behavior at different scales, from local to global. However, analyzing and interpreting these complex datasets pose significant challenges that require innovative solutions.  One promising approach towards addressing these difficulties is through machine learning techniques, particularly deep learning algorithms that can model nonlinear relationships among variables. These methods have been shown to improve predictions and generate insights into spatial processes, but their implementation faces several obstacles, including data quality issues, scale dependency, and computational limitations.  This review discusses the current state of geostatistical learning research, highlighting both the promises and pitfalls of using advanced statistical models in environmental science, urban planning, transportation management, epidemiology, and other domains where space matters. We argue that further development of effective geostatistical learning approaches requires integrating diverse disciplines to tackle interdisciplinary problems, strengthening collaboration across academic boundaries, and establishing open access to high-quality datasets to facilitate reproducibility and comparison. Our synthesis identifies future directions for methodological research and application perspectives, paving the way toward more informed decision making under uncertainty and limited information.  Keywords: Geostatistical analysis; Machine learning; Deep learning; Spatial statistics; Geographic information systems; Environmental monitoring; Remote sensing; Urban informatics; Epidemiology.",1
"We introduce QuerYD, a new large-scale dataset for retrieval and event localisation in video. A unique feature of our dataset is the availability of two audio tracks for each video: the original audio, and a high-quality spoken description of the visual content. The dataset is based on YouDescribe, a volunteer project that assists visually-impaired people by attaching voiced narrations to existing YouTube videos. This ever-growing collection of videos contains highly detailed, temporally aligned audio and text annotations. The content descriptions are more relevant than dialogue, and more detailed than previous description attempts, which can be observed to contain many superficial or uninformative descriptions. To demonstrate the utility of the QuerYD dataset, we show that it can be used to train and benchmark strong models for retrieval and event localisation. Data, code and models are made publicly available, and we hope that QuerYD inspires further research on video understanding with written and spoken natural language.",0
"Creating detailed datasets that contain both video data and corresponding textual annotations is essential for developing computer vision algorithms that can effectively process natural scenes. While several large-scale datasets have been released recently, there remains a shortage of high-quality resources featuring synchronized video and metadata, particularly those with strong attention paid to maintaining consistency between visual elements and their associated descriptions. To address this need, we introduce QuerYD, a novel video dataset containing over 20 hours of footage with meticulous text and audio narration tracks. Each clip features rich metadata describing camera movements, object identities, and scene events, enabling researchers to study these phenomena at scale while benefiting from precise correspondences across multiple modalities. In total, our resource contains more than 86,000 frames organized into 40 sequences capturing diverse real-world environments, including indoor spaces, outdoor scenes, and dynamic situations involving humans. Our work presents an important contribution to the community by providing a new benchmark to advance the state of art for multi-modal understanding tasks. We expect that QuerYD will serve as a valuable foundation for future endeavors exploring topics such as activity recognition, object detection, video description, and scene understanding.",1
"Recent works have demonstrated reasonable success of representation learning in hypercomplex space. Specifically, ""fully-connected layers with Quaternions"" (4D hypercomplex numbers), which replace real-valued matrix multiplications in fully-connected layers with Hamilton products of Quaternions, both enjoy parameter savings with only 1/4 learnable parameters and achieve comparable performance in various applications. However, one key caveat is that hypercomplex space only exists at very few predefined dimensions (4D, 8D, and 16D). This restricts the flexibility of models that leverage hypercomplex multiplications. To this end, we propose parameterizing hypercomplex multiplications, allowing models to learn multiplication rules from data regardless of whether such rules are predefined. As a result, our method not only subsumes the Hamilton product, but also learns to operate on any arbitrary nD hypercomplex space, providing more architectural flexibility using arbitrarily $1/n$ learnable parameters compared with the fully-connected layer counterpart. Experiments of applications to the LSTM and Transformer models on natural language inference, machine translation, text style transfer, and subject verb agreement demonstrate architectural flexibility and effectiveness of the proposed approach.",0
"This paper presents a novel approach to deep learning using hypercomplex multiplications with quaternions. We introduce a new parameterization method that allows for efficient training on large datasets while preserving important structural properties of the data. By utilizing a low-dimensional embedding of the quaternion group, we show how to achieve state-of-the-art performance across multiple benchmark tasks without sacrificing interpretability. Our work demonstrates the potential of quaternionic neural networks as a powerful tool for artificial intelligence applications, such as computer vision and natural language processing.",1
"The success of deep learning in the computer vision and natural language processing communities can be attributed to training of very deep neural networks with millions or billions of parameters which can then be trained with massive amounts of data. However, similar trend has largely eluded training of deep reinforcement learning (RL) algorithms where larger networks do not lead to performance improvement. Previous work has shown that this is mostly due to instability during training of deep RL agents when using larger networks. In this paper, we make an attempt to understand and address training of larger networks for deep RL. We first show that naively increasing network capacity does not improve performance. Then, we propose a novel method that consists of 1) wider networks with DenseNet connection, 2) decoupling representation learning from training of RL, 3) a distributed training method to mitigate overfitting problems. Using this three-fold technique, we show that we can train very large networks that result in significant performance gains. We present several ablation studies to demonstrate the efficacy of the proposed method and some intuitive understanding of the reasons for performance gain. We show that our proposed method outperforms other baseline algorithms on several challenging locomotion tasks.",0
"Abstract: Improving deep reinforcement learning algorithms for training larger neural networks remains a challenging problem due to their inherent complexity and computational requirements. In this work, we present new techniques that leverage recent advances in optimization methods and parallel computing to efficiently train large-scale deep reinforcement learning models. Our approach combines stochastic gradient descent (SGD) with second-order momentum factors and adaptive learning rate scheduling to achieve faster convergence rates without sacrificing accuracy. To further enhance scalability and efficiency, we implement our algorithm on GPUs using CUDA and distribute computation across multiple devices using synchronous stochastic gradient descent (SSGD). Experimental results demonstrate significant improvements over state-of-the-art approaches on benchmark domains, including Atari games, Mujoco locomotion tasks, and Humanoid robotics simulations. Our proposed method paves the way for more advanced deep reinforcement learning applications with larger neural architectures and increased realism in simulation environments.",1
"Image Captioning is an arduous task of producing syntactically and semantically correct textual descriptions of an image in natural language with context related to the image. Existing notable pieces of research in Bengali Image Captioning (BIC) are based on encoder-decoder architecture. This paper presents an end-to-end image captioning system utilizing a multimodal architecture by combining a one-dimensional convolutional neural network (CNN) to encode sequence information with a pre-trained ResNet-50 model image encoder for extracting region-based visual features. We investigate our approach's performance on the BanglaLekhaImageCaptions dataset using the existing evaluation metrics and perform a human evaluation for qualitative analysis. Experiments show that our approach's language encoder captures the fine-grained information in the caption, and combined with the image features, it generates accurate and diversified caption. Our work outperforms all the existing BIC works and achieves a new state-of-the-art (SOTA) performance by scoring 0.651 on BLUE-1, 0.572 on CIDEr, 0.297 on METEOR, 0.434 on ROUGE, and 0.357 on SPICE.",0
"In recent years, image caption generation has gained significant attention as a challenging task due to its potential applications in different domains such as assistive technology, education, entertainment, and more. However, most research focuses on English images and few attempts have been made towards other languages such as Bengali. This paper presents an improved methodology for generating descriptive and accurate captions for Bengali images using a deep Convolutional Neural Network (CNN) based Encoder-Decoder Model. Our approach employs pre-trained CNN models for feature extraction from input images, followed by Recurrent Neural Networks (RNNs) for sequence generation tasks. Additionally, we introduce novel techniques such as data augmentation, attention mechanism, and adversarial training to enhance the performance of our system. We evaluate our proposed framework against existing methods on two benchmark datasets and demonstrate that it outperforms them significantly in terms of both quantitative metrics and visual inspection. Finally, we provide detailed analysis of the results and discuss future directions for improving Bengali image captioning further.",1
"Structured matrices, such as those derived from Kronecker products (KP), are effective at compressing neural networks, but can lead to unacceptable accuracy loss when applied to large models. In this paper, we propose the notion of doping -- addition of an extremely sparse matrix to a structured matrix. Doping facilitates additional degrees of freedom for a small number of parameters, allowing them to independently diverge from the fixed structure. To train LSTMs with doped structured matrices, we introduce the additional parameter matrix while slowly annealing its sparsity level. However, we find that performance degrades as we slowly sparsify the doping matrix, due to co-matrix adaptation (CMA) between the structured and the sparse matrices. We address this over dependence on the sparse matrix using a co-matrix dropout regularization (CMR) scheme. We provide empirical evidence to show that doping, CMA and CMR are concepts generally applicable to multiple structured matrices (Kronecker Product, LMF, Hybrid Matrix Decomposition). Additionally, results with doped kronecker product matrices demonstrate state-of-the-art accuracy at large compression factors (10 - 25x) across 4 natural language processing applications with minor loss in accuracy. Doped KP compression technique outperforms previous state-of-the art compression results by achieving 1.3 - 2.4x higher compression factor at a similar accuracy, while also beating strong alternatives like pruning and low-rank methods by a large margin (8% or more). Additionally, we show that doped KP can be deployed on commodity hardware using the current software stack and achieve 2.5 - 5.5x inference run-time speed-up over baseline.",0
"Artificial neural networks have recently attracted a lot of attention due to their success in solving many challenges in natural language processing, computer vision, speech recognition, game playing, robotics, bioinformatics and other domains. In particular, Long Short Term Memory (LSTM) has been proven effective and widely adopted as a state-of-the-art model architecture. However, training large scale neural network models can often take days or even weeks on powerful GPU clusters which makes them impractical for many applications. Furthermore, inference time in these models can affect latency sensitive tasks such as realtime text generation and machine translation. Consequently, there is growing interest in designing more efficient models that trade off some amount of accuracy for speed without significantly sacrificing performance. In this work, we introduce Doping; a novel model pruning technique that uses sparse structured additive matrices to compress deep LSTM architectures by selectively removing connections between neurons during training while still preserving model expressiveness. We show through extensive experimentation that our method results in significant reduction in parameters while only marginally impacting test set perplexity compared to the full unpruned model across multiple NLP benchmarks datasets such as Penn Treebank, BooksCorpus, and Movie Reviews. Our method achieves over 97% parameter savings while maintaining near baseline performance in most cases. Additionally, we analyze the effect of different hyperparameter choices such as sparsity thresholds, regularization terms, and matrix multiplicand selection criteria towards optimal model size reduction while minimizing loss in accuracy. Overall, our proposed approach represents an effec",1
"Analog electronic and optical computing exhibit tremendous advantages over digital computing for accelerating deep learning when operations are executed at low precision. In this work, we derive a relationship between analog precision, which is limited by noise, and digital bit precision. We propose extending analog computing architectures to support varying levels of precision by repeating operations and averaging the result, decreasing the impact of noise. Such architectures enable programmable tradeoffs between precision and other desirable performance metrics such as energy efficiency or throughput. To utilize dynamic precision, we propose a method for learning the precision of each layer of a pre-trained model without retraining network weights. We evaluate this method on analog architectures subject to a variety of noise sources such as shot noise, thermal noise, and weight noise and find that employing dynamic precision reduces energy consumption by up to 89% for computer vision models such as Resnet50 and by 24% for natural language processing models such as BERT. In one example, we apply dynamic precision to a shot-noise limited homodyne optical neural network and simulate inference at an optical energy consumption of 2.7 aJ/MAC for Resnet50 and 1.6 aJ/MAC for BERT with 2% accuracy degradation.",0
"This paper presents a novel approach to precision analog computing for neural networks, which enables dynamic adjustment of precision based on network activity. Traditional neural networks use digital hardware that requires precise arithmetic operations at each layer of computation. However, these computations can consume significant power and cause delays due to limited bandwidth and serial processing. In contrast, our method leverages the parallelism and energy efficiency of analog circuits while maintaining high accuracy by dynamically controlling precision levels during inference. Our results show that, compared to state-of-the-art digital implementations, our proposed scheme achieves comparable accuracies with reduced latency and up to three orders of magnitude lower energy consumption. Additionally, we demonstrate how to integrate our method into commonly used deep learning frameworks without requiring modifications to existing models. Our work paves the way towards enabling more efficient and adaptive artificial intelligence systems that operate beyond the capabilities of current digital architectures.",1
"A pruning-based AutoML framework for run-time reconfigurability, namely RT3, is proposed in this work. This enables Transformer-based large Natural Language Processing (NLP) models to be efficiently executed on resource-constrained mobile devices and reconfigured (i.e., switching models for dynamic hardware conditions) at run-time. Such reconfigurability is the key to save energy for battery-powered mobile devices, which widely use dynamic voltage and frequency scaling (DVFS) technique for hardware reconfiguration to prolong battery life. In this work, we creatively explore a hybrid block-structured pruning (BP) and pattern pruning (PP) for Transformer-based models and first attempt to combine hardware and software reconfiguration to maximally save energy for battery-powered mobile devices. Specifically, RT3 integrates two-level optimizations: First, it utilizes an efficient BP as the first-step compression for resource-constrained mobile devices; then, RT3 heuristically generates a shrunken search space based on the first level optimization and searches multiple pattern sets with diverse sparsity for PP via reinforcement learning to support lightweight software reconfiguration, which corresponds to available frequency levels of DVFS (i.e., hardware reconfiguration). At run-time, RT3 can switch the lightweight pattern sets within 45ms to guarantee the required real-time constraint at different frequency levels. Results further show that RT3 can prolong battery life over 4x improvement with less than 1% accuracy loss for Transformer and 1.5% score decrease for DistilBERT.",0
"Here is a possible abstract:  With mobile devices becoming increasingly powerful, there is growing interest in enabling complex artificial intelligence (AI) models such as transformers on these devices. However, many popular deep learning frameworks are unable to take advantage of hardware acceleration due to their complexity, which limits performance gains. In addition, current approaches only consider offline execution and lack run-time reconfigurability, making them inflexible during deployment. To address these issues, we propose a novel framework called ""Dancing along Battery"" that enables efficient runtime configuration of transformer-based models using lightweight kernel primitives. We evaluate our approach through extensive experiments on mobile devices and demonstrate significant improvements over state-of-the-art methods, achieving up to 9x speedup while maintaining comparable accuracy. Our framework has potential applications across different domains including natural language processing and computer vision tasks, providing researchers and developers with greater flexibility in designing new AI systems optimized for battery life and computational constraints.",1
"The knowledge that data lies close to a particular submanifold of the ambient Euclidean space may be useful in a number of ways. For instance, one may want to automatically mark any point far away from the submanifold as an outlier, or to use its geodesic distance to measure similarity between points. Classical problems for manifold learning are often posed in a very high dimension, e.g. for spaces of images or spaces of representations of words. Today, with deep representation learning on the rise in areas such as computer vision and natural language processing, many problems of this kind may be transformed into problems of moderately high dimension, typically of the order of hundreds. Motivated by this, we propose a manifold learning technique suitable for moderately high dimension and large datasets. The manifold is learned from the training data in the form of an intersection of quadric hypersurfaces -- simple but expressive objects. At test time, this manifold can be used to introduce an outlier score for arbitrary new points and to improve a given similarity metric by incorporating learned geometric structure into it.",0
This paper presents a method for using quadric hypersurfaces to intersect manifolds in high dimensional spaces for the purpose of manifold learning. The approach leverages the properties of quadrics to construct low dimensional representations that preserve key features of the original data set. Experiments demonstrate that the resulting embedding captures important underlying structure while reducing noise. We discuss potential applications including computer vision and nonlinear dimension reduction. The work builds on prior research at the intersection of algebraic geometry and machine learning.,1
"Machine learning and deep learning have shown great promise in mobile sensing applications, including Human Activity Recognition. However, the performance of such models in real-world settings largely depends on the availability of large datasets that captures diverse behaviors. Recently, studies in computer vision and natural language processing have shown that leveraging massive amounts of unlabeled data enables performance on par with state-of-the-art supervised models.   In this work, we present SelfHAR, a semi-supervised model that effectively learns to leverage unlabeled mobile sensing datasets to complement small labeled datasets. Our approach combines teacher-student self-training, which distills the knowledge of unlabeled and labeled datasets while allowing for data augmentation, and multi-task self-supervision, which learns robust signal-level representations by predicting distorted versions of the input.   We evaluated SelfHAR on various HAR datasets and showed state-of-the-art performance over supervised and previous semi-supervised approaches, with up to 12% increase in F1 score using the same number of model parameters at inference. Furthermore, SelfHAR is data-efficient, reaching similar performance using up to 10 times less labeled data compared to supervised approaches. Our work not only achieves state-of-the-art performance in a diverse set of HAR datasets, but also sheds light on how pre-training tasks may affect downstream performance.",0
"Despite significant advances in human activity recognition (HAR) research, there remains a need for more accurate systems that can recognize complex activities without the use of costly sensors. In this work, we propose SelfHAR, a self-supervised learning framework that utilizes unlabeled data to improve HAR performance. Our approach leverages semi-supervised fine-tuning techniques to adapt pretrained models on smaller labeled datasets. We demonstrate the effectiveness of our method using two publicly available benchmarks and show that SelfHAR outperforms several state-of-the-art methods while requiring significantly less labeled data. Our results suggest that self-training with unlabeled data could play an important role in building robust and efficient HAR systems.",1
"The count-min sketch (CMS) is a randomized data structure that provides estimates of tokens' frequencies in a large data stream using a compressed representation of the data by random hashing. In this paper, we rely on a recent Bayesian nonparametric (BNP) view on the CMS to develop a novel learning-augmented CMS under power-law data streams. We assume that tokens in the stream are drawn from an unknown discrete distribution, which is endowed with a normalized inverse Gaussian process (NIGP) prior. Then, using distributional properties of the NIGP, we compute the posterior distribution of a token's frequency in the stream, given the hashed data, and in turn corresponding BNP estimates. Applications to synthetic and real data show that our approach achieves a remarkable performance in the estimation of low-frequency tokens. This is known to be a desirable feature in the context of natural language processing, where it is indeed common in the context of the power-law behaviour of the data.",0
"This article presents a Bayesian nonparametric method for modeling count-min sketches (CMS) on high dimensional, streaming data. CMS is a popular algorithm used for estimating frequency moments over a massive database while reducing space complexity by several orders of magnitude. Traditional approaches assume that the underlying distribution follows a fixed parametric family like Poisson or zipfian, which may result in poor accuracy due to misspecification errors. In contrast, our proposed framework utilizes a nonparametric prior distribution using Chinese restaurant processes to allow for flexibility in accommodating heterogeneous datasets without assuming a specific shape. We show through extensive simulations that our method significantly outperforms existing methods across different scenarios and can adapt well to both sparse and dense data distributions, including power law models. Our results have significant implications in various applications where space efficiency is crucial, such as online advertising and network monitoring systems.",1
"Veracity is an essential key in research and development of innovative products. Live Emotion analysis and verification nullify deceit made to complainers on live chat, corroborate messages of both ends in messaging apps and promote an honest conversation between users. The main concept behind this emotion artificial intelligent verifier is to license or decline message accountability by comparing variegated emotions of chat app users recognized through facial expressions and text prediction. In this paper, a proposed emotion intelligent live detector acts as an honest arbiter who distributes facial emotions into labels namely, Happiness, Sadness, Surprise, and Hate. Further, it separately predicts a label of messages through text classification. Finally, it compares both labels and declares the message as a fraud or a bonafide. For emotion detection, we deployed Convolutional Neural Network (CNN) using a miniXception model and for text prediction, we selected Support Vector Machine (SVM) natural language processing probability classifier due to receiving the best accuracy on training dataset after applying Support Vector Machine (SVM), Random Forest Classifier, Naive Bayes Classifier, and Logistic regression.",0
"This paper presents a novel approach to verifying user emotions through chat applications by utilizing machine learning algorithms and natural language processing techniques. Our proposed system, called ""Lie-Sensor,"" uses emotional intelligence to detect deception in online communication, providing a more accurate assessment of users' true feelings compared to traditional methods such as questionnaires or facial analysis software. In addition to improving interpersonal communication in online platforms, our model can also serve as an effective tool for mental health monitoring and preventing cyberbullying. We describe the methodology used in designing our algorithm and provide experimental results demonstrating its effectiveness in identifying deceptive behavior in real-life scenarios. Overall, we believe that Lie-Sensor has great potential for enhancing the quality and authenticity of human interactions in digital environments.",1
"Transformers have shown outstanding results for natural language understanding and, more recently, for image classification. We here extend this work and propose a transformer-based approach for image retrieval: we adopt vision transformers for generating image descriptors and train the resulting model with a metric learning objective, which combines a contrastive loss with a differential entropy regularizer. Our results show consistent and significant improvements of transformers over convolution-based approaches. In particular, our method outperforms the state of the art on several public benchmarks for category-level retrieval, namely Stanford Online Product, In-Shop and CUB-200. Furthermore, our experiments on ROxford and RParis also show that, in comparable settings, transformers are competitive for particular object retrieval, especially in the regime of short vector representations and low-resolution images.",0
"Recent advances in transformer architectures have led to significant improvements in natural language processing tasks such as language translation, text generation, and sentiment analysis. However, these models are often computationally expensive, which makes them difficult to apply to high resolution image data like photos. To address this issue, we propose training vision transformers (ViTs) specifically designed for large scale image retrieval applications that can effectively utilize both local and global information from images while maintaining efficient computational performance. Our ViT model achieves state-of-the-art results on several benchmark datasets including COCO (Microsoft Common Objects in Context), Instagram, Landmarks, Flickr8K, and VGG dataset, outperforming popular convolutional neural network based approaches such as ResNet and DenseNet. Furthermore, our approach provides efficient implementation details, making it feasible to deploy ViT systems at scale. Overall, this work demonstrates the effectiveness and efficiency of vision transformers for large-scale image retrieval applications.",1
"We address the problem of learning on sets of features, motivated by the need of performing pooling operations in long biological sequences of varying sizes, with long-range dependencies, and possibly few labeled data. To address this challenging task, we introduce a parametrized representation of fixed size, which embeds and then aggregates elements from a given input set according to the optimal transport plan between the set and a trainable reference. Our approach scales to large datasets and allows end-to-end training of the reference, while also providing a simple unsupervised learning mechanism with small computational cost. Our aggregation technique admits two useful interpretations: it may be seen as a mechanism related to attention layers in neural networks, or it may be seen as a scalable surrogate of a classical optimal transport-based kernel. We experimentally demonstrate the effectiveness of our approach on biological sequences, achieving state-of-the-art results for protein fold recognition and detection of chromatin profiles tasks, and, as a proof of concept, we show promising results for processing natural language sequences. We provide an open-source implementation of our embedding that can be used alone or as a module in larger learning models at https://github.com/claying/OTK.",0
"In recent years, transport embeddings have emerged as a powerful tool in computer vision tasks such as image classification, object detection, and semantic segmentation. However, most existing transport embedding methods are limited by their reliance on simple metrics like Euclidean distance, which can fail to capture complex relationships between images. This study proposes a novel approach called trainable optimal transport (OT) embedding that addresses these limitations. Our method introduces learnable variables into OT planning, allowing us to optimize more effectively over both space and time. We demonstrate our approach using a feature aggregation task where two images must be aligned based on a learned correspondence between their features. With minimal modification, we show that our formulation closely approximates attention mechanisms commonly used in deep learning models. Our experiments highlight the superiority of our approach compared to popular baselines across several benchmark datasets for both feature aggregation and attention-related tasks. Overall, we provide compelling evidence of the effectiveness and versatility of trainable OT embeddings in computer vision.",1
"We consider the problem of referring segmentation in images and videos with natural language. Given an input image (or video) and a referring expression, the goal is to segment the entity referred by the expression in the image or video. In this paper, we propose a cross-modal self-attention (CMSA) module to utilize fine details of individual words and the input image or video, which effectively captures the long-range dependencies between linguistic and visual features. Our model can adaptively focus on informative words in the referring expression and important regions in the visual input. We further propose a gated multi-level fusion (GMLF) module to selectively integrate self-attentive cross-modal features corresponding to different levels of visual features. This module controls the feature fusion of information flow of features at different levels with high-level and low-level semantic information related to different attentive words. Besides, we introduce cross-frame self-attention (CFSA) module to effectively integrate temporal information in consecutive frames which extends our method in the case of referring segmentation in videos. Experiments on benchmark datasets of four referring image datasets and two actor and action video segmentation datasets consistently demonstrate that our proposed approach outperforms existing state-of-the-art methods.",0
"In recent years, referring expression (REF) segmentation has become an important task in computer vision, with applications ranging from image and video retrieval to natural language processing. Existing approaches rely on visual features alone, without taking into account cross-modal relationships between textual descriptions and image content. To address this limitation, we propose a new approach that uses cross-modal self-attention networks (CSAMs). CSAMs enable us to effectively capture the relationship between REF segments and their corresponding textual descriptions. Our method outperforms state-of-the-art methods on two benchmark datasets, demonstrating the effectiveness of our approach. We believe that CSAMs have great potential for a wide range of problems involving multi-modal data.",1
"In the last decades, extreme classification has become an essential topic for deep learning. It has achieved great success in many areas, especially in computer vision and natural language processing (NLP). However, it is very challenging to train a deep model with millions of classes due to the memory and computation explosion in the last output layer. In this paper, we propose a large-scale training system to address these challenges. First, we build a hybrid parallel training framework to make the training process feasible. Second, we propose a novel softmax variation named KNN softmax, which reduces both the GPU memory consumption and computation costs and improves the throughput of training. Then, to eliminate the communication overhead, we propose a new overlapping pipeline and a gradient sparsification method. Furthermore, we design a fast continuous convergence strategy to reduce total training iterations by adaptively adjusting learning rate and updating model parameters. With the help of all the proposed methods, we gain 3.9$\times$ throughput of our training system and reduce almost 60\% of training iterations. The experimental results show that using an in-house 256 GPUs cluster, we could train a classifier of 100 million classes on Alibaba Retail Product Dataset in about five days while achieving a comparable accuracy with the naive softmax training process.",0
"Abstract: We present a large-scale training system capable of handling extremely high dimensionality datasets with up to billions of features and millions of examples while achieving state-of-the art accuracy and speed. Our method leverages distributed computing across hundreds of GPUs and innovative use of tensor processing units (TPUs). To handle such high dimensional datasets, we first apply principle component analysis (PCA) to compress features into low dimensions while minimizing loss. We then apply stochastic gradient descent (SGD), momentum SGD, and Adam optimizers to train highly accurate models rapidly on our system which can scale linearly as more data becomes available. Compared against other methods, our approach achieves higher accuracy in less time than comparable systems on common benchmark datasets. This allows us to perform classification tasks that previously would have been impractical on current hardware. With these advances in place, we successfully demonstrate our framework on large-scale image recognition using ALIBLABA’s massive dataset consisting of one hundred million images from three popular Chinese online shopping sites, TMall, Taobao, Jingdongmall etc.. Overall our method can reduce risk of overfitting by tuning hyperparameters on validation set during training, and achieve good scalability with respect to growing volume of images. Furthermore, we hope that our open source implementation of our algorithm can serve as a model for others looking to solve problems similar to those faced by AliBabba.com, allowing them to rapidly iterate new ideas at scale and deliver value to customers faster than ever before. Note: I removed some details about how many gpus they used but you should keep it because it was important",1
"Deep learning-based models have been very successful in achieving state-of-the-art results in many of the computer vision, speech recognition, and natural language processing tasks in the last few years. These models seem a natural fit for handling the ever-increasing scale of biometric recognition problems, from cellphone authentication to airport security systems. Deep learning-based models have increasingly been leveraged to improve the accuracy of different biometric recognition systems in recent years. In this work, we provide a comprehensive survey of more than 120 promising works on biometric recognition (including face, fingerprint, iris, palmprint, ear, voice, signature, and gait recognition), which deploy deep learning models, and show their strengths and potentials in different applications. For each biometric, we first introduce the available datasets that are widely used in the literature and their characteristics. We will then talk about several promising deep learning works developed for that biometric, and show their performance on popular public benchmarks. We will also discuss some of the main challenges while using these models for biometric recognition, and possible future directions to which research in this area is headed.",0
"Modern security systems rely heavily on biometric recognition methods to ensure that only authorized users can access certain facilities. However, traditional methods such as fingerprinting have several drawbacks including high error rates during identification, storage capacity limitations, and difficulty capturing minutiae details from certain materials. To overcome these problems, researchers began exploring deep learning techniques to improve the accuracy and reliability of biometric recognition systems. This survey provides an overview of current trends in biometrics using deep learning. The first part discusses different types of biometric data, including facial features, hand geometry, voice patterns, iris scanning, gait analysis, and others. The second part describes popular deep learning architectures used in bio-metric applications such as convolutional neural networks (CNN), recurrent neural networks (RNN), graph neural networks (GNN) and so forth. We focus on how deep learning has impacted three important aspects of biometric recognition systems: feature extraction, classification, and quality control. We explain each aspect’s importance in detail and provide examples of recent studies conducted by prominent research groups in academia or industry. Finally, we conclude our study by analyzing the benefits and challenges brought by incorporating artificial intelligence into biometric verification processes and suggest potential future directions in advancing this growing field. By reviewing representative literature from international journals, conference proceedings, books, technical reports and online repositories, this comprehensive report highlights emerging issues regarding the adoption of advanced technology in biometric authentication scenarios. Our work fills an existing gap in understanding how state-of-the-art machine learning models have transformed biometrics in terms o",1
"The count-min sketch (CMS) is a time and memory efficient randomized data structure that provides estimates of tokens' frequencies in a data stream, i.e. point queries, based on random hashed data. Learning-augmented CMSs improve the CMS by learning models that allow to better exploit data properties. In this paper, we focus on the learning-augmented CMS of Cai, Mitzenmacher and Adams (\textit{NeurIPS} 2018), which relies on Bayesian nonparametric (BNP) modeling of a data stream via Dirichlet process (DP) priors. This is referred to as the CMS-DP, and it leads to BNP estimates of a point query as posterior means of the point query given the hashed data. While BNPs is proved to be a powerful tool for developing robust learning-augmented CMSs, ideas and methods behind the CMS-DP are tailored to point queries under DP priors, and they can not be used for other priors or more general queries. In this paper, we present an alternative, and more flexible, derivation of the CMS-DP such that: i) it allows to make use of the Pitman-Yor process (PYP) prior, which is arguably the most popular generalization of the DP prior; ii) it can be readily applied to the more general problem of estimating range queries. This leads to develop a novel learning-augmented CMS under power-law data streams, referred to as the CMS-PYP, which relies on BNP modeling of the stream via PYP priors. Applications to synthetic and real data show that the CMS-PYP outperforms the CMS and the CMS-DP in the estimation of low-frequency tokens; this known to be a critical feature in natural language processing, where it is indeed common to encounter power-law data streams.",0
"""Learning-Augmented Count-Min Sketches Via Bayesian Nonparametrics"" presents a new approach to improving count-min sketches by incorporating machine learning techniques into their design. Count-min sketches are data structures that can estimate set membership probabilities quickly and accurately, making them valuable tools in many applications such as streaming algorithms and data analytics. However, they have limitations due to the fixed sizes of their hash tables, which may result in overestimation or underestimation of certain probabilities. To address these issues, we propose using Bayesian nonparametric methods to adaptively learn the optimal table sizes based on the incoming data stream. Our method uses a hierarchical prior distribution over the learned sizes, allowing us to capture uncertainty in our estimates while providing robustness against outliers. Experimental results demonstrate that our proposed solution significantly reduces error rates compared to traditional count-min sketches, especially when dealing with imbalanced datasets or varying data distributions. Overall, this work highlights the potential benefits of combining classical computer science techniques with modern machine learning approaches.",1
"Unsupervised representation learning techniques, such as learning word embeddings, have had a significant impact on the field of natural language processing. Similar representation learning techniques have not yet become commonplace in the context of 3D vision. This, despite the fact that the physical 3D spaces have a similar semantic structure to bodies of text: words are surrounded by words that are semantically related, just like objects are surrounded by other objects that are similar in concept and usage.   In this work, we exploit this structure in learning semantically meaningful low dimensional vector representations of objects. We learn these vector representations by mining a dataset of scanned 3D spaces using an unsupervised algorithm. We represent objects as point clouds, a flexible and general representation for 3D data, which we encode into a vector representation. We show that using our method to include context increases the ability of a clustering algorithm to distinguish different semantic classes from each other. Furthermore, we show that our algorithm produces continuous and meaningful object embeddings through interpolation experiments.",0
"Here is a possible abstract:  Unsupervised learning has emerged as a powerful tool for extracting features from raw data, enabling computers to learn complex representations without human supervision. In particular, deep learning methods such as autoencoders have shown great promise in feature learning tasks, achieving state-of-the-art results on many challenging problems. However, these successes primarily concern image and video datasets, while less attention has been paid to point clouds, which represent another important type of high-dimensional input data. This gap motivates our work on unsupervised object-level feature learning for point cloud inputs. Our method, called Points2Vec, applies classical techniques from computer vision and machine learning in order to obtain robust, interpretable features that capture spatial structure and correspondences across multiple objects. We evaluate our approach using both quantitative metrics and qualitative visualizations, demonstrating its effectiveness in producing meaningful representations that can serve as input to downstream machine learning models. Overall, Points2Vec represents a promising contribution towards advancing the science of unsupervised representation learning for real-world applications involving point cloud data.  Please note that I am only able to provide you one possible example of a good abstract if there is any problem please tell me I will fix it right away",1
"Graph neural networks are emerging as continuation of deep learning success w.r.t. graph data. Tens of different graph neural network variants have been proposed, most following a neighborhood aggregation scheme, where the node features are updated via aggregating features of its neighboring nodes from layer to layer. Though related research surges, the power of GNNs are still not on-par-with their counterpart CNNs in computer vision and RNNs in natural language processing. We rethink this problem from the perspective of information propagation, and propose to enhance information propagation among GNN layers by combining heterogeneous aggregations. We argue that as richer information are propagated from shallow to deep layers, the discriminative capability of features formulated by GNN can benefit from it. As our first attempt in this direction, a new generic GNN layer formulation and upon this a new GNN variant referred as HAG-Net is proposed. We empirically validate the effectiveness of HAG-Net on a number of graph classification benchmarks, and elaborate all the design options and criterions along with.",0
"In recent years, graph neural networks (GNNs) have emerged as powerful tools for modeling complex relationships and patterns within large datasets. However, one limitation of GNNs is their reliance on homogeneous aggregation methods, which can lead to suboptimal performance in certain scenarios. To address this issue, we propose a novel approach that utilizes heterogeneous aggregation techniques to enhance information propagation in GNNs. Our method enables GNNs to learn more diverse representations of data nodes and capture unique characteristics of different edge types, resulting in improved accuracy and robustness. We evaluate our technique using several benchmark datasets and demonstrate its effectiveness through extensive experiments. Our results show significant improvements over existing state-of-the-art approaches, validating the potential of our proposed method for enhancing GNN performance. Overall, our work contributes new insights into the design and operation of GNN architectures, paving the way for future research in this exciting field.",1
"Self-supervised learning has gained popularity because of its ability to avoid the cost of annotating large-scale datasets. It is capable of adopting self-defined pseudo labels as supervision and use the learned representations for several downstream tasks. Specifically, contrastive learning has recently become a dominant component in self-supervised learning methods for computer vision, natural language processing (NLP), and other domains. It aims at embedding augmented versions of the same sample close to each other while trying to push away embeddings from different samples. This paper provides an extensive review of self-supervised methods that follow the contrastive approach. The work explains commonly used pretext tasks in a contrastive learning setup, followed by different architectures that have been proposed so far. Next, we have a performance comparison of different methods for multiple downstream tasks such as image classification, object detection, and action recognition. Finally, we conclude with the limitations of the current methods and the need for further techniques and future directions to make substantial progress.",0
"This survey presents a comprehensive overview of contrastive self-supervised learning (SSL), a rapidly growing subfield within machine learning that has emerged as one of the most promising techniques for unlocking the full potential of deep neural networks. SSL involves training artificial intelligence models by presenting them with two different representations of the same data point and asking them to predict whether the two samples belong to the same class or not. By taking advantage of large amounts of unlabeled data and exploiting the intrinsic structure of the data distribution, SSL can improve model performance across a variety of tasks while requiring significantly less labeled data compared to traditional supervised learning approaches. In this survey, we first introduce the fundamental concepts behind SSL and provide an analysis of its strengths and weaknesses. We then explore several representative SSL methods and architectures and discuss their applications in computer vision, natural language processing, speech recognition, robotics, reinforcement learning, and other areas. Additionally, we examine several open challenges faced by researchers working in SSL, including stability issues, negative sample quality, and scalability bottlenecks, among others. Finally, we conclude with future directions for research in SSL and highlight some of the exciting opportunities for applying SSL beyond deep learning. Overall, our goal is to provide readers with a solid foundation for understanding and implementing SSL in practice and inspire further innovation in this rapidly evolving field.",1
"The parameters of a neural network are naturally organized in groups, some of which might not contribute to its overall performance. To prune out unimportant groups of parameters, we can include some non-differentiable penalty to the objective function, and minimize it using proximal gradient methods. In this paper, we derive the weighted proximal operator, which is a necessary component of these proximal methods, of two structured sparsity inducing penalties. Moreover, they can be approximated efficiently with a numerical solver, and despite this approximation, we prove that existing convergence guarantees are preserved when these operators are integrated as part of a generic adaptive proximal method. Finally, we show that this adaptive method, together with the weighted proximal operators derived here, is indeed capable of finding solutions with structure in their sparsity patterns, on representative examples from computer vision and natural language processing.",0
"One approach to improving performance in deep learning models is through structured sparsity inducing adaptive optimizers. These techniques aim to reduce model complexity by selectively pruning neurons that have little effect on the output, while retaining those that contribute most significantly. This can lead to better generalization ability and faster training times. In recent years, several such methods have been proposed, including dropout regularization, Lasso regression, and neural network pruning. Despite their successes, these approaches suffer from limitations such as high computational cost, lack of scalability, and limited applicability across different architectures and datasets. To address these issues, we propose a new method called SSAO (Structured Sparsity Inducing Adaptive Optimizer), which utilizes a biologically inspired mechanism to achieve structured sparsity without sacrificing accuracy. We evaluate our method on a range of benchmark tasks and demonstrate its superiority over existing state-of-the-art algorithms. Our results show that SSAO achieves significant reduction in model size with minimal loss in accuracy, making it a promising tool for deep learning researchers seeking improved efficiency in both time and resources.",1
"Predicting motion of surrounding agents is critical to real-world applications of tactical path planning for autonomous driving. Due to the complex temporal dependencies and social interactions of agents, on-line trajectory prediction is a challenging task. With the development of attention mechanism in recent years, transformer model has been applied in natural language sequence processing first and then image processing. In this paper, we present a Spatial-Channel Transformer Network for trajectory prediction with attention functions. Instead of RNN models, we employ transformer model to capture the spatial-temporal features of agents. A channel-wise module is inserted to measure the social interaction between agents. We find that the Spatial-Channel Transformer Network achieves promising results on real-world trajectory prediction datasets on the traffic scenes.",0
"This paper presents a new approach for trajectory prediction using the spatial transformer network (STN) architecture. The authors propose to apply the STN framework directly to traffic scenes by considering all detected objects as individual instances to predict their future locations. The proposed method utilizes both RGB image data and trajectories from multiple previous frames to enhance accuracy. Extensive experiments were conducted on real-world datasets showing that the STN outperforms traditional methods such as social force models and convolutional neural networks. The results demonstrate the effectiveness of applying STNs to trajectory prediction tasks on traffic scenarios. Keywords: Spatial Transformer Network, Trajectory Prediction, Traffic Scene, Convolutional Neural Networks",1
"Deep learning has shown its power in many applications, including object detection in images, natural-language understanding, and speech recognition. To make it more accessible to end users, many deep learning models are now embedded in mobile apps. Compared to offloading deep learning from smartphones to the cloud, performing machine learning on-device can help improve latency, connectivity, and power consumption. However, most deep learning models within Android apps can easily be obtained via mature reverse engineering, while the models' exposure may invite adversarial attacks. In this study, we propose a simple but effective approach to hacking deep learning models using adversarial attacks by identifying highly similar pre-trained models from TensorFlow Hub. All 10 real-world Android apps in the experiment are successfully attacked by our approach. Apart from the feasibility of the model attack, we also carry out an empirical study that investigates the characteristics of deep learning models used by hundreds of Android apps on Google Play. The results show that many of them are similar to each other and widely use fine-tuning techniques to pre-trained models on the Internet.",0
"This sounds interesting! Here's a sample abstract that I came up with based on your request: ---  In recent years, deep learning models have become increasingly popular for use in mobile applications such as image classification, object recognition, speech recognition, and natural language processing. However, these models can be vulnerable to adversarial attacks, which seek to manipulate input data in order to cause errors or mispredictions by the model. In this work, we evaluate the robustness of on-device deep learning models running on Android devices against such attacks. We consider two types of adversarial attacks - targeted attacks, where the goal is to make the model output a specific incorrect label; and untargeted attacks, where the objective is simply to cause any incorrect label. Our results show that some models are more resistant to adversarial attacks than others, but overall there remains room for improvement in terms of model robustness. We provide recommendations for developers wishing to implement more robust models in their apps, including best practices for training, testing, and deployment. Overall, our findings highlight the importance of considering adversarial robustness alongside accuracy and efficiency when selecting deep learning models for deployment on consumer devices.",1
"In this paper we investigate the problem of automatically naming pieces of assembly code. Where by naming we mean assigning to an assembly function a string of words that would likely be assigned by a human reverse engineer. We formally and precisely define the framework in which our investigation takes place. That is we define the problem, we provide reasonable justifications for the choices that we made for the design of training and the tests. We performed an analysis on a large real-world corpora constituted by nearly 9 millions of functions taken from more than 22k softwares. In such framework we test baselines coming from the field of Natural Language Processing (e.g., Seq2Seq networks and Transformer). Interestingly, our evaluation shows promising results beating the state-of-the-art and reaching good performance. We investigate the applicability of tine-tuning (i.e., taking a model already trained on a large generic corpora and retraining it for a specific task). Such technique is popular and well-known in the NLP field. Our results confirm that fine-tuning is effective even when neural networks are applied to binaries. We show that a model, pre-trained on the aforementioned corpora, when fine-tuned has higher performances on specific domains (such as predicting names in system utilites, malware, etc).",0
"Title: Naming functions using neural networks  Abstract: One problem facing binary analysis today is that functions often have no name, making them hard to study further. To address this issue, we propose using machine learning techniques on symbolic representations of code fragments extracted from binaries to assign names to functions. We first describe our method for generating symbolic forms from assembly code and show how these forms can be used as inputs to neural network models. Then, we present two novel ways of leveraging semantic features related to control flow graphs derived from the same symbolic representation to improve function naming accuracy. Experimental results demonstrate that our approach achieves state-of-the-art performance at automatically assigning accurate function names to stripped binaries, outperforming several competitive baselines across five different benchmark datasets.",1
"Summarization of long sequences into a concise statement is a core problem in natural language processing, requiring non-trivial understanding of the input. Based on the promising results of graph neural networks on highly structured data, we develop a framework to extend existing sequence encoders with a graph component that can reason about long-distance relationships in weakly structured data such as text. In an extensive evaluation, we show that the resulting hybrid sequence-graph models outperform both pure sequence models as well as pure graph models on a range of summarization tasks.",0
"Automatically summarizing documents has been a challenging problem for natural language processing (NLP) researchers, as manually creating high-quality summaries can be time-consuming and difficult. In recent years, deep learning models have shown promise in tackling this task, but there remains a need for further improvement in terms of both performance and interpretability. In this paper, we present a novel approach called structured neural summarization that combines the strengths of both rule-based systems and deep learning methods to produce more accurate and coherent summaries. Our method incorporates structured representations, such as constituency parse trees, into the training process to ensure that the resulting summary retains important grammatical structures. We evaluate our approach on several benchmark datasets and demonstrate significant improvements over existing state-of-the-art methods. Additionally, we provide analysis showing how the model makes use of structured features during inference. Overall, our work shows that incorporating explicit structure into neural networks can lead to substantial advances in text summarization tasks.",1
"We describe a procedure for explaining neurons in deep representations by identifying compositional logical concepts that closely approximate neuron behavior. Compared to prior work that uses atomic labels as explanations, analyzing neurons compositionally allows us to more precisely and expressively characterize their behavior. We use this procedure to answer several questions on interpretability in models for vision and natural language processing. First, we examine the kinds of abstractions learned by neurons. In image classification, we find that many neurons learn highly abstract but semantically coherent visual concepts, while other polysemantic neurons detect multiple unrelated features; in natural language inference (NLI), neurons learn shallow lexical heuristics from dataset biases. Second, we see whether compositional explanations give us insight into model performance: vision neurons that detect human-interpretable concepts are positively correlated with task performance, while NLI neurons that fire for shallow heuristics are negatively correlated with task performance. Finally, we show how compositional explanations provide an accessible way for end users to produce simple ""copy-paste"" adversarial examples that change model behavior in predictable ways.",0
"Abstract: The ability of neurons to process complex patterns lies at the heart of our understanding of how the brain functions. However, explaining precisely how these cells achieve such feats has proven challenging. In recent years, researchers have turned to compositional explanations as a potential solution to address this gap in knowledge. This approach involves breaking down complex systems into simpler parts and examining how those components interact to produce emergent properties. By applying this methodology to neuronal computation, we can gain insights into their function that would otherwise remain elusive.  In this review article, we explore the latest developments in using compositional explanations to analyze the operation of individual neurons. We begin by discussing the basic concepts behind this approach, including the principles of compositionality and emergence. Next, we examine several specific examples where compositional analyses have been employed to shed light on the workings of single neurons. These studies span multiple areas within neuroscience, ranging from sensory processing to decision making and beyond.  Finally, we consider some of the outstanding questions surrounding the application of compositional reasoning to neural phenomena. For instance, while compositional analyses have already yielded valuable insights, they cannot fully capture the complexity inherent to real nervous systems. Nevertheless, this approach holds great promise for improving our comprehension of how neurons operate, both individually and collectively.  By combining theoretical and empirical perspectives, we aim to provide readers with a broad perspective on the use of compositional explanations in studying neurons. Ultimately, we hope this synthesis will inspire new research directions and further deepen our understanding of neural computing mechanisms.",1
"Existing deep learning models applied to reaction prediction in organic chemistry can reach high levels of accuracy ( 90% for Natural Language Processing-based ones). With no chemical knowledge embedded than the information learnt from reaction data, the quality of the data sets plays a crucial role in the performance of the prediction models. While human curation is prohibitively expensive, the need for unaided approaches to remove chemically incorrect entries from existing data sets is essential to improve artificial intelligence models' performance in synthetic chemistry tasks. Here we propose a machine learning-based, unassisted approach to remove chemically wrong entries from chemical reaction collections. We applied this method to the collection of chemical reactions Pistachio and to an open data set, both extracted from USPTO (United States Patent Office) patents. Our results show an improved prediction quality for models trained on the cleaned and balanced data sets. For the retrosynthetic models, the round-trip accuracy metric grows by 13 percentage points and the value of the cumulative Jensen Shannon divergence decreases by 30% compared to its original record. The coverage remains high with 97%, and the value of the class-diversity is not affected by the cleaning. The proposed strategy is the first unassisted rule-free technique to address automatic noise reduction in chemical data sets.",0
"This paper presents an approach for unassisted noise reduction in chemical reaction data sets through the use of advanced statistical techniques and machine learning algorithms. The proposed methodology enables the automatic detection and removal of unwanted background signals from the raw spectroscopic data without any human intervention, resulting in improved signal quality and more accurate analysis of complex chemical reactions. By leveraging recent advances in computational chemistry and applied mathematics, our algorithm is able to outperform traditional methods currently employed by scientists and researchers. We demonstrate the effectiveness of our technique on several case studies, showing that it consistently leads to better results than state-of-the-art methods. Our work represents an important step forward towards fully automating the process of noise reduction in chemical reaction data, which has significant implications for improving scientific knowledge and technological innovation across multiple disciplines.",1
"In this paper, we present a novel approach for conformal prediction (CP), in which we aim to identify a set of promising prediction candidates -- in place of a single prediction. This set is guaranteed to contain a correct answer with high probability, and is well-suited for many open-ended classification tasks. In the standard CP paradigm, the predicted set can often be unusably large and also costly to obtain. This is particularly pervasive in settings where the correct answer is not unique, and the number of total possible answers is high. We first expand the CP correctness criterion to allow for additional, inferred ""admissible"" answers, which can substantially reduce the size of the predicted set while still providing valid performance guarantees. Second, we amortize costs by conformalizing prediction cascades, in which we aggressively prune implausible labels early on by using progressively stronger classifiers -- again, while still providing valid performance guarantees. We demonstrate the empirical effectiveness of our approach for multiple applications in natural language processing and computational chemistry for drug discovery.",0
"This is an article on conformal prediction which can be used to predict outcomes based on existing data sets. Explain the process of cascading inference and expanded admission in your own words. Include any relevant theory or application examples from real world scenarios in your explanation to make it more interesting and easier to follow along with. Abstract should be approximately 150-300 words. Abstract: Efficient conformal prediction is achieved through the use of cascaded inference and expanded admission. In cascading inference, multiple models are created at different resolutions, each using the output of the previous model as input. By doing so, the overall error rate of the system decreases, leading to improved performance. Expanded admission refers to the inclusion of additional training data points that lie outside the initial set of observations, allowing for better generalization and reduced uncertainty in predictions. Examples of these methods in action may be found in fields such as finance, where accurate forecasting of market trends is crucial for investors to make informed decisions. Overall, efficient conformal prediction has many applications across industries and holds great potential for driving innovation and progress.",1
"Deep Neural Networks (DNNs) learn representation from data with an impressive capability, and brought important breakthroughs for processing images, time-series, natural language, audio, video, and many others. In the remote sensing field, surveys and literature revisions specifically involving DNNs algorithms' applications have been conducted in an attempt to summarize the amount of information produced in its subfields. Recently, Unmanned Aerial Vehicles (UAV) based applications have dominated aerial sensing research. However, a literature revision that combines both ""deep learning"" and ""UAV remote sensing"" thematics has not yet been conducted. The motivation for our work was to present a comprehensive review of the fundamentals of Deep Learning (DL) applied in UAV-based imagery. We focused mainly on describing classification and regression techniques used in recent applications with UAV-acquired data. For that, a total of 232 papers published in international scientific journal databases was examined. We gathered the published material and evaluated their characteristics regarding application, sensor, and technique used. We relate how DL presents promising results and has the potential for processing tasks associated with UAV-based image data. Lastly, we project future perspectives, commentating on prominent DL paths to be explored in the UAV remote sensing field. Our revision consists of a friendly-approach to introduce, commentate, and summarize the state-of-the-art in UAV-based image applications with DNNs algorithms in diverse subfields of remote sensing, grouping it in the environmental, urban, and agricultural contexts.",0
"Title: ""Deep Learning Applications for Unmanned Aerial Vehicle (UAV) Remote Sensing""  Abstract:  Unmanned Aerial Vehicles (UAVs), commonly known as drones, have emerged as powerful tools in remote sensing applications due to their ability to capture high-resolution images from different angles and altitudes. With advancements in computer vision and machine learning algorithms, deep learning techniques can now provide accurate and efficient processing of these images, enabling new opportunities for analysis and decision making. This review focuses on recent developments in deep learning applications for UAV remote sensing, including object detection, image segmentation, classification, and change detection. We highlight successful case studies in various fields such as precision agriculture, environmental monitoring, urban planning, archaeology, and disaster management. Additionally, we discuss future research directions and challenges that need to be addressed to fully realize the potential of deep learning technologies for UAV-based remote sensing. Our findings demonstrate that deep learning methods offer valuable contributions to the field by increasing accuracy, reducing manual labor, and providing insights beyond traditional remote sensing approaches. Overall, this work serves as a comprehensive survey of the latest trends in deep learning applications for UAV remote sensing, aimed at scientists, practitioners, and policymakers alike.",1
"We propose a new framework, Translation between Augmented Natural Languages (TANL), to solve many structured prediction language tasks including joint entity and relation extraction, nested named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. Instead of tackling the problem by training task-specific discriminative classifiers, we frame it as a translation task between augmented natural languages, from which the task-relevant information can be easily extracted. Our approach can match or outperform task-specific models on all tasks, and in particular, achieves new state-of-the-art results on joint entity and relation extraction (CoNLL04, ADE, NYT, and ACE2005 datasets), relation classification (FewRel and TACRED), and semantic role labeling (CoNLL-2005 and CoNLL-2012). We accomplish this while using the same architecture and hyperparameters for all tasks and even when training a single model to solve all tasks at the same time (multi-task learning). Finally, we show that our framework can also significantly improve the performance in a low-resource regime, thanks to better use of label semantics.",0
"This paper presents a novel approach for structured prediction tasks using natural language processing (NLP) techniques that involves translating augmented natural languages into their corresponding structured representations. We propose that by augmenting natural languages with additional symbols and grammar constructs, we can capture more complex relationships between concepts, leading to improved predictions. Our method leverages recent advances in NLP such as pretraining large language models on massive amounts of text data, fine-tuning them for task-specific objectives, and integrating external knowledge sources like WordNet. Experimental results on several benchmark datasets demonstrate the effectiveness of our method over baseline approaches, showing consistent improvements across different domains and task types.",1
"Cycle-consistent training is widely used for jointly learning a forward and inverse mapping between two domains of interest without the cumbersome requirement of collecting matched pairs within each domain. In this regard, the implicit assumption is that there exists (at least approximately) a ground-truth bijection such that a given input from either domain can be accurately reconstructed from successive application of the respective mappings. But in many applications no such bijection can be expected to exist and large reconstruction errors can compromise the success of cycle-consistent training. As one important instance of this limitation, we consider practically-relevant situations where there exists a many-to-one or surjective mapping between domains. To address this regime, we develop a conditional variational autoencoder (CVAE) approach that can be viewed as converting surjective mappings to implicit bijections whereby reconstruction errors in both directions can be minimized, and as a natural byproduct, realistic output diversity can be obtained in the one-to-many direction. As theoretical motivation, we analyze a simplified scenario whereby minima of the proposed CVAE-based energy function align with the recovery of ground-truth surjective mappings. On the empirical side, we consider a synthetic image dataset with known ground-truth, as well as a real-world application involving natural language generation from knowledge graphs and vice versa, a prototypical surjective case. For the latter, our CVAE pipeline can capture such many-to-one mappings during cycle training while promoting textural diversity for graph-to-text tasks. Our code is available at github.com/QipengGuo/CycleGT   *A condensed version of this paper has been accepted to AISTATS 2021. This version contains additional content and updates.",0
"In this paper we present a system for training machine learning models that can generalize well on unseen tasks by using cycle consistency as a regularizer. This approach allows us to train models on large datasets where each task has many possible solutions but only one correct output label, such as language translation or semantic segmentation. We first introduce a new framework called DALL-E (short for ""deep autoencoder latent laundering""), which learns mappings from input data to hidden representations and back again in a cycle-consistency manner. By mapping inputs back to their original form, our model encourages them to capture task-invariant features that are robust to changes in the number and type of classes. Next, we demonstrate how to incorporate these cycle consistent maps into state-of-the-art machine learning algorithms like GPT-4 (Generative Pre-training Transformer) to improve performance over prior methods. Finally, we evaluate our method on several challenging benchmarks including image classification, natural language understanding, and reinforcement learning, showing significant improvements over baseline systems. These results highlight the potential power of our approach for enabling human-like intelligence across multiple domains.",1
"We propose a generalization of transformer neural network architecture for arbitrary graphs. The original transformer was designed for Natural Language Processing (NLP), which operates on fully connected graphs representing all connections between the words in a sequence. Such architecture does not leverage the graph connectivity inductive bias, and can perform poorly when the graph topology is important and has not been encoded into the node features. We introduce a graph transformer with four new properties compared to the standard model. First, the attention mechanism is a function of the neighborhood connectivity for each node in the graph. Second, the positional encoding is represented by the Laplacian eigenvectors, which naturally generalize the sinusoidal positional encodings often used in NLP. Third, the layer normalization is replaced by a batch normalization layer, which provides faster training and better generalization performance. Finally, the architecture is extended to edge feature representation, which can be critical to tasks s.a. chemistry (bond type) or link prediction (entity relationship in knowledge graphs). Numerical experiments on a graph benchmark demonstrate the performance of the proposed graph transformer architecture. This work closes the gap between the original transformer, which was designed for the limited case of line graphs, and graph neural networks, that can work with arbitrary graphs. As our architecture is simple and generic, we believe it can be used as a black box for future applications that wish to consider transformer and graphs.",0
"In recent years, deep learning has proven particularly effective on graph data such as social networks (LinkedIn), knowledge graphs (Wikipedia), biochemical compounds (proteins) and computer programs (code). This paper describes how transformer networks can be adapted to work with such graph data. We have developed a new system called ""GraphTransform"" that uses graph convolutional neural network layers to extend the power of transformers to these kinds of inputs. Our results show that our method significantly improves performance over baseline models across multiple datasets, including those used by major corporations such as Facebook and Google. Overall, we believe our work has broad implications for applications ranging from drug discovery to code generation.",1
"Many irregular domains such as social networks, financial transactions, neuron connections, and natural language constructs are represented using graph structures. In recent years, a variety of graph neural networks (GNNs) have been successfully applied for representation learning and prediction on such graphs. In many of the real-world applications, the underlying graph changes over time, however, most of the existing GNNs are inadequate for handling such dynamic graphs. In this paper we propose a novel technique for learning embeddings of dynamic graphs using a tensor algebra framework. Our method extends the popular graph convolutional network (GCN) for learning representations of dynamic graphs using the recently proposed tensor M-product technique. Theoretical results presented establish a connection between the proposed tensor approach and spectral convolution of tensors. The proposed method TM-GCN is consistent with the Message Passing Neural Network (MPNN) framework, accounting for both spatial and temporal message passing. Numerical experiments on real-world datasets demonstrate the performance of the proposed method for edge classification and link prediction tasks on dynamic graphs. We also consider an application related to the COVID-19 pandemic, and show how our method can be used for early detection of infected individuals from contact tracing data.",0
"Dynamic graph convolutional networks have gained increasing interest due to their ability to effectively process data on irregular graphs. Traditional convolutional neural network (CNN) architectures are limited by their reliance on regular grid structures and cannot easily adapt to the complexities found in many real-world applications such as traffic management systems, social network analysis, biological pathway modeling, and financial market analysis. This work presents a novel methodology that leverages the tensor m-product operation to enable dynamic processing of irregular graphs. By incorporating a dynamic parameterization scheme, our approach can learn a different set of kernels at each layer of the graph convolution network, providing increased flexibility over traditional static kernel methods. Experimental results demonstrate improved performance compared to baseline methods across a range of application domains. Our framework paves the way towards more effective handling of irregularly structured big data through scalable machine learning algorithms that leverage the unique properties of modern computing paradigms such as GPU acceleration and distributed computing.",1
"Visual question answering requires a deep understanding of both images and natural language. However, most methods mainly focus on visual concept; such as the relationships between various objects. The limited use of object categories combined with their relationships or simple question embedding is insufficient for representing complex scenes and explaining decisions. To address this limitation, we propose the use of text expressions generated for images, because such expressions have few structural constraints and can provide richer descriptions of images. The generated expressions can be incorporated with visual features and question embedding to obtain the question-relevant answer. A joint-embedding multi-head attention network is also proposed to model three different information modalities with co-attention. We quantitatively and qualitatively evaluated the proposed method on the VQA v2 dataset and compared it with state-of-the-art methods in terms of answer prediction. The quality of the generated expressions was also evaluated on the RefCOCO, RefCOCO+, and RefCOCOg datasets. Experimental results demonstrate the effectiveness of the proposed method and reveal that it outperformed all of the competing methods in terms of both quantitative and qualitative results.",0
"Include as many of the following words as possible: visual question answering (VQA), referring expression generation (REG), language model, scene understanding, localization. Use at least five keywords in total. Output should be less than 450 words.",1
"A common vision from science fiction is that robots will one day inhabit our physical spaces, sense the world as we do, assist our physical labours, and communicate with us through natural language. Here we study how to design artificial agents that can interact naturally with humans using the simplification of a virtual environment. This setting nevertheless integrates a number of the central challenges of artificial intelligence (AI) research: complex visual perception and goal-directed physical control, grounded language comprehension and production, and multi-agent social interaction. To build agents that can robustly interact with humans, we would ideally train them while they interact with humans. However, this is presently impractical. Therefore, we approximate the role of the human with another learned agent, and use ideas from inverse reinforcement learning to reduce the disparities between human-human and agent-agent interactive behaviour. Rigorously evaluating our agents poses a great challenge, so we develop a variety of behavioural tests, including evaluation by humans who watch videos of agents or interact directly with them. These evaluations convincingly demonstrate that interactive training and auxiliary losses improve agent behaviour beyond what is achieved by supervised learning of actions alone. Further, we demonstrate that agent capabilities generalise beyond literal experiences in the dataset. Finally, we train evaluation models whose ratings of agents agree well with human judgement, thus permitting the evaluation of new agent models without additional effort. Taken together, our results in this virtual environment provide evidence that large-scale human behavioural imitation is a promising tool to create intelligent, interactive agents, and the challenge of reliably evaluating such agents is possible to surmount.",0
"Humans have always been fascinated by machines that can think like humans. From the earliest Greek myths to modern science fiction stories, we have imagined robots as our equals - intelligent beings with their own thoughts, feelings, and motivations. In recent years, advances in artificial intelligence (AI) have made these fantasies seem more plausible than ever before. Scientists and engineers have created algorithms and computer programs that can solve problems, recognize objects, and even engage in conversations with human-like wit and humor.  Despite these impressive accomplishments, however, most AI systems remain far behind the cognitive abilities of even the average human brain. We still lack a deep understanding of how the human mind works, and so building machines that truly mimic human thinking remains an elusive goal. But that hasn't stopped researchers from trying. And while today's efforts at imitating interactive intelligence may seem crude compared to future achievements, they already provide valuable insights into both human nature and machine learning. This paper explores some of the latest developments in AI technology and how they might shape our perceptions of consciousness, creativity, and other qualities traditionally associated with living organisms. By examining examples such as chatbots, game agents, and language models, we find that although machines cannot yet replicate all aspects of human thought, they nevertheless offer intriguing glimpses into the inner workings of both brains and computers. Ultimately, whether viewed as friends or foes, imitations of interactive intelligence challenge us to rethink the boundaries between humanity and machinery.",1
"On-device Deep Neural Networks (DNNs) have recently gained more attention due to the increasing computing power of the mobile devices and the number of applications in Computer Vision (CV), Natural Language Processing (NLP), and Internet of Things (IoTs). Unfortunately, the existing efficient convolutional neural network (CNN) architectures designed for CV tasks are not directly applicable to NLP tasks and the tiny Recurrent Neural Network (RNN) architectures have been designed primarily for IoT applications. In NLP applications, although model compression has seen initial success in on-device text classification, there are at least three major challenges yet to be addressed: adversarial robustness, explainability, and personalization. Here we attempt to tackle these challenges by designing a new training scheme for model compression and adversarial robustness, including the optimization of an explainable feature mapping objective, a knowledge distillation objective, and an adversarially robustness objective. The resulting compressed model is personalized using on-device private training data via fine-tuning. We perform extensive experiments to compare our approach with both compact RNN (e.g., FastGRNN) and compressed RNN (e.g., PRADO) architectures in both natural and adversarial NLP test settings.",0
"This paper presents a methodology for adversarially robust model compression that allows on-device personalization for text classification tasks. We propose an approach based on pruning techniques that preserves accuracy while reducing computational overheads. Additionally, we introduce methods to improve explainability by generating explanations during deployment directly from compressed models. Our experiments demonstrate consistent performance across different datasets without significant loss in accuracy due to pruning. Furthermore, our system provides more interpretable results compared to baseline approaches, especially under adverse conditions such as noisy inputs and domain shifts. In conclusion, we show that our method can provide high prediction quality in addition to efficient inference latency while maintaining interpretability at both training and runtime phases, making it suitable for mobile platforms where privacy concerns require personalized model execution at edge devices.",1
"In this paper we propose a new framework - MoViLan (Modular Vision and Language) for execution of visually grounded natural language instructions for day to day indoor household tasks. While several data-driven, end-to-end learning frameworks have been proposed for targeted navigation tasks based on the vision and language modalities, performance on recent benchmark data sets revealed the gap in developing comprehensive techniques for long horizon, compositional tasks (involving manipulation and navigation) with diverse object categories, realistic instructions and visual scenarios with non-reversible state changes. We propose a modular approach to deal with the combined navigation and object interaction problem without the need for strictly aligned vision and language training data (e.g., in the form of expert demonstrated trajectories). Such an approach is a significant departure from the traditional end-to-end techniques in this space and allows for a more tractable training process with separate vision and language data sets. Specifically, we propose a novel geometry-aware mapping technique for cluttered indoor environments, and a language understanding model generalized for household instruction following. We demonstrate a significant increase in success rates for long-horizon, compositional tasks over the baseline on the recently released benchmark data set-ALFRED.",0
"This paper presents a new framework for visual navigation and manipulation tasks that can handle long time horizons and complex environments. Our approach uses a combination of modular planning techniques and high-level abstraction to allow agents to operate effectively in uncertain and dynamic settings. We evaluate our method on a range of challenging benchmarks, demonstrating robust performance across a variety of environments and task types. Our work advances the state of the art in visual navigation, enabling agents to plan and execute more effective policies in realistic scenarios.",1
"Discrete latent spaces in variational autoencoders have been shown to effectively capture the data distribution for many real-world problems such as natural language understanding, human intent prediction, and visual scene representation. However, discrete latent spaces need to be sufficiently large to capture the complexities of real-world data, rendering downstream tasks computationally challenging. For instance, performing motion planning in a high-dimensional latent representation of the environment could be intractable. We consider the problem of sparsifying the discrete latent space of a trained conditional variational autoencoder, while preserving its learned multimodality. As a post hoc latent space reduction technique, we use evidential theory to identify the latent classes that receive direct evidence from a particular input condition and filter out those that do not. Experiments on diverse tasks, such as image generation and human behavior prediction, demonstrate the effectiveness of our proposed technique at reducing the discrete latent sample space size of a model while maintaining its learned multimodality.",0
This sounds like an interesting topic! Can you provide more context or clarify your question? Also please note that writing a good abstract takes skill and practice so I may need further guidance on how to structure my response.,1
"Policy specification is a process by which a human can initialize a robot's behaviour and, in turn, warm-start policy optimization via Reinforcement Learning (RL). While policy specification/design is inherently a collaborative process, modern methods based on Learning from Demonstration or Deep RL lack the model interpretability and accessibility to be classified as such. Current state-of-the-art methods for policy specification rely on black-box models, which are an insufficient means of collaboration for non-expert users: These models provide no means of inspecting policies learnt by the agent and are not focused on creating a usable modality for teaching robot behaviour. In this paper, we propose a novel machine learning framework that enables humans to 1) specify, through natural language, interpretable policies in the form of easy-to-understand decision trees, 2) leverage these policies to warm-start reinforcement learning and 3) outperform baselines that lack our natural language initialization mechanism. We train our approach by collecting a first-of-its-kind corpus mapping free-form natural language policy descriptions to decision tree-based policies. We show that our novel framework translates natural language to decision trees with a 96% and 97% accuracy on a held-out corpus across two domains, respectively. Finally, we validate that policies initialized with natural language commands are able to significantly outperform relevant baselines (p  0.001) that do not benefit from our natural language-based warm-start technique.",0
"As of late, there has been increasing interest in creating intelligent agents that can act autonomously in complex, uncertain environments while remaining transparent, interpretable, and explainable. To address these concerns, we propose a novel framework that utilizes natural language processing techniques to effectively translate human instructions into policy specifications, allowing end users to have control over their agents without requiring extensive knowledge of Reinforcement Learning (RL) algorithms or Markov Decision Processes (MDPs). Our approach enables efficient synthesis of both model-free and model-based policies by leveraging deep neural networks that capture essential features from raw sensory input data and contextualizing them with linguistic expressions provided by humans. By facilitating seamless interaction between humans and machines via natural language, our method offers substantial benefits for applications where interpretability, transparency, and safety are crucial, such as healthcare, finance, transportation, education, entertainment, and other domains involving decision making under uncertainty. --- Author Keywords: Natural Language Processing; Human-Machine Interaction; Model-Free and Model-Based Policies; Transparent and Explainable Intelligent Agents; Policy Search and Optimization; Refinement Techniques. Ensure your text adheres to our journal guidelines at https://arxiv.org/abs/2019.6986v4. If submission fails due to length restrictions, please remove any supplementary content like figures, tables, equations etc., then resubmit. Your uploaded files should include a PDF file containing the entire manuscript including tables, figures, etc. Any additional non-PDF source files will be ignored during submission but can optionally be added later if required",1
"Self-attention architectures, which are rapidly pushing the frontier in natural language processing, demonstrate a surprising depth-inefficient behavior: previous works indicate that increasing the internal representation (network width) is just as useful as increasing the number of self-attention layers (network depth). We theoretically predict a width-dependent transition between depth-efficiency and depth-inefficiency in self-attention. We conduct systematic empirical ablations on networks of depths 6 to 48 that clearly reveal the theoretically predicted behaviors, and provide explicit quantitative suggestions regarding the optimal depth-to-width allocation for a given self-attention network size. The race towards beyond 1-Trillion parameter language models renders informed guidelines for increasing self-attention depth and width in tandem an essential ingredient. Our guidelines elucidate the depth-to-width trade-off in self-attention networks of sizes up to the scale of GPT3 (which we project to be too deep for its size), and beyond, marking an unprecedented width of 30K as optimal for a 1-Trillion parameter network.",0
"In natural language processing (NLP), self-attention has emerged as a powerful mechanism for modeling interdependencies between sequences. However, existing methods often struggle with computation complexity due to their quadratic space/time growth w.r.t. input length. This study proposes a novel family of efficient attention mechanisms called ""Depthwise Separable Attention"" (DSA) that adaptively separates depth (channel dimension) from width (sequence length). Unlike prior works that perform pointwise convolutions across all dimensions simultaneously, DSA processes each position along a sequence independently by attending only over a small set of informative positions sampled uniformly at random from the entire sequence. By decoupling the complexities of query processing from both input length and channel size, we achieve unprecedented computational efficiency while retaining competitive performance on various NLP tasks. Extensive empirical studies confirm our method can scale up existing transformer architectures by orders of magnitude without loss of accuracy, enabling new possibilities for real-world applications such as low-latency translation. Our findings have important implications for developing effective machine learning models that balance time/space cost with generalization ability.",1
"The task of Video Question Answering (VideoQA) consists in answering natural language questions about a video and serves as a proxy to evaluate the performance of a model in scene sequence understanding. Most methods designed for VideoQA up-to-date are end-to-end deep learning architectures which struggle at complex temporal and causal reasoning and provide limited transparency in reasoning steps. We present the HySTER: a Hybrid Spatio-Temporal Event Reasoner to reason over physical events in videos. Our model leverages the strength of deep learning methods to extract information from video frames with the reasoning capabilities and explainability of symbolic artificial intelligence in an answer set programming framework. We define a method based on general temporal, causal and physics rules which can be transferred across tasks. We apply our model to the CLEVRER dataset and demonstrate state-of-the-art results in question answering accuracy. This work sets the foundations for the incorporation of inductive logic programming in the field of VideoQA.",0
"This paper presents a new algorithm called HySTER (Hybrid Spatio-Temporal Event Reasoner) that addresses the problem of event reasoning in spatial domains. HySTER combines logical and geometric representations of events to reason about spatio-temporal relationships within a domain. By utilizing both forms of representation, HySTER can effectively handle uncertainty, occlusion, and other complexities often found in real world applications. Our approach has been validated through simulation experiments and demonstrates significant improvement over previous methods in terms of accuracy and scalability. We believe that our work provides a promising direction towards building more intelligent systems capable of robust event monitoring and decision making in dynamic environments.",1
"Training robust deep learning models for down-stream tasks is a critical challenge. Research has shown that down-stream models can be easily fooled with adversarial inputs that look like the training data, but slightly perturbed, in a way imperceptible to humans. Understanding the behavior of natural language models under these attacks is crucial to better defend these models against such attacks. In the black-box attack setting, where no access to model parameters is available, the attacker can only query the output information from the targeted model to craft a successful attack. Current black-box state-of-the-art models are costly in both computational complexity and number of queries needed to craft successful adversarial examples. For real world scenarios, the number of queries is critical, where less queries are desired to avoid suspicion towards an attacking agent. In this paper, we propose Explain2Attack, a black-box adversarial attack on text classification task. Instead of searching for important words to be perturbed by querying the target model, Explain2Attack employs an interpretable substitute model from a similar domain to learn word importance scores. We show that our framework either achieves or out-performs attack rates of the state-of-the-art models, yet with lower queries cost and higher efficiency.",0
"This paper presents a new method for generating adversarial attacks on text classification models that utilizes cross-domain interpretability techniques. We propose the use of explanations generated by these methods as guidance for creating effective perturbation strategies, allowing us to generate highly potent attacks using minimal modifications. Our approach, which we call ""Explain2Attack,"" combines state-of-the-art textual feature attribution algorithms with domain adaptation techniques to effectively transfer interpretability knowledge across domains. Experimental results demonstrate the efficacy of our proposed framework against diverse text classification models trained on various datasets, indicating significant improvement over prior attack approaches. Overall, this work provides important insights into understanding and mitigating vulnerabilities in modern deep learning systems, particularly those operating in security-sensitive applications such as natural language processing.",1
"In this paper, we propose CI-VI an efficient and scalable solver for semi-implicit variational inference (SIVI). Our method, first, maps SIVI's evidence lower bound (ELBO) to a form involving a nonlinear functional nesting of expected values and then develops a rigorous optimiser capable of correctly handling bias inherent to nonlinear nested expectations using an extrapolation-smoothing mechanism coupled with gradient sketching. Our theoretical results demonstrate convergence to a stationary point of the ELBO in general non-convex settings typically arising when using deep network models and an order of $O(t^{-\frac{4}{5}})$ gradient-bias-vanishing rate. We believe these results generalise beyond the specific nesting arising from SIVI to other forms. Finally, in a set of experiments, we demonstrate the effectiveness of our algorithm in approximating complex posteriors on various data-sets including those from natural language processing.",0
"Abstract This paper presents a new method called Efficient Semi-Implicit Variational Inference (ESIVI) which combines variational inference methods from statistical machine learning with classical implicit solvers based on matrix factorization techniques. Our approach uses dual decomposition to efficiently optimize over model parameters without requiring the computation of explicit gradients, while still ensuring that all stationary points of the problem have been found. We demonstrate the effectiveness of our approach using both synthetic data experiments and real-world image denoising tasks, showing improvements in accuracy compared to state-of-the art algorithms.",1
"Identifier names convey useful information about the intended semantics of code. Name-based program analyses use this information, e.g., to detect bugs, to predict types, and to improve the readability of code. At the core of name-based analyses are semantic representations of identifiers, e.g., in the form of learned embeddings. The high-level goal of such a representation is to encode whether two identifiers, e.g., len and size, are semantically similar. Unfortunately, it is currently unclear to what extent semantic representations match the semantic relatedness and similarity perceived by developers. This paper presents IdBench, the first benchmark for evaluating semantic representations against a ground truth created from thousands of ratings by 500 software developers. We use IdBench to study state-of-the-art embedding techniques proposed for natural language, an embedding technique specifically designed for source code, and lexical string distance functions. Our results show that the effectiveness of semantic representations varies significantly and that the best available embeddings successfully represent semantic relatedness. On the downside, no existing technique provides a satisfactory representation of semantic similarities, among other reasons because identifiers with opposing meanings are incorrectly considered to be similar, which may lead to fatal mistakes, e.g., in a refactoring tool. Studying the strengths and weaknesses of the different techniques shows that they complement each other. As a first step toward exploiting this complementarity, we present an ensemble model that combines existing techniques and that clearly outperforms the best available semantic representation.",0
"How can we effectively evaluate semantic representations of identifier names in source code? This task becomes more challenging as software systems grow larger and become increasingly complex. In our research, we introduce IdBench, a framework that enables systematic evaluation of semantic identifier name representations through benchmarking studies. By leveraging existing tools for static analysis, IdBench generates representative samples of identifiers from large software repositories, enabling comprehensive evaluations without requiring access to the entirety of any given repository. Our approach uses natural language processing techniques to create gold standard references for each benchmark based on expert annotations. We apply our methodology by evaluating three state-of-the-art semantic representations across multiple programming languages, including Java, C++, Python, JavaScript, and TypeScript. Through these experiments, we demonstrate how IdBench enables automated and detailed comparisons between different approaches to modeling semantic meaning using identifier names, providing new insights into current limitations and opportunities for future work. In summary, IdBench represents a step forward in advancing the study of identifiers within software engineering contexts, allowing researchers to draw accurate conclusions regarding their performance at scale.",1
"Video captioning is a popular task that challenges models to describe events in videos using natural language. In this work, we investigate the ability of various visual feature representations derived from state-of-the-art convolutional neural networks to capture high-level semantic context. We introduce the Weighted Additive Fusion Transformer with Memory Augmented Encoders (WAFTM), a captioning model that incorporates memory in a transformer encoder and uses a novel method, to fuse features, that ensures due importance is given to more significant representations. We illustrate a gain in performance realized by applying Word-Piece Tokenization and a popular REINFORCE algorithm. Finally, we benchmark our model on two datasets and obtain a CIDEr of 92.4 on MSVD and a METEOR of 0.091 on the ActivityNet Captions Dataset.",0
"This paper presents an exploratory study on the use of visual features and their weighted-additive fusion for video captioning tasks. The authors investigate different combinations of pre-trained convolutional neural network (CNN) models and their corresponding visual feature extraction techniques, such as VGGNet, ResNet, InceptionV3, and MobileNet, to identify which combination produces the most accurate captions. Additionally, they propose a weighted additive approach that combines multiple visual features from different CNNs into one overall representation, allowing each model to contribute its strengths towards generating more effective captions. To evaluate the effectiveness of these approaches, experiments were conducted using several popular benchmark datasets, including MSR-VTT, MSVD, and LSMDC. Results demonstrate the potential benefits of incorporating both visual and textual modalities and highlight promising future directions for video captioning research. Overall, this work contributes to the growing body of knowledge in computer vision and natural language processing by advancing methods for automated video understanding through caption generation.",1
"In this paper, we teach machines to understand visuals and natural language by learning the mapping between sentences and noisy video snippets without explicit annotations. Firstly, we define a self-supervised learning framework that captures the cross-modal information. A novel adversarial learning module is then introduced to explicitly handle the noises in the natural videos, where the subtitle sentences are not guaranteed to be strongly corresponded to the video snippets. For training and evaluation, we contribute a new dataset `ApartmenTour' that contains a large number of online videos and subtitles. We carry out experiments on the bidirectional retrieval tasks between sentences and videos, and the results demonstrate that our proposed model achieves the state-of-the-art performance on both retrieval tasks and exceeds several strong baselines. The dataset can be downloaded at https://github.com/zyj-13/WAL.",0
"In recent years, self-supervised learning has emerged as a powerful tool for training machine vision models on large amounts of data without relying on labeled examples. However, most existing methods have focused on still images rather than videos. In this work, we propose a novel approach that uses natural language instructions and video frames to learn visual representations that can generalize to new contexts. Our method consists of three main components: a textual prompt generator that generates natural language descriptions of scenes in noisy real-world videos; a pretext task module that learns to predict which pixels match the given description; and a model that maps learned features into a semantic space where they can be compared against human judgments. We evaluate our approach using benchmark datasets and demonstrate its effectiveness at mapping visual content to natural language descriptors and vice versa. Our results show that our method outperforms state-of-the-art baselines, providing evidence that watch and learn strategies can indeed scale to complex real-world environments.",1
"Transformer has become the new standard method in natural language processing (NLP), and it also attracts research interests in computer vision area. In this paper we investigate the application of Transformer in Image Quality (TRIQ) assessment. Following the original Transformer encoder employed in Vision Transformer (ViT), we propose an architecture of using a shallow Transformer encoder on the top of a feature map extracted by convolution neural networks (CNN). Adaptive positional embedding is employed in the Transformer encoder to handle images with arbitrary resolutions. Different settings of Transformer architectures have been investigated on publicly available image quality databases. We have found that the proposed TRIQ architecture achieves outstanding performance. The implementation of TRIQ is published on Github (https://github.com/junyongyou/triq).",0
"This paper presents a new method using deep learning techniques to improve image quality assessment (QA). We use transformers as a base model to provide rich feature representations that capture both local features such as edges, corners, textures, etc., and global features like objectness, scenes, lighting conditions, etc. Our experiments on several widely used benchmark datasets show promising results outperforming classical metrics by large margins in terms of accuracy and robustness. Additionally, we demonstrate the generalization capability of our proposed approach on previously unseen data by evaluating it under real-world scenarios encountered during content delivery over cellular networks where QoE can vary significantly due to changes in network conditions.",1
"Uncertainty quantification (UQ) plays a pivotal role in reduction of uncertainties during both optimization and decision making processes. It can be applied to solve a variety of real-world applications in science and engineering. Bayesian approximation and ensemble learning techniques are two most widely-used UQ methods in the literature. In this regard, researchers have proposed different UQ methods and examined their performance in a variety of applications such as computer vision (e.g., self-driving cars and object detection), image processing (e.g., image restoration), medical image analysis (e.g., medical image classification and segmentation), natural language processing (e.g., text classification, social media texts and recidivism risk-scoring), bioinformatics, etc. This study reviews recent advances in UQ methods used in deep learning. Moreover, we also investigate the application of these methods in reinforcement learning (RL). Then, we outline a few important applications of UQ methods. Finally, we briefly highlight the fundamental research challenges faced by UQ methods and discuss the future research directions in this field.",0
"In recent years, there has been significant interest in quantifying uncertainty in deep learning models due to their increasing adoption in real world applications where decision making and reliability can have serious consequences if incorrect predictions are made by these systems. Deep neural networks (DNNs) rely on large amounts of data which can often suffer from noise, corruption or insufficient representation leading to high variance during optimization resulting in poor generalization performance. This paper reviews several techniques developed for model calibration to reduce overconfidence of DNNs through probabilistic methods such as Bayesian inference and Monte Carlo dropout (MC). Techniques like Bayesian Neural Networks enable efficient estimation of confidence intervals which is crucial when operating under limited computational resources. MC provides a computationally cheap alternative that approximates predictive uncertainties at test time without requiring additional training which makes it more suitable when deploying the models onto edge devices like mobile phones. These techniques are applied into different domains including image classification, object detection, sentiment analysis and natural language processing illustrating improved performances using uncertainty estimates. Furthermore, we discuss limitations that arise when applying the previously mentioned approaches due to neglected challenges related to hyperparameter tuning, loss landscapes, non stationary environments and data quality assessment . As future work direction, we propose solutions directed towards mitigating some of these issues in order to further improve accuracy and trustworthiness in deep learning systems. Finally, in conclusion we provide recommendations for researchers interested in studying these topics to expand upon the current state of art given our detailed insight i",1
"When minimizing the empirical risk in binary classification, it is a common practice to replace the zero-one loss with a surrogate loss to make the learning objective feasible to optimize. Examples of well-known surrogate losses for binary classification include the logistic loss, hinge loss, and sigmoid loss. It is known that the choice of a surrogate loss can highly influence the performance of the trained classifier and therefore it should be carefully chosen. Recently, surrogate losses that satisfy a certain symmetric condition (aka., symmetric losses) have demonstrated their usefulness in learning from corrupted labels. In this article, we provide an overview of symmetric losses and their applications. First, we review how a symmetric loss can yield robust classification from corrupted labels in balanced error rate (BER) minimization and area under the receiver operating characteristic curve (AUC) maximization. Then, we demonstrate how the robust AUC maximization method can benefit natural language processing in the problem where we want to learn only from relevant keywords and unlabeled documents. Finally, we conclude this article by discussing future directions, including potential applications of symmetric losses for reliable machine learning and the design of non-symmetric losses that can benefit from the symmetric condition.",0
"In recent years, machine learning has been revolutionized by deep learning techniques that allow models to learn highly expressive function approximators from large datasets. However, these powerful methods have often yielded surprisingly poor results on out-of-sample data compared to classical statistics or other modeling approaches. To address this issue, we present a new approach called symmetric loss (SL) which focuses on improving accuracy for both training and test sets. We demonstrate how SL can recover previously known insights and provide better predictions for challenging benchmarks such as CIFAR, Penn Treebank, and TIMIT, using popular architectures like ResNet and Transformer models trained via backprop. This work extends prior art that relies solely on asymmetrical metrics like cross entropy loss during training to achieve desired performance. Our findings suggest that incorporating symmetrization into modern machine learning pipelines may improve overall predictability and reliability. Future studies can further explore the relationship between dataset size and model capacity relative to the degree of symmetrization applied towards achieving high validation accuracy. The code accompanying this article implements our technique in PyTorch.",1
"Recently, a special kind of graph, i.e., supernet, which allows two nodes connected by multi-choice edges, has exhibited its power in neural architecture search (NAS) by searching for better architectures for computer vision (CV) and natural language processing (NLP) tasks. In this paper, we discover that the design of such discrete architectures also appears in many other important learning tasks, e.g., logical chain inference in knowledge graphs (KGs) and meta-path discovery in heterogeneous information networks (HINs). Thus, we are motivated to generalize the supernet search problem on a broader horizon. However, none of the existing works are effective since the supernet topology is highly task-dependent and diverse. To address this issue, we propose to tensorize the supernet, i.e., unify the subgraph search problems by a tensor formulation and encode the topology inside the supernet by a tensor network. We further propose an efficient algorithm that admits both stochastic and deterministic objectives to solve the search problem. Finally, we perform extensive experiments on diverse learning tasks, i.e., architecture design for CV, logic inference for KG, and meta-path discovery for HIN. Empirical results demonstrate that our method leads to better performance and architectures.",0
"""The problem we study concerns finding a subgraph within another graph satisfying certain properties: these may depend on edge labels as well as vertex colors. We develop an algorithm that efficiently builds tensors from scratch by performing tensor operations in parallel across multiple cores, reducing overall memory usage compared to traditional techniques."" This sounds interesting! Can you tell me more about why this might be important? Would other approaches take longer, use up more resources or produce inferior results? Are there real world applications where such techniques could make a difference? What kind of graphs are we talking about here? It seems like it might be quite flexible approach that can handle different kinds of constraints... Is that accurate?",1
"Machine Learning has been applied in a wide range of tasks throughout the last years, ranging from image classification to autonomous driving and natural language processing. Restricted Boltzmann Machine (RBM) has received recent attention and relies on an energy-based structure to model data probability distributions. Notwithstanding, such a technique is susceptible to adversarial manipulation, i.e., slightly or profoundly modified data. An alternative to overcome the adversarial problem lies in the Generative Adversarial Networks (GAN), capable of modeling data distributions and generating adversarial data that resemble the original ones. Therefore, this work proposes to artificially generate RBMs using Adversarial Learning, where pre-trained weight matrices serve as the GAN inputs. Furthermore, it proposes to sample copious amounts of matrices and combine them into ensembles, alleviating the burden of training new models'. Experimental results demonstrate the suitability of the proposed approach under image reconstruction and image classification tasks, and describe how artificial-based ensembles are alternatives to pre-training vast amounts of RBMs.",0
"Title: A Novel Approach to Ensemble Learning using Generative Models. This paper introduces a new method for ensemble learning that leverages adversarial training on generative models. Our approach uses restricted Boltzmann machines (RBMs) as the foundation of our generative models, which allows us to efficiently learn complex relationships within large datasets. By incorporating an adversarial component into the training process, we can improve the generalization performance of these models across different tasks and domains. In particular, we show how this approach outperforms traditional ensembling methods in several benchmark experiments. We believe this work represents an important step towards more effective ensemble learning techniques in deep neural networks.",1
"Image captioning is a challenging computer vision task, which aims to generate a natural language description of an image. Most recent researches follow the encoder-decoder framework which depends heavily on the previous generated words for the current prediction. Such methods can not effectively take advantage of the future predicted information to learn complete semantics. In this paper, we propose Context-Aware Auxiliary Guidance (CAAG) mechanism that can guide the captioning model to perceive global contexts. Upon the captioning model, CAAG performs semantic attention that selectively concentrates on useful information of the global predictions to reproduce the current generation. To validate the adaptability of the method, we apply CAAG to three popular captioners and our proposal achieves competitive performance on the challenging Microsoft COCO image captioning benchmark, e.g. 132.2 CIDEr-D score on Karpathy split and 130.7 CIDEr-D (c40) score on official online evaluation server.",0
"Our work presents a novel model for image caption generation that uses auxiliary guidance derived from contextual relationships within images themselves. These relationships could reflect spatial, temporal, causal, or other forms of logical interdependencies among multiple objects visible in the scene. By incorporating these cues into our captioning pipeline, we aim to improve the quality and coherence of generated descriptions by aligning them more closely with visual features present in input images. This approach brings together insights from both computer vision and natural language processing domains, and advances our understanding of how humans process complex scenes through integrating multimodal signals. We demonstrate on standard benchmark datasets the effectiveness of using context-aware auxiliary guidance in generating accurate, informative, and diverse captions, outperforming strong baselines that rely exclusively on image content or text prompts alone. While previous efforts have explored adding external knowledge sources like semantic concepts or human annotations to enhance captioning capabilities, here we focus instead on discovering latent relationships hidden inside each individual picture, which might serve as internal supervision or complementary signal when such explicit data is scarce or unavailable. Our findings suggest promising directions towards developing intelligent agents capable of understanding and narrating complex dynamic events involving numerous entities acting over time.",1
"The number of videos being produced and consequently stored in databases for video streaming platforms has been increasing exponentially over time. This vast database should be easily index-able to find the requisite clip or video to match the given search specification, preferably in the form of a textual query. This work aims to provide an end-to-end pipeline to search a video database with a voice query from the end user. The pipeline makes use of Recurrent Neural Networks in combination with Convolutional Neural Networks to generate captions of the video clips present in the database.",0
"This paper presents a method for searching raw video databases using natural language queries. We propose a novel approach that combines computer vision techniques with natural language processing algorithms to efficiently retrieve relevant footage from large collections of unstructured videos. Our system processes spoken language input and generates visual semantic representations, which are used to match against indexed frames from the database. Experimental results demonstrate the effectiveness of our approach in accurately identifying key segments from raw video data based on user-provided query statements. The proposed method offers significant potential benefits for applications such as multimedia retrieval, surveillance analysis, and automated content creation.",1
"Deep generative models have achieved great success in areas such as image, speech, and natural language processing in the past few years. Thanks to the advances in graph-based deep learning, and in particular graph representation learning, deep graph generation methods have recently emerged with new applications ranging from discovering novel molecular structures to modeling social networks. This paper conducts a comprehensive survey on deep learning-based graph generation approaches and classifies them into five broad categories, namely, autoregressive, autoencoder-based, RL-based, adversarial, and flow-based graph generators, providing the readers a detailed description of the methods in each class. We also present publicly available source codes, commonly used datasets, and the most widely utilized evaluation metrics. Finally, we highlight the existing challenges and discuss future research directions.",0
"As machine learning models have advanced over recent years, generating large-scale graphs has become an essential task across several domains such as social networks, transportation systems, biological networks, and chemical compounds. Many different algorithms have been developed for graph generation; however, due to their popularity and importance, deep learning approaches have gained significant attention in recent times. In this survey, we provide a comprehensive review of state-of-the-art deep graph generators that have recently emerged. We discuss their underlying principles and architectures, evaluate their effectiveness on multiple benchmark datasets, compare their performance against traditional methods, and highlight open challenges and future research directions. Our aim is to give readers a detailed understanding of deep graph generators and inspire new ideas for further improvement in graph generation tasks using deep learning techniques.",1
"With the advent of state-of-the-art machine learning and deep learning technologies, several industries are moving towards the field. Applications of such technologies are highly diverse ranging from natural language processing to computer vision. Object recognition is one such area in the computer vision domain. Although proven to perform with high accuracy, there are still areas where such models can be improved. This is in-fact highly important in real-world use cases like autonomous driving or cancer detection, that are highly sensitive and expect such technologies to have almost no uncertainties. In this paper, we attempt to visualise the uncertainties in object recognition models and propose a correction process via user feedback. We further demonstrate our approach on the data provided by the VAST 2020 Mini-Challenge 2.",0
"This article describes how we used machine learning models that process images taken by smartphone cameras to identify objects such as cars, houses, animals, etc., but they didn’t work very well, so we collected new data using an interactive game on real phones connected via wifi from Amazon Mechanical Turk workers, and fed them into another model which was able to give correct answers over 97% of the time. Our techniques were inspired by prior researchers who had similar problems but did it manually instead of using crowdsourcing like us. We expect our method to become more important as computer vision is applied to many different fields because it allows models to keep improving as they learn from their mistakes. We hope other researchers can build upon our work to create even better methods for fixing errors in object recognition systems. We close by discussing several promising directions for future research.",1
"One of the most challenging problems in the field of intrusion detection is anomaly detection for discrete event logs. While most earlier work focused on applying unsupervised learning upon engineered features, most recent work has started to resolve this challenge by applying deep learning methodology to abstraction of discrete event entries. Inspired by natural language processing, LSTM-based anomaly detection models were proposed. They try to predict upcoming events, and raise an anomaly alert when a prediction fails to meet a certain criterion. However, such a predict-next-event methodology has a fundamental limitation: event predictions may not be able to fully exploit the distinctive characteristics of sequences. This limitation leads to high false positives (FPs) and high false negatives (FNs). It is also critical to examine the structure of sequences and the bi-directional causality among individual events. To this end, we propose a new methodology: Recomposing event sequences as anomaly detection. We propose DabLog, a Deep Autoencoder-Based anomaly detection method for discrete event Logs. The fundamental difference is that, rather than predicting upcoming events, our approach determines whether a sequence is normal or abnormal by analyzing (encoding) and reconstructing (decoding) the given sequence. Our evaluation results show that our new methodology can significantly reduce the numbers of FPs and FNs, hence achieving a higher $F_1$ score.",0
"Our proposed method offers a novel approach to anomaly detection for discrete events based on autoencoders. By utilizing a combination of recomposition and prediction techniques, we are able to accurately identify rare events that deviate from expected behavior while maintaining computational efficiency. This method is particularly suited for high-dimensional data sets with complex dependencies, where traditional anomaly detection methods may struggle. We demonstrate the effectiveness of our approach through comprehensive experiments on real-world datasets, showing significant improvement over state-of-the-art baseline models. Overall, our work presents a promising new direction for unsupervised learning in the field of anomaly detection.",1
"Electrical energy is a vital part of modern life, and expectations for grid resilience to allow a continuous and reliable energy supply has tremendously increased even during adverse events (e.g., Ukraine cyber-attack, Hurricane Maria). The global pandemic COVID-19 has raised the electric energy reliability risk due to potential workforce disruptions, supply chain interruptions, and increased possible cybersecurity threats. The pandemic introduces a significant degree of uncertainly to the grid operation in the presence of other extreme events like natural disasters, unprecedented outages, aging power grids, high proliferation of distributed generation, and cyber-attacks. This situation increases the need for measures for the resiliency of power grids to mitigate the impacts of the pandemic as well as simultaneous extreme events. Solutions to manage such an adverse scenario will be multi-fold: a) emergency planning and organizational support, b) following safety protocol, c) utilizing enhanced automation and sensing for situational awareness, and d) integration of advanced technologies and data points for ML-driven enhanced decision support. Enhanced digitalization and automation resulted in better network visibility at various levels, including generation, transmission, and distribution. These data or information can be utilized to take advantage of advanced machine learning techniques for automation and increased power grid resilience. In this paper, a) we review the impact of COVID-19 on power grid operations and actions taken by operators/organizations to minimize the impact of COVID-19, and b) we have presented the recently developed tool and concepts using natural language processing (NLP) in the domain of machine learning and artificial intelligence that can be used for increasing resiliency of power systems in normal and in extreme scenarios such as COVID-19 pandemics.",0
"This paper discusses how data can be used to improve grid resilience during a crisis like Covid-19. We focus on describing two important aspects related to grid operation that benefit from data analysis: contingency planning, which seeks to prepare for potential extreme events; and real-time operation, where timely decisions aimed at preventing cascading failures have significant impacts. With respect to contingency planning, historical grid event data, combined with weather forecasting models, can provide insights into the likelihood of specific risks occurring. By leveraging these data streams, utilities can optimize their investment in infrastructure maintenance and enhance emergency response plans under resource constraints. Real-time operation involves managing uncontrollable loads (such as distributed photovoltaics) while maintaining reliable service quality to customers. To achieve this goal, advanced algorithms need access to granular power consumption data. Given appropriate security measures, sharing aggregate load shapes within distribution feeders among multiple parties could lead to improved management strategies without compromising privacy. For both use cases, we highlight benefits from integrating diverse datasets originating from external sources beyond traditional SCADA systems. Open availability of such datasets further encourages innovation through research communities worldwide. The contributions provided by this work intend to spark a productive discussion on responsible collection and usage of data assets critical to enhancing overall system reliability. Ultimately, society must find ways to collectively support sustainable development without sacrificing the privacy and autonomy of individual citizens. These dual objectives require tradeoffs demanding thoughtful consideration throughout all sectors, including utilit",1
"We address the problem of retrieving a specific moment from an untrimmed video by a query sentence. This is a challenging problem because a target moment may take place in relations to other temporal moments in the untrimmed video. Existing methods cannot tackle this challenge well since they consider temporal moments individually and neglect the temporal dependencies. In this paper, we model the temporal relations between video moments by a two-dimensional map, where one dimension indicates the starting time of a moment and the other indicates the end time. This 2D temporal map can cover diverse video moments with different lengths, while representing their adjacent relations. Based on the 2D map, we propose a Temporal Adjacent Network (2D-TAN), a single-shot framework for moment localization. It is capable of encoding the adjacent temporal relation, while learning discriminative features for matching video moments with referring expressions. We evaluate the proposed 2D-TAN on three challenging benchmarks, i.e., Charades-STA, ActivityNet Captions, and TACoS, where our 2D-TAN outperforms the state-of-the-art.",0
"Automatically detecting moments from videos using natural language queries has been challenging due to the difficulty in accurately aligning textual descriptions with specific frames. In this work, we propose a novel approach called ""Learning 2D Temporal Adjacent Network"" (LaTAN) that utilizes temporal adjacent relationships to learn discriminative moment localizations directly from video clips and natural language query sequences. LaTAN extracts spatio-temporal features by sampling dense grid locations within a clip at different time intervals. Our design enables end-to-end training of feature extraction and subsequent matching components without requiring any external supervision. We demonstrate that our method outperforms prior state-of-the-art approaches on two benchmark datasets, MovieCorpus and Charades, yielding significantly higher recall@1 scores while maintaining competitive nDCG performance. This suggests that our proposed model can effectively learn discriminative representations for moments localized through natural language queries. Overall, these results show promise towards building intelligent video retrieval systems based on fine-grained textual inputs.",1
"Generative Adversarial Networks (GANs) have been extremely successful in various application domains such as computer vision, medicine, and natural language processing. Moreover, transforming an object or person to a desired shape become a well-studied research in the GANs. GANs are powerful models for learning complex distributions to synthesize semantically meaningful samples. However, there is a lack of comprehensive review in this field, especially lack of a collection of GANs loss-variant, evaluation metrics, remedies for diverse image generation, and stable training. Given the current fast GANs development, in this survey, we provide a comprehensive review of adversarial models for image synthesis. We summarize the synthetic image generation methods, and discuss the categories including image-to-image translation, fusion image generation, label-to-image mapping, and text-to-image translation. We organize the literature based on their base models, developed ideas related to architectures, constraints, loss functions, evaluation metrics, and training datasets. We present milestones of adversarial models, review an extensive selection of previous works in various categories, and present insights on the development route from the model-based to data-driven methods. Further, we highlight a range of potential future research directions. One of the unique features of this review is that all software implementations of these GAN methods and datasets have been collected and made available in one place at https://github.com/pshams55/GAN-Case-Study.",0
"Artificial intelligence has seen rapid advancements in recent years, particularly in image synthesis using generative adversarial networks (GANs). In this paper, we aim to provide a comprehensive survey on GAN-based image synthesis techniques and present case studies showcasing their applications across domains. Our study begins by providing background information on GANs, which have emerged as one of the most prominent deep learning models for generating high-quality images. Subsequently, we analyze different architectures used in GANs for image generation tasks such as DCGAN, ProGAN, SNGAN, WGAN-GP, etc., along with their variants. We focus on examining each architecture’s strengths, weaknesses, and key differences that make them suitable for particular applications. Moreover, we discuss recent advancements made in designing objective functions for improving GAN performance, including perceptual loss, feature matching, multi-scale discriminator analysis, and more. Additionally, we explore techniques employed to stabilize training, enhance convergence speed, reduce mode collapse issues, and increase diversity during image generation. Moving beyond traditional frameworks like DCGAN and ProGAN, our survey explores the broader landscape of advanced methods based on conditional image synthesis, semi-supervised learning, texture synthesis, attribute transfer, style transfer, data augmentation, domain adaptation, and multimodal fusion. Finally, we present several case studies demonstrating how GANs can be leveraged effectively in computer vision problems like image translation, superresolution, video frame interpolation, object detection/segmentation mask generation, image retouching, face editing, zero-shot learning, cross-modal retrieval, creative graphics, and more. Overall, this paper provides readers wi",1
"We study reinforcement learning (RL) for text-based games, which are interactive simulations in the context of natural language. While different methods have been developed to represent the environment information and language actions, existing RL agents are not empowered with any reasoning capabilities to deal with textual games. In this work, we aim to conduct explicit reasoning with knowledge graphs for decision making, so that the actions of an agent are generated and supported by an interpretable inference procedure. We propose a stacked hierarchical attention mechanism to construct an explicit representation of the reasoning process by exploiting the structure of the knowledge graph. We extensively evaluate our method on a number of man-made benchmark games, and the experimental results demonstrate that our method performs better than existing text-based agents.",0
"This paper presents a deep reinforcement learning algorithm that uses stacked hierarchical attention mechanisms to address some of the challenges faced by traditional methods. The proposed approach is evaluated on several text-based games and shows significant improvement over state-of-the-art models. In addition, we provide a comprehensive analysis of our method's performance, including comparisons with baseline algorithms and ablation studies. Our findings demonstrate the effectiveness of using stacked hierarchical attention mechanisms for deep reinforcement learning tasks in complex environments such as text-based games. Overall, this work advances the field of artificial intelligence towards more intelligent agents capable of performing well in difficult situations. We believe our contributions will spur further research in developing adaptive agents that can effectively learn from their interactions with humans and other decision makers in rich, uncertain environments.",1
"Vision-and-Language Navigation (VLN) requires an agent to navigate in a real-world environment following natural language instructions. From both the textual and visual perspectives, we find that the relationships among the scene, its objects,and directional clues are essential for the agent to interpret complex instructions and correctly perceive the environment. To capture and utilize the relationships, we propose a novel Language and Visual Entity Relationship Graph for modelling the inter-modal relationships between text and vision, and the intra-modal relationships among visual entities. We propose a message passing algorithm for propagating information between language elements and visual entities in the graph, which we then combine to determine the next action to take. Experiments show that by taking advantage of the relationships we are able to improve over state-of-the-art. On the Room-to-Room (R2R) benchmark, our method achieves the new best performance on the test unseen split with success rate weighted by path length (SPL) of 52%. On the Room-for-Room (R4R) dataset, our method significantly improves the previous best from 13% to 34% on the success weighted by normalized dynamic time warping (SDTW). Code is available at: https://github.com/YicongHong/Entity-Graph-VLN.",0
"This paper presents a method that uses language and visual representations to create relationship graphs among different entities within agent navigation tasks. By creating entity relationships using natural language processing techniques such as named entity recognition (NER) and dependency parsing, we can generate entity graphs that represent important connections between objects in a given environment. These graph structures allow agents to reason about their surroundings more efficiently and accurately than by relying solely on textual descriptions. Our approach improves over previous work by incorporating both static visual features and dynamic sensor data into our relation extraction process, allowing us to obtain richer and more accurate representation of navigational environments. Experimental results demonstrate significant improvements over baseline methods across multiple metrics in simulation and real-world environments. We anticipate future applications of our approach for virtual and augmented reality systems, robotics, and other areas where spatial reasoning is crucial.",1
"Deep neural networks have been successfully deployed in various domains of artificial intelligence, including computer vision and natural language processing. We observe that the current standard procedure for training DNNs discards all the learned information in the past epochs except the current learned weights. An interesting question is: is this discarded information indeed useless? We argue that the discarded information can benefit the subsequent training. In this paper, we propose learning with retrospection (LWR) which makes use of the learned information in the past epochs to guide the subsequent training. LWR is a simple yet effective training framework to improve accuracies, calibration, and robustness of DNNs without introducing any additional network parameters or inference cost, only with a negligible training overhead. Extensive experiments on several benchmark datasets demonstrate the superiority of LWR for training DNNs.",0
"This may need reworking into something like ""Retrospective learning allows individuals and organizations alike to reflect on past experiences and adapt future actions accordingly. By acknowledging their strengths and weaknesses and identifying potential opportunities and threats, retrospection empowers both personal and professional growth. However, while many benefits have been identified through research and practice, little attention has been paid towards understanding how and why certain methods of retrospection work better than others. Therefore, we aim to examine different methodologies used in retrospection practices and identify those that lead to the most effective outcomes. Our goal is to provide insights into how retrospective learning can be improved by optimizing techniques to enhance organizational development."" -- I think you did a good job summarizing the idea behind the article! Here are some suggestions to make your abstract more specific: * Provide examples of the types of methodology or techniques that will be discussed. For example, will you be comparing retrospectives vs after action reviews? Will there be discussion of specific facilitation techniques or questions used during these exercises? * Include a statement about your hypothesis or prediction about which methods might show superior results. Without any mention of hypotheses or predictions it sounds like this paper will purely be descriptive. Even if you cannot test them experimentally yourself because of time/access constraints saying what factors you predict could influence effectiveness would add depth to the abstract. * Discuss whether this study takes place at one company over time, several companies over time or via case studies etc.. Adding a few details about the scope of the study helps give readers context about how generalizable the findings will be. Feel free to use this as inspiration fo",1
"The Visual Question Answering (VQA) task combines challenges for processing data with both Visual and Linguistic processing, to answer basic `common sense' questions about given images. Given an image and a question in natural language, the VQA system tries to find the correct answer to it using visual elements of the image and inference gathered from textual questions. In this survey, we cover and discuss the recent datasets released in the VQA domain dealing with various types of question-formats and robustness of the machine-learning models. Next, we discuss about new deep learning models that have shown promising results over the VQA datasets. At the end, we present and discuss some of the results computed by us over the vanilla VQA model, Stacked Attention Network and the VQA Challenge 2017 winner model. We also provide the detailed analysis along with the challenges and future research directions.",0
"Title: Visual Question Answering (VQA) has emerged as one of the most challenging tasks in computer vision due to its requirement for both high level understanding of images and natural language processing capabilities. Recently, deep learning techniques have shown significant improvements in VQA performance, making it possible to accurately generate answers based on visual context. This paper provides a survey of current state-of-the-art approaches to VQA, including convolutional neural networks, recurrent neural networks, transformers, attention mechanisms, memory modules, generative models, and multi-modal fusion methods. We provide insights into how these architectures are used to address different components of the VQA task such as image feature extraction, question representation, joint embedding space construction, answer prediction, and knowledge integration. Furthermore, we present a comprehensive evaluation of numerous state-of-the-art VQA algorithms, which highlights their respective strengths and weaknesses. Finally, we identify key research directions that could lead to further advances in VQA, namely more effective use of external resources, better handling of uncertainty and ambiguity, and development of metrics capable of capturing subtle differences in VQA model quality. Overall, our study demonstrates the great potential of deep learning in achieving superior performance in VQA while underscoring some of the limitations that must still be overcome.",1
"There is very little notable research on generating descriptions of the Bengali language. About 243 million people speak in Bengali, and it is the 7th most spoken language on the planet. The purpose of this research is to propose a CNN and Bidirectional GRU based architecture model that generates natural language captions in the Bengali language from an image. Bengali people can use this research to break the language barrier and better understand each other's perspectives. It will also help many blind people with their everyday lives. This paper used an encoder-decoder approach to generate captions. We used a pre-trained Deep convolutional neural network (DCNN) called InceptonV3image embedding model as the encoder for analysis, classification, and annotation of the dataset's images Bidirectional Gated Recurrent unit (BGRU) layer as the decoder to generate captions. Argmax and Beam search is used to produce the highest possible quality of the captions. A new dataset called BNATURE is used, which comprises 8000 images with five captions per image. It is used for training and testing the proposed model. We obtained BLEU-1, BLEU-2, BLEU-3, BLEU-4 and Meteor is 42.6, 27.95, 23, 66, 16.41, 28.7 respectively.",0
"Abstract: In recent years, deep learning has made significant strides towards enabling machines to generate human-like language, including image descriptions and translations. However, most research in natural language processing (NLP) relies on large amounts of hand-annotated data which may not always be feasible or available. To address this issue, we propose a novel approach using convolutional neural networks (CNNs) and bidirectional gated recurrent units (BGRUs) to generate Bangla captions from images. Our model takes as input an image and generates a sequence of Bangla characters, effectively translating the visual content into text. We evaluate our model against several benchmark datasets and show promising results, outperforming existing state-of-the-art methods. Our method allows us to generate accurate Bangla captions without requiring extensive manual annotations or parallel corpora. Additionally, our approach can potentially be extended to other NLP tasks such as machine translation or summarization. Overall, our work represents a step forward in advancing the field of computer vision and natural language generation.  Please note that this is just a sample, any similarity found would only be by chance.",1
"Mesh is a powerful data structure for 3D shapes. Representation learning for 3D meshes is important in many computer vision and graphics applications. The recent success of convolutional neural networks (CNNs) for structured data (e.g., images) suggests the value of adapting insight from CNN for 3D shapes. However, 3D shape data are irregular since each node's neighbors are unordered. Various graph neural networks for 3D shapes have been developed with isotropic filters or predefined local coordinate systems to overcome the node inconsistency on graphs. However, isotropic filters or predefined local coordinate systems limit the representation power. In this paper, we propose a local structure-aware anisotropic convolutional operation (LSA-Conv) that learns adaptive weighting matrices for each node according to the local neighboring structure and performs shared anisotropic filters. In fact, the learnable weighting matrix is similar to the attention matrix in the random synthesizer -- a new Transformer model for natural language processing (NLP). Comprehensive experiments demonstrate that our model produces significant improvement in 3D shape reconstruction compared to state-of-the-art methods.",0
"This paper presents a method for learning local neighboring structure for robust 3D shape representation. We address the challenge that many existing methods suffer from when dealing with noisy or incomplete data by learning the intrinsic neighborhood relationships within shapes. Our approach uses a graph neural network (GNN) framework to capture these relationships and obtains high-quality representations of complex shapes. The proposed model effectively captures both global context and local details, resulting in more accurate shape reconstruction compared to previous state-of-the-art approaches. Furthermore, we demonstrate our method's effectiveness through extensive experiments on several benchmark datasets and provide visualization results for better understanding. Overall, our research contributes new insights into the field of computer vision and serves as a valuable resource for future studies in related areas.",1
"In recent years, deep learning has made great progress in many fields such as image recognition, natural language processing, speech recognition and video super-resolution. In this survey, we comprehensively investigate 33 state-of-the-art video super-resolution (VSR) methods based on deep learning. It is well known that the leverage of information within video frames is important for video super-resolution. Thus we propose a taxonomy and classify the methods into six sub-categories according to the ways of utilizing inter-frame information. Moreover, the architectures and implementation details of all the methods are depicted in detail. Finally, we summarize and compare the performance of the representative VSR method on some benchmark datasets. We also discuss some challenges, which need to be further addressed by researchers in the community of VSR. To the best of our knowledge, this work is the first systematic review on VSR tasks, and it is expected to make a contribution to the development of recent studies in this area and potentially deepen our understanding to the VSR techniques based on deep learning.",0
"In recent years, video super resolution (VSR) has become increasingly popular due to advances in deep learning techniques and computer vision algorithms. VSR refers to the process of enhancing the resolution of low-resolution videos by using advanced image processing methods. This comprehensive survey provides a detailed overview of state-of-the-art VSR techniques based on deep learning approaches. We begin by introducing background knowledge in Section 2, followed by discussions of existing VSR methods in Sections 3–4. Then, we provide evaluations and comparisons of different VSR models in Section 5 and Section 6 concludes our work by highlighting future research directions. Our aim is to give readers a clear understanding of VSR technology and its application areas such as surveillance systems and medical imaging. By doing so, this survey can serve both as a reference guide and starting point for those interested in VSR studies. Title: Video Super Resolution Based on Deep Learning: A Comprehensive Survey.",1
"Since the publication of the original Transformer architecture (Vaswani et al. 2017), Transformers revolutionized the field of Natural Language Processing. This, mainly due to their ability to understand timely dependencies better than competing RNN-based architectures. Surprisingly, this architecture change does not affect the field of Reinforcement Learning (RL), even though RNNs are quite popular in RL, and time dependencies are very common in RL. Recently, Parisotto et al. 2019) conducted the first promising research of Transformers in RL. To support the findings of this work, this paper seeks to provide an additional example of a Transformer-based RL method. Specifically, the goal is a simple Transformer-based Deep Q-Learning method that is stable over several environments. Due to the unstable nature of Transformers and RL, an extensive method search was conducted to arrive at a final method that leverages developments around Transformers as well as Q-learning. The proposed method can match the performance of classic Q-learning on control environments while showing potential on some selected Atari benchmarks. Furthermore, it was critically evaluated to give additional insights into the relation between Transformers and RL.",0
"Transformers have become popular models for natural language processing tasks due to their ability to model long-range dependencies effectively. However, using transformer architectures directly for action sequence generation has been shown to result in unstable training dynamics and suboptimal performance. In this paper, we propose several techniques that aim to stabilize the training process of transformer-based action sequence generators, resulting in improved stability and better overall performance. We evaluate our proposed methods on three different domains: text-based games, code completion, and machine translation. Our results show that by applying these techniques, we can significantly improve the quality of generated sequences while reducing instability during training. This research contributes to the field of natural language processing by providing insights into how to optimize the use of transformers for generating action sequences. Overall, our work demonstrates the potential for transformer-based approaches to be effective tools for solving complex sequential decision problems.",1
"Deep neural networks (DNNs) have achieved outstanding performance in a wide range of applications, e.g., image classification, natural language processing, etc. Despite the good performance, the huge number of parameters in DNNs brings challenges to efficient training of DNNs and also their deployment in low-end devices with limited computing resources. In this paper, we explore the correlations in the weight matrices, and approximate the weight matrices with the low-rank block-term tensors. We name the new corresponding structure as block-term tensor layers (BT-layers), which can be easily adapted to neural network models, such as CNNs and RNNs. In particular, the inputs and the outputs in BT-layers are reshaped into low-dimensional high-order tensors with a similar or improved representation power. Sufficient experiments have demonstrated that BT-layers in CNNs and RNNs can achieve a very large compression ratio on the number of parameters while preserving or improving the representation power of the original DNNs.",0
"Abstract: Block-term tensor neural networks (BTNN) is a novel technique that has emerged from the field of machine learning as a powerful approach for solving complex problems that involve large amounts of data. BTNN combines traditional deep learning techniques such as convolutional nets and recurrent nets, while leveraging innovative new features like attention mechanisms and latent variable representations that allow for greater flexibility and interpretability. By breaking down larger datasets into smaller ""block"" components, these models can efficiently process vast quantities of data without sacrificing precision or accuracy. With applications ranging from natural language processing to image recognition, BTNN represents a promising advance in artificial intelligence research, capable of enabling breakthroughs across many fields. Keywords: block-term tensors, neural networks, attention mechanism, latent variables, natural language processing, image recognition.",1
"Biological data including gene expression data are generally high-dimensional and require efficient, generalizable, and scalable machine-learning methods to discover their complex nonlinear patterns. The recent advances in machine learning can be attributed to deep neural networks (DNNs), which excel in various tasks in terms of computer vision and natural language processing. However, standard DNNs are not appropriate for high-dimensional datasets generated in biology because they have many parameters, which in turn require many samples. In this paper, we propose a DNN-based, nonlinear feature selection method, called the feature selection network (FsNet), for high-dimensional and small number of sample data. Specifically, FsNet comprises a selection layer that selects features and a reconstruction layer that stabilizes the training. Because a large number of parameters in the selection and reconstruction layers can easily result in overfitting under a limited number of samples, we use two tiny networks to predict the large, virtual weight matrices of the selection and reconstruction layers. Experimental results on several real-world, high-dimensional biological datasets demonstrate the efficacy of the proposed method.",0
"Title: FsNet: Feature Selection Network on High-dimensional Biological Data  Abstract: This paper presents a new method for selecting features from high-dimensional biological data sets. The proposed approach uses a neural network architecture that learns which features are most important for predicting the target variable of interest. Unlike traditional feature selection methods that rely on heuristics or statistical measures, our method is trained end-to-end and can handle complex relationships among variables. We demonstrate the effectiveness of our method using simulation studies and real-world datasets, showing that FsNet outperforms other feature selection techniques while reducing computational cost and increasing interpretability. Our findings have important implications for bioinformatics researchers who must deal with large, complex datasets in their work.",1
"Transformers have become the dominant model in natural language processing, owing to their ability to pretrain on massive amounts of data, then transfer to smaller, more specific tasks via fine-tuning. The Vision Transformer was the first major attempt to apply a pure transformer model directly to images as input, demonstrating that as compared to convolutional networks, transformer-based architectures can achieve competitive results on benchmark classification tasks. However, the computational complexity of the attention operator means that we are limited to low-resolution inputs. For more complex tasks such as detection or segmentation, maintaining a high input resolution is crucial to ensure that models can properly identify and reflect fine details in their output. This naturally raises the question of whether or not transformer-based architectures such as the Vision Transformer are capable of performing tasks other than classification. In this paper, we determine that Vision Transformers can be used as a backbone by a common detection task head to produce competitive COCO results. The model that we propose, ViT-FRCNN, demonstrates several known properties associated with transformers, including large pretraining capacity and fast fine-tuning performance. We also investigate improvements over a standard detection backbone, including superior performance on out-of-domain images, better performance on large objects, and a lessened reliance on non-maximum suppression. We view ViT-FRCNN as an important stepping stone toward a pure-transformer solution of complex vision tasks such as object detection.",0
"This paper presents a new architecture that uses transformer networks to address challenges inherent in object detection tasks. We demonstrate how our novel approach outperforms traditional convolutional neural network (CNN) based architectures on benchmark datasets. Our model is trained using multi-scale features extracted from the last layer of pre-trained CNNs. Experimental results show significant improvements compared to state-of-the-art CNN-based methods. Additionally, we provide an ablation study to evaluate the impact of each component of our proposed method. In conclusion, this work advances the field of computer vision by introducing transformers as a promising alternative for object detection tasks.",1
"Modern Neural Networks are eminent in achieving state of the art performance on tasks under Computer Vision, Natural Language Processing and related verticals. However, they are notorious for their voracious memory and compute appetite which further obstructs their deployment on resource limited edge devices. In order to achieve edge deployment, researchers have developed pruning and quantization algorithms to compress such networks without compromising their efficacy. Such compression algorithms are broadly experimented on standalone CNN and RNN architectures while in this work, we present an unconventional end to end compression pipeline of a CNN-LSTM based Image Captioning model. The model is trained using VGG16 or ResNet50 as an encoder and an LSTM decoder on the flickr8k dataset. We then examine the effects of different compression architectures on the model and design a compression architecture that achieves a 73.1% reduction in model size, 71.3% reduction in inference time and a 7.7% increase in BLEU score as compared to its uncompressed counterpart.",0
"Title: Efficient CNN-LSTM Based Image Captioning Using Neural Network Compression Abstract In recent years, image captioning has become one of the most popular research topics in computer vision. One of the key challenges facing the field is making these models more efficient without compromising their accuracy. This paper proposes an approach that leverages Convolutional Neural Networks (CNN) along with Long Short Term Memory units (LSTM) for generating descriptions associated with images from large datasets. We use neural network compression techniques such as quantization and pruning to reduce computational complexity while maintaining accuracy. Our method achieves state-of-the-art performance on standard benchmarks like COCO and Flickr8K. Additionally, we demonstrate improved speed over other existing methods by up to two times. These results indicate that our approach can provide a fast yet accurate solution for automated image description generation, thus helping drive advances in numerous applications involving image understanding.",1
"Pre-trained language models have achieved state-of-the-art accuracies on various text classification tasks, e.g., sentiment analysis, natural language inference, and semantic textual similarity. However, the reliability of the fine-tuned text classifiers is an often underlooked performance criterion. For instance, one may desire a model that can detect out-of-distribution (OOD) samples (drawn far from training distribution) or be robust against domain shifts. We claim that one central obstacle to the reliability is the over-reliance of the model on a limited number of keywords, instead of looking at the whole context. In particular, we find that (a) OOD samples often contain in-distribution keywords, while (b) cross-domain samples may not always contain keywords; over-relying on the keywords can be problematic for both cases. In light of this observation, we propose a simple yet effective fine-tuning method, coined masked keyword regularization (MASKER), that facilitates context-based prediction. MASKER regularizes the model to reconstruct the keywords from the rest of the words and make low-confidence predictions without enough context. When applied to various pre-trained language models (e.g., BERT, RoBERTa, and ALBERT), we demonstrate that MASKER improves OOD detection and cross-domain generalization without degrading classification accuracy. Code is available at https://github.com/alinlab/MASKER.",0
"In recent years, natural language processing has seen significant advancements due to the incorporation of deep learning techniques like neural networks. However, training these models often requires large amounts of labeled data which can be expensive and time consuming to collect. One approach to tackle this issue is to use regularization methods that improve model performance by reducing overfitting on the limited amount of available data. Recent works have shown promising results using masking techniques such as WordDropout and synonym replacement to simulate missing or alternative word tokens during training. This paper presents a novel method called Masker, which extends previous work by utilizing both contextual information and domain knowledge to generate more appropriate replacements for keywords in text data. Experimental evaluations demonstrate the effectiveness of our proposed method compared to state-of-the-art baselines across multiple benchmark datasets. Our analysis shows that integrating external knowledge into the masking process leads to improved generalization performance, making the trained models better equipped to handle out-of-domain inputs. Overall, our findings highlight the potential benefits of integrating external knowledge sources for improving reliability in NLP applications.",1
"Image captioning transforms complex visual information into abstract natural language for representation, which can help computers understanding the world quickly. However, due to the complexity of the real environment, it needs to identify key objects and realize their connections, and further generate natural language. The whole process involves a visual understanding module and a language generation module, which brings more challenges to the design of deep neural networks than other tasks. Neural Architecture Search (NAS) has shown its important role in a variety of image recognition tasks. Besides, RNN plays an essential role in the image captioning task. We introduce a AutoCaption method to better design the decoder module of the image captioning where we use the NAS to design the decoder module called AutoRNN automatically. We use the reinforcement learning method based on shared parameters for automatic design the AutoRNN efficiently. The search space of the AutoCaption includes connections between the layers and the operations in layers both, and it can make AutoRNN express more architectures. In particular, RNN is equivalent to a subset of our search space. Experiments on the MSCOCO datasets show that our AutoCaption model can achieve better performance than traditional hand-design methods. Our AutoCaption obtains the best published CIDEr performance of 135.8% on COCO Karpathy test split. When further using ensemble technology, CIDEr is boosted up to 139.5%.",0
"As natural language processing techniques have advanced, image captioning has become an increasingly popular task that can provide valuable insights into image content without requiring explicit annotations. Existing approaches typically involve handcrafted feature extraction from images combined with recurrent neural networks (RNNs) for sequence modeling. In our work we propose using Neural Architecture Search (NAS), which automatically searches over a wide space of candidate architectures, allowing us to select a high-performing architecture tailored specifically to the image captioning problem. We evaluate our approach against state-of-the art methods on multiple datasets including COCO and Flickr8k and show significant improvements, indicating the potential benefits of utilizing NAS for designing neural network architectures in computer vision tasks.  If you want to increase your knowledge base and creative thinking ability , i think reading books would be better . When I read something new to me , my brain starts exploring connections and trying to find relationships between concepts etc",1
"Although Transformer models such as Google's BERT and OpenAI's GPT-3 are successful in many natural language processing tasks, training and deploying these models are costly and inefficient.Even if pre-trained models are used, deploying these models still remained a challenge due to their large size. Apart from deployment, these models take higher time during inference restricting user-friendliness. The main bottleneck is self-attention which uses quadratic time and space with respect to the sequence length. In order to reduce the quadratic time complexity of the self-attention mechanism, Linformer by Facebook's AI research team was introduced where they showed that the self-attention mechanism can be approximated by a low-rank matrix and exploiting this finding, a new method for self-attention with linear time and space complexity was proposed by them. In the Linformer, the time complexity depends on the projection mapping dimension which acts as a hyperparameter and affects the performance of the model, tuning this hyperparameter can be time-consuming. In this paper, I proposed an alternative method for self-attention with linear complexity in time and space and is independent of the projection mapping dimension. Since this method works for long sequences this can be used for images as well as audios.",0
"Title: Improving Self-Attention Mechanisms for Transformer Models through Linear Complexity Reduction Abstract In recent years, deep learning has seen significant advancements in natural language processing (NLP) tasks such as machine translation, text classification, and sentiment analysis, among others. This owes largely to the introduction of transformer models and their variants that have achieved state-of-the-art results on these NLP tasks. However, one major drawback of current transformer architectures is their high computational requirements, which makes them impractical for use on low-resource devices. To address this issue, we propose a modification to the popular attention mechanism used in transformers called “Linformer,” which reduces the quadratic time complexity of the self-attention operation to linear, while maintaining comparable performance. Our modifications involve replacing the softmax function, typically used for normalization, with a more efficient formulation based on Gaussian distributions. Experimental evaluation shows that our proposed model outperforms the original version of Linformer in several NLP benchmarks, including BLEU score, F1 measure, and accuracy. These promising results highlight the potential of our approach towards improving the scalability of transformer models without compromising their effectiveness. Keywords: transformer models, self-attention mechanisms, linear complexity reduction, Gaussian distribution, scalability.",1
