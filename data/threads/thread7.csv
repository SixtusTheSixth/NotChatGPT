"We propose a novel video inpainting algorithm that simultaneously hallucinates missing appearance and motion (optical flow) information, building upon the recent 'Deep Image Prior' (DIP) that exploits convolutional network architectures to enforce plausible texture in static images. In extending DIP to video we make two important contributions. First, we show that coherent video inpainting is possible without a priori training. We take a generative approach to inpainting based on internal (within-video) learning without reliance upon an external corpus of visual data to train a one-size-fits-all model for the large space of general videos. Second, we show that such a framework can jointly generate both appearance and flow, whilst exploiting these complementary modalities to ensure mutual consistency. We show that leveraging appearance statistics specific to each video achieves visually plausible results whilst handling the challenging problem of long-term consistency.",0
"Our proposed video inpainting algorithm is a fresh approach that addresses the simultaneous reconstruction of missing appearance and motion data (optical flow) by utilizing the 'Deep Image Prior' (DIP) methodology. DIP is a convolutional network architecture that ensures realistic texture in still images. Our innovation builds upon this by extending DIP to video and making two crucial contributions. Firstly, we demonstrate that coherent video inpainting is achievable without relying on pre-training. Our approach is generative, where we use internal learning within each video to fill in the gaps, instead of using a one-size-fits-all model trained on a vast dataset of visual data. Secondly, we show that our framework can produce both appearance and flow data simultaneously, utilizing these two modalities to ensure accuracy and consistency. By analyzing the appearance statistics of each video, our approach achieves credible results while overcoming the challenge of long-term consistency.",1
"Recently, it is increasingly popular to equip mobile RGB cameras with Time-of-Flight (ToF) sensors for active depth sensing. However, for off-the-shelf ToF sensors, one must tackle two problems in order to obtain high-quality depth with respect to the RGB camera, namely 1) online calibration and alignment; and 2) complicated error correction for ToF depth sensing. In this work, we propose a framework for jointly alignment and refinement via deep learning. First, a cross-modal optical flow between the RGB image and the ToF amplitude image is estimated for alignment. The aligned depth is then refined via an improved kernel predicting network that performs kernel normalization and applies the bias prior to the dynamic convolution. To enrich our data for end-to-end training, we have also synthesized a dataset using tools from computer graphics. Experimental results demonstrate the effectiveness of our approach, achieving state-of-the-art for ToF refinement.",0
"In recent times, it has become increasingly popular to equip mobile RGB cameras with Time-of-Flight (ToF) sensors for active depth sensing. However, when using off-the-shelf ToF sensors, obtaining high-quality depth with respect to the RGB camera requires addressing two problems - online calibration and alignment, and complicated error correction for ToF depth sensing. In this study, we present a framework that uses deep learning to jointly align and refine the data. The first step involves estimating a cross-modal optical flow between the RGB image and the ToF amplitude image for alignment. The aligned depth is then refined using an improved kernel predicting network that performs kernel normalization and applies the bias prior to the dynamic convolution. To enrich the data for end-to-end training, we have synthesized a dataset using computer graphics tools. Our experimental results demonstrate the effectiveness of our approach, which achieves state-of-the-art performance for ToF refinement.",1
"The goal of this study is to develop and analyze multimodal models for predicting experienced affective responses of viewers watching movie clips. We develop hybrid multimodal prediction models based on both the video and audio of the clips. For the video content, we hypothesize that both image content and motion are crucial features for evoked emotion prediction. To capture such information, we extract features from RGB frames and optical flow using pre-trained neural networks. For the audio model, we compute an enhanced set of low-level descriptors including intensity, loudness, cepstrum, linear predictor coefficients, pitch and voice quality. Both visual and audio features are then concatenated to create audio-visual features, which are used to predict the evoked emotion. To classify the movie clips into the corresponding affective response categories, we propose two approaches based on deep neural network models. The first one is based on fully connected layers without memory on the time component, the second incorporates the sequential dependency with a long short-term memory recurrent neural network (LSTM). We perform a thorough analysis of the importance of each feature set. Our experiments reveal that in our set-up, predicting emotions at each time step independently gives slightly better accuracy performance than with the LSTM. Interestingly, we also observe that the optical flow is more informative than the RGB in videos, and overall, models using audio features are more accurate than those based on video features when making the final prediction of evoked emotions.",0
"The aim of this research is to create and assess models that can predict the emotional responses of viewers while watching movie clips using multiple modes of input. We have developed hybrid models that utilize both audio and visual data from the clips. To achieve this, we have extracted features from RGB frames and optical flow using pre-trained neural networks to capture image content and motion for the video model. For the audio model, we have computed a set of low-level descriptors consisting of intensity, loudness, cepstrum, linear predictor coefficients, pitch, and voice quality. We have then combined these audio and visual features to generate audio-visual features that can predict the evoked emotions. To classify the movie clips into the corresponding affective response categories, we have proposed two approaches using deep neural network models. One approach is based on fully connected layers without memory on the time component, and the other incorporates the sequential dependency with a long short-term memory recurrent neural network (LSTM). In our experiments, we have analyzed the importance of each feature set and found that predicting emotions independently at each time step gives slightly better accuracy performance than with the LSTM. Furthermore, we have observed that optical flow is more informative than RGB in videos, and models that use audio features are more accurate than those based on video features when making the final prediction of evoked emotions.",1
"We focus on the word-level visual lipreading, which requires to decode the word from the speaker's video. Recently, many state-of-the-art visual lipreading methods explore the end-to-end trainable deep models, involving the use of 2D convolutional networks (e.g., ResNet) as the front-end visual feature extractor and the sequential model (e.g., Bi-LSTM or Bi-GRU) as the back-end. Although a deep 2D convolution neural network can provide informative image-based features, it ignores the temporal motion existing between the adjacent frames. In this work, we investigate the spatial-temporal capacity power of I3D (Inflated 3D ConvNet) for visual lipreading. We demonstrate that, after pre-trained on the large-scale video action recognition dataset (e.g., Kinetics), our models show a considerable improvement of performance on the task of lipreading. A comparison between a set of video model architectures and input data representation is also reported. Our extensive experiments on LRW shows that a two-stream I3D model with RGB video and optical flow as the inputs achieves the state-of-the-art performance.",0
"Our focus is on visual lipreading at the word level, which involves decoding the speaker's video to identify the spoken word. Recently, state-of-the-art visual lipreading techniques have used end-to-end trainable deep models, utilizing 2D convolutional networks (such as ResNet) as the visual feature extractor at the front-end and sequential models (such as Bi-LSTM or Bi-GRU) at the back-end. However, deep 2D convolution neural networks neglect the temporal motion between adjacent frames, even though they provide informative image-based features. In this study, we explore the spatial-temporal capacity of I3D (Inflated 3D ConvNet) for visual lipreading. Our pre-trained models on the large-scale video action recognition dataset (Kinetics) show significant improvements in lipreading performance. We also compare various video model architectures and input data representations. Our extensive experiments on LRW demonstrate that a two-stream I3D model with RGB video and optical flow inputs achieves superior performance, making it the new state-of-the-art.",1
"Predicting depth from a monocular video sequence is an important task for autonomous driving. Although it has advanced considerably in the past few years, recent methods based on convolutional neural networks (CNNs) discard temporal coherence in the video sequence and estimate depth independently for each frame, which often leads to undesired inconsistent results over time. To address this problem, we propose to memorize temporal consistency in the video sequence, and leverage it for the task of depth prediction. To this end, we introduce a two-stream CNN with a flow-guided memory module, where each stream encodes visual and temporal features, respectively. The memory module, implemented using convolutional gated recurrent units (ConvGRUs), inputs visual and temporal features sequentially together with optical flow tailored to our task. It memorizes trajectories of individual features selectively and propagates spatial information over time, enforcing a long-term temporal consistency to prediction results. We evaluate our method on the KITTI benchmark dataset in terms of depth prediction accuracy, temporal consistency and runtime, and achieve a new state of the art. We also provide an extensive experimental analysis, clearly demonstrating the effectiveness of our approach to memorizing temporal consistency for depth prediction.",0
"Autonomous driving heavily relies on accurately predicting depth from a monocular video sequence. Despite significant advancements in recent years, current methods utilizing convolutional neural networks (CNNs) discard temporal coherence and estimate depth independently for each frame, leading to inconsistent results over time. To overcome this issue, we propose a two-stream CNN with a flow-guided memory module that memorizes temporal consistency in the video sequence to aid in depth prediction. The memory module, comprising convolutional gated recurrent units (ConvGRUs), selectively memorizes trajectories of individual features and propagates spatial information over time, promoting long-term temporal consistency. Our approach achieves state-of-the-art results in depth prediction accuracy, temporal consistency, and runtime on the KITTI benchmark dataset. Our extensive experimental analysis confirms the effectiveness of our method in memorizing temporal consistency for depth prediction.",1
"We present a method for decomposing the 3D scene flow observed from a moving stereo rig into stationary scene elements and dynamic object motion. Our unsupervised learning framework jointly reasons about the camera motion, optical flow, and 3D motion of moving objects. Three cooperating networks predict stereo matching, camera motion, and residual flow, which represents the flow component due to object motion and not from camera motion. Based on rigid projective geometry, the estimated stereo depth is used to guide the camera motion estimation, and the depth and camera motion are used to guide the residual flow estimation. We also explicitly estimate the 3D scene flow of dynamic objects based on the residual flow and scene depth. Experiments on the KITTI dataset demonstrate the effectiveness of our approach and show that our method outperforms other state-of-the-art algorithms on the optical flow and visual odometry tasks.",0
"Our approach involves breaking down the 3D scene flow captured by a moving stereo rig into two components: stationary scene elements and dynamic object motion. This is achieved through an unsupervised learning framework that considers camera motion, optical flow, and 3D motion of moving objects. Three networks collaborate to predict stereo matching, camera motion, and residual flow, which captures flow due to object motion and not camera motion. Our method uses rigid projective geometry to guide the camera motion estimation based on the estimated stereo depth, while the residual flow estimation is guided by depth and camera motion. Additionally, we explicitly estimate the 3D scene flow of dynamic objects using residual flow and scene depth. We conducted experiments on the KITTI dataset, which demonstrate the superiority of our approach over other state-of-the-art algorithms on optical flow and visual odometry tasks.",1
"Dashboard cameras capture a tremendous amount of driving scene video each day. These videos are purposefully coupled with vehicle sensing data, such as from the speedometer and inertial sensors, providing an additional sensing modality for free. In this work, we leverage the large-scale unlabeled yet naturally paired data for visual representation learning in the driving scenario. A representation is learned in an end-to-end self-supervised framework for predicting dense optical flow from a single frame with paired sensing data. We postulate that success on this task requires the network to learn semantic and geometric knowledge in the ego-centric view. For example, forecasting a future view to be seen from a moving vehicle requires an understanding of scene depth, scale, and movement of objects. We demonstrate that our learned representation can benefit other tasks that require detailed scene understanding and outperforms competing unsupervised representations on semantic segmentation.",0
"Every day, dashboard cameras capture an enormous amount of driving scene footage, which is intentionally combined with vehicle sensing data, such as from the speedometer and inertial sensors, to create an additional sensing method for free. Our research takes advantage of this vast amount of unlabeled yet naturally paired data for visual representation learning in driving scenarios. We teach the network to predict dense optical flow from a single frame with paired sensing data by using an end-to-end self-supervised framework. In order to succeed in this task, we assume that the network must learn semantic and geometric knowledge in the ego-centric view. For instance, predicting a future view from a moving vehicle necessitates an understanding of scene depth, scale, and object movement. Our study demonstrates that our learned representation can benefit other tasks that require detailed scene comprehension and outperforms competing unsupervised representations on semantic segmentation.",1
"We propose a learning-based method that solves monocular stereo and can be extended to fuse depth information from multiple target frames. Given two unconstrained images from a monocular camera with known intrinsic calibration, our network estimates relative camera poses and the depth map of the source image. The core contribution of the proposed method is threefold. First, a network is tailored for static scenes that jointly estimates the optical flow and camera motion. By the joint estimation, the optical flow search space is gradually reduced resulting in an efficient and accurate flow estimation. Second, a novel triangulation layer is proposed to encode the estimated optical flow and camera motion while avoiding common numerical issues caused by epipolar. Third, beyond two-view depth estimation, we further extend the above networks to fuse depth information from multiple target images and estimate the depth map of the source image. To further benefit the research community, we introduce tools to generate photorealistic structure-from-motion datasets such that deep networks can be well trained and evaluated. The proposed method is compared with previous methods and achieves state-of-the-art results within less time. Images from real-world applications and Google Earth are used to demonstrate the generalization ability of the method.",0
"Our proposal introduces a learning-based method for solving monocular stereo that can be expanded to incorporate depth information from multiple target frames. The method involves using two unconstrained images from a monocular camera with known intrinsic calibration, where a network estimates the relative camera poses and depth map of the source image. The proposed method has three core contributions. Firstly, a network specifically designed for static scenes jointly estimates optical flow and camera motion, reducing the search space and improving flow estimation. Secondly, a novel triangulation layer encodes the estimated optical flow and camera motion while avoiding numerical issues caused by epipolar. Thirdly, the network is extended beyond two-view depth estimation to fuse depth information from multiple target images and estimate the depth map of the source image. Additionally, we provide tools to generate photorealistic structure-from-motion datasets for training and evaluation purposes. Our method outperforms previous methods and achieves state-of-the-art results in less time. The generalization ability of the method is demonstrated using images from real-world applications and Google Earth.",1
"We present GLNet, a self-supervised framework for learning depth, optical flow, camera pose and intrinsic parameters from monocular video - addressing the difficulty of acquiring realistic ground-truth for such tasks. We propose three contributions: 1) we design new loss functions that capture multiple geometric constraints (eg. epipolar geometry) as well as an adaptive photometric loss that supports multiple moving objects, rigid and non-rigid, 2) we extend the model such that it predicts camera intrinsics, making it applicable to uncalibrated video, and 3) we propose several online refinement strategies that rely on the symmetry of our self-supervised loss in training and testing, in particular optimizing model parameters and/or the output of different tasks, thus leveraging their mutual interactions. The idea of jointly optimizing the system output, under all geometric and photometric constraints can be viewed as a dense generalization of classical bundle adjustment. We demonstrate the effectiveness of our method on KITTI and Cityscapes, where we outperform previous self-supervised approaches on multiple tasks. We also show good generalization for transfer learning in YouTube videos.",0
"GLNet is a framework that uses self-supervision to learn depth, optical flow, camera pose, and intrinsic parameters from monocular video. This approach addresses the challenge of obtaining realistic ground-truth data for these tasks. Our framework has three key contributions. Firstly, we introduce new loss functions that capture multiple geometric constraints, such as epipolar geometry, and an adaptive photometric loss that accommodates multiple moving objects, rigid and non-rigid. Secondly, we extend the model to predict camera intrinsics, making it suitable for uncalibrated video. Thirdly, we propose several online refinement strategies that optimize model parameters and/or the output of different tasks by leveraging their mutual interactions. This dense optimization approach can be seen as a generalization of classical bundle adjustment. We demonstrate the effectiveness of GLNet on KITTI and Cityscapes datasets, outperforming previous self-supervised methods on multiple tasks. Our framework also shows good generalization for transfer learning in YouTube videos.",1
"Dense prediction tasks typically employ encoder-decoder architectures, but the prevalent convolutions in the decoder are not image-adaptive and can lead to boundary artifacts. Different generalized convolution operations have been introduced to counteract this. We go beyond these by leveraging guidance data to redefine their inherent notion of proximity. Our proposed network layer builds on the permutohedral lattice, which performs sparse convolutions in a high-dimensional space allowing for powerful non-local operations despite small filters. Multiple features with different characteristics span this permutohedral space. In contrast to prior work, we learn these features in a task-specific manner by generalizing the basic permutohedral operations to learnt feature representations. As the resulting objective is complex, a carefully designed framework and learning procedure are introduced, yielding rich feature embeddings in practice. We demonstrate the general applicability of our approach in different joint upsampling tasks. When adding our network layer to state-of-the-art networks for optical flow and semantic segmentation, boundary artifacts are removed and the accuracy is improved.",0
"Tasks requiring dense prediction often use encoder-decoder architectures, but the common convolutions in the decoder may not adapt to images and can create boundary artifacts. To address this, various generalized convolution operations have been introduced. However, we propose a novel approach that uses guidance data to redefine the inherent proximity concept. Our network layer builds on the permutohedral lattice, which performs sparse convolutions in a high-dimensional space and enables non-local operations despite using small filters. Multiple features with distinct characteristics are present in this permutohedral space. Unlike earlier work, we learn these features in a task-specific manner by generalizing the basic permutohedral operations to learned feature representations. Since the resulting objective is complex, we introduce a well-designed framework and learning procedure to produce rich feature embeddings in practice. Our approach is generally applicable in various joint upsampling tasks, and when added to state-of-the-art networks for optical flow and semantic segmentation, it removes boundary artifacts and improves accuracy.",1
"Prediction and interpolation for long-range video data involves the complex task of modeling motion trajectories for each visible object, occlusions and dis-occlusions, as well as appearance changes due to viewpoint and lighting. Optical flow based techniques generalize but are suitable only for short temporal ranges. Many methods opt to project the video frames to a low dimensional latent space, achieving long-range predictions. However, these latent representations are often non-interpretable, and therefore difficult to manipulate. This work poses video prediction and interpolation as unsupervised latent structure inference followed by a temporal prediction in this latent space. The latent representations capture foreground semantics without explicit supervision such as keypoints or poses. Further, as each landmark can be mapped to a coordinate indicating where a semantic part is positioned, we can reliably interpolate within the coordinate domain to achieve predictable motion interpolation. Given an image decoder capable of mapping these landmarks back to the image domain, we are able to achieve high-quality long-range video interpolation and extrapolation by operating on the landmark representation space.",0
"The complex task of predicting and interpolating long-range video data involves modeling motion trajectories for visible objects, accounting for occlusions, dis-occlusions, and changes in appearance due to lighting and viewpoint. While optical flow techniques can generalize, they are only suitable for short temporal ranges. Some methods use a low dimensional latent space to achieve long-range predictions, but these representations are often difficult to interpret and manipulate. This work suggests an approach that involves unsupervised latent structure inference followed by temporal prediction in the latent space. The latent representations capture foreground semantics without explicit supervision, and mapping landmarks to coordinates allows for reliable interpolation in the coordinate domain. By using an image decoder to map landmarks back to the image domain, high-quality long-range video interpolation and extrapolation can be achieved by operating in the landmark representation space.",1
"Motion estimation (ME) and motion compensation (MC) have been widely used for classical video frame interpolation systems over the past decades. Recently, a number of data-driven frame interpolation methods based on convolutional neural networks have been proposed. However, existing learning based methods typically estimate either flow or compensation kernels, thereby limiting performance on both computational efficiency and interpolation accuracy. In this work, we propose a motion estimation and compensation driven neural network for video frame interpolation. A novel adaptive warping layer is developed to integrate both optical flow and interpolation kernels to synthesize target frame pixels. This layer is fully differentiable such that both the flow and kernel estimation networks can be optimized jointly. The proposed model benefits from the advantages of motion estimation and compensation methods without using hand-crafted features. Compared to existing methods, our approach is computationally efficient and able to generate more visually appealing results. Furthermore, the proposed MEMC-Net can be seamlessly adapted to several video enhancement tasks, e.g., super-resolution, denoising, and deblocking. Extensive quantitative and qualitative evaluations demonstrate that the proposed method performs favorably against the state-of-the-art video frame interpolation and enhancement algorithms on a wide range of datasets.",0
"Classical video frame interpolation systems have relied on motion estimation (ME) and motion compensation (MC) for many years. Recently, convolutional neural networks have been used for data-driven frame interpolation methods. However, these methods usually estimate either flow or compensation kernels, which reduces both computational efficiency and interpolation accuracy. To address this issue, we propose a motion estimation and compensation driven neural network for video frame interpolation. Our approach utilizes an adaptive warping layer that integrates optical flow and interpolation kernels to synthesize target frame pixels. This layer is fully differentiable, allowing the flow and kernel estimation networks to be optimized jointly. Our model does not require hand-crafted features, making it computationally efficient and capable of producing visually appealing results. Additionally, the proposed MEMC-Net can be easily adapted to other video enhancement tasks such as super-resolution, denoising, and deblocking. Extensive evaluations show that our method outperforms state-of-the-art video frame interpolation and enhancement algorithms on various datasets.",1
"Pedestrian action recognition and intention prediction is one of the core issues in the field of autonomous driving. In this research field, action recognition is one of the key technologies. A large number of scholars have done a lot of work to im-prove the accuracy of the algorithm for the task. However, there are relatively few studies and improvements in the computational complexity of algorithms and sys-tem real-time. In the autonomous driving application scenario, the real-time per-formance and ultra-low latency of the algorithm are extremely important evalua-tion indicators, which are directly related to the availability and safety of the au-tonomous driving system. To this end, we construct a bypass enhanced RGB flow model, which combines the previous two-branch algorithm to extract RGB feature information and optical flow feature information respectively. In the train-ing phase, the two branches are merged by distillation method, and the bypass enhancement is combined in the inference phase to ensure accuracy. The real-time behavior of the behavior recognition algorithm is significantly improved on the premise that the accuracy does not decrease. Experiments confirm the superiority and effectiveness of our algorithm.",0
"Recognizing and predicting pedestrian actions is a crucial aspect of autonomous driving. Action recognition is a key technology in this research field, with many scholars striving to enhance the accuracy of algorithms. However, there is limited focus on improving the computational complexity of algorithms and achieving real-time system performance. In autonomous driving scenarios, the real-time performance and ultra-low latency of algorithms are crucial indicators for evaluating availability and safety. To address this, we developed a bypass enhanced RGB flow model that combines two-branch algorithms for extracting RGB and optical flow feature information. By merging the branches using a distillation method during training and implementing bypass enhancement during inference, we ensured accuracy while significantly improving real-time behavior recognition. Our experiments confirmed the effectiveness and superiority of our algorithm.",1
"Due to better video quality and higher frame rate, the performance of multiple object tracking issues has been greatly improved in recent years. However, in real application scenarios, camera motion and noisy per frame detection results degrade the performance of trackers significantly. High-speed and high-quality multiple object trackers are still in urgent demand. In this paper, we propose a new multiple object tracker following the popular tracking-by-detection scheme. We tackle the camera motion problem with an optical flow network and utilize an auxiliary tracker to deal with the missing detection problem. Besides, we use both the appearance and motion information to improve the matching quality. The experimental results on the VisDrone-MOT dataset show that our approach can improve the performance of multiple object tracking significantly while achieving a high efficiency.",0
"Recent years have witnessed significant improvements in the performance of multiple object tracking issues, thanks to better video quality and higher frame rate. However, the real application scenarios are plagued with camera motion and noisy per frame detection results, which considerably degrade the performance of trackers. Consequently, there is a pressing need for high-speed and high-quality multiple object trackers. To address this challenge, the present paper proposes a new multiple object tracker that adopts the popular tracking-by-detection scheme. The proposed tracker employs an optical flow network to tackle camera motion and an auxiliary tracker to handle missing detection cases. Furthermore, both appearance and motion information are utilized to enhance the matching quality. The experimental evaluation on the VisDrone-MOT dataset demonstrates that our approach significantly improves the performance of multiple object tracking while ensuring high efficiency.",1
"This paper presents a novel obstacle avoidance system for road robots equipped with RGB-D sensor that captures scenes of its way forward. The purpose of the system is to have road robots move around autonomously and constantly without any collision even with small obstacles, which are often missed by existing solutions. For each input RGB-D image, the system uses a new two-stage semantic segmentation network followed by the morphological processing to generate the accurate semantic map containing road and obstacles. Based on the map, the local path planning is applied to avoid possible collision. Additionally, optical flow supervision and motion blurring augmented training scheme is applied to improve temporal consistency between adjacent frames and overcome the disturbance caused by camera shake. Various experiments are conducted to show that the proposed architecture obtains high performance both in indoor and outdoor scenarios.",0
"In this article, a unique system for avoiding obstacles is presented for road robots that utilize an RGB-D sensor to capture images of their surroundings ahead. The aim of this system is to enable autonomous movement of road robots, ensuring that they can move continuously without any collisions, even with small obstacles that may be missed by existing solutions. To achieve this, the system utilizes a new two-stage semantic segmentation network and morphological processing to generate an accurate semantic map containing information about the road and obstacles. This map is then used to plan a local path and avoid potential collisions. To improve temporal consistency between frames and overcome the effects of camera shake, an optical flow supervision and motion blurring augmented training scheme is also applied. The proposed architecture is shown to perform well in both indoor and outdoor scenarios through various experiments.",1
"Detecting action units (AUs) on human faces is challenging because various AUs make subtle facial appearance change over various regions at different scales. Current works have attempted to recognize AUs by emphasizing important regions. However, the incorporation of expert prior knowledge into region definition remains under-exploited, and current AU detection approaches do not use regional convolutional neural networks (R-CNN) with expert prior knowledge to directly focus on AU-related regions adaptively. By incorporating expert prior knowledge, we propose a novel R-CNN based model named AU R-CNN. The proposed solution offers two main contributions: (1) AU R-CNN directly observes different facial regions, where various AUs are located. Specifically, we define an AU partition rule which encodes the expert prior knowledge into the region definition and RoI-level label definition. This design produces considerably better detection performance than existing approaches. (2) We integrate various dynamic models (including convolutional long short-term memory, two stream network, conditional random field, and temporal action localization network) into AU R-CNN and then investigate and analyze the reason behind the performance of dynamic models. Experiment results demonstrate that \textit{only} static RGB image information and no optical flow-based AU R-CNN surpasses the one fused with dynamic models. AU R-CNN is also superior to traditional CNNs that use the same backbone on varying image resolutions. State-of-the-art recognition performance of AU detection is achieved. The complete network is end-to-end trainable. Experiments on BP4D and DISFA datasets show the effectiveness of our approach. The implementation code is available online.",0
"Detecting action units (AUs) on human faces poses a challenge due to the subtle changes that occur over different regions and scales. Previous attempts at AU recognition have focused on identifying important regions, but incorporating expert prior knowledge into region definition has not been fully explored. Current AU detection methods also fail to utilize regional convolutional neural networks (R-CNN) to adaptively target AU-related regions. To address these limitations, we introduce a novel R-CNN model called AU R-CNN that incorporates expert prior knowledge and offers two key contributions. Firstly, AU R-CNN directly observes facial regions where AUs are located using an AU partition rule that encodes expert prior knowledge into region and RoI-level label definitions. This design achieves better detection performance than existing approaches. Secondly, we integrate various dynamic models into AU R-CNN and analyze the reason behind their performance. Experiments show that AU R-CNN with only static RGB image information surpasses the one fused with dynamic models, and achieves state-of-the-art recognition performance. Our approach is end-to-end trainable and effective on BP4D and DISFA datasets. Implementation code is available online.",1
"We present a 3D Convolutional Neural Networks (CNNs) based single shot detector for spatial-temporal action detection tasks. Our model includes: (1) two short-term appearance and motion streams, with single RGB and optical flow image input separately, in order to capture the spatial and temporal information for the current frame; (2) two long-term 3D ConvNet based stream, working on sequences of continuous RGB and optical flow images to capture the context from past frames. Our model achieves strong performance for action detection in video and can be easily integrated into any current two-stream action detection methods. We report a frame-mAP of 71.30% on the challenging UCF101-24 actions dataset, achieving the state-of-the-art result of the one-stage methods. To the best of our knowledge, our work is the first system that combined 3D CNN and SSD in action detection tasks.",0
"We have developed a single shot detector for spatial-temporal action detection tasks using 3D Convolutional Neural Networks (CNNs). Our model comprises two short-term appearance and motion streams, which capture spatial and temporal information for the current frame, and two long-term 3D ConvNet based streams that operate on sequences of continuous RGB and optical flow images to capture context from previous frames. Our model performs well in action detection in videos and can be easily integrated into existing two-stream action detection approaches. We achieved a frame-mAP of 71.30% on the challenging UCF101-24 actions dataset, which is the state-of-the-art result for one-stage methods. Our work is the first to combine 3D CNN and SSD in action detection tasks.",1
"In the recent year, state-of-the-art for facial micro-expression recognition have been significantly advanced by deep neural networks. The robustness of deep learning has yielded promising performance beyond that of traditional handcrafted approaches. Most works in literature emphasized on increasing the depth of networks and employing highly complex objective functions to learn more features. In this paper, we design a Shallow Triple Stream Three-dimensional CNN (STSTNet) that is computationally light whilst capable of extracting discriminative high level features and details of micro-expressions. The network learns from three optical flow features (i.e., optical strain, horizontal and vertical optical flow fields) computed based on the onset and apex frames of each video. Our experimental results demonstrate the effectiveness of the proposed STSTNet, which obtained an unweighted average recall rate of 0.7605 and unweighted F1-score of 0.7353 on the composite database consisting of 442 samples from the SMIC, CASME II and SAMM databases.",0
"Deep neural networks have significantly advanced the state-of-the-art for facial micro-expression recognition in recent years, surpassing traditional handcrafted approaches in terms of performance. Previous studies have focused on increasing network depth and using complex objective functions to learn more features. However, we propose a computationally light Shallow Triple Stream Three-dimensional CNN (STSTNet) that can extract discriminative high-level features and micro-expression details. Our network utilizes three optical flow features (optical strain, horizontal and vertical optical flow fields) based on onset and apex video frames. Our experimental results demonstrate the effectiveness of STSTNet, achieving an unweighted average recall rate of 0.7605 and unweighted F1-score of 0.7353 on a composite database of 442 samples from SMIC, CASME II, and SAMM databases.",1
"Avoiding bottleneck situations in crowds is critical for the safety and comfort of people at large events or in public transportation. Based on the work of Lagrangian motion analysis we propose a novel video-based bottleneckdetector by identifying characteristic stowage patterns in crowd-movements captured by optical flow fields. The Lagrangian framework allows to assess complex timedependent crowd-motion dynamics at large temporal scales near the bottleneck by two dimensional Lagrangian fields. In particular we propose long-term temporal filtered Finite Time Lyapunov Exponents (FTLE) fields that provide towards a more global segmentation of the crowd movements and allows to capture its deformations when a crowd is passing a bottleneck. Finally, these deformations are used for an automatic spatio-temporal detection of such situations. The performance of the proposed approach is shown in extensive evaluations on the existing J\""ulich and AGORASET datasets, that we have updated with ground truth data for spatio-temporal bottleneck analysis.",0
"Ensuring the safety and comfort of individuals in large gatherings or public transportation requires the avoidance of bottleneck scenarios. To address this, we suggest a new bottleneck detection method that utilizes Lagrangian motion analysis to identify distinct stowage patterns in crowd movements captured by optical flow fields. This approach enables the assessment of complex time-dependent crowd-motion dynamics at large temporal scales near the bottleneck through two-dimensional Lagrangian fields. We suggest using long-term temporal filtered Finite Time Lyapunov Exponents (FTLE) fields to achieve a more comprehensive segmentation of the crowd's movements and to capture its deformations as it passes through a bottleneck. These deformations are then used for an automatic spatio-temporal detection of such situations. We demonstrate the effectiveness of this method by conducting extensive evaluations on the existing J""ulich and AGORASET datasets, which we supplemented with ground truth data for spatio-temporal bottleneck analysis.",1
"Event cameras are vision sensors that record asynchronous streams of per-pixel brightness changes, referred to as ""events"". They have appealing advantages over frame-based cameras for computer vision, including high temporal resolution, high dynamic range, and no motion blur. Due to the sparse, non-uniform spatiotemporal layout of the event signal, pattern recognition algorithms typically aggregate events into a grid-based representation and subsequently process it by a standard vision pipeline, e.g., Convolutional Neural Network (CNN). In this work, we introduce a general framework to convert event streams into grid-based representations through a sequence of differentiable operations. Our framework comes with two main advantages: (i) allows learning the input event representation together with the task dedicated network in an end to end manner, and (ii) lays out a taxonomy that unifies the majority of extant event representations in the literature and identifies novel ones. Empirically, we show that our approach to learning the event representation end-to-end yields an improvement of approximately 12% on optical flow estimation and object recognition over state-of-the-art methods.",0
"Event cameras are sensors that capture asynchronous streams of changes in brightness on a per-pixel basis, which are called ""events"". They offer several advantages over traditional frame-based cameras for computer vision, such as a high dynamic range, high temporal resolution, and no motion blur. However, due to the sparse and non-uniform nature of the event signal, pattern recognition algorithms usually group events into a grid-based format and process them using a standard vision pipeline, such as a Convolutional Neural Network (CNN). This study presents a general framework that can convert event streams into grid-based representations using a sequence of differentiable operations. The framework has two main benefits: (i) it enables learning the input event representation and dedicated network for a task in an end-to-end manner, and (ii) it provides a taxonomy that unifies existing event representations in the literature and identifies new ones. The study demonstrates that the proposed approach for learning the event representation end-to-end yields a 12% improvement in optical flow estimation and object recognition over state-of-the-art methods.",1
"Video objection detection (VID) has been a rising research direction in recent years. A central issue of VID is the appearance degradation of video frames caused by fast motion. This problem is essentially ill-posed for a single frame. Therefore, aggregating features from other frames becomes a natural choice. Existing methods rely heavily on optical flow or recurrent neural networks for feature aggregation. However, these methods emphasize more on the temporally nearby frames. In this work, we argue that aggregating features in the full-sequence level will lead to more discriminative and robust features for video object detection. To achieve this goal, we devise a novel Sequence Level Semantics Aggregation (SELSA) module. We further demonstrate the close relationship between the proposed method and the classic spectral clustering method, providing a novel view for understanding the VID problem. We test the proposed method on the ImageNet VID and the EPIC KITCHENS dataset and achieve new state-of-the-art results. Our method does not need complicated postprocessing methods such as Seq-NMS or Tubelet rescoring, which keeps the pipeline simple and clean.",0
"Recently, Video objection detection (VID) has become a popular research area. One of the main challenges in VID is the deterioration of video frames due to fast motion, which is a complex problem for a single frame. Therefore, it is natural to aggregate features from other frames. Existing methods rely heavily on optical flow or recurrent neural networks, which focus on nearby frames. However, in this study, we argue that aggregating features at the full-sequence level will result in more distinctive and durable features for video object detection. To achieve this, we introduce a novel Sequence Level Semantics Aggregation (SELSA) module. We also show a new perspective for understanding the VID problem by highlighting the relationship between our approach and the classic spectral clustering method. Our proposed method achieves state-of-the-art results on the ImageNet VID and the EPIC KITCHENS dataset without the need for complicated postprocessing techniques such as Seq-NMS or Tubelet rescoring, keeping the pipeline straightforward and concise.",1
"As a milestone for video object segmentation, one-shot video object segmentation (OSVOS) has achieved a large margin compared to the conventional optical-flow based methods regarding to the segmentation accuracy. Its excellent performance mainly benefit from the three-step training mechanism, that are: (1) acquiring object features on the base dataset (i.e. ImageNet), (2) training the parent network on the training set of the target dataset (i.e. DAVIS-2016) to be capable of differentiating the object of interest from the background. (3) online fine-tuning the interested object on the first frame of the target test set to overfit its appearance, then the model can be utilized to segment the same object in the rest frames of that video. In this paper, we argue that for the step (2), OSVOS has the limitation to 'overemphasize' the generic semantic object information while 'dilute' the instance cues of the object(s), which largely block the whole training process. Through adding a common module, video loss, which we formulate with various forms of constraints (including weighted BCE loss, high-dimensional triplet loss, as well as a novel mixed instance-aware video loss), to train the parent network in the step (2), the network is then better prepared for the step (3), i.e. online fine-tuning on the target instance. Through extensive experiments using different network structures as the backbone, we show that the proposed video loss module can improve the segmentation performance significantly, compared to that of OSVOS. Meanwhile, since video loss is a common module, it can be generalized to other fine-tuning based methods and similar vision tasks such as depth estimation and saliency detection.",0
"The one-shot video object segmentation (OSVOS) method has surpassed conventional optical-flow based techniques in segmentation accuracy. This is mainly due to its three-step training process: (1) acquiring object features from the base dataset ImageNet, (2) training the parent network on the target dataset DAVIS-2016 to differentiate the object of interest from the background, and (3) online fine-tuning of the interested object on the first frame of the target test set to overfit its appearance. However, we argue that step (2) of OSVOS overemphasizes generic semantic object information, diluting the instance cues of the object(s), which hinders the training process. To overcome this, we propose adding a video loss module with various constraints, such as weighted BCE loss, high-dimensional triplet loss, and a novel mixed instance-aware video loss, to train the parent network in step (2). This prepares the network for step (3) and significantly improves segmentation performance compared to OSVOS. Moreover, video loss is a common module that can be applied to other fine-tuning based methods and similar vision tasks like depth estimation and saliency detection.",1
"This draft summarizes some basics about geometric computer vision needed to implement efficient computer vision algorithms for applications that use measurements from at least one digital camera mounted on a moving platform with a special focus on automotive applications processing image streams taken from cameras mounted on a car. Our intention is twofold: On the one hand, we would like to introduce well-known basic geometric relations in a compact way that can also be found in lecture books about geometric computer vision like [1, 2]. On the other hand, we would like to share some experience about subtleties that should be taken into account in order to set up quite simple but robust and fast vision algorithms that are able to run in real time. We added a conglomeration of literature, we found to be relevant when implementing basic algorithms like optical flow, visual odometry and structure from motion. The reader should get some feeling about how the estimates of these algorithms are interrelated, which parts of the algorithms are critical in terms of robustness and what kind of additional assumptions can be useful to constrain the solution space of the underlying usually non-convex optimization problems.",0
"The aim of this document is to provide an overview of the fundamentals of geometric computer vision necessary for creating efficient computer vision algorithms for applications utilizing measurements from one or more digital cameras mounted on a moving platform, with a particular emphasis on processing image streams captured by cameras installed on vehicles. Our objective is two-fold: firstly, we seek to present an abridged version of commonly known geometric relationships that are also featured in textbooks on geometric computer vision, such as [1, 2]. Secondly, we aim to share our insights on the intricacies that must be taken into consideration when devising simple yet robust and quick vision algorithms that can operate in real-time. We have included a collection of relevant literature that we came across while implementing elementary algorithms like optical flow, visual odometry, and structure from motion. Our readers will gain an understanding of how these algorithms' estimations are interconnected, the critical aspects of the algorithms' robustness, and the additional suppositions that can be advantageous in limiting the solution space of the typically non-convex optimization problems at their core.",1
"In this paper, we revive the use of old-fashioned handcrafted video representations for action recognition and put new life into these techniques via a CNN-based hallucination step. Despite of the use of RGB and optical flow frames, the I3D model (amongst others) thrives on combining its output with the Improved Dense Trajectory (IDT) and extracted with its low-level video descriptors encoded via Bag-of-Words (BoW) and Fisher Vectors (FV). Such a fusion of CNNs and handcrafted representations is time-consuming due to pre-processing, descriptor extraction, encoding and tuning parameters. Thus, we propose an end-to-end trainable network with streams which learn the IDT-based BoW/FV representations at the training stage and are simple to integrate with the I3D model. Specifically, each stream takes I3D feature maps ahead of the last 1D conv. layer and learns to `translate' these maps to BoW/FV representations. Thus, our model can hallucinate and use such synthesized BoW/FV representations at the testing stage. We show that even features of the entire I3D optical flow stream can be hallucinated thus simplifying the pipeline. Our model saves 20-55h of computations and yields state-of-the-art results on four publicly available datasets.",0
"This paper proposes the revival of old-fashioned handcrafted video representations for action recognition, utilizing a CNN-based hallucination step to enhance these techniques. The I3D model, which combines RGB and optical flow frames, integrates its output with Improved Dense Trajectory (IDT) and low-level video descriptors encoded via Bag-of-Words (BoW) and Fisher Vectors (FV) to achieve success. However, this fusion of CNNs and handcrafted representations is time-consuming due to pre-processing, descriptor extraction, encoding, and tuning parameters. To address this issue, the authors suggest an end-to-end trainable network with streams that learn the IDT-based BoW/FV representations during the training stage, making it simple to integrate with the I3D model. The proposed model can hallucinate and use synthesized BoW/FV representations during the testing stage, saving 20-55h of computations and achieving state-of-the-art results on four publicly available datasets. Additionally, the model can hallucinate even the features of the entire I3D optical flow stream, simplifying the pipeline.",1
"Transferring image-based object detectors to the domain of videos remains a challenging problem. Previous efforts mostly exploit optical flow to propagate features across frames, aiming to achieve a good trade-off between accuracy and efficiency. However, introducing an extra model to estimate optical flow can significantly increase the overall model size. The gap between optical flow and high-level features can also hinder it from establishing spatial correspondence accurately. Instead of relying on optical flow, this paper proposes a novel module called Progressive Sparse Local Attention (PSLA), which establishes the spatial correspondence between features across frames in a local region with progressively sparser stride and uses the correspondence to propagate features. Based on PSLA, Recursive Feature Updating (RFU) and Dense Feature Transforming (DenseFT) are proposed to model temporal appearance and enrich feature representation respectively in a novel video object detection framework. Experiments on ImageNet VID show that our method achieves the best accuracy compared to existing methods with smaller model size and acceptable runtime speed.",0
"The task of transferring object detectors from images to videos remains a challenging endeavor. Previous attempts have primarily relied on optical flow to disseminate features across frames, with the aim of striking a balance between accuracy and efficiency. However, including an additional model to estimate optical flow can significantly augment the overall model size, and the disparity between optical flow and high-level features may impede accurate establishment of spatial correspondence. Rather than relying on optical flow, this study proposes a fresh module called Progressive Sparse Local Attention (PSLA), which establishes spatial correspondence between features across frames in a local area with progressively sparser stride and utilizes the correspondence to disseminate features. Recursive Feature Updating (RFU) and Dense Feature Transforming (DenseFT) are introduced based on PSLA to model temporal appearance and enhance feature representation, respectively, in a new video object detection framework. Experiments conducted on ImageNet VID demonstrate that our method achieves the best accuracy compared to existing methods with a smaller model size and acceptable runtime speed.",1
"We study the video super-resolution (SR) problem for facilitating video analytics tasks, e.g. action recognition, instead of for visual quality. The popular action recognition methods based on convolutional networks, exemplified by two-stream networks, are not directly applicable on video of low spatial resolution. This can be remedied by performing video SR prior to recognition, which motivates us to improve the SR procedure for recognition accuracy. Tailored for two-stream action recognition networks, we propose two video SR methods for the spatial and temporal streams respectively. On the one hand, we observe that regions with action are more important to recognition, and we propose an optical-flow guided weighted mean-squared-error loss for our spatial-oriented SR (SoSR) network to emphasize the reconstruction of moving objects. On the other hand, we observe that existing video SR methods incur temporal discontinuity between frames, which also worsens the recognition accuracy, and we propose a siamese network for our temporal-oriented SR (ToSR) training that emphasizes the temporal continuity between consecutive frames. We perform experiments using two state-of-the-art action recognition networks and two well-known datasets--UCF101 and HMDB51. Results demonstrate the effectiveness of our proposed SoSR and ToSR in improving recognition accuracy.",0
"Our focus is on studying the video super-resolution (SR) problem to aid video analytics tasks such as action recognition, rather than solely for visual quality. Existing action recognition methods that depend on convolutional networks, such as two-stream networks, cannot be used directly on low spatial resolution videos. To address this, we propose two video SR methods tailored for two-stream action recognition networks, one for spatial-oriented SR (SoSR) and the other for temporal-oriented SR (ToSR). Our SoSR network emphasizes the reconstruction of moving objects by using an optical-flow guided weighted mean-squared-error loss, recognizing the importance of action regions. Meanwhile, our ToSR network uses a siamese network to emphasize the temporal continuity between consecutive frames, which is often disrupted by existing video SR methods. We evaluated our proposed methods on two well-known datasets, UCF101 and HMDB51, using two state-of-the-art action recognition networks, and the results demonstrate the effectiveness of our proposed SoSR and ToSR in improving recognition accuracy.",1
"When a deep neural network is trained on data with only image-level labeling, the regions activated in each image tend to identify only a small region of the target object. We propose a method of using videos automatically harvested from the web to identify a larger region of the target object by using temporal information, which is not present in the static image. The temporal variations in a video allow different regions of the target object to be activated. We obtain an activated region in each frame of a video, and then aggregate the regions from successive frames into a single image, using a warping technique based on optical flow. The resulting localization maps cover more of the target object, and can then be used as proxy ground-truth to train a segmentation network. This simple approach outperforms existing methods under the same level of supervision, and even approaches relying on extra annotations. Based on VGG-16 and ResNet 101 backbones, our method achieves the mIoU of 65.0 and 67.4, respectively, on PASCAL VOC 2012 test images, which represents a new state-of-the-art.",0
"When data with only image-level labeling is used to train a deep neural network, the activated regions in each image are limited to a small part of the target object. To address this, we propose a solution that involves using videos sourced from the internet, which contain temporal information that is absent in static images. The temporal variations in the videos enable different regions of the target object to be activated. We extract an activated region from every frame of the video, and then combine the regions from successive frames into a single image using a warping technique based on optical flow. This results in localization maps that cover a larger portion of the target object, which can then be utilized as a substitute ground-truth to train a segmentation network. Our approach surpasses existing methods under the same level of supervision and even approaches that rely on additional annotations. Our method, based on VGG-16 and ResNet 101 backbones, achieves an mIoU of 65.0 and 67.4 on PASCAL VOC 2012 test images, respectively, which sets a new state-of-the-art.",1
"Anomaly detection plays in many fields of research, along with the strongly related task of outlier detection, a very important role. Especially within the context of the automated analysis of video material recorded by surveillance cameras, abnormal situations can be of very different nature. For this purpose this work investigates Generative-Adversarial-Network-based methods (GAN) for anomaly detection related to surveillance applications. The focus is on the usage of static camera setups, since this kind of camera is one of the most often used and belongs to the lower price segment. In order to address this task, multiple subtasks are evaluated, including the influence of existing optical flow methods for the incorporation of short-term temporal information, different forms of network setups and losses for GANs, and the use of morphological operations for further performance improvement. With these extension we achieved up to 2.4% better results. Furthermore, the final method reduced the anomaly detection error for GAN-based methods by about 42.8%.",0
"In various research fields, the significance of anomaly detection and outlier detection is highly notable. In particular, when analyzing automated surveillance camera footage, anomalous situations can vary greatly. This study focuses on the use of Generative-Adversarial-Network-based (GAN) techniques for anomaly detection in surveillance applications, with an emphasis on static camera setups, which are common and affordable. The study evaluates multiple subtasks, such as the impact of optical flow techniques for short-term temporal information integration, network setups and losses for GANs, and the use of morphological operations for performance enhancement. By incorporating these extensions, the study achieved a 2.4% improvement in results. Additionally, the final method successfully reduced the anomaly detection error for GAN-based methods by approximately 42.8%.",1
"In this paper we present mono-stixels, a compact environment representation specially designed for dynamic street scenes. Mono-stixels are a novel approach to estimate stixels from a monocular camera sequence instead of the traditionally used stereo depth measurements. Our approach jointly infers the depth, motion and semantic information of the dynamic scene as a 1D energy minimization problem based on optical flow estimates, pixel-wise semantic segmentation and camera motion. The optical flow of a stixel is described by a homography. By applying the mono-stixel model the degrees of freedom of a stixel-homography are reduced to only up to two degrees of freedom. Furthermore, we exploit a scene model and semantic information to handle moving objects. In our experiments we use the public available DeepFlow for optical flow estimation and FCN8s for the semantic information as inputs and show on the KITTI 2015 dataset that mono-stixels provide a compact and reliable depth reconstruction of both the static and moving parts of the scene. Thereby, mono-stixels overcome the limitation to static scenes of previous structure-from-motion approaches.",0
"The focus of this paper is the presentation of mono-stixels, a novel method to represent dynamic street scenes in a compact form. The uniqueness of mono-stixels lies in their estimation from a monocular camera sequence, unlike traditional approaches that use stereo depth measurements. Our approach involves the use of optical flow, pixel-wise semantic segmentation, and camera motion to jointly infer depth, motion, and semantic information as a 1D energy minimization problem. The optical flow of a stixel is defined by a homography, and by applying the mono-stixel model, the degrees of freedom are reduced to a maximum of two. We also use a scene model and semantic information to handle moving objects. In our experiments, we use DeepFlow for optical flow estimation and FCN8s for semantic information as inputs, and demonstrate on the KITTI 2015 dataset that mono-stixels provide a reliable reconstruction of both static and moving elements of the scene. This overcomes the limitations of previous structure-from-motion approaches that were confined to static scenes.",1
"Images of static scenes submerged beneath a wavy water surface exhibit severe non-rigid distortions. The physics of water flow suggests that water surfaces possess spatio-temporal smoothness and temporal periodicity. Hence they possess a sparse representation in the 3D discrete Fourier (DFT) basis. Motivated by this, we pose the task of restoration of such video sequences as a compressed sensing (CS) problem. We begin by tracking a few salient feature points across the frames of a video sequence of the submerged scene. Using these point trajectories, we show that the motion fields at all other (non-tracked) points can be effectively estimated using a typical CS solver. This by itself is a novel contribution in the field of non-rigid motion estimation. We show that this method outperforms state of the art algorithms for underwater image restoration. We further consider a simple optical flow algorithm based on local polynomial expansion of the image frames (PEOF). Surprisingly, we demonstrate that PEOF is more efficient and often outperforms all the state of the art methods in terms of numerical measures. Finally, we demonstrate that a two-stage approach consisting of the CS step followed by PEOF much more accurately preserves the image structure and improves the (visual as well as numerical) video quality as compared to just the PEOF stage.",0
"Severely non-rigid distortions are present in images of static scenes beneath wavy water surfaces. The spatio-temporal smoothness and temporal periodicity of water flow indicate that water surfaces have a sparse representation in the 3D discrete Fourier (DFT) basis. Therefore, we propose to restore such video sequences by treating the task as a compressed sensing (CS) problem. Initially, we track a few prominent feature points in the submerged scene's video sequence. By using these point trajectories, we can effectively estimate the motion fields at all other non-tracked points through a standard CS solver, which is a novel development in non-rigid motion estimation. Our method outperforms the current state-of-the-art algorithms for underwater image restoration. We also assess a straightforward optical flow algorithm based on local polynomial expansion of the image frames (PEOF) and find that it is often more effective than the state-of-the-art methods in terms of numerical measures. Finally, we demonstrate that a two-stage approach, including the CS step followed by PEOF, results in better preservation of image structure and improved visual and numerical video quality compared to just the PEOF stage.",1
"Egocentric action anticipation consists in understanding which objects the camera wearer will interact with in the near future and which actions they will perform. We tackle the problem proposing an architecture able to anticipate actions at multiple temporal scales using two LSTMs to 1) summarize the past, and 2) formulate predictions about the future. The input video is processed considering three complimentary modalities: appearance (RGB), motion (optical flow) and objects (object-based features). Modality-specific predictions are fused using a novel Modality ATTention (MATT) mechanism which learns to weigh modalities in an adaptive fashion. Extensive evaluations on two large-scale benchmark datasets show that our method outperforms prior art by up to +7% on the challenging EPIC-Kitchens dataset including more than 2500 actions, and generalizes to EGTEA Gaze+. Our approach is also shown to generalize to the tasks of early action recognition and action recognition. Our method is ranked first in the public leaderboard of the EPIC-Kitchens egocentric action anticipation challenge 2019. Please see our web pages for code and examples: http://iplab.dmi.unict.it/rulstm - https://github.com/fpv-iplab/rulstm.",0
"The concept of egocentric action anticipation involves predicting which objects a camera wearer will interact with and the actions they will perform in the near future. To solve this problem, we propose an architecture that uses two LSTMs to summarize past events and formulate predictions about the future at multiple temporal scales. We process the input video using three modalities, including appearance, motion, and objects. Our approach fuses modality-specific predictions using a novel Modality ATTention mechanism that learns to weigh modalities in an adaptive way. Our method outperforms prior art by up to +7% on the challenging EPIC-Kitchens dataset containing more than 2500 actions and generalizes to EGTEA Gaze+. We demonstrate that our method also works for early action recognition and action recognition tasks. Our approach is ranked first in the public leaderboard of the 2019 EPIC-Kitchens egocentric action anticipation challenge. Examples and code can be found on our web pages: http://iplab.dmi.unict.it/rulstm - https://github.com/fpv-iplab/rulstm.",1
"In this paper, we develop a modified differential Structure from Motion (SfM) algorithm that can estimate relative pose from two consecutive frames despite of Rolling Shutter (RS) artifacts. In particular, we show that under constant velocity assumption, the errors induced by the rolling shutter effect can be easily rectified by a linear scaling operation on each optical flow. We further propose a 9-point algorithm to recover the relative pose of a rolling shutter camera that undergoes constant acceleration motion. We demonstrate that the dense depth maps recovered from the relative pose of the RS camera can be used in a RS-aware warping for image rectification to recover high-quality Global Shutter (GS) images. Experiments on both synthetic and real RS images show that our RS-aware differential SfM algorithm produces more accurate results on relative pose estimation and 3D reconstruction from images distorted by RS effect compared to standard SfM algorithms that assume a GS camera model. We also demonstrate that our RS-aware warping for image rectification method outperforms state-of-the-art commercial software products, i.e. Adobe After Effects and Apple Imovie, at removing RS artifacts.",0
"The objective of this study is to enhance the Structure from Motion (SfM) algorithm to estimate relative pose from two consecutive frames, while considering Rolling Shutter (RS) artifacts. A modified differential SfM algorithm is proposed that rectifies the errors caused by the rolling shutter effect through a linear scaling operation on each optical flow under the assumption of constant velocity. Additionally, a 9-point algorithm is presented to recover the relative pose of a rolling shutter camera that undergoes constant acceleration motion. The dense depth maps obtained from the relative pose of the RS camera are used for RS-aware warping to rectify images and recover high-quality Global Shutter (GS) images. Experimental results indicate that the RS-aware differential SfM algorithm is more accurate in relative pose estimation and 3D reconstruction from images distorted by RS effect than standard SfM algorithms that assume a GS camera model. The RS-aware warping for image rectification method proposed in this study performs better than the state-of-the-art commercial software products, such as Adobe After Effects and Apple Imovie, in removing RS artifacts.",1
"We address the challenging task of foreground object discovery and segmentation in video. We introduce an efficient solution, suitable for both unsupervised and supervised scenarios, based on a spacetime graph representation of the video sequence. We ensure a fine grained representation with one-to-one correspondences between graph nodes and video pixels. We formulate the task as a spectral clustering problem by exploiting the spatio-temporal consistency between the scene elements in terms of motion and appearance. Graph nodes that belong to the main object of interest should form a strong cluster, as they are linked through long range optical flow chains and have similar motion and appearance features along those chains. On one hand, the optimization problem aims to maximize the segmentation clustering score based on the motion structure through space and time. On the other hand, the segmentation should be consistent with respect to node features. Our approach leads to a graph formulation in which the segmentation solution becomes the principal eigenvector of a novel Feature-Motion matrix. While the actual matrix is not computed explicitly, the proposed algorithm efficiently computes, in a few iteration steps, the principal eigenvector that captures the segmentation of the main object in the video. The proposed algorithm, GO-VOS, produces a global optimum solution and, consequently, it does not depend on initialization. In practice, GO-VOS achieves state of the art results on three challenging datasets used in current literature: DAVIS, SegTrack and YouTube-Objects.",0
"Our aim is to discover and segment foreground objects in video, a task that poses significant challenges. We present a solution that is efficient for both supervised and unsupervised scenarios, using a spacetime graph representation of the video sequence. Our approach ensures a precise representation with a one-to-one correspondence between graph nodes and video pixels. We approach the task by formulating it as a spectral clustering problem, taking advantage of spatio-temporal consistency between scene elements in terms of appearance and motion. Our algorithm aims to form a strong cluster of graph nodes that belong to the primary object of interest, linked through long-range optical flow chains and possessing similar motion and appearance features. The objective is to maximize the segmentation clustering score based on motion structure through space and time while also ensuring consistency with respect to node features. Our approach leads to a graph formulation where the segmentation solution becomes the principal eigenvector of a novel Feature-Motion matrix that is efficiently computed in a few iterations. Our algorithm, GO-VOS, produces a global optimum solution that does not depend on initialization. In practice, GO-VOS achieves state-of-the-art results on three challenging datasets: DAVIS, SegTrack, and YouTube-Objects.",1
"In this paper, we propose a convolutional layer inspired by optical flow algorithms to learn motion representations. Our representation flow layer is a fully-differentiable layer designed to capture the `flow' of any representation channel within a convolutional neural network for action recognition. Its parameters for iterative flow optimization are learned in an end-to-end fashion together with the other CNN model parameters, maximizing the action recognition performance. Furthermore, we newly introduce the concept of learning `flow of flow' representations by stacking multiple representation flow layers. We conducted extensive experimental evaluations, confirming its advantages over previous recognition models using traditional optical flows in both computational speed and performance. Code/models available here: https://piergiaj.github.io/rep-flow-site/",0
"The paper proposes a convolutional layer that takes inspiration from optical flow algorithms to acquire motion representations. The representation flow layer, which is fully-differentiable, is intended to capture the flow of any representation channel in a convolutional neural network for action recognition. Its iterative flow optimization parameters are learned alongside the other CNN model parameters in an end-to-end manner to maximize action recognition performance. Additionally, the paper introduces the idea of learning 'flow of flow' representations by stacking multiple representation flow layers. Extensive experimental evaluations were conducted, demonstrating its superiority over previous recognition models that employ traditional optical flows in terms of both computational speed and performance. Code/models are accessible at https://piergiaj.github.io/rep-flow-site/.",1
"Video deblurring is a challenging task due to the spatially variant blur caused by camera shake, object motions, and depth variations, etc. Existing methods usually estimate optical flow in the blurry video to align consecutive frames or approximate blur kernels. However, they tend to generate artifacts or cannot effectively remove blur when the estimated optical flow is not accurate. To overcome the limitation of separate optical flow estimation, we propose a Spatio-Temporal Filter Adaptive Network (STFAN) for the alignment and deblurring in a unified framework. The proposed STFAN takes both blurry and restored images of the previous frame as well as blurry image of the current frame as input, and dynamically generates the spatially adaptive filters for the alignment and deblurring. We then propose the new Filter Adaptive Convolutional (FAC) layer to align the deblurred features of the previous frame with the current frame and remove the spatially variant blur from the features of the current frame. Finally, we develop a reconstruction network which takes the fusion of two transformed features to restore the clear frames. Both quantitative and qualitative evaluation results on the benchmark datasets and real-world videos demonstrate that the proposed algorithm performs favorably against state-of-the-art methods in terms of accuracy, speed as well as model size.",0
"The removal of blur from videos is a difficult task as the blur is not uniform due to various factors such as camera shake, object movements, and variations in depth. Current methods use optical flow estimation to align frames or approximate blur kernels, but these methods often generate artifacts or fail to remove blur when the optical flow estimation is inaccurate. To address this issue, we propose a Spatio-Temporal Filter Adaptive Network (STFAN) that combines alignment and deblurring in a single framework. The STFAN takes blurry and restored images of the previous frame, as well as the blurry image of the current frame, as inputs, and generates adaptive filters for alignment and deblurring. We also introduce the Filter Adaptive Convolutional (FAC) layer to align deblurred features of the previous frame with the current frame and remove the spatially variant blur from the current frame's features. Finally, we develop a reconstruction network that restores clear frames by fusing the transformed features. Evaluations show that our method outperforms state-of-the-art approaches in terms of accuracy, speed, and model size on benchmark datasets and real-world videos.",1
"This paper presents novel techniques for recovering 3D dense scene flow, based on differential analysis of 4D light fields. The key enabling result is a per-ray linear equation, called the ray flow equation, that relates 3D scene flow to 4D light field gradients. The ray flow equation is invariant to 3D scene structure and applicable to a general class of scenes, but is under-constrained (3 unknowns per equation). Thus, additional constraints must be imposed to recover motion. We develop two families of scene flow algorithms by leveraging the structural similarity between ray flow and optical flow equations: local 'Lucas-Kanade' ray flow and global 'Horn-Schunck' ray flow, inspired by corresponding optical flow methods. We also develop a combined local-global method by utilizing the correspondence structure in the light fields. We demonstrate high precision 3D scene flow recovery for a wide range of scenarios, including rotation and non-rigid motion. We analyze the theoretical and practical performance limits of the proposed techniques via the light field structure tensor, a 3x3 matrix that encodes the local structure of light fields. We envision that the proposed analysis and algorithms will lead to design of future light-field cameras that are optimized for motion sensing, in addition to depth sensing.",0
"In this article, innovative methods for retrieving 3D dense scene flow are presented. The techniques rely on differential analysis of 4D light fields, resulting in a per-ray linear equation known as the ray flow equation. This equation is invariant to 3D scene structure and applicable to a broad range of scenes, but it is under-constrained, with three unknowns per equation. To recover motion, additional restrictions are necessary. Two scene flow algorithms are introduced, using the structural similarity between ray flow and optical flow equations: local 'Lucas-Kanade' ray flow and global 'Horn-Schunck' ray flow, as well as a combined local-global method that utilizes the correspondence structure in light fields. The techniques show high precision 3D scene flow recovery for different scenarios, including rotation and non-rigid motion. The light field structure tensor is analyzed to determine the theoretical and practical performance limits of the proposed techniques. These methods are expected to lead to future light-field cameras optimized for motion sensing, in addition to depth sensing.",1
"Most of current Convolution Neural Network (CNN) based methods for optical flow estimation focus on learning optical flow on synthetic datasets with groundtruth, which is not practical. In this paper, we propose an unsupervised optical flow estimation framework named PCLNet. It uses pyramid Convolution LSTM (ConvLSTM) with the constraint of adjacent frame reconstruction, which allows flexibly estimating multi-frame optical flows from any video clip. Besides, by decoupling motion feature learning and optical flow representation, our method avoids complex short-cut connections used in existing frameworks while improving accuracy of optical flow estimation. Moreover, different from those methods using specialized CNN architectures for capturing motion, our framework directly learns optical flow from the features of generic CNNs and thus can be easily embedded in any CNN based frameworks for other tasks. Extensive experiments have verified that our method not only estimates optical flow effectively and accurately, but also obtains comparable performance on action recognition.",0
"The majority of Convolution Neural Network (CNN) based methods for estimating optical flow focus on synthetic datasets with groundtruth, which is not feasible. This article proposes an unsupervised optical flow estimation framework, PCLNet, which utilizes pyramid Convolution LSTM (ConvLSTM) with the limitation of adjacent frame reconstruction. This allows for flexible estimation of multi-frame optical flows from any video clip. Additionally, by separating motion feature learning and optical flow representation, our method eliminates the need for complex short-cut connections used in other frameworks, while improving the accuracy of optical flow estimation. Furthermore, unlike other methods that use specialized CNN architectures to capture motion, our framework directly learns optical flow from the features of generic CNNs, making it simple to incorporate into any CNN-based frameworks for other tasks. Through numerous experiments, we have confirmed that our method is effective and accurate in estimating optical flow, while also achieving comparable performance in action recognition.",1
"Video stabilization algorithms are of greater importance nowadays with the prevalence of hand-held devices which unavoidably produce videos with undesirable shaky motions. In this paper we propose a data-driven online video stabilization method along with a paired dataset for deep learning. The network processes each unsteady frame progressively in a multi-scale manner, from low resolution to high resolution, and then outputs an affine transformation to stabilize the frame. Different from conventional methods which require explicit feature tracking or optical flow estimation, the underlying stabilization process is learned implicitly from the training data, and the stabilization process can be done online. Since there are limited public video stabilization datasets available, we synthesized unstable videos with different extent of shake that simulate real-life camera movement. Experiments show that our method is able to outperform other stabilization methods in several unstable samples while remaining comparable in general. Also, our method is tested on complex contents and found robust enough to dampen these samples to some extent even it was not explicitly trained in the contents.",0
"Due to the prevalence of hand-held devices that produce videos with unwanted shaky motions, video stabilization algorithms have become increasingly important. This paper presents a data-driven online video stabilization method, including a paired dataset for deep learning. The network progressively processes each unstable frame in a multi-scale manner, from low to high resolution, and produces an affine transformation to stabilize the frame. Unlike traditional methods that require explicit feature tracking or optical flow estimation, our approach implicitly learns the underlying stabilization process from the training data, enabling online stabilization. Since few public video stabilization datasets exist, we synthesized unstable videos with varying degrees of shake to simulate real-life camera movement. Our experiments show that our method outperforms other stabilization methods in several unstable samples while maintaining comparable performance overall. Additionally, our method was tested on complex content and found to be sufficiently robust to reduce shaking, even though it was not specifically trained on that content.",1
"We focus on the word-level visual lipreading, which requires recognizing the word being spoken, given only the video but not the audio. State-of-the-art methods explore the use of end-to-end neural networks, including a shallow (up to three layers) 3D convolutional neural network (CNN) + a deep 2D CNN (e.g., ResNet) as the front-end to extract visual features, and a recurrent neural network (e.g., bidirectional LSTM) as the back-end for classification. In this work, we propose to replace the shallow 3D CNNs + deep 2D CNNs front-end with recent successful deep 3D CNNs --- two-stream (i.e., grayscale video and optical flow streams) I3D. We evaluate different combinations of front-end and back-end modules with the grayscale video and optical flow inputs on the LRW dataset. The experiments show that, compared to the shallow 3D CNNs + deep 2D CNNs front-end, the deep 3D CNNs front-end with pre-training on the large-scale image and video datasets (e.g., ImageNet and Kinetics) can improve the classification accuracy. Also, we demonstrate that using the optical flow input alone can achieve comparable performance as using the grayscale video as input. Moreover, the two-stream network using both the grayscale video and optical flow inputs can further improve the performance. Overall, our two-stream I3D front-end with a Bi-LSTM back-end results in an absolute improvement of 5.3% over the previous art on the LRW dataset.",0
"Our focus is on visual lipreading at the word level, where the aim is to recognize spoken words solely from their video, without audio. State-of-the-art techniques use neural networks, with a 3D CNN paired with a deep 2D CNN to extract visual features, and a recurrent neural network for classification. We propose replacing the 3D CNN + deep 2D CNN front-end with a newer, more successful deep 3D CNN - the two-stream I3D, which uses both grayscale video and optical flow streams. We test various combinations of front-end and back-end modules using both types of inputs on the LRW dataset. The results show that the deep 3D CNNs front-end, pre-trained on large-scale image and video datasets, improves classification accuracy over the shallow 3D CNNs + deep 2D CNNs front-end. We also show that using optical flow alone achieves comparable performance to grayscale video input, while using both further enhances the network's performance. Our two-stream I3D front-end with a Bi-LSTM back-end achieves a 5.3% improvement over the previous art on the LRW dataset.",1
"Many road accidents occur due to distracted drivers. Today, driver monitoring is essential even for the latest autonomous vehicles to alert distracted drivers in order to take over control of the vehicle in case of emergency. In this paper, a spatio-temporal approach is applied to classify drivers' distraction level and movement decisions using convolutional neural networks (CNNs). We approach this problem as action recognition to benefit from temporal information in addition to spatial information. Our approach relies on features extracted from sparsely selected frames of an action using a pre-trained BN-Inception network. Experiments show that our approach outperforms the state-of-the art results on the Distracted Driver Dataset (96.31%), with an accuracy of 99.10% for 10-class classification while providing real-time performance. We also analyzed the impact of fusion using RGB and optical flow modalities with a very recent data level fusion strategy. The results on the Distracted Driver and Brain4Cars datasets show that fusion of these modalities further increases the accuracy.",0
"Distracted drivers are a major contributor to road accidents, even in the latest autonomous vehicles. Therefore, it is crucial to monitor drivers to alert them in case of emergency. This paper proposes a spatio-temporal approach using convolutional neural networks (CNNs) to classify drivers' level of distraction and movement decisions. The problem is approached as action recognition, allowing for the utilization of both spatial and temporal information. The approach uses features extracted from sparsely selected frames of an action using a pre-trained BN-Inception network. Results show that this approach outperforms the state-of-the-art results on the Distracted Driver Dataset with 99.10% accuracy for 10-class classification while also providing real-time performance. The study also analyzes the impact of fusion using RGB and optical flow modalities with recent data level fusion strategy, showing an increase in accuracy on both the Distracted Driver and Brain4Cars datasets.",1
"Optical Flow (OF) and depth are commonly used for visual odometry since they provide sufficient information about camera ego-motion in a rigid scene. We reformulate the problem of ego-motion estimation as a problem of motion estimation of a 3D-scene with respect to a static camera. The entire scene motion can be represented as a combination of motions of its visible points. Using OF and depth we estimate a motion of each point in terms of 6DoF and represent results in the form of motion maps, each one addressing single degree of freedom. In this work we provide motion maps as inputs to a deep neural network that predicts 6DoF of scene motion. Through our evaluation on outdoor and indoor datasets we show that utilizing motion maps leads to accuracy improvement in comparison with naive stacking of depth and OF. Another contribution of our work is a novel network architecture that efficiently exploits motion maps and outperforms learnable RGB/RGB-D baselines.",0
"Visual odometry commonly relies on Optical Flow (OF) and depth to gather enough information about camera ego-motion in a rigid scene. To transform the problem of ego-motion estimation, we view it as a problem of motion estimation of a 3D-scene in relation to a static camera. We can represent the entire scene motion through a combination of the motions of its visible points. By using OF and depth, we estimate the motion of each point in terms of 6DoF and present the results as motion maps, where each map addresses a single degree of freedom. Our approach involves providing motion maps as inputs to a deep neural network that predicts the 6DoF of scene motion. Our evaluation using outdoor and indoor datasets demonstrates that using motion maps results in improved accuracy compared to the naive stacking of depth and OF. Furthermore, our work introduces a new network architecture that efficiently utilizes motion maps and outperforms learnable RGB/RGB-D baselines.",1
"In this technical report we investigate speed estimation of the ego-vehicle on the KITTI benchmark using state-of-the-art deep neural network based optical flow and single-view depth prediction methods. Using a straightforward intuitive approach and approximating a single scale factor, we evaluate several application schemes of the deep networks and formulate meaningful conclusions such as: combining depth information with optical flow improves speed estimation accuracy as opposed to using optical flow alone; the quality of the deep neural network methods influences speed estimation performance; using the depth and optical flow results from smaller crops of wide images degrades performance. With these observations in mind, we achieve a RMSE of less than 1 m/s for vehicle speed estimation using monocular images as input from recordings of the KITTI benchmark. Limitations and possible future directions are discussed as well.",0
"This technical report delves into the estimation of the ego-vehicle's speed on the KITTI benchmark. To accomplish this, we utilize advanced deep neural network-based optical flow and single-view depth prediction techniques. By employing a straightforward and intuitive approach, we assess various deep network application schemes and reach significant conclusions. For example, we discover that combining depth information with optical flow leads to more accurate speed estimation than using optical flow alone. Additionally, the quality of the deep neural network methods has an impact on speed estimation performance. Furthermore, utilizing the depth and optical flow results from smaller image crops reduces performance. With these insights, we achieve an RMSE of under 1 m/s in vehicle speed estimation using monocular images as input from KITTI benchmark recordings. We also discuss limitations and possible future directions.",1
"Learning to estimate 3D geometry in a single frame and optical flow from consecutive frames by watching unlabeled videos via deep convolutional network has made significant progress recently. Current state-of-the-art (SoTA) methods treat the two tasks independently. One typical assumption of the existing depth estimation methods is that the scenes contain no independent moving objects. while object moving could be easily modeled using optical flow. In this paper, we propose to address the two tasks as a whole, i.e. to jointly understand per-pixel 3D geometry and motion. This eliminates the need of static scene assumption and enforces the inherent geometrical consistency during the learning process, yielding significantly improved results for both tasks. We call our method as ""Every Pixel Counts++"" or ""EPC++"". Specifically, during training, given two consecutive frames from a video, we adopt three parallel networks to predict the camera motion (MotionNet), dense depth map (DepthNet), and per-pixel optical flow between two frames (OptFlowNet) respectively. The three types of information are fed into a holistic 3D motion parser (HMP), and per-pixel 3D motion of both rigid background and moving objects are disentangled and recovered. Comprehensive experiments were conducted on datasets with different scenes, including driving scenario (KITTI 2012 and KITTI 2015 datasets), mixed outdoor/indoor scenes (Make3D) and synthetic animation (MPI Sintel dataset). Performance on the five tasks of depth estimation, optical flow estimation, odometry, moving object segmentation and scene flow estimation shows that our approach outperforms other SoTA methods. Code will be available at: https://github.com/chenxuluo/EPC.",0
"Recently, there has been significant progress in using deep convolutional networks to estimate 3D geometry in a single frame and optical flow from consecutive frames by watching unlabeled videos. However, current state-of-the-art methods treat these two tasks independently, assuming that scenes contain no independent moving objects. This paper proposes a new approach that jointly understands per-pixel 3D geometry and motion, eliminating the need for static scene assumptions and enforcing geometrical consistency during the learning process. The proposed method, called ""Every Pixel Counts++"" or ""EPC++"", uses three parallel networks to predict camera motion, dense depth map, and per-pixel optical flow, which are then fed into a holistic 3D motion parser to disentangle and recover per-pixel 3D motion of both rigid background and moving objects. The proposed approach outperforms other state-of-the-art methods on various datasets, including driving scenarios, mixed outdoor/indoor scenes, and synthetic animation. Code will be available at https://github.com/chenxuluo/EPC.",1
"We present Accel, a novel semantic video segmentation system that achieves high accuracy at low inference cost by combining the predictions of two network branches: (1) a reference branch that extracts high-detail features on a reference keyframe, and warps these features forward using frame-to-frame optical flow estimates, and (2) an update branch that computes features of adjustable quality on the current frame, performing a temporal update at each video frame. The modularity of the update branch, where feature subnetworks of varying layer depth can be inserted (e.g. ResNet-18 to ResNet-101), enables operation over a new, state-of-the-art accuracy-throughput trade-off spectrum. Over this curve, Accel models achieve both higher accuracy and faster inference times than the closest comparable single-frame segmentation networks. In general, Accel significantly outperforms previous work on efficient semantic video segmentation, correcting warping-related error that compounds on datasets with complex dynamics. Accel is end-to-end trainable and highly modular: the reference network, the optical flow network, and the update network can each be selected independently, depending on application requirements, and then jointly fine-tuned. The result is a robust, general system for fast, high-accuracy semantic segmentation on video.",0
"Introducing Accel, a new semantic video segmentation system that combines the predictions of two network branches to achieve high accuracy at low inference cost. The first branch, called the reference branch, extracts high-detail features on a reference keyframe and warps them forward using frame-to-frame optical flow estimates. The second branch, known as the update branch, computes features of adjustable quality on the current frame and performs a temporal update at each video frame. The update branch's modularity allows for the insertion of feature subnetworks of varying layer depth, resulting in a new accuracy-throughput trade-off spectrum. Accel models achieve higher accuracy and faster inference times than the closest comparable single-frame segmentation networks. Furthermore, Accel outperforms previous work on efficient semantic video segmentation by correcting warping-related error that compounds on datasets with complex dynamics. Accel is end-to-end trainable and highly modular, making it a robust, general system for fast, high-accuracy semantic segmentation on video that can be tailored to specific application requirements.",1
"Appearance and motion are two key components to depict and characterize the video content. Currently, the two-stream models have achieved state-of-the-art performances on video classification. However, extracting motion information, specifically in the form of optical flow features, is extremely computationally expensive, especially for large-scale video classification. In this paper, we propose a motion hallucination network, namely MoNet, to imagine the optical flow features from the appearance features, with no reliance on the optical flow computation. Specifically, MoNet models the temporal relationships of the appearance features and exploits the contextual relationships of the optical flow features with concurrent connections. Extensive experimental results demonstrate that the proposed MoNet can effectively and efficiently hallucinate the optical flow features, which together with the appearance features consistently improve the video classification performances. Moreover, MoNet can help cutting down almost a half of computational and data-storage burdens for the two-stream video classification. Our code is available at: https://github.com/YongyiTang92/MoNet-Features.",0
"To depict and characterize video content, appearance and motion are essential components. While the two-stream models have achieved exceptional performance in video classification, extracting motion information in the form of optical flow features is computationally expensive, particularly for large-scale video classification. This paper introduces MoNet, a motion hallucination network that can imagine optical flow features from appearance features without relying on optical flow computation. The proposed model models the temporal relationships of the appearance features and exploits the contextual relationships of the optical flow features with concurrent connections. Experimental results demonstrate that MoNet can effectively and efficiently hallucinate optical flow features, which, when combined with appearance features, consistently improve video classification performance. Furthermore, MoNet reduces almost half of the computational and data-storage burdens for two-stream video classification. The code is available at https://github.com/YongyiTang92/MoNet-Features.",1
"The goal of this paper is to detect the spatio-temporal extent of an action. The two-stream detection network based on RGB and flow provides state-of-the-art accuracy at the expense of a large model-size and heavy computation. We propose to embed RGB and optical-flow into a single two-in-one stream network with new layers. A motion condition layer extracts motion information from flow images, which is leveraged by the motion modulation layer to generate transformation parameters for modulating the low-level RGB features. The method is easily embedded in existing appearance- or two-stream action detection networks, and trained end-to-end. Experiments demonstrate that leveraging the motion condition to modulate RGB features improves detection accuracy. With only half the computation and parameters of the state-of-the-art two-stream methods, our two-in-one stream still achieves impressive results on UCF101-24, UCFSports and J-HMDB.",0
"The aim of this article is to identify the spatial and temporal scope of an action. Although the two-stream detection network based on RGB and flow is highly accurate, it requires a large model-size and heavy computation. To address this issue, we propose a two-in-one stream network that combines RGB and optical-flow with new layers. The motion condition layer extracts motion information from flow images, which is utilized by the motion modulation layer to generate transformation parameters for modulating the low-level RGB features. This approach can be easily integrated into existing appearance- or two-stream action detection networks and trained end-to-end. Results show that using the motion condition to modulate RGB features enhances detection accuracy. Despite having only half the computation and parameters of the state-of-the-art two-stream methods, our two-in-one stream still achieves impressive results on UCF101-24, UCFSports and J-HMDB.",1
"Convolutional neural networks (CNNs) can model complicated non-linear relations between images. However, they are notoriously sensitive to small changes in the input. Most CNNs trained to describe image-to-image mappings generate temporally unstable results when applied to video sequences, leading to flickering artifacts and other inconsistencies over time. In order to use CNNs for video material, previous methods have relied on estimating dense frame-to-frame motion information (optical flow) in the training and/or the inference phase, or by exploring recurrent learning structures. We take a different approach to the problem, posing temporal stability as a regularization of the cost function. The regularization is formulated to account for different types of motion that can occur between frames, so that temporally stable CNNs can be trained without the need for video material or expensive motion estimation. The training can be performed as a fine-tuning operation, without architectural modifications of the CNN. Our evaluation shows that the training strategy leads to large improvements in temporal smoothness. Moreover, for small datasets the regularization can help in boosting the generalization performance to a much larger extent than what is possible with na\""ive augmentation strategies.",0
"CNNs have the ability to represent complex non-linear relationships between images; however, they are highly sensitive to minor changes in the input. When applied to video sequences, CNNs designed for image-to-image mappings result in unstable outcomes, leading to inconsistencies and flickering artifacts over time. Prior methods have relied on estimating dense frame-to-frame motion information or using recurrent learning structures to address this problem. We propose an alternative approach by incorporating temporal stability as a regularization term in the cost function. This regularization factor accounts for various types of motion between frames, enabling the training of temporally stable CNNs without requiring video materials or costly motion estimation. The CNN architecture does not require any modifications, and the training can be performed through fine-tuning. Our results demonstrate that this training strategy significantly improves temporal smoothness. Additionally, for small datasets, the regularization can enhance generalization performance more effectively than naive augmentation strategies.",1
"We present a new method to learn video representations from unlabeled data. Given large-scale unlabeled video data, the objective is to benefit from such data by learning a generic and transferable representation space that can be directly used for a new task such as zero/few-shot learning. We formulate our unsupervised representation learning as a multi-modal, multi-task learning problem, where the representations are also shared across different modalities via distillation. Further, we also introduce the concept of finding a better loss function to train such multi-task multi-modal representation space using an evolutionary algorithm; our method automatically searches over different combinations of loss functions capturing multiple (self-supervised) tasks and modalities. Our formulation allows for the distillation of audio, optical flow and temporal information into a single, RGB-based convolutional neural network. We also compare the effects of using additional unlabeled video data and evaluate our representation learning on standard public video datasets.",0
"A novel approach to acquiring video representations from unlabelled data is introduced. The aim is to leverage this data to acquire a versatile and adaptable representation space that can be used directly for tasks like zero/few-shot learning. Our unsupervised learning process is framed as a multi-modal, multi-task learning challenge, where distillation is used to share representations across different modalities. To improve the training of this multi-modal, multi-task representation space, we propose a new loss function search method using evolutionary algorithms. Our framework can incorporate audio, optical flow and temporal information into a single, RGB-based convolutional neural network. The impact of using additional unlabelled video data is also discussed, and our representation learning approach is evaluated on established public video datasets.",1
"We address the problem of temporal activity detection in continuous, untrimmed video streams. This is a difficult task that requires extracting meaningful spatio-temporal features to capture activities, accurately localizing the start and end times of each activity. We introduce a new model, Region Convolutional 3D Network (R-C3D), which encodes the video streams using a three-dimensional fully convolutional network, then generates candidate temporal regions containing activities and finally classifies selected regions into specific activities. Computation is saved due to the sharing of convolutional features between the proposal and the classification pipelines. We further improve the detection performance by efficiently integrating an optical flow based motion stream with the original RGB stream. The two-stream network is jointly optimized by fusing the flow and RGB feature maps at different levels. Additionally, the training stage incorporates an online hard example mining strategy to address the extreme foreground-background imbalance typically observed in any detection pipeline. Instead of heuristically sampling the candidate segments for the final activity classification stage, we rank them according to their performance and only select the worst performers to update the model. This improves the model without heavy hyper-parameter tuning. Extensive experiments on three benchmark datasets are carried out to show superior performance over existing temporal activity detection methods. Our model achieves state-of-the-art results on the THUMOS'14 and Charades datasets. We further demonstrate that our model is a general temporal activity detection framework that does not rely on assumptions about particular dataset properties by evaluating our approach on the ActivityNet dataset.",0
"Our focus is on detecting temporal activity in continuous, untrimmed video streams. This is a challenging task that requires identifying meaningful spatio-temporal features to accurately locate the start and end times of each activity. We propose a novel model, the Region Convolutional 3D Network (R-C3D), which encodes the video streams using a three-dimensional fully convolutional network. The model generates temporal regions containing activities and classifies them into specific activities. Sharing convolutional features between the proposal and classification pipelines reduces computation. To improve detection performance, we integrate an optical flow-based motion stream with the original RGB stream and jointly optimize the two-stream network by fusing the flow and RGB feature maps. The training stage includes an online hard example mining strategy to address foreground-background imbalance. We rank candidate segments according to their performance and only select the worst performers to update the model, improving the model without requiring heavy hyper-parameter tuning. We conduct experiments on three benchmark datasets and achieve state-of-the-art results on the THUMOS'14 and Charades datasets. Our model is a general temporal activity detection framework that can be applied without assumptions about dataset properties, as demonstrated on the ActivityNet dataset.",1
"Anomaly detection in crowd videos has become a popular area of research for the computer vision community. Several existing methods generally perform a prior training about the scene with or without the use of labeled data. However, it is difficult to always guarantee the availability of prior data, especially, for scenarios like remote area surveillance. To address such challenge, we propose an adaptive training-less system capable of detecting anomaly on-the-fly while dynamically estimating and adjusting response based on certain parameters. This makes our system both training-less and adaptive in nature. Our pipeline consists of three main components, namely, adaptive 3D-DCT model for multi-object detection-based association, local motion structure description through saliency modulated optic flow, and anomaly detection based on earth movers distance (EMD). The proposed model, despite being training-free, is found to achieve comparable performance with several state-of-the-art methods on the publicly available UCSD, UMN, CHUK-Avenue and ShanghaiTech datasets.",0
"The detection of anomalies in crowd videos has become a popular research topic within the computer vision community. Existing methods typically involve prior training of the scene, with or without labeled data. However, it is often difficult to ensure the availability of prior data, particularly in remote surveillance scenarios. In response to this challenge, we have developed an adaptive training-less system that can detect anomalies on-the-fly, while dynamically estimating and adjusting the response based on specific parameters. This makes our system both adaptive and training-free. Our pipeline comprises three main components: an adaptive 3D-DCT model for multi-object detection-based association, local motion structure description via saliency modulated optic flow, and anomaly detection using earth movers distance (EMD). Despite its lack of training, our proposed model achieves comparable performance to several state-of-the-art methods on publicly available datasets such as UCSD, UMN, CHUK-Avenue, and ShanghaiTech.",1
"In this dissertation, I present my work towards exploring temporal information for better video understanding. Specifically, I have worked on two problems: action recognition and semantic segmentation. For action recognition, I have proposed a framework, termed hidden two-stream networks, to learn an optimal motion representation that does not require the computation of optical flow. My framework alleviates several challenges faced in video classification, such as learning motion representations, real-time inference, multi-framerate handling, generalizability to unseen actions, etc. For semantic segmentation, I have introduced a general framework that uses video prediction models to synthesize new training samples. By scaling up the training dataset, my trained models are more accurate and robust than previous models even without modifications to the network architectures or objective functions. I believe videos have much more potential to be mined, and temporal information is one of the most important cues for machines to perceive the visual world better.",0
"My dissertation focuses on enhancing video understanding by exploring temporal information. I have addressed two problems, namely action recognition and semantic segmentation. To tackle the former, I have devised a novel framework called hidden two-stream networks that facilitates optimal motion representation without the need for optical flow computation. This approach overcomes several video classification challenges such as multi-framerate handling, real-time inference, learning motion representations, and generalizability to unseen actions. In terms of semantic segmentation, I have proposed a comprehensive framework that utilizes video prediction models to generate new training samples. This approach has helped me to augment the size of the training dataset, resulting in more accurate and robust trained models that do not require any modifications to the network architectures or objective functions. I am convinced that there is enormous potential in mining videos, and temporal information is an invaluable cue that can significantly enhance machines' ability to perceive the visual world.",1
"Mathematical optimization is widely used in various research fields. With a carefully-designed objective function, mathematical optimization can be quite helpful in solving many problems. However, objective functions are usually hand-crafted and designing a good one can be quite challenging. In this paper, we propose a novel framework to learn the objective function based on a neural net-work. The basic idea is to consider the neural network as an objective function, and the input as an optimization variable. For the learning of objective function from the training data, two processes are conducted: In the inner process, the optimization variable (the input of the network) are optimized to minimize the objective function (the network output), while fixing the network weights. In the outer process, on the other hand, the weights are optimized based on how close the final solution of the inner process is to the desired solution. After learning the objective function, the solution for the test set is obtained in the same manner of the inner process. The potential and applicability of our approach are demonstrated by the experiments on toy examples and a computer vision task, optical flow.",0
"Mathematical optimization is a widely used tool in various research fields, and can be especially helpful in solving problems when a well-designed objective function is used. However, creating a good objective function can be difficult as they are typically crafted by hand. In this study, we introduce a unique framework for learning the objective function using a neural network. The neural network acts as the objective function, with the input serving as the optimization variable. Our process involves two steps: firstly, the optimization variable is optimized to minimize the network output while the network weights remain fixed. Secondly, the weights are optimized based on how closely the final solution of the first step matches the desired solution. After learning the objective function, the solution for the test set is obtained using the same process as the first step. We demonstrate the effectiveness and versatility of our approach through experiments on toy examples and a computer vision task, optical flow.",1
"Precipitation nowcasting is a short-range forecast of rain/snow (up to 2 hours), often displayed on top of the geographical map by the weather service. Modern precipitation nowcasting algorithms rely on the extrapolation of observations by ground-based radars via optical flow techniques or neural network models. Dependent on these radars, typical nowcasting is limited to the regions around their locations. We have developed a method for precipitation nowcasting based on geostationary satellite imagery and incorporated the resulting data into the Yandex.Weather precipitation map (including an alerting service with push notifications for products in the Yandex ecosystem), thus expanding its coverage and paving the way to a truly global nowcasting service.",0
"The prediction of rain or snow in the near future, known as precipitation nowcasting, is typically provided by weather services for up to two hours. This information is often displayed on top of a map to show the areas affected. Current methods rely on ground-based radars and use optical flow techniques or neural network models to extrapolate observations. However, the coverage of such systems is limited to the areas around the radar. To solve this issue, we have developed a new approach that uses geostationary satellite imagery. By incorporating this data into the Yandex.Weather precipitation map, we have expanded the coverage of the service, and created a truly global nowcasting service. This includes an alerting service with push notifications for products in the Yandex ecosystem.",1
"Stereo matching and flow estimation are two essential tasks for scene understanding, spatially in 3D and temporally in motion. Existing approaches have been focused on the unsupervised setting due to the limited resource to obtain the large-scale ground truth data. To construct a self-learnable objective, co-related tasks are often linked together to form a joint framework. However, the prior work usually utilizes independent networks for each task, thus not allowing to learn shared feature representations across models. In this paper, we propose a single and principled network to jointly learn spatiotemporal correspondence for stereo matching and flow estimation, with a newly designed geometric connection as the unsupervised signal for temporally adjacent stereo pairs. We show that our method performs favorably against several state-of-the-art baselines for both unsupervised depth and flow estimation on the KITTI benchmark dataset.",0
"Stereo matching and flow estimation are crucial for comprehending scenes in both 3D space and motion. Due to limited resources for obtaining large-scale ground truth data, previous approaches have concentrated on the unsupervised setting. To create a self-learnable objective, related tasks are often combined to form a joint framework. However, prior work has used separate networks for each task, preventing shared feature representations across models. This paper proposes a single and organized network that simultaneously learns spatiotemporal correspondence for stereo matching and flow estimation. A newly designed geometric connection serves as the unsupervised signal for temporally adjacent stereo pairs. The results show that our method outperforms several state-of-the-art baselines for both unsupervised depth and flow estimation on the KITTI benchmark dataset.",1
"Modern optical flow methods make use of salient scene feature points detected and matched within the scene as a basis for sparse-to-dense optical flow estimation. Current feature detectors however either give sparse, non uniform point clouds (resulting in flow inaccuracies) or lack the efficiency for frame-rate real-time applications. In this work we use the novel Dense Gradient Based Features (DeGraF) as the input to a sparse-to-dense optical flow scheme. This consists of three stages: 1) efficient detection of uniformly distributed Dense Gradient Based Features (DeGraF); 2) feature tracking via robust local optical flow; and 3) edge preserving flow interpolation to recover overall dense optical flow. The tunable density and uniformity of DeGraF features yield superior dense optical flow estimation compared to other popular feature detectors within this three stage pipeline. Furthermore, the comparable speed of feature detection also lends itself well to the aim of real-time optical flow recovery. Evaluation on established real-world benchmark datasets show test performance in an autonomous vehicle setting where DeGraF-Flow shows promising results in terms of accuracy with competitive computational efficiency among non-GPU based methods, including a marked increase in speed over the conceptually similar EpicFlow approach.",0
"Sparse-to-dense optical flow estimation in modern methods uses detected and matched salient scene feature points. However, the current feature detectors are either inefficient for real-time applications or result in flow inaccuracies due to non-uniform point clouds. This study introduces Dense Gradient Based Features (DeGraF) as a novel input to the sparse-to-dense optical flow scheme, consisting of three stages: efficient detection of uniformly distributed DeGraF features, feature tracking via robust local optical flow, and edge preserving flow interpolation for overall dense optical flow recovery. DeGraF features yield superior dense optical flow estimation with tunable density and uniformity, making it suitable for real-time optical flow recovery. Evaluation on established real-world benchmark datasets shows promising results in an autonomous vehicle setting, with DeGraF-Flow's accuracy and computational efficiency being comparable to non-GPU based methods, and faster than the similar EpicFlow approach.",1
"Video representation is a key challenge in many computer vision applications such as video classification, video captioning, and video surveillance. In this paper, we propose a novel approach for video representation that captures meaningful information including motion and appearance from a sequence of video frames and compacts it into a single image. To this end, we compute the optical flow and use it in a least squares optimization to find a new image, the so-called Flow Profile Image (FPI). This image encodes motions as well as foreground appearance information while background information is removed. The quality of this image is validated in activity recognition experiments and the results are compared with other video representation techniques such as dynamic images [1] and eigen images [2]. The experimental results as well as visual quality confirm that FPIs can be successfully used in video processing applications.",0
"Many computer vision applications, like video classification, video captioning, and video surveillance, face the challenge of video representation. This paper introduces a new method for video representation that captures essential information, such as motion and appearance, from a series of video frames and condenses it into one image called the Flow Profile Image (FPI). Using optical flow and least squares optimization, the FPI encodes motion and foreground appearance details while removing background information. We compared the FPIs to other video representation techniques, like dynamic images and eigen images, in activity recognition experiments. The results confirm that FPIs are a valuable tool in video processing applications, as they provide excellent visual quality and experimental results.",1
"Video inpainting, which aims at filling in missing regions of a video, remains challenging due to the difficulty of preserving the precise spatial and temporal coherence of video contents. In this work we propose a novel flow-guided video inpainting approach. Rather than filling in the RGB pixels of each frame directly, we consider video inpainting as a pixel propagation problem. We first synthesize a spatially and temporally coherent optical flow field across video frames using a newly designed Deep Flow Completion network. Then the synthesized flow field is used to guide the propagation of pixels to fill up the missing regions in the video. Specifically, the Deep Flow Completion network follows a coarse-to-fine refinement to complete the flow fields, while their quality is further improved by hard flow example mining. Following the guide of the completed flow, the missing video regions can be filled up precisely. Our method is evaluated on DAVIS and YouTube-VOS datasets qualitatively and quantitatively, achieving the state-of-the-art performance in terms of inpainting quality and speed.",0
"The task of video inpainting, which involves filling in gaps in a video, is still challenging because it is difficult to maintain the precise spatial and temporal coherence of the video content. To address this issue, we have proposed a new flow-guided video inpainting approach. Rather than filling in the RGB pixels of each frame directly, our method treats video inpainting as a pixel propagation problem. We first create a coherent optical flow field across video frames using a newly developed Deep Flow Completion network. We then use this synthesized flow field to guide the propagation of pixels to fill in the missing regions of the video. We refine the flow fields in a coarse-to-fine manner using the Deep Flow Completion network, and further enhance their quality through hard flow example mining. By following the guide of the completed flow, our method can accurately fill in the missing video regions. We have evaluated our approach on the DAVIS and YouTube-VOS datasets and achieved state-of-the-art performance in terms of both inpainting quality and speed.",1
"In this paper, we present a new inpainting framework for recovering missing regions of video frames. Compared with image inpainting, performing this task on video presents new challenges such as how to preserving temporal consistency and spatial details, as well as how to handle arbitrary input video size and length fast and efficiently. Towards this end, we propose a novel deep learning architecture which incorporates ConvLSTM and optical flow for modeling the spatial-temporal consistency in videos. It also saves much computational resource such that our method can handle videos with larger frame size and arbitrary length streamingly in real-time. Furthermore, to generate an accurate optical flow from corrupted frames, we propose a robust flow generation module, where two sources of flows are fed and a flow blending network is trained to fuse them. We conduct extensive experiments to evaluate our method in various scenarios and different datasets, both qualitatively and quantitatively. The experimental results demonstrate the superior of our method compared with the state-of-the-art inpainting approaches.",0
"A new framework for filling in missing regions of video frames is presented in this paper. The challenges of video inpainting are discussed, including preserving temporal consistency and spatial details, and handling arbitrary video size and length quickly and effectively. To address these challenges, a novel deep learning architecture is proposed that incorporates ConvLSTM and optical flow to model spatial-temporal consistency in videos. This architecture is computationally efficient, allowing for real-time processing of videos with larger frame sizes and arbitrary lengths. Additionally, a robust flow generation module is proposed to generate accurate optical flow from corrupted frames. Extensive experiments are conducted to evaluate the proposed method, demonstrating its superiority over state-of-the-art inpainting approaches.",1
"Motion has shown to be useful for video understanding, where motion is typically represented by optical flow. However, computing flow from video frames is very time-consuming. Recent works directly leverage the motion vectors and residuals readily available in the compressed video to represent motion at no cost. While this avoids flow computation, it also hurts accuracy since the motion vector is noisy and has substantially reduced resolution, which makes it a less discriminative motion representation. To remedy these issues, we propose a lightweight generator network, which reduces noises in motion vectors and captures fine motion details, achieving a more Discriminative Motion Cue (DMC) representation. Since optical flow is a more accurate motion representation, we train the DMC generator to approximate flow using a reconstruction loss and a generative adversarial loss, jointly with the downstream action classification task. Extensive evaluations on three action recognition benchmarks (HMDB-51, UCF-101, and a subset of Kinetics) confirm the effectiveness of our method. Our full system, consisting of the generator and the classifier, is coined as DMC-Net which obtains high accuracy close to that of using flow and runs two orders of magnitude faster than using optical flow at inference time.",0
"The use of motion has proven beneficial for comprehending videos, with optical flow being the typical representation. However, determining flow from video frames is a time-intensive process. Recent research has utilized readily available motion vectors and residuals from compressed videos as a cost-free substitute for flow computation. This approach sacrifices accuracy due to the noisy and low-resolution motion vector, resulting in a less effective motion representation. To address this, we suggest a lightweight generator network that minimizes noise in motion vectors and captures intricate motion details, producing a more Discriminative Motion Cue (DMC) representation. The DMC generator is trained to approximate flow using a reconstruction loss and a generative adversarial loss alongside the downstream action classification objective since optical flow is a more precise representation of motion. Our method is extensively evaluated on three action recognition benchmarks (HMDB-51, UCF-101, and a subset of Kinetics), confirming its effectiveness. Our complete system, comprising the generator and classifier, is named DMC-Net and achieves high accuracy similar to that of using flow while running two orders of magnitude faster at inference time.",1
"In this paper, we adapt the geodesic distance-based recursive filter to the sparse data interpolation problem. The proposed technique is general and can be easily applied to any kind of sparse data. We demonstrate the superiority over other interpolation techniques in three experiments for qualitative and quantitative evaluation.   In addition, we compare our method with the popular interpolation algorithm presented in the EpicFlow optical flow paper that is intuitively motivated by a similar geodesic distance principle. The comparison shows that our algorithm is more accurate and considerably faster than the EpicFlow interpolation technique.",0
"The geodesic distance-based recursive filter has been modified to address the sparse data interpolation issue in this study. This technique is versatile and can be utilized for any type of sparse data. Through three qualitative and quantitative experiments, we prove its superiority over other interpolation methods. Furthermore, we compare our approach with the geodesic distance-inspired EpicFlow optical flow paper's popular interpolation algorithm and reveal that our method is not only more precise but also faster.",1
"Multiview stereo aims to reconstruct scene depth from images acquired by a camera under arbitrary motion. Recent methods address this problem through deep learning, which can utilize semantic cues to deal with challenges such as textureless and reflective regions. In this paper, we present a convolutional neural network called DPSNet (Deep Plane Sweep Network) whose design is inspired by best practices of traditional geometry-based approaches for dense depth reconstruction. Rather than directly estimating depth and/or optical flow correspondence from image pairs as done in many previous deep learning methods, DPSNet takes a plane sweep approach that involves building a cost volume from deep features using the plane sweep algorithm, regularizing the cost volume via a context-aware cost aggregation, and regressing the dense depth map from the cost volume. The cost volume is constructed using a differentiable warping process that allows for end-to-end training of the network. Through the effective incorporation of conventional multiview stereo concepts within a deep learning framework, DPSNet achieves state-of-the-art reconstruction results on a variety of challenging datasets.",0
"The goal of multiview stereo is to determine the depth of a scene based on photos taken by a moving camera. Recent methods have utilized deep learning to overcome difficulties such as reflective and textureless areas by incorporating semantic cues. This article introduces DPSNet, a convolutional neural network that incorporates traditional geometry-based techniques for dense depth reconstruction. Instead of estimating depth and/or optical flow correspondence directly from image pairs like previous deep learning methods, DPSNet utilizes the plane sweep algorithm to create a cost volume from deep features. The cost volume is then regularized via context-aware cost aggregation and used to regress a dense depth map. The network is trained end-to-end using a differentiable warping process. By incorporating conventional multiview stereo concepts within a deep learning framework, DPSNet achieves exceptional results on challenging datasets.",1
"Processing and fusing information among multi-modal is a very useful technique for achieving high performance in many computer vision problems. In order to tackle multi-modal information more effectively, we introduce a novel framework for multi-modal fusion: Cross-modal Message Passing (CMMP). Specifically, we propose a cross-modal message passing mechanism to fuse two-stream network for action recognition, which composes of an appearance modal network (RGB image) and a motion modal (optical flow image) network. The objectives of individual networks in this framework are two-fold: a standard classification objective and a competing objective. The classification object ensures that each modal network predicts the true action category while the competing objective encourages each modal network to outperform the other one. We quantitatively show that the proposed CMMP fuses the traditional two-stream network more effectively, and outperforms all existing two-stream fusion method on UCF-101 and HMDB-51 datasets.",0
"The fusion of information across multiple modes is a highly effective technique for achieving superior performance in various computer vision tasks. To enhance the handling of multi-modal information, we present a new framework for multi-modal fusion called Cross-modal Message Passing (CMMP). Our approach includes a cross-modal message passing mechanism that fuses two-stream networks for action recognition, comprising an appearance modal network (RGB image) and a motion modal (optical flow image) network. The objectives of the individual networks in this framework are twofold: a standard classification objective and a competing objective. The classification objective ensures that each modal network accurately predicts the action category, while the competing objective incentivizes each modal network to outperform the other. Our quantitative results demonstrate that the proposed CMMP is more effective at fusing the traditional two-stream network and outperforms all existing two-stream fusion methods on UCF-101 and HMDB-51 datasets.",1
In this paper we present a self-supervised method for representation learning utilizing two different modalities. Based on the observation that cross-modal information has a high semantic meaning we propose a method to effectively exploit this signal. For our approach we utilize video data since it is available on a large scale and provides easily accessible modalities given by RGB and optical flow. We demonstrate state-of-the-art performance on highly contested action recognition datasets in the context of self-supervised learning. We show that our feature representation also transfers to other tasks and conduct extensive ablation studies to validate our core contributions. Code and model can be found at https://github.com/nawidsayed/Cross-and-Learn.,0
"This paper introduces a technique for self-supervised representation learning that involves utilizing two different modalities. The method is based on the idea that cross-modal information carries significant semantic meaning and proposes a strategy for effectively exploiting this signal. The approach makes use of video data as it is widely available and offers easily accessible modalities in the form of RGB and optical flow. The study demonstrates exceptional performance on competitive action recognition datasets within the self-supervised learning context. Additionally, the research shows that the feature representation can be applied to other tasks, and extensive ablation studies confirm the core contributions. Interested individuals can access the code and model at https://github.com/nawidsayed/Cross-and-Learn.",1
