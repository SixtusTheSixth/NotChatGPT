"Teaching plays a very important role in our society, by spreading human knowledge and educating our next generations. A good teacher will select appropriate teaching materials, impact suitable methodologies, and set up targeted examinations, according to the learning behaviors of the students. In the field of artificial intelligence, however, one has not fully explored the role of teaching, and pays most attention to machine \emph{learning}. In this paper, we argue that equal attention, if not more, should be paid to teaching, and furthermore, an optimization framework (instead of heuristics) should be used to obtain good teaching strategies. We call this approach `learning to teach'. In the approach, two intelligent agents interact with each other: a student model (which corresponds to the learner in traditional machine learning algorithms), and a teacher model (which determines the appropriate data, loss function, and hypothesis space to facilitate the training of the student model). The teacher model leverages the feedback from the student model to optimize its own teaching strategies by means of reinforcement learning, so as to achieve teacher-student co-evolution. To demonstrate the practical value of our proposed approach, we take the training of deep neural networks (DNN) as an example, and show that by using the learning to teach techniques, we are able to use much less training data and fewer iterations to achieve almost the same accuracy for different kinds of DNN models (e.g., multi-layer perceptron, convolutional neural networks and recurrent neural networks) under various machine learning tasks (e.g., image classification and text understanding).",0
"The act of teaching is crucial in our society as it spreads knowledge and educates future generations. A competent teacher will carefully choose appropriate teaching materials, employ effective methodologies, and design targeted examinations based on the learning behaviors of their students. However, the role of teaching in the field of artificial intelligence has not been fully explored, with more emphasis placed on machine learning. This paper argues that equal attention should be given to teaching and proposes an optimization framework, called ""learning to teach,"" to develop effective teaching strategies. This approach involves two intelligent agents - a student model and a teacher model - that interact with each other. The teacher model uses reinforcement learning to optimize its teaching strategies based on feedback from the student model, resulting in teacher-student co-evolution. We demonstrate the practical application of this approach by using it to train deep neural networks. Our results show that using learning to teach techniques can achieve similar accuracy with less training data and fewer iterations for various types of DNN models, such as multi-layer perceptron, convolutional neural networks, and recurrent neural networks, for different machine learning tasks, including image classification and text understanding.",1
"Multi-shot pedestrian re-identification problem is at the core of surveillance video analysis. It matches two tracks of pedestrians from different cameras. In contrary to existing works that aggregate single frames features by time series model such as recurrent neural network, in this paper, we propose an interpretable reinforcement learning based approach to this problem. Particularly, we train an agent to verify a pair of images at each time. The agent could choose to output the result (same or different) or request another pair of images to verify (unsure). By this way, our model implicitly learns the difficulty of image pairs, and postpone the decision when the model does not accumulate enough evidence. Moreover, by adjusting the reward for unsure action, we can easily trade off between speed and accuracy. In three open benchmarks, our method are competitive with the state-of-the-art methods while only using 3% to 6% images. These promising results demonstrate that our method is favorable in both efficiency and performance.",0
"At the heart of surveillance video analysis lies the multi-shot pedestrian re-identification problem, which involves matching two pedestrian tracks from different cameras. In contrast to existing approaches that use time series models such as recurrent neural network to aggregate single frames features, this paper proposes an interpretable reinforcement learning-based solution. Specifically, the approach trains an agent to verify pairs of images at each time, with the option to output a result (same or different) or request additional image pairs (unsure). This enables the model to learn the difficulty of image pairs and delay decisions until sufficient evidence is accumulated. By adjusting the reward for unsure actions, the model can balance speed and accuracy. In three open benchmarks, the proposed method is competitive with state-of-the-art approaches while using only 3% to 6% of images, indicating its effectiveness and efficiency.",1
"Current reinforcement learning (RL) methods can successfully learn single tasks but often generalize poorly to modest perturbations in task domain or training procedure. In this work, we present a decoupled learning strategy for RL that creates a shared representation space where knowledge can be robustly transferred. We separate learning the task representation, the forward dynamics, the inverse dynamics and the reward function of the domain, and show that this decoupling improves performance within the task, transfers well to changes in dynamics and reward, and can be effectively used for online planning. Empirical results show good performance in both continuous and discrete RL domains.",0
"Although current reinforcement learning (RL) techniques can effectively learn individual tasks, they often struggle to adapt to slight variations in the task domain or training procedure. This study introduces a decoupled learning approach for RL that establishes a shared representation space capable of robustly transferring knowledge. By separating the learning of the task representation, forward and inverse dynamics, and reward function of the domain, we demonstrate that this decoupling enhances performance within the task, transfers well to changes in dynamics and reward, and can be used for online planning. Our empirical findings exhibit favorable outcomes in both continuous and discrete RL domains.",1
"How would you search for a unique, fashionable shoe that a friend wore and you want to buy, but you didn't take a picture? Existing approaches propose interactive image search as a promising venue. However, they either entrust the user with taking the initiative to provide informative feedback, or give all control to the system which determines informative questions to ask. Instead, we propose a mixed-initiative framework where both the user and system can be active participants, depending on whose initiative will be more beneficial for obtaining high-quality search results. We develop a reinforcement learning approach which dynamically decides which of three interaction opportunities to give to the user: drawing a sketch, providing free-form attribute feedback, or answering attribute-based questions. By allowing these three options, our system optimizes both the informativeness and exploration capabilities allowing faster image retrieval. We outperform three baselines on three datasets and extensive experimental settings.",0
"If you want to find a unique and fashionable shoe that a friend wore, but you didn't take a picture, how would you go about it? One approach is interactive image search, but existing methods either rely on the user to provide feedback or leave everything up to the system. We suggest a mixed-initiative framework where both the user and system can participate in the search, depending on who can contribute more to obtaining high-quality results. Our reinforcement learning approach dynamically determines which of three interaction opportunities to offer the user: sketching, providing attribute feedback, or answering attribute-based questions. By allowing these options, our system optimizes both informativeness and exploration for faster image retrieval. We outperform three baselines on three datasets and in various experimental settings.",1
"Deep Reinforcement Learning (RL) recently emerged as one of the most competitive approaches for learning in sequential decision making problems with fully observable environments, e.g., computer Go. However, very little work has been done in deep RL to handle partially observable environments. We propose a new architecture called Action-specific Deep Recurrent Q-Network (ADRQN) to enhance learning performance in partially observable domains. Actions are encoded by a fully connected layer and coupled with a convolutional observation to form an action-observation pair. The time series of action-observation pairs are then integrated by an LSTM layer that learns latent states based on which a fully connected layer computes Q-values as in conventional Deep Q-Networks (DQNs). We demonstrate the effectiveness of our new architecture in several partially observable domains, including flickering Atari games.",0
"One of the most competitive approaches to learning in fully observable environments, such as computer Go, is Deep Reinforcement Learning (RL). However, there has been little research in using deep RL to handle partially observable environments. To address this, we propose the Action-specific Deep Recurrent Q-Network (ADRQN) architecture to improve learning performance in partially observable domains. A fully connected layer encodes actions, which are then combined with a convolutional observation to create an action-observation pair. An LSTM layer integrates the time series of action-observation pairs and learns latent states, from which a fully connected layer computes Q-values, similar to conventional Deep Q-Networks (DQNs). We demonstrate the effectiveness of our new architecture in various partially observable domains, including flickering Atari games.",1
"For many applications with limited computation, communication, storage and energy resources, there is an imperative need of computer vision methods that could select an informative subset of the input video for efficient processing at or near real time. In the literature, there are two relevant groups of approaches: generating a trailer for a video or fast-forwarding while watching/processing the video. The first group is supported by video summarization techniques, which require processing of the entire video to select an important subset for showing to users. In the second group, current fast-forwarding methods depend on either manual control or automatic adaptation of playback speed, which often do not present an accurate representation and may still require processing of every frame. In this paper, we introduce FastForwardNet (FFNet), a reinforcement learning agent that gets inspiration from video summarization and does fast-forwarding differently. It is an online framework that automatically fast-forwards a video and presents a representative subset of frames to users on the fly. It does not require processing the entire video, but just the portion that is selected by the fast-forward agent, which makes the process very computationally efficient. The online nature of our proposed method also enables the users to begin fast-forwarding at any point of the video. Experiments on two real-world datasets demonstrate that our method can provide better representation of the input video with much less processing requirement.",0
"Computer vision methods that can efficiently process input video in real time are in high demand for applications with limited computation, communication, storage, and energy resources. There are two relevant approaches in the literature: generating a trailer for the video or fast-forwarding while watching/processing the video. Video summarization techniques are used in the first group, which involves processing the entire video to select an important subset for users. Current fast-forwarding methods in the second group often do not provide an accurate representation and may still require processing every frame. To address these limitations, we present FastForwardNet (FFNet), an online framework that automatically fast-forwards a video and presents a representative subset of frames to users in real time. Our method is computationally efficient as it only requires processing the portion of the video selected by the fast-forward agent. Additionally, users can begin fast-forwarding at any point in the video. Our experiments on two real-world datasets demonstrate that FFNet provides a better representation of the input video with significantly less processing requirement.",1
"Deep reinforcement learning has shown its success in game playing. However, 2.5D fighting games would be a challenging task to handle due to ambiguity in visual appearances like height or depth of the characters. Moreover, actions in such games typically involve particular sequential action orders, which also makes the network design very difficult. Based on the network of Asynchronous Advantage Actor-Critic (A3C), we create an OpenAI-gym-like gaming environment with the game of Little Fighter 2 (LF2), and present a novel A3C+ network for learning RL agents. The introduced model includes a Recurrent Info network, which utilizes game-related info features with recurrent layers to observe combo skills for fighting. In the experiments, we consider LF2 in different settings, which successfully demonstrates the use of our proposed model for learning 2.5D fighting games.",0
"Although deep reinforcement learning has proven to be effective in game playing, handling 2.5D fighting games is particularly challenging due to the visual ambiguity surrounding character height and depth. Additionally, the sequential nature of actions in these games makes designing a network even more difficult. To overcome these obstacles, we utilized the Asynchronous Advantage Actor-Critic (A3C) network to create a game environment similar to OpenAI-gym, using Little Fighter 2 (LF2). Our novel A3C+ network includes a Recurrent Info network that uses recurrent layers to observe combo skills for fighting while utilizing game-related info features. Through experiments with LF2 in various settings, we successfully demonstrate the effectiveness of our proposed model for learning to play 2.5D fighting games.",1
"In reinforcement learning, temporal difference (TD) is the most direct algorithm to learn the value function of a policy. For large or infinite state spaces, exact representations of the value function are usually not available, and it must be approximated by a function in some parametric family.   However, with \emph{nonlinear} parametric approximations (such as neural networks), TD is not guaranteed to converge to a good approximation of the true value function within the family, and is known to diverge even in relatively simple cases. TD lacks an interpretation as a stochastic gradient descent of an error between the true and approximate value functions, which would provide such guarantees.   We prove that approximate TD is a gradient descent provided the current policy is \emph{reversible}. This holds even with nonlinear approximations.   A policy with transition probabilities $P(s,s')$ between states is reversible if there exists a function $\mu$ over states such that $\frac{P(s,s')}{P(s',s)}=\frac{\mu(s')}{\mu(s)}$. In particular, every move can be undone with some probability. This condition is restrictive; it is satisfied, for instance, for a navigation problem in any unoriented graph.   In this case, approximate TD is exactly a gradient descent of the \emph{Dirichlet norm}, the norm of the difference of \emph{gradients} between the true and approximate value functions. The Dirichlet norm also controls the bias of approximate policy gradient. These results hold even with no decay factor ($\gamma=1$) and do not rely on contractivity of the Bellman operator, thus proving stability of TD even with $\gamma=1$ for reversible policies.",0
"Temporal difference (TD) is a popular algorithm used in reinforcement learning to learn the value function of a policy. However, when dealing with large or infinite state spaces, it becomes necessary to approximate the value function using a parametric function. Nonlinear parametric approximations such as neural networks can cause TD to diverge, making it difficult to obtain a good approximation of the true value function within the parametric family. This is because TD does not have an interpretation as a stochastic gradient descent of an error between the true and approximate value functions, which would provide guarantees. In this study, we demonstrate that approximate TD can be a gradient descent if the current policy is reversible, even with nonlinear approximations. A reversible policy is one in which every move can be undone with some probability. This condition is restrictive but is satisfied in some cases such as a navigation problem in any unoriented graph. In these instances, approximate TD is a gradient descent of the Dirichlet norm, which controls the bias of approximate policy gradient. These findings are significant as they prove the stability of TD even when the decay factor is equal to one and do not rely on the contractivity of the Bellman operator.",1
"Multimodal wearable sensor data classification plays an important role in ubiquitous computing and has a wide range of applications in scenarios from healthcare to entertainment. However, most existing work in this field employs domain-specific approaches and is thus ineffective in complex sit- uations where multi-modality sensor data are col- lected. Moreover, the wearable sensor data are less informative than the conventional data such as texts or images. In this paper, to improve the adapt- ability of such classification methods across differ- ent application domains, we turn this classification task into a game and apply a deep reinforcement learning scheme to deal with complex situations dynamically. Additionally, we introduce a selective attention mechanism into the reinforcement learn- ing scheme to focus on the crucial dimensions of the data. This mechanism helps to capture extra information from the signal and thus it is able to significantly improve the discriminative power of the classifier. We carry out several experiments on three wearable sensor datasets and demonstrate the competitive performance of the proposed approach compared to several state-of-the-art baselines.",0
"The classification of multimodal wearable sensor data is crucial in ubiquitous computing and has diverse applications ranging from healthcare to entertainment. However, the majority of existing research in this area employs domain-specific techniques and is ineffective in complex situations where multiple sensor data are collected. Additionally, wearable sensor data is less informative compared to conventional data like text or images. To enhance the adaptability of classification methods across various application domains, our paper presents a game-based approach using deep reinforcement learning to dynamically handle complex situations. Moreover, we introduce a selective attention mechanism into the reinforcement learning scheme to concentrate on critical data dimensions. This mechanism captures additional information from the signal and significantly enhances the classifier's discriminative power. We conducted several experiments on three wearable sensor datasets and demonstrated the competitive performance of our approach compared to several state-of-the-art baselines.",1
"Recently experience replay is widely used in various deep reinforcement learning (RL) algorithms, in this paper we rethink the utility of experience replay. It introduces a new hyper-parameter, the memory buffer size, which needs carefully tuning. However unfortunately the importance of this new hyper-parameter has been underestimated in the community for a long time. In this paper we did a systematic empirical study of experience replay under various function representations. We showcase that a large replay buffer can significantly hurt the performance. Moreover, we propose a simple O(1) method to remedy the negative influence of a large replay buffer. We showcase its utility in both simple grid world and challenging domains like Atari games.",0
"The use of experience replay has become widespread in deep reinforcement learning (RL) algorithms. However, this paper aims to reevaluate the significance of experience replay and its new hyper-parameter, the memory buffer size, which requires careful tuning. Unfortunately, the community has overlooked the importance of this hyper-parameter for a long time. This paper presents a systematic empirical study of experience replay using various function representations. Our findings show that a large replay buffer can significantly harm performance. To address this issue, we introduce a simple O(1) method to counteract the negative effects of a large replay buffer. We demonstrate the effectiveness of our proposed method in both simple grid world and challenging Atari games.",1
"We propose a metalearning approach for learning gradient-based reinforcement learning (RL) algorithms. The idea is to evolve a differentiable loss function, such that an agent, which optimizes its policy to minimize this loss, will achieve high rewards. The loss is parametrized via temporal convolutions over the agent's experience. Because this loss is highly flexible in its ability to take into account the agent's history, it enables fast task learning. Empirical results show that our evolved policy gradient algorithm (EPG) achieves faster learning on several randomized environments compared to an off-the-shelf policy gradient method. We also demonstrate that EPG's learned loss can generalize to out-of-distribution test time tasks, and exhibits qualitatively different behavior from other popular metalearning algorithms.",0
"Our proposal involves a metalearning strategy to acquire gradient-based reinforcement learning (RL) techniques. The approach centers on the creation of a differentiable loss function that, when optimized by an agent, leads to high rewards. This loss function is constructed using temporal convolutions applied to the agent's experience, resulting in a highly adaptable approach that facilitates quick task learning. Our experimental findings indicate that our evolved policy gradient algorithm (EPG) outperforms an off-the-shelf policy gradient method in terms of faster learning in various randomized environments. Furthermore, we demonstrate that EPG's learned loss can generalize to new tasks outside of its training data, displaying different behavior from other well-known metalearning algorithms.",1
"Standard model-free deep reinforcement learning (RL) algorithms sample a new initial state for each trial, allowing them to optimize policies that can perform well even in highly stochastic environments. However, problems that exhibit considerable initial state variation typically produce high-variance gradient estimates for model-free RL, making direct policy or value function optimization challenging. In this paper, we develop a novel algorithm that instead partitions the initial state space into ""slices"", and optimizes an ensemble of policies, each on a different slice. The ensemble is gradually unified into a single policy that can succeed on the whole state space. This approach, which we term divide-and-conquer RL, is able to solve complex tasks where conventional deep RL methods are ineffective. Our results show that divide-and-conquer RL greatly outperforms conventional policy gradient methods on challenging grasping, manipulation, and locomotion tasks, and exceeds the performance of a variety of prior methods. Videos of policies learned by our algorithm can be viewed at http://bit.ly/dnc-rl",0
"The standard model-free deep reinforcement learning algorithms optimize policies that can perform well in unpredictable environments by sampling a new initial state for each trial. However, problems with significant initial state variation produce high-variance gradient estimates for model-free RL, which makes direct policy or value function optimization a challenge. In this paper, we propose a new algorithm called divide-and-conquer RL that partitions the initial state space into slices and optimizes an ensemble of policies, each on a different slice. This ensemble is gradually merged into a single policy that can perform well on the entire state space. Our approach is effective in solving complex tasks where traditional deep RL methods are not successful. We have observed that our divide-and-conquer RL significantly outperforms conventional policy gradient methods in grasping, manipulation, and locomotion tasks. Additionally, our algorithm's learned policies can be viewed in videos available at http://bit.ly/dnc-rl.",1
"In deep reinforcement learning (RL) tasks, an efficient exploration mechanism should be able to encourage an agent to take actions that lead to less frequent states which may yield higher accumulative future return. However, both knowing about the future and evaluating the frequentness of states are non-trivial tasks, especially for deep RL domains, where a state is represented by high-dimensional image frames. In this paper, we propose a novel informed exploration framework for deep RL, where we build the capability for an RL agent to predict over the future transitions and evaluate the frequentness for the predicted future frames in a meaningful manner. To this end, we train a deep prediction model to predict future frames given a state-action pair, and a convolutional autoencoder model to hash over the seen frames. In addition, to utilize the counts derived from the seen frames to evaluate the frequentness for the predicted frames, we tackle the challenge of matching the predicted future frames and their corresponding seen frames at the latent feature level. In this way, we derive a reliable metric for evaluating the novelty of the future direction pointed by each action, and hence inform the agent to explore the least frequent one.",0
"Efficient exploration mechanisms in deep reinforcement learning (RL) tasks require agents to take actions that result in less frequent states, which can lead to higher accumulative future returns. However, this is challenging, particularly in deep RL domains where states are represented by high-dimensional image frames and predicting the future and evaluating state frequentness is difficult. In this study, we propose an informed exploration framework for deep RL, where RL agents can predict future transitions and evaluate frequentness for predicted future frames. Our approach trains a deep prediction model to predict future frames and a convolutional autoencoder model to hash seen frames. To evaluate frequentness for predicted frames, we match predicted and seen frames at the latent feature level, providing a reliable metric for determining the novelty of future directions and encouraging agents to explore the least frequent one.",1
"Approximate linear programming (ALP) represents one of the major algorithmic families to solve large-scale Markov decision processes (MDP). In this work, we study a primal-dual formulation of the ALP, and develop a scalable, model-free algorithm called bilinear $\pi$ learning for reinforcement learning when a sampling oracle is provided. This algorithm enjoys a number of advantages. First, it adopts (bi)linear models to represent the high-dimensional value function and state-action distributions, using given state and action features. Its run-time complexity depends on the number of features, not the size of the underlying MDPs. Second, it operates in a fully online fashion without having to store any sample, thus having minimal memory footprint. Third, we prove that it is sample-efficient, solving for the optimal policy to high precision with a sample complexity linear in the dimension of the parameter space.",0
"The Approximate Linear Programming (ALP) algorithm is a significant approach to solving large-scale Markov decision processes (MDP). This study examines a primal-dual formulation of ALP and introduces a model-free algorithm called bilinear $\pi$ learning for reinforcement learning with a sampling oracle. This algorithm has several benefits. It uses (bi)linear models to represent the high-dimensional value function and state-action distributions, which rely on the given state and action features. Its runtime complexity is dependent on the number of features rather than the MDP's size. Additionally, it operates entirely online and requires no sample storage, resulting in a minimal memory footprint. Finally, the algorithm is sample-efficient and can achieve optimal policy with high precision, with a sample complexity that is linear in the dimension of the parameter space.",1
Policy gradient methods are widely used in reinforcement learning algorithms to search for better policies in the parameterized policy space. They do gradient search in the policy space and are known to converge very slowly. Nesterov developed an accelerated gradient search algorithm for convex optimization problems. This has been recently extended for non-convex and also stochastic optimization. We use Nesterov's acceleration for policy gradient search in the well-known actor-critic algorithm and show the convergence using ODE method. We tested this algorithm on a scheduling problem. Here an incoming job is scheduled into one of the four queues based on the queue lengths. We see from experimental results that algorithm using Nesterov's acceleration has significantly better performance compared to algorithm which do not use acceleration. To the best of our knowledge this is the first time Nesterov's acceleration has been used with actor-critic algorithm.,0
"Reinforcement learning algorithms often utilize policy gradient methods to explore parameterized policy spaces in search of better policies. These methods involve gradient searches in the policy space, but they are notorious for their slow convergence rates. Nesterov's accelerated gradient search algorithm was initially developed for convex optimization problems, but it has since been extended to non-convex and stochastic optimization. We have implemented Nesterov's acceleration in the actor-critic algorithm for policy gradient search and demonstrated its convergence using the ODE method. Our algorithm was tested on a scheduling problem where jobs are assigned to one of four queues based on queue lengths. Our experimental results show that the algorithm using Nesterov's acceleration outperforms those without acceleration. To our knowledge, this is the first instance of Nesterov's acceleration being applied to the actor-critic algorithm.",1
"In this report, we present a new reinforcement learning (RL) benchmark based on the Sonic the Hedgehog (TM) video game franchise. This benchmark is intended to measure the performance of transfer learning and few-shot learning algorithms in the RL domain. We also present and evaluate some baseline algorithms on the new benchmark.",0
"Our report introduces a novel benchmark for reinforcement learning (RL) that utilizes the Sonic the Hedgehog (TM) video game series. The benchmark's primary goal is to assess the capabilities of transfer learning and few-shot learning algorithms in the RL domain. Additionally, we assess several baseline algorithms' performance on this new benchmark.",1
"Projective simulation (PS) is a model for intelligent agents with a deliberation capacity that is based on episodic memory. The model has been shown to provide a flexible framework for constructing reinforcement-learning agents, and it allows for quantum mechanical generalization, which leads to a speed-up in deliberation time. PS agents have been applied successfully in the context of complex skill learning in robotics, and in the design of state-of-the-art quantum experiments. In this paper, we study the performance of projective simulation in two benchmarking problems in navigation, namely the grid world and the mountain car problem. The performance of PS is compared to standard tabular reinforcement learning approaches, Q-learning and SARSA. Our comparison demonstrates that the performance of PS and standard learning approaches are qualitatively and quantitatively similar, while it is much easier to choose optimal model parameters in case of projective simulation, with a reduced computational effort of one to two orders of magnitude. Our results show that the projective simulation model stands out for its simplicity in terms of the number of model parameters, which makes it simple to set up the learning agent in unknown task environments.",0
"The model called Projective Simulation (PS) uses episodic memory to create intelligent agents with a deliberation capacity. This flexible framework is effective in constructing reinforcement-learning agents and allows for quantum mechanical generalization, leading to faster deliberation. PS has been successfully applied in complex skill learning for robotics and state-of-the-art quantum experiments. This study analyzes the performance of PS in two benchmarking problems in navigation - the grid world and the mountain car problem. PS is compared to standard tabular reinforcement learning approaches, Q-learning and SARSA, and the results show that PS and standard learning approaches perform similarly. However, choosing optimal model parameters is easier in PS, with a reduced computational effort of one to two orders of magnitude. The simplicity of the PS model in terms of the number of parameters makes it easy to set up the learning agent in unknown task environments.",1
"Deep Reinforcement Learning (deep RL) has made several breakthroughs in recent years in applications ranging from complex control tasks in unmanned vehicles to game playing. Despite their success, deep RL still lacks several important capacities of human intelligence, such as transfer learning, abstraction and interpretability. Deep Symbolic Reinforcement Learning (DSRL) seeks to incorporate such capacities to deep Q-networks (DQN) by learning a relevant symbolic representation prior to using Q-learning. In this paper, we propose a novel extension of DSRL, which we call Symbolic Reinforcement Learning with Common Sense (SRL+CS), offering a better balance between generalization and specialization, inspired by principles of common sense when assigning rewards and aggregating Q-values. Experiments reported in this paper show that SRL+CS learns consistently faster than Q-learning and DSRL, achieving also a higher accuracy. In the hardest case, where agents were trained in a deterministic environment and tested in a random environment, SRL+CS achieves nearly 100% average accuracy compared to DSRL's 70% and DQN's 50% accuracy. To the best of our knowledge, this is the first case of near perfect zero-shot transfer learning using Reinforcement Learning.",0
"In recent years, Deep Reinforcement Learning (deep RL) has achieved significant breakthroughs in various fields, from unmanned vehicle control to game playing. Despite its success, deep RL still lacks some crucial human intelligence capabilities such as transfer learning, abstraction, and interpretability. Deep Symbolic Reinforcement Learning (DSRL) aims to address these shortcomings by learning a relevant symbolic representation before using Q-learning in deep Q-networks (DQN). This paper proposes an innovative extension of DSRL, called Symbolic Reinforcement Learning with Common Sense (SRL+CS), that balances generalization and specialization better and incorporates common sense principles when assigning rewards and aggregating Q-values. Experimental results indicate that SRL+CS outperforms Q-learning and DSRL by consistently learning faster and achieving higher accuracy. In the most challenging scenario, where agents were trained in a deterministic environment and tested in a random environment, SRL+CS achieved nearly 100% average accuracy, compared to DSRL's 70% and DQN's 50%. This is the first instance of near-perfect zero-shot transfer learning using Reinforcement Learning, to the best of our knowledge.",1
"A critical and challenging problem in reinforcement learning is how to learn the state-action value function from the experience replay buffer and simultaneously keep sample efficiency and faster convergence to a high quality solution. In prior works, transitions are uniformly sampled at random from the replay buffer or sampled based on their priority measured by temporal-difference (TD) error. However, these approaches do not fully take into consideration the intrinsic characteristics of transition distribution in the state space and could result in redundant and unnecessary TD updates, slowing down the convergence of the learning procedure. To overcome this problem, we propose a novel state distribution-aware sampling method to balance the replay times for transitions with skew distribution, which takes into account both the occurrence frequencies of transitions and the uncertainty of state-action values. Consequently, our approach could reduce the unnecessary TD updates and increase the TD updates for state-action value with more uncertainty, making the experience replay more effective and efficient. Extensive experiments are conducted on both classic control tasks and Atari 2600 games based on OpenAI gym platform and the experimental results demonstrate the effectiveness of our approach in comparison with the standard DQN approach.",0
"Reinforcement learning faces a complex and critical issue regarding how to learn the state-action value function while maintaining both sample efficiency and faster convergence to a high-quality solution from the experience replay buffer. Previous research has used random uniform sampling or prioritized sampling based on temporal-difference error to select transitions from the replay buffer, but this can result in unnecessary TD updates and slow down the learning process. To tackle this problem, we propose a novel state distribution-aware sampling method that considers the occurrence frequencies of transitions and the uncertainty of state-action values to balance replay times for transitions with skewed distribution. This approach reduces redundant TD updates and increases updates for state-action values with more uncertainty, making the experience replay more efficient. We test our approach on classic control tasks and Atari 2600 games using the OpenAI gym platform, and the results demonstrate its effectiveness in comparison to the standard DQN approach.",1
"This work adopts the very successful distributional perspective on reinforcement learning and adapts it to the continuous control setting. We combine this within a distributed framework for off-policy learning in order to develop what we call the Distributed Distributional Deep Deterministic Policy Gradient algorithm, D4PG. We also combine this technique with a number of additional, simple improvements such as the use of $N$-step returns and prioritized experience replay. Experimentally we examine the contribution of each of these individual components, and show how they interact, as well as their combined contributions. Our results show that across a wide variety of simple control tasks, difficult manipulation tasks, and a set of hard obstacle-based locomotion tasks the D4PG algorithm achieves state of the art performance.",0
"The approach taken in this study involves utilizing the distributional perspective on reinforcement learning, which has been highly successful, and modifying it to fit the continuous control setting. This is accomplished by integrating it into a distributed framework for off-policy learning, creating the Distributed Distributional Deep Deterministic Policy Gradient algorithm (D4PG). Additional enhancements such as the implementation of $N$-step returns and prioritized experience replay were also incorporated. Through experimentation, the individual contributions of each technique were examined, as well as their combined impact. The results demonstrated that the D4PG algorithm outperformed other methods in a diverse range of control tasks, manipulation tasks, and obstacle-based locomotion tasks, establishing it as the leading approach.",1
"We consider the problem of representing collective behavior of large populations and predicting the evolution of a population distribution over a discrete state space. A discrete time mean field game (MFG) is motivated as an interpretable model founded on game theory for understanding the aggregate effect of individual actions and predicting the temporal evolution of population distributions. We achieve a synthesis of MFG and Markov decision processes (MDP) by showing that a special MFG is reducible to an MDP. This enables us to broaden the scope of mean field game theory and infer MFG models of large real-world systems via deep inverse reinforcement learning. Our method learns both the reward function and forward dynamics of an MFG from real data, and we report the first empirical test of a mean field game model of a real-world social media population.",0
"Our focus is on how to represent the behavior of large populations and predict the evolution of a population distribution over a discrete state space. To achieve this, we propose a discrete time mean field game (MFG) model that is based on game theory. This model enables us to understand how individual actions affect the population as a whole and predict how the distribution of the population will evolve over time. We combine MFG with Markov decision processes (MDP) to create a special MFG that is reducible to an MDP. This allows us to expand the reach of mean field game theory and use deep inverse reinforcement learning to infer MFG models of real-world systems. Our approach involves learning both the reward function and forward dynamics of an MFG from real data. We also present the first empirical test of a mean field game model of a real-world social media population.",1
"One of the most significant bottleneck in training large scale machine learning models on parameter server (PS) is the communication overhead, because it needs to frequently exchange the model gradients between the workers and servers during the training iterations. Gradient quantization has been proposed as an effective approach to reducing the communication volume. One key issue in gradient quantization is setting the number of bits for quantizing the gradients. Small number of bits can significantly reduce the communication overhead while hurts the gradient accuracies, and vise versa. An ideal quantization method would dynamically balance the communication overhead and model accuracy, through adjusting the number bits according to the knowledge learned from the immediate past training iterations. Existing methods, however, quantize the gradients either with fixed number of bits, or with predefined heuristic rules. In this paper we propose a novel adaptive quantization method within the framework of reinforcement learning. The method, referred to as MQGrad, formalizes the selection of quantization bits as actions in a Markov decision process (MDP) where the MDP states records the information collected from the past optimization iterations (e.g., the sequence of the loss function values). During the training iterations of a machine learning algorithm, MQGrad continuously updates the MDP state according to the changes of the loss function. Based on the information, MDP learns to select the optimal actions (number of bits) to quantize the gradients. Experimental results based on a benchmark dataset showed that MQGrad can accelerate the learning of a large scale deep neural network while keeping its prediction accuracies.",0
"The main challenge to training large scale machine learning models on parameter server (PS) is the communication overhead, which involves frequent exchange of model gradients between workers and servers during training iterations. To reduce communication volume, gradient quantization has been proposed as an effective approach. However, the number of bits used for quantizing the gradients is critical, as a small number of bits can significantly reduce communication overhead while compromising gradient accuracies, and vice versa. An ideal quantization method should dynamically balance communication overhead and model accuracy by adjusting the number of bits based on knowledge gained from past training iterations. Existing methods use fixed numbers of bits or predefined heuristic rules for gradient quantization. This paper proposes MQGrad, an adaptive quantization method based on reinforcement learning. MQGrad selects quantization bits as actions in a Markov decision process (MDP) using information collected from past optimization iterations (e.g., loss function values). During machine learning algorithm training iterations, MQGrad continuously updates the MDP state according to loss function changes, learning to select optimal actions (number of bits) for gradient quantization. Benchmark dataset experiments showed that MQGrad accelerates the learning of large scale deep neural networks while maintaining prediction accuracies.",1
"Recent years have witnessed significant progresses in deep Reinforcement Learning (RL). Empowered with large scale neural networks, carefully designed architectures, novel training algorithms and massively parallel computing devices, researchers are able to attack many challenging RL problems. However, in machine learning, more training power comes with a potential risk of more overfitting. As deep RL techniques are being applied to critical problems such as healthcare and finance, it is important to understand the generalization behaviors of the trained agents. In this paper, we conduct a systematic study of standard RL agents and find that they could overfit in various ways. Moreover, overfitting could happen ""robustly"": commonly used techniques in RL that add stochasticity do not necessarily prevent or detect overfitting. In particular, the same agents and learning algorithms could have drastically different test performance, even when all of them achieve optimal rewards during training. The observations call for more principled and careful evaluation protocols in RL. We conclude with a general discussion on overfitting in RL and a study of the generalization behaviors from the perspective of inductive bias.",0
"Deep Reinforcement Learning has made significant advancements in recent years thanks to large scale neural networks, innovative training algorithms, and parallel computing devices. However, the increased training power comes with the potential risk of overfitting, which is a concern in critical fields like healthcare and finance. To address this, we conducted a systematic study of standard RL agents and found that they can overfit in various ways, and this overfitting can happen even when commonly used techniques that add stochasticity are utilized. We observed that the same agents and learning algorithms can have drastically different test performance, even when they achieve optimal rewards during training. Therefore, more careful evaluation protocols are necessary in RL. We conclude with a discussion on overfitting in RL and a study of generalization behaviors from the perspective of inductive bias.",1
"Image segmentation needs both local boundary position information and global object context information. The performance of the recent state-of-the-art method, fully convolutional networks, reaches a bottleneck due to the neural network limit after balancing between the two types of information simultaneously in an end-to-end training style. To overcome this problem, we divide the semantic image segmentation into temporal subtasks. First, we find a possible pixel position of some object boundary; then trace the boundary at steps within a limited length until the whole object is outlined. We present the first deep reinforcement learning approach to semantic image segmentation, called DeepOutline, which outperforms other algorithms in Coco detection leaderboard in the middle and large size person category in Coco val2017 dataset. Meanwhile, it provides an insight into a divide and conquer way by reinforcement learning on computer vision problems.",0
"Both local boundary position information and global object context information are required for image segmentation. However, the fully convolutional networks, which are the most advanced method available, have reached a bottleneck. This is due to the neural network limit when balancing both types of information simultaneously in an end-to-end training approach. To solve this issue, we have divided the semantic image segmentation into temporal subtasks. First, we locate a possible pixel position of an object boundary and then trace the boundary at steps within a limited length until the entire object is outlined. Our solution, DeepOutline, is the first deep reinforcement learning approach to semantic image segmentation. It outperforms other algorithms in the Coco detection leaderboard, particularly in the middle and large size person category in Coco val2017 dataset. Furthermore, our approach provides insight into a divide and conquer strategy using reinforcement learning for computer vision problems.",1
"In real world systems, the predictions of deployed Machine Learned models affect the training data available to build subsequent models. This introduces a bias in the training data that needs to be addressed. Existing solutions to this problem attempt to resolve the problem by either casting this in the reinforcement learning framework or by quantifying the bias and re-weighting the loss functions. In this work, we develop a novel Adversarial Neural Network (ANN) model, an alternative approach which creates a representation of the data that is invariant to the bias. We take the Paid Search auction as our working example and ad display position features as the confounding features for this setting. We show the success of this approach empirically on both synthetic data as well as real world paid search auction data from a major search engine.",0
"The deployment of Machine Learned models in real world systems can affect subsequent model training data, creating a bias that requires attention. To address this issue, existing solutions employ reinforcement learning frameworks or re-weighting loss functions to quantify the bias. However, we propose a new approach using an Adversarial Neural Network (ANN) model that generates a bias-invariant data representation. Our study focuses on the Paid Search auction and ad display position features as confounding factors. Empirical evidence from synthetic and real world paid search auction data from a major search engine supports the effectiveness of our method.",1
"Very recently proximal policy optimization (PPO) algorithms have been proposed as first-order optimization methods for effective reinforcement learning. While PPO is inspired by the same learning theory that justifies trust region policy optimization (TRPO), PPO substantially simplifies algorithm design and improves data efficiency by performing multiple epochs of \emph{clipped policy optimization} from sampled data. Although clipping in PPO stands for an important new mechanism for efficient and reliable policy update, it may fail to adaptively improve learning performance in accordance with the importance of each sampled state. To address this issue, a new surrogate learning objective featuring an adaptive clipping mechanism is proposed in this paper, enabling us to develop a new algorithm, known as PPO-$\lambda$. PPO-$\lambda$ optimizes policies repeatedly based on a theoretical target for adaptive policy improvement. Meanwhile, destructively large policy update can be effectively prevented through both clipping and adaptive control of a hyperparameter $\lambda$ in PPO-$\lambda$, ensuring high learning reliability. PPO-$\lambda$ enjoys the same simple and efficient design as PPO. Empirically on several Atari game playing tasks and benchmark control tasks, PPO-$\lambda$ also achieved clearly better performance than PPO.",0
"Recently, first-order optimization methods for effective reinforcement learning have been proposed in the form of proximal policy optimization (PPO) algorithms. Inspired by the same learning theory that justifies trust region policy optimization (TRPO), PPO simplifies algorithm design and improves data efficiency by performing multiple epochs of clipped policy optimization from sampled data. While PPO's clipping mechanism is an important new feature for efficient and reliable policy update, it may not adaptively improve learning performance in accordance with the importance of each sampled state. To address this issue, a new algorithm known as PPO-$\lambda$ is proposed in this paper, which optimizes policies repeatedly based on a theoretical target for adaptive policy improvement. PPO-$\lambda$ prevents destructively large policy updates through both clipping and adaptive control of a hyperparameter $\lambda$, ensuring high learning reliability. PPO-$\lambda$ shares the same simple and efficient design as PPO, and empirically achieved better performance than PPO on several Atari game playing tasks and benchmark control tasks.",1
"Determinantal point processes (DPPs) are an important concept in random matrix theory and combinatorics. They have also recently attracted interest in the study of numerical methods for machine learning, as they offer an elegant ""missing link"" between independent Monte Carlo sampling and deterministic evaluation on regular grids, applicable to a general set of spaces. This is helpful whenever an algorithm explores to reduce uncertainty, such as in active learning, Bayesian optimization, reinforcement learning, and marginalization in graphical models. To draw samples from a DPP in practice, existing literature focuses on approximate schemes of low cost, or comparably inefficient exact algorithms like rejection sampling. We point out that, for many settings of relevance to machine learning, it is also possible to draw exact samples from DPPs on continuous domains. We start from an intuitive example on the real line, which is then generalized to multivariate real vector spaces. We also compare to previously studied approximations, showing that exact sampling, despite higher cost, can be preferable where precision is needed.",0
"Determinantal point processes (DPPs) are a significant concept in the fields of random matrix theory and combinatorics, as well as in the study of numerical methods for machine learning. They provide a valuable bridge between independent Monte Carlo sampling and deterministic evaluation on regular grids, making them useful in a variety of spaces. DPPs are particularly beneficial in algorithms that aim to reduce uncertainty, such as active learning, Bayesian optimization, reinforcement learning, and marginalization in graphical models. While existing literature on DPPs focuses on low-cost approximate schemes or inefficient exact algorithms like rejection sampling, we argue that it is possible to draw exact samples from DPPs on continuous domains in many machine learning settings. We provide an intuitive example on the real line, which is then extended to multivariate real vector spaces. We also demonstrate that, despite being more costly, exact sampling can be preferable to previously studied approximations when precision is necessary.",1
"Motivated by recent advance of machine learning using Deep Reinforcement Learning this paper proposes a modified architecture that produces more robust agents and speeds up the training process. Our architecture is based on Asynchronous Advantage Actor-Critic (A3C) algorithm where the total input dimensionality is halved by dividing the input into two independent streams. We use ViZDoom, 3D world software that is based on the classical first person shooter video game, Doom, as a test case. The experiments show that in comparison to single input agents, the proposed architecture succeeds to have the same playing performance and shows more robust behavior, achieving significant reduction in the number of training parameters of almost 30%.",0
"This paper introduces a modified architecture that enhances the robustness of agents and accelerates the training process, inspired by recent progress in Deep Reinforcement Learning. Our proposed architecture is based on the Asynchronous Advantage Actor-Critic (A3C) algorithm and divides the input into two independent streams, halving the total input dimensionality. To test our architecture, we utilize ViZDoom, a 3D world software based on the popular first-person shooter game, Doom. Our experiments demonstrate that the proposed architecture, with its reduced number of training parameters by almost 30%, achieves comparable playing performance to single input agents while exhibiting more resilient behavior.",1
"Learning-based color enhancement approaches typically learn to map from input images to retouched images. Most of existing methods require expensive pairs of input-retouched images or produce results in a non-interpretable way. In this paper, we present a deep reinforcement learning (DRL) based method for color enhancement to explicitly model the step-wise nature of human retouching process. We cast a color enhancement process as a Markov Decision Process where actions are defined as global color adjustment operations. Then we train our agent to learn the optimal global enhancement sequence of the actions. In addition, we present a 'distort-and-recover' training scheme which only requires high-quality reference images for training instead of input and retouched image pairs. Given high-quality reference images, we distort the images' color distribution and form distorted-reference image pairs for training. Through extensive experiments, we show that our method produces decent enhancement results and our DRL approach is more suitable for the 'distort-and-recover' training scheme than previous supervised approaches. Supplementary material and code are available at https://sites.google.com/view/distort-and-recover/",0
"Typically, learning-based color enhancement methods learn to transform input images into retouched images. However, most of these approaches either necessitate costly input-retouched image pairs or yield non-interpretable outcomes. This paper introduces a novel approach to color enhancement based on deep reinforcement learning (DRL), which models the incremental nature of human retouching. The color enhancement process is represented as a Markov Decision Process, where global color adjustment operations are classified as actions. Through training, our agent learns the optimal sequence of actions for global enhancement. Furthermore, we propose a 'distort-and-recover' training scheme that only requires high-quality reference images instead of input and retouched image pairs. By distorting the color distribution of reference images, we form distorted-reference image pairs for training. We demonstrate through extensive experiments that our approach yields satisfactory enhancement results and is more suitable for the 'distort-and-recover' training scheme than previous supervised methods. Supplementary material and code can be accessed at https://sites.google.com/view/distort-and-recover/.",1
"This paper presents an open-source enforcement learning toolkit named CytonRL (https://github.com/arthurxlw/cytonRL). The toolkit implements four recent advanced deep Q-learning algorithms from scratch using C++ and NVIDIA's GPU-accelerated libraries. The code is simple and elegant, owing to an open-source general-purpose neural network library named CytonLib. Benchmark shows that the toolkit achieves competitive performances on the popular Atari game of Breakout.",0
"A toolkit called CytonRL (https://github.com/arthurxlw/cytonRL), designed for enforcement learning and open-source, is presented in this paper. The toolkit has been developed using C++ and NVIDIA's GPU-accelerated libraries and includes four advanced deep Q-learning algorithms. The implementation is elegant and straightforward, thanks to the open-source neural network library CytonLib. The benchmark results indicate that the toolkit performs competitively on the popular Atari game Breakout.",1
"The recent popularity of deep neural networks (DNNs) has generated a lot of research interest in performing DNN-related computation efficiently. However, the primary focus is usually very narrow and limited to (i) inference -- i.e. how to efficiently execute already trained models and (ii) image classification networks as the primary benchmark for evaluation.   Our primary goal in this work is to break this myopic view by (i) proposing a new benchmark for DNN training, called TBD (TBD is short for Training Benchmark for DNNs), that uses a representative set of DNN models that cover a wide range of machine learning applications: image classification, machine translation, speech recognition, object detection, adversarial networks, reinforcement learning, and (ii) by performing an extensive performance analysis of training these different applications on three major deep learning frameworks (TensorFlow, MXNet, CNTK) across different hardware configurations (single-GPU, multi-GPU, and multi-machine). TBD currently covers six major application domains and eight different state-of-the-art models.   We present a new toolchain for performance analysis for these models that combines the targeted usage of existing performance analysis tools, careful selection of new and existing metrics and methodologies to analyze the results, and utilization of domain specific characteristics of DNN training. We also build a new set of tools for memory profiling in all three major frameworks; much needed tools that can finally shed some light on precisely how much memory is consumed by different data structures (weights, activations, gradients, workspace) in DNN training. By using our tools and methodologies, we make several important observations and recommendations on where the future research and optimization of DNN training should be focused.",0
"The recent popularity of deep neural networks (DNNs) has sparked research interest in efficient computation for DNN-related tasks. However, the research focus has been narrow, mainly on inference and image classification networks. In this work, we aim to broaden the perspective by proposing a new benchmark for DNN training, called TBD, which includes a representative set of DNN models covering various machine learning applications. We conduct an extensive performance analysis of training these models on three major deep learning frameworks across different hardware configurations. TBD currently covers six major application domains and eight state-of-the-art models. We introduce a new toolchain for performance analysis that utilizes existing and new metrics and methodologies to analyze results, as well as domain-specific characteristics of DNN training. We also present a new set of memory profiling tools for all three major frameworks. Our observations and recommendations highlight areas for future research and optimization of DNN training.",1
"We propose expected policy gradients (EPG), which unify stochastic policy gradients (SPG) and deterministic policy gradients (DPG) for reinforcement learning. Inspired by expected sarsa, EPG integrates across the action when estimating the gradient, instead of relying only on the action in the sampled trajectory. We establish a new general policy gradient theorem, of which the stochastic and deterministic policy gradient theorems are special cases. We also prove that EPG reduces the variance of the gradient estimates without requiring deterministic policies and, for the Gaussian case, with no computational overhead. Finally, we show that it is optimal in a certain sense to explore with a Gaussian policy such that the covariance is proportional to the exponential of the scaled Hessian of the critic with respect to the actions. We present empirical results confirming that this new form of exploration substantially outperforms DPG with the Ornstein-Uhlenbeck heuristic in four challenging MuJoCo domains.",0
"Our proposed method, Expected Policy Gradients (EPG), combines Stochastic Policy Gradients (SPG) and Deterministic Policy Gradients (DPG) for reinforcement learning. EPG is inspired by Expected SARSA and estimates the gradient by integrating across actions, rather than only relying on actions in the sampled trajectory. We introduce a new policy gradient theorem that encompasses both stochastic and deterministic policy gradient theorems as special cases. Furthermore, we demonstrate that EPG reduces the variance of gradient estimates, without necessitating deterministic policies and with no additional computational cost in the Gaussian case. We also prove that exploring with a Gaussian policy proportional to the exponential of the scaled Hessian of the critic with respect to actions is optimal in a certain sense. Finally, we present empirical evidence that our proposed exploration method outperforms DPG with the Ornstein-Uhlenbeck heuristic in four challenging MuJoCo domains.",1
In this work we introduce the application of black-box quantum control as an interesting rein- forcement learning problem to the machine learning community. We analyze the structure of the reinforcement learning problems arising in quantum physics and argue that agents parameterized by long short-term memory (LSTM) networks trained via stochastic policy gradients yield a general method to solving them. In this context we introduce a variant of the proximal policy optimization (PPO) algorithm called the memory proximal policy optimization (MPPO) which is based on this analysis. We then show how it can be applied to specific learning tasks and present results of nu- merical experiments showing that our method achieves state-of-the-art results for several learning tasks in quantum control with discrete and continouous control parameters.,0
"This work presents the use of black-box quantum control as a reinforcement learning problem for the machine learning community. The reinforcement learning problems in quantum physics are examined, and it is suggested that agents trained with stochastic policy gradients and parameterized by long short-term memory (LSTM) networks provide a general solution. To this end, a variant of the proximal policy optimization (PPO) algorithm, called memory proximal policy optimization (MPPO), is introduced. The MPPO algorithm is applied to specific learning tasks, and the results of numerical experiments demonstrate that our method achieves state-of-the-art results for several learning tasks in quantum control with both discrete and continuous control parameters.",1
"Human free-hand sketches have been studied in various contexts including sketch recognition, synthesis and fine-grained sketch-based image retrieval (FG-SBIR). A fundamental challenge for sketch analysis is to deal with drastically different human drawing styles, particularly in terms of abstraction level. In this work, we propose the first stroke-level sketch abstraction model based on the insight of sketch abstraction as a process of trading off between the recognizability of a sketch and the number of strokes used to draw it. Concretely, we train a model for abstract sketch generation through reinforcement learning of a stroke removal policy that learns to predict which strokes can be safely removed without affecting recognizability. We show that our abstraction model can be used for various sketch analysis tasks including: (1) modeling stroke saliency and understanding the decision of sketch recognition models, (2) synthesizing sketches of variable abstraction for a given category, or reference object instance in a photo, and (3) training a FG-SBIR model with photos only, bypassing the expensive photo-sketch pair collection step.",0
"The study of human free-hand sketches has been conducted in different areas such as sketch recognition, synthesis, and fine-grained sketch-based image retrieval. A major obstacle in sketch analysis is the wide range of drawing styles used by humans, particularly in terms of abstraction. This research introduces a stroke-level sketch abstraction model that considers sketch abstraction as a trade-off between sketch recognizability and the number of strokes used. Specifically, a model is trained to generate abstract sketches using reinforcement learning of a stroke removal policy that predicts which strokes can be safely removed without affecting recognizability. This abstraction model has various applications, including modeling stroke saliency, synthesizing sketches of different abstraction levels for a specific category, and training a FG-SBIR model without the need for photo-sketch pairs.",1
"Exploration is a fundamental aspect of Reinforcement Learning, typically implemented using stochastic action-selection. Exploration, however, can be more efficient if directed toward gaining new world knowledge. Visit-counters have been proven useful both in practice and in theory for directed exploration. However, a major limitation of counters is their locality. While there are a few model-based solutions to this shortcoming, a model-free approach is still missing. We propose $E$-values, a generalization of counters that can be used to evaluate the propagating exploratory value over state-action trajectories. We compare our approach to commonly used RL techniques, and show that using $E$-values improves learning and performance over traditional counters. We also show how our method can be implemented with function approximation to efficiently learn continuous MDPs. We demonstrate this by showing that our approach surpasses state of the art performance in the Freeway Atari 2600 game.",0
"Reinforcement Learning relies heavily on exploration, which is often accomplished through stochastic action-selection. However, exploration can be made more effective by focusing on acquiring new knowledge about the environment. Visit-counters are a proven method for achieving directed exploration, but they are limited in their effectiveness due to their locality. Although there are some model-based solutions to this issue, a model-free approach has not been developed yet. Our proposed solution is $E$-values, which are a more generalized version of counters. These values can be used to evaluate the exploratory value as it propagates through state-action trajectories. We have compared our approach to traditional RL techniques and have shown that using $E$-values leads to improved learning and performance compared to using traditional counters. We have also demonstrated how our method can be applied to function approximation to efficiently learn continuous MDPs. Our results show that our approach outperforms the state-of-the-art in the Freeway Atari 2600 game.",1
"We investigate a novel approach for image restoration by reinforcement learning. Unlike existing studies that mostly train a single large network for a specialized task, we prepare a toolbox consisting of small-scale convolutional networks of different complexities and specialized in different tasks. Our method, RL-Restore, then learns a policy to select appropriate tools from the toolbox to progressively restore the quality of a corrupted image. We formulate a step-wise reward function proportional to how well the image is restored at each step to learn the action policy. We also devise a joint learning scheme to train the agent and tools for better performance in handling uncertainty. In comparison to conventional human-designed networks, RL-Restore is capable of restoring images corrupted with complex and unknown distortions in a more parameter-efficient manner using the dynamically formed toolchain.",0
"Our study introduces a unique method of image restoration through reinforcement learning. In contrast to previous research that mainly focuses on training a single large network for a specific task, we create a toolbox containing various small-scale convolutional networks with different complexities and specializations. Using our approach, RL-Restore, we teach the system to select suitable tools from the toolbox to progressively improve the quality of a damaged image. To accomplish this, we develop a step-wise reward function that gauges the image's restoration progress and use it to train the action policy. Furthermore, we implement a joint learning mechanism that enables the agent and tools to manage uncertainty more effectively. Unlike traditional human-designed networks, RL-Restore utilizes a dynamically constructed toolchain to repair images that have complicated and unknown distortions, resulting in more efficient parameters.",1
"Teaching an agent to navigate in an unseen 3D environment is a challenging task, even in the event of simulated environments. To generalize to unseen environments, an agent needs to be robust to low-level variations (e.g. color, texture, object changes), and also high-level variations (e.g. layout changes of the environment). To improve overall generalization, all types of variations in the environment have to be taken under consideration via different level of data augmentation steps. To this end, we propose House3D, a rich, extensible and efficient environment that contains 45,622 human-designed 3D scenes of visually realistic houses, ranging from single-room studios to multi-storied houses, equipped with a diverse set of fully labeled 3D objects, textures and scene layouts, based on the SUNCG dataset (Song et.al.). The diversity in House3D opens the door towards scene-level augmentation, while the label-rich nature of House3D enables us to inject pixel- & task-level augmentations such as domain randomization (Toubin et. al.) and multi-task training. Using a subset of houses in House3D, we show that reinforcement learning agents trained with an enhancement of different levels of augmentations perform much better in unseen environments than our baselines with raw RGB input by over 8% in terms of navigation success rate. House3D is publicly available at http://github.com/facebookresearch/House3D.",0
"Teaching an agent to navigate through an unfamiliar 3D environment is a tough challenge, even in simulated environments. For the agent to adapt to unseen environments, it must be resistant to low-level and high-level variations like color, texture, object, and layout changes. Therefore, to improve overall adaptability, all types of variations in the environment have to be considered at different levels of data augmentation. To achieve this, we introduce House3D, a comprehensive and efficient environment containing 45,622 visually realistic 3D scenes of houses, from single-room studios to multi-storied houses, equipped with diverse, fully labeled 3D objects, textures, and scene layouts. The House3D's diversity allows for scene-level augmentation, while its label-rich nature enables pixel and task-level augmentations like domain randomization and multi-task training. Using a subset of House3D, we demonstrate that reinforcement learning agents trained with a variety of augmentations outperform our baselines with raw RGB input by over 8% in terms of navigation success rate. House3D is available for public use at http://github.com/facebookresearch/House3D.",1
"Hierarchical Modular Reinforcement Learning (HMRL), consists of 2 layered learning where Profit Sharing works to plan a prey position in the higher layer and Q-learning method trains the state-actions to the target in the lower layer. In this paper, we expanded HMRL to multi-target problem to take the distance between targets to the consideration. The function, called `AT field', can estimate the interests for an agent according to the distance between 2 agents and the advantage/disadvantage of the other agent. Moreover, the knowledge related to state-action rules is extracted by C4.5. The action under the situation is decided by using the acquired knowledge. To verify the effectiveness of proposed method, some experimental results are reported.",0
"The concept of Hierarchical Modular Reinforcement Learning (HMRL) involves a two-layered learning approach, wherein Profit Sharing is utilized to strategize the position of the prey in the higher layer, while the Q-learning method trains the state-actions to the target in the lower layer. Our research aims to extend HMRL to address multi-target problems by considering the distance between targets. To achieve this, we introduced a new function, referred to as the `AT field,' that estimates the interests of an agent based on the distance between two agents and the relative advantage/disadvantage of the other agent. Additionally, we leveraged the C4.5 algorithm to extract knowledge related to state-action rules, which is then used to make decisions regarding actions in a given situation. We present experimental results to demonstrate the effectiveness of our proposed method.",1
"Dialogue assistants are rapidly becoming an indispensable daily aid. To avoid the significant effort needed to hand-craft the required dialogue flow, the Dialogue Management (DM) module can be cast as a continuous Markov Decision Process (MDP) and trained through Reinforcement Learning (RL). Several RL models have been investigated over recent years. However, the lack of a common benchmarking framework makes it difficult to perform a fair comparison between different models and their capability to generalise to different environments. Therefore, this paper proposes a set of challenging simulated environments for dialogue model development and evaluation. To provide some baselines, we investigate a number of representative parametric algorithms, namely deep reinforcement learning algorithms - DQN, A2C and Natural Actor-Critic and compare them to a non-parametric model, GP-SARSA. Both the environments and policy models are implemented using the publicly available PyDial toolkit and released on-line, in order to establish a testbed framework for further experiments and to facilitate experimental reproducibility.",0
"Dialogue assistants are quickly becoming a vital tool for daily use. However, creating the necessary dialogue flow can be a significant effort. To alleviate this, the Dialogue Management (DM) module can be transformed into a continuous Markov Decision Process (MDP) and trained using Reinforcement Learning (RL). Over the years, various RL models have been explored, but the lack of a standard benchmarking framework makes it challenging to compare different models and their ability to adapt to different environments. Therefore, this study suggests a series of challenging simulated environments to develop and assess dialogue models. To provide a baseline, representative parametric algorithms, such as DQN, A2C, and Natural Actor-Critic, and a non-parametric model, GP-SARSA, were investigated. The on-line release of both the environments and policy models using the publicly accessible PyDial toolkit establishes a testbed framework for further experimentation and facilitates experimental reproducibility.",1
"A robot that can carry out a natural-language instruction has been a dream since before the Jetsons cartoon series imagined a life of leisure mediated by a fleet of attentive robot helpers. It is a dream that remains stubbornly distant. However, recent advances in vision and language methods have made incredible progress in closely related areas. This is significant because a robot interpreting a natural-language navigation instruction on the basis of what it sees is carrying out a vision and language process that is similar to Visual Question Answering. Both tasks can be interpreted as visually grounded sequence-to-sequence translation problems, and many of the same methods are applicable. To enable and encourage the application of vision and language methods to the problem of interpreting visually-grounded navigation instructions, we present the Matterport3D Simulator -- a large-scale reinforcement learning environment based on real imagery. Using this simulator, which can in future support a range of embodied vision and language tasks, we provide the first benchmark dataset for visually-grounded natural language navigation in real buildings -- the Room-to-Room (R2R) dataset.",0
"For decades, people have dreamed of a robot that could understand and carry out instructions given in natural language. This idea was popularized in shows like The Jetsons, where robots assisted humans with daily tasks. Unfortunately, this dream has yet to become a reality. However, recent advancements in language and vision technologies have made significant strides in related areas. These advancements are particularly important for tasks like natural-language navigation, which require robots to interpret instructions based on what they see. This process is similar to Visual Question Answering, as both tasks involve visually-grounded sequence-to-sequence translation problems. To encourage the application of these methods to natural-language navigation, the Matterport3D Simulator has been developed. This reinforcement learning environment uses real imagery and can support a variety of embodied vision and language tasks. The Room-to-Room (R2R) dataset is the first benchmark dataset for visually-grounded natural language navigation in real buildings and was created using this simulator.",1
"In 2015, Google's DeepMind announced an advancement in creating an autonomous agent based on deep reinforcement learning (DRL) that could beat a professional player in a series of 49 Atari games. However, the current manifestation of DRL is still immature, and has significant drawbacks. One of DRL's imperfections is its lack of ""exploration"" during the training process, especially when working with high-dimensional problems. In this paper, we propose a mixed strategy approach that mimics behaviors of human when interacting with environment, and create a ""thinking"" agent that allows for more efficient exploration in the DRL training process. The simulation results based on the Breakout game show that our scheme achieves a higher probability of obtaining a maximum score than does the baseline DRL algorithm, i.e., the asynchronous advantage actor-critic method. The proposed scheme therefore can be applied effectively to solving a complicated task in a real-world application.",0
"Google's DeepMind made an announcement in 2015 about developing an independent agent utilizing deep reinforcement learning (DRL) that could outperform a professional player in 49 Atari games. However, DRL's current form is still in its early stages and has some significant limitations, such as insufficient exploration during the training process, especially when working with high-dimensional problems. In this article, we suggest a mixed strategy approach that mimics human behavior when interacting with the environment, resulting in a ""thinking"" agent that can explore more efficiently during the DRL training process. Our simulation results, based on the Breakout game, demonstrate that our approach has a higher chance of achieving the maximum score than the baseline DRL algorithm, the asynchronous advantage actor-critic method. As a result, our proposed scheme can be effectively applied to solve complex real-world challenges.",1
"A key challenge in complex visuomotor control is learning abstract representations that are effective for specifying goals, planning, and generalization. To this end, we introduce universal planning networks (UPN). UPNs embed differentiable planning within a goal-directed policy. This planning computation unrolls a forward model in a latent space and infers an optimal action plan through gradient descent trajectory optimization. The plan-by-gradient-descent process and its underlying representations are learned end-to-end to directly optimize a supervised imitation learning objective. We find that the representations learned are not only effective for goal-directed visual imitation via gradient-based trajectory optimization, but can also provide a metric for specifying goals using images. The learned representations can be leveraged to specify distance-based rewards to reach new target states for model-free reinforcement learning, resulting in substantially more effective learning when solving new tasks described via image-based goals. We were able to achieve successful transfer of visuomotor planning strategies across robots with significantly different morphologies and actuation capabilities.",0
"The challenge of complex visuomotor control involves acquiring abstract representations that are useful for planning, goal specification, and generalization. To address this challenge, we present universal planning networks (UPN), which integrate differentiable planning into a goal-directed policy. This planning computation utilizes a latent space forward model to generate an optimal action plan through gradient descent trajectory optimization. The plan-by-gradient-descent process and its underlying representations are trained end-to-end to optimize a supervised imitation learning objective. The learned representations are effective for gradient-based trajectory optimization in goal-directed visual imitation and can also serve as a metric for image-based goal specification. Moreover, the representations can be utilized to define distance-based rewards for model-free reinforcement learning, leading to more effective learning when tackling new tasks that involve image-based goals. We were able to successfully transfer visuomotor planning strategies across robots with different morphologies and actuation capabilities.",1
"To ensure stability of learning, state-of-the-art generalized policy iteration algorithms augment the policy improvement step with a trust region constraint bounding the information loss. The size of the trust region is commonly determined by the Kullback-Leibler (KL) divergence, which not only captures the notion of distance well but also yields closed-form solutions. In this paper, we consider a more general class of f-divergences and derive the corresponding policy update rules. The generic solution is expressed through the derivative of the convex conjugate function to f and includes the KL solution as a special case. Within the class of f-divergences, we further focus on a one-parameter family of $\alpha$-divergences to study effects of the choice of divergence on policy improvement. Previously known as well as new policy updates emerge for different values of $\alpha$. We show that every type of policy update comes with a compatible policy evaluation resulting from the chosen f-divergence. Interestingly, the mean-squared Bellman error minimization is closely related to policy evaluation with the Pearson $\chi^2$-divergence penalty, while the KL divergence results in the soft-max policy update and a log-sum-exp critic. We carry out asymptotic analysis of the solutions for different values of $\alpha$ and demonstrate the effects of using different divergence functions on a multi-armed bandit problem and on common standard reinforcement learning problems.",0
"Cutting-edge generalized policy iteration algorithms utilize trust region constraints to limit information loss during the policy improvement step, ensuring stable learning. The Kullback-Leibler (KL) divergence is commonly used to determine the size of the trust region due to its ability to measure distance and provide closed-form solutions. However, this paper explores a broader range of f-divergences and their corresponding policy update rules, which can be expressed through the derivative of the convex conjugate function to f and include the KL solution as a special case. The focus is on a one-parameter family of $\alpha$-divergences, which generate different policy updates and evaluations. The Pearson $\chi^2$-divergence penalty is closely related to mean-squared Bellman error minimization, while the KL divergence results in a soft-max policy update and a log-sum-exp critic. The effects of using different divergence functions are analyzed via asymptotic analysis and demonstrated in various reinforcement learning problems.",1
"All reinforcement learning algorithms must handle the trade-off between exploration and exploitation. Many state-of-the-art deep reinforcement learning methods use noise in the action selection, such as Gaussian noise in policy gradient methods or $\epsilon$-greedy in Q-learning. While these methods are appealing due to their simplicity, they do not explore the state space in a methodical manner. We present an approach that uses a model to derive reward bonuses as a means of intrinsic motivation to improve model-free reinforcement learning. A key insight of our approach is that this dynamics model can be learned in the latent feature space of a value function, representing the dynamics of the agent and the environment. This method is both theoretically grounded and computationally advantageous, permitting the efficient use of Bayesian information-theoretic methods in high-dimensional state spaces. We evaluate our method on several continuous control tasks, focusing on improving exploration.",0
"Reinforcement learning algorithms all face the challenge of balancing exploration and exploitation. While some cutting-edge deep reinforcement learning techniques rely on action selection noise like Gaussian noise in policy gradient methods or $\epsilon$-greedy in Q-learning, these methods are not systematic in exploring the state space. Our proposal offers an alternative that employs a model to generate reward bonuses serving as intrinsic motivation for enhancing model-free reinforcement learning. The crux of our approach is that the dynamics model can be acquired in the latent feature space of a value function that illustrates the interaction between the agent and the environment. This method is both theoretically sound and computationally advantageous, allowing for the efficient use of Bayesian information-theoretic methods in high-dimensional state spaces. We demonstrate the effectiveness of our method in improving exploration by evaluating it on several continuous control tasks.",1
"Advances in deep generative networks have led to impressive results in recent years. Nevertheless, such models can often waste their capacity on the minutiae of datasets, presumably due to weak inductive biases in their decoders. This is where graphics engines may come in handy since they abstract away low-level details and represent images as high-level programs. Current methods that combine deep learning and renderers are limited by hand-crafted likelihood or distance functions, a need for large amounts of supervision, or difficulties in scaling their inference algorithms to richer datasets. To mitigate these issues, we present SPIRAL, an adversarially trained agent that generates a program which is executed by a graphics engine to interpret and sample images. The goal of this agent is to fool a discriminator network that distinguishes between real and rendered data, trained with a distributed reinforcement learning setup without any supervision. A surprising finding is that using the discriminator's output as a reward signal is the key to allow the agent to make meaningful progress at matching the desired output rendering. To the best of our knowledge, this is the first demonstration of an end-to-end, unsupervised and adversarial inverse graphics agent on challenging real world (MNIST, Omniglot, CelebA) and synthetic 3D datasets.",0
"Recent years have seen significant advancements in deep generative networks. However, these models can sometimes focus too much on minor details in datasets due to their decoders lacking strong inductive biases. Graphics engines could be useful in solving this problem by representing images as high-level programs that abstract away low-level details. Unfortunately, current methods that combine deep learning and renderers have limitations, such as requiring large amounts of supervision or facing difficulties in scaling their inference algorithms. To address these issues, we have developed SPIRAL, an adversarially trained agent that generates a program executed by a graphics engine to interpret and sample images. The agent's objective is to deceive a discriminator network that distinguishes between real and rendered data, trained using a distributed reinforcement learning setup without any supervision. Interestingly, using the discriminator's output as a reward signal was found to be crucial for the agent to make meaningful progress in rendering output. Our work is the first demonstration of an end-to-end, unsupervised, and adversarial inverse graphics agent on challenging real-world and synthetic datasets including MNIST, Omniglot, CelebA, and 3D images.",1
"In this paper, we present an online reinforcement learning algorithm, called Renewal Monte Carlo (RMC), for infinite horizon Markov decision processes with a designated start state. RMC is a Monte Carlo algorithm and retains the advantages of Monte Carlo methods including low bias, simplicity, and ease of implementation while, at the same time, circumvents their key drawbacks of high variance and delayed (end of episode) updates. The key ideas behind RMC are as follows. First, under any reasonable policy, the reward process is ergodic. So, by renewal theory, the performance of a policy is equal to the ratio of expected discounted reward to the expected discounted time over a regenerative cycle. Second, by carefully examining the expression for performance gradient, we propose a stochastic approximation algorithm that only requires estimates of the expected discounted reward and discounted time over a regenerative cycle and their gradients. We propose two unbiased estimators for evaluating performance gradients---a likelihood ratio based estimator and a simultaneous perturbation based estimator---and show that for both estimators, RMC converges to a locally optimal policy. We generalize the RMC algorithm to post-decision state models and also present a variant that converges faster to an approximately optimal policy. We conclude by presenting numerical experiments on a randomly generated MDP, event-triggered communication, and inventory management.",0
"The article introduces Renewal Monte Carlo (RMC), an online reinforcement learning algorithm that deals with infinite horizon Markov decision processes that have a specific start state. RMC is a Monte Carlo algorithm that has low bias, simplicity, and ease of implementation, while overcoming the high variance and delayed updates drawbacks of Monte Carlo methods. RMC's approach is based on two key ideas. Firstly, the reward process is ergodic for any policy, and a policy's performance is the ratio of the expected discounted reward to the expected discounted time over a regenerative cycle. Secondly, the performance gradient expression is examined, and a stochastic approximation algorithm is proposed that only requires estimates of the expected discounted reward and discounted time over a regenerative cycle and their gradients. Two unbiased estimators are proposed for performance gradients, and it is shown that RMC converges to a locally optimal policy for both estimators. The article also presents variations of the RMC algorithm and concludes with numerical experiments on different scenarios.",1
"Stochastic neural net weights are used in a variety of contexts, including regularization, Bayesian neural nets, exploration in reinforcement learning, and evolution strategies. Unfortunately, due to the large number of weights, all the examples in a mini-batch typically share the same weight perturbation, thereby limiting the variance reduction effect of large mini-batches. We introduce flipout, an efficient method for decorrelating the gradients within a mini-batch by implicitly sampling pseudo-independent weight perturbations for each example. Empirically, flipout achieves the ideal linear variance reduction for fully connected networks, convolutional networks, and RNNs. We find significant speedups in training neural networks with multiplicative Gaussian perturbations. We show that flipout is effective at regularizing LSTMs, and outperforms previous methods. Flipout also enables us to vectorize evolution strategies: in our experiments, a single GPU with flipout can handle the same throughput as at least 40 CPU cores using existing methods, equivalent to a factor-of-4 cost reduction on Amazon Web Services.",0
"Stochastic neural net weights have diverse applications, such as regularization, Bayesian neural nets, exploration in reinforcement learning, and evolution strategies. However, the vast number of weights in a mini-batch constrains the variance reduction effect of large mini-batches, as all examples in a mini-batch typically share the same weight perturbation. To address this issue, we propose flipout, an efficient method that implicitly samples pseudo-independent weight perturbations for each example, thus decorrelating the gradients within a mini-batch. Empirical evidence indicates that flipout achieves the ideal linear variance reduction for fully connected networks, convolutional networks, and RNNs. Moreover, it speeds up training neural networks with multiplicative Gaussian perturbations, and is effective at regularizing LSTMs, surpassing previous approaches. Additionally, flipout facilitates the vectorization of evolution strategies, enabling a single GPU with flipout to handle the same throughput as at least 40 CPU cores using existing methods, resulting in a four-fold cost reduction on Amazon Web Services.",1
"The problem of object localization and recognition on autonomous mobile robots is still an active topic. In this context, we tackle the problem of learning a model of visual saliency directly on a robot. This model, learned and improved on-the-fly during the robot's exploration provides an efficient tool for localizing relevant objects within their environment. The proposed approach includes two intertwined components. On the one hand, we describe a method for learning and incrementally updating a model of visual saliency from a depth-based object detector. This model of saliency can also be exploited to produce bounding box proposals around objects of interest. On the other hand, we investigate an autonomous exploration technique to efficiently learn such a saliency model. The proposed exploration, called Reinforcement Learning-Intelligent Adaptive Curiosity (RL-IAC) is able to drive the robot's exploration so that samples selected by the robot are likely to improve the current model of saliency. We then demonstrate that such a saliency model learned directly on a robot outperforms several state-of-the-art saliency techniques, and that RL-IAC can drastically decrease the required time for learning a reliable saliency model.",0
"Object localization and recognition on autonomous mobile robots is an ongoing issue. Our aim is to address this by developing a model of visual saliency on a robot. This model is learned and enhanced during the robot's exploration, and it efficiently localizes important objects within its environment. Our proposed method consists of two interconnected components. Firstly, we present a technique to learn and update a model of visual saliency using a depth-based object detector. This model can also be used to generate bounding box suggestions for objects of interest. Secondly, we explore an autonomous exploration strategy, RL-IAC, which can efficiently learn the saliency model. This approach drives the robot's exploration so that selected samples improve the current saliency model. Our results show that the saliency model learned directly on a robot outperforms several state-of-the-art saliency techniques, and RL-IAC significantly reduces the time required to learn a reliable saliency model.",1
"Learning probability distributions on the weights of neural networks (NNs) has recently proven beneficial in many applications. Bayesian methods, such as Stein variational gradient descent (SVGD), offer an elegant framework to reason about NN model uncertainty. However, by assuming independent Gaussian priors for the individual NN weights (as often applied), SVGD does not impose prior knowledge that there is often structural information (dependence) among weights. We propose efficient posterior learning of structural weight uncertainty, within an SVGD framework, by employing matrix variate Gaussian priors on NN parameters. We further investigate the learned structural uncertainty in sequential decision-making problems, including contextual bandits and reinforcement learning. Experiments on several synthetic and real datasets indicate the superiority of our model, compared with state-of-the-art methods.",0
"In recent times, the acquisition of knowledge on probability distributions related to neural network (NN) weights has been advantageous in numerous applications. Bayesian techniques, like Stein variational gradient descent (SVGD), provide an effective basis for reasoning about NN model uncertainty. Nonetheless, relying on independent Gaussian priors for individual NN weights (as is often done), SVGD does not take into account the existence of structural information (dependence) among weights, which is commonly present. We suggest an efficient manner of acquiring posterior knowledge on structural weight uncertainty within an SVGD framework, by making use of matrix variate Gaussian priors on NN parameters. Additionally, we analyze the acquired structural uncertainty in contexts that require sequential decision-making, such as contextual bandits and reinforcement learning. Our experiments on several synthetic and actual datasets demonstrate that our model outperforms state-of-the-art methods.",1
"In the NIPS 2017 Learning to Run challenge, participants were tasked with building a controller for a musculoskeletal model to make it run as fast as possible through an obstacle course. Top participants were invited to describe their algorithms. In this work, we present eight solutions that used deep reinforcement learning approaches, based on algorithms such as Deep Deterministic Policy Gradient, Proximal Policy Optimization, and Trust Region Policy Optimization. Many solutions use similar relaxations and heuristics, such as reward shaping, frame skipping, discretization of the action space, symmetry, and policy blending. However, each of the eight teams implemented different modifications of the known algorithms.",0
"The NIPS 2017 Learning to Run challenge required participants to develop a controller for a musculoskeletal model that could traverse an obstacle course as quickly as possible. The best performers were given the opportunity to explain their methods. This study showcases eight solutions that leverage deep reinforcement learning techniques, including Deep Deterministic Policy Gradient, Proximal Policy Optimization, and Trust Region Policy Optimization. Although several solutions utilize comparable relaxations and heuristics such as reward shaping, frame skipping, and policy blending, each team introduced their own unique adjustments to the established algorithms.",1
"Recent advances in policy gradient methods and deep learning have demonstrated their applicability for complex reinforcement learning problems. However, the variance of the performance gradient estimates obtained from the simulation is often excessive, leading to poor sample efficiency. In this paper, we apply the stochastic variance reduced gradient descent (SVRG) to model-free policy gradient to significantly improve the sample-efficiency. The SVRG estimation is incorporated into a trust-region Newton conjugate gradient framework for the policy optimization. On several Mujoco tasks, our method achieves significantly better performance compared to the state-of-the-art model-free policy gradient methods in robotic continuous control such as trust region policy optimization (TRPO)",0
"Policy gradient methods and deep learning have made substantial progress lately, proving their effectiveness for intricate reinforcement learning problems. Nevertheless, the performance gradient estimates derived from simulation frequently exhibit excessive variance, leading to suboptimal sample efficiency. This study introduces the stochastic variance reduced gradient descent (SVRG) technique to enhance the sample efficiency of model-free policy gradient. The SVRG estimation is integrated into a trust-region Newton conjugate gradient framework to optimize policy. Our approach surpasses state-of-the-art model-free policy gradient methods like trust region policy optimization (TRPO) in robotic continuous control, as evidenced by superior performance on several Mujoco tasks.",1
"Existing inefficient traffic light control causes numerous problems, such as long delay and waste of energy. To improve efficiency, taking real-time traffic information as an input and dynamically adjusting the traffic light duration accordingly is a must. In terms of how to dynamically adjust traffic signals' duration, existing works either split the traffic signal into equal duration or extract limited traffic information from the real data. In this paper, we study how to decide the traffic signals' duration based on the collected data from different sensors and vehicular networks. We propose a deep reinforcement learning model to control the traffic light. In the model, we quantify the complex traffic scenario as states by collecting data and dividing the whole intersection into small grids. The timing changes of a traffic light are the actions, which are modeled as a high-dimension Markov decision process. The reward is the cumulative waiting time difference between two cycles. To solve the model, a convolutional neural network is employed to map the states to rewards. The proposed model is composed of several components to improve the performance, such as dueling network, target network, double Q-learning network, and prioritized experience replay. We evaluate our model via simulation in the Simulation of Urban MObility (SUMO) in a vehicular network, and the simulation results show the efficiency of our model in controlling traffic lights.",0
"Numerous problems arise due to the existing inefficient control of traffic lights, including long delays and energy wastage. To enhance efficiency, it is necessary to use real-time traffic information as input and dynamically adjust the duration of traffic lights accordingly. Previous studies either split the traffic signal into equal duration or extract limited traffic information from real data to dynamically adjust the duration of traffic signals. This paper explores how to determine the duration of traffic signals based on data collected from various sensors and vehicular networks. We propose a deep reinforcement learning model to control traffic lights, which quantifies the complex traffic scenario as states by collecting data and dividing the intersection into small grids. The timing changes of a traffic light are modeled as a high-dimension Markov decision process, with the reward being the cumulative waiting time difference between two cycles. To solve the model, a convolutional neural network is used to map the states to rewards. Our proposed model includes several components, such as dueling network, target network, double Q-learning network, and prioritized experience replay, to improve its performance. We evaluate our model through simulation in the Simulation of Urban MObility (SUMO) in a vehicular network, and the results demonstrate the efficiency of our model in controlling traffic lights.",1
"Video captioning is the task of automatically generating a textual description of the actions in a video. Although previous work (e.g. sequence-to-sequence model) has shown promising results in abstracting a coarse description of a short video, it is still very challenging to caption a video containing multiple fine-grained actions with a detailed description. This paper aims to address the challenge by proposing a novel hierarchical reinforcement learning framework for video captioning, where a high-level Manager module learns to design sub-goals and a low-level Worker module recognizes the primitive actions to fulfill the sub-goal. With this compositional framework to reinforce video captioning at different levels, our approach significantly outperforms all the baseline methods on a newly introduced large-scale dataset for fine-grained video captioning. Furthermore, our non-ensemble model has already achieved the state-of-the-art results on the widely-used MSR-VTT dataset.",0
"The task of generating a written explanation of the actions in a video is known as video captioning. While previous attempts, such as the sequence-to-sequence model, have shown potential in providing a general description of a brief video, it remains difficult to caption a video that includes multiple detailed actions. This article proposes a new hierarchical reinforcement learning structure for video captioning, where a Manager module at a high level learns to create sub-goals and a Worker module at a low level recognizes the basic actions necessary to achieve the sub-goal. By reinforcing video captioning at different levels with this compositional structure, our method outperforms all baseline methods on a recently introduced large-scale dataset for detailed video captioning. Additionally, our single model has already achieved the best results on the widely used MSR-VTT dataset.",1
"Animals execute goal-directed behaviours despite the limited range and scope of their sensors. To cope, they explore environments and store memories maintaining estimates of important information that is not presently available. Recently, progress has been made with artificial intelligence (AI) agents that learn to perform tasks from sensory input, even at a human level, by merging reinforcement learning (RL) algorithms with deep neural networks, and the excitement surrounding these results has led to the pursuit of related ideas as explanations of non-human animal learning. However, we demonstrate that contemporary RL algorithms struggle to solve simple tasks when enough information is concealed from the sensors of the agent, a property called ""partial observability"". An obvious requirement for handling partially observed tasks is access to extensive memory, but we show memory is not enough; it is critical that the right information be stored in the right format. We develop a model, the Memory, RL, and Inference Network (MERLIN), in which memory formation is guided by a process of predictive modeling. MERLIN facilitates the solution of tasks in 3D virtual reality environments for which partial observability is severe and memories must be maintained over long durations. Our model demonstrates a single learning agent architecture that can solve canonical behavioural tasks in psychology and neurobiology without strong simplifying assumptions about the dimensionality of sensory input or the duration of experiences.",0
"Despite their limited sensory range, animals are able to carry out goal-directed behaviors by exploring their environment and storing memories of important information not immediately available. While recent progress has been made in developing artificial intelligence agents that can perform tasks at a human level by merging reinforcement learning algorithms with deep neural networks, these agents still struggle with partially observable tasks. To solve this problem, access to extensive memory is not enough; the right information must also be stored in the right format. The Memory, RL, and Inference Network (MERLIN) is a model that uses predictive modeling to guide memory formation and can solve tasks in 3D virtual reality environments with severe partial observability. This model provides a single learning agent architecture that can solve behavioral tasks in psychology and neurobiology without any assumptions about sensory input dimensionality or experience duration.",1
"We introduce a generic framework that reduces the computational cost of object detection while retaining accuracy for scenarios where objects with varied sizes appear in high resolution images. Detection progresses in a coarse-to-fine manner, first on a down-sampled version of the image and then on a sequence of higher resolution regions identified as likely to improve the detection accuracy. Built upon reinforcement learning, our approach consists of a model (R-net) that uses coarse detection results to predict the potential accuracy gain for analyzing a region at a higher resolution and another model (Q-net) that sequentially selects regions to zoom in. Experiments on the Caltech Pedestrians dataset show that our approach reduces the number of processed pixels by over 50% without a drop in detection accuracy. The merits of our approach become more significant on a high resolution test set collected from YFCC100M dataset, where our approach maintains high detection performance while reducing the number of processed pixels by about 70% and the detection time by over 50%.",0
"To address the challenge of detecting objects of varied sizes in high resolution images, we have developed a versatile framework that maintains accuracy while minimizing computational costs. Our approach involves a stepwise detection process, starting with a low-resolution image and gradually analyzing higher resolution regions that are likely to enhance detection accuracy. Our approach is underpinned by reinforcement learning, with two models (R-net and Q-net) that predict accuracy gain and sequentially identify regions for analysis. Our experiments on the Caltech Pedestrians dataset demonstrate that our approach reduces processed pixels by over 50% without sacrificing detection accuracy. Our framework is particularly effective on a high resolution test set from the YFCC100M dataset, where we achieve high detection performance, reduce processed pixels by about 70%, and decrease detection time by over 50%.",1
"Unfair pricing policies have been shown to be one of the most negative perceptions customers can have concerning pricing, and may result in long-term losses for a company. Despite the fact that dynamic pricing models help companies maximize revenue, fairness and equality should be taken into account in order to avoid unfair price differences between groups of customers. This paper shows how to solve dynamic pricing by using Reinforcement Learning (RL) techniques so that prices are maximized while keeping a balance between revenue and fairness. We demonstrate that RL provides two main features to support fairness in dynamic pricing: on the one hand, RL is able to learn from recent experience, adapting the pricing policy to complex market environments; on the other hand, it provides a trade-off between short and long-term objectives, hence integrating fairness into the model's core. Considering these two features, we propose the application of RL for revenue optimization, with the additional integration of fairness as part of the learning procedure by using Jain's index as a metric. Results in a simulated environment show a significant improvement in fairness while at the same time maintaining optimisation of revenue.",0
"Customers often perceive unfair pricing policies as a negative aspect of pricing, which can result in long-term losses for a company. Although dynamic pricing models can help maximize revenue, companies should consider fairness and equality to avoid creating unfair price differences among customer groups. This paper proposes using Reinforcement Learning (RL) techniques to solve dynamic pricing issues by balancing revenue and fairness. RL offers two main features for supporting fairness in dynamic pricing: learning from recent experience to adapt to complex market environments and providing a trade-off between short and long-term objectives. The proposed solution integrates fairness into the learning procedure using Jain's index as a metric. Results from a simulated environment show a significant improvement in fairness while maintaining revenue optimization.",1
"Goals for reinforcement learning problems are typically defined through hand-specified rewards. To design such problems, developers of learning algorithms must inherently be aware of what the task goals are, yet we often require agents to discover them on their own without any supervision beyond these sparse rewards. While much of the power of reinforcement learning derives from the concept that agents can learn with little guidance, this requirement greatly burdens the training process. If we relax this one restriction and endow the agent with knowledge of the reward function, and in particular of the goal, we can leverage backwards induction to accelerate training. To achieve this, we propose training a model to learn to take imagined reversal steps from known goal states. Rather than training an agent exclusively to determine how to reach a goal while moving forwards in time, our approach travels backwards to jointly predict how we got there. We evaluate our work in Gridworld and Towers of Hanoi and empirically demonstrate that it yields better performance than standard DDQN.",0
"Typically, reinforcement learning problems involve defining goals through manually specified rewards. Developers of learning algorithms must have knowledge of the task goals to design these problems. However, agents are often required to discover the goals on their own with sparse rewards, which increases the burden of the training process. Although reinforcement learning allows agents to learn with minimal guidance, we can accelerate training by providing the agent with knowledge of the reward function and goal. Our proposal involves training a model to learn to take imagined reversal steps from known goal states, rather than training an agent to reach a goal while moving forwards in time. We apply our approach to Gridworld and Towers of Hanoi and show that it performs better than standard DDQN through empirical evaluation.",1
"Manually labeling datasets with object masks is extremely time consuming. In this work, we follow the idea of Polygon-RNN to produce polygonal annotations of objects interactively using humans-in-the-loop. We introduce several important improvements to the model: 1) we design a new CNN encoder architecture, 2) show how to effectively train the model with Reinforcement Learning, and 3) significantly increase the output resolution using a Graph Neural Network, allowing the model to accurately annotate high-resolution objects in images. Extensive evaluation on the Cityscapes dataset shows that our model, which we refer to as Polygon-RNN++, significantly outperforms the original model in both automatic (10% absolute and 16% relative improvement in mean IoU) and interactive modes (requiring 50% fewer clicks by annotators). We further analyze the cross-domain scenario in which our model is trained on one dataset, and used out of the box on datasets from varying domains. The results show that Polygon-RNN++ exhibits powerful generalization capabilities, achieving significant improvements over existing pixel-wise methods. Using simple online fine-tuning we further achieve a high reduction in annotation time for new datasets, moving a step closer towards an interactive annotation tool to be used in practice.",0
"The manual labeling of datasets with object masks is a time-consuming process. In this study, we adopt the concept of Polygon-RNN to generate polygonal annotations of objects interactively with human involvement. We have made several crucial enhancements to the model, including the development of a novel CNN encoder architecture, effective training with Reinforcement Learning, and the use of a Graph Neural Network to significantly increase output resolution and accurately annotate high-resolution objects in images. Our model, referred to as Polygon-RNN++, has been extensively evaluated on the Cityscapes dataset and has shown significant improvement over the original model, both in automatic and interactive modes. It has also demonstrated strong generalization capabilities when trained on one dataset and used on others from different domains. By using simple online fine-tuning, we have achieved a significant reduction in annotation time for new datasets, bringing us closer to an interactive annotation tool for practical use.",1
"Active Reinforcement Learning (ARL) is a twist on RL where the agent observes reward information only if it pays a cost. This subtle change makes exploration substantially more challenging. Powerful principles in RL like optimism, Thompson sampling, and random exploration do not help with ARL. We relate ARL in tabular environments to Bayes-Adaptive MDPs. We provide an ARL algorithm using Monte-Carlo Tree Search that is asymptotically Bayes optimal. Experimentally, this algorithm is near-optimal on small Bandit problems and MDPs. On larger MDPs it outperforms a Q-learner augmented with specialised heuristics for ARL. By analysing exploration behaviour in detail, we uncover obstacles to scaling up simulation-based algorithms for ARL.",0
"Active Reinforcement Learning (ARL) is a modified version of RL in which the agent only receives reward information if it incurs a cost, making exploration more difficult. RL techniques such as optimism, Thompson sampling, and random exploration are not applicable to ARL. We connect ARL in tabular environments to Bayes-Adaptive MDPs and present an ARL algorithm that uses Monte-Carlo Tree Search and is asymptotically Bayes optimal. Our algorithm performs nearly optimally on small Bandit problems and MDPs and outperforms a Q-learner with specialized heuristics for ARL on larger MDPs. By examining exploration behavior in depth, we identify challenges to scaling up simulation-based algorithms for ARL.",1
"The performance of off-policy learning, including deep Q-learning and deep deterministic policy gradient (DDPG), critically depends on the choice of the exploration policy. Existing exploration methods are mostly based on adding noise to the on-going actor policy and can only explore \emph{local} regions close to what the actor policy dictates. In this work, we develop a simple meta-policy gradient algorithm that allows us to adaptively learn the exploration policy in DDPG. Our algorithm allows us to train flexible exploration behaviors that are independent of the actor policy, yielding a \emph{global exploration} that significantly speeds up the learning process. With an extensive study, we show that our method significantly improves the sample-efficiency of DDPG on a variety of reinforcement learning tasks.",0
"The success of deep Q-learning and deep deterministic policy gradient (DDPG) in off-policy learning largely relies on the selection of the exploration policy. Most current exploration techniques involve introducing noise to the existing actor policy, which restricts exploration to nearby areas. Our research proposes a straightforward meta-policy gradient algorithm that can adaptively train the exploration policy in DDPG. Our approach enables us to develop versatile exploration behaviors that are not influenced by the actor policy, resulting in a more extensive exploration range that accelerates the learning process. Through a comprehensive analysis, we demonstrate that our method considerably enhances the sample-efficiency of DDPG in various reinforcement learning tasks.",1
"Inspired by the seminal work on Stein Variational Inference and Stein Variational Policy Gradient, we derived a method to generate samples from the posterior variational parameter distribution by \textit{explicitly} minimizing the KL divergence to match the target distribution in an amortize fashion. Consequently, we applied this varational inference technique into vanilla policy gradient, TRPO and PPO with Bayesian Neural Network parameterizations for reinforcement learning problems.",0
"We developed a technique for producing samples from the posterior variational parameter distribution that draws influence from the influential research on Stein Variational Inference and Stein Variational Policy Gradient. Our approach involves minimizing the KL divergence to ameliorate the matching of the target distribution in an amortized manner. As a result, we integrated this variational inference method into reinforcement learning problems utilizing Bayesian Neural Network parameterizations for vanilla policy gradient, TRPO, and PPO.",1
"Here we propose using the successor representation (SR) to accelerate learning in a constructive knowledge system based on general value functions (GVFs). In real-world settings like robotics for unstructured and dynamic environments, it is infeasible to model all meaningful aspects of a system and its environment by hand due to both complexity and size. Instead, robots must be capable of learning and adapting to changes in their environment and task, incrementally constructing models from their own experience. GVFs, taken from the field of reinforcement learning (RL), are a way of modeling the world as predictive questions. One approach to such models proposes a massive network of interconnected and interdependent GVFs, which are incrementally added over time. It is reasonable to expect that new, incrementally added predictions can be learned more swiftly if the learning process leverages knowledge gained from past experience. The SR provides such a means of separating the dynamics of the world from the prediction targets and thus capturing regularities that can be reused across multiple GVFs. As a primary contribution of this work, we show that using SR-based predictions can improve sample efficiency and learning speed in a continual learning setting where new predictions are incrementally added and learned over time. We analyze our approach in a grid-world and then demonstrate its potential on data from a physical robot arm.",0
"Our proposal suggests utilizing the successor representation (SR) to expedite learning in a constructive knowledge system that is based on general value functions (GVFs). In dynamic and unstructured environments such as robotics, it is impractical to manually model all essential aspects of the system and its surroundings due to their complexity and size. Therefore, robots must possess the ability to learn and adapt to changes in their environment and task, constructing models incrementally from their own experiences. GVFs, which stem from reinforcement learning (RL), are a strategy for modeling the world as predictive questions. One method involves a network of interconnected and interdependent GVFs that grow over time. It is reasonable to assume that the learning process can be accelerated by leveraging past knowledge to swiftly learn new predictions. The SR allows for the separation of world dynamics from prediction targets, capturing regularities that can be reused across multiple GVFs. Our primary contribution demonstrates that the utilization of SR-based predictions can enhance sample efficiency and learning speed in a continual learning setting, where new predictions are added and learned over time. We evaluate our approach in a grid-world and then showcase its potential with data from a physical robot arm.",1
"RANSAC is an important algorithm in robust optimization and a central building block for many computer vision applications. In recent years, traditionally hand-crafted pipelines have been replaced by deep learning pipelines, which can be trained in an end-to-end fashion. However, RANSAC has so far not been used as part of such deep learning pipelines, because its hypothesis selection procedure is non-differentiable. In this work, we present two different ways to overcome this limitation. The most promising approach is inspired by reinforcement learning, namely to replace the deterministic hypothesis selection by a probabilistic selection for which we can derive the expected loss w.r.t. to all learnable parameters. We call this approach DSAC, the differentiable counterpart of RANSAC. We apply DSAC to the problem of camera localization, where deep learning has so far failed to improve on traditional approaches. We demonstrate that by directly minimizing the expected loss of the output camera poses, robustly estimated by RANSAC, we achieve an increase in accuracy. In the future, any deep learning pipeline can use DSAC as a robust optimization component.",0
"RANSAC is a significant algorithm in robust optimization, which is widely used in computer vision applications. However, with the emergence of deep learning pipelines that can be trained end-to-end, RANSAC's non-differentiable hypothesis selection process has prevented it from being integrated into such pipelines. In this study, we present two methods to address this issue. The more promising solution is based on reinforcement learning, where we replace deterministic hypothesis selection with probabilistic selection, allowing us to derive the expected loss concerning the learnable parameters. We call this approach DSAC, the differentiable version of RANSAC. We apply DSAC to the camera localization problem, where deep learning has not yet proven more effective than traditional methods. We demonstrate that by minimizing the expected loss of the output camera poses, robustly estimated by RANSAC, we can improve accuracy. In the future, DSAC can be implemented as a robust optimization component in any deep learning pipeline.",1
"Policy gradient methods have enjoyed great success in deep reinforcement learning but suffer from high variance of gradient estimates. The high variance problem is particularly exasperated in problems with long horizons or high-dimensional action spaces. To mitigate this issue, we derive a bias-free action-dependent baseline for variance reduction which fully exploits the structural form of the stochastic policy itself and does not make any additional assumptions about the MDP. We demonstrate and quantify the benefit of the action-dependent baseline through both theoretical analysis as well as numerical results, including an analysis of the suboptimality of the optimal state-dependent baseline. The result is a computationally efficient policy gradient algorithm, which scales to high-dimensional control problems, as demonstrated by a synthetic 2000-dimensional target matching task. Our experimental results indicate that action-dependent baselines allow for faster learning on standard reinforcement learning benchmarks and high-dimensional hand manipulation and synthetic tasks. Finally, we show that the general idea of including additional information in baselines for improved variance reduction can be extended to partially observed and multi-agent tasks.",0
"Although policy gradient methods have been successful in deep reinforcement learning, they are hindered by the high variance of gradient estimates, which is especially prevalent in problems with long horizons or high-dimensional action spaces. To address this, we have developed a bias-free action-dependent baseline for variance reduction, which utilizes the structural form of the stochastic policy and does not rely on any additional MDP assumptions. Our approach has been validated through theoretical analysis and numerical results, including a comparison with the suboptimal state-dependent baseline. This has resulted in a computationally efficient policy gradient algorithm that can handle high-dimensional control problems, as shown in a synthetic 2000-dimensional target matching task. Our experiments demonstrate that the use of action-dependent baselines leads to faster learning on standard reinforcement learning benchmarks and high-dimensional hand manipulation and synthetic tasks. We also extend the concept of incorporating additional information into baselines for improved variance reduction to partially observed and multi-agent tasks.",1
"We propose a self-supervised approach for learning representations and robotic behaviors entirely from unlabeled videos recorded from multiple viewpoints, and study how this representation can be used in two robotic imitation settings: imitating object interactions from videos of humans, and imitating human poses. Imitation of human behavior requires a viewpoint-invariant representation that captures the relationships between end-effectors (hands or robot grippers) and the environment, object attributes, and body pose. We train our representations using a metric learning loss, where multiple simultaneous viewpoints of the same observation are attracted in the embedding space, while being repelled from temporal neighbors which are often visually similar but functionally different. In other words, the model simultaneously learns to recognize what is common between different-looking images, and what is different between similar-looking images. This signal causes our model to discover attributes that do not change across viewpoint, but do change across time, while ignoring nuisance variables such as occlusions, motion blur, lighting and background. We demonstrate that this representation can be used by a robot to directly mimic human poses without an explicit correspondence, and that it can be used as a reward function within a reinforcement learning algorithm. While representations are learned from an unlabeled collection of task-related videos, robot behaviors such as pouring are learned by watching a single 3rd-person demonstration by a human. Reward functions obtained by following the human demonstrations under the learned representation enable efficient reinforcement learning that is practical for real-world robotic systems. Video results, open-source code and dataset are available at https://sermanet.github.io/imitate",0
"Our proposed approach involves using self-supervised learning to acquire representations and robotic behaviors solely from unlabeled videos recorded from various angles. We examine how this representation can be utilized in two robotic imitation scenarios: imitating human object interactions from video footage, and imitating human poses. A viewpoint-invariant representation that captures the relationship between end-effectors (hands or robot grippers) and the environment, object attributes, and body pose is required for human behavior imitation. We train our representation using a metric learning loss, where the embedding space attracts multiple simultaneous viewpoints of the same observation, but repels temporal neighbors that are visually similar but functionally different. Our model is trained to recognize commonalities between dissimilar images and differences between similar ones. This approach enables the discovery of attributes that remain constant across viewpoints but vary over time, while ignoring nuisance variables such as occlusions, motion blur, lighting, and background. We demonstrate that this representation can be used by a robot to directly mimic human poses without requiring an explicit correspondence, and that it can function as a reward function in a reinforcement learning algorithm. Although representations are learned from an unlabeled set of task-related videos, robot behaviors such as pouring are learned from a single third-person demonstration by a human. The reward functions obtained by following the human demonstrations under the learned representation facilitate efficient reinforcement learning that is practical for real-world robotic systems. Our video results, open-source code, and dataset are available at https://sermanet.github.io/imitate.",1
"Reinforcement learning is a promising approach to developing hard-to-engineer adaptive solutions for complex and diverse robotic tasks. However, learning with real-world robots is often unreliable and difficult, which resulted in their low adoption in reinforcement learning research. This difficulty is worsened by the lack of guidelines for setting up learning tasks with robots. In this work, we develop a learning task with a UR5 robotic arm to bring to light some key elements of a task setup and study their contributions to the challenges with robots. We find that learning performance can be highly sensitive to the setup, and thus oversights and omissions in setup details can make effective learning, reproducibility, and fair comparison hard. Our study suggests some mitigating steps to help future experimenters avoid difficulties and pitfalls. We show that highly reliable and repeatable experiments can be performed in our setup, indicating the possibility of reinforcement learning research extensively based on real-world robots.",0
"Reinforcement learning is a promising means of creating adaptable solutions for complex and diverse robotic tasks that are difficult to engineer. However, using real-world robots for learning is often unreliable and challenging, leading to low adoption in reinforcement learning research. This problem is compounded by the absence of guidelines for setting up learning tasks involving robots. In this research, we establish a learning task utilizing a UR5 robotic arm to identify critical elements of task setup and assess their impact on the challenges involved with robots. Our findings demonstrate that learning performance is significantly influenced by the setup, and hence, overlooking or omitting setup details can impede effective learning, reproducibility, and fair comparisons. Our study recommends some mitigating measures to help future experimenters avoid difficulties and pitfalls. We demonstrate that our setup enables highly reliable and repeatable experiments, indicating that reinforcement learning research extensively based on real-world robots is feasible.",1
"A common belief in model-free reinforcement learning is that methods based on random search in the parameter space of policies exhibit significantly worse sample complexity than those that explore the space of actions. We dispel such beliefs by introducing a random search method for training static, linear policies for continuous control problems, matching state-of-the-art sample efficiency on the benchmark MuJoCo locomotion tasks. Our method also finds a nearly optimal controller for a challenging instance of the Linear Quadratic Regulator, a classical problem in control theory, when the dynamics are not known. Computationally, our random search algorithm is at least 15 times more efficient than the fastest competing model-free methods on these benchmarks. We take advantage of this computational efficiency to evaluate the performance of our method over hundreds of random seeds and many different hyperparameter configurations for each benchmark task. Our simulations highlight a high variability in performance in these benchmark tasks, suggesting that commonly used estimations of sample efficiency do not adequately evaluate the performance of RL algorithms.",0
"In model-free reinforcement learning, it is commonly believed that exploring the space of actions leads to better sample efficiency than random search in the parameter space of policies. However, we prove this belief wrong by introducing a random search method that trains static, linear policies for continuous control problems. Our method matches the state-of-the-art sample efficiency on the MuJoCo locomotion tasks and finds a near-optimal controller for a difficult instance of the Linear Quadratic Regulator problem, even when the dynamics are not known. Our random search algorithm is at least 15 times more efficient than the fastest competing model-free methods on these benchmarks. Additionally, we evaluate the performance of our method over hundreds of random seeds and various hyperparameter configurations for each benchmark task. Our simulations suggest that commonly used estimations of sample efficiency do not adequately evaluate the performance of RL algorithms, as there is a high variability in performance in these benchmark tasks.",1
"An online reinforcement learning algorithm is anytime if it does not need to know in advance the horizon T of the experiment. A well-known technique to obtain an anytime algorithm from any non-anytime algorithm is the ""Doubling Trick"". In the context of adversarial or stochastic multi-armed bandits, the performance of an algorithm is measured by its regret, and we study two families of sequences of growing horizons (geometric and exponential) to generalize previously known results that certain doubling tricks can be used to conserve certain regret bounds. In a broad setting, we prove that a geometric doubling trick can be used to conserve (minimax) bounds in $R\_T = O(\sqrt{T})$ but cannot conserve (distribution-dependent) bounds in $R\_T = O(\log T)$. We give insights as to why exponential doubling tricks may be better, as they conserve bounds in $R\_T = O(\log T)$, and are close to conserving bounds in $R\_T = O(\sqrt{T})$.",0
"If an online reinforcement learning algorithm can function without prior knowledge of the experiment's horizon T, it is considered an anytime algorithm. The ""Doubling Trick"" is a well-known technique for converting non-anytime algorithms into anytime algorithms. In the context of adversarial or stochastic multi-armed bandits, regret is used to measure an algorithm's performance, and we examine two sequences of growing horizons (geometric and exponential) to extend existing knowledge that certain doubling tricks can maintain specific regret bounds. Our findings demonstrate that a geometric doubling trick can preserve (minimax) bounds in $R\_T = O(\sqrt{T})$ but not maintain (distribution-dependent) bounds in $R\_T = O(\log T)$ in a broad setting. Exponential doubling tricks may be superior since they can maintain bounds in $R\_T = O(\log T)$ and are close to preserving bounds in $R\_T = O(\sqrt{T})$.",1
"Model-free deep reinforcement learning has been shown to exhibit good performance in domains ranging from video games to simulated robotic manipulation and locomotion. However, model-free methods are known to perform poorly when the interaction time with the environment is limited, as is the case for most real-world robotic tasks. In this paper, we study how maximum entropy policies trained using soft Q-learning can be applied to real-world robotic manipulation. The application of this method to real-world manipulation is facilitated by two important features of soft Q-learning. First, soft Q-learning can learn multimodal exploration strategies by learning policies represented by expressive energy-based models. Second, we show that policies learned with soft Q-learning can be composed to create new policies, and that the optimality of the resulting policy can be bounded in terms of the divergence between the composed policies. This compositionality provides an especially valuable tool for real-world manipulation, where constructing new policies by composing existing skills can provide a large gain in efficiency over training from scratch. Our experimental evaluation demonstrates that soft Q-learning is substantially more sample efficient than prior model-free deep reinforcement learning methods, and that compositionality can be performed for both simulated and real-world tasks.",0
"The effectiveness of model-free deep reinforcement learning has been demonstrated in various domains, including video games and simulated robotic manipulation and locomotion. However, these methods are not ideal for real-world robotic tasks with limited interaction time. To address this issue, we investigated the use of maximum entropy policies trained via soft Q-learning for real-world robotic manipulation. Two key features of soft Q-learning make it suitable for such applications: the ability to learn multimodal exploration strategies through expressive energy-based models and the capability to compose policies to create new ones while bounding the optimality based on the divergence between the policies. This compositionality is particularly useful in real-world manipulation, where it can lead to significant efficiency gains. Our experiments indicate that soft Q-learning is more sample efficient than previous model-free deep reinforcement learning methods, and that compositionality can be applied to both simulated and real-world tasks.",1
"In recent years, visual question answering (VQA) has become topical. The premise of VQA's significance as a benchmark in AI, is that both the image and textual question need to be well understood and mutually grounded in order to infer the correct answer. However, current VQA models perhaps `understand' less than initially hoped, and instead master the easier task of exploiting cues given away in the question and biases in the answer distribution. In this paper we propose the inverse problem of VQA (iVQA). The iVQA task is to generate a question that corresponds to a given image and answer pair. We propose a variational iVQA model that can generate diverse, grammatically correct and content correlated questions that match the given answer. Based on this model, we show that iVQA is an interesting benchmark for visuo-linguistic understanding, and a more challenging alternative to VQA because an iVQA model needs to understand the image better to be successful. As a second contribution, we show how to use iVQA in a novel reinforcement learning framework to diagnose any existing VQA model by way of exposing its belief set: the set of question-answer pairs that the VQA model would predict true for a given image. This provides a completely new window into what VQA models `believe' about images. We show that existing VQA models have more erroneous beliefs than previously thought, revealing their intrinsic weaknesses. Suggestions are then made on how to address these weaknesses going forward.",0
"Visual question answering (VQA) has become a popular topic in recent years. The significance of VQA lies in its ability to serve as a benchmark in AI. Both the image and the textual question must be well understood and grounded for the correct answer to be inferred. However, current VQA models may not understand as much as initially expected. Instead, they may rely on exploiting cues in the question and biases in the answer distribution. This paper introduces the inverse problem of VQA (iVQA), which requires generating a question corresponding to a given image and answer pair. We propose a variational iVQA model that can generate diverse, grammatically correct, and content-correlated questions matching the answer. We demonstrate that iVQA is an interesting benchmark for visuo-linguistic understanding and a more challenging alternative to VQA because an iVQA model must understand the image better to be successful. The paper also introduces a novel reinforcement learning framework that uses iVQA to diagnose existing VQA models by exposing their belief set. This approach provides new insights into what VQA models ""believe"" about images. We show that existing VQA models have more erroneous beliefs than previously thought, revealing their intrinsic weaknesses. Finally, we offer suggestions on how to address these weaknesses going forward.",1
"The existing image captioning approaches typically train a one-stage sentence decoder, which is difficult to generate rich fine-grained descriptions. On the other hand, multi-stage image caption model is hard to train due to the vanishing gradient problem. In this paper, we propose a coarse-to-fine multi-stage prediction framework for image captioning, composed of multiple decoders each of which operates on the output of the previous stage, producing increasingly refined image descriptions. Our proposed learning approach addresses the difficulty of vanishing gradients during training by providing a learning objective function that enforces intermediate supervisions. Particularly, we optimize our model with a reinforcement learning approach which utilizes the output of each intermediate decoder's test-time inference algorithm as well as the output of its preceding decoder to normalize the rewards, which simultaneously solves the well-known exposure bias problem and the loss-evaluation mismatch problem. We extensively evaluate the proposed approach on MSCOCO and show that our approach can achieve the state-of-the-art performance.",0
"The typical method for image captioning involves training a one-stage sentence decoder, which struggles to produce detailed descriptions. Conversely, a multi-stage image caption model is difficult to train due to the vanishing gradient problem. To address these issues, we propose a coarse-to-fine multi-stage prediction framework for image captioning. This framework consists of multiple decoders, each building on the output of the previous stage to generate increasingly refined image descriptions. To overcome the vanishing gradient challenge during training, we introduce an objective function that enforces intermediate supervisions. In addition, we optimize our model using a reinforcement learning approach that utilizes the output of each intermediate decoder's test-time inference algorithm and the output of its preceding decoder to normalize the rewards. This approach simultaneously solves the exposure bias problem and the loss-evaluation mismatch problem. We conduct a thorough evaluation of our approach on MSCOCO and demonstrate that it achieves state-of-the-art performance.",1
"Many practical environments contain catastrophic states that an optimal agent would visit infrequently or never. Even on toy problems, Deep Reinforcement Learning (DRL) agents tend to periodically revisit these states upon forgetting their existence under a new policy. We introduce intrinsic fear (IF), a learned reward shaping that guards DRL agents against periodic catastrophes. IF agents possess a fear model trained to predict the probability of imminent catastrophe. This score is then used to penalize the Q-learning objective. Our theoretical analysis bounds the reduction in average return due to learning on the perturbed objective. We also prove robustness to classification errors. As a bonus, IF models tend to learn faster, owing to reward shaping. Experiments demonstrate that intrinsic-fear DQNs solve otherwise pathological environments and improve on several Atari games.",0
"In practical situations, there are states that an optimal agent would avoid or rarely visit. However, Deep Reinforcement Learning (DRL) agents tend to forget and revisit these states periodically, even on simpler problems. To address this issue, we propose intrinsic fear (IF), a reward shaping technique that trains DRL agents to recognize and avoid catastrophic states. IF agents use a fear model to predict the likelihood of such states and penalize the Q-learning objective accordingly. Our theoretical analysis shows that this approach can reduce the average return, but we also prove that it is resilient to classification errors. Additionally, IF models learn faster thanks to reward shaping. Experimentally, we show that IF DQNs can successfully navigate otherwise problematic environments and outperform regular DQNs on several Atari games.",1
"In this work, we provide theoretical guarantees for reward decomposition in deterministic MDPs. Reward decomposition is a special case of Hierarchical Reinforcement Learning, that allows one to learn many policies in parallel and combine them into a composite solution. Our approach builds on mapping this problem into a Reward Discounted Traveling Salesman Problem, and then deriving approximate solutions for it. In particular, we focus on approximate solutions that are local, i.e., solutions that only observe information about the current state. Local policies are easy to implement and do not require substantial computational resources as they do not perform planning. While local deterministic policies, like Nearest Neighbor, are being used in practice for hierarchical reinforcement learning, we propose three stochastic policies that guarantee better performance than any deterministic policy.",0
"The objective of our research is to provide a theoretical basis for reward decomposition in deterministic MDPs. This technique is a form of Hierarchical Reinforcement Learning that enables the learning of multiple policies simultaneously and their integration into a unified solution. To achieve this, we have formulated a solution to the Reward Discounted Traveling Salesman Problem, which allows for an approximate solution to this complex problem. Our study has focused on local solutions that offer several advantages, including ease of implementation and minimal computational resources. While deterministic policies such as Nearest Neighbor are currently in use, we introduce three new stochastic policies that provide superior performance.",1
"The ability to interpret machine learning models has become increasingly important now that machine learning is used to inform consequential decisions. We propose an approach called model extraction for interpreting complex, blackbox models. Our approach approximates the complex model using a much more interpretable model; as long as the approximation quality is good, then statistical properties of the complex model are reflected in the interpretable model. We show how model extraction can be used to understand and debug random forests and neural nets trained on several datasets from the UCI Machine Learning Repository, as well as control policies learned for several classical reinforcement learning problems.",0
"As machine learning is increasingly relied upon to make important decisions, the ability to interpret its models has become crucial. Our proposed method, called model extraction, aims to interpret complex blackbox models by approximating them with more easily interpretable models. If the approximation is of high quality, statistical properties of the complex model can be reflected in the interpretable model. We demonstrate the effectiveness of model extraction in understanding and debugging random forests and neural nets trained on various datasets from the UCI Machine Learning Repository, as well as controlling policies learned for classical reinforcement learning problems.",1
"Genetic algorithms have been widely used in many practical optimization problems. Inspired by natural selection, operators, including mutation, crossover and selection, provide effective heuristics for search and black-box optimization. However, they have not been shown useful for deep reinforcement learning, possibly due to the catastrophic consequence of parameter crossovers of neural networks. Here, we present Genetic Policy Optimization (GPO), a new genetic algorithm for sample-efficient deep policy optimization. GPO uses imitation learning for policy crossover in the state space and applies policy gradient methods for mutation. Our experiments on MuJoCo tasks show that GPO as a genetic algorithm is able to provide superior performance over the state-of-the-art policy gradient methods and achieves comparable or higher sample efficiency.",0
"The practical optimization problems have widely used genetic algorithms. The operators such as mutation, crossover, and selection, inspired by natural selection, have proved to be efficient heuristics for search and black-box optimization. However, these algorithms have not been successful in deep reinforcement learning due to the negative consequences resulting from parameter crossovers of neural networks. In this study, we introduce a new genetic algorithm called Genetic Policy Optimization (GPO) that utilizes imitation learning for policy crossover in the state space and policy gradient methods for mutation. Through experiments on MuJoCo tasks, we have demonstrated that GPO outperforms state-of-the-art policy gradient methods and achieves comparable or higher sample efficiency.",1
"Image cropping aims at improving the aesthetic quality of images by adjusting their composition. Most weakly supervised cropping methods (without bounding box supervision) rely on the sliding window mechanism. The sliding window mechanism requires fixed aspect ratios and limits the cropping region with arbitrary size. Moreover, the sliding window method usually produces tens of thousands of windows on the input image which is very time-consuming. Motivated by these challenges, we firstly formulate the aesthetic image cropping as a sequential decision-making process and propose a weakly supervised Aesthetics Aware Reinforcement Learning (A2-RL) framework to address this problem. Particularly, the proposed method develops an aesthetics aware reward function which especially benefits image cropping. Similar to human's decision making, we use a comprehensive state representation including both the current observation and the historical experience. We train the agent using the actor-critic architecture in an end-to-end manner. The agent is evaluated on several popular unseen cropping datasets. Experiment results show that our method achieves the state-of-the-art performance with much fewer candidate windows and much less time compared with previous weakly supervised methods.",0
"The purpose of image cropping is to enhance the visual appeal of images by adjusting their composition. Most methods for weakly supervised cropping, which do not involve bounding box supervision, use the sliding window mechanism. However, this method has limitations, such as fixed aspect ratios and arbitrary cropping regions, and can create a large number of windows that are time-consuming to process. To address these challenges, we propose a weakly supervised Aesthetics Aware Reinforcement Learning (A2-RL) framework that formulates the image cropping process as a sequential decision-making process. Our approach develops an aesthetics-aware reward function that is optimized for image cropping and uses a comprehensive state representation that includes current observation and historical experience. We train the agent using the actor-critic architecture in an end-to-end manner and evaluate it on several popular datasets. Our method achieves state-of-the-art performance with fewer candidate windows and less time than previous weakly supervised methods.",1
"We introduce a new class of reinforcement learning methods referred to as {\em episodic multi-armed bandits} (eMAB). In eMAB the learner proceeds in {\em episodes}, each composed of several {\em steps}, in which it chooses an action and observes a feedback signal. Moreover, in each step, it can take a special action, called the $stop$ action, that ends the current episode. After the $stop$ action is taken, the learner collects a terminal reward, and observes the costs and terminal rewards associated with each step of the episode. The goal of the learner is to maximize its cumulative gain (i.e., the terminal reward minus costs) over all episodes by learning to choose the best sequence of actions based on the feedback. First, we define an {\em oracle} benchmark, which sequentially selects the actions that maximize the expected immediate gain. Then, we propose our online learning algorithm, named {\em FeedBack Adaptive Learning} (FeedBAL), and prove that its regret with respect to the benchmark is bounded with high probability and increases logarithmically in expectation. Moreover, the regret only has polynomial dependence on the number of steps, actions and states. eMAB can be used to model applications that involve humans in the loop, ranging from personalized medical screening to personalized web-based education, where sequences of actions are taken in each episode, and optimal behavior requires adapting the chosen actions based on the feedback.",0
"We present a novel category of reinforcement learning strategies called ""episodic multi-armed bandits"" (eMAB). The eMAB approach involves the learner operating in ""episodes,"" which consist of multiple ""steps."" During each step, the learner selects an action and then receives feedback. Additionally, the learner can select a ""stop"" action that concludes the episode. When the ""stop"" action is taken, the learner receives a terminal reward and observes the costs and terminal rewards associated with each step of the episode. The objective of the learner is to optimize its cumulative gain by choosing the best sequence of actions based on the feedback. We begin by defining an ""oracle"" benchmark that chooses actions that maximize the anticipated immediate gain. We then introduce our online learning algorithm, named ""FeedBack Adaptive Learning"" (FeedBAL), and demonstrate that its regret with respect to the benchmark is limited with a high probability and increases logarithmically in expectation. Furthermore, the regret is only polynomially dependent on the number of steps, actions, and states. The eMAB approach can be used to simulate scenarios in which humans are actively involved, ranging from personalized medical screening to personalized web-based education, where optimal behavior necessitates adjusting the chosen actions based on feedback.",1
"Deep Q-learning is investigated as an end-to-end solution to estimate the optimal strategies for acting on time series input. Experiments are conducted on two idealized trading games. 1) Univariate: the only input is a wave-like price time series, and 2) Bivariate: the input includes a random stepwise price time series and a noisy signal time series, which is positively correlated with future price changes. The Univariate game tests whether the agent can capture the underlying dynamics, and the Bivariate game tests whether the agent can utilize the hidden relation among the inputs. Stacked Gated Recurrent Unit (GRU), Long Short-Term Memory (LSTM) units, Convolutional Neural Network (CNN), and multi-layer perceptron (MLP) are used to model Q values. For both games, all agents successfully find a profitable strategy. The GRU-based agents show best overall performance in the Univariate game, while the MLP-based agents outperform others in the Bivariate game.",0
"The study examines Deep Q-learning as a comprehensive approach to determine the most effective tactics to act on time series input. The study involves conducting experiments on two trading games: 1) Univariate, where the input is solely a wave-like price time series, and 2) Bivariate, where the input consists of a random stepwise price time series and a noisy signal time series that is positively associated with future price changes. The purpose of the Univariate game is to test whether the agent can grasp the fundamental dynamics, while the Bivariate game assesses whether the agent can utilize the hidden relationship between the inputs. To model Q values, Stacked Gated Recurrent Unit (GRU), Long Short-Term Memory (LSTM) units, Convolutional Neural Network (CNN), and multi-layer perceptron (MLP) are employed. All agents in both games successfully identify a profitable strategy. The GRU-based agents show superior overall performance in the Univariate game, while the MLP-based agents outperform the others in the Bivariate game.",1
"The purpose of this technical report is two-fold. First of all, it introduces a suite of challenging continuous control tasks (integrated with OpenAI Gym) based on currently existing robotics hardware. The tasks include pushing, sliding and pick & place with a Fetch robotic arm as well as in-hand object manipulation with a Shadow Dexterous Hand. All tasks have sparse binary rewards and follow a Multi-Goal Reinforcement Learning (RL) framework in which an agent is told what to do using an additional input.   The second part of the paper presents a set of concrete research ideas for improving RL algorithms, most of which are related to Multi-Goal RL and Hindsight Experience Replay.",0
"The technical report serves two purposes. Firstly, it introduces a collection of demanding continuous control tasks which are integrated with OpenAI Gym and use current robotics hardware. These tasks include manipulating objects with a Fetch robotic arm, such as pushing, sliding, and pick & place, as well as in-hand object manipulation with a Shadow Dexterous Hand. All tasks have sparse binary rewards and follow a Multi-Goal Reinforcement Learning (RL) framework, in which an agent is given extra input on what to do. Secondly, the report presents several specific research ideas for enhancing RL algorithms, with a focus on Multi-Goal RL and Hindsight Experience Replay.",1
"We present a method for using previously-trained 'teacher' agents to kickstart the training of a new 'student' agent. To this end, we leverage ideas from policy distillation and population based training. Our method places no constraints on the architecture of the teacher or student agents, and it regulates itself to allow the students to surpass their teachers in performance. We show that, on a challenging and computationally-intensive multi-task benchmark (DMLab-30), kickstarted training improves the data efficiency of new agents, making it significantly easier to iterate on their design. We also show that the same kickstarting pipeline can allow a single student agent to leverage multiple 'expert' teachers which specialize on individual tasks. In this setting kickstarting yields surprisingly large gains, with the kickstarted agent matching the performance of an agent trained from scratch in almost 10x fewer steps, and surpassing its final performance by 42 percent. Kickstarting is conceptually simple and can easily be incorporated into reinforcement learning experiments.",0
"We have developed a technique that utilizes 'teacher' agents that have been previously trained to initiate the training of a new 'student' agent. Our method draws inspiration from policy distillation and population based training, and it does not impose any limitations on the structure of the teacher or student agents. Moreover, the method allows the student agents to exceed the performance of their teachers. Our research demonstrates that kickstarted training significantly enhances the efficiency of new agent data, which simplifies the process of iterating on their design. Additionally, we have revealed that a single student agent can benefit from multiple 'expert' teachers that specialize in specific tasks. This approach yields remarkable gains, with the kickstarted agent achieving performance comparable to an agent trained from scratch in roughly ten times fewer steps and surpassing its final performance by 42 percent. Kickstarting is an uncomplicated concept that can be readily integrated into reinforcement learning experiments.",1
"During the 2017 NBA playoffs, Celtics coach Brad Stevens was faced with a difficult decision when defending against the Cavaliers: ""Do you double and risk giving up easy shots, or stay at home and do the best you can?"" It's a tough call, but finding a good defensive strategy that effectively incorporates doubling can make all the difference in the NBA. In this paper, we analyze double teaming in the NBA, quantifying the trade-off between risk and reward. Using player trajectory data pertaining to over 643,000 possessions, we identified when the ball handler was double teamed. Given these data and the corresponding outcome (i.e., was the defense successful), we used deep reinforcement learning to estimate the quality of the defensive actions. We present qualitative and quantitative results summarizing our learned defensive strategy for defending. We show that our policy value estimates are predictive of points per possession and win percentage. Overall, the proposed framework represents a step toward a more comprehensive understanding of defensive strategies in the NBA.",0
"In the 2017 NBA playoffs, Celtics coach Brad Stevens faced a challenging dilemma while defending against the Cavaliers: should he double-team and risk conceding easy shots or remain defensive and do his best? While it's a tough decision, doubling can significantly impact defensive strategies in the NBA. This paper examines double teaming in the NBA and evaluates the trade-off between risk and reward. We used player trajectory data from over 643,000 possessions to determine when the ball handler was double-teamed. We utilized deep reinforcement learning to estimate the quality of defensive actions and present qualitative and quantitative results summarizing our learned defensive strategy. Our policy value estimates are predictive of points per possession and win percentage. In conclusion, our proposed framework represents a significant step towards comprehensively understanding defensive strategies in the NBA.",1
"In recent years, deep learning techniques have been developed to improve the performance of program synthesis from input-output examples. Albeit its significant progress, the programs that can be synthesized by state-of-the-art approaches are still simple in terms of their complexity. In this work, we move a significant step forward along this direction by proposing a new class of challenging tasks in the domain of program synthesis from input-output examples: learning a context-free parser from pairs of input programs and their parse trees. We show that this class of tasks are much more challenging than previously studied tasks, and the test accuracy of existing approaches is almost 0%.   We tackle the challenges by developing three novel techniques inspired by three novel observations, which reveal the key ingredients of using deep learning to synthesize a complex program. First, the use of a non-differentiable machine is the key to effectively restrict the search space. Thus our proposed approach learns a neural program operating a domain-specific non-differentiable machine. Second, recursion is the key to achieve generalizability. Thus, we bake-in the notion of recursion in the design of our non-differentiable machine. Third, reinforcement learning is the key to learn how to operate the non-differentiable machine, but it is also hard to train the model effectively with existing reinforcement learning algorithms from a cold boot. We develop a novel two-phase reinforcement learning-based search algorithm to overcome this issue. In our evaluation, we show that using our novel approach, neural parsing programs can be learned to achieve 100% test accuracy on test inputs that are 500x longer than the training samples.",0
"Recent years have seen the development of deep learning techniques to enhance program synthesis from input-output examples. Despite notable progress, current approaches can only synthesize simple programs. This study proposes a new class of challenging tasks in program synthesis from input-output examples: learning a context-free parser from pairs of input programs and their parse trees. These tasks are more challenging than previous ones, with existing approaches achieving almost 0% accuracy. To address these challenges, the study develops three novel techniques based on key observations. Firstly, a non-differentiable machine effectively restricts the search space. Thus, the proposed approach learns a neural program operating a domain-specific non-differentiable machine. Secondly, recursion achieves generalizability, which is baked into the design of the non-differentiable machine. Thirdly, reinforcement learning is necessary to learn how to operate the machine, but training the model is difficult with existing algorithms. Thus, a novel two-phase reinforcement learning-based search algorithm is developed. Results demonstrate that the proposed approach achieves 100% test accuracy on inputs 500 times longer than training samples.",1
"Reinforcement Learning and the Evolutionary Strategy are two major approaches in addressing complicated control problems. Both are strong contenders and have their own devotee communities. Both groups have been very active in developing new advances in their own domain and devising, in recent years, leading-edge techniques to address complex continuous control tasks. Here, in the context of Deep Reinforcement Learning, we formulate a parallelized version of the Proximal Policy Optimization method and a Deep Deterministic Policy Gradient method. Moreover, we conduct a thorough comparison between the state-of-the-art techniques in both camps fro continuous control; evolutionary methods and Deep Reinforcement Learning methods. The results show there is no consistent winner.",0
"Two prominent approaches for tackling intricate control problems are Reinforcement Learning and Evolutionary Strategy. Each has its own enthusiastic following and has made significant strides in their respective fields. In recent years, both groups have developed innovative techniques to address demanding continuous control tasks. For this study, we present a parallelized version of the Proximal Policy Optimization method and a Deep Deterministic Policy Gradient method within the context of Deep Reinforcement Learning. Additionally, we conduct a comprehensive comparison of the latest techniques from both Evolutionary Strategy and Deep Reinforcement Learning for continuous control tasks. The outcomes indicate that there is no clear victor.",1
"We introduce Recurrent Predictive State Policy (RPSP) networks, a recurrent architecture that brings insights from predictive state representations to reinforcement learning in partially observable environments. Predictive state policy networks consist of a recursive filter, which keeps track of a belief about the state of the environment, and a reactive policy that directly maps beliefs to actions, to maximize the cumulative reward. The recursive filter leverages predictive state representations (PSRs) (Rosencrantz and Gordon, 2004; Sun et al., 2016) by modeling predictive state-- a prediction of the distribution of future observations conditioned on history and future actions. This representation gives rise to a rich class of statistically consistent algorithms (Hefny et al., 2018) to initialize the recursive filter. Predictive state serves as an equivalent representation of a belief state. Therefore, the policy component of the RPSP-network can be purely reactive, simplifying training while still allowing optimal behaviour. Moreover, we use the PSR interpretation during training as well, by incorporating prediction error in the loss function. The entire network (recursive filter and reactive policy) is still differentiable and can be trained using gradient based methods. We optimize our policy using a combination of policy gradient based on rewards (Williams, 1992) and gradient descent based on prediction error. We show the efficacy of RPSP-networks under partial observability on a set of robotic control tasks from OpenAI Gym. We empirically show that RPSP-networks perform well compared with memory-preserving networks such as GRUs, as well as finite memory models, being the overall best performing method.",0
"We present Recurrent Predictive State Policy (RPSP) networks, a type of recurrent architecture designed for reinforcement learning in partially observable environments. These networks utilize predictive state representations (PSRs) to keep track of the environment's state and map beliefs to actions that maximize the cumulative reward. The recursive filter of the RPSP network models predictive state, which predicts the distribution of future observations based on history and future actions, and is used to initialize the filter. The policy component of RPSP can be purely reactive due to the predictive state representation, simplifying training while still allowing optimal behavior. We incorporate prediction error into the loss function during training and optimize using a combination of policy gradient and gradient descent. We demonstrate the effectiveness of RPSP networks on robotic control tasks from OpenAI Gym, outperforming memory-preserving networks such as GRUs and finite memory models.",1
"We introduce Dynamic Deep Neural Networks (D2NN), a new type of feed-forward deep neural network that allows selective execution. Given an input, only a subset of D2NN neurons are executed, and the particular subset is determined by the D2NN itself. By pruning unnecessary computation depending on input, D2NNs provide a way to improve computational efficiency. To achieve dynamic selective execution, a D2NN augments a feed-forward deep neural network (directed acyclic graph of differentiable modules) with controller modules. Each controller module is a sub-network whose output is a decision that controls whether other modules can execute. A D2NN is trained end to end. Both regular and controller modules in a D2NN are learnable and are jointly trained to optimize both accuracy and efficiency. Such training is achieved by integrating backpropagation with reinforcement learning. With extensive experiments of various D2NN architectures on image classification tasks, we demonstrate that D2NNs are general and flexible, and can effectively optimize accuracy-efficiency trade-offs.",0
"We have developed Dynamic Deep Neural Networks (D2NNs), a novel form of feed-forward deep neural network that can selectively execute. Instead of activating all D2NN neurons for a given input, only a specific subset is determined by the network itself. D2NNs are designed to boost computational efficiency by removing unnecessary computations based on input. To enable dynamic selective execution, a D2NN adds controller modules to a feed-forward deep neural network (a directed acyclic graph of differentiable modules). Each controller module is a sub-network that produces a decision dictating whether other modules can execute. D2NNs are trained end-to-end, with both regular and controller modules being learnable and jointly optimized for accuracy and efficiency. Backpropagation is combined with reinforcement learning for training. Through extensive experiments on image classification tasks, we demonstrate that D2NNs are versatile and adaptable, allowing for effective optimization of the accuracy-efficiency trade-off.",1
"In video captioning task, the best practice has been achieved by attention-based models which associate salient visual components with sentences in the video. However, existing study follows a common procedure which includes a frame-level appearance modeling and motion modeling on equal interval frame sampling, which may bring about redundant visual information, sensitivity to content noise and unnecessary computation cost.   We propose a plug-and-play PickNet to perform informative frame picking in video captioning. Based on a standard Encoder-Decoder framework, we develop a reinforcement-learning-based procedure to train the network sequentially, where the reward of each frame picking action is designed by maximizing visual diversity and minimizing textual discrepancy. If the candidate is rewarded, it will be selected and the corresponding latent representation of Encoder-Decoder will be updated for future trials. This procedure goes on until the end of the video sequence. Consequently, a compact frame subset can be selected to represent the visual information and perform video captioning without performance degradation. Experiment results shows that our model can use 6-8 frames to achieve competitive performance across popular benchmarks.",0
"Attention-based models have been successful in video captioning by linking important visual elements to sentences in the video. However, traditional methods involve appearance and motion modeling on every frame, resulting in excessive visual information, sensitivity to content noise, and unnecessary computation costs. Our proposed solution is the PickNet, a plug-and-play tool that selects informative frames for video captioning. Using a reinforcement-learning-based procedure, the network is trained sequentially to maximize visual diversity and minimize textual discrepancy for each frame picking action. If a candidate frame is rewarded, it is selected and the Encoder-Decoder's latent representation is updated for future trials until the end of the video sequence. With our method, a compact frame subset can represent visual information and perform video captioning without performance degradation. Our experimental results show that our model can achieve competitive performance using 6-8 frames across popular benchmarks.",1
"We propose a distributed architecture for deep reinforcement learning at scale, that enables agents to learn effectively from orders of magnitude more data than previously possible. The algorithm decouples acting from learning: the actors interact with their own instances of the environment by selecting actions according to a shared neural network, and accumulate the resulting experience in a shared experience replay memory; the learner replays samples of experience and updates the neural network. The architecture relies on prioritized experience replay to focus only on the most significant data generated by the actors. Our architecture substantially improves the state of the art on the Arcade Learning Environment, achieving better final performance in a fraction of the wall-clock training time.",0
"Our proposal is to utilize a distributed architecture for deep reinforcement learning on a large scale. This architecture allows agents to learn more effectively by processing a significantly larger amount of data than previously possible. Our algorithm separates the act of decision-making from the learning process. The actors select actions based on a shared neural network while interacting with their own instance of the environment, and store their experiences in a shared memory for later use. The learner then reviews samples from this memory and updates the neural network accordingly. Our architecture prioritizes the most important data generated by the actors, resulting in more efficient use of resources. Our approach significantly surpasses the current state-of-the-art in the Arcade Learning Environment, achieving superior performance in a fraction of the training time.",1
"Recent model-free reinforcement learning algorithms have proposed incorporating learned dynamics models as a source of additional data with the intention of reducing sample complexity. Such methods hold the promise of incorporating imagined data coupled with a notion of model uncertainty to accelerate the learning of continuous control tasks. Unfortunately, they rely on heuristics that limit usage of the dynamics model. We present model-based value expansion, which controls for uncertainty in the model by only allowing imagination to fixed depth. By enabling wider use of learned dynamics models within a model-free reinforcement learning algorithm, we improve value estimation, which, in turn, reduces the sample complexity of learning.",0
"New model-free reinforcement learning techniques suggest integrating learned dynamics models to decrease sample complexity. These approaches aim to incorporate imagined data and model uncertainty to expedite learning continuous control tasks. However, they rely on restrictive heuristics that restrict the dynamics model's usage. Our proposal, model-based value expansion, manages the model's uncertainty by limiting imagination to a fixed depth. By broadening the learned dynamics models' usage within a model-free reinforcement learning algorithm, we enhance value estimation, resulting in reduced sample complexity for learning.",1
"We propose Scheduled Auxiliary Control (SAC-X), a new learning paradigm in the context of Reinforcement Learning (RL). SAC-X enables learning of complex behaviors - from scratch - in the presence of multiple sparse reward signals. To this end, the agent is equipped with a set of general auxiliary tasks, that it attempts to learn simultaneously via off-policy RL. The key idea behind our method is that active (learned) scheduling and execution of auxiliary policies allows the agent to efficiently explore its environment - enabling it to excel at sparse reward RL. Our experiments in several challenging robotic manipulation settings demonstrate the power of our approach.",0
"In the realm of Reinforcement Learning (RL), we present a new learning technique called Scheduled Auxiliary Control (SAC-X). SAC-X allows for the acquisition of intricate behaviors from scratch, even in the presence of multiple sparse reward signals. The agent is furnished with a collection of general auxiliary tasks that it endeavors to learn simultaneously through off-policy RL. Our approach centers around the concept that by actively scheduling and executing auxiliary policies, the agent can explore its environment efficiently and excel in sparse reward RL. In various difficult robotic manipulation scenarios, our experiments showcase the efficacy of our methodology.",1
"Most reinforcement learning algorithms are inefficient for learning multiple tasks in complex robotic systems, where different tasks share a set of actions. In such environments a compound policy may be learnt with shared neural network parameters, which performs multiple tasks concurrently. However such compound policy may get biased towards a task or the gradients from different tasks negate each other, making the learning unstable and sometimes less data efficient. In this paper, we propose a new approach for simultaneous training of multiple tasks sharing a set of common actions in continuous action spaces, which we call as DiGrad (Differential Policy Gradient). The proposed framework is based on differential policy gradients and can accommodate multi-task learning in a single actor-critic network. We also propose a simple heuristic in the differential policy gradient update to further improve the learning. The proposed architecture was tested on 8 link planar manipulator and 27 degrees of freedom(DoF) Humanoid for learning multi-goal reachability tasks for 3 and 2 end effectors respectively. We show that our approach supports efficient multi-task learning in complex robotic systems, outperforming related methods in continuous action spaces.",0
"Learning multiple tasks in complex robotic systems using reinforcement learning algorithms can be inefficient as different tasks share a set of actions. To overcome this, a compound policy can be learned with shared neural network parameters that can perform multiple tasks concurrently. However, this approach has limitations as the compound policy may become biased towards a specific task or the gradients from different tasks may cancel each other, leading to unstable learning and reduced data efficiency. Our paper introduces a new approach called DiGrad (Differential Policy Gradient) for simultaneous training of multiple tasks that share a set of common actions in continuous action spaces. This framework is based on differential policy gradients and can handle multi-task learning in a single actor-critic network. We also propose a simple heuristic in the differential policy gradient update to enhance the learning. We tested our approach on a planar manipulator with 8 links and a humanoid with 27 degrees of freedom for learning multi-goal reachability tasks for 3 and 2 end effectors respectively. Our results demonstrate that DiGrad supports efficient multi-task learning in complex robotic systems and outperforms related methods in continuous action spaces.",1
"We consider the problem of \emph{fully decentralized} multi-agent reinforcement learning (MARL), where the agents are located at the nodes of a time-varying communication network. Specifically, we assume that the reward functions of the agents might correspond to different tasks, and are only known to the corresponding agent. Moreover, each agent makes individual decisions based on both the information observed locally and the messages received from its neighbors over the network. Within this setting, the collective goal of the agents is to maximize the globally averaged return over the network through exchanging information with their neighbors. To this end, we propose two decentralized actor-critic algorithms with function approximation, which are applicable to large-scale MARL problems where both the number of states and the number of agents are massively large. Under the decentralized structure, the actor step is performed individually by each agent with no need to infer the policies of others. For the critic step, we propose a consensus update via communication over the network. Our algorithms are fully incremental and can be implemented in an online fashion. Convergence analyses of the algorithms are provided when the value functions are approximated within the class of linear functions. Extensive simulation results with both linear and nonlinear function approximations are presented to validate the proposed algorithms. Our work appears to be the first study of fully decentralized MARL algorithms for networked agents with function approximation, with provable convergence guarantees.",0
"The focus of our study is on fully decentralized multi-agent reinforcement learning (MARL). Our agents are located at the nodes of a communication network that varies over time. We assume that each agent has a unique reward function that is only known to that agent. Additionally, agents make individual decisions based on local information and messages received from their neighbors. The collective goal of the agents is to maximize the globally averaged return over the network by exchanging information with their neighbors. We introduce two decentralized actor-critic algorithms with function approximation that can be applied to large-scale MARL problems with a massive number of states and agents. Our algorithms are fully incremental and can be implemented online. The actor step is performed independently by each agent, and the critic step is updated via communication over the network. We provide convergence analyses of the algorithms when the value functions are approximated within the class of linear functions. We validate the proposed algorithms with extensive simulation results using both linear and nonlinear function approximations. Our study is the first to introduce fully decentralized MARL algorithms with function approximation for networked agents, with provable convergence guarantees.",1
"Variational Bayesian neural nets combine the flexibility of deep learning with Bayesian uncertainty estimation. Unfortunately, there is a tradeoff between cheap but simple variational families (e.g.~fully factorized) or expensive and complicated inference procedures. We show that natural gradient ascent with adaptive weight noise implicitly fits a variational posterior to maximize the evidence lower bound (ELBO). This insight allows us to train full-covariance, fully factorized, or matrix-variate Gaussian variational posteriors using noisy versions of natural gradient, Adam, and K-FAC, respectively, making it possible to scale up to modern-size ConvNets. On standard regression benchmarks, our noisy K-FAC algorithm makes better predictions and matches Hamiltonian Monte Carlo's predictive variances better than existing methods. Its improved uncertainty estimates lead to more efficient exploration in active learning, and intrinsic motivation for reinforcement learning.",0
"Variational Bayesian neural networks blend the flexible nature of deep learning with Bayesian uncertainty evaluation. However, the selection between inexpensive yet simple variational families (such as fully factorized) or costly and intricate inference procedures is a challenge. Our study indicates that natural gradient ascent with adaptive weight noise implicitly adapts a variational posterior to optimize the evidence lower bound (ELBO). This realization enables us to train full-covariance, fully factorized, or matrix-variate Gaussian variational posteriors using different noisy versions of natural gradient, Adam, and K-FAC, enabling the scaling up of modern ConvNets. Our noisy K-FAC algorithm, when tested on standard regression benchmarks, outperforms existing methods by producing better predictions and matching Hamiltonian Monte Carlo's predictive variances. Its enhanced uncertainty estimates lead to more efficient exploration in active learning and intrinsic motivation for reinforcement learning.",1
"Recent advances in deep reinforcement learning have made significant strides in performance on applications such as Go and Atari games. However, developing practical methods to balance exploration and exploitation in complex domains remains largely unsolved. Thompson Sampling and its extension to reinforcement learning provide an elegant approach to exploration that only requires access to posterior samples of the model. At the same time, advances in approximate Bayesian methods have made posterior approximation for flexible neural network models practical. Thus, it is attractive to consider approximate Bayesian neural networks in a Thompson Sampling framework. To understand the impact of using an approximate posterior on Thompson Sampling, we benchmark well-established and recently developed methods for approximate posterior sampling combined with Thompson Sampling over a series of contextual bandit problems. We found that many approaches that have been successful in the supervised learning setting underperformed in the sequential decision-making scenario. In particular, we highlight the challenge of adapting slowly converging uncertainty estimates to the online setting.",0
"Great strides have been made in deep reinforcement learning recently, particularly in the areas of Go and Atari games. However, there remains a significant challenge in balancing exploration and exploitation in complex domains. To address this, Thompson Sampling and its reinforcement learning extension offer a promising approach to exploration that only requires access to posterior samples of the model. In addition, recent advances in approximate Bayesian methods have made it practical to use posterior approximation for flexible neural network models. This makes it appealing to consider using approximate Bayesian neural networks in a Thompson Sampling framework. To assess the impact of using an approximate posterior on Thompson Sampling, we tested established and newly developed methods for approximate posterior sampling combined with Thompson Sampling on a series of contextual bandit problems. Our findings indicated that many of the approaches that have been successful in supervised learning did not perform as well in sequential decision-making scenarios. The challenge of adapting slowly converging uncertainty estimates to the online setting was particularly notable.",1
"Exploration in environments with sparse rewards has been a persistent problem in reinforcement learning (RL). Many tasks are natural to specify with a sparse reward, and manually shaping a reward function can result in suboptimal performance. However, finding a non-zero reward is exponentially more difficult with increasing task horizon or action dimensionality. This puts many real-world tasks out of practical reach of RL methods. In this work, we use demonstrations to overcome the exploration problem and successfully learn to perform long-horizon, multi-step robotics tasks with continuous control such as stacking blocks with a robot arm. Our method, which builds on top of Deep Deterministic Policy Gradients and Hindsight Experience Replay, provides an order of magnitude of speedup over RL on simulated robotics tasks. It is simple to implement and makes only the additional assumption that we can collect a small set of demonstrations. Furthermore, our method is able to solve tasks not solvable by either RL or behavior cloning alone, and often ends up outperforming the demonstrator policy.",0
"Reinforcement learning (RL) has faced the challenge of exploring environments with sparse rewards. While some tasks are naturally suited to a sparse reward system, manually shaping rewards can lead to suboptimal results. Unfortunately, finding a non-zero reward becomes increasingly difficult as task horizon and action dimensionality increase, limiting the practicality of RL in real-world scenarios. To address this issue, our study leverages demonstrations to overcome the exploration problem and successfully complete complex robotics tasks with continuous control, such as stacking blocks with a robot arm. Our approach, which builds on Deep Deterministic Policy Gradients and Hindsight Experience Replay, offers a significant speedup over RL when applied to simulated robotics tasks and requires only a small set of demonstrations. Additionally, our method can solve previously unsolvable tasks and often outperforms the demonstrator policy.",1
"Gradient-based optimization is the foundation of deep learning and reinforcement learning. Even when the mechanism being optimized is unknown or not differentiable, optimization using high-variance or biased gradient estimates is still often the best strategy. We introduce a general framework for learning low-variance, unbiased gradient estimators for black-box functions of random variables. Our method uses gradients of a neural network trained jointly with model parameters or policies, and is applicable in both discrete and continuous settings. We demonstrate this framework for training discrete latent-variable models. We also give an unbiased, action-conditional extension of the advantage actor-critic reinforcement learning algorithm.",0
"The basis of deep learning and reinforcement learning is gradient-based optimization. Despite the possibility of not knowing or having a non-differentiable mechanism to optimize, utilizing high-variance or biased gradient estimates is usually the optimal approach. To address this, we present a general framework that allows for learning low-variance, unbiased gradient estimators for black-box functions of random variables. Our approach involves utilizing the gradients of a neural network that is trained in conjunction with model parameters or policies, and it can be applied to both continuous and discrete settings. We exhibit the effectiveness of this framework by implementing it in the training of discrete latent-variable models. Additionally, we introduce an action-conditional extension of the advantage actor-critic reinforcement learning algorithm that is unbiased.",1
"Options in reinforcement learning allow agents to hierarchically decompose a task into subtasks, having the potential to speed up learning and planning. However, autonomously learning effective sets of options is still a major challenge in the field. In this paper we focus on the recently introduced idea of using representation learning methods to guide the option discovery process. Specifically, we look at eigenoptions, options obtained from representations that encode diffusive information flow in the environment. We extend the existing algorithms for eigenoption discovery to settings with stochastic transitions and in which handcrafted features are not available. We propose an algorithm that discovers eigenoptions while learning non-linear state representations from raw pixels. It exploits recent successes in the deep reinforcement learning literature and the equivalence between proto-value functions and the successor representation. We use traditional tabular domains to provide intuition about our approach and Atari 2600 games to demonstrate its potential.",0
"Reinforcement learning offers the possibility for agents to break down a task into smaller subtasks, which can enhance the speed of learning and planning. Nonetheless, discovering useful options autonomously remains a significant challenge in the field. Our paper focuses on the concept of using representation learning methods to guide option discovery, specifically investigating eigenoptions. These options are obtained from representations that encode diffusive information flow in the environment. We have expanded existing algorithms for eigenoption discovery to include settings where stochastic transitions and handcrafted features are not available. Our proposed algorithm discovers eigenoptions while learning non-linear state representations from raw pixels, utilizing the successes of deep reinforcement learning and the equivalence between proto-value functions and the successor representation. We provide insight into our approach using traditional tabular domains and showcase its potential with Atari 2600 games.",1
"Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum.   We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task.",0
"One of the primary difficulties in Reinforcement Learning (RL) is dealing with limited rewards. To address this issue, we introduce a new method called Hindsight Experience Replay. This technique enables efficient sample learning from sparse binary incentives, thus eliminating the need for intricate reward engineering. It can be integrated with any off-policy RL algorithm and functions as an implicit curriculum. We apply this approach to the manipulation of objects using a robotic arm and test it on three different tasks: pushing, sliding, and pick-and-place. Binary rewards indicating task completion are the only incentives used. Our ablation studies demonstrate that Hindsight Experience Replay is an essential component for successful training in these challenging environments. Furthermore, we prove that our policies, developed using a physics simulation, can be implemented on a physical robot and effectively execute the assigned task.",1
"Policy gradient methods have achieved remarkable successes in solving challenging reinforcement learning problems. However, it still often suffers from the large variance issue on policy gradient estimation, which leads to poor sample efficiency during training. In this work, we propose a control variate method to effectively reduce variance for policy gradient methods. Motivated by the Stein's identity, our method extends the previous control variate methods used in REINFORCE and advantage actor-critic by introducing more general action-dependent baseline functions. Empirical studies show that our method significantly improves the sample efficiency of the state-of-the-art policy gradient approaches.",0
"The use of policy gradient methods has led to impressive accomplishments in tackling difficult reinforcement learning problems. Nevertheless, these methods frequently encounter the obstacle of high variance in policy gradient estimation, resulting in suboptimal training sample efficiency. This paper suggests a control variate approach to effectively mitigate the variance issue associated with policy gradient methods. Inspired by Stein's identity, our technique expands on previous control variate methods implemented in REINFORCE and advantage actor-critic by introducing more comprehensive action-dependent baseline functions. Empirical evidence indicates that our approach notably enhances the sample efficiency of the most advanced policy gradient methodologies.",1
"We study an important yet under-addressed problem of quickly and safely improving policies in online reinforcement learning domains. As its solution, we propose a novel exploration strategy - diverse exploration (DE), which learns and deploys a diverse set of safe policies to explore the environment. We provide DE theory explaining why diversity in behavior policies enables effective exploration without sacrificing exploitation. Our empirical study shows that an online policy improvement algorithm framework implementing the DE strategy can achieve both fast policy improvement and safe online performance.",0
"The problem of enhancing policies in online reinforcement learning domains in a rapid and secure manner has received insufficient attention. To tackle this issue, we suggest a unique exploration method called diverse exploration (DE), which trains and employs a diverse range of secure policies for environment exploration. We present the theory behind DE, which establishes that varied behavioral policies aid in fruitful exploration without compromising exploitation. Our experimental findings indicate that an online policy enhancement algorithm framework utilizing the DE approach can attain speedy policy improvement and safe online performance.",1
"In recent years, Deep Reinforcement Learning has made impressive advances in solving several important benchmark problems for sequential decision making. Many control applications use a generic multilayer perceptron (MLP) for non-vision parts of the policy network. In this work, we propose a new neural network architecture for the policy network representation that is simple yet effective. The proposed Structured Control Net (SCN) splits the generic MLP into two separate sub-modules: a nonlinear control module and a linear control module. Intuitively, the nonlinear control is for forward-looking and global control, while the linear control stabilizes the local dynamics around the residual of global control. We hypothesize that this will bring together the benefits of both linear and nonlinear policies: improve training sample efficiency, final episodic reward, and generalization of learned policy, while requiring a smaller network and being generally applicable to different training methods. We validated our hypothesis with competitive results on simulations from OpenAI MuJoCo, Roboschool, Atari, and a custom 2D urban driving environment, with various ablation and generalization tests, trained with multiple black-box and policy gradient training methods. The proposed architecture has the potential to improve upon broader control tasks by incorporating problem specific priors into the architecture. As a case study, we demonstrate much improved performance for locomotion tasks by emulating the biological central pattern generators (CPGs) as the nonlinear part of the architecture.",0
"Over the past few years, Deep Reinforcement Learning has made significant progress in addressing various critical problems related to sequential decision-making. For many control applications, a generic multilayer perceptron (MLP) is used for non-vision parts of the policy network. This study introduces a new neural network architecture for the policy network representation that is straightforward yet highly effective. The proposed Structured Control Net (SCN) divides the generic MLP into two sub-modules. The nonlinear control module is responsible for forward-looking and global control, while the linear control module stabilizes the local dynamics around the residual of global control. The combination of linear and nonlinear policies enhances training sample efficiency, final episodic reward, and the generalization of learned policy. Additionally, this architecture requires a smaller network and is applicable to different training methods. The hypothesis was validated by achieving competitive results on simulations from OpenAI MuJoCo, Roboschool, Atari, and a custom 2D urban driving environment, using various ablation and generalization tests, and employing multiple black-box and policy gradient training methods. Furthermore, incorporating problem-specific priors into the architecture has the potential to enhance broader control tasks. As a case study, locomotion tasks were improved by emulating biological central pattern generators (CPGs) as the nonlinear part of the architecture.",1
"Distributional approaches to value-based reinforcement learning model the entire distribution of returns, rather than just their expected values, and have recently been shown to yield state-of-the-art empirical performance. This was demonstrated by the recently proposed C51 algorithm, based on categorical distributional reinforcement learning (CDRL) [Bellemare et al., 2017]. However, the theoretical properties of CDRL algorithms are not yet well understood. In this paper, we introduce a framework to analyse CDRL algorithms, establish the importance of the projected distributional Bellman operator in distributional RL, draw fundamental connections between CDRL and the Cram\'er distance, and give a proof of convergence for sample-based categorical distributional reinforcement learning algorithms.",0
"Value-based reinforcement learning using distributional approaches considers the whole range of return distributions, instead of just their expected values. This method has shown superior empirical performance, as demonstrated by the C51 algorithm based on categorical distributional reinforcement learning (CDRL) proposed by Bellemare et al. in 2017. However, the theoretical properties of CDRL algorithms are not yet fully comprehended. In this study, we present a framework for analyzing CDRL algorithms, emphasize the importance of the projected distributional Bellman operator in distributional RL, establish essential links between CDRL and the Cram\'er distance, and provide evidence of convergence for sample-based categorical distributional reinforcement learning algorithms.",1
"Actor-critic methods solve reinforcement learning problems by updating a parameterized policy known as an actor in a direction that increases an estimate of the expected return known as a critic. However, existing actor-critic methods only use values or gradients of the critic to update the policy parameter. In this paper, we propose a novel actor-critic method called the guide actor-critic (GAC). GAC firstly learns a guide actor that locally maximizes the critic and then it updates the policy parameter based on the guide actor by supervised learning. Our main theoretical contributions are two folds. First, we show that GAC updates the guide actor by performing second-order optimization in the action space where the curvature matrix is based on the Hessians of the critic. Second, we show that the deterministic policy gradient method is a special case of GAC when the Hessians are ignored. Through experiments, we show that our method is a promising reinforcement learning method for continuous controls.",0
"The reinforcement learning problems can be solved by actor-critic methods, which involve updating a parameterized policy known as an actor to increase an estimate of the expected return, called a critic. However, current actor-critic methods only utilize values or gradients of the critic to update the policy parameter. To overcome this limitation, our study proposes a new actor-critic method named the guide actor-critic (GAC). The GAC approach initially learns a guide actor that locally maximizes the critic, and then updates the policy parameter based on the guide actor through supervised learning. Our research makes two primary theoretical contributions. Firstly, we demonstrate that GAC updates the guide actor through second-order optimization in the action space, wherein the curvature matrix is based on the Hessians of the critic. Secondly, we reveal that deterministic policy gradient method is a special case of GAC when the Hessians are ignored. Our empirical results indicate that the GAC method is a promising reinforcement learning technique for continuous controls.",1
"The design of good heuristics or approximation algorithms for NP-hard combinatorial optimization problems often requires significant specialized knowledge and trial-and-error. Can we automate this challenging, tedious process, and learn the algorithms instead? In many real-world applications, it is typically the case that the same optimization problem is solved again and again on a regular basis, maintaining the same problem structure but differing in the data. This provides an opportunity for learning heuristic algorithms that exploit the structure of such recurring problems. In this paper, we propose a unique combination of reinforcement learning and graph embedding to address this challenge. The learned greedy policy behaves like a meta-algorithm that incrementally constructs a solution, and the action is determined by the output of a graph embedding network capturing the current state of the solution. We show that our framework can be applied to a diverse range of optimization problems over graphs, and learns effective algorithms for the Minimum Vertex Cover, Maximum Cut and Traveling Salesman problems.",0
"Developing effective heuristics or approximation algorithms for NP-hard combinatorial optimization problems often requires specialized knowledge and trial-and-error. Is it possible to automate this laborious process and learn the algorithms instead? Frequently, the same optimization problem is tackled repeatedly in real-world scenarios, maintaining the problem structure but varying in data. This presents an opportunity for learning heuristic algorithms that utilize the structure of these recurring problems. This paper proposes a novel combination of reinforcement learning and graph embedding to overcome this challenge. The learned greedy policy acts as a meta-algorithm that constructs a solution incrementally, with the action determined by the output of a graph embedding network that captures the current state of the solution. Our framework can be applied to a variety of optimization problems over graphs and can learn effective algorithms for the Minimum Vertex Cover, Maximum Cut, and Traveling Salesman problems.",1
"Exploration is a fundamental challenge in reinforcement learning (RL). Many of the current exploration methods for deep RL use task-agnostic objectives, such as information gain or bonuses based on state visitation. However, many practical applications of RL involve learning more than a single task, and prior tasks can be used to inform how exploration should be performed in new tasks. In this work, we explore how prior tasks can inform an agent about how to explore effectively in new situations. We introduce a novel gradient-based fast adaptation algorithm -- model agnostic exploration with structured noise (MAESN) -- to learn exploration strategies from prior experience. The prior experience is used both to initialize a policy and to acquire a latent exploration space that can inject structured stochasticity into a policy, producing exploration strategies that are informed by prior knowledge and are more effective than random action-space noise. We show that MAESN is more effective at learning exploration strategies when compared to prior meta-RL methods, RL without learned exploration strategies, and task-agnostic exploration methods. We evaluate our method on a variety of simulated tasks: locomotion with a wheeled robot, locomotion with a quadrupedal walker, and object manipulation.",0
"Reinforcement learning (RL) faces a significant challenge in exploration, which many current deep RL methods address by utilizing task-agnostic objectives, such as information gain or bonuses based on state visitation. However, RL applications often involve learning multiple tasks, and past tasks can inform exploration strategies for new ones. This study investigates how prior tasks can guide effective exploration in new situations, introducing a new gradient-based fast adaptation algorithm called model agnostic exploration with structured noise (MAESN) that leverages prior experience. MAESN initializes a policy and acquires a latent exploration space that can insert structured stochasticity into the policy, leading to exploration strategies that are informed by past knowledge and more effective than random action-space noise. Compared to prior meta-RL methods, RL without learned exploration strategies, and task-agnostic exploration methods, MAESN proves more effective at learning exploration strategies. The method is evaluated on various simulated tasks, including locomotion with a wheeled robot, locomotion with a quadrupedal walker, and object manipulation.",1
"Mild cognitive impairment (MCI) is a prodromal phase in the progression from normal aging to dementia, especially Alzheimers disease. Even though there is mild cognitive decline in MCI patients, they have normal overall cognition and thus is challenging to distinguish from normal aging. Using transcribed data obtained from recorded conversational interactions between participants and trained interviewers, and applying supervised learning models to these data, a recent clinical trial has shown a promising result in differentiating MCI from normal aging. However, the substantial amount of interactions with medical staff can still incur significant medical care expenses in practice. In this paper, we propose a novel reinforcement learning (RL) framework to train an efficient dialogue agent on existing transcripts from clinical trials. Specifically, the agent is trained to sketch disease-specific lexical probability distribution, and thus to converse in a way that maximizes the diagnosis accuracy and minimizes the number of conversation turns. We evaluate the performance of the proposed reinforcement learning framework on the MCI diagnosis from a real clinical trial. The results show that while using only a few turns of conversation, our framework can significantly outperform state-of-the-art supervised learning approaches.",0
"Mild cognitive impairment (MCI) is a preliminary stage in the progression towards dementia, particularly Alzheimer's disease. Although MCI patients experience mild cognitive decline, their overall cognition remains normal, making it difficult to distinguish from normal aging. A recent clinical trial employed supervised learning models on transcribed conversational data between participants and trained interviewers, exhibiting a promising outcome in identifying MCI patients. However, the frequent interactions with medical staff may result in significant medical care expenses. To address this, we propose a novel reinforcement learning (RL) framework to train an efficient dialogue agent on existing transcripts from clinical trials. The agent is trained to create a disease-specific lexical probability distribution, enabling it to converse in a manner that maximizes diagnosis accuracy while minimizing conversation turns. The proposed RL framework's performance is evaluated based on MCI diagnosis from a real clinical trial, indicating that it significantly outperforms state-of-the-art supervised learning methods while utilizing only a few conversation turns.",1
"While great advances are made in pattern recognition and machine learning, the successes of such fields remain restricted to narrow applications and seem to break down when training data is scarce, a shift in domain occurs, or when intelligent reasoning is required for rapid adaptation to new environments. In this work, we list several of the shortcomings of modern machine-learning solutions, specifically in the contexts of computer vision and in reinforcement learning and suggest directions to explore in order to try to ameliorate these weaknesses.",0
"Although pattern recognition and machine learning have made significant progress, their achievements are limited to specific applications and are not effective when there is a lack of training data, a change in domain, or the need for quick adaptation to new environments. In this study, we identify various inadequacies of current machine-learning approaches, particularly in the areas of computer vision and reinforcement learning, and propose potential avenues for addressing these limitations.",1
"We introduce Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects. In contrast to most existing model-based reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, I2As learn to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. I2As show improved data efficiency, performance, and robustness to model misspecification compared to several baselines.",0
"Our new approach to deep reinforcement learning, called Imagination-Augmented Agents (I2As), merges model-free and model-based aspects. Unlike traditional model-based reinforcement learning and planning techniques that dictate how a model should be utilized to create a policy, I2As learn to interpret forecasts from a trained environment model in various ways by integrating predictions as supplementary context in deep policy networks. I2As demonstrate better performance, data efficiency, and resilience to errors in model specification than several baseline methods.",1
"Video summarization aims to facilitate large-scale video browsing by producing short, concise summaries that are diverse and representative of original videos. In this paper, we formulate video summarization as a sequential decision-making process and develop a deep summarization network (DSN) to summarize videos. DSN predicts for each video frame a probability, which indicates how likely a frame is selected, and then takes actions based on the probability distributions to select frames, forming video summaries. To train our DSN, we propose an end-to-end, reinforcement learning-based framework, where we design a novel reward function that jointly accounts for diversity and representativeness of generated summaries and does not rely on labels or user interactions at all. During training, the reward function judges how diverse and representative the generated summaries are, while DSN strives for earning higher rewards by learning to produce more diverse and more representative summaries. Since labels are not required, our method can be fully unsupervised. Extensive experiments on two benchmark datasets show that our unsupervised method not only outperforms other state-of-the-art unsupervised methods, but also is comparable to or even superior than most of published supervised approaches.",0
"The goal of video summarization is to make it easier to browse through large amounts of video footage by creating brief, but comprehensive summaries that accurately represent the original content. This study presents a deep summarization network (DSN) that uses a sequential decision-making process to select frames from a video based on their probability of being relevant. To train the DSN, the researchers developed a reinforcement learning-based framework that uses a novel reward function to evaluate the diversity and representativeness of the generated summaries. Unlike other methods, this approach does not rely on user interactions or labels, making it fully unsupervised. The researchers conducted extensive experiments on two benchmark datasets, which demonstrated that their unsupervised method performs better than other unsupervised approaches and is comparable to or better than most supervised methods.",1
"Deep reinforcement learning has demonstrated increasing capabilities for continuous control problems, including agents that can move with skill and agility through their environment. An open problem in this setting is that of developing good strategies for integrating or merging policies for multiple skills, where each individual skill is a specialist in a specific skill and its associated state distribution. We extend policy distillation methods to the continuous action setting and leverage this technique to combine expert policies, as evaluated in the domain of simulated bipedal locomotion across different classes of terrain. We also introduce an input injection method for augmenting an existing policy network to exploit new input features. Lastly, our method uses transfer learning to assist in the efficient acquisition of new skills. The combination of these methods allows a policy to be incrementally augmented with new skills. We compare our progressive learning and integration via distillation (PLAID) method against three alternative baselines.",0
"The ability of deep reinforcement learning to handle continuous control problems has been steadily increasing. This includes the development of agents that can deftly and efficiently navigate through their environment. However, a challenge in this area is to effectively merge policies for multiple skills, each of which is specialized in its own skill and state distribution. To address this, we extend policy distillation methods to the continuous action setting and apply it to expert policies in the simulated bipedal locomotion domain across various terrain classes. We also introduce an input injection method to add new input features to an existing policy network. Additionally, our method employs transfer learning to help acquire new skills more efficiently. By combining these techniques, we enable the gradual augmentation of a policy with new skills. We evaluate our progressive learning and integration via distillation (PLAID) method against three alternative baselines.",1
"Sepsis is a life-threatening condition affecting one million people per year in the US in which dysregulation of the body's own immune system causes damage to its tissues, resulting in a 28 - 50% mortality rate. Clinical trials for sepsis treatment over the last 20 years have failed to produce a single currently FDA approved drug treatment. In this study, we attempt to discover an effective cytokine mediation treatment strategy for sepsis using a previously developed agent-based model that simulates the innate immune response to infection: the Innate Immune Response agent-based model (IIRABM). Previous attempts at reducing mortality with multi-cytokine mediation using the IIRABM have failed to reduce mortality across all patient parameterizations and motivated us to investigate whether adaptive, personalized multi-cytokine mediation can control the trajectory of sepsis and lower patient mortality. We used the IIRABM to compute a treatment policy in which systemic patient measurements are used in a feedback loop to inform future treatment. Using deep reinforcement learning, we identified a policy that achieves 0% mortality on the patient parameterization on which it was trained. More importantly, this policy also achieves 0.8% mortality over 500 randomly selected patient parameterizations with baseline mortalities ranging from 1 - 99% (with an average of 49%) spanning the entire clinically plausible parameter space of the IIRABM. These results suggest that adaptive, personalized multi-cytokine mediation therapy could be a promising approach for treating sepsis. We hope that this work motivates researchers to consider such an approach as part of future clinical trials. To the best of our knowledge, this work is the first to consider adaptive, personalized multi-cytokine mediation therapy for sepsis, and is the first to exploit deep reinforcement learning on a biological simulation.",0
"Sepsis is a severe medical condition that endangers the lives of one million people annually in the United States. It occurs when the body's immune system malfunctions and damages its tissues, resulting in a mortality rate of 28-50%. Despite numerous clinical trials carried out over the last two decades, no drug treatment has been approved by the FDA. This study aims to discover an effective treatment strategy for sepsis using the Innate Immune Response agent-based model (IIRABM), which simulates the immune response to infection. Previous attempts to reduce mortality using multi-cytokine mediation have failed to produce positive results and prompted us to explore the possibility of personalized multi-cytokine mediation treatment. We used the IIRABM to develop a treatment policy that employs systemic patient measurements to inform future treatment. Deep reinforcement learning was utilized to identify a policy that achieved 0% mortality on the patient parameterization on which it was trained. Importantly, this policy also achieved a mortality rate of 0.8% over 500 randomly selected patient parameterizations, with baseline mortalities ranging from 1-99%, spanning the entire clinically plausible parameter space of the IIRABM. These findings support the potential of adaptive, personalized multi-cytokine mediation therapy for the treatment of sepsis. This work is the first to consider this approach and to employ deep reinforcement learning on a biological simulation. We hope that our study encourages future clinical trials to explore adaptive, personalized multi-cytokine mediation therapy for the treatment of sepsis.",1
"A key challenge in model-based reinforcement learning (RL) is to synthesize computationally efficient and accurate environment models. We show that carefully designed generative models that learn and operate on compact state representations, so-called state-space models, substantially reduce the computational costs for predicting outcomes of sequences of actions. Extensive experiments establish that state-space models accurately capture the dynamics of Atari games from the Arcade Learning Environment from raw pixels. The computational speed-up of state-space models while maintaining high accuracy makes their application in RL feasible: We demonstrate that agents which query these models for decision making outperform strong model-free baselines on the game MSPACMAN, demonstrating the potential of using learned environment models for planning.",0
"One of the main obstacles in model-based reinforcement learning (RL) is developing environment models that are both efficient and precise. Our research demonstrates that state-space models, which operate on condensed state representations and are constructed through careful generative modeling, significantly decrease the computational resources needed to forecast action sequences. Our extensive experiments confirm that state-space models accurately capture the dynamics of Atari games from raw pixels. The improved computational efficiency of state-space models coupled with their high accuracy make them a viable option for RL applications. Our study showcases that agents utilizing these models for decision making outperform strong model-free baselines in the game MSPACMAN, indicating the potential for using learned environment models in planning.",1
"To overcome the limitations of Neural Programmer-Interpreters (NPI) in its universality and learnability, we propose the incorporation of combinator abstraction into neural programing and a new NPI architecture to support this abstraction, which we call Combinatory Neural Programmer-Interpreter (CNPI). Combinator abstraction dramatically reduces the number and complexity of programs that need to be interpreted by the core controller of CNPI, while still allowing the CNPI to represent and interpret arbitrary complex programs by the collaboration of the core with the other components. We propose a small set of four combinators to capture the most pervasive programming patterns. Due to the finiteness and simplicity of this combinator set and the offloading of some burden of interpretation from the core, we are able construct a CNPI that is universal with respect to the set of all combinatorizable programs, which is adequate for solving most algorithmic tasks. Moreover, besides supervised training on execution traces, CNPI can be trained by policy gradient reinforcement learning with appropriately designed curricula.",0
"Our proposal is to enhance the universality and learnability of Neural Programmer-Interpreters (NPI) by introducing combinator abstraction into neural programming. This will involve a new NPI architecture called Combinatory Neural Programmer-Interpreter (CNPI). Combinator abstraction simplifies the number and complexity of programs that the core controller of CNPI has to interpret, while still enabling the CNPI to interpret complex programs by collaborating with other components. We suggest a small set of four combinators to capture the most common programming patterns. The finiteness and simplicity of this set, along with the offloading of interpretation burden from the core, facilitates the construction of a CNPI that is universal with respect to the set of all combinatorizable programs. This is suitable for solving most algorithmic tasks. Furthermore, CNPI can be trained by policy gradient reinforcement learning with appropriately designed curricula, in addition to supervised training on execution traces.",1
"Cross-modal hashing aims to map heterogeneous multimedia data into a common Hamming space, which can realize fast and flexible retrieval across different modalities. Supervised cross-modal hashing methods have achieved considerable progress by incorporating semantic side information. However, they mainly have two limitations: (1) Heavily rely on large-scale labeled cross-modal training data which are labor intensive and hard to obtain. (2) Ignore the rich information contained in the large amount of unlabeled data across different modalities, especially the margin examples that are easily to be incorrectly retrieved, which can help to model the correlations. To address these problems, in this paper we propose a novel Semi-supervised Cross-Modal Hashing approach by Generative Adversarial Network (SCH-GAN). We aim to take advantage of GAN's ability for modeling data distributions to promote cross-modal hashing learning in an adversarial way. The main contributions can be summarized as follows: (1) We propose a novel generative adversarial network for cross-modal hashing. In our proposed SCH-GAN, the generative model tries to select margin examples of one modality from unlabeled data when giving a query of another modality. While the discriminative model tries to distinguish the selected examples and true positive examples of the query. These two models play a minimax game so that the generative model can promote the hashing performance of discriminative model. (2) We propose a reinforcement learning based algorithm to drive the training of proposed SCH-GAN. The generative model takes the correlation score predicted by discriminative model as a reward, and tries to select the examples close to the margin to promote discriminative model by maximizing the margin between positive and negative data. Experiments on 3 widely-used datasets verify the effectiveness of our proposed approach.",0
"The objective of cross-modal hashing is to convert dissimilar multimedia data into a Hamming space that is uniform, making it possible to retrieve information quickly and flexibly across various modalities. Although supervised cross-modal hashing techniques have made notable advances by including semantic side data, they face two significant challenges: (1) they need extensive labeled data, which is arduous and challenging to obtain, and (2) they do not make use of the abundant unlabeled data found across different modalities, particularly the margin examples that could assist in modeling correlations. To tackle these issues, this study introduces a new approach called Semi-supervised Cross-Modal Hashing by Generative Adversarial Network (SCH-GAN). The study aims to leverage the GAN's capacity for modeling data distributions to enhance cross-modal hashing learning in an adversarial manner. The main contributions of this paper are two-fold: (1) it proposes a new generative adversarial network for cross-modal hashing, where the generative model chooses margin examples of one modality from the unlabeled data, while the discriminative model distinguishes between the chosen examples and the true positive examples of the query. The two models engage in a minimax game to enhance the hashing performance of the discriminative model. (2) It introduces a reinforcement learning-based algorithm to drive the training of SCH-GAN. The generative model uses the correlation score predicted by the discriminative model as a reward and selects examples close to the margin to enhance the discriminative model by maximizing the margin between positive and negative data. The study verifies the effectiveness of the proposed method through experiments on three widely-used datasets.",1
"In unsupervised data generation tasks, besides the generation of a sample based on previous observations, one would often like to give hints to the model in order to bias the generation towards desirable metrics. We propose a method that combines Generative Adversarial Networks (GANs) and reinforcement learning (RL) in order to accomplish exactly that. While RL biases the data generation process towards arbitrary metrics, the GAN component of the reward function ensures that the model still remembers information learned from data. We build upon previous results that incorporated GANs and RL in order to generate sequence data and test this model in several settings for the generation of molecules encoded as text sequences (SMILES) and in the context of music generation, showing for each case that we can effectively bias the generation process towards desired metrics.",0
"When generating data without supervision, it is often desirable to bias the process towards certain metrics. To achieve this, we propose a method that combines Generative Adversarial Networks (GANs) and reinforcement learning (RL). By using RL, the data generation process can be biased towards specific metrics, while the GAN component of the reward function ensures that the model still retains information learned from previous observations. Our approach builds upon previous studies that utilized GANs and RL to generate sequence data. We test our model in several settings, including generating molecules encoded as text sequences (SMILES) and music generation. Our results demonstrate that our method can effectively bias the data generation process towards desired metrics.",1
"Reliable and effective multi-task learning is a prerequisite for the development of robotic agents that can quickly learn to accomplish related, everyday tasks. However, in the reinforcement learning domain, multi-task learning has not exhibited the same level of success as in other domains, such as computer vision. In addition, most reinforcement learning research on multi-task learning has been focused on discrete action spaces, which are not used for robotic control in the real-world. In this work, we apply multi-task learning methods to continuous action spaces and benchmark their performance on a series of simulated continuous control tasks. Most notably, we show that multi-task learning outperforms our baselines and alternative knowledge sharing methods.",0
"For the development of efficient robotic agents capable of learning related everyday tasks quickly, reliable and effective multi-task learning is crucial. However, the success of multi-task learning in the reinforcement learning domain has not been as significant as in other domains, such as computer vision. Moreover, the research on multi-task learning in reinforcement learning has mainly concentrated on discrete action spaces that are not practical for robotic control in the real world. Therefore, in this study, we have applied multi-task learning techniques to continuous action spaces and evaluated their performance on various simulated continuous control tasks. Our results demonstrate that multi-task learning surpasses our baselines and alternative knowledge sharing approaches.",1
"With the increasing popularity of video sharing websites such as YouTube and Facebook, multimodal sentiment analysis has received increasing attention from the scientific community. Contrary to previous works in multimodal sentiment analysis which focus on holistic information in speech segments such as bag of words representations and average facial expression intensity, we develop a novel deep architecture for multimodal sentiment analysis that performs modality fusion at the word level. In this paper, we propose the Gated Multimodal Embedding LSTM with Temporal Attention (GME-LSTM(A)) model that is composed of 2 modules. The Gated Multimodal Embedding alleviates the difficulties of fusion when there are noisy modalities. The LSTM with Temporal Attention performs word level fusion at a finer fusion resolution between input modalities and attends to the most important time steps. As a result, the GME-LSTM(A) is able to better model the multimodal structure of speech through time and perform better sentiment comprehension. We demonstrate the effectiveness of this approach on the publicly-available Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis (CMU-MOSI) dataset by achieving state-of-the-art sentiment classification and regression results. Qualitative analysis on our model emphasizes the importance of the Temporal Attention Layer in sentiment prediction because the additional acoustic and visual modalities are noisy. We also demonstrate the effectiveness of the Gated Multimodal Embedding in selectively filtering these noisy modalities out. Our results and analysis open new areas in the study of sentiment analysis in human communication and provide new models for multimodal fusion.",0
"The scientific community has taken an interest in multimodal sentiment analysis due to the growing popularity of video sharing platforms like YouTube and Facebook. Previous works in this field have focused on holistic information in speech segments, such as bag of words representations and average facial expression intensity. However, a new deep architecture for multimodal sentiment analysis has been developed, which performs modality fusion at the word level. The Gated Multimodal Embedding LSTM with Temporal Attention (GME-LSTM(A)) model is composed of two modules that address the challenges of fusion when there are noisy modalities. The Gated Multimodal Embedding module filters out these modalities, while the LSTM with Temporal Attention performs word-level fusion. Our model achieved state-of-the-art sentiment classification and regression results on the Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis (CMU-MOSI) dataset. Our qualitative analysis highlighted the importance of the Temporal Attention Layer in sentiment prediction, particularly when additional acoustic and visual modalities are noisy. Our study contributes to the development of new models for multimodal fusion and opens up new areas of research in sentiment analysis in human communication.",1
"Deep reinforcement learning (RL) methods generally engage in exploratory behavior through noise injection in the action space. An alternative is to add noise directly to the agent's parameters, which can lead to more consistent exploration and a richer set of behaviors. Methods such as evolutionary strategies use parameter perturbations, but discard all temporal structure in the process and require significantly more samples. Combining parameter noise with traditional RL methods allows to combine the best of both worlds. We demonstrate that both off- and on-policy methods benefit from this approach through experimental comparison of DQN, DDPG, and TRPO on high-dimensional discrete action environments as well as continuous control tasks. Our results show that RL with parameter noise learns more efficiently than traditional RL with action space noise and evolutionary strategies individually.",0
"Rather than injecting noise into the action space, deep reinforcement learning (RL) techniques can alternatively introduce noise directly into the agent's parameters. This approach can result in more consistent exploration and a wider range of behaviors. While methods like evolutionary strategies use parameter perturbations, they lose all temporal structure and require a greater number of samples. By combining parameter noise with traditional RL techniques, the benefits of both methods can be leveraged. Through experimentation with DQN, DDPG, and TRPO on high-dimensional discrete action environments and continuous control tasks, we show that both off-policy and on-policy methods benefit from this approach. Our findings indicate that RL with parameter noise is more efficient than traditional RL with action space noise and evolutionary strategies on their own.",1
"Localization is the problem of estimating the location of an autonomous agent from an observation and a map of the environment. Traditional methods of localization, which filter the belief based on the observations, are sub-optimal in the number of steps required, as they do not decide the actions taken by the agent. We propose ""Active Neural Localizer"", a fully differentiable neural network that learns to localize accurately and efficiently. The proposed model incorporates ideas of traditional filtering-based localization methods, by using a structured belief of the state with multiplicative interactions to propagate belief, and combines it with a policy model to localize accurately while minimizing the number of steps required for localization. Active Neural Localizer is trained end-to-end with reinforcement learning. We use a variety of simulation environments for our experiments which include random 2D mazes, random mazes in the Doom game engine and a photo-realistic environment in the Unreal game engine. The results on the 2D environments show the effectiveness of the learned policy in an idealistic setting while results on the 3D environments demonstrate the model's capability of learning the policy and perceptual model jointly from raw-pixel based RGB observations. We also show that a model trained on random textures in the Doom environment generalizes well to a photo-realistic office space environment in the Unreal engine.",0
"To estimate the location of an autonomous agent in an environment, the problem of localization arises. Traditional methods of localization, which involve filtering the belief based on observations, are not optimal due to the lack of control over the agent's actions. Our proposed solution called ""Active Neural Localizer"" is a fully differentiable neural network that learns to accurately and efficiently localize. The model combines structured belief with multiplicative interactions to propagate belief and a policy model to minimize the number of steps required for localization. Training is done end-to-end with reinforcement learning. The model's effectiveness is tested in various simulation environments such as 2D and 3D mazes and photo-realistic environments in the Doom and Unreal game engines. Results show that the learned policy is effective in idealistic settings and that the model can learn the policy and perceptual model jointly from raw-pixel based RGB observations. Additionally, the model trained on random textures in the Doom environment generalizes well to a photo-realistic office space environment in the Unreal engine.",1
"The dramatic success of deep neural networks across multiple application areas often relies on experts painstakingly designing a network architecture specific to each task. To simplify this process and make it more accessible, an emerging research effort seeks to automate the design of neural network architectures, using e.g. evolutionary algorithms or reinforcement learning or simple search in a constrained space of neural modules.   Considering the typical size of the search space (e.g. $10^{10}$ candidates for a $10$-layer network) and the cost of evaluating a single candidate, current architecture search methods are very restricted. They either rely on static pre-built modules to be recombined for the task at hand, or they define a static hand-crafted framework within which they can generate new architectures from the simplest possible operations.   In this paper, we relax these restrictions, by capitalizing on the collective wisdom contained in the plethora of neural networks published in online code repositories. Concretely, we (a) extract and publish GitGraph, a corpus of neural architectures and their descriptions; (b) we create problem-specific neural architecture search spaces, implemented as a textual search mechanism over GitGraph; (c) we propose a method of identifying unique common subgraphs within the architectures solving each problem (e.g., image processing, reinforcement learning), that can then serve as modules in the newly created problem specific neural search space.",0
"Deep neural networks have achieved remarkable success in various fields, but require experts to design specific network architectures for each task. To simplify this process, researchers are exploring ways to automate the design of neural network architectures, such as using evolutionary algorithms, reinforcement learning, or simple search within a limited space of neural modules. However, current architecture search methods are constrained due to the vast search space and high cost of evaluating a single candidate. To address this, we propose a new approach that leverages the collective knowledge within the numerous neural networks published in online code repositories. Specifically, we (a) create GitGraph, a dataset of neural architectures and their descriptions; (b) develop a problem-specific neural architecture search mechanism using GitGraph; and (c) identify common subgraphs within architectures solving each problem, which can be used as modules in the new problem-specific neural search space.",1
"Recent work in deep reinforcement learning has allowed algorithms to learn complex tasks such as Atari 2600 games just from the reward provided by the game, but these algorithms presently require millions of training steps in order to learn, making them approximately five orders of magnitude slower than humans. One reason for this is that humans build robust shared representations that are applicable to collections of problems, making it much easier to assimilate new variants. This paper first introduces the idea of automatically-generated game sets to aid in transfer learning research, and then demonstrates the utility of shared representations by showing that models can substantially benefit from the incorporation of relevant architectural priors. This technique affords a remarkable 50x positive transfer on a toy problem-set.",0
"The advancement of deep reinforcement learning has enabled machines to learn complicated tasks like Atari 2600 games solely based on the game's reward. However, these algorithms currently require millions of training steps, which makes them almost five orders of magnitude slower than humans. One of the reasons for this is that humans have the ability to construct strong shared representations that can be applied to different problems, making it simpler to learn new variations. This study introduces the concept of automatically generated game sets to assist in transfer learning research. It then demonstrates the effectiveness of shared representations by revealing that incorporating relevant architectural priors can significantly benefit models. Implementing this technique resulted in a remarkable 50x positive transfer on a toy problem-set.",1
"Rapid advances of hardware-based technologies during the past decades have opened up new possibilities for Life scientists to gather multimodal data in various application domains (e.g., Omics, Bioimaging, Medical Imaging, and [Brain/Body]-Machine Interfaces), thus generating novel opportunities for development of dedicated data intensive machine learning techniques. Overall, recent research in Deep learning (DL), Reinforcement learning (RL), and their combination (Deep RL) promise to revolutionize Artificial Intelligence. The growth in computational power accompanied by faster and increased data storage and declining computing costs have already allowed scientists in various fields to apply these techniques on datasets that were previously intractable for their size and complexity. This review article provides a comprehensive survey on the application of DL, RL, and Deep RL techniques in mining Biological data. In addition, we compare performances of DL techniques when applied to different datasets across various application domains. Finally, we outline open issues in this challenging research area and discuss future development perspectives.",0
"The advancement of hardware-based technology in recent decades has created new opportunities for Life scientists to gather multimodal data in various application domains such as Omics, Bioimaging, Medical Imaging, and [Brain/Body]-Machine Interfaces. This has led to the development of dedicated data intensive machine learning techniques. The emergence of Deep learning (DL), Reinforcement learning (RL), and their combination (Deep RL) holds great promise for revolutionizing Artificial Intelligence. The increased computational power, faster and larger data storage, and declining computing costs have enabled scientists in various fields to apply these techniques on previously intractable datasets due to their size and complexity. This review article provides a comprehensive survey of the application of DL, RL, and Deep RL techniques in mining Biological data. Additionally, the performances of DL techniques on different datasets across various application domains are compared. Finally, open issues in this challenging research area are outlined and future development perspectives are discussed.",1
"Statistical performance bounds for reinforcement learning (RL) algorithms can be critical for high-stakes applications like healthcare. This paper introduces a new framework for theoretically measuring the performance of such algorithms called Uniform-PAC, which is a strengthening of the classical Probably Approximately Correct (PAC) framework. In contrast to the PAC framework, the uniform version may be used to derive high probability regret guarantees and so forms a bridge between the two setups that has been missing in the literature. We demonstrate the benefits of the new framework for finite-state episodic MDPs with a new algorithm that is Uniform-PAC and simultaneously achieves optimal regret and PAC guarantees except for a factor of the horizon.",0
"In healthcare and other high-stakes applications, it is crucial to have statistical performance bounds for reinforcement learning (RL) algorithms. A new framework, called Uniform-PAC, has been introduced in this paper to measure the performance of such algorithms theoretically. The Uniform-PAC framework is a stronger version of the traditional Probably Approximately Correct (PAC) framework. Unlike the PAC framework, the uniform version provides high probability regret guarantees and serves as a connection between the two setups that were previously missing in the literature. A new algorithm has been developed that is Uniform-PAC and achieves optimal regret and PAC guarantees except for a horizon factor. The benefits of the new framework have been demonstrated for finite-state episodic MDPs.",1
"Deep learning (DL) advances state-of-the-art reinforcement learning (RL), by incorporating deep neural networks in learning representations from the input to RL. However, the conventional deep neural network architecture is limited in learning representations for multi-task RL (MT-RL), as multiple tasks can refer to different kinds of representations. In this paper, we thus propose a novel deep neural network architecture, namely generalization tower network (GTN), which can achieve MT-RL within a single learned model. Specifically, the architecture of GTN is composed of both horizontal and vertical streams. In our GTN architecture, horizontal streams are used to learn representation shared in similar tasks. In contrast, the vertical streams are introduced to be more suitable for handling diverse tasks, which encodes hierarchical shared knowledge of these tasks. The effectiveness of the introduced vertical stream is validated by experimental results. Experimental results further verify that our GTN architecture is able to advance the state-of-the-art MT-RL, via being tested on 51 Atari games.",0
"The use of deep neural networks in reinforcement learning has resulted in significant progress, known as deep learning. However, traditional deep neural network structures have limitations in multi-task reinforcement learning, as different tasks may require different representations. To address this issue, we propose a new deep neural network architecture called the generalization tower network (GTN) that can accomplish multi-task reinforcement learning within one model. The GTN architecture comprises horizontal and vertical streams, where horizontal streams learn shared representations for similar tasks, and vertical streams encode hierarchical shared knowledge for diverse tasks. Results from experiments demonstrate that the vertical stream is effective, and the GTN architecture advances multi-task reinforcement learning, as shown in tests on 51 Atari games.",1
"Multi-task learning (MTL) with neural networks leverages commonalities in tasks to improve performance, but often suffers from task interference which reduces the benefits of transfer. To address this issue we introduce the routing network paradigm, a novel neural network and training algorithm. A routing network is a kind of self-organizing neural network consisting of two components: a router and a set of one or more function blocks. A function block may be any neural network - for example a fully-connected or a convolutional layer. Given an input the router makes a routing decision, choosing a function block to apply and passing the output back to the router recursively, terminating when a fixed recursion depth is reached. In this way the routing network dynamically composes different function blocks for each input. We employ a collaborative multi-agent reinforcement learning (MARL) approach to jointly train the router and function blocks. We evaluate our model against cross-stitch networks and shared-layer baselines on multi-task settings of the MNIST, mini-imagenet, and CIFAR-100 datasets. Our experiments demonstrate a significant improvement in accuracy, with sharper convergence. In addition, routing networks have nearly constant per-task training cost while cross-stitch networks scale linearly with the number of tasks. On CIFAR-100 (20 tasks) we obtain cross-stitch performance levels with an 85% reduction in training time.",0
"The use of Multi-task learning (MTL) with neural networks aims to enhance performance by leveraging commonalities in tasks. However, task interference often affects this approach, reducing the advantages of transfer. To combat this issue, we introduce a new neural network and training algorithm known as the routing network paradigm. This network is a self-organizing neural network that comprises a router and one or more function blocks, which can be any neural network, for example, a fully-connected or a convolutional layer. When given an input, the router chooses a function block to apply and returns the output recursively to the router until a fixed recursion depth is reached. By doing so, the routing network dynamically composes different function blocks for each input. The router and function blocks are jointly trained using a collaborative multi-agent reinforcement learning (MARL) approach. Our experiments show that our model outperforms cross-stitch networks and shared-layer baselines on multi-task settings of the MNIST, mini-imagenet, and CIFAR-100 datasets. We achieve a significant improvement in accuracy, with sharper convergence. Furthermore, routing networks have nearly constant per-task training cost, while cross-stitch networks scale linearly with the number of tasks. On CIFAR-100 (20 tasks), we achieve cross-stitch performance levels with an 85% reduction in training time.",1
"Stochastic composition optimization draws much attention recently and has been successful in many emerging applications of machine learning, statistical analysis, and reinforcement learning. In this paper, we focus on the composition problem with nonsmooth regularization penalty. Previous works either have slow convergence rate or do not provide complete convergence analysis for the general problem. In this paper, we tackle these two issues by proposing a new stochastic composition optimization method for composition problem with nonsmooth regularization penalty. In our method, we apply variance reduction technique to accelerate the speed of convergence. To the best of our knowledge, our method admits the fastest convergence rate for stochastic composition optimization: for strongly convex composition problem, our algorithm is proved to admit linear convergence; for general composition problem, our algorithm significantly improves the state-of-the-art convergence rate from $O(T^{-1/2})$ to $O((n_1+n_2)^{{2}/{3}}T^{-1})$. Finally, we apply our proposed algorithm to portfolio management and policy evaluation in reinforcement learning. Experimental results verify our theoretical analysis.",0
"Recently, stochastic composition optimization has gained significant attention due to its success in various machine learning, statistical analysis, and reinforcement learning applications. This paper focuses on the composition problem with nonsmooth regularization penalty, a problem that previous methods have struggled with due to slow convergence rates and incomplete convergence analysis. To address these issues, we propose a new stochastic composition optimization method that utilizes variance reduction techniques to accelerate convergence. Our method achieves the fastest convergence rate for stochastic composition optimization, with linear convergence for strongly convex composition problems and significantly improved convergence rates for general problems. We demonstrate the effectiveness of our algorithm in portfolio management and policy evaluation in reinforcement learning, with experimental results supporting our theoretical analysis.",1
"Autoregressive generative models consistently achieve the best results in density estimation tasks involving high dimensional data, such as images or audio. They pose density estimation as a sequence modeling task, where a recurrent neural network (RNN) models the conditional distribution over the next element conditioned on all previous elements. In this paradigm, the bottleneck is the extent to which the RNN can model long-range dependencies, and the most successful approaches rely on causal convolutions, which offer better access to earlier parts of the sequence than conventional RNNs. Taking inspiration from recent work in meta reinforcement learning, where dealing with long-range dependencies is also essential, we introduce a new generative model architecture that combines causal convolutions with self attention. In this note, we describe the resulting model and present state-of-the-art log-likelihood results on CIFAR-10 (2.85 bits per dim) and $32 \times 32$ ImageNet (3.80 bits per dim). Our implementation is available at https://github.com/neocxi/pixelsnail-public",0
"High dimensional data, such as images or audio, requires density estimation, which is best achieved through autoregressive generative models. These models view density estimation as a sequence modeling task, where a recurrent neural network (RNN) models the conditional distribution of the next element based on previous elements. The effectiveness of this approach is limited by the RNN's ability to model long-range dependencies, which can be improved by using causal convolutions instead of conventional RNNs. Our new generative model architecture combines causal convolutions with self-attention, inspired by recent work in meta reinforcement learning. Our implementation achieves state-of-the-art log-likelihood results on CIFAR-10 (2.85 bits per dim) and $32 \times 32$ ImageNet (3.80 bits per dim), and is available at https://github.com/neocxi/pixelsnail-public.",1
"Captioning models are typically trained using the cross-entropy loss. However, their performance is evaluated on other metrics designed to better correlate with human assessments. Recently, it has been shown that reinforcement learning (RL) can directly optimize these metrics in tasks such as captioning. However, this is computationally costly and requires specifying a baseline reward at each step to make training converge. We propose a fast approach to optimize one's objective of interest through the REINFORCE algorithm. First we show that, by replacing model samples with ground-truth sentences, RL training can be seen as a form of weighted cross-entropy loss, giving a fast, RL-based pre-training algorithm. Second, we propose to use the consensus among ground-truth captions of the same video as the baseline reward. This can be computed very efficiently. We call the complete proposal Consensus-based Sequence Training (CST). Applied to the MSRVTT video captioning benchmark, our proposals train significantly faster than comparable methods and establish a new state-of-the-art on the task, improving the CIDEr score from 47.3 to 54.2.",0
"Typically, captioning models are trained using the cross-entropy loss, but their performance is evaluated using other metrics that are better aligned with human assessments. Recently, it has been discovered that reinforcement learning (RL) can optimize these metrics more directly for tasks such as captioning. However, this approach is computationally expensive and requires a baseline reward at each step to ensure successful training. To address this issue, we present a new method based on the REINFORCE algorithm that quickly optimizes desired objectives. Firstly, we demonstrate that RL training can be viewed as a form of weighted cross-entropy loss by replacing model samples with ground-truth sentences, enabling fast RL-based pre-training. Secondly, we suggest using the consensus among ground-truth captions of the same video as the baseline reward, which is computationally efficient. Our complete approach, called Consensus-based Sequence Training (CST), trains much faster than comparable methods on the MSRVTT video captioning benchmark and achieves a new state-of-the-art by improving the CIDEr score from 47.3 to 54.2.",1
"Two main families of reinforcement learning algorithms, Q-learning and policy gradients, have recently been proven to be equivalent when using a softmax relaxation on one part, and an entropic regularization on the other. We relate this result to the well-known convex duality of Shannon entropy and the softmax function. Such a result is also known as the Donsker-Varadhan formula. This provides a short proof of the equivalence. We then interpret this duality further, and use ideas of convex analysis to prove a new policy inequality relative to soft Q-learning.",0
"Recently, Q-learning and policy gradients, which are the two primary families of reinforcement learning algorithms, were found to be equivalent when a softmax relaxation is applied to one component and an entropic regularization to the other. The relationship between this outcome and the established convex duality of the softmax function and Shannon entropy, also known as the Donsker-Varadhan formula, offers a concise confirmation of the equivalence. Furthermore, we utilize the principles of convex analysis to demonstrate a new policy inequality with respect to soft Q-learning by interpreting this duality.",1
"Reinforcement learning (RL) has been successfully used to solve many continuous control tasks. Despite its impressive results however, fundamental questions regarding the sample complexity of RL on continuous problems remain open. We study the performance of RL in this setting by considering the behavior of the Least-Squares Temporal Difference (LSTD) estimator on the classic Linear Quadratic Regulator (LQR) problem from optimal control. We give the first finite-time analysis of the number of samples needed to estimate the value function for a fixed static state-feedback policy to within $\varepsilon$-relative error. In the process of deriving our result, we give a general characterization for when the minimum eigenvalue of the empirical covariance matrix formed along the sample path of a fast-mixing stochastic process concentrates above zero, extending a result by Koltchinskii and Mendelson in the independent covariates setting. Finally, we provide experimental evidence indicating that our analysis correctly captures the qualitative behavior of LSTD on several LQR instances.",0
"Although Reinforcement Learning (RL) has proven effective in addressing numerous continuous control tasks, questions regarding RL's sample complexity on continuous problems persist. We investigate RL's performance in this context by examining the Least-Squares Temporal Difference (LSTD) estimator's behavior in the Linear Quadratic Regulator (LQR) problem from optimal control. Our study presents the first finite-time analysis of the number of samples required to estimate the value function for a fixed static state-feedback policy to within $\varepsilon$-relative error. Along the way, we provide a comprehensive description of when the minimum eigenvalue of the empirical covariance matrix formed from a fast-mixing stochastic process concentrates above zero, which extends a prior result by Koltchinskii and Mendelson in the independent covariates setting. Finally, we offer experimental evidence demonstrating that our analysis accurately captures LSTD's qualitative performance on various LQR instances.",1
"It is common to implicitly assume access to intelligently captured inputs (e.g., photos from a human photographer), yet autonomously capturing good observations is itself a major challenge. We address the problem of learning to look around: if a visual agent has the ability to voluntarily acquire new views to observe its environment, how can it learn efficient exploratory behaviors to acquire informative observations? We propose a reinforcement learning solution, where the agent is rewarded for actions that reduce its uncertainty about the unobserved portions of its environment. Based on this principle, we develop a recurrent neural network-based approach to perform active completion of panoramic natural scenes and 3D object shapes. Crucially, the learned policies are not tied to any recognition task nor to the particular semantic content seen during training. As a result, 1) the learned ""look around"" behavior is relevant even for new tasks in unseen environments, and 2) training data acquisition involves no manual labeling. Through tests in diverse settings, we demonstrate that our approach learns useful generic policies that transfer to new unseen tasks and environments. Completion episodes are shown at https://goo.gl/BgWX3W.",0
"Capturing good observations autonomously is a significant challenge, despite the common assumption of access to intelligently captured inputs. Our focus is on the problem of teaching a visual agent to look around and learn efficient exploratory behaviors to acquire informative observations. We propose a reinforcement learning solution where the agent is rewarded for actions that lessen its uncertainty about unobserved areas in its environment. Our approach involves a recurrent neural network-based method for active completion of panoramic natural scenes and 3D object shapes. Crucially, our learned policies are not tied to any recognition task or semantic content seen during training. This allows for the learned ""look around"" behavior to be relevant for new tasks and unseen environments, and training data acquisition requires no manual labeling. Our approach is demonstrated through tests in diverse settings, showing that our approach learns useful generic policies that transfer to new tasks and environments. Completion episodes can be viewed at https://goo.gl/BgWX3W.",1
"Humans are remarkably proficient at controlling their limbs and tools from a wide range of viewpoints and angles, even in the presence of optical distortions. In robotics, this ability is referred to as visual servoing: moving a tool or end-point to a desired location using primarily visual feedback. In this paper, we study how viewpoint-invariant visual servoing skills can be learned automatically in a robotic manipulation scenario. To this end, we train a deep recurrent controller that can automatically determine which actions move the end-point of a robotic arm to a desired object. The problem that must be solved by this controller is fundamentally ambiguous: under severe variation in viewpoint, it may be impossible to determine the actions in a single feedforward operation. Instead, our visual servoing system must use its memory of past movements to understand how the actions affect the robot motion from the current viewpoint, correcting mistakes and gradually moving closer to the target. This ability is in stark contrast to most visual servoing methods, which either assume known dynamics or require a calibration phase. We show how we can learn this recurrent controller using simulated data and a reinforcement learning objective. We then describe how the resulting model can be transferred to a real-world robot by disentangling perception from control and only adapting the visual layers. The adapted model can servo to previously unseen objects from novel viewpoints on a real-world Kuka IIWA robotic arm. For supplementary videos, see: https://fsadeghi.github.io/Sim2RealViewInvariantServo",0
"The dexterity of humans in controlling their limbs and tools from various perspectives and angles, even in the presence of optical distortions, is impressive. This ability, known as visual servoing, is applied in robotics to move a tool or endpoint to a desired location with visual feedback. This study focuses on learning viewpoint-invariant visual servoing skills in a robotic manipulation scenario. The researchers trained a deep recurrent controller to determine the actions that move the robotic arm's endpoint to a target object. However, this problem is fundamentally ambiguous, especially under severe variation in viewpoint. The visual servoing system uses its memory of past movements to understand the actions' effects on the robot's motion from the current viewpoint, correcting mistakes and gradually moving closer to the target. This skill contrasts with most visual servoing methods, which require a calibration phase or assume known dynamics. The researchers used simulated data and reinforcement learning to train the recurrent controller, then adapted the resulting model to real-world robots by disentangling perception from control and only adjusting the visual layers. The model successfully servoed to new objects from different viewpoints on a Kuka IIWA robotic arm. Supplementary videos are available at https://fsadeghi.github.io/Sim2RealViewInvariantServo.",1
"Recognizing multiple labels of images is a fundamental but challenging task in computer vision, and remarkable progress has been attained by localizing semantic-aware image regions and predicting their labels with deep convolutional neural networks. The step of hypothesis regions (region proposals) localization in these existing multi-label image recognition pipelines, however, usually takes redundant computation cost, e.g., generating hundreds of meaningless proposals with non-discriminative information and extracting their features, and the spatial contextual dependency modeling among the localized regions are often ignored or over-simplified. To resolve these issues, this paper proposes a recurrent attention reinforcement learning framework to iteratively discover a sequence of attentional and informative regions that are related to different semantic objects and further predict label scores conditioned on these regions. Besides, our method explicitly models long-term dependencies among these attentional regions that help to capture semantic label co-occurrence and thus facilitate multi-label recognition. Extensive experiments and comparisons on two large-scale benchmarks (i.e., PASCAL VOC and MS-COCO) show that our model achieves superior performance over existing state-of-the-art methods in both performance and efficiency as well as explicitly identifying image-level semantic labels to specific object regions.",0
"Computer vision faces the challenge of recognizing multiple labels in images. Localizing semantic-aware regions and predicting their labels with deep convolutional neural networks has shown significant progress. However, the hypothesis regions localization, or region proposals, in these multi-label image recognition pipelines often result in redundant computation costs. This happens when hundreds of meaningless proposals with non-discriminative information are generated, and their features extracted. Moreover, the spatial contextual dependency modeling among the localized regions is often ignored or oversimplified. To address these issues, this paper introduces a recurrent attention reinforcement learning framework. This framework iteratively discovers a sequence of attentional and informative regions related to different semantic objects. It further predicts label scores conditioned on these regions. Additionally, it explicitly models long-term dependencies among these attentional regions, which facilitate multi-label recognition by capturing semantic label co-occurrence. The proposed model achieves superior performance over existing state-of-the-art methods in both performance and efficiency while identifying image-level semantic labels to specific object regions. Extensive experiments and comparisons on two large-scale benchmarks, namely PASCAL VOC and MS-COCO, support these findings.",1
"Policy gradients methods often achieve better performance when the change in policy is limited to a small Kullback-Leibler divergence. We derive policy gradients where the change in policy is limited to a small Wasserstein distance (or trust region). This is done in the discrete and continuous multi-armed bandit settings with entropy regularisation. We show that in the small steps limit with respect to the Wasserstein distance $W_2$, policy dynamics are governed by the Fokker-Planck (heat) equation, following the Jordan-Kinderlehrer-Otto result. This means that policies undergo diffusion and advection, concentrating near actions with high reward. This helps elucidate the nature of convergence in the probability matching setup, and provides justification for empirical practices such as Gaussian policy priors and additive gradient noise.",0
"When the change in policy is restricted to a small Kullback-Leibler divergence, policy gradient methods tend to perform better. Our approach involves limiting the change in policy to a small Wasserstein distance (or trust region), and we apply it to both the discrete and continuous multi-armed bandit scenarios with entropy regularization. As we approach small steps relative to the Wasserstein distance $W_2$, the Fokker-Planck (heat) equation governs the policy dynamics, in accordance with the Jordan-Kinderlehrer-Otto result. This means that policies experience diffusion and advection, concentrating near actions with high reward. Our findings shed light on the nature of convergence in probability matching setup, and also offer a rationale for common empirical practices such as Gaussian policy priors and additive gradient noise.",1
"Bayesian optimisation has been successfully applied to a variety of reinforcement learning problems. However, the traditional approach for learning optimal policies in simulators does not utilise the opportunity to improve learning by adjusting certain environment variables: state features that are unobservable and randomly determined by the environment in a physical setting but are controllable in a simulator. This paper considers the problem of finding a robust policy while taking into account the impact of environment variables. We present Alternating Optimisation and Quadrature (ALOQ), which uses Bayesian optimisation and Bayesian quadrature to address such settings. ALOQ is robust to the presence of significant rare events, which may not be observable under random sampling, but play a substantial role in determining the optimal policy. Experimental results across different domains show that ALOQ can learn more efficiently and robustly than existing methods.",0
"Various reinforcement learning problems have seen success with Bayesian optimisation, but the traditional simulator-based approach for learning optimal policies does not take advantage of the opportunity to improve learning by modifying certain environment variables. These variables are typically unobservable state features randomly determined by the environment in a physical setting but controllable in a simulator. This paper addresses the challenge of finding a strong policy while considering the impact of these environment variables. We introduce Alternating Optimisation and Quadrature (ALOQ), which combines Bayesian optimisation and Bayesian quadrature to handle such scenarios. ALOQ is capable of handling significant rare events that may not be observable through random sampling but still have a significant impact on the optimal policy. Experimental results across various domains demonstrate that ALOQ can learn more effectively and robustly than existing methods.",1
"While bigger and deeper neural network architectures continue to advance the state-of-the-art for many computer vision tasks, real-world adoption of these networks is impeded by hardware and speed constraints. Conventional model compression methods attempt to address this problem by modifying the architecture manually or using pre-defined heuristics. Since the space of all reduced architectures is very large, modifying the architecture of a deep neural network in this way is a difficult task. In this paper, we tackle this issue by introducing a principled method for learning reduced network architectures in a data-driven way using reinforcement learning. Our approach takes a larger `teacher' network as input and outputs a compressed `student' network derived from the `teacher' network. In the first stage of our method, a recurrent policy network aggressively removes layers from the large `teacher' model. In the second stage, another recurrent policy network carefully reduces the size of each remaining layer. The resulting network is then evaluated to obtain a reward -- a score based on the accuracy and compression of the network. Our approach uses this reward signal with policy gradients to train the policies to find a locally optimal student network. Our experiments show that we can achieve compression rates of more than 10x for models such as ResNet-34 while maintaining similar performance to the input `teacher' network. We also present a valuable transfer learning result which shows that policies which are pre-trained on smaller `teacher' networks can be used to rapidly speed up training on larger `teacher' networks.",0
"Although larger and deeper neural networks have advanced computer vision tasks, their real-world adoption is hindered by hardware and speed limitations. Traditional model compression methods aim to solve this by modifying the architecture manually or using pre-defined heuristics. However, this is challenging since the space of reduced architectures is vast. This paper proposes a data-driven approach using reinforcement learning to learn compressed network architectures. The method takes a larger 'teacher' network as input and outputs a compressed 'student' network. The approach involves two stages: aggressively removing layers from the 'teacher' model using a recurrent policy network, and carefully reducing the size of each remaining layer using another recurrent policy network. The resulting network is evaluated based on accuracy and compression, and the reward signal is used with policy gradients to train the policies to find locally optimal student networks. The experiments show that models such as ResNet-34 can be compressed by more than 10x while maintaining similar performance to the input 'teacher' network. Furthermore, pre-trained policies on smaller 'teacher' networks can speed up training on larger 'teacher' networks.",1
"Machine learning models are powerful but fallible. Generating adversarial examples - inputs deliberately crafted to cause model misclassification or other errors - can yield important insight into model assumptions and vulnerabilities. Despite significant recent work on adversarial example generation targeting image classifiers, relatively little work exists exploring adversarial example generation for text classifiers; additionally, many existing adversarial example generation algorithms require full access to target model parameters, rendering them impractical for many real-world attacks. In this work, we introduce DANCin SEQ2SEQ, a GAN-inspired algorithm for adversarial text example generation targeting largely black-box text classifiers. We recast adversarial text example generation as a reinforcement learning problem, and demonstrate that our algorithm offers preliminary but promising steps towards generating semantically meaningful adversarial text examples in a real-world attack scenario.",0
"Although machine learning models are potent, they are still prone to errors. By crafting adversarial examples - inputs that are intentionally created to cause model misclassification or other types of errors - we can gain valuable insight into model assumptions and vulnerabilities. Despite the considerable efforts devoted to generating adversarial examples for image classifiers, there has been little exploration into generating these examples for text classifiers. Moreover, most existing adversarial example generation algorithms require full access to the target model parameters, which makes them impractical for real-world attacks. In this study, we propose DANCin SEQ2SEQ, a GAN-inspired algorithm that generates adversarial examples for text classifiers, even in a mostly black-box scenario. We reframe adversarial text example generation as a reinforcement learning problem and demonstrate that our algorithm takes promising initial steps towards generating semantically meaningful adversarial text examples that could be used in real-world attacks.",1
"Deep reinforcement learning (DRL) has shown incredible performance in learning various tasks to the human level. However, unlike human perception, current DRL models connect the entire low-level sensory input to the state-action values rather than exploiting the relationship between and among entities that constitute the sensory input. Because of this difference, DRL needs vast amount of experience samples to learn. In this paper, we propose a Multi-focus Attention Network (MANet) which mimics human ability to spatially abstract the low-level sensory input into multiple entities and attend to them simultaneously. The proposed method first divides the low-level input into several segments which we refer to as partial states. After this segmentation, parallel attention layers attend to the partial states relevant to solving the task. Our model estimates state-action values using these attended partial states. In our experiments, MANet attains highest scores with significantly less experience samples. Additionally, the model shows higher performance compared to the Deep Q-network and the single attention model as benchmarks. Furthermore, we extend our model to attentive communication model for performing multi-agent cooperative tasks. In multi-agent cooperative task experiments, our model shows 20% faster learning than existing state-of-the-art model.",0
"Although deep reinforcement learning (DRL) has demonstrated exceptional proficiency in mastering various tasks at a human level, it differs from human perception in that DRL models connect the entire low-level sensory input to state-action values, rather than exploiting the relationships and interconnections between the entities that make up the sensory input. Consequently, vast amounts of experience samples are required for DRL to learn. This paper proposes a Multi-focus Attention Network (MANet) that replicates human ability to abstract the low-level sensory input into multiple entities and simultaneously attend to them. The proposed approach segments the low-level input into partial states, which are subsequently addressed by parallel attention layers that are relevant to solving the task, and the model estimates state-action values based on these attended partial states. In our experiments, MANet achieves the highest scores with significantly fewer experience samples than other benchmarks, including the Deep Q-network and the single attention model. Moreover, our model extends to an attentive communication model for performing multi-agent cooperative tasks, where it demonstrates a 20% faster learning rate than the current state-of-the-art model.",1
"This paper aims at theoretically and empirically comparing two standard optimization criteria for Reinforcement Learning: i) maximization of the mean value and ii) minimization of the Bellman residual. For that purpose, we place ourselves in the framework of policy search algorithms, that are usually designed to maximize the mean value, and derive a method that minimizes the residual $\|T_* v_\pi - v_\pi\|_{1,\nu}$ over policies. A theoretical analysis shows how good this proxy is to policy optimization, and notably that it is better than its value-based counterpart. We also propose experiments on randomly generated generic Markov decision processes, specifically designed for studying the influence of the involved concentrability coefficient. They show that the Bellman residual is generally a bad proxy to policy optimization and that directly maximizing the mean value is much better, despite the current lack of deep theoretical analysis. This might seem obvious, as directly addressing the problem of interest is usually better, but given the prevalence of (projected) Bellman residual minimization in value-based reinforcement learning, we believe that this question is worth to be considered.",0
"This paper aims to compare two common optimization criteria for Reinforcement Learning, namely maximizing the mean value and minimizing the Bellman residual, both theoretically and empirically. To achieve this, we adopt policy search algorithms, which are typically designed for maximizing the mean value, and develop a method for minimizing the residual $\|T_* v_\pi - v_\pi\|_{1,\nu}$ across policies. Our theoretical analysis demonstrates the efficacy of this approach for policy optimization, surpassing its value-based counterpart. We also present experiments on randomly generated generic Markov decision processes, specifically designed to study the influence of the concentrability coefficient. These experiments reveal that minimizing the Bellman residual is generally not a good approach for policy optimization, and that directly maximizing the mean value is far superior, despite the current lack of deep theoretical analysis. This may seem self-evident, since addressing the problem directly is usually better, but given the widespread use of (projected) Bellman residual minimization in value-based reinforcement learning, we believe that this issue warrants further investigation.",1
"A core aspect of human intelligence is the ability to learn new tasks quickly and switch between them flexibly. Here, we describe a modular continual reinforcement learning paradigm inspired by these abilities. We first introduce a visual interaction environment that allows many types of tasks to be unified in a single framework. We then describe a reward map prediction scheme that learns new tasks robustly in the very large state and action spaces required by such an environment. We investigate how properties of module architecture influence efficiency of task learning, showing that a module motif incorporating specific design principles (e.g. early bottlenecks, low-order polynomial nonlinearities, and symmetry) significantly outperforms more standard neural network motifs, needing fewer training examples and fewer neurons to achieve high levels of performance. Finally, we present a meta-controller architecture for task switching based on a dynamic neural voting scheme, which allows new modules to use information learned from previously-seen tasks to substantially improve their own learning efficiency.",0
"The ability to quickly learn new tasks and adapt to changes is a crucial element of human intelligence. In this study, we propose a modular continual reinforcement learning approach that draws inspiration from these abilities. Our method incorporates a visual interaction environment that can accommodate various tasks within a single framework. We also introduce a reward map prediction technique that can effectively learn new tasks in large state and action spaces. We investigate the impact of module architecture on task learning efficiency and find that a module motif with specific design principles (such as early bottlenecks, low-order polynomial nonlinearities, and symmetry) outperforms standard neural network motifs, requiring fewer training examples and neurons to achieve high performance. Finally, we present a meta-controller architecture for task switching that utilizes a dynamic neural voting scheme, which improves learning efficiency by allowing new modules to leverage information learned from previously-seen tasks.",1
"Recent improvements in deep reinforcement learning have allowed to solve problems in many 2D domains such as Atari games. However, in complex 3D environments, numerous learning episodes are required which may be too time consuming or even impossible especially in real-world scenarios. We present a new architecture to combine external knowledge and deep reinforcement learning using only visual input. A key concept of our system is augmenting image input by adding environment feature information and combining two sources of decision. We evaluate the performances of our method in a 3D partially-observable environment from the Microsoft Malmo platform. Experimental evaluation exhibits higher performance and faster learning compared to a single reinforcement learning model.",0
"Advancements in deep reinforcement learning have enabled problem-solving in multiple 2D domains, including Atari games. However, learning in complex 3D environments necessitates several episodes, which may be impractical or infeasible in real-world scenarios. To address this, we propose a novel architecture that integrates external knowledge with deep reinforcement learning, utilizing solely visual input. Our system augments image input by integrating environmental feature information and fusing two sources of decision. We assess our method's efficiency in a 3D partially-observable environment using the Microsoft Malmo platform. Experimental results demonstrate superior performance and quicker learning compared to a single reinforcement learning model.",1
"We present MINOS, a simulator designed to support the development of multisensory models for goal-directed navigation in complex indoor environments. The simulator leverages large datasets of complex 3D environments and supports flexible configuration of multimodal sensor suites. We use MINOS to benchmark deep-learning-based navigation methods, to analyze the influence of environmental complexity on navigation performance, and to carry out a controlled study of multimodality in sensorimotor learning. The experiments show that current deep reinforcement learning approaches fail in large realistic environments. The experiments also indicate that multimodality is beneficial in learning to navigate cluttered scenes. MINOS is released open-source to the research community at http://minosworld.org . A video that shows MINOS can be found at https://youtu.be/c0mL9K64q84",0
"Introducing MINOS, a simulator that facilitates the creation of multisensory models for navigating complex indoor environments. By utilizing extensive 3D datasets and flexible sensor suite configurations, MINOS allows for the evaluation of deep-learning-based navigation methods, analysis of environmental effects on performance, and controlled studies of sensorimotor learning with multimodality. Results reveal the inadequacy of current deep reinforcement learning approaches in large, realistic environments, while also demonstrating the benefits of multimodal learning in navigating cluttered scenes. MINOS is available as an open-source tool for the research community at http://minosworld.org , and a video showcasing its capabilities can be viewed at https://youtu.be/c0mL9K64q84.",1
"This paper proposes adversarial attacks for Reinforcement Learning (RL) and then improves the robustness of Deep Reinforcement Learning algorithms (DRL) to parameter uncertainties with the help of these attacks. We show that even a naively engineered attack successfully degrades the performance of DRL algorithm. We further improve the attack using gradient information of an engineered loss function which leads to further degradation in performance. These attacks are then leveraged during training to improve the robustness of RL within robust control framework. We show that this adversarial training of DRL algorithms like Deep Double Q learning and Deep Deterministic Policy Gradients leads to significant increase in robustness to parameter variations for RL benchmarks such as Cart-pole, Mountain Car, Hopper and Half Cheetah environment.",0
"In this paper, we introduce adversarial attacks to Reinforcement Learning (RL) and demonstrate how these attacks can enhance the robustness of Deep Reinforcement Learning algorithms (DRL) against parameter uncertainties. Our findings reveal that even a simple attack can significantly impair the performance of DRL algorithms and that incorporating gradient information of an engineered loss function can further deteriorate performance. We utilize these attacks during training to augment the robustness of RL in a robust control framework. Our results demonstrate that adversarial training of DRL algorithms, such as Deep Double Q learning and Deep Deterministic Policy Gradients, can substantially improve their resilience to parameter variations in widely-used RL benchmarks, including Cart-pole, Mountain Car, Hopper, and Half Cheetah environment.",1
"We develop a parameterized Primal-Dual $\pi$ Learning method based on deep neural networks for Markov decision process with large state space and off-policy reinforcement learning. In contrast to the popular Q-learning and actor-critic methods that are based on successive approximations to the nonlinear Bellman equation, our method makes primal-dual updates to the policy and value functions utilizing the fundamental linear Bellman duality. Naive parametrization of the primal-dual $\pi$ learning method using deep neural networks would encounter two major challenges: (1) each update requires computing a probability distribution over the state space and is intractable; (2) the iterates are unstable since the parameterized Lagrangian function is no longer linear. We address these challenges by proposing a relaxed Lagrangian formulation with a regularization penalty using the advantage function. We show that the dual policy update step in our method is equivalent to the policy gradient update in the actor-critic method in some special case, while the value updates differ substantially. The main advantage of the primal-dual $\pi$ learning method lies in that the value and policy updates are closely coupled together using the Bellman duality and therefore more informative. Experiments on a simple cart-pole problem show that the algorithm significantly outperforms the one-step temporal-difference actor-critic method, which is the most relevant benchmark method to compare with. We believe that the primal-dual updates to the value and policy functions would expedite the learning process. The proposed methods might open a door to more efficient algorithms and sharper theoretical analysis.",0
"Our approach is to create a parameterized Primal-Dual $\pi$ Learning method for off-policy reinforcement learning in Markov decision processes with large state spaces, utilizing deep neural networks. While existing methods rely on approximating the nonlinear Bellman equation, we use primal-dual updates to both policy and value functions with the linear Bellman duality. However, implementing this method with deep neural networks presents two significant challenges: the need to compute a probability distribution over the state space for each update is infeasible, and the iterates are unstable as the parameterized Lagrangian function is no longer linear. To address these challenges, we propose a relaxed Lagrangian formulation with a regularization penalty using the advantage function. Our dual policy update step is equivalent to the policy gradient update in the actor-critic method in some cases, but our value updates differ significantly. The main advantage of our method is the close coupling between value and policy updates using the Bellman duality, making them more informative. Experiments on a cart-pole problem demonstrate that our method outperforms the one-step temporal-difference actor-critic method, which is the most relevant benchmark method. We expect that our approach will accelerate the learning process and lead to more efficient algorithms and sharper theoretical analysis.",1
"Glycemic control is essential for critical care. However, it is a challenging task because there has been no study on personalized optimal strategies for glycemic control. This work aims to learn personalized optimal glycemic trajectories for severely ill septic patients by learning data-driven policies to identify optimal targeted blood glucose levels as a reference for clinicians. We encoded patient states using a sparse autoencoder and adopted a reinforcement learning paradigm using policy iteration to learn the optimal policy from data. We also estimated the expected return following the policy learned from the recorded glycemic trajectories, which yielded a function indicating the relationship between real blood glucose values and 90-day mortality rates. This suggests that the learned optimal policy could reduce the patients' estimated 90-day mortality rate by 6.3%, from 31% to 24.7%. The result demonstrates that reinforcement learning with appropriate patient state encoding can potentially provide optimal glycemic trajectories and allow clinicians to design a personalized strategy for glycemic control in septic patients.",0
"Achieving glycemic control in critical care is crucial, but challenging due to the lack of personalized optimal strategies. This study aimed to address this issue by developing data-driven policies to identify optimal targeted blood glucose levels for severely ill septic patients. Patient states were encoded using a sparse autoencoder, and a reinforcement learning paradigm was adopted to learn the optimal policy from data. The expected return following the policy was estimated, revealing a relationship between real blood glucose values and 90-day mortality rates. The results showed that the learned optimal policy could potentially reduce the patients' estimated 90-day mortality rate by 6.3%, from 31% to 24.7%. These findings demonstrate the potential of reinforcement learning and appropriate patient state encoding to provide optimal glycemic trajectories, allowing clinicians to design personalized strategies for glycemic control in septic patients.",1
"Predictable Feature Analysis (PFA) (Richthofer, Wiskott, ICMLA 2015) is an algorithm that performs dimensionality reduction on high dimensional input signal. It extracts those subsignals that are most predictable according to a certain prediction model. We refer to these extracted signals as predictable features.   In this work we extend the notion of PFA to take supplementary information into account for improving its predictions. Such information can be a multidimensional signal like the main input to PFA, but is regarded external. That means it won't participate in the feature extraction - no features get extracted or composed of it. Features will be exclusively extracted from the main input such that they are most predictable based on themselves and the supplementary information. We refer to this enhanced PFA as PFAx (PFA extended).   Even more important than improving prediction quality is to observe the effect of supplementary information on feature selection. PFAx transparently provides insight how the supplementary information adds to prediction quality and whether it is valuable at all. Finally we show how to invert that relation and can generate the supplementary information such that it would yield a certain desired outcome of the main signal.   We apply this to a setting inspired by reinforcement learning and let the algorithm learn how to control an agent in an environment. With this method it is feasible to locally optimize the agent's state, i.e. reach a certain goal that is near enough. We are preparing a follow-up paper that extends this method such that also global optimization is feasible.",0
"The Predictable Feature Analysis (PFA) algorithm, developed by Richthofer and Wiskott in 2015, is used to reduce the dimensionality of high dimensional input signals by extracting the subsignals that are most predictable based on a specific prediction model. These extracted signals, known as predictable features, are exclusively derived from the main input, with no influence from external multidimensional signals. However, in this study, we have extended the concept of PFA to incorporate supplementary information that can potentially enhance the accuracy of predictions. While this supplementary information is not involved in feature extraction, PFAx (PFA extended) uses it to improve the predictability of features based on both the main input and the supplementary information. PFAx also provides valuable insights into the effect of supplementary information on feature selection and prediction quality. Additionally, we demonstrate in our study how the relationship between the main signal and the supplementary information can be inverted to achieve a specific desired outcome. We have applied this method to a reinforcement learning scenario to optimize the state of an agent in an environment. We are currently working on a follow-up paper that expands this method to enable global optimization.",1
"We introduce MAgent, a platform to support research and development of many-agent reinforcement learning. Unlike previous research platforms on single or multi-agent reinforcement learning, MAgent focuses on supporting the tasks and the applications that require hundreds to millions of agents. Within the interactions among a population of agents, it enables not only the study of learning algorithms for agents' optimal polices, but more importantly, the observation and understanding of individual agent's behaviors and social phenomena emerging from the AI society, including communication languages, leaderships, altruism. MAgent is highly scalable and can host up to one million agents on a single GPU server. MAgent also provides flexible configurations for AI researchers to design their customized environments and agents. In this demo, we present three environments designed on MAgent and show emerged collective intelligence by learning from scratch.",0
"MAgent is a novel platform that aims to facilitate the advancement of many-agent reinforcement learning research and development. Unlike existing research platforms that focus on single or multi-agent reinforcement learning, MAgent caters to tasks and applications that necessitate hundreds to millions of agents. By enabling the observation and comprehension of individual agent behaviors and social phenomena that arise from interactions among a population of agents, such as communication languages, leaderships, and altruism, MAgent allows for the study of learning algorithms for agents' optimal policies. MAgent is highly scalable and can support up to one million agents on a single GPU server. Additionally, MAgent offers customizable environments and agents, allowing AI researchers to tailor their configurations as per their requirements. In this demonstration, we showcase three environments that we designed on MAgent, highlighting the collective intelligence that emerged from learning from scratch.",1
"We study online reinforcement learning in average-reward stochastic games (SGs). An SG models a two-player zero-sum game in a Markov environment, where state transitions and one-step payoffs are determined simultaneously by a learner and an adversary. We propose the UCSG algorithm that achieves a sublinear regret compared to the game value when competing with an arbitrary opponent. This result improves previous ones under the same setting. The regret bound has a dependency on the diameter, which is an intrinsic value related to the mixing property of SGs. If we let the opponent play an optimistic best response to the learner, UCSG finds an $\varepsilon$-maximin stationary policy with a sample complexity of $\tilde{\mathcal{O}}\left(\text{poly}(1/\varepsilon)\right)$, where $\varepsilon$ is the gap to the best policy.",0
"Our focus is on studying online reinforcement learning in average-reward stochastic games (SGs), which are models of two-player zero-sum games in a Markov environment. In these games, both state transitions and one-step payoffs are determined simultaneously by a learner and an adversary. We have developed the UCSG algorithm, which outperforms previous methods by achieving sublinear regret compared to the game value when competing with any opponent. The regret bound is dependent on the diameter, which is a value that is related to the mixing property of SGs. If the opponent plays an optimistic best response to the learner, UCSG can find an $\varepsilon$-maximin stationary policy with a sample complexity of $\tilde{\mathcal{O}}\left(\text{poly}(1/\varepsilon)\right)$. This approach is highly effective in minimizing the gap to the best policy.",1
"Model-free deep reinforcement learning algorithms have been shown to be capable of learning a wide range of robotic skills, but typically require a very large number of samples to achieve good performance. Model-based algorithms, in principle, can provide for much more efficient learning, but have proven difficult to extend to expressive, high-capacity models such as deep neural networks. In this work, we demonstrate that medium-sized neural network models can in fact be combined with model predictive control (MPC) to achieve excellent sample complexity in a model-based reinforcement learning algorithm, producing stable and plausible gaits to accomplish various complex locomotion tasks. We also propose using deep neural network dynamics models to initialize a model-free learner, in order to combine the sample efficiency of model-based approaches with the high task-specific performance of model-free methods. We empirically demonstrate on MuJoCo locomotion tasks that our pure model-based approach trained on just random action data can follow arbitrary trajectories with excellent sample efficiency, and that our hybrid algorithm can accelerate model-free learning on high-speed benchmark tasks, achieving sample efficiency gains of 3-5x on swimmer, cheetah, hopper, and ant agents. Videos can be found at https://sites.google.com/view/mbmf",0
"Although model-free deep reinforcement learning algorithms are capable of learning a variety of robotic skills, they often require a large number of samples to perform well. Model-based algorithms can offer more efficient learning, but are difficult to extend to high-capacity models such as deep neural networks. This study demonstrates that medium-sized neural network models can be combined with model predictive control (MPC) to achieve excellent sample efficiency in a model-based reinforcement learning algorithm, producing stable gaits for various complex locomotion tasks. Additionally, deep neural network dynamics models can be used to initialize a model-free learner, allowing for a combination of the efficiency of model-based approaches with the performance of model-free methods. Empirical evidence shows that the pure model-based approach trained solely on random action data can follow any trajectory with excellent sample efficiency, and that the hybrid algorithm can accelerate model-free learning on high-speed benchmark tasks, achieving sample efficiency gains of 3-5x on swimmer, cheetah, hopper, and ant agents. Videos of the results can be found at https://sites.google.com/view/mbmf.",1
"We present a new AI task -- Embodied Question Answering (EmbodiedQA) -- where an agent is spawned at a random location in a 3D environment and asked a question (""What color is the car?""). In order to answer, the agent must first intelligently navigate to explore the environment, gather information through first-person (egocentric) vision, and then answer the question (""orange"").   This challenging task requires a range of AI skills -- active perception, language understanding, goal-driven navigation, commonsense reasoning, and grounding of language into actions. In this work, we develop the environments, end-to-end-trained reinforcement learning agents, and evaluation protocols for EmbodiedQA.",0
"A new AI task called Embodied Question Answering (EmbodiedQA) is introduced in which an agent is placed at a random location in a 3D environment and asked a question (""What color is the car?""). To provide an answer, the agent must first navigate intelligently to explore the surroundings, gather information through first-person (egocentric) vision, and then answer the question (""orange""). This task is challenging as it requires a variety of AI skills such as active perception, language comprehension, goal-driven navigation, commonsense reasoning, and language grounding into actions. The development of environments, reinforcement learning agents, and assessment protocols for EmbodiedQA is presented in this study.",1
"Learning to Optimize is a recently proposed framework for learning optimization algorithms using reinforcement learning. In this paper, we explore learning an optimization algorithm for training shallow neural nets. Such high-dimensional stochastic optimization problems present interesting challenges for existing reinforcement learning algorithms. We develop an extension that is suited to learning optimization algorithms in this setting and demonstrate that the learned optimization algorithm consistently outperforms other known optimization algorithms even on unseen tasks and is robust to changes in stochasticity of gradients and the neural net architecture. More specifically, we show that an optimization algorithm trained with the proposed method on the problem of training a neural net on MNIST generalizes to the problems of training neural nets on the Toronto Faces Dataset, CIFAR-10 and CIFAR-100.",0
"A framework called Learning to Optimize has been proposed, which utilizes reinforcement learning to teach optimization algorithms. Our research delves into using this framework to train shallow neural nets with an optimization algorithm. This poses challenges for existing reinforcement learning algorithms due to the high-dimensional stochastic optimization problems involved. Therefore, we have developed an extension that addresses this issue and demonstrated that the learned optimization algorithm surpasses other known optimization algorithms, even when applied to new tasks and faced with changes in gradient stochasticity and neural net architecture. Specifically, we have proven that the proposed optimization algorithm, which was trained on the MNIST problem, can be applied to training neural nets for the Toronto Faces Dataset, CIFAR-10, and CIFAR-100.",1
"In statistical dialogue management, the dialogue manager learns a policy that maps a belief state to an action for the system to perform. Efficient exploration is key to successful policy optimisation. Current deep reinforcement learning methods are very promising but rely on epsilon-greedy exploration, thus subjecting the user to a random choice of action during learning. Alternative approaches such as Gaussian Process SARSA (GPSARSA) estimate uncertainties and are sample efficient, leading to better user experience, but on the expense of a greater computational complexity. This paper examines approaches to extract uncertainty estimates from deep Q-networks (DQN) in the context of dialogue management. We perform an extensive benchmark of deep Bayesian methods to extract uncertainty estimates, namely Bayes-By-Backprop, dropout, its concrete variation, bootstrapped ensemble and alpha-divergences, combining it with DQN algorithm.",0
"The process of statistical dialogue management involves the dialogue manager learning a policy that links a belief state to a system action. Effective policy optimization relies on efficient exploration. Although current deep reinforcement learning methods show promise, they use epsilon-greedy exploration, which results in the user being subjected to random action choices during learning. To improve the user experience, alternative approaches such as Gaussian Process SARSA (GPSARSA) estimate uncertainties and are more sample efficient, but this comes at the cost of greater computational complexity. This paper investigates ways to extract uncertainty estimates from deep Q-networks (DQN) in the context of dialogue management. We extensively benchmark different deep Bayesian methods, including Bayes-By-Backprop, dropout, its concrete variation, bootstrapped ensemble, and alpha-divergences, and combine them with the DQN algorithm.",1
"Learning an optimal policy from a multi-modal reward function is a challenging problem in reinforcement learning (RL). Hierarchical RL (HRL) tackles this problem by learning a hierarchical policy, where multiple option policies are in charge of different strategies corresponding to modes of a reward function and a gating policy selects the best option for a given context. Although HRL has been demonstrated to be promising, current state-of-the-art methods cannot still perform well in complex real-world problems due to the difficulty of identifying modes of the reward function. In this paper, we propose a novel method called hierarchical policy search via return-weighted density estimation (HPSDE), which can efficiently identify the modes through density estimation with return-weighted importance sampling. Our proposed method finds option policies corresponding to the modes of the return function and automatically determines the number and the location of option policies, which significantly reduces the burden of hyper-parameters tuning. Through experiments, we demonstrate that the proposed HPSDE successfully learns option policies corresponding to modes of the return function and that it can be successfully applied to a challenging motion planning problem of a redundant robotic manipulator.",0
"Reinforcement learning (RL) faces a challenging obstacle when learning an optimal policy from a multi-modal reward function. To address this problem, hierarchical RL (HRL) has been developed, which involves learning a hierarchical policy with multiple option policies assigned to different strategies based on the modes of a reward function. However, existing HRL methods struggle to perform well in complex real-world situations due to the difficulty of detecting modes of the reward function. To overcome this issue, we propose a new method called hierarchical policy search via return-weighted density estimation (HPSDE). Our approach efficiently identifies modes by utilizing density estimation with return-weighted importance sampling. HPSDE determines the number and location of option policies automatically, which significantly reduces the need for hyper-parameter tuning. Experimental results demonstrate that HPSDE successfully learns option policies corresponding to modes of the return function and can be applied to solve challenging motion planning problems for redundant robotic manipulators.",1
"We view intersection handling on autonomous vehicles as a reinforcement learning problem, and study its behavior in a transfer learning setting. We show that a network trained on one type of intersection generally is not able to generalize to other intersections. However, a network that is pre-trained on one intersection and fine-tuned on another performs better on the new task compared to training in isolation. This network also retains knowledge of the prior task, even though some forgetting occurs. Finally, we show that the benefits of fine-tuning hold when transferring simulated intersection handling knowledge to a real autonomous vehicle.",0
"The handling of intersections in autonomous vehicles is perceived as a problem of reinforcement learning, and we investigate its performance in a transfer learning context. Our findings indicate that a network trained on a specific type of intersection does not have the ability to generalize to other intersections. Nonetheless, a pre-trained network on one intersection that is further refined on another performs better on the new task than when trained in isolation. This network also retains knowledge of the previous task despite some degree of forgetting. Ultimately, we demonstrate that fine-tuning results in advantages when transferring simulated intersection handling knowledge to an actual autonomous vehicle.",1
"Safely exploring an unknown dynamical system is critical to the deployment of reinforcement learning (RL) in physical systems where failures may have catastrophic consequences. In scenarios where one knows little about the dynamics, diverse transition data covering relevant regions of state-action space is needed to apply either model-based or model-free RL. Motivated by the cooling of Google's data centers, we study how one can safely identify the parameters of a system model with a desired accuracy and confidence level. In particular, we focus on learning an unknown linear system with Gaussian noise assuming only that, initially, a nominal safe action is known. Define safety as satisfying specific linear constraints on the state space (e.g., requirements on process variable) that must hold over the span of an entire trajectory, and given a Probably Approximately Correct (PAC) style bound on the estimation error of model parameters, we show how to compute safe regions of action space by gradually growing a ball around the nominal safe action. One can apply any exploration strategy where actions are chosen from such safe regions. Experiments on a stylized model of data center cooling dynamics show how computing proper safe regions can increase the sample efficiency of safe exploration.",0
"Ensuring the safe exploration of an unfamiliar dynamical system is crucial for the successful implementation of reinforcement learning (RL) in physical systems where any failure could result in dire consequences. If one possesses limited knowledge of the dynamics, it is necessary to gather a range of transition data that covers relevant regions of the state-action space to apply either model-based or model-free RL. Our study is motivated by the cooling of Google's data centers and aims to identify the system model's parameters safely with the desired level of accuracy and confidence. Specifically, we concentrate on learning an unknown linear system with Gaussian noise, starting with knowledge of a nominal safe action. We define safety as fulfilling specific linear constraints on the state space, which must remain valid throughout an entire trajectory. Given a Probably Approximately Correct (PAC) style bound on the model parameter estimation error, we demonstrate how to calculate safe regions of action space by gradually expanding a ball around the nominal safe action. Any exploration strategy may be utilized with actions chosen from these safe regions. Our experiments on a stylized model of data center cooling dynamics show that determining appropriate safe regions can increase the sample efficiency of safe exploration.",1
