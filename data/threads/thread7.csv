"Learning an efficient update rule from data that promotes rapid learning of new tasks from the same distribution remains an open problem in meta-learning. Typically, previous works have approached this issue either by attempting to train a neural network that directly produces updates or by attempting to learn better initialisations or scaling factors for a gradient-based update rule. Both of these approaches pose challenges. On one hand, directly producing an update forgoes a useful inductive bias and can easily lead to non-converging behaviour. On the other hand, approaches that try to control a gradient-based update rule typically resort to computing gradients through the learning process to obtain their meta-gradients, leading to methods that can not scale beyond few-shot task adaptation. In this work, we propose Warped Gradient Descent (WarpGrad), a method that intersects these approaches to mitigate their limitations. WarpGrad meta-learns an efficiently parameterised preconditioning matrix that facilitates gradient descent across the task distribution. Preconditioning arises by interleaving non-linear layers, referred to as warp-layers, between the layers of a task-learner. Warp-layers are meta-learned without backpropagating through the task training process in a manner similar to methods that learn to directly produce updates. WarpGrad is computationally efficient, easy to implement, and can scale to arbitrarily large meta-learning problems. We provide a geometrical interpretation of the approach and evaluate its effectiveness in a variety of settings, including few-shot, standard supervised, continual and reinforcement learning.",0
"The challenge of meta-learning is to find an effective update rule that can promote swift learning of new tasks from the same distribution. In previous studies, researchers have attempted to tackle this issue by either training a neural network to produce updates directly or by attempting to enhance the initialisations or scaling factors of a gradient-based update rule. However, both methods present difficulties. Directly producing updates eliminates a valuable inductive bias and can lead to non-convergence. Meanwhile, approaches that manipulate a gradient-based update rule require gradient calculations throughout the learning process, limiting their scalability to only a few-shot task adaptation. This study introduces Warped Gradient Descent (WarpGrad), a method that combines these approaches to address their limitations. WarpGrad learns an optimally parameterised preconditioning matrix that facilitates gradient descent across the task distribution, achieved by using non-linear warp-layers placed between the layers of a task-learner. These warp-layers are learned meta-learners that are not backpropagated through the task training process, similar to methods that learn to directly produce updates. WarpGrad is computationally efficient, easy to implement, and can be used in meta-learning problems of any size. The effectiveness of this approach is evaluated in various settings, including few-shot, standard supervised, continual and reinforcement learning, and a geometric interpretation of the method is provided.",1
"Deep reinforcement learning has the potential to train robots to perform complex tasks in the real world without requiring accurate models of the robot or its environment. A practical approach is to train agents in simulation, and then transfer them to the real world. One popular method for achieving transferability is to use domain randomisation, which involves randomly perturbing various aspects of a simulated environment in order to make trained agents robust to the reality gap. However, less work has gone into understanding such agents - which are deployed in the real world - beyond task performance. In this work we examine such agents, through qualitative and quantitative comparisons between agents trained with and without visual domain randomisation. We train agents for Fetch and Jaco robots on a visuomotor control task and evaluate how well they generalise using different testing conditions. Finally, we investigate the internals of the trained agents by using a suite of interpretability techniques. Our results show that the primary outcome of domain randomisation is more robust, entangled representations, accompanied with larger weights with greater spatial structure; moreover, the types of changes are heavily influenced by the task setup and presence of additional proprioceptive inputs. Additionally, we demonstrate that our domain randomised agents require higher sample complexity, can overfit and more heavily rely on recurrent processing. Furthermore, even with an improved saliency method introduced in this work, we show that qualitative studies may not always correspond with quantitative measures, necessitating the combination of inspection tools in order to provide sufficient insights into the behaviour of trained agents.",0
"The potential of deep reinforcement learning is to train robots to perform complex tasks in the real world without the need for accurate models of the robot or its environment. One way to achieve this is to train agents in simulation and then transfer them to the real world. This transferability is often accomplished through domain randomisation, which involves randomly perturbing various aspects of a simulated environment to make trained agents more robust to the reality gap. However, little attention has been paid to understanding these agents beyond their task performance in the real world. In this study, we examine these agents by comparing those trained with and without visual domain randomisation using qualitative and quantitative methods. We trained agents for Fetch and Jaco robots on a visuomotor control task and evaluated their generalisation abilities under different testing conditions. Additionally, we investigated the internals of the trained agents using a suite of interpretability techniques. Our findings suggest that domain randomisation primarily creates more robust, entangled representations with larger weights and greater spatial structure. However, the types of changes depend heavily on the task setup and the presence of additional proprioceptive inputs. We also found that domain randomised agents require more sample complexity, can overfit, and more heavily rely on recurrent processing. Finally, we demonstrate that a combination of inspection tools is necessary to provide sufficient insights into the behaviour of trained agents, as qualitative studies may not always correspond with quantitative measures.",1
"In this work we present a new method of black-box optimization and constraint satisfaction. Existing algorithms that have attempted to solve this problem are unable to consider multiple modes, and are not able to adapt to changes in environment dynamics. To address these issues, we developed a modified Cross-Entropy Method (CEM) that uses a masked auto-regressive neural network for modeling uniform distributions over the solution space. We train the model using maximum entropy policy gradient methods from Reinforcement Learning. Our algorithm is able to express complicated solution spaces, thus allowing it to track a variety of different solution regions. We empirically compare our algorithm with variations of CEM, including one with a Gaussian prior with fixed variance, and demonstrate better performance in terms of: number of diverse solutions, better mode discovery in multi-modal problems, and better sample efficiency in certain cases.",0
"Our study introduces a novel approach for black-box optimization and constraint satisfaction. Current methods have limitations in handling multiple modes and adapting to changes in environmental dynamics. To overcome these challenges, we have devised a modified Cross-Entropy Method (CEM) that incorporates a masked auto-regressive neural network to model uniform distributions across the solution space. Maximum entropy policy gradient techniques from Reinforcement Learning are employed to train the model. Our method can handle complex solution spaces, enabling it to track various solution regions. We have performed a comparative analysis with different CEM variations, including one with a Gaussian prior with fixed variance. Our method outperforms the others in terms of discovering multiple solutions, identifying modes in multi-modal problems, and enhancing sample efficiency in particular cases, as validated by our empirical results.",1
"Signal temporal logic (STL) is an expressive language to specify time-bound real-world robotic tasks and safety specifications. Recently, there has been an interest in learning optimal policies to satisfy STL specifications via reinforcement learning (RL). Learning to satisfy STL specifications often needs a sufficient length of state history to compute reward and the next action. The need for history results in exponential state-space growth for the learning problem. Thus the learning problem becomes computationally intractable for most real-world applications. In this paper, we propose a compact means to capture state history in a new augmented state-space representation. An approximation to the objective (maximizing probability of satisfaction) is proposed and solved for in the new augmented state-space. We show the performance bound of the approximate solution and compare it with the solution of an existing technique via simulations.",0
"Signal temporal logic (STL) is a powerful tool for specifying time-constrained tasks and safety requirements for real-world robots. Recently, there has been growing interest in using reinforcement learning (RL) to learn optimal policies that satisfy STL specifications. However, this learning process typically requires a lengthy state history to compute rewards and determine the next action, which leads to exponential growth in the state space. Consequently, this renders the learning problem computationally impractical for most real-world applications. This paper proposes a novel method of capturing state history in an augmented state-space representation, which is more compact. We also introduce an approximation to the objective, which involves maximizing the probability of satisfaction, and solve it within the new augmented state-space. We evaluate the performance of our solution and compare it with an existing technique via simulations.",1
"Policy evaluation is a key process in Reinforcement Learning (RL). It assesses a given policy by estimating the corresponding value function. When using parameterized value functions, common approaches minimize the sum of squared Bellman temporal-difference errors and receive a point-estimate for the parameters. Kalman-based and Gaussian-processes based frameworks were suggested to evaluate the policy by treating the value as a random variable. These frameworks can learn uncertainties over the value parameters and exploit them for policy exploration. When adopting these frameworks to solve deep RL tasks, several limitations are revealed: excessive computations in each optimization step, difficulty with handling batches of samples which slows training and the effect of memory in stochastic environments which prevents off-policy learning. In this work, we discuss these limitations and propose to overcome them by an alternative general framework, based on the extended Kalman filter. We devise an optimization method, called Kalman Optimization for Value Approximation (KOVA) that can be incorporated as a policy evaluation component in policy optimization algorithms. KOVA minimizes a regularized objective function that concerns both parameter and noisy return uncertainties. We analyze the properties of KOVA and present its performance on deep RL control tasks.",0
"Reinforcement Learning (RL) heavily relies on policy evaluation, which involves estimating the value function of a given policy. To achieve this, common methods involve minimizing the sum of squared Bellman temporal-difference errors when using parameterized value functions. However, this approach only provides a point-estimate for the parameters. To address this limitation, Kalman-based and Gaussian-processes based frameworks treat the value as a random variable, enabling them to learn uncertainties over the value parameters and use them for policy exploration. However, when applying these frameworks to deep RL tasks, several challenges arise, including excessive computations, difficulty handling batches of samples, and memory effects in stochastic environments that hinder off-policy learning. To overcome these limitations, we propose an alternative framework based on the extended Kalman filter, called Kalman Optimization for Value Approximation (KOVA). KOVA is an optimization method that minimizes a regularized objective function considering both parameter and noisy return uncertainties. We examine the properties of KOVA and demonstrate its performance on deep RL control tasks, making it a promising policy evaluation component for policy optimization algorithms.",1
"Policy gradient reinforcement learning (RL) algorithms have achieved impressive performance in challenging learning tasks such as continuous control, but suffer from high sample complexity. Experience replay is a commonly used approach to improve sample efficiency, but gradient estimators using past trajectories typically have high variance. Existing sampling strategies for experience replay like uniform sampling or prioritised experience replay do not explicitly try to control the variance of the gradient estimates. In this paper, we propose an online learning algorithm, adaptive experience selection (AES), to adaptively learn an experience sampling distribution that explicitly minimises this variance. Using a regret minimisation approach, AES iteratively updates the experience sampling distribution to match the performance of a competitor distribution assumed to have optimal variance. Sample non-stationarity is addressed by proposing a dynamic (i.e. time changing) competitor distribution for which a closed-form solution is proposed. We demonstrate that AES is a low-regret algorithm with reasonable sample complexity. Empirically, AES has been implemented for deep deterministic policy gradient and soft actor critic algorithms, and tested on 8 continuous control tasks from the OpenAI Gym library. Ours results show that AES leads to significantly improved performance compared to currently available experience sampling strategies for policy gradient.",0
"Despite achieving impressive performance in challenging learning tasks such as continuous control, policy gradient reinforcement learning (RL) algorithms are plagued by high sample complexity. Experience replay is a commonly used approach to address this issue, but gradient estimators employing past trajectories often have high variance. Existing experience replay sampling strategies such as uniform sampling or prioritised experience replay do not explicitly aim to control gradient estimate variance. To address this problem, we propose an online learning algorithm called adaptive experience selection (AES) that adaptively learns an experience sampling distribution to minimize variance. Using a regret minimisation approach, AES updates the experience sampling distribution iteratively to match the performance of a competitor distribution assumed to have optimal variance. We address sample non-stationarity by proposing a dynamic competitor distribution with a closed-form solution. Our empirical results demonstrate that AES is a low-regret algorithm with reasonable sample complexity. We implemented AES for deep deterministic policy gradient and soft actor critic algorithms and tested it on 8 continuous control tasks from the OpenAI Gym library, showing significant improvement over currently available experience sampling strategies for policy gradient.",1
"Learning-based approaches for semantic segmentation have two inherent challenges. First, acquiring pixel-wise labels is expensive and time-consuming. Second, realistic segmentation datasets are highly unbalanced: some categories are much more abundant than others, biasing the performance to the most represented ones. In this paper, we are interested in focusing human labelling effort on a small subset of a larger pool of data, minimizing this effort while maximizing performance of a segmentation model on a hold-out set. We present a new active learning strategy for semantic segmentation based on deep reinforcement learning (RL). An agent learns a policy to select a subset of small informative image regions -- opposed to entire images -- to be labeled, from a pool of unlabeled data. The region selection decision is made based on predictions and uncertainties of the segmentation model being trained. Our method proposes a new modification of the deep Q-network (DQN) formulation for active learning, adapting it to the large-scale nature of semantic segmentation problems. We test the proof of concept in CamVid and provide results in the large-scale dataset Cityscapes. On Cityscapes, our deep RL region-based DQN approach requires roughly 30% less additional labeled data than our most competitive baseline to reach the same performance. Moreover, we find that our method asks for more labels of under-represented categories compared to the baselines, improving their performance and helping to mitigate class imbalance.",0
"Semantic segmentation using learning-based approaches faces two challenges. Firstly, labeling pixels is a time-consuming and expensive process. Secondly, segmentation datasets are imbalanced, with certain categories being more prevalent than others, resulting in biased performance towards the most common categories. The aim of this paper is to minimize human labeling effort while maximizing segmentation model performance on a hold-out set by focusing on a small subset of a larger dataset. This is achieved through a new active learning strategy based on deep reinforcement learning (RL). The proposed method selects informative image regions, instead of entire images, to label from a pool of unlabeled data. The selection is based on predictions and uncertainties of the segmentation model being trained. A new modification of deep Q-network (DQN) formulation for active learning is proposed, adapted to the large-scale nature of semantic segmentation problems. The proof of concept is tested on CamVid and Cityscapes datasets. The results show that our deep RL region-based DQN approach requires roughly 30% less additional labeled data to achieve the same performance as the most competitive baseline. Additionally, our method identifies and labels under-represented categories, improving their performance and mitigating class imbalance.",1
"This work considers two distinct settings: imitation learning and goal-conditioned reinforcement learning. In either case, effective solutions require the agent to reliably reach a specified state (a goal), or set of states (a demonstration). Drawing a connection between probabilistic long-term dynamics and the desired value function, this work introduces an approach which utilizes recent advances in density estimation to effectively learn to reach a given state. As our first contribution, we use this approach for goal-conditioned reinforcement learning and show that it is both efficient and does not suffer from hindsight bias in stochastic domains. As our second contribution, we extend the approach to imitation learning and show that it achieves state-of-the art demonstration sample-efficiency on standard benchmark tasks.",0
"The work discussed in this article focuses on two different scenarios: imitation learning and goal-conditioned reinforcement learning. In both cases, the agent must be able to successfully reach a designated state or set of states. By linking the desired value function with probabilistic long-term dynamics, the work introduces an approach that utilizes recent advances in density estimation to learn how to reach a specific state. The first contribution of this approach is demonstrated in goal-conditioned reinforcement learning, where it proves to be effective and free from hindsight bias in stochastic environments. The second contribution is in imitation learning, where it achieves state-of-the-art demonstration sample-efficiency on standard benchmark tasks.",1
"We propose an algorithm combining calibrated prediction and generalization bounds from learning theory to construct confidence sets for deep neural networks with PAC guarantees---i.e., the confidence set for a given input contains the true label with high probability. We demonstrate how our approach can be used to construct PAC confidence sets on ResNet for ImageNet, a visual object tracking model, and a dynamics model for the half-cheetah reinforcement learning problem.",0
"Our proposal is to use a combination of calibrated prediction and generalization bounds from learning theory as an algorithm for creating confidence sets for deep neural networks with PAC guarantees. In other words, the confidence set for a specific input would highly likely include the correct label. We illustrate how our method can be applied to construct PAC confidence sets on various models such as ResNet for ImageNet, a visual object tracking model, and a dynamics model for the half-cheetah reinforcement learning challenge.",1
"While image captioning through machines requires structured learning and basis for interpretation, improvement requires multiple context understanding and processing in a meaningful way. This research will provide a novel concept for context combination and will impact many applications to deal visual features as an equivalence of descriptions of objects, activities and events. There are three components of our architecture: Feature Distribution Composition (FDC) Layer Attention, Multiple Role Representation Crossover (MRRC) Attention Layer and the Language Decoder. FDC Layer Attention helps in generating the weighted attention from RCNN features, MRRC Attention Layer acts as intermediate representation processing and helps in generating the next word attention, while Language Decoder helps in estimation of the likelihood for the next probable word in the sentence. We demonstrated effectiveness of FDC, MRRC, regional object feature attention and reinforcement learning for effective learning to generate better captions from images. The performance of our model enhanced previous performances by 35.3\% and created a new standard and theory for representation generation based on logic, better interpretability and contexts.",0
"Machine image captioning requires structured learning and a basis for interpretation, but to improve it, multiple contexts must be understood and processed in a meaningful way. This study proposes a novel concept for combining context, which will have a significant impact on applications that deal with visual features as descriptions of objects, activities, and events. Our architecture comprises three components: the Feature Distribution Composition (FDC) Layer Attention, Multiple Role Representation Crossover (MRRC) Attention Layer, and the Language Decoder. The FDC Layer Attention generates weighted attention from RCNN features, while the MRRC Attention Layer acts as an intermediate representation processor to generate the next word attention. The Language Decoder estimates the likelihood of the next probable word in the sentence. Our model's effectiveness was demonstrated using FDC, MRRC, regional object feature attention, and reinforcement learning to generate better captions from images. The performance of our model surpassed previous performances by 35.3\%, creating a new standard and theory for representation generation based on logic, better interpretability, and contexts.",1
"Personal mobile sensing is fast permeating our daily lives to enable activity monitoring, healthcare and rehabilitation. Combined with deep learning, these applications have achieved significant success in recent years. Different from conventional cloud-based paradigms, running deep learning on devices offers several advantages including data privacy preservation and low-latency response for both model inference and update. Since data collection is costly in reality, Google's Federated Learning offers not only complete data privacy but also better model robustness based on multiple user data. However, personal mobile sensing applications are mostly user-specific and highly affected by environment. As a result, continuous local changes may seriously affect the performance of a global model generated by Federated Learning. In addition, deploying Federated Learning on a local server, e.g., edge server, may quickly reach the bottleneck due to resource constraint and serious failure by attacks. Towards pushing deep learning on devices, we present MDLdroid, a novel decentralized mobile deep learning framework to enable resource-aware on-device collaborative learning for personal mobile sensing applications. To address resource limitation, we propose a ChainSGD-reduce approach which includes a novel chain-directed Synchronous Stochastic Gradient Descent algorithm to effectively reduce overhead among multiple devices. We also design an agent-based multi-goal reinforcement learning mechanism to balance resources in a fair and efficient manner. Our evaluations show that our model training on off-the-shelf mobile devices achieves 2x to 3.5x faster than single-device training, and 1.5x faster than the master-slave approach.",0
"The use of personal mobile sensing has become increasingly prevalent in our daily lives for activity monitoring, healthcare, and rehabilitation. When combined with deep learning, these applications have seen significant success in recent years. Unlike traditional cloud-based models, implementing deep learning on mobile devices provides several benefits such as preserving data privacy and offering low-latency response times for model inference and updates. Google's Federated Learning not only ensures complete data privacy but also improves model robustness by utilizing multiple user data. However, personal mobile sensing applications are highly dependent on the user and environment, making constant changes a challenge for global models generated by Federated Learning. Additionally, deploying this model on a local server may result in resource limitations and serious failure due to attacks. To address these challenges, we present MDLdroid, a novel decentralized mobile deep learning framework that enables resource-aware on-device collaborative learning for personal mobile sensing applications. Our approach includes a ChainSGD-reduce algorithm to effectively reduce overhead among multiple devices and an agent-based multi-goal reinforcement learning mechanism to balance resources fairly and efficiently. Our evaluations demonstrate that our model training on off-the-shelf mobile devices is 2-3.5 times faster than single-device training and 1.5 times faster than the master-slave approach.",1
"Neural Architecture Search has shown potential to automate the design of neural networks. Deep Reinforcement Learning based agents can learn complex architectural patterns, as well as explore a vast and compositional search space. On the other hand, evolutionary algorithms offer higher sample efficiency, which is critical for such a resource intensive application. In order to capture the best of both worlds, we propose a class of Evolutionary-Neural hybrid agents (Evo-NAS). We show that the Evo-NAS agent outperforms both neural and evolutionary agents when applied to architecture search for a suite of text and image classification benchmarks. On a high-complexity architecture search space for image classification, the Evo-NAS agent surpasses the accuracy achieved by commonly used agents with only 1/3 of the search cost.",0
"Automating the design of neural networks is possible with Neural Architecture Search. Deep Reinforcement Learning agents have the ability to comprehend intricate architectural patterns and navigate a vast and complex search space. Meanwhile, evolutionary algorithms offer better sample efficiency which is crucial for a resource-intensive field like this. To leverage the benefits of both methods, we introduce a new type of agent called Evolutionary-Neural hybrid agents (Evo-NAS). Our research demonstrates that Evo-NAS is more effective than both neural and evolutionary agents when applied to architecture search for a range of text and image classification benchmarks. In fact, our Evo-NAS agent surpasses the accuracy of commonly used agents on a high-complexity architecture search space for image classification, with only 1/3 of the search cost.",1
"Deep reinforcement learning (RL) agents often fail to generalize to unseen environments (yet semantically similar to trained agents), particularly when they are trained on high-dimensional state spaces, such as images. In this paper, we propose a simple technique to improve a generalization ability of deep RL agents by introducing a randomized (convolutional) neural network that randomly perturbs input observations. It enables trained agents to adapt to new domains by learning robust features invariant across varied and randomized environments. Furthermore, we consider an inference method based on the Monte Carlo approximation to reduce the variance induced by this randomization. We demonstrate the superiority of our method across 2D CoinRun, 3D DeepMind Lab exploration and 3D robotics control tasks: it significantly outperforms various regularization and data augmentation methods for the same purpose.",0
"The ability of deep reinforcement learning (RL) agents to perform well in new environments that are similar to their trained environments is often limited, especially when they are trained on complex state spaces like images. To address this issue, we propose a simple method in this paper to enhance the generalization capability of deep RL agents. This involves incorporating a randomized (convolutional) neural network that modifies input observations in a random manner, enabling the trained agents to acquire resilient features that remain consistent across diverse and randomized environments. Additionally, we adopt an inference technique that utilizes the Monte Carlo approximation to decrease the variance caused by this randomization. Our results, which cover 2D CoinRun, 3D DeepMind Lab exploration, and 3D robotics control tasks, demonstrate that our approach outperforms other regularization and data augmentation methods that serve the same purpose.",1
"Machine learning has shown growing success in recent years. However, current machine learning systems are highly specialized, trained for particular problems or domains, and typically on a single narrow dataset. Human learning, on the other hand, is highly general and adaptable. Never-ending learning is a machine learning paradigm that aims to bridge this gap, with the goal of encouraging researchers to design machine learning systems that can learn to perform a wider variety of inter-related tasks in more complex environments. To date, there is no environment or testbed to facilitate the development and evaluation of never-ending learning systems. To this end, we propose the Jelly Bean World testbed. The Jelly Bean World allows experimentation over two-dimensional grid worlds which are filled with items and in which agents can navigate. This testbed provides environments that are sufficiently complex and where more generally intelligent algorithms ought to perform better than current state-of-the-art reinforcement learning approaches. It does so by producing non-stationary environments and facilitating experimentation with multi-task, multi-agent, multi-modal, and curriculum learning settings. We hope that this new freely-available software will prompt new research and interest in the development and evaluation of never-ending learning systems and more broadly, general intelligence systems.",0
"In recent years, machine learning has demonstrated significant advancements. However, current machine learning systems are limited in their specialization, trained solely for specific domains or problems and frequently on a single dataset. In contrast, human learning is versatile and adaptable. The never-ending learning machine learning paradigm aims to bridge this gap, encouraging the creation of machine learning systems that can learn to perform diverse tasks in complex environments. Despite this, there is no available environment or testbed to develop and assess never-ending learning systems. To address this, we propose the Jelly Bean World testbed, a two-dimensional grid world filled with items where agents can navigate. This testbed provides complex environments where more intelligent algorithms should outperform current reinforcement learning approaches. By producing non-stationary environments and allowing experimentation with multi-task, multi-agent, multi-modal, and curriculum learning settings, we hope that this freely available software will inspire further research into never-ending learning systems and general intelligence systems.",1
"Conventionally, model-based reinforcement learning (MBRL) aims to learn a global model for the dynamics of the environment. A good model can potentially enable planning algorithms to generate a large variety of behaviors and solve diverse tasks. However, learning an accurate model for complex dynamical systems is difficult, and even then, the model might not generalize well outside the distribution of states on which it was trained. In this work, we combine model-based learning with model-free learning of primitives that make model-based planning easy. To that end, we aim to answer the question: how can we discover skills whose outcomes are easy to predict? We propose an unsupervised learning algorithm, Dynamics-Aware Discovery of Skills (DADS), which simultaneously discovers predictable behaviors and learns their dynamics. Our method can leverage continuous skill spaces, theoretically, allowing us to learn infinitely many behaviors even for high-dimensional state-spaces. We demonstrate that zero-shot planning in the learned latent space significantly outperforms standard MBRL and model-free goal-conditioned RL, can handle sparse-reward tasks, and substantially improves over prior hierarchical RL methods for unsupervised skill discovery.",0
"Traditionally, model-based reinforcement learning (MBRL) endeavors to acquire an all-encompassing model of the environment's dynamics. This model has the potential to facilitate planning algorithms to generate various behaviors and accomplish diverse tasks. Nevertheless, developing an accurate model for intricate dynamic systems is challenging, and even if one is created, it may not work well beyond the state distribution on which it was trained. In this study, we merge model-based learning with model-free learning of primitives that simplify model-based planning. We investigate how to discover skills whose results are easy to anticipate. We suggest an unsupervised learning algorithm, Dynamics-Aware Discovery of Skills (DADS), that simultaneously identifies predictable behaviors and learns their dynamics. Our approach can use continuous skill spaces, theoretically enabling us to learn an infinite number of behaviors even for state-spaces with high dimensionality. We demonstrate that planning without prior experience in the learned latent space outperforms standard MBRL and model-free goal-conditioned RL, can handle sparse-reward tasks, and significantly enhances prior hierarchical RL techniques for unsupervised skill discovery.",1
"Quantum hardware and quantum-inspired algorithms are becoming increasingly popular for combinatorial optimization. However, these algorithms may require careful hyperparameter tuning for each problem instance. We use a reinforcement learning agent in conjunction with a quantum-inspired algorithm to solve the Ising energy minimization problem, which is equivalent to the Maximum Cut problem. The agent controls the algorithm by tuning one of its parameters with the goal of improving recently seen solutions. We propose a new Rescaled Ranked Reward (R3) method that enables stable single-player version of self-play training that helps the agent to escape local optima. The training on any problem instance can be accelerated by applying transfer learning from an agent trained on randomly generated problems. Our approach allows sampling high-quality solutions to the Ising problem with high probability and outperforms both baseline heuristics and a black-box hyperparameter optimization approach.",0
"Combinatorial optimization is increasingly turning to quantum hardware and quantum-inspired algorithms. However, these algorithms may require individual hyperparameter tuning for each problem instance. To solve the Ising energy minimization problem, which is the same as the Maximum Cut problem, we employ a reinforcement learning agent in conjunction with a quantum-inspired algorithm. By adjusting one of its parameters, the agent controls the algorithm to enhance recently seen solutions. We propose a new technique, the Rescaled Ranked Reward (R3), which provides a stable single-player version of self-play training, allowing the agent to avoid local optima. Transfer learning from an agent trained on randomly generated issues can accelerate training on any problem instance. Our method enables sampling of high-quality solutions to the Ising problem with high probability and outperforms both baseline heuristics and a black-box hyperparameter optimization approach.",1
"Biological evolution has distilled the experiences of many learners into the general learning algorithms of humans. Our novel meta reinforcement learning algorithm MetaGenRL is inspired by this process. MetaGenRL distills the experiences of many complex agents to meta-learn a low-complexity neural objective function that decides how future individuals will learn. Unlike recent meta-RL algorithms, MetaGenRL can generalize to new environments that are entirely different from those used for meta-training. In some cases, it even outperforms human-engineered RL algorithms. MetaGenRL uses off-policy second-order gradients during meta-training that greatly increase its sample efficiency.",0
"The general learning algorithms of humans have been shaped by the experiences of many learners through biological evolution. Our new algorithm, MetaGenRL, is based on this process and aims to distill the experiences of complex agents to create a neural objective function that determines future learning. Unlike other meta-RL algorithms, MetaGenRL can adapt to entirely new environments and has even outperformed RL algorithms designed by humans. MetaGenRL uses off-policy second-order gradients during meta-training, which significantly improves its sample efficiency.",1
"This paper introduces the Behaviour Suite for Reinforcement Learning, or bsuite for short. bsuite is a collection of carefully-designed experiments that investigate core capabilities of reinforcement learning (RL) agents with two objectives. First, to collect clear, informative and scalable problems that capture key issues in the design of general and efficient learning algorithms. Second, to study agent behaviour through their performance on these shared benchmarks. To complement this effort, we open source github.com/deepmind/bsuite, which automates evaluation and analysis of any agent on bsuite. This library facilitates reproducible and accessible research on the core issues in RL, and ultimately the design of superior learning algorithms. Our code is Python, and easy to use within existing projects. We include examples with OpenAI Baselines, Dopamine as well as new reference implementations. Going forward, we hope to incorporate more excellent experiments from the research community, and commit to a periodic review of bsuite from a committee of prominent researchers.",0
"The paper presents the Behaviour Suite for Reinforcement Learning, abbreviated as bsuite. This suite comprises of a set of meticulously-designed experiments that aim to explore the fundamental capabilities of reinforcement learning (RL) agents. The objectives of bsuite are twofold: firstly, to gather well-defined, informative, and scalable challenges that capture the key aspects of designing efficient and general learning algorithms; secondly, to examine agent behavior based on their performance on these shared benchmarks. To facilitate reproducible and accessible research on the core issues in RL and the development of superior learning algorithms, we have open-sourced the code at github.com/deepmind/bsuite. This library, written in Python, can be easily integrated into existing projects and offers examples with OpenAI Baselines, Dopamine, and new reference implementations. Additionally, we aim to incorporate more outstanding experiments from the research community and subject bsuite to regular reviews by a committee of distinguished researchers.",1
"Recent years have witnessed a tremendous improvement of deep reinforcement learning. However, a challenging problem is that an agent may suffer from inefficient exploration, particularly for on-policy methods. Previous exploration methods either rely on complex structure to estimate the novelty of states, or incur sensitive hyper-parameters causing instability. We propose an efficient exploration method, Multi-Path Policy Optimization (MPPO), which does not incur high computation cost and ensures stability. MPPO maintains an efficient mechanism that effectively utilizes a population of diverse policies to enable better exploration, especially in sparse environments. We also give a theoretical guarantee of the stable performance. We build our scheme upon two widely-adopted on-policy methods, the Trust-Region Policy Optimization algorithm and Proximal Policy Optimization algorithm. We conduct extensive experiments on several MuJoCo tasks and their sparsified variants to fairly evaluate the proposed method. Results show that MPPO significantly outperforms state-of-the-art exploration methods in terms of both sample efficiency and final performance.",0
"In recent years, there has been a remarkable advancement in deep reinforcement learning. However, a challenging issue that persists is the inefficient exploration of the agent, particularly for on-policy methods. Prior exploration techniques have relied on complex structures to gauge the novelty of states, or they have caused instability due to sensitive hyper-parameters. To address this, we propose a cost-effective exploration method called Multi-Path Policy Optimization (MPPO) that ensures stability. MPPO effectively utilizes a variety of policies to enable better exploration in sparse environments. Furthermore, we provide a theoretical guarantee of its stable performance. Our scheme is built on two commonly-used on-policy methods: the Trust-Region Policy Optimization algorithm and Proximal Policy Optimization algorithm. We conduct extensive experiments on various MuJoCo tasks and their sparsified variants to evaluate the proposed method accurately. Our results indicate that MPPO surpasses state-of-the-art exploration methods in terms of both sample efficiency and final performance.",1
"We propose a reinforcement learning agent to solve hard exploration games by learning a range of directed exploratory policies. We construct an episodic memory-based intrinsic reward using k-nearest neighbors over the agent's recent experience to train the directed exploratory policies, thereby encouraging the agent to repeatedly revisit all states in its environment. A self-supervised inverse dynamics model is used to train the embeddings of the nearest neighbour lookup, biasing the novelty signal towards what the agent can control. We employ the framework of Universal Value Function Approximators (UVFA) to simultaneously learn many directed exploration policies with the same neural network, with different trade-offs between exploration and exploitation. By using the same neural network for different degrees of exploration/exploitation, transfer is demonstrated from predominantly exploratory policies yielding effective exploitative policies. The proposed method can be incorporated to run with modern distributed RL agents that collect large amounts of experience from many actors running in parallel on separate environment instances. Our method doubles the performance of the base agent in all hard exploration in the Atari-57 suite while maintaining a very high score across the remaining games, obtaining a median human normalised score of 1344.0%. Notably, the proposed method is the first algorithm to achieve non-zero rewards (with a mean score of 8,400) in the game of Pitfall! without using demonstrations or hand-crafted features.",0
"We suggest an approach using reinforcement learning to tackle difficult exploration games. The agent learns various directed exploratory policies through the use of an episodic memory-based intrinsic reward, which is trained using k-nearest neighbors over the agent's recent experience. This encourages the agent to revisit all states in its environment. The nearest neighbour lookup is trained using a self-supervised inverse dynamics model, which biases the novelty signal towards what the agent can control. We utilize the Universal Value Function Approximators (UVFA) framework to train many directed exploration policies simultaneously with the same neural network, with differing trade-offs between exploration and exploitation. Our proposed method can be incorporated into modern distributed RL agents, which gather large amounts of experience from multiple actors running in parallel on separate environment instances. Our approach doubles the performance of the base agent in all hard exploration in the Atari-57 suite while maintaining a very high score across the remaining games, obtaining a median human normalised score of 1344.0%. Notably, our proposed method is the first algorithm to achieve non-zero rewards (with a mean score of 8,400) in the game of Pitfall! without using demonstrations or hand-crafted features.",1
"Transferring knowledge across tasks to improve data-efficiency is one of the open key challenges in the field of global black-box optimization. Readily available algorithms are typically designed to be universal optimizers and, therefore, often suboptimal for specific tasks. We propose a novel transfer learning method to obtain customized optimizers within the well-established framework of Bayesian optimization, allowing our algorithm to utilize the proven generalization capabilities of Gaussian processes. Using reinforcement learning to meta-train an acquisition function (AF) on a set of related tasks, the proposed method learns to extract implicit structural information and to exploit it for improved data-efficiency. We present experiments on a simulation-to-real transfer task as well as on several synthetic functions and on two hyperparameter search problems. The results show that our algorithm (1) automatically identifies structural properties of objective functions from available source tasks or simulations, (2) performs favourably in settings with both scarse and abundant source data, and (3) falls back to the performance level of general AFs if no particular structure is present.",0
"Enhancing data-efficiency by transferring knowledge between tasks is a critical challenge in global black-box optimization. Existing algorithms are generally universal optimizers and may not be optimal for specific tasks. To address this, we propose a new transfer learning approach that creates customized optimizers within the Bayesian optimization framework, leveraging the proven generalization abilities of Gaussian processes. Our method uses reinforcement learning to train an acquisition function (AF) on a set of related tasks, allowing it to extract implicit structural information and improve data-efficiency. We evaluate our algorithm on a range of tasks, including a simulation-to-real transfer task and two hyperparameter search problems, and find that it (1) identifies objective function structural properties from available source data, (2) performs well with both scarce and abundant source data, and (3) reverts to general AF performance in the absence of specific structural information.",1
"Disentangled representations have recently been shown to improve fairness, data efficiency and generalisation in simple supervised and reinforcement learning tasks. To extend the benefits of disentangled representations to more complex domains and practical applications, it is important to enable hyperparameter tuning and model selection of existing unsupervised approaches without requiring access to ground truth attribute labels, which are not available for most datasets. This paper addresses this problem by introducing a simple yet robust and reliable method for unsupervised disentangled model selection. Our approach, Unsupervised Disentanglement Ranking (UDR), leverages the recent theoretical results that explain why variational autoencoders disentangle (Rolinek et al, 2019), to quantify the quality of disentanglement by performing pairwise comparisons between trained model representations. We show that our approach performs comparably to the existing supervised alternatives across 5,400 models from six state of the art unsupervised disentangled representation learning model classes. Furthermore, we show that the ranking produced by our approach correlates well with the final task performance on two different domains.",0
"Recent studies have demonstrated that disentangled representations can enhance fairness, data efficiency, and generalization in basic supervised and reinforcement learning tasks. However, in order to apply disentangled representations to more complex domains and practical applications, it is crucial to facilitate hyperparameter tuning and model selection of current unsupervised approaches without access to ground truth attribute labels. Since most datasets do not provide such labels, this paper proposes an unsupervised disentangled model selection method called Unsupervised Disentanglement Ranking (UDR). UDR is a simple yet robust and dependable approach that uses pairwise comparisons between trained model representations to quantify disentanglement quality. The paper shows that UDR performs just as well as existing supervised alternatives across 5,400 models from six state-of-the-art unsupervised disentangled representation learning model classes. Additionally, the study reveals that the ranking generated by UDR is highly correlated with the final task performance on two distinct domains.",1
"The Hamiltonian formalism plays a central role in classical and quantum physics. Hamiltonians are the main tool for modelling the continuous time evolution of systems with conserved quantities, and they come equipped with many useful properties, like time reversibility and smooth interpolation in time. These properties are important for many machine learning problems - from sequence prediction to reinforcement learning and density modelling - but are not typically provided out of the box by standard tools such as recurrent neural networks. In this paper, we introduce the Hamiltonian Generative Network (HGN), the first approach capable of consistently learning Hamiltonian dynamics from high-dimensional observations (such as images) without restrictive domain assumptions. Once trained, we can use HGN to sample new trajectories, perform rollouts both forward and backward in time and even speed up or slow down the learned dynamics. We demonstrate how a simple modification of the network architecture turns HGN into a powerful normalising flow model, called Neural Hamiltonian Flow (NHF), that uses Hamiltonian dynamics to model expressive densities. We hope that our work serves as a first practical demonstration of the value that the Hamiltonian formalism can bring to deep learning.",0
"The Hamiltonian formalism is a crucial aspect of classical and quantum physics that facilitates the modeling of continuous time evolution in systems with conserved quantities. Hamiltonians possess desirable properties like time reversibility and smooth time interpolation, which are relevant to a variety of machine learning problems. However, these properties are typically not readily available through standard tools like recurrent neural networks. This paper introduces the Hamiltonian Generative Network (HGN), which can consistently learn Hamiltonian dynamics from high-dimensional observations like images, without any restrictive domain assumptions. With the trained HGN, new trajectories can be sampled, and rollouts can be performed both forward and backward in time. Furthermore, the learned dynamics can be sped up or slowed down. A simple modification of the network architecture transforms HGN into Neural Hamiltonian Flow (NHF), a powerful normalizing flow model that uses Hamiltonian dynamics to model expressive densities. The authors hope that their work represents the first practical demonstration of the applicability of the Hamiltonian formalism to deep learning.",1
"Reinforcement learning requires manual specification of a reward function to learn a task. While in principle this reward function only needs to specify the task goal, in practice reinforcement learning can be very time-consuming or even infeasible unless the reward function is shaped so as to provide a smooth gradient towards a successful outcome. This shaping is difficult to specify by hand, particularly when the task is learned from raw observations, such as images. In this paper, we study how we can automatically learn dynamical distances: a measure of the expected number of time steps to reach a given goal state from any other state. These dynamical distances can be used to provide well-shaped reward functions for reaching new goals, making it possible to learn complex tasks efficiently. We show that dynamical distances can be used in a semi-supervised regime, where unsupervised interaction with the environment is used to learn the dynamical distances, while a small amount of preference supervision is used to determine the task goal, without any manually engineered reward function or goal examples. We evaluate our method both on a real-world robot and in simulation. We show that our method can learn to turn a valve with a real-world 9-DoF hand, using raw image observations and just ten preference labels, without any other supervision. Videos of the learned skills can be found on the project website: https://sites.google.com/view/dynamical-distance-learning.",0
"To learn a task through reinforcement learning, a reward function must be manually specified. Although this function only needs to define the task goal, it can be time-consuming or impractical without a smooth gradient towards success. This is particularly challenging when learning from raw observations, such as images. In this study, we investigate the automatic learning of dynamical distances, which measure the expected number of time steps to reach a goal state from any other state. These distances can create well-shaped reward functions for efficiently learning complex tasks. Our method uses unsupervised interaction with the environment to learn dynamical distances and a small amount of preference supervision to determine the task goal, without any manually engineered reward function or goal examples. We demonstrate the effectiveness of our approach on a real-world robot and in simulation, successfully teaching it to turn a valve with just ten preference labels and no other supervision. Videos of the learned skills are available on our project website: https://sites.google.com/view/dynamical-distance-learning.",1
"Model-based reinforcement learning has been empirically demonstrated as a successful strategy to improve sample efficiency. In particular, Dyna is an elegant model-based architecture integrating learning and planning that provides huge flexibility of using a model. One of the most important components in Dyna is called search-control, which refers to the process of generating state or state-action pairs from which we query the model to acquire simulated experiences. Search-control is critical in improving learning efficiency. In this work, we propose a simple and novel search-control strategy by searching high frequency regions of the value function. Our main intuition is built on Shannon sampling theorem from signal processing, which indicates that a high frequency signal requires more samples to reconstruct. We empirically show that a high frequency function is more difficult to approximate. This suggests a search-control strategy: we should use states from high frequency regions of the value function to query the model to acquire more samples. We develop a simple strategy to locally measure the frequency of a function by gradient and hessian norms, and provide theoretical justification for this approach. We then apply our strategy to search-control in Dyna, and conduct experiments to show its property and effectiveness on benchmark domains.",0
"Empirical evidence indicates that utilizing model-based reinforcement learning can effectively enhance sample efficiency. A notable model-based architecture, Dyna, facilitates learning and planning through its adaptable model utilization. Its search-control component is crucial in generating state or state-action pairs, from which simulated experiences can be obtained. Hence, this component significantly improves learning efficiency. This study proposes a novel search-control strategy, which involves searching high frequency regions of the value function. The rationale behind this strategy is based on Shannon sampling theorem and the difficulty in approximating high frequency functions. The proposed strategy involves measuring the function's frequency through gradient and hessian norms, and we offer theoretical justification for this strategy. We apply this approach to the search-control component of Dyna and conduct experiments to demonstrate its effectiveness in benchmark domains.",1
"Similarity graphs are an active research direction for the nearest neighbor search (NNS) problem. New algorithms for similarity graph construction are continuously being proposed and analyzed by both theoreticians and practitioners. However, existing construction algorithms are mostly based on heuristics and do not explicitly maximize the target performance measure, i.e., search recall. Therefore, at the moment it is not clear whether the performance of similarity graphs has plateaued or more effective graphs can be constructed with more theoretically grounded methods. In this paper, we introduce a new principled algorithm, based on adjacency matrix optimization, which explicitly maximizes search efficiency. Namely, we propose a probabilistic model of a similarity graph defined in terms of its edge probabilities and show how to learn these probabilities from data as a reinforcement learning task. As confirmed by experiments, the proposed construction method can be used to refine the state-of-the-art similarity graphs, achieving higher recall rates for the same number of distance computations. Furthermore, we analyze the learned graphs and reveal the structural properties that are responsible for more efficient search.",0
"The search for the nearest neighbor problem has led to an active research direction in similarity graphs. Both theoreticians and practitioners continue to propose and analyze new algorithms for similarity graph construction. However, existing construction algorithms primarily rely on heuristics and fail to explicitly maximize the target performance measure, which is search recall. This leaves uncertainty as to whether the performance of similarity graphs has reached its peak or if more effective graphs can be constructed using more theoretically grounded methods. This paper introduces a new principled algorithm, which utilizes adjacency matrix optimization to maximize search efficiency. The proposed probabilistic model of a similarity graph is defined in terms of edge probabilities and can be learned from data using reinforcement learning. Through experiments, the proposed construction method demonstrates the ability to refine state-of-the-art similarity graphs, achieving higher recall rates for the same number of distance computations. Additionally, the learned graphs are analyzed to reveal the structural properties responsible for more efficient search.",1
"While learning models are typically studied for inputs in the form of a fixed dimensional feature vector, real world data is rarely found in this form. In order to meet the basic requirement of traditional learning models, structural data generally have to be converted into fix-length vectors in a handcrafted manner, which is tedious and may even incur information loss. A common form of structured data is what we term ""semantic tree-structures"", corresponding to data where rich semantic information is encoded in a compositional manner, such as those expressed in JavaScript Object Notation (JSON) and eXtensible Markup Language (XML). For tree-structured data, several learning models have been studied to allow for working directly on raw tree-structure data, However such learning models are limited to either a specific tree-topology or a specific tree-structured data format, e.g., synthetic parse trees. In this paper, we propose a novel framework for end-to-end learning on generic semantic tree-structured data of arbitrary topology and heterogeneous data types, such as data expressed in JSON, XML and so on. Motivated by the works in recursive and recurrent neural networks, we develop exemplar neural implementations of our framework for the JSON format. We evaluate our approach on several UCI benchmark datasets, including ablation and data-efficiency studies, and on a toy reinforcement learning task. Experimental results suggest that our framework yields comparable performance to use of standard models with dedicated feature-vectors in general, and even exceeds baseline performance in cases where compositional nature of the data is particularly important.   The source code for a JSON-based implementation of our framework along with experiments can be downloaded at https://github.com/EndingCredits/json2vec.",0
"Although learning models are typically designed for fixed dimensional feature vectors, real world data is rarely found in this form. To meet the requirements of traditional models, structural data must be manually converted into fixed-length vectors, which can be tedious and result in information loss. Semantic tree-structures, which contain rich semantic information in a compositional manner, are a common form of structured data expressed in JSON and XML. However, existing learning models for tree-structured data are limited to specific tree-topologies or formats. This paper proposes a novel framework for end-to-end learning on generic semantic tree-structured data of arbitrary topology and heterogeneous data types, such as JSON and XML. The framework is based on recursive and recurrent neural networks, with exemplar neural implementations for JSON format. The approach is evaluated on several UCI benchmark datasets and a toy reinforcement learning task, showing comparable performance to standard models with dedicated feature-vectors and even outperforming them in cases where data compositionality is important. The source code for a JSON-based implementation of the framework and experiments is available at https://github.com/EndingCredits/json2vec.",1
"XCS constitutes the most deeply investigated classifier system today. It bears strong potentials and comes with inherent capabilities for mastering a variety of different learning tasks. Besides outstanding successes in various classification and regression tasks, XCS also proved very effective in certain multi-step environments from the domain of reinforcement learning. Especially in the latter domain, recent advances have been mainly driven by algorithms which model their policies based on deep neural networks -- among which the Deep-Q-Network (DQN) is a prominent representative. Experience Replay (ER) constitutes one of the crucial factors for the DQN's successes, since it facilitates stabilized training of the neural network-based Q-function approximators. Surprisingly, XCS barely takes advantage of similar mechanisms that leverage stored raw experiences encountered so far. To bridge this gap, this paper investigates the benefits of extending XCS with ER. On the one hand, we demonstrate that for single-step tasks ER bears massive potential for improvements in terms of sample efficiency. On the shady side, however, we reveal that the use of ER might further aggravate well-studied issues not yet solved for XCS when applied to sequential decision problems demanding for long-action-chains.",0
"XCS is currently the most extensively researched classifier system and possesses inherent capabilities to learn various tasks. It has had exceptional achievements in classification and regression tasks and has also proven to be effective in multi-step environments within reinforcement learning. However, recent advancements in this field have been driven by algorithms that use deep neural networks, with the Deep-Q-Network being a prominent example. Experience Replay is a crucial factor for the success of DQN, as it stabilizes the training of neural network-based Q-function approximators. Surprisingly, XCS does not utilize similar mechanisms to leverage stored experiences. This paper aims to investigate the benefits of extending XCS with ER. The study shows that ER has great potential for improving the sample efficiency of single-step tasks. However, it also highlights that the use of ER may exacerbate well-known issues in XCS when applied to sequential decision problems that require long-action-chains.",1
"In this paper, we derive a generalization of the Speedy Q-learning (SQL) algorithm that was proposed in the Reinforcement Learning (RL) literature to handle slow convergence of Watkins' Q-learning. In most RL algorithms such as Q-learning, the Bellman equation and the Bellman operator play an important role. It is possible to generalize the Bellman operator using the technique of successive relaxation. We use the generalized Bellman operator to derive a simple and efficient family of algorithms called Generalized Speedy Q-learning (GSQL-w) and analyze its finite time performance. We show that GSQL-w has an improved finite time performance bound compared to SQL for the case when the relaxation parameter w is greater than 1. This improvement is a consequence of the contraction factor of the generalized Bellman operator being less than that of the standard Bellman operator. Numerical experiments are provided to demonstrate the empirical performance of the GSQL-w algorithm.",0
"The aim of this paper is to present a modification of the Speedy Q-learning (SQL) algorithm, which was previously introduced in the Reinforcement Learning (RL) literature to overcome the slow convergence of Watkins' Q-learning. The Bellman equation and operator are essential components of most RL algorithms, including Q-learning. By utilizing the technique of successive relaxation, we generalized the Bellman operator, resulting in a new set of algorithms called Generalized Speedy Q-learning (GSQL-w). We evaluated the finite time performance of GSQL-w and found that it outperforms SQL when the relaxation parameter w is greater than 1. This improvement is due to the contraction factor of the generalized Bellman operator being smaller than that of the standard Bellman operator. We conducted numerical experiments to substantiate the empirical performance of the GSQL-w algorithm.",1
"Deep Reinforcement Learning (RL) is proven powerful for decision making in simulated environments. However, training deep RL model is challenging in real world applications such as production-scale health-care or recommender systems because of the expensiveness of interaction and limitation of budget at deployment. One aspect of the data inefficiency comes from the expensive hyper-parameter tuning when optimizing deep neural networks. We propose Adaptive Behavior Policy Sharing (ABPS), a data-efficient training algorithm that allows sharing of experience collected by behavior policy that is adaptively selected from a pool of agents trained with an ensemble of hyper-parameters. We further extend ABPS to evolve hyper-parameters during training by hybridizing ABPS with an adapted version of Population Based Training (ABPS-PBT). We conduct experiments with multiple Atari games with up to 16 hyper-parameter/architecture setups. ABPS achieves superior overall performance, reduced variance on top 25% agents, and equivalent performance on the best agent compared to conventional hyper-parameter tuning with independent training, even though ABPS only requires the same number of environmental interactions as training a single agent. We also show that ABPS-PBT further improves the convergence speed and reduces the variance.",0
"Decision making in simulated environments can benefit greatly from Deep Reinforcement Learning (RL), but applying this method in real-world applications like health-care or recommender systems can be challenging due to high interaction costs and limited budgets. One problem contributing to data inefficiency is the expensive hyper-parameter tuning required to optimize deep neural networks. To address this issue, we introduce Adaptive Behavior Policy Sharing (ABPS), a training algorithm that allows sharing of experience gathered by a behavior policy selected from a group of agents trained with different hyper-parameters. We also expand on ABPS by hybridizing it with an adapted version of Population Based Training (ABPS-PBT) to evolve hyper-parameters during training. Our experiments with multiple Atari games show that ABPS achieves superior overall performance, reduced variance on top 25% agents, and equivalent performance on the best agent when compared to conventional hyper-parameter tuning with independent training. Additionally, ABPS-PBT further improves convergence speed and reduces variance. Notably, ABPS only requires the same number of environmental interactions as training a single agent.",1
"Transfer and adaptation to new unknown environmental dynamics is a key challenge for reinforcement learning (RL). An even greater challenge is performing near-optimally in a single attempt at test time, possibly without access to dense rewards, which is not addressed by current methods that require multiple experience rollouts for adaptation. To achieve single episode transfer in a family of environments with related dynamics, we propose a general algorithm that optimizes a probe and an inference model to rapidly estimate underlying latent variables of test dynamics, which are then immediately used as input to a universal control policy. This modular approach enables integration of state-of-the-art algorithms for variational inference or RL. Moreover, our approach does not require access to rewards at test time, allowing it to perform in settings where existing adaptive approaches cannot. In diverse experimental domains with a single episode test constraint, our method significantly outperforms existing adaptive approaches and shows favorable performance against baselines for robust transfer.",0
"Reinforcement learning (RL) faces a significant challenge in adapting to new and unfamiliar environmental dynamics. The difficulty is compounded by the need to perform optimally in a single attempt at test time, sometimes without the benefit of dense rewards. Current methods require multiple experience rollouts for adaptation, but this fails to address the issue satisfactorily. Our proposed algorithm overcomes these challenges by optimizing a probe and inference model that rapidly estimates latent variables of test dynamics. This modular approach supports the integration of variational inference or RL algorithms, and it does not require access to rewards at test time. We have tested our approach in different experimental domains and achieved significant improvements over existing adaptive approaches and robust transfer baselines, demonstrating its effectiveness.",1
"We study the role of intrinsic motivation as an exploration bias for reinforcement learning in sparse-reward synergistic tasks, which are tasks where multiple agents must work together to achieve a goal they could not individually. Our key idea is that a good guiding principle for intrinsic motivation in synergistic tasks is to take actions which affect the world in ways that would not be achieved if the agents were acting on their own. Thus, we propose to incentivize agents to take (joint) actions whose effects cannot be predicted via a composition of the predicted effect for each individual agent. We study two instantiations of this idea, one based on the true states encountered, and another based on a dynamics model trained concurrently with the policy. While the former is simpler, the latter has the benefit of being analytically differentiable with respect to the action taken. We validate our approach in robotic bimanual manipulation and multi-agent locomotion tasks with sparse rewards; we find that our approach yields more efficient learning than both 1) training with only the sparse reward and 2) using the typical surprise-based formulation of intrinsic motivation, which does not bias toward synergistic behavior. Videos are available on the project webpage: https://sites.google.com/view/iclr2020-synergistic.",0
"Our research delves into the significance of intrinsic motivation in guiding exploration bias for reinforcement learning in sparse-reward synergistic tasks. These tasks involve multiple agents collaborating to achieve a common goal, which they cannot achieve independently. Our main proposition is that intrinsic motivation can act as a guiding principle in synergistic tasks by prompting agents to take actions that result in world-changing outcomes that would not occur if they acted alone. To achieve this, we suggest incentivizing agents to take joint actions that cannot be predicted by combining the predicted outcome for each agent individually. Our research explores two ways to implement this idea, one based on real states encountered, and the other based on a concurrent dynamics model. While the former is simpler, the latter is analytically differentiable with respect to the action taken. We tested our approach in bimanual manipulation and multi-agent locomotion tasks with sparse rewards, and it proved to be more effective than both training with only sparse rewards and the typical surprise-based intrinsic motivation formulation, which does not encourage synergistic behavior. For more information, you can view our project webpage that contains videos: https://sites.google.com/view/iclr2020-synergistic.",1
"Lack of reliability is a well-known issue for reinforcement learning (RL) algorithms. This problem has gained increasing attention in recent years, and efforts to improve it have grown substantially. To aid RL researchers and production users with the evaluation and improvement of reliability, we propose a set of metrics that quantitatively measure different aspects of reliability. In this work, we focus on variability and risk, both during training and after learning (on a fixed policy). We designed these metrics to be general-purpose, and we also designed complementary statistical tests to enable rigorous comparisons on these metrics. In this paper, we first describe the desired properties of the metrics and their design, the aspects of reliability that they measure, and their applicability to different scenarios. We then describe the statistical tests and make additional practical recommendations for reporting results. The metrics and accompanying statistical tools have been made available as an open-source library at https://github.com/google-research/rl-reliability-metrics. We apply our metrics to a set of common RL algorithms and environments, compare them, and analyze the results.",0
"Reinforcement learning (RL) algorithms have long been plagued by the issue of unreliability, which has recently garnered increased attention and efforts to improve it. To assist RL researchers and production users in evaluating and enhancing reliability, we propose a series of metrics that quantitatively measure distinct aspects of reliability, with a particular focus on variability and risk during training and after learning on a fixed policy. Our metrics are designed to be versatile, and we have also developed complementary statistical tests to allow for rigorous comparisons. In this article, we outline the metrics and their design, the aspects of reliability that they assess, and their relevance to various circumstances, as well as provide practical advice for presenting findings. The metrics and statistical tools are available as an open-source library on https://github.com/google-research/rl-reliability-metrics. We have applied our metrics to several common RL algorithms and environments, compared the results, and analyzed them.",1
"We present a modern scalable reinforcement learning agent called SEED (Scalable, Efficient Deep-RL). By effectively utilizing modern accelerators, we show that it is not only possible to train on millions of frames per second but also to lower the cost of experiments compared to current methods. We achieve this with a simple architecture that features centralized inference and an optimized communication layer. SEED adopts two state of the art distributed algorithms, IMPALA/V-trace (policy gradients) and R2D2 (Q-learning), and is evaluated on Atari-57, DeepMind Lab and Google Research Football. We improve the state of the art on Football and are able to reach state of the art on Atari-57 three times faster in wall-time. For the scenarios we consider, a 40% to 80% cost reduction for running experiments is achieved. The implementation along with experiments is open-sourced so results can be reproduced and novel ideas tried out.",0
"Introducing SEED (Scalable, Efficient Deep-RL), a modern reinforcement learning agent that can be scaled up to meet the needs of various experiments. By capitalizing on contemporary accelerators, we demonstrate that SEED can be trained on millions of frames per second while lowering the cost of experiments compared to existing methods. SEED employs a straightforward architecture with centralized inference and an optimized communication layer to achieve this. IMPALA/V-trace (policy gradients) and R2D2 (Q-learning), two cutting-edge distributed algorithms, are utilized by SEED, which is assessed against Atari-57, DeepMind Lab, and Google Research Football. We outperform existing techniques in Football and reach state-of-the-art performance in Atari-57 in three times less wall-time. For the scenarios we examined, we were able to reduce the cost of running experiments by 40% to 80%. The implementation and experiments are available as open source, allowing for reproducibility and the exploration of new ideas.",1
"Learning to cooperate is crucially important in multi-agent environments. The key is to understand the mutual interplay between agents. However, multi-agent environments are highly dynamic, where agents keep moving and their neighbors change quickly. This makes it hard to learn abstract representations of mutual interplay between agents. To tackle these difficulties, we propose graph convolutional reinforcement learning, where graph convolution adapts to the dynamics of the underlying graph of the multi-agent environment, and relation kernels capture the interplay between agents by their relation representations. Latent features produced by convolutional layers from gradually increased receptive fields are exploited to learn cooperation, and cooperation is further improved by temporal relation regularization for consistency. Empirically, we show that our method substantially outperforms existing methods in a variety of cooperative scenarios.",0
"In multi-agent environments, it is crucial to learn how to cooperate by understanding the mutual interplay between agents. However, due to the highly dynamic nature of such environments, where agents move frequently and their neighbors change rapidly, it is difficult to develop abstract representations of this interplay. To address these challenges, we propose a graph convolutional reinforcement learning approach that adapts to the dynamic underlying graph of the multi-agent environment using relation kernels to capture the interplay between agents. This method utilizes latent features generated by convolutional layers with gradually increased receptive fields to facilitate cooperation, and incorporates temporal relation regularization for consistency to further enhance cooperation. We demonstrate empirically that our approach significantly outperforms existing methods in various cooperative scenarios.",1
"We provide a framework for incorporating robustness -- to perturbations in the transition dynamics which we refer to as model misspecification -- into continuous control Reinforcement Learning (RL) algorithms. We specifically focus on incorporating robustness into a state-of-the-art continuous control RL algorithm called Maximum a-posteriori Policy Optimization (MPO). We achieve this by learning a policy that optimizes for a worst case expected return objective and derive a corresponding robust entropy-regularized Bellman contraction operator. In addition, we introduce a less conservative, soft-robust, entropy-regularized objective with a corresponding Bellman operator. We show that both, robust and soft-robust policies, outperform their non-robust counterparts in nine Mujoco domains with environment perturbations. In addition, we show improved robust performance on a high-dimensional, simulated, dexterous robotic hand. Finally, we present multiple investigative experiments that provide a deeper insight into the robustness framework. This includes an adaptation to another continuous control RL algorithm as well as learning the uncertainty set from offline data. Performance videos can be found online at https://sites.google.com/view/robust-rl.",0
"Our aim is to enhance the resilience of continuous control Reinforcement Learning (RL) algorithms to model misspecification by incorporating a robustness framework. Our focus is on Maximum a-posteriori Policy Optimization (MPO), a cutting-edge continuous control RL algorithm. To achieve this, we teach the policy to optimize for a worst-case expected return objective while establishing a corresponding robust entropy-regularized Bellman contraction operator. We also introduce a less conservative soft-robust entropy-regularized objective with a corresponding Bellman operator. We demonstrate that both robust and soft-robust policies surpass their non-robust counterparts in nine Mujoco domains with environment perturbations, as well as showing better robust performance on a high-dimensional, simulated, dexterous robotic hand. Moreover, we conduct several investigative experiments to gain a more profound understanding of the robustness framework, including adapting to another continuous control RL algorithm and learning the uncertainty set from offline data. Visit our website at https://sites.google.com/view/robust-rl to view performance videos.",1
"Deep artificial neural networks, trained with labeled data sets are widely used in numerous vision and robotics applications today. In terms of AI, these are called reflex models, referring to the fact that they do not self-evolve or actively adapt to environmental changes. As demand for intelligent robot control expands to many high level tasks, reinforcement learning and state based models play an increasingly important role. Herein, in computer vision and robotics domain, we study a novel approach to add reinforcement controls onto the image recognition reflex models to attain better overall performance, specifically to a wider environment range beyond what is expected of the task reflex models. Follow a common infrastructure with environment sensing and AI based modeling of self-adaptive agents, we implement multiple types of AI control agents. To the end, we provide comparative results of these agents with baseline, and an insightful analysis of their benefit to improve overall image recognition performance in real world.",0
"Today, labeled data sets are used to train deep artificial neural networks that are widely utilized in numerous vision and robotics applications. These are known as reflex models in the realm of AI, as they do not self-evolve or actively adapt to changes in their surroundings. However, with the increasing demand for intelligent robot control in high-level tasks, reinforcement learning and state-based models are becoming more crucial. In the domain of computer vision and robotics, we present a new approach to enhance the overall performance of image recognition reflex models by adding reinforcement controls. This is specifically aimed at expanding their scope beyond the expected environment range. To achieve this, we adopt a common infrastructure that involves environment sensing and AI-based modeling of self-adaptive agents, implementing multiple types of AI control agents. Finally, we provide comparative results of these agents with the baseline and an insightful analysis of their benefits in improving overall image recognition performance in the real world.",1
"This work explores the large-scale multi-agent communication mechanism under a multi-agent reinforcement learning (MARL) setting. We summarize the general categories of topology for communication structures in MARL literature, which are often manually specified. Then we propose a novel framework termed as Learning Structured Communication (LSC) by using a more flexible and efficient communication topology. Our framework allows for adaptive agent grouping to form different hierarchical formations over episodes, which is generated by an auxiliary task combined with a hierarchical routing protocol. Given each formed topology, a hierarchical graph neural network is learned to enable effective message information generation and propagation among inter- and intra-group communications. In contrast to existing communication mechanisms, our method has an explicit while learnable design for hierarchical communication. Experiments on challenging tasks show the proposed LSC enjoys high communication efficiency, scalability, and global cooperation capability.",0
"The objective of this study is to investigate the multi-agent communication mechanism in a multi-agent reinforcement learning (MARL) environment. The different communication structures in MARL literature are typically manually determined. We provide an overview of the typical topology categories and then introduce a new framework called Learning Structured Communication (LSC) that uses a more flexible and efficient communication topology. Our framework enables adaptive agent grouping to form various hierarchical formations, generated by an auxiliary task and hierarchical routing protocol. We then use a hierarchical graph neural network to facilitate effective message information generation and propagation. Our approach is unique in that it has an explicit yet learnable design for hierarchical communication, unlike existing communication methods. We conducted experiments on challenging tasks, and the results showed that our proposed LSC framework is highly efficient, scalable and has strong global cooperation capabilities.",1
"We explore fixed-horizon temporal difference (TD) methods, reinforcement learning algorithms for a new kind of value function that predicts the sum of rewards over a $\textit{fixed}$ number of future time steps. To learn the value function for horizon $h$, these algorithms bootstrap from the value function for horizon $h-1$, or some shorter horizon. Because no value function bootstraps from itself, fixed-horizon methods are immune to the stability problems that plague other off-policy TD methods using function approximation (also known as ""the deadly triad""). Although fixed-horizon methods require the storage of additional value functions, this gives the agent additional predictive power, while the added complexity can be substantially reduced via parallel updates, shared weights, and $n$-step bootstrapping. We show how to use fixed-horizon value functions to solve reinforcement learning problems competitively with methods such as Q-learning that learn conventional value functions. We also prove convergence of fixed-horizon temporal difference methods with linear and general function approximation. Taken together, our results establish fixed-horizon TD methods as a viable new way of avoiding the stability problems of the deadly triad.",0
"In this article, we investigate reinforcement learning algorithms known as fixed-horizon temporal difference (TD) methods. These methods utilize a value function that predicts the sum of rewards over a fixed number of future time steps. To learn the value function for a given horizon, the algorithms bootstrap from the value function for the previous horizon or a shorter one. Unlike other off-policy TD methods using function approximation, fixed-horizon methods do not suffer from stability issues, as they do not bootstrap from themselves. While fixed-horizon methods require additional value functions to be stored, they provide the agent with more predictive power. The added complexity can be reduced through parallel updates, shared weights, and n-step bootstrapping. We demonstrate that fixed-horizon value functions can be used to solve reinforcement learning problems just as effectively as conventional value functions. We also prove the convergence of fixed-horizon TD methods using linear and general function approximation. Our findings establish fixed-horizon TD methods as a viable alternative to conventional methods, bypassing the stability problems associated with the deadly triad.",1
"Through multi-agent competition, the simple objective of hide-and-seek, and standard reinforcement learning algorithms at scale, we find that agents create a self-supervised autocurriculum inducing multiple distinct rounds of emergent strategy, many of which require sophisticated tool use and coordination. We find clear evidence of six emergent phases in agent strategy in our environment, each of which creates a new pressure for the opposing team to adapt; for instance, agents learn to build multi-object shelters using moveable boxes which in turn leads to agents discovering that they can overcome obstacles using ramps. We further provide evidence that multi-agent competition may scale better with increasing environment complexity and leads to behavior that centers around far more human-relevant skills than other self-supervised reinforcement learning methods such as intrinsic motivation. Finally, we propose transfer and fine-tuning as a way to quantitatively evaluate targeted capabilities, and we compare hide-and-seek agents to both intrinsic motivation and random initialization baselines in a suite of domain-specific intelligence tests.",0
"By utilizing multi-agent competition, standard reinforcement learning algorithms, and a simple hide-and-seek objective, we have observed agents developing a self-supervised autocurriculum that generates several rounds of emergent strategy. These strategies require advanced coordination and tool use and result in six distinct phases of agent strategy. Each phase creates a new challenge for the opposing team, such as agents constructing multi-object shelters with moveable boxes, which leads to the discovery that ramps can help overcome obstacles. We have found that multi-agent competition scales better with increasing environment complexity, and the resulting behavior focuses on more human-relevant skills than other self-supervised reinforcement learning methods. Additionally, we propose transfer and fine-tuning as a quantitative evaluation method for targeted capabilities, and we compare hide-and-seek agents to intrinsic motivation and random initialization baselines in domain-specific intelligence tests.",1
"We present a deep reinforcement learning approach to minimizing the execution cost of neural network computation graphs in an optimizing compiler. Unlike earlier learning-based works that require training the optimizer on the same graph to be optimized, we propose a learning approach that trains an optimizer offline and then generalizes to previously unseen graphs without further training. This allows our approach to produce high-quality execution decisions on real-world TensorFlow graphs in seconds instead of hours. We consider two optimization tasks for computation graphs: minimizing running time and peak memory usage. In comparison to an extensive set of baselines, our approach achieves significant improvements over classical and other learning-based methods on these two tasks.",0
"Our study presents a novel deep reinforcement learning method for minimizing the execution cost of neural network computation graphs in an optimizing compiler. Unlike previous learning-based approaches that require training the optimizer on the same graph to be optimized, our proposed learning method trains an optimizer offline and can be applied to previously unseen graphs without further training. This enables our approach to generate high-quality execution decisions on real-world TensorFlow graphs in a matter of seconds, rather than hours. Our research focuses on two optimization tasks for computation graphs: minimizing running time and peak memory usage. Compared to a comprehensive set of baselines, our approach outperforms classical and other learning-based methods on both tasks.",1
"This paper proposes the first model-free Reinforcement Learning (RL) framework to synthesise policies for unknown, and continuous-state Markov Decision Processes (MDPs), such that a given linear temporal property is satisfied. We convert the given property into a Limit Deterministic Buchi Automaton (LDBA), namely a finite-state machine expressing the property. Exploiting the structure of the LDBA, we shape a synchronous reward function on-the-fly, so that an RL algorithm can synthesise a policy resulting in traces that probabilistically satisfy the linear temporal property. This probability (certificate) is also calculated in parallel with policy learning when the state space of the MDP is finite: as such, the RL algorithm produces a policy that is certified with respect to the property. Under the assumption of finite state space, theoretical guarantees are provided on the convergence of the RL algorithm to an optimal policy, maximising the above probability. We also show that our method produces ''best available'' control policies when the logical property cannot be satisfied. In the general case of a continuous state space, we propose a neural network architecture for RL and we empirically show that the algorithm finds satisfying policies, if there exist such policies. The performance of the proposed framework is evaluated via a set of numerical examples and benchmarks, where we observe an improvement of one order of magnitude in the number of iterations required for the policy synthesis, compared to existing approaches whenever available.",0
"The paper suggests a new method for Reinforcement Learning (RL) that does not rely on a pre-existing model. This framework is designed to develop policies for unknown and continuous-state Markov Decision Processes (MDPs) that meet a specified linear temporal property. To achieve this, the property is transformed into a Limit Deterministic Buchi Automaton (LDBA), which is a finite-state machine that expresses the property. By using the LDBA structure, a synchronous reward function is created in real-time, allowing the RL algorithm to develop a policy that produces traces with a high probability of meeting the linear temporal property. The certification process runs parallel to policy learning, and when the MDP state space is finite, the algorithm can produce a policy that is verified with respect to the property. The RL algorithm is proven to converge to an optimal policy that maximizes the probability of meeting the property, subject to the assumption of a finite state space. If the logical property cannot be satisfied, our method produces the ""best available"" control policies. For continuous state spaces, a neural network architecture is proposed for RL, and empirical evidence shows that the algorithm can find satisfying policies if they exist. The proposed framework is evaluated using several benchmarks, and the results indicate that it requires one order of magnitude less iteration than existing approaches, where available.",1
"A/B testing, or online experiment is a standard business strategy to compare a new product with an old one in pharmaceutical, technological, and traditional industries. Major challenges arise in online experiments where there is only one unit that receives a sequence of treatments over time. In those experiments, the treatment at a given time impacts current outcome as well as future outcomes. The aim of this paper is to introduce a reinforcement learning framework for carrying A/B testing, while characterizing the long-term treatment effects. Our proposed testing procedure allows for sequential monitoring and online updating, so it is generally applicable to a variety of treatment designs in different industries. In addition, we systematically investigate the theoretical properties (e.g., asymptotic distribution and power) of our testing procedure. Finally, we apply our framework to both synthetic datasets and a real-world data example obtained from a ride-sharing company to illustrate its usefulness.",0
"A/B testing, which is also known as online experimentation, is a common business strategy in various industries such as pharmaceutical, technological, and traditional. However, conducting online experiments can be challenging, especially when there is only one unit that receives a sequence of treatments over time, as the treatment at a given time can affect both current and future outcomes. This paper aims to introduce a reinforcement learning framework for carrying out A/B testing while identifying the long-term treatment effects. Our proposed testing procedure enables sequential monitoring and online updating, making it versatile and applicable to different treatment designs across industries. We also examine the theoretical properties, such as asymptotic distribution and power, of our testing procedure systematically. Finally, we demonstrate the effectiveness of our framework by applying it to synthetic datasets and a real-world data example obtained from a ride-sharing company.",1
"A simple and natural algorithm for reinforcement learning is Monte Carlo Exploring States (MCES), where the Q-function is estimated by averaging the Monte Carlo returns, and the policy is improved by choosing actions that maximize the current estimate of the Q-function. Exploration is performed by ""exploring starts"", that is, each episode begins with a randomly chosen state and action and then follows the current policy. Establishing convergence for this algorithm has been an open problem for more than 20 years. We make headway with this problem by proving convergence for Optimal Policy Feed-Forward MDPs, which are MDPs whose states are not revisited within any episode for an optimal policy. Such MDPs include all deterministic environments (including Cliff Walking and other gridworld examples) and a large class of stochastic environments (including Blackjack). The convergence results presented here make progress for this long-standing open problem in reinforcement learning.",0
"Monte Carlo Exploring States (MCES) is a straightforward and innate algorithm used in reinforcement learning. It estimates the Q-function by averaging the Monte Carlo returns and enhances the policy by selecting actions that maximize the current Q-function estimate. To explore, ""exploring starts"" are used, where each episode begins with a randomly chosen state and action, and then follows the current policy. However, establishing convergence for this algorithm has been an unresolved issue for over two decades. Our recent research makes progress in solving this problem by proving convergence for Optimal Policy Feed-Forward MDPs, which are MDPs that do not revisit states within any episode for an optimal policy. These MDPs encompass all deterministic environments, such as Cliff Walking and other gridworld examples, and a vast number of stochastic environments, including Blackjack. The convergence results we present are a significant advancement in resolving this long-standing problem in reinforcement learning.",1
"Collaborative filtering is widely used in modern recommender systems. Recent research shows that variational autoencoders (VAEs) yield state-of-the-art performance by integrating flexible representations from deep neural networks into latent variable models, mitigating limitations of traditional linear factor models. VAEs are typically trained by maximizing the likelihood (MLE) of users interacting with ground-truth items. While simple and often effective, MLE-based training does not directly maximize the recommendation-quality metrics one typically cares about, such as top-N ranking. In this paper we investigate new methods for training collaborative filtering models based on actor-critic reinforcement learning, to directly optimize the non-differentiable quality metrics of interest. Specifically, we train a critic network to approximate ranking-based metrics, and then update the actor network (represented here by a VAE) to directly optimize against the learned metrics. In contrast to traditional learning-to-rank methods that require to re-run the optimization procedure for new lists, our critic-based method amortizes the scoring process with a neural network, and can directly provide the (approximate) ranking scores for new lists. Empirically, we show that the proposed methods outperform several state-of-the-art baselines, including recently-proposed deep learning approaches, on three large-scale real-world datasets. The code to reproduce the experimental results and figure plots is on Github: https://github.com/samlobel/RaCT_CF",0
"Modern recommender systems frequently utilize collaborative filtering. Recent research has revealed that variational autoencoders (VAEs) produce superior results by incorporating adaptable representations from deep neural networks into latent variable models, which mitigates the restrictions of traditional linear factor models. While VAEs are typically trained through maximizing the likelihood (MLE) of users interacting with ground-truth items, this method does not directly optimize the quality metrics of interest, such as top-N ranking. This paper explores new approaches for training collaborative filtering models based on actor-critic reinforcement learning, which directly optimizes non-differentiable quality metrics. The proposed method trains a critic network to approximate ranking-based metrics and updates the actor network (represented by a VAE) to optimize against the learned metrics. Unlike traditional learning-to-rank techniques that require re-optimization for new lists, the critic-based method amortizes the scoring process with a neural network and can directly provide ranking scores for new lists. Empirical evidence indicates that the proposed method outperforms state-of-the-art baselines, including recent deep learning methods, on three large-scale real-world datasets. The experimental results and figure plots can be reproduced using the code on Github: https://github.com/samlobel/RaCT_CF.",1
"In deep reinforcement learning, building policies of high-quality is challenging when the feature space of states is small and the training data is limited. Despite the success of previous transfer learning approaches in deep reinforcement learning, directly transferring data or models from an agent to another agent is often not allowed due to the privacy of data and/or models in many privacy-aware applications. In this paper, we propose a novel deep reinforcement learning framework to federatively build models of high-quality for agents with consideration of their privacies, namely Federated deep Reinforcement Learning (FedRL). To protect the privacy of data and models, we exploit Gausian differentials on the information shared with each other when updating their local models. In the experiment, we evaluate our FedRL framework in two diverse domains, Grid-world and Text2Action domains, by comparing to various baselines.",0
"Developing high-quality policies in deep reinforcement learning poses a challenge when the state feature space is limited and the training data is scarce. While transfer learning has proven to be successful in the past, transferring data or models between agents is often prohibited in privacy-aware applications. To address this issue, we introduce a new deep reinforcement learning framework called Federated deep Reinforcement Learning (FedRL) that takes privacy into account when building high-quality models for agents. Gaussian differentials are employed to safeguard the privacy of data and models during the information-sharing process. Our FedRL framework is evaluated in two different domains, Grid-world and Text2Action, and compared to various baseline models.",1
"In recent years, advances in deep learning have enabled the application of reinforcement learning algorithms in complex domains. However, they lack the theoretical guarantees which are present in the tabular setting and suffer from many stability and reproducibility problems \citep{henderson2018deep}. In this work, we suggest a simple approach for improving stability and providing probabilistic performance improvement in off-policy actor-critic deep reinforcement learning regimes. Experiments on continuous action spaces, in the MuJoCo control suite, show that our proposed method reduces the variance of the process and improves the overall performance.",0
"Over the past few years, deep learning progress has made it possible to use reinforcement learning algorithms in intricate domains. Nevertheless, these algorithms do not possess the same theoretical guarantees as those in the tabular environment and are prone to numerous stability and reproducibility issues, according to Henderson et al. (2018). As a solution, we propose a straightforward technique to enhance stability and offer probabilistic performance enhancement in off-policy actor-critic deep reinforcement learning systems. Our experimentation on continuous action spaces in the MuJoCo control suite demonstrates that our suggested method curtails process variance and heightens overall performance.",1
"Recent researches show that machine learning has the potential to learn better heuristics than the one designed by human for solving combinatorial optimization problems. The deep neural network is used to characterize the input instance for constructing a feasible solution incrementally. Recently, an attention model is proposed to solve routing problems. In this model, the state of an instance is represented by node features that are fixed over time. However, the fact is, the state of an instance is changed according to the decision that the model made at different construction steps, and the node features should be updated correspondingly. Therefore, this paper presents a dynamic attention model with dynamic encoder-decoder architecture, which enables the model to explore node features dynamically and exploit hidden structure information effectively at different construction steps. This paper focuses on a challenging NP-hard problem, vehicle routing problem. The experiments indicate that our model outperforms the previous methods and also shows a good generalization performance.",0
"Recent studies reveal that machine learning has the potential to acquire superior heuristics for resolving combinatorial optimization problems than those formulated by humans. A feasible solution is constructed incrementally by utilizing a deep neural network that characterizes the input instance. An attention model has been introduced to tackle routing problems, where the node features represent the state of an instance that remains constant over time. However, the state of an instance alters based on the model's decisions during various construction stages, necessitating a corresponding update of the node features. In light of this, the study proposes a dynamic attention model with a dynamic encoder-decoder structure that enables the model to investigate node features dynamically and exploit hidden structure information effectively at different construction stages. The focus is on the vehicle routing problem, a challenging NP-hard problem. The experiments demonstrate that the proposed model outperforms previous approaches and exhibits good generalization performance.",1
"Can simple algorithms with a good representation solve challenging reinforcement learning problems? In this work, we answer this question in the affirmative, where we take ""simple learning algorithm"" to be tabular Q-Learning, the ""good representations"" to be a learned state abstraction, and ""challenging problems"" to be continuous control tasks. Our main contribution is a learning algorithm that abstracts a continuous state-space into a discrete one. We transfer this learned representation to unseen problems to enable effective learning. We provide theory showing that learned abstractions maintain a bounded value loss, and we report experiments showing that the abstractions empower tabular Q-Learning to learn efficiently in unseen tasks.",0
"Is it possible for uncomplicated algorithms, paired with efficient representations, to tackle complex reinforcement learning problems? This study provides an affirmative answer by utilizing tabular Q-Learning as the simple learning algorithm, a learned state abstraction as the good representation, and continuous control tasks as the challenging problems. The primary contribution is a learning algorithm that transforms a continuous state-space into a discrete one, which can be applied to new problems for more effective learning. Theoretical analysis reveals that learned abstractions maintain a bounded value loss, and experimental results demonstrate that the abstractions assist tabular Q-Learning in efficient learning for unseen tasks.",1
"Capsule Networks (CapsNets) have been proposed as an alternative to Convolutional Neural Networks (CNNs). This paper showcases how CapsNets are more capable than CNNs for autonomous agent exploration of realistic scenarios. In real world navigation, rewards external to agents may be rare. In turn, reinforcement learning algorithms can struggle to form meaningful policy functions. This paper's approach Capsules Exploration Module (Caps-EM) pairs a CapsNets architecture with an Advantage Actor Critic algorithm. Other approaches for navigating sparse environments require intrinsic reward generators, such as the Intrinsic Curiosity Module (ICM) and Augmented Curiosity Modules (ACM). Caps-EM uses a more compact architecture without need for intrinsic rewards. Tested using ViZDoom, the Caps-EM uses 44% and 83% fewer trainable network parameters than the ICM and Depth-Augmented Curiosity Module (D-ACM), respectively, for 1141% and 437% average time improvement over the ICM and D-ACM, respectively, for converging to a policy function across ""My Way Home"" scenarios.",0
"Capsule Networks (CapsNets) have emerged as an alternative to Convolutional Neural Networks (CNNs), and have been shown to be more effective for autonomous agent exploration of realistic scenarios. In scenarios where rewards external to agents are rare, reinforcement learning algorithms may struggle to form meaningful policy functions. This paper proposes the Capsules Exploration Module (Caps-EM), which pairs a CapsNets architecture with an Advantage Actor Critic algorithm to navigate sparse environments without the need for intrinsic reward generators. The Caps-EM approach uses a more compact architecture and requires 44% and 83% fewer trainable network parameters than the Intrinsic Curiosity Module (ICM) and Depth-Augmented Curiosity Module (D-ACM), respectively. Additionally, the Caps-EM approach shows an average time improvement of 1141% and 437% over the ICM and D-ACM, respectively, for converging to a policy function across ""My Way Home"" scenarios when tested using ViZDoom.",1
"Labeling data (e.g., labeling the people, objects, actions and scene in images) comprehensively and efficiently is a widely needed but challenging task. Numerous models were proposed to label various data and many approaches were designed to enhance the ability of deep learning models or accelerate them. Unfortunately, a single machine-learning model is not powerful enough to extract various semantic information from data. Given certain applications, such as image retrieval platforms and photo album management apps, it is often required to execute a collection of models to obtain sufficient labels. With limited computing resources and stringent delay, given a data stream and a collection of applicable resource-hungry deep-learning models, we design a novel approach to adaptively schedule a subset of these models to execute on each data item, aiming to maximize the value of the model output (e.g., the number of high-confidence labels). Achieving this lofty goal is nontrivial since a model's output on any data item is content-dependent and unknown until we execute it. To tackle this, we propose an Adaptive Model Scheduling framework, consisting of 1) a deep reinforcement learning-based approach to predict the value of unexecuted models by mining semantic relationship among diverse models, and 2) two heuristic algorithms to adaptively schedule the model execution order under a deadline or deadline-memory constraints respectively. The proposed framework doesn't require any prior knowledge of the data, which works as a powerful complement to existing model optimization technologies. We conduct extensive evaluations on five diverse image datasets and 30 popular image labeling models to demonstrate the effectiveness of our design: our design could save around 53\% execution time without loss of any valuable labels.",0
"Comprehensively and efficiently labeling data, such as people, objects, actions, and scenes in images, is a challenging task that requires the use of multiple machine-learning models. While many models have been proposed and approaches designed to enhance the performance of deep learning models, a single model is not powerful enough to extract all semantic information from data. For applications like image retrieval platforms and photo album management apps, executing a collection of models to obtain sufficient labels is often required, but this can be difficult due to limited computing resources and stringent delay. To address this issue, we propose an Adaptive Model Scheduling framework, which uses a deep reinforcement learning-based approach to predict the value of unexecuted models and two heuristic algorithms to adaptively schedule model execution order. Our approach saves around 53% execution time without loss of any valuable labels, making it a powerful complement to existing model optimization technologies.",1
"This paper presents a deep reinforcement learning algorithm for online accompaniment generation, with potential for real-time interactive human-machine duet improvisation. Different from offline music generation and harmonization, online music accompaniment requires the algorithm to respond to human input and generate the machine counterpart in a sequential order. We cast this as a reinforcement learning problem, where the generation agent learns a policy to generate a musical note (action) based on previously generated context (state). The key of this algorithm is the well-functioning reward model. Instead of defining it using music composition rules, we learn this model from monophonic and polyphonic training data. This model considers the compatibility of the machine-generated note with both the machine-generated context and the human-generated context. Experiments show that this algorithm is able to respond to the human part and generate a melodic, harmonic and diverse machine part. Subjective evaluations on preferences show that the proposed algorithm generates music pieces of higher quality than the baseline method.",0
"In this paper, a deep reinforcement learning algorithm is presented for generating online accompaniment, which has the potential to enable real-time interactive duet improvisation between humans and machines. Unlike offline music generation and harmonization, online music accompaniment requires the algorithm to respond to human input and generate the corresponding machine counterpart in a sequential manner. To address this, we frame the problem as a reinforcement learning task, where the generation agent learns a policy for generating a musical note (i.e., action) based on the previously generated context (i.e., state). The algorithm's effectiveness relies heavily on the reward model, which we develop by training it on monophonic and polyphonic data instead of relying on music composition rules. This reward model considers the compatibility of the machine-generated note with both the machine-generated and human-generated contexts. Experimental results demonstrate that the proposed algorithm can successfully respond to the human part and generate a diverse, melodic, and harmonic machine part. Additionally, subjective evaluations indicate that the proposed algorithm generates music pieces of higher quality than the baseline method.",1
"Network quantization is one of the most hardware friendly techniques to enable the deployment of convolutional neural networks (CNNs) on low-power mobile devices. Recent network quantization techniques quantize each weight kernel in a convolutional layer independently for higher inference accuracy, since the weight kernels in a layer exhibit different variances and hence have different amounts of redundancy. The quantization bitwidth or bit number (QBN) directly decides the inference accuracy, latency, energy and hardware overhead. To effectively reduce the redundancy and accelerate CNN inferences, various weight kernels should be quantized with different QBNs. However, prior works use only one QBN to quantize each convolutional layer or the entire CNN, because the design space of searching a QBN for each weight kernel is too large. The hand-crafted heuristic of the kernel-wise QBN search is so sophisticated that domain experts can obtain only sub-optimal results. It is difficult for even deep reinforcement learning (DRL) Deep Deterministic Policy Gradient (DDPG)-based agents to find a kernel-wise QBN configuration that can achieve reasonable inference accuracy. In this paper, we propose a hierarchical-DRL-based kernel-wise network quantization technique, AutoQ, to automatically search a QBN for each weight kernel, and choose another QBN for each activation layer. Compared to the models quantized by the state-of-the-art DRL-based schemes, on average, the same models quantized by AutoQ reduce the inference latency by 54.06\%, and decrease the inference energy consumption by 50.69\%, while achieving the same inference accuracy.",0
"One of the most hardware-friendly techniques for implementing convolutional neural networks (CNNs) on low-power mobile devices is network quantization. Current network quantization methods quantify each weight kernel in a convolutional layer independently to improve inference accuracy, given that weight kernels in a layer possess different variances and hence contain varying amounts of redundancy. The inference accuracy, latency, energy, and hardware overhead are all directly influenced by the quantization bitwidth or bit number (QBN). To effectively speed up CNN inferences and reduce redundancy, different weight kernels must be quantized using different QBNs. However, existing approaches employ only one QBN for each convolutional layer or the entire CNN, as the space for searching a QBN for each weight kernel is too vast. The kernel-wise QBN search heuristic is so complex that domain experts can only obtain suboptimal outcomes. Even deep reinforcement learning (DRL) Deep Deterministic Policy Gradient (DDPG)-based agents find it challenging to locate a kernel-wise QBN configuration that can achieve reasonable inference accuracy. In this paper, we present AutoQ, a hierarchical-DRL-based kernel-wise network quantization technique that automatically searches for a QBN for each weight kernel and selects another QBN for each activation layer. Compared to models quantized by state-of-the-art DRL-based approaches, AutoQ reduces inference latency by 54.06% and lowers inference energy consumption by 50.69%, while maintaining the same inference accuracy.",1
"In reinforcement learning, we can learn a model of future observations and rewards, and use it to plan the agent's next actions. However, jointly modeling future observations can be computationally expensive or even intractable if the observations are high-dimensional (e.g. images). For this reason, previous works have considered partial models, which model only part of the observation. In this paper, we show that partial models can be causally incorrect: they are confounded by the observations they don't model, and can therefore lead to incorrect planning. To address this, we introduce a general family of partial models that are provably causally correct, yet remain fast because they do not need to fully model future observations.",0
"Reinforcement learning involves creating a model of future observations and rewards to guide the agent's actions. However, creating a joint model of future observations can be difficult and slow when dealing with high-dimensional observations like images. Previous research has used partial models that only account for some parts of the observation. However, these partial models can be misleading and cause incorrect planning due to the observations they don't model. To solve this issue, we present a new family of partial models that are provably causally correct and efficient since they do not require a complete model of future observations.",1
"The optimal policy of a reinforcement learning problem is often discontinuous and non-smooth. I.e., for two states with similar representations, their optimal policies can be significantly different. In this case, representing the entire policy with a function approximator (FA) with shared parameters for all states maybe not desirable, as the generalization ability of parameters sharing makes representing discontinuous, non-smooth policies difficult. A common way to solve this problem, known as Mixture-of-Experts, is to represent the policy as the weighted sum of multiple components, where different components perform well on different parts of the state space. Following this idea and inspired by a recent work called advantage-weighted information maximization, we propose to learn for each state weights of these components, so that they entail the information of the state itself and also the preferred action learned so far for the state. The action preference is characterized via the advantage function. In this case, the weight of each component would only be large for certain groups of states whose representations are similar and preferred action representations are also similar. Therefore each component is easy to be represented. We call a policy parameterized in this way an Advantage Weighted Mixture Policy (AWMP) and apply this idea to improve soft-actor-critic (SAC), one of the most competitive continuous control algorithm. Experimental results demonstrate that SAC with AWMP clearly outperforms SAC in four commonly used continuous control tasks and achieve stable performance across different random seeds.",0
"The optimal policy in a reinforcement learning problem can be discontinuous and non-smooth, resulting in significantly different optimal policies for states with similar representations. Therefore, representing the entire policy with a shared parameter function approximator may not be effective due to the difficulty in representing discontinuous, non-smooth policies. To address this issue, a common approach is to use a Mixture-of-Experts, which represents the policy as a weighted sum of multiple components that perform well on different parts of the state space. Inspired by advantage-weighted information maximization, we propose to learn weights for each state's components that capture the state's information and preferred action based on the advantage function. Our proposed method, called Advantage Weighted Mixture Policy (AWMP), parameterizes the policy in this way and applies it to improve soft-actor-critic (SAC), a competitive continuous control algorithm. Experimental results show that SAC with AWMP outperforms SAC in four continuous control tasks and achieves stable performance across different random seeds.",1
"Exploration is widely regarded as one of the most challenging aspects of reinforcement learning (RL), with many naive approaches succumbing to exponential sample complexity. To isolate the challenges of exploration, we propose a new ""reward-free RL"" framework. In the exploration phase, the agent first collects trajectories from an MDP $\mathcal{M}$ without a pre-specified reward function. After exploration, it is tasked with computing near-optimal policies under for $\mathcal{M}$ for a collection of given reward functions. This framework is particularly suitable when there are many reward functions of interest, or when the reward function is shaped by an external agent to elicit desired behavior.   We give an efficient algorithm that conducts $\tilde{\mathcal{O}}(S^2A\mathrm{poly}(H)/\epsilon^2)$ episodes of exploration and returns $\epsilon$-suboptimal policies for an arbitrary number of reward functions. We achieve this by finding exploratory policies that visit each ""significant"" state with probability proportional to its maximum visitation probability under any possible policy. Moreover, our planning procedure can be instantiated by any black-box approximate planner, such as value iteration or natural policy gradient. We also give a nearly-matching $\Omega(S^2AH^2/\epsilon^2)$ lower bound, demonstrating the near-optimality of our algorithm in this setting.",0
"Reinforcement learning (RL) is widely recognized as a difficult task due to the challenge of exploration. Many naive approaches fail due to the exponential sample complexity. To address this issue, we propose a new framework called ""reward-free RL"". In this framework, the agent collects trajectories from an MDP $\mathcal{M}$ without a pre-specified reward function during the exploration phase. After exploration, the agent is required to compute near-optimal policies for $\mathcal{M}$ for a collection of given reward functions. This framework is particularly beneficial when there are numerous reward functions or when the reward function is shaped by an external agent. To conduct exploration efficiently, we introduce an algorithm that conducts $\tilde{\mathcal{O}}(S^2A\mathrm{poly}(H)/\epsilon^2)$ episodes of exploration and returns $\epsilon$-suboptimal policies for any number of reward functions. Our algorithm achieves this by finding exploratory policies that visit each ""significant"" state with probability proportional to its maximum visitation probability under any possible policy. Our planning procedure can be instantiated by any black-box approximate planner, such as value iteration or natural policy gradient. A nearly-matching $\Omega(S^2AH^2/\epsilon^2)$ lower bound is given to demonstrate the near-optimality of our algorithm in this setting.",1
"Tasks involving localization, memorization and planning in partially observable 3D environments are an ongoing challenge in Deep Reinforcement Learning. We present EgoMap, a spatially structured neural memory architecture. EgoMap augments a deep reinforcement learning agent's performance in 3D environments on challenging tasks with multi-step objectives. The EgoMap architecture incorporates several inductive biases including a differentiable inverse projection of CNN feature vectors onto a top-down spatially structured map. The map is updated with ego-motion measurements through a differentiable affine transform. We show this architecture outperforms both standard recurrent agents and state of the art agents with structured memory. We demonstrate that incorporating these inductive biases into an agent's architecture allows for stable training with reward alone, circumventing the expense of acquiring and labelling expert trajectories. A detailed ablation study demonstrates the impact of key aspects of the architecture and through extensive qualitative analysis, we show how the agent exploits its structured internal memory to achieve higher performance.",0
"Deep Reinforcement Learning faces ongoing challenges in completing tasks that involve localization, memorization, and planning in partially observable 3D environments. To overcome these challenges, we introduce EgoMap, a neural memory architecture that enhances the performance of deep reinforcement learning agents on multi-step objective tasks in 3D environments. EgoMap incorporates inductive biases, including a top-down spatially structured map created through a differentiable inverse projection of CNN feature vectors, with updates from ego-motion measurements using a differentiable affine transform. Our experiments show that EgoMap outperforms both standard recurrent agents and state-of-the-art agents with structured memory. By incorporating these inductive biases into an agent's architecture, we achieve stable training with reward alone, eliminating the need for expert trajectory acquisition and labeling. Through a detailed ablation study and extensive qualitative analysis, we demonstrate the impact of key aspects of the architecture and how the agent utilizes its structured internal memory to achieve high performance.",1
"Model-Based Reinforcement Learning (MBRL) offers a promising direction for sample efficient learning, often achieving state of the art results for continuous control tasks. However, many existing MBRL methods rely on combining greedy policies with exploration heuristics, and even those which utilize principled exploration bonuses construct dual objectives in an ad hoc fashion. In this paper we introduce Ready Policy One (RP1), a framework that views MBRL as an active learning problem, where we aim to improve the world model in the fewest samples possible. RP1 achieves this by utilizing a hybrid objective function, which crucially adapts during optimization, allowing the algorithm to trade off reward v.s. exploration at different stages of learning. In addition, we introduce a principled mechanism to terminate sample collection once we have a rich enough trajectory batch to improve the model. We rigorously evaluate our method on a variety of continuous control tasks, and demonstrate statistically significant gains over existing approaches.",0
"The use of Model-Based Reinforcement Learning (MBRL) has shown great potential in achieving efficient learning for continuous control tasks. However, many current methods of MBRL rely on combining greedy policies with exploration heuristics, and even those that use principled exploration bonuses construct dual objectives in an ad hoc manner. Our study introduces a new approach called Ready Policy One (RP1) that views MBRL as an active learning problem. The goal is to improve the world model with the least amount of samples possible. RP1 uses a hybrid objective function that adjusts during optimization, allowing for a balance between reward and exploration at different stages of learning. Additionally, we present a principled way to stop sample collection once there is a sufficient trajectory batch to improve the model. We conducted a thorough evaluation of our approach on various continuous control tasks and demonstrated significant improvements compared to existing methods.",1
"Key challenges for the deployment of reinforcement learning (RL) agents in the real world are the discovery, representation and reuse of skills in the absence of a reward function. To this end, we propose a novel approach to learn a task-agnostic skill embedding space from unlabeled multi-view videos. Our method learns a general skill embedding independently from the task context by using an adversarial loss. We combine a metric learning loss, which utilizes temporal video coherence to learn a state representation, with an entropy regularized adversarial skill-transfer loss. The metric learning loss learns a disentangled representation by attracting simultaneous viewpoints of the same observations and repelling visually similar frames from temporal neighbors. The adversarial skill-transfer loss enhances re-usability of learned skill embeddings over multiple task domains. We show that the learned embedding enables training of continuous control policies to solve novel tasks that require the interpolation of previously seen skills. Our extensive evaluation with both simulation and real world data demonstrates the effectiveness of our method in learning transferable skills from unlabeled interaction videos and composing them for new tasks. Code, pretrained models and dataset are available at http://robotskills.cs.uni-freiburg.de",0
"The main obstacles to implementing reinforcement learning (RL) agents in real-life scenarios include the identification, demonstration, and reuse of abilities without a reward function. As a solution, we introduce a new technique for acquiring a task-agnostic skill embedding space from unlabelled multi-view videos. By utilizing an adversarial loss, our approach learns a general skill embedding that is independent of any specific task context. We merge a metric learning loss, which employs temporal video coherence to establish a state representation, with an entropy regularized adversarial skill-transfer loss. The metric learning loss generates a disentangled representation by attracting simultaneous perspectives of identical observations while repelling visually comparable frames from temporal neighbors. The adversarial skill-transfer loss boosts the re-usability of learned skill embeddings across various task domains. We demonstrate that the acquired embedding enables the development of continuous control policies to tackle novel tasks that demand the interpolation of previously acquired abilities. Our thorough evaluation with both simulated and actual data illustrates the efficacy of our method in learning transferable skills from unlabelled interaction videos and combining them for new tasks. The code, pretrained models, and dataset are all available at http://robotskills.cs.uni-freiburg.de.",1
"The options framework is a popular approach for building temporally extended actions in reinforcement learning. In particular, the option-critic architecture provides general purpose policy gradient theorems for learning actions from scratch that are extended in time. However, past work makes the key assumption that each of the components of option-critic has independent parameters. In this work we note that while this key assumption of the policy gradient theorems of option-critic holds in the tabular case, it is always violated in practice for the deep function approximation setting. We thus reconsider this assumption and consider more general extensions of option-critic and hierarchical option-critic training that optimize for the full architecture with each update. It turns out that not assuming parameter independence challenges a belief in prior work that training the policy over options can be disentangled from the dynamics of the underlying options. In fact, learning can be sped up by focusing the policy over options on states where options are actually likely to terminate. We put our new algorithms to the test in application to sample efficient learning of Atari games, and demonstrate significantly improved stability and faster convergence when learning long options.",0
"The use of the options framework is a popular means of constructing temporally extended actions in reinforcement learning. One notable option-critic architecture provides general-purpose policy gradient theorems that allow for learning actions from scratch, which are extended over time. However, previous research has assumed that the option-critic components have independent parameters. Our research shows that while this assumption holds true for tabular cases, it is not the case for deep function approximation scenarios. As such, we explore more inclusive extensions of option-critic and hierarchical option-critic training, which optimize the full architecture with each update. By not assuming parameter independence, we challenge the notion that training policies over options can be separated from the dynamics of the underlying options. In fact, we found that learning can be accelerated by focusing the policy over options on states where options are more likely to terminate. To test our new algorithms, we applied them to learning Atari games with sample efficiency. Our results showed significantly improved stability and faster convergence when learning long options.",1
"The Markov assumption (MA) is fundamental to the empirical validity of reinforcement learning. In this paper, we propose a novel Forward-Backward Learning procedure to test MA in sequential decision making. The proposed test does not assume any parametric form on the joint distribution of the observed data and plays an important role for identifying the optimal policy in high-order Markov decision processes and partially observable MDPs. We apply our test to both synthetic datasets and a real data example from mobile health studies to illustrate its usefulness.",0
Reinforcement learning's empirical validity depends on the Markov assumption (MA). This paper presents a new Forward-Backward Learning method to evaluate MA in sequential decision making. The proposed evaluation approach does not presume any specific parametric form for the observed data's joint distribution and is critical in identifying the optimal policy for high-order Markov decision processes and partially observable MDPs. The usefulness of our test is illustrated by applying it to synthetic datasets and a real mobile health study dataset.,1
Being able to reach any desired location in the environment can be a valuable asset for an agent. Learning a policy to navigate between all pairs of states individually is often not feasible. An all-goals updating algorithm uses each transition to learn Q-values towards all goals simultaneously and off-policy. However the expensive numerous updates in parallel limited the approach to small tabular cases so far. To tackle this problem we propose to use convolutional network architectures to generate Q-values and updates for a large number of goals at once. We demonstrate the accuracy and generalization qualities of the proposed method on randomly generated mazes and Sokoban puzzles. In the case of on-screen goal coordinates the resulting mapping from frames to distance-maps directly informs the agent about which places are reachable and in how many steps. As an example of application we show that replacing the random actions in epsilon-greedy exploration by several actions towards feasible goals generates better exploratory trajectories on Montezuma's Revenge and Super Mario All-Stars games.,0
"Having the ability to access any location in the environment is a valuable advantage for an agent, but it is often not feasible to learn a policy to navigate between each pair of states individually. An all-goals updating algorithm is used to learn Q-values towards all goals simultaneously and off-policy, but the costly numerous updates in parallel have restricted the approach to small tabular cases. To overcome this issue, we suggest using convolutional network architectures to generate Q-values and updates for a large number of goals at once. We have demonstrated the accuracy and generalization qualities of the proposed method on randomly generated mazes and Sokoban puzzles. In the case of on-screen goal coordinates, the resulting mapping from frames to distance-maps informs the agent directly about reachable places and the number of steps needed. As an illustration of application, we have shown that replacing the random actions in epsilon-greedy exploration by several actions towards feasible goals generates better exploratory trajectories on Montezuma's Revenge and Super Mario All-Stars games.",1
"This paper presents a reinforcement learning approach to synthesizing task-driven control policies for robotic systems equipped with rich sensory modalities (e.g., vision or depth). Standard reinforcement learning algorithms typically produce policies that tightly couple control actions to the entirety of the system's state and rich sensor observations. As a consequence, the resulting policies can often be sensitive to changes in task-irrelevant portions of the state or observations (e.g., changing background colors). In contrast, the approach we present here learns to create a task-driven representation that is used to compute control actions. Formally, this is achieved by deriving a policy gradient-style algorithm that creates an information bottleneck between the states and the task-driven representation; this constrains actions to only depend on task-relevant information. We demonstrate our approach in a thorough set of simulation results on multiple examples including a grasping task that utilizes depth images and a ball-catching task that utilizes RGB images. Comparisons with a standard policy gradient approach demonstrate that the task-driven policies produced by our algorithm are often significantly more robust to sensor noise and task-irrelevant changes in the environment.",0
"The focus of this paper is on using reinforcement learning to develop control policies for robotic systems with advanced sensory capabilities. While traditional reinforcement learning methods tightly link control actions to the system's entire state and sensory observations, these policies can be vulnerable to changes in irrelevant aspects of the environment. In contrast, our approach involves creating a task-specific representation that is utilized to determine control actions. Through the use of a policy gradient-style algorithm, we establish an information bottleneck that restricts actions to task-relevant information. Our simulations demonstrate the effectiveness of this approach, including successful execution of tasks that use either depth or RGB images. Comparing our method to standard policy gradient techniques, we show that our task-driven policies are more resilient to sensor noise and extraneous environmental changes.",1
"An important component of many Deep Reinforcement Learning algorithms is the Experience Replay which serves as a storage mechanism or memory of made experiences. These experiences are used for training and help the agent to stably find the perfect trajectory through the problem space. The classic Experience Replay however makes only use of the experiences it actually made, but the stored samples bear great potential in form of knowledge about the problem that can be extracted. We present an algorithm that creates synthetic experiences in a nondeterministic discrete environment to assist the learner. The Interpolated Experience Replay is evaluated on the FrozenLake environment and we show that it can support the agent to learn faster and even better than the classic version.",0
"The Experience Replay is a crucial aspect of many Deep Reinforcement Learning algorithms as it stores memories of past experiences to aid in training and enable the agent to navigate the problem space efficiently. Although the classic version only utilizes the actual experiences, the stored samples contain valuable knowledge that can be extracted. To address this, we introduce a new algorithm that generates synthetic experiences in a nondeterministic discrete environment to assist the learning process. Our Interpolated Experience Replay is evaluated on the FrozenLake environment, demonstrating superior performance compared to the classic version, enabling the agent to learn faster and more effectively.",1
"Recent findings in neuroscience suggest that the human brain represents information in a geometric structure (for instance, through conceptual spaces). In order to communicate, we flatten the complex representation of entities and their attributes into a single word or a sentence. In this paper we use graph convolutional networks to support the evolution of language and cooperation in multi-agent systems. Motivated by an image-based referential game, we propose a graph referential game with varying degrees of complexity, and we provide strong baseline models that exhibit desirable properties in terms of language emergence and cooperation. We show that the emerged communication protocol is robust, that the agents uncover the true factors of variation in the game, and that they learn to generalize beyond the samples encountered during training.",0
"Recent neuroscience discoveries suggest that the human brain uses geometrical structures, such as conceptual spaces, to represent information. However, when we communicate, we simplify these complex representations into single words or sentences. This paper explores how graph convolutional networks can aid language evolution and cooperation in multi-agent systems. Our proposed graph referential game, based on an image-based referential game, has varying levels of complexity, and our strong baseline models demonstrate desirable properties in language emergence and cooperation. We find that the communication protocol that emerges is resilient, with agents identifying the true game variables and generalizing beyond the training set.",1
"Linear two-timescale stochastic approximation (SA) scheme is an important class of algorithms which has become popular in reinforcement learning (RL), particularly for the policy evaluation problem. Recently, a number of works have been devoted to establishing the finite time analysis of the scheme, especially under the Markovian (non-i.i.d.) noise settings that are ubiquitous in practice. In this paper, we provide a finite-time analysis for linear two timescale SA. Our bounds show that there is no discrepancy in the convergence rate between Markovian and martingale noise, only the constants are affected by the mixing time of the Markov chain. With an appropriate step size schedule, the transient term in the expected error bound is $o(1/k^c)$ and the steady-state term is ${\cal O}(1/k)$, where $c>1$ and $k$ is the iteration number. Furthermore, we present an asymptotic expansion of the expected error with a matching lower bound of $\Omega(1/k)$. A simple numerical experiment is presented to support our theory.",0
"The stochastic approximation (SA) scheme with linear two-timescales is a widely used algorithm in reinforcement learning (RL) for policy evaluation. Recent studies have focused on analyzing this scheme's finite-time behavior, specifically in the presence of Markovian noise that is commonly encountered in practice. In this paper, we provide a finite-time analysis for the linear two-timescale SA scheme. Our analysis shows that the convergence rate is unaffected by the type of noise, but the constants in the bounds are impacted by the Markov chain's mixing time. By using a step size schedule, we demonstrate that the expected error bound has a transient term of $o(1/k^c)$ and a steady-state term of ${\cal O}(1/k)$, where $c>1$ and $k$ is the iteration number. Additionally, we present an asymptotic expansion of the expected error and a corresponding lower bound of $\Omega(1/k)$. Finally, we conduct a simple numerical experiment to validate our theoretical findings.",1
"Vacant taxi drivers' passenger seeking process in a road network generates additional vehicle miles traveled, adding congestion and pollution into the road network and the environment. This paper aims to employ a Markov Decision Process (MDP) to model idle e-hailing drivers' optimal sequential decisions in passenger-seeking. Transportation network companies (TNC) or e-hailing (e.g., Didi, Uber) drivers exhibit different behaviors from traditional taxi drivers because e-hailing drivers do not need to actually search for passengers. Instead, they reposition themselves so that the matching platform can match a passenger. Accordingly, we incorporate e-hailing drivers' new features into our MDP model. The reward function used in the MDP model is uncovered by leveraging an inverse reinforcement learning technique. We then use 44,160 Didi drivers' 3-day trajectories to train the model. To validate the effectiveness of the model, a Monte Carlo simulation is conducted to simulate the performance of drivers under the guidance of the optimal policy, which is then compared with the performance of drivers following one baseline heuristic, namely, the local hotspot strategy. The results show that our model is able to achieve a 17.5% improvement over the local hotspot strategy in terms of the rate of return. The proposed MDP model captures the supply-demand ratio considering the fact that the number of drivers in this study is sufficiently large and thus the number of unmatched orders is assumed to be negligible. To better incorporate the competition among multiple drivers into the model, we have also devised and calibrated a dynamic adjustment strategy of the order matching probability.",0
"The process of vacant taxi drivers seeking passengers in a road network contributes to increased vehicle miles traveled, causing congestion and pollution in the environment. This study aims to utilize a Markov Decision Process (MDP) to model the optimal decision-making process of idle e-hailing drivers in their passenger-seeking efforts. Unlike traditional taxi drivers, e-hailing drivers do not actively search for passengers, but rather reposition themselves to be matched with a passenger through a matching platform. To account for these differences, the MDP model incorporates the unique features of e-hailing drivers. The reward function used in the model is determined through an inverse reinforcement learning technique, and the model is trained using 44,160 Didi drivers' trajectories. To validate the effectiveness of the model, a Monte Carlo simulation is conducted, comparing the performance of drivers following the optimal policy with drivers following a baseline heuristic. The results demonstrate a 17.5% improvement in the rate of return using the proposed MDP model. The model considers the supply-demand ratio, assuming that the number of unmatched orders is negligible due to the large number of drivers in the study. To account for competition among drivers, a dynamic adjustment strategy of the order matching probability is also devised and calibrated.",1
"Recent studies have shown that reinforcement learning (RL) models are vulnerable in various noisy scenarios. For instance, the observed reward channel is often subject to noise in practice (e.g., when rewards are collected through sensors), and is therefore not credible. In addition, for applications such as robotics, a deep reinforcement learning (DRL) algorithm can be manipulated to produce arbitrary errors by receiving corrupted rewards. In this paper, we consider noisy RL problems with perturbed rewards, which can be approximated with a confusion matrix. We develop a robust RL framework that enables agents to learn in noisy environments where only perturbed rewards are observed. Our solution framework builds on existing RL/DRL algorithms and firstly addresses the biased noisy reward setting without any assumptions on the true distribution (e.g., zero-mean Gaussian noise as made in previous works). The core ideas of our solution include estimating a reward confusion matrix and defining a set of unbiased surrogate rewards. We prove the convergence and sample complexity of our approach. Extensive experiments on different DRL platforms show that trained policies based on our estimated surrogate reward can achieve higher expected rewards, and converge faster than existing baselines. For instance, the state-of-the-art PPO algorithm is able to obtain 84.6% and 80.8% improvements on average score for five Atari games, with error rates as 10% and 30% respectively.",0
"Recent research has revealed that reinforcement learning (RL) models are susceptible to errors in noisy situations. This is particularly evident when rewards are collected through sensors, as the observed reward channel is often noisy and therefore unreliable. In addition, deep reinforcement learning (DRL) algorithms used in applications such as robotics can produce erroneous outcomes when receiving corrupted rewards. In this study, we focus on noisy RL problems with perturbed rewards, which can be approximated with a confusion matrix. We have developed a robust RL framework that allows agents to learn in noisy environments where only perturbed rewards are observed. Our approach is based on existing RL/DRL algorithms and addresses biased noisy reward settings without making assumptions on the true distribution, such as zero-mean Gaussian noise. The core ideas of our approach include estimating a reward confusion matrix and defining a set of unbiased surrogate rewards. We have demonstrated the convergence and sample complexity of our approach through experiments on different DRL platforms. Our results indicate that trained policies based on our estimated surrogate reward can achieve higher expected rewards and converge faster than existing baselines. For example, the state-of-the-art PPO algorithm improved the average score for five Atari games by 84.6% and 80.8% with error rates of 10% and 30%, respectively.",1
"Off-policy ensemble reinforcement learning (RL) methods have demonstrated impressive results across a range of RL benchmark tasks. Recent works suggest that directly imitating experts' policies in a supervised manner before or during the course of training enables faster policy improvement for an RL agent. Motivated by these recent insights, we propose Periodic Intra-Ensemble Knowledge Distillation (PIEKD). PIEKD is a learning framework that uses an ensemble of policies to act in the environment while periodically sharing knowledge amongst policies in the ensemble through knowledge distillation. Our experiments demonstrate that PIEKD improves upon a state-of-the-art RL method in sample efficiency on several challenging MuJoCo benchmark tasks. Additionally, we perform ablation studies to better understand PIEKD.",0
"Ensemble reinforcement learning techniques that are off-policy have shown impressive results in various RL benchmark tasks. Recent research suggests that imitating experts' policies via supervised learning before or during training can expedite policy improvement for an RL agent. In light of this insight, we present the Periodic Intra-Ensemble Knowledge Distillation (PIEKD) framework. PIEKD utilizes an ensemble of policies to operate in the environment and periodically shares knowledge among the policies through knowledge distillation. Our experiments prove that PIEKD enhances sample efficiency compared to a state-of-the-art RL method on numerous challenging MuJoCo benchmark tasks. Additionally, we conduct ablation studies to gain a better understanding of the PIEKD approach.",1
"In recent years significant progress has been made in dealing with challenging problems using reinforcement learning.Despite its great success, reinforcement learning still faces challenge in continuous control tasks. Conventional methods always compute the derivatives of the optimal goal with a costly computation resources, and are inefficient, unstable and lack of robust-ness when dealing with such tasks. Alternatively, derivative-based methods treat the optimization process as a blackbox and show robustness and stability in learning continuous control tasks, but not data efficient in learning. The combination of both methods so as to get the best of the both has raised attention. However, most of the existing combination works adopt complex neural networks (NNs) as the policy for control. The double-edged sword of deep NNs can yield better performance, but also makes it difficult for parameter tuning and computation. To this end, in this paper we presents a novel method called FiDi-RL, which incorporates deep RL with Finite-Difference (FiDi) policy search.FiDi-RL combines Deep Deterministic Policy Gradients (DDPG)with Augment Random Search (ARS) and aims at improving the data efficiency of ARS. The empirical results show that FiDi-RL can improves the performance and stability of ARS, and provide competitive results against some existing deep reinforcement learning methods",0
"Reinforcement learning has made significant strides in solving challenging problems in recent years. However, it still faces difficulties in continuous control tasks. Traditional methods require expensive computation resources to compute the derivatives of the optimal goal, making them inefficient, unstable, and lacking in robustness for such tasks. On the other hand, derivative-based methods treat the optimization process as a blackbox, showing robustness and stability in learning continuous control tasks, but not data efficiency. Combining both methods has gained attention, but most existing works adopt complex neural networks as the control policy, which can result in better performance but make parameter tuning and computation difficult. In this paper, we introduce FiDi-RL, a novel method that combines deep RL with Finite-Difference policy search to improve data efficiency. FiDi-RL combines Deep Deterministic Policy Gradients with Augment Random Search and improves the performance and stability of ARS while providing competitive results against existing deep reinforcement learning methods.",1
"Understanding how goal states control behavior is a question ripe for interrogation by new methods from machine learning. These methods require large and labeled datasets to train models. To annotate a large-scale image dataset with observed search fixations, we collected 16,184 fixations from people searching for either microwaves or clocks in a dataset of 4,366 images (MS-COCO). We then used this behaviorally-annotated dataset and the machine learning method of Inverse-Reinforcement Learning (IRL) to learn target-specific reward functions and policies for these two target goals. Finally, we used these learned policies to predict the fixations of 60 new behavioral searchers (clock = 30, microwave = 30) in a disjoint test dataset of kitchen scenes depicting both a microwave and a clock (thus controlling for differences in low-level image contrast). We found that the IRL model predicted behavioral search efficiency and fixation-density maps using multiple metrics. Moreover, reward maps from the IRL model revealed target-specific patterns that suggest, not just attention guidance by target features, but also guidance by scene context (e.g., fixations along walls in the search of clocks). Using machine learning and the psychologically-meaningful principle of reward, it is possible to learn the visual features used in goal-directed attention control.",0
"The use of machine learning techniques presents an opportunity to investigate how behavior is influenced by goal states. To train models, large and labeled datasets are required. To create a dataset for this purpose, we collected 16,184 fixations from individuals searching for microwaves or clocks in a dataset of 4,366 images. This dataset was then used to train the machine learning method of Inverse-Reinforcement Learning (IRL) to learn target-specific reward functions and policies for these two target goals. The learned policies were subsequently utilized to predict the fixations of 60 new behavioral searchers in a distinct test dataset of kitchen scenes depicting both a microwave and a clock. The IRL model was found to efficiently predict behavioral search efficiency and fixation-density maps using various metrics. Additionally, reward maps from the IRL model revealed patterns specific to the target that suggest that scene context, in addition to target features, play a role in guiding attention. By incorporating the psychologically-meaningful principle of reward into machine learning, it is possible to learn the visual features utilized in goal-directed attention control.",1
"In this work, we present a reinforcement learning algorithm that can find a variety of policies (novel policies) for a task that is given by a task reward function. Our method does this by creating a second reward function that recognizes previously seen state sequences and rewards those by novelty, which is measured using autoencoders that have been trained on state sequences from previously discovered policies. We present a two-objective update technique for policy gradient algorithms in which each update of the policy is a compromise between improving the task reward and improving the novelty reward. Using this method, we end up with a collection of policies that solves a given task as well as carrying out action sequences that are distinct from one another. We demonstrate this method on maze navigation tasks, a reaching task for a simulated robot arm, and a locomotion task for a hopper. We also demonstrate the effectiveness of our approach on deceptive tasks in which policy gradient methods often get stuck.",0
"Our work introduces a reinforcement learning algorithm that can explore various policies for a given task reward function. To achieve this, we establish a second reward function that recognizes previously encountered state sequences and rewards them based on their novelty. To measure novelty, we use autoencoders that were trained on state sequences from previously discovered policies. We propose a two-objective update technique for policy gradient algorithms, where each policy update balances improving the task reward and novelty reward. Using this approach, we obtain a set of policies that solve the given task while executing unique action sequences. Our approach is demonstrated on maze navigation tasks, a reaching task for a simulated robot arm, and a locomotion task for a hopper. Additionally, we validate the effectiveness of our method on deceptive tasks, where policy gradient algorithms often struggle.",1
"Many real-world problems can be reduced to combinatorial optimization on a graph, where the subset or ordering of vertices that maximize some objective function must be found. With such tasks often NP-hard and analytically intractable, reinforcement learning (RL) has shown promise as a framework with which efficient heuristic methods to tackle these problems can be learned. Previous works construct the solution subset incrementally, adding one element at a time, however, the irreversible nature of this approach prevents the agent from revising its earlier decisions, which may be necessary given the complexity of the optimization task. We instead propose that the agent should seek to continuously improve the solution by learning to explore at test time. Our approach of exploratory combinatorial optimization (ECO-DQN) is, in principle, applicable to any combinatorial problem that can be defined on a graph. Experimentally, we show our method to produce state-of-the-art RL performance on the Maximum Cut problem. Moreover, because ECO-DQN can start from any arbitrary configuration, it can be combined with other search methods to further improve performance, which we demonstrate using a simple random search.",0
"The optimization of graphs through combinatorial methods is a common solution for many practical problems. However, this process is often difficult due to its complexity and the fact that it is typically NP-hard and not easily solved through analytical means. Reinforcement learning (RL) serves as a promising framework for developing efficient heuristic methods to tackle these problems. Previous methods have constructed solution subsets incrementally, which limits the ability to revise earlier decisions. Instead, we propose the use of exploratory combinatorial optimization (ECO-DQN) to continuously improve solutions through test time exploration. Our approach is suitable for any combinatorial problem on a graph and produces state-of-the-art RL performance on the Maximum Cut problem. Additionally, ECO-DQN can be combined with other search methods to further improve performance, as demonstrated through the use of a simple random search, and can start from any arbitrary configuration.",1
"It is difficult to be able to imitate well in unknown states from a small amount of expert data and sampling data. Supervised learning methods such as Behavioral Cloning do not require sampling data, but usually suffer from distribution shift. The methods based on reinforcement learning, such as inverse reinforcement learning and generative adversarial imitation learning (GAIL), can learn from only a few expert data. However, they often need to interact with the environment. Soft Q imitation learning addressed the problems, and it was shown that it could learn efficiently by combining Behavioral Cloning and soft Q-learning with constant rewards. In order to make this algorithm more robust to distribution shift, we propose Discriminator Soft Actor Critic (DSAC). It uses a reward function based on adversarial inverse reinforcement learning instead of constant rewards. We evaluated it on PyBullet environments with only four expert trajectories.",0
"Repurposing: Replicating an unfamiliar state with precision using minimal expert data and sampling data is challenging. While methods like Behavioral Cloning do not need sampling data, they often suffer from distribution shift. Reinforcement learning-based techniques such as inverse reinforcement learning and generative adversarial imitation learning (GAIL) can learn from a few expert data, but they require interaction with the environment. Soft Q imitation learning resolves these issues by combining Behavioral Cloning and soft Q-learning with constant rewards. To enhance the algorithm's resilience to distribution shift, we propose Discriminator Soft Actor Critic (DSAC), which employs a reward function based on adversarial inverse reinforcement learning instead of constant rewards. With only four expert trajectories, we tested the model on PyBullet environments.",1
"We study locally differentially private algorithms for reinforcement learning to obtain a robust policy that performs well across distributed private environments. Our algorithm protects the information of local agents' models from being exploited by adversarial reverse engineering. Since a local policy is strongly being affected by the individual environment, the output of the agent may release the private information unconsciously. In our proposed algorithm, local agents update the model in their environments and report noisy gradients designed to satisfy local differential privacy (LDP) that gives a rigorous local privacy guarantee. By utilizing a set of reported noisy gradients, a central aggregator updates its model and delivers it to different local agents. In our empirical evaluation, we demonstrate how our method performs well under LDP. To the best of our knowledge, this is the first work that actualizes distributed reinforcement learning under LDP. This work enables us to obtain a robust agent that performs well across distributed private environments.",0
"Our focus is on locally differentially private algorithms for reinforcement learning. The aim is to develop a policy that can function effectively across various private environments while safeguarding the models of local agents from being exploited by hostile reverse engineering. Since the output of an agent is heavily influenced by their individual environment, there is a risk of unintentionally disclosing private information. In our proposed algorithm, local agents update their models in their respective environments and report noisy gradients that comply with local differential privacy (LDP) guidelines. This ensures that local privacy is rigorously maintained. The central aggregator utilizes the reported noisy gradients to update its model, which is then distributed to the various local agents. Our empirical evaluation demonstrates the effectiveness of our method under LDP, making it the first work to implement distributed reinforcement learning under LDP. This work enables us to obtain a robust agent that performs well across distributed private environments.",1
"Multi-objective Neural Architecture Search (NAS) aims to discover novel architectures in the presence of multiple conflicting objectives. Despite recent progress, the problem of approximating the full Pareto front accurately and efficiently remains challenging. In this work, we explore the novel reinforcement learning (RL) based paradigm of non-stationary policy gradient (NPG). NPG utilizes a non-stationary reward function, and encourages a continuous adaptation of the policy to capture the entire Pareto front efficiently. We introduce two novel reward functions with elements from the dominant paradigms of scalarization and evolution. To handle non-stationarity, we propose a new exploration scheme using cosine temperature decay with warm restarts. For fast and accurate architecture evaluation, we introduce a novel pre-trained shared model that we continuously fine-tune throughout training. Our extensive experimental study with various datasets shows that our framework can approximate the full Pareto front well at fast speeds. Moreover, our discovered cells can achieve supreme predictive performance compared to other multi-objective NAS methods, and other single-objective NAS methods at similar network sizes. Our work demonstrates the potential of NPG as a simple, efficient, and effective paradigm for multi-objective NAS.",0
"The aim of Multi-objective Neural Architecture Search (NAS) is to find new architectures that can satisfy multiple objectives, but accurately and efficiently approximating the entire Pareto front remains a challenge. In this study, we investigate a novel reinforcement learning (RL) based approach called non-stationary policy gradient (NPG). NPG employs a non-stationary reward function to encourage the policy to adapt continuously and capture the entire Pareto front. We propose two unique reward functions that incorporate aspects of scalarization and evolution to tackle non-stationarity, and we present a new exploration scheme using cosine temperature decay with warm restarts. To enable fast and accurate architecture evaluation, we introduce a pre-trained shared model that we fine-tune throughout training. Our experiments show that our approach can efficiently approximate the full Pareto front and achieve superior predictive performance compared to other multi-objective NAS methods and single-objective NAS methods at similar network sizes. Our findings indicate that NPG is a simple, efficient, and effective paradigm for multi-objective NAS.",1
"Imitation learning allows agents to learn complex behaviors from demonstrations. However, learning a complex vision-based task may require an impractical number of demonstrations. Meta-imitation learning is a promising approach towards enabling agents to learn a new task from one or a few demonstrations by leveraging experience from learning similar tasks. In the presence of task ambiguity or unobserved dynamics, demonstrations alone may not provide enough information; an agent must also try the task to successfully infer a policy. In this work, we propose a method that can learn to learn from both demonstrations and trial-and-error experience with sparse reward feedback. In comparison to meta-imitation, this approach enables the agent to effectively and efficiently improve itself autonomously beyond the demonstration data. In comparison to meta-reinforcement learning, we can scale to substantially broader distributions of tasks, as the demonstration reduces the burden of exploration. Our experiments show that our method significantly outperforms prior approaches on a set of challenging, vision-based control tasks.",0
"The method of imitation learning permits agents to acquire intricate behaviors by observing demonstrations. Nonetheless, mastering a complex task based on vision may necessitate an unfeasible number of demonstrations. To overcome this, meta-imitation learning is a promising approach that allows agents to learn a new task by utilizing knowledge gained from similar tasks, even with just one or a few demonstrations. However, if task ambiguity or unobserved dynamics are present, demonstrations alone may not be sufficient, and the agent must try the task to deduce a policy. In this study, we propose a technique that can learn from both demonstrations and trial-and-error experience, even with sparse feedback. This approach enables the agent to enhance its performance autonomously beyond the demonstration data, making it more effective and efficient than meta-imitation. Furthermore, compared to meta-reinforcement learning, our method can scale to a wider array of tasks, as demonstrations decrease the need for exploration. Our experiments demonstrate that our approach outperforms previous methods on challenging control tasks that rely on vision.",1
"Academic research in the field of autonomous vehicles has reached high popularity in recent years related to several topics as sensor technologies, V2X communications, safety, security, decision making, control, and even legal and standardization rules. Besides classic control design approaches, Artificial Intelligence and Machine Learning methods are present in almost all of these fields. Another part of research focuses on different layers of Motion Planning, such as strategic decisions, trajectory planning, and control. A wide range of techniques in Machine Learning itself have been developed, and this article describes one of these fields, Deep Reinforcement Learning (DRL). The paper provides insight into the hierarchical motion planning problem and describes the basics of DRL. The main elements of designing such a system are the modeling of the environment, the modeling abstractions, the description of the state and the perception models, the appropriate rewarding, and the realization of the underlying neural network. The paper describes vehicle models, simulation possibilities and computational requirements. Strategic decisions on different layers and the observation models, e.g., continuous and discrete state representations, grid-based, and camera-based solutions are presented. The paper surveys the state-of-art solutions systematized by the different tasks and levels of autonomous driving, such as car-following, lane-keeping, trajectory following, merging, or driving in dense traffic. Finally, open questions and future challenges are discussed.",0
"Autonomous vehicle research has become increasingly popular in recent years, with a focus on a variety of topics, including sensor technologies, V2X communications, safety, security, decision making, control, and legal and standardization rules. While traditional control design approaches are still utilized, Artificial Intelligence and Machine Learning methods are now present in almost all of these fields. Additionally, research has delved into different layers of Motion Planning, such as strategic decisions, trajectory planning, and control. This article specifically focuses on one area of Machine Learning, Deep Reinforcement Learning (DRL), and its application in hierarchical motion planning. The paper outlines the necessary elements for designing a DRL system, including environment modeling, abstraction modeling, state and perception models, appropriate rewarding, and the underlying neural network. The paper also explores vehicle models, simulation options, and computational requirements, as well as strategic decisions and observation models. Furthermore, the paper surveys the current state-of-the-art solutions based on different tasks and levels of autonomous driving, such as car-following, lane-keeping, trajectory following, merging, and driving in dense traffic. Finally, the paper concludes with a discussion on open questions and future challenges.",1
"Motivated by the emerging use of multi-agent reinforcement learning (MARL) in engineering applications such as networked robotics, swarming drones, and sensor networks, we investigate the policy evaluation problem in a fully decentralized setting, using temporal-difference (TD) learning with linear function approximation to handle large state spaces in practice. The goal of a group of agents is to collaboratively learn the value function of a given policy from locally private rewards observed in a shared environment, through exchanging local estimates with neighbors. Despite their simplicity and widespread use, our theoretical understanding of such decentralized TD learning algorithms remains limited. Existing results were obtained based on i.i.d. data samples, or by imposing an `additional' projection step to control the `gradient' bias incurred by the Markovian observations. In this paper, we provide a finite-sample analysis of the fully decentralized TD(0) learning under both i.i.d. as well as Markovian samples, and prove that all local estimates converge linearly to a small neighborhood of the optimum. The resultant error bounds are the first of its type---in the sense that they hold under the most practical assumptions ---which is made possible by means of a novel multi-step Lyapunov analysis.",0
"Our research is motivated by the growing application of multi-agent reinforcement learning (MARL) in engineering fields, such as networked robotics, swarming drones, and sensor networks. We focus on solving the policy evaluation problem in a fully decentralized setting, utilizing temporal-difference (TD) learning with linear function approximation to handle large state spaces. Our aim is for a group of agents to collectively learn the value function of a given policy from locally private rewards observed in a shared environment, by exchanging local estimates with neighboring agents. However, the theoretical understanding of decentralized TD learning algorithms remains limited, with existing results being based on i.i.d. data samples or added projection steps to control the gradient bias. In our study, we provide a finite-sample analysis of the fully decentralized TD(0) learning under both i.i.d. and Markovian samples. We prove that all local estimates converge linearly to a small neighborhood of the optimum, and our error bounds are the first of their kind, holding under realistic assumptions. This is made achievable through a novel multi-step Lyapunov analysis.",1
"We introduce gym-city, a Reinforcement Learning environment that uses SimCity 1's game engine to simulate an urban environment, wherein agents might seek to optimize one or a combination of any number of city-wide metrics, on gameboards of various sizes. We focus on population, and analyze our agents' ability to generalize to larger map-sizes than those seen during training. The environment is interactive, allowing a human player to build alongside agents during training and inference, potentially influencing the course of their learning, or manually probing and evaluating their performance. To test our agents' ability to capture distance-agnostic relationships between elements of the gameboard, we design a minigame within the environment which is, by design, unsolvable at large enough scales given strictly local strategies. Given the game engine's extensive use of Cellular Automata, we also train our agents to ""play"" Conway's Game of Life -- again optimizing for population -- and examine their behaviour at multiple scales. To make our models compatible with variable-scale gameplay, we use Neural Networks with recursive weights and structure -- fractals to be truncated at different depths, dependent upon the size of the gameboard.",0
"We present gym-city, a Reinforcement Learning environment that utilizes the game engine of SimCity 1 to create a simulated urban environment. The gameboards come in various sizes, and the agents may optimize one or more city-wide metrics. Our primary focus is population, and we assess the agents' capacity to adapt to larger map-sizes than those encountered during training. The environment is interactive, allowing human players to collaborate with agents during training and inference, which may impact their learning trajectory, or manually evaluate their performance. We develop a minigame within the environment to test the agents' ability to capture distance-agnostic relationships between gameboard elements. The game is purposefully unsolvable with strictly local strategies at large scales. Because the game engine relies heavily on Cellular Automata, we train our agents to play Conway's Game of Life as well, with a focus on optimizing population and observing their behavior at various scales. To ensure compatibility with variable-scale gameplay, we employ Neural Networks with recursive weights and structure, employing fractals truncated to different depths depending on the gameboard size.",1
"Multi-agent systems exhibit complex behaviors that emanate from the interactions of multiple agents in a shared environment. In this work, we are interested in controlling one agent in a multi-agent system and successfully learn to interact with the other agents that have fixed policies. Modeling the behavior of other agents (opponents) is essential in understanding the interactions of the agents in the system. By taking advantage of recent advances in unsupervised learning, we propose modeling opponents using variational autoencoders. Additionally, many existing methods in the literature assume that the opponent models have access to opponent's observations and actions during both training and execution. To eliminate this assumption, we propose a modification that attempts to identify the underlying opponent model using only local information of our agent, such as its observations, actions, and rewards. The experiments indicate that our opponent modeling methods achieve equal or greater episodic returns in reinforcement learning tasks against another modeling method.",0
"The interactions of multiple agents in a shared environment result in complex behaviors in multi-agent systems. Our focus in this study is on controlling one agent in such a system and learning to interact with other agents who have fixed policies. It is crucial to model the behavior of opponents in order to understand the interactions of agents. We propose using variational autoencoders to model opponents, taking advantage of recent advancements in unsupervised learning. Many existing methods assume that opponent models have access to their observations and actions during both training and execution. To remove this assumption, we propose a modification that can identify the underlying opponent model using only local information of our agent, such as its observations, actions, and rewards. Our experiments show that our opponent modeling methods achieve similar or better episodic returns in reinforcement learning tasks compared to other modeling methods.",1
"The crucial components of a conventional image registration method are the choice of the right feature representations and similarity measures. These two components, although elaborately designed, are somewhat handcrafted using human knowledge. To this end, these two components are tackled in an end-to-end manner via reinforcement learning in this work. Specifically, an artificial agent, which is composed of a combined policy and value network, is trained to adjust the moving image toward the right direction. We train this network using an asynchronous reinforcement learning algorithm, where a customized reward function is also leveraged to encourage robust image registration. This trained network is further incorporated with a lookahead inference to improve the registration capability. The advantage of this algorithm is fully demonstrated by our superior performance on clinical MR and CT image pairs to other state-of-the-art medical image registration methods.",0
"In a conventional image registration method, selecting appropriate feature representations and similarity measures is crucial. Despite being thoughtfully designed, these components are often manually crafted based on human knowledge. To address this, we employ reinforcement learning to address these components in an end-to-end manner. An artificial agent consisting of a policy and value network is trained to adjust the moving image for optimal results. We use an asynchronous reinforcement learning algorithm and a customized reward function to encourage robust image registration. Additionally, we incorporate a lookahead inference to enhance the registration capability. Our approach outperforms other state-of-the-art medical image registration methods, as demonstrated by our superior performance on clinical MR and CT image pairs.",1
"We consider the problem of off-policy evaluation for reinforcement learning, where the goal is to estimate the expected reward of a target policy $\pi$ using offline data collected by running a logging policy $\mu$. Standard importance-sampling based approaches for this problem suffer from a variance that scales exponentially with time horizon $H$, which motivates a splurge of recent interest in alternatives that break the ""Curse of Horizon"" (Liu et al. 2018, Xie et al. 2019). In particular, it was shown that a marginalized importance sampling (MIS) approach can be used to achieve an estimation error of order $O(H^3/ n)$ in mean square error (MSE) under an episodic Markov Decision Process model with finite states and potentially infinite actions. The MSE bound however is still a factor of $H$ away from a Cramer-Rao lower bound of order $\Omega(H^2/n)$. In this paper, we prove that with a simple modification to the MIS estimator, we can asymptotically attain the Cramer-Rao lower bound, provided that the action space is finite. We also provide a general method for constructing MIS estimators with high-probability error bounds.",0
"The problem of evaluating the expected reward of a target policy $\pi$ using offline data collected by a logging policy $\mu$ in reinforcement learning is examined. Traditional importance-sampling based approaches suffer from a variance issue that increases exponentially with the time horizon $H"", thus generating interest in alternative approaches that eliminate the ""Curse of Horizon."" Recent research has found that marginalized importance sampling (MIS) can be used to achieve an estimation error of order $O(H^3/n)$ in mean square error (MSE) under an episodic Markov Decision Process model with finite states and potentially infinite actions. However, the MSE bound is still a factor of $H$ away from the Cramer-Rao lower bound of order $\Omega(H^2/n)$. This paper demonstrates that by modifying the MIS estimator, the Cramer-Rao lower bound can be asymptotically achieved, provided that the action space is finite. A general method for constructing MIS estimators with high-probability error bounds is also provided.",1
"Regret analysis is challenging in Multi-Agent Reinforcement Learning (MARL) primarily due to the dynamical environments and the decentralized information among agents. We attempt to solve this challenge in the context of decentralized learning in multi-agent linear-quadratic (LQ) dynamical systems. We begin with a simple setup consisting of two agents and two dynamically decoupled stochastic linear systems, each system controlled by an agent. The systems are coupled through a quadratic cost function. When both systems' dynamics are unknown and there is no communication among the agents, we show that no learning policy can generate sub-linear in $T$ regret, where $T$ is the time horizon. When only one system's dynamics are unknown and there is one-directional communication from the agent controlling the unknown system to the other agent, we propose a MARL algorithm based on the construction of an auxiliary single-agent LQ problem. The auxiliary single-agent problem in the proposed MARL algorithm serves as an implicit coordination mechanism among the two learning agents. This allows the agents to achieve a regret within $O(\sqrt{T})$ of the regret of the auxiliary single-agent problem. Consequently, using existing results for single-agent LQ regret, our algorithm provides a $\tilde{O}(\sqrt{T})$ regret bound. (Here $\tilde{O}(\cdot)$ hides constants and logarithmic factors). Our numerical experiments indicate that this bound is matched in practice. From the two-agent problem, we extend our results to multi-agent LQ systems with certain communication patterns.",0
"The difficulty of regret analysis in Multi-Agent Reinforcement Learning (MARL) arises from the complex and changing environments, as well as the decentralized nature of information among agents. To address this challenge, we focus on decentralized learning in multi-agent linear-quadratic (LQ) dynamical systems. Initially, we consider a basic scenario with two agents controlling two independent stochastic linear systems connected by a quadratic cost function. In situations where both system dynamics are uncertain and agents cannot communicate, we prove that no learning policy can achieve sub-linear regret in the time horizon T. However, when one system's dynamics are unknown and there is one-way communication from the agent controlling that system to the other agent, we propose a MARL algorithm based on an auxiliary single-agent LQ problem. This algorithm implicitly coordinates the agents, allowing them to achieve a regret within O(sqrt(T)) of the auxiliary single-agent problem. Our algorithm provides a regret bound of approximately O(sqrt(T)), which is verified through numerical experiments. Furthermore, we extend our findings to multi-agent LQ systems with specific communication patterns.",1
"A lifelong reinforcement learning system is a learning system that has the ability to learn through trail-and-error interaction with the environment over its lifetime. In this paper, I give some arguments to show that the traditional reinforcement learning paradigm fails to model this type of learning system. Some insights into lifelong reinforcement learning are provided, along with a simplistic prototype lifelong reinforcement learning system.",0
The paper discusses the concept of a lifelong reinforcement learning system that can learn from its interactions with the environment over its entire lifetime. It argues that the conventional reinforcement learning approach is inadequate for modeling this type of system. The paper also provides some insights into lifelong reinforcement learning and presents a basic prototype of such a system.,1
"With the explosive growth of online products and content, recommendation techniques have been considered as an effective tool to overcome information overload, improve user experience, and boost business revenue. In recent years, we have observed a new desideratum of considering long-term rewards of multiple related recommendation tasks simultaneously. The consideration of long-term rewards is strongly tied to business revenue and growth. Learning multiple tasks simultaneously could generally improve the performance of individual task due to knowledge sharing in multi-task learning. While a few existing works have studied long-term rewards in recommendations, they mainly focus on a single recommendation task. In this paper, we propose {\it PoDiRe}: a \underline{po}licy \underline{di}stilled \underline{re}commender that can address long-term rewards of recommendations and simultaneously handle multiple recommendation tasks. This novel recommendation solution is based on a marriage of deep reinforcement learning and knowledge distillation techniques, which is able to establish knowledge sharing among different tasks and reduce the size of a learning model. The resulting model is expected to attain better performance and lower response latency for real-time recommendation services. In collaboration with Samsung Game Launcher, one of the world's largest commercial mobile game platforms, we conduct a comprehensive experimental study on large-scale real data with hundreds of millions of events and show that our solution outperforms many state-of-the-art methods in terms of several standard evaluation metrics.",0
"Recommendation techniques have proven to be an effective solution for managing the overwhelming amount of online products and content, enhancing user experience, and increasing business revenue. However, there is a growing need to consider the long-term benefits of multiple related recommendation tasks to further promote business growth. Incorporating long-term rewards is critical to overall business revenue. Additionally, learning multiple tasks simultaneously can improve individual task performance by sharing knowledge. While some studies have examined long-term rewards in recommendations, they have primarily focused on a single task. This paper introduces ""PoDiRe,"" a novel recommendation solution that uses deep reinforcement learning and knowledge distillation techniques to address long-term rewards of recommendations while handling multiple tasks. This approach enables knowledge sharing among different tasks and reduces the model's size, resulting in better performance and lower response latency for real-time recommendation services. The proposed solution is evaluated using large-scale real data from Samsung Game Launcher, one of the world's largest mobile game platforms. The results show that PoDiRe outperforms many state-of-the-art methods in several standard evaluation metrics.",1
"Constrained Markov Decision Processes are a class of stochastic decision problems in which the decision maker must select a policy that satisfies auxiliary cost constraints. This paper extends upper confidence reinforcement learning for settings in which the reward function and the constraints, described by cost functions, are unknown a priori but the transition kernel is known. Such a setting is well-motivated by a number of applications including exploration of unknown, potentially unsafe, environments. We present an algorithm C-UCRL and show that it achieves sub-linear regret ($ O(T^{\frac{3}{4}}\sqrt{\log(T/\delta)})$) with respect to the reward while satisfying the constraints even while learning with probability $1-\delta$. Illustrative examples are provided.",0
"The paper discusses Constrained Markov Decision Processes, which are a type of decision problem that involve selecting a policy that meets cost constraints. It introduces an extension to upper confidence reinforcement learning for situations where the reward function and cost functions are not known beforehand, but the transition kernel is known. This is useful for exploring unknown or potentially dangerous environments. The C-UCRL algorithm is presented, which achieves sub-linear regret and satisfies the constraints while learning. Examples are given to illustrate the algorithm's effectiveness.",1
"While traditional methods for instruction-following typically assume prior linguistic and perceptual knowledge, many recent works in reinforcement learning (RL) have proposed learning policies end-to-end, typically by training neural networks to map joint representations of observations and instructions directly to actions. In this work, we present a novel framework for learning to perform temporally extended tasks using spatial reasoning in the RL framework, by sequentially imagining visual goals and choosing appropriate actions to fulfill imagined goals. Our framework operates on raw pixel images, assumes no prior linguistic or perceptual knowledge, and learns via intrinsic motivation and a single extrinsic reward signal measuring task completion. We validate our method in two environments with a robot arm in a simulated interactive 3D environment. Our method outperforms two flat architectures with raw-pixel and ground-truth states, and a hierarchical architecture with ground-truth states on object arrangement tasks.",0
"While conventional approaches to following instructions rely on pre-existing linguistic and perceptual knowledge, recent works in reinforcement learning (RL) have suggested end-to-end policy learning. This is achieved by training neural networks to directly map joint representations of observations and instructions to actions. Our study introduces a novel framework for learning spatial reasoning in the RL framework, which involves sequentially imagining visual objectives and selecting appropriate actions to achieve them. Our framework operates on raw pixel images, does not require prior linguistic or perceptual knowledge, and learns through intrinsic motivation and a single extrinsic reward signal that measures task completion. We validate our approach in two environments using a robot arm in a simulated interactive 3D environment. Our methodology surpasses two flat architectures with raw-pixel and ground-truth states, as well as a hierarchical architecture with ground-truth states, in object arrangement tasks.",1
"Stochastic compositional optimization arises in many important machine learning tasks such as value function evaluation in reinforcement learning and portfolio management. The objective function is the composition of two expectations of stochastic functions, and is more challenging to optimize than vanilla stochastic optimization problems. In this paper, we investigate the stochastic compositional optimization in the general smooth non-convex setting. We employ a recently developed idea of \textit{Stochastic Recursive Gradient Descent} to design a novel algorithm named SARAH-Compositional, and prove a sharp Incremental First-order Oracle (IFO) complexity upper bound for stochastic compositional optimization: $\mathcal{O}((n+m)^{1/2} \varepsilon^{-2})$ in the finite-sum case and $\mathcal{O}(\varepsilon^{-3})$ in the online case. Such a complexity is known to be the best one among IFO complexity results for non-convex stochastic compositional optimization, and is believed to be optimal. Our experiments validate the theoretical performance of our algorithm.",0
"Stochastic compositional optimization is a common challenge in various machine learning applications, including reinforcement learning and portfolio management. The objective function involves two expectations of stochastic functions, which makes it more difficult to optimize than traditional stochastic optimization problems. In this study, we explore stochastic compositional optimization in the smooth non-convex setting. To address this challenge, we introduce a novel algorithm called SARAH-Compositional, which utilizes the concept of Stochastic Recursive Gradient Descent. We also demonstrate a sharp Incremental First-order Oracle (IFO) complexity upper bound for stochastic compositional optimization, with a complexity of $\mathcal{O}((n+m)^{1/2} \varepsilon^{-2})$ in the finite-sum case and $\mathcal{O}(\varepsilon^{-3})$ in the online case. This complexity is the best known IFO complexity result for non-convex stochastic compositional optimization and is believed to be optimal. Our experimental results confirm the effectiveness of our algorithm in practice.",1
"A variety of cooperative multi-agent control problems require agents to achieve individual goals while contributing to collective success. This multi-goal multi-agent setting poses difficulties for recent algorithms, which primarily target settings with a single global reward, due to two new challenges: efficient exploration for learning both individual goal attainment and cooperation for others' success, and credit-assignment for interactions between actions and goals of different agents. To address both challenges, we restructure the problem into a novel two-stage curriculum, in which single-agent goal attainment is learned prior to learning multi-agent cooperation, and we derive a new multi-goal multi-agent policy gradient with a credit function for localized credit assignment. We use a function augmentation scheme to bridge value and policy functions across the curriculum. The complete architecture, called CM3, learns significantly faster than direct adaptations of existing algorithms on three challenging multi-goal multi-agent problems: cooperative navigation in difficult formations, negotiating multi-vehicle lane changes in the SUMO traffic simulator, and strategic cooperation in a Checkers environment.",0
"There are many situations where multiple agents must work together to achieve both individual and collective goals. However, current algorithms struggle with this multi-goal and multi-agent environment because they are designed for single global rewards. This presents two challenges: efficient exploration to learn how to achieve individual goals and cooperate with others, and assigning credit for interactions between different agents' actions and goals. To solve these issues, we propose a novel two-stage approach where agents first learn to achieve their individual goals before learning to cooperate with others. We also introduce a new policy gradient algorithm that incorporates a credit function for localized credit assignment. Our complete architecture, called CM3, outperforms existing algorithms on three challenging multi-goal multi-agent problems: cooperative navigation in difficult formations, multi-vehicle lane changes in the SUMO traffic simulator, and strategic cooperation in a Checkers environment.",1
"This paper presented a deep reinforcement learning method named Double Deep Q-networks to design an end-to-end vision-based adaptive cruise control (ACC) system. A simulation environment of a highway scene was set up in Unity, which is a game engine that provided both physical models of vehicles and feature data for training and testing. Well-designed reward functions associated with the following distance and throttle/brake force were implemented in the reinforcement learning model for both internal combustion engine (ICE) vehicles and electric vehicles (EV) to perform adaptive cruise control. The gap statistics and total energy consumption are evaluated for different vehicle types to explore the relationship between reward functions and powertrain characteristics. Compared with the traditional radar-based ACC systems or human-in-the-loop simulation, the proposed vision-based ACC system can generate either a better gap regulated trajectory or a smoother speed trajectory depending on the preset reward function. The proposed system can be well adaptive to different speed trajectories of the preceding vehicle and operated in real-time.",0
"In this article, a method called Double Deep Q-networks was presented for deep reinforcement learning, which was used to create an adaptive cruise control (ACC) system that relied on vision-based technology. A simulation environment was established using the Unity game engine, providing both physical models of vehicles and feature data for training and testing. Reward functions were implemented to regulate distance and throttle/brake force for both ICE vehicles and EVs. The relationship between reward functions and powertrain characteristics was explored by evaluating gap statistics and total energy consumption for different types of vehicles. The proposed system outperformed traditional radar-based ACC or human-in-the-loop simulation by generating a better gap regulated trajectory or a smoother speed trajectory, depending on the preset reward function. The system can adapt to different speed trajectories of the preceding vehicle and operate in real-time.",1
"Recent successful deep reinforcement learning algorithms, such as Trust Region Policy Optimization (TRPO) or Proximal Policy Optimization (PPO), are fundamentally variations of conservative policy iteration (CPI). These algorithms iterate policy evaluation followed by a softened policy improvement step. As so, they are naturally on-policy. In this paper, we propose to combine (any kind of) soft greediness with Modified Policy Iteration (MPI). The proposed abstract framework applies repeatedly: (i) a partial policy evaluation step that allows off-policy learning and (ii) any softened greedy step. Our contribution can be seen as a new generic tool for the deep reinforcement learning toolbox. As a proof of concept, we instantiate this framework with the PPO greediness. Comparison to the original PPO shows that our algorithm is much more sample efficient. We also show that it is competitive with the state-of-art off-policy algorithm Soft Actor Critic (SAC).",0
"The latest successful deep reinforcement learning algorithms, such as Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO), are essentially modified versions of conservative policy iteration (CPI). These algorithms follow a cycle of policy evaluation and a softened policy improvement step, making them naturally on-policy. In this article, we suggest combining any form of soft greediness with Modified Policy Iteration (MPI). This abstract framework involves (i) carrying out a partial policy evaluation step to allow for off-policy learning and (ii) any softened greedy step. Our contribution can be regarded as a new, generic tool for the deep reinforcement learning toolbox. As a proof of concept, we apply this framework with the PPO greediness. Our algorithm is shown to be more sample efficient than the original PPO, and we demonstrate that it is comparable to the state-of-the-art off-policy algorithm, Soft Actor Critic (SAC).",1
"Interactive Fiction games are text-based simulations in which an agent interacts with the world purely through natural language. They are ideal environments for studying how to extend reinforcement learning agents to meet the challenges of natural language understanding, partial observability, and action generation in combinatorially-large text-based action spaces. We present KG-A2C, an agent that builds a dynamic knowledge graph while exploring and generates actions using a template-based action space. We contend that the dual uses of the knowledge graph to reason about game state and to constrain natural language generation are the keys to scalable exploration of combinatorially large natural language actions. Results across a wide variety of IF games show that KG-A2C outperforms current IF agents despite the exponential increase in action space size.",0
"Text-based simulations called Interactive Fiction games allow an agent to interact with a world solely through natural language. They present an ideal environment to explore how reinforcement learning agents can be expanded to tackle challenges such as natural language understanding, partial observability, and action generation within vast text-based action spaces. Our research introduces KG-A2C, an agent that constructs a dynamic knowledge graph while exploring and utilizes a template-based action space to generate actions. We propose that the dual role of the knowledge graph in reasoning about game state and constraining natural language generation is essential for scaling exploration of massive natural language actions. Our experiments across a range of IF games demonstrate that KG-A2C surpasses current IF agents, despite the exponential increase in the size of the action space.",1
"We revisit residual algorithms in both model-free and model-based reinforcement learning settings. We propose the bidirectional target network technique to stabilize residual algorithms, yielding a residual version of DDPG that significantly outperforms vanilla DDPG in the DeepMind Control Suite benchmark. Moreover, we find the residual algorithm an effective approach to the distribution mismatch problem in model-based planning. Compared with the existing TD($k$) method, our residual-based method makes weaker assumptions about the model and yields a greater performance boost.",0
"Our study examines residual algorithms in model-free and model-based reinforcement learning scenarios. To stabilize these algorithms, we suggest using the bidirectional target network technique. This approach results in a residual version of DDPG that surpasses vanilla DDPG in the DeepMind Control Suite benchmark. Additionally, we discover that the residual algorithm is an effective solution to the distribution mismatch issue in model-based planning. Compared to the TD($k$) method, our residual-based technique relies on fewer model assumptions and provides a more significant performance improvement.",1
"Achieving faster execution with shorter compilation time can foster further diversity and innovation in neural networks. However, the current paradigm of executing neural networks either relies on hand-optimized libraries, traditional compilation heuristics, or very recently genetic algorithms and other stochastic methods. These methods suffer from frequent costly hardware measurements rendering them not only too time consuming but also suboptimal. As such, we devise a solution that can learn to quickly adapt to a previously unseen design space for code optimization, both accelerating the search and improving the output performance. This solution dubbed Chameleon leverages reinforcement learning whose solution takes fewer steps to converge, and develops an adaptive sampling algorithm that not only focuses on the costly samples (real hardware measurements) on representative points but also uses a domain-knowledge inspired logic to improve the samples itself. Experimentation with real hardware shows that Chameleon provides 4.45x speed up in optimization time over AutoTVM, while also improving inference time of the modern deep networks by 5.6%.",0
"Encouraging greater diversity and innovation in neural networks can be achieved by enhancing their execution speed and reducing compilation time. However, the current approach to executing neural networks relies on hand-optimized libraries, traditional compilation heuristics, or stochastic methods such as genetic algorithms. These methods often require costly hardware measurements, making them both time-consuming and suboptimal. To address this, we have developed a solution called Chameleon that utilizes reinforcement learning to quickly adapt to new code optimization designs. Chameleon also includes an adaptive sampling algorithm that focuses on representative points while using domain-knowledge logic to enhance the sample quality. Real hardware experimentation has demonstrated that Chameleon achieves a 4.45x speedup in optimization time compared to AutoTVM and improves the inference time of modern deep networks by 5.6%.",1
"As reinforcement learning agents are tasked with solving more challenging and diverse tasks, the ability to incorporate prior knowledge into the learning system and to exploit reusable structure in solution space is likely to become increasingly important. The KL-regularized expected reward objective constitutes one possible tool to this end. It introduces an additional component, a default or prior behavior, which can be learned alongside the policy and as such partially transforms the reinforcement learning problem into one of behavior modelling. In this work we consider the implications of this framework in cases where both the policy and default behavior are augmented with latent variables. We discuss how the resulting hierarchical structures can be used to implement different inductive biases and how their modularity can benefit transfer. Empirically we find that they can lead to faster learning and transfer on a range of continuous control tasks.",0
"As reinforcement learning agents face more difficult and varied challenges, the ability to incorporate previous knowledge into the learning system and take advantage of reusable structures in solution space will become increasingly crucial. The KL-regularized expected reward objective is a possible tool for achieving this goal. It adds a default or prior behavior, which can be learned alongside the policy, and thus partially transforms the reinforcement learning problem into one of behavior modeling. This paper explores the implications of this framework when both the policy and default behavior are augmented with latent variables. We examine how the resulting hierarchical structures can be utilized to implement distinct inductive biases and how their modularity can aid in transfer. From our experiments, we discovered that such structures can facilitate faster learning and transfer for a variety of continuous control tasks.",1
"The practical usage of reinforcement learning agents is often bottlenecked by the duration of training time. To accelerate training, practitioners often turn to distributed reinforcement learning architectures to parallelize and accelerate the training process. However, modern methods for scalable reinforcement learning (RL) often tradeoff between the throughput of samples that an RL agent can learn from (sample throughput) and the quality of learning from each sample (sample efficiency). In these scalable RL architectures, as one increases sample throughput (i.e. increasing parallelization in IMPALA), sample efficiency drops significantly. To address this, we propose a new distributed reinforcement learning algorithm, IMPACT. IMPACT extends IMPALA with three changes: a target network for stabilizing the surrogate objective, a circular buffer, and truncated importance sampling. In discrete action-space environments, we show that IMPACT attains higher reward and, simultaneously, achieves up to 30% decrease in training wall-time than that of IMPALA. For continuous control environments, IMPACT trains faster than existing scalable agents while preserving the sample efficiency of synchronous PPO.",0
"The effectiveness of reinforcement learning agents is frequently hindered by the length of time required for training. In order to speed up the process, experts often implement distributed reinforcement learning architectures to parallelize and hasten training. Nevertheless, current methods for scalable reinforcement learning often necessitate a choice between the amount of samples that an RL agent is capable of learning from (sample throughput) and the quality of learning from each sample (sample efficiency). In these scalable RL architectures, increasing parallelization in IMPALA leads to a substantial decrease in sample efficiency. To tackle this challenge, we propose a novel distributed reinforcement learning algorithm called IMPACT. IMPACT augments IMPALA with three modifications: a target network to stabilize the surrogate objective, a circular buffer, and truncated importance sampling. In environments with discrete action-spaces, our tests indicate that IMPACT attains greater rewards while concurrently accomplishing up to a 30% reduction in training time in comparison to IMPALA. In terms of continuous control environments, IMPACT trains quicker than other scalable agents while upholding the sample efficiency of synchronous PPO.",1
"Applying Q-learning to high-dimensional or continuous action spaces can be difficult due to the required maximization over the set of possible actions. Motivated by techniques from amortized inference, we replace the expensive maximization over all actions with a maximization over a small subset of possible actions sampled from a learned proposal distribution. The resulting approach, which we dub Amortized Q-learning (AQL), is able to handle discrete, continuous, or hybrid action spaces while maintaining the benefits of Q-learning. Our experiments on continuous control tasks with up to 21 dimensional actions show that AQL outperforms D3PG (Barth-Maron et al, 2018) and QT-Opt (Kalashnikov et al, 2018). Experiments on structured discrete action spaces demonstrate that AQL can efficiently learn good policies in spaces with thousands of discrete actions.",0
"The challenge of implementing Q-learning in high-dimensional or continuous action spaces lies in the need to maximize the set of possible actions. To address this issue, we have employed the principles of amortized inference to replace the costly maximization process with a smaller subset of actions sampled from a learned proposal distribution. This approach, called Amortized Q-learning (AQL), is effective in managing discrete, continuous, or hybrid action spaces while retaining the benefits of Q-learning. Our experiments on tasks involving up to 21 dimensional actions indicate that AQL surpasses the performance of D3PG (Barth-Maron et al, 2018) and QT-Opt (Kalashnikov et al, 2018). Moreover, our tests on structured discrete action spaces demonstrate that AQL can efficiently learn proficient policies in spaces with thousands of discrete actions.",1
"The goal of this paper is to present a method for simultaneous trajectory and local stabilizing policy optimization to generate local policies for trajectory-centric model-based reinforcement learning (MBRL). This is motivated by the fact that global policy optimization for non-linear systems could be a very challenging problem both algorithmically and numerically. However, a lot of robotic manipulation tasks are trajectory-centric, and thus do not require a global model or policy. Due to inaccuracies in the learned model estimates, an open-loop trajectory optimization process mostly results in very poor performance when used on the real system. Motivated by these problems, we try to formulate the problem of trajectory optimization and local policy synthesis as a single optimization problem. It is then solved simultaneously as an instance of nonlinear programming. We provide some results for analysis as well as achieved performance of the proposed technique under some simplifying assumptions.",0
"The aim of this paper is to propose a method that combines trajectory and local stabilizing policy optimization to create local policies for model-based reinforcement learning (MBRL) that focuses on trajectories. This approach is inspired by the difficulty of global policy optimization for non-linear systems, which is challenging both algorithmically and numerically. However, as many robotic manipulation tasks are trajectory-centric, a global model or policy is not always necessary. Unfortunately, open-loop trajectory optimization often leads to poor performance due to inaccurate model estimates. To address these issues, we present a method that formulates trajectory optimization and local policy synthesis as a joint optimization problem, which is solved simultaneously using nonlinear programming. We provide results and performance analysis of this technique under simplifying assumptions.",1
"Cooperative Multi-agent Reinforcement Learning (MARL) is crucial for cooperative decentralized decision learning in many domains such as search and rescue, drone surveillance, package delivery and fire fighting problems. In these domains, a key challenge is learning with a few good experiences, i.e., positive reinforcements are obtained only in a few situations (e.g., on extinguishing a fire or tracking a crime or delivering a package) and in most other situations there is zero or negative reinforcement. Learning decisions with a few good experiences is extremely challenging in cooperative MARL problems due to three reasons. First, compared to the single agent case, exploration is harder as multiple agents have to be coordinated to receive a good experience. Second, environment is not stationary as all the agents are learning at the same time (and hence change policies). Third, scale of problem increases significantly with every additional agent.   Relevant existing work is extensive and has focussed on dealing with a few good experiences in single-agent RL problems or on scalable approaches for handling non-stationarity in MARL problems. Unfortunately, neither of these approaches (or their extensions) are able to address the problem of sparse good experiences effectively. Therefore, we provide a novel fictitious self imitation approach that is able to simultaneously handle non-stationarity and sparse good experiences in a scalable manner. Finally, we provide a thorough comparison (experimental or descriptive) against relevant cooperative MARL algorithms to demonstrate the utility of our approach.",0
"Cooperative Multi-agent Reinforcement Learning (MARL) plays a vital role in decentralized decision making across various domains, including search and rescue, drone surveillance, package delivery, and fire fighting. However, these domains face the challenge of learning from a limited number of positive reinforcements, while most situations yield zero or negative reinforcement. This poses a significant challenge in cooperative MARL problems for three reasons. Firstly, coordinating multiple agents for exploration is difficult. Secondly, the environment is non-stationary, as all agents learn simultaneously. Thirdly, the scale of the problem increases with each additional agent. While existing work has focused on handling non-stationarity or a few good experiences in single-agent RL problems or scalable MARL approaches, they fail to address the problem of scarce positive reinforcements effectively. Hence, we propose a new approach called fictitious self-imitation that can handle non-stationarity and sparse good experiences in a scalable manner. Finally, we present a comprehensive comparison of our approach with relevant cooperative MARL algorithms to demonstrate its effectiveness.",1
"Reinforcement learning algorithms use correlations between policies and rewards to improve agent performance. But in dynamic or sparsely rewarding environments these correlations are often too small, or rewarding events are too infrequent to make learning feasible. Human education instead relies on curricula--the breakdown of tasks into simpler, static challenges with dense rewards--to build up to complex behaviors. While curricula are also useful for artificial agents, hand-crafting them is time consuming. This has lead researchers to explore automatic curriculum generation. Here we explore automatic curriculum generation in rich, dynamic environments. Using a setter-solver paradigm we show the importance of considering goal validity, goal feasibility, and goal coverage to construct useful curricula. We demonstrate the success of our approach in rich but sparsely rewarding 2D and 3D environments, where an agent is tasked to achieve a single goal selected from a set of possible goals that varies between episodes, and identify challenges for future work. Finally, we demonstrate the value of a novel technique that guides agents towards a desired goal distribution. Altogether, these results represent a substantial step towards applying automatic task curricula to learn complex, otherwise unlearnable goals, and to our knowledge are the first to demonstrate automated curriculum generation for goal-conditioned agents in environments where the possible goals vary between episodes.",0
"Reinforcement learning algorithms aim to enhance the performance of agents by utilizing the relationship between policies and rewards. However, in dynamic or sparsely rewarding environments, these associations are typically too weak, or rewarding events occur too infrequently, making learning difficult. In contrast, human education utilizes curricula, which divide tasks into simpler, static challenges with dense rewards, to develop complex behaviors. Although curricula are also useful for artificial agents, creating them manually is a time-consuming process. Consequently, researchers have explored automatic curriculum generation. In this study, we investigate automatic curriculum generation in rich, dynamic environments using a setter-solver approach. Our research emphasizes the importance of considering goal validity, goal feasibility, and goal coverage to create effective curricula. We demonstrate the effectiveness of our method in rich but sparsely rewarding 2D and 3D environments, where an agent is assigned a single goal chosen from a set of possible goals that varies between episodes, and we identify areas for future research. Furthermore, we introduce a new technique that guides agents towards a desired goal distribution, which adds value to our approach. In summary, our findings represent a significant advancement in the use of automatic task curricula to learn complex, otherwise unattainable goals. Additionally, our study is the first to demonstrate automatic curriculum generation for goal-conditioned agents in environments where the possible goals vary between episodes.",1
"We propose a new perspective on adversarial attacks against deep reinforcement learning agents. Our main contribution is CopyCAT, a targeted attack able to consistently lure an agent into following an outsider's policy. It is pre-computed, therefore fast inferred, and could thus be usable in a real-time scenario. We show its effectiveness on Atari 2600 games in the novel read-only setting. In this setting, the adversary cannot directly modify the agent's state -- its representation of the environment -- but can only attack the agent's observation -- its perception of the environment. Directly modifying the agent's state would require a write-access to the agent's inner workings and we argue that this assumption is too strong in realistic settings.",0
"Our perspective on adversarial attacks against deep reinforcement learning agents introduces CopyCAT as our main contribution. This targeted attack consistently lures an agent into following an outsider's policy and is pre-computed, allowing for fast inference and potential use in real-time scenarios. We demonstrate its effectiveness through Atari 2600 games in the read-only setting, where the adversary can only attack the agent's observation but not directly modify its state. We argue that the assumption of having write-access to the agent's inner workings is too strong for realistic settings.",1
"Centralised training with decentralised execution is an important setting for cooperative deep multi-agent reinforcement learning due to communication constraints during execution and computational tractability in training. In this paper, we analyse value-based methods that are known to have superior performance in complex environments [43]. We specifically focus on QMIX [40], the current state-of-the-art in this domain. We show that the representational constraints on the joint action-values introduced by QMIX and similar methods lead to provably poor exploration and suboptimality. Furthermore, we propose a novel approach called MAVEN that hybridises value and policy-based methods by introducing a latent space for hierarchical control. The value-based agents condition their behaviour on the shared latent variable controlled by a hierarchical policy. This allows MAVEN to achieve committed, temporally extended exploration, which is key to solving complex multi-agent tasks. Our experimental results show that MAVEN achieves significant performance improvements on the challenging SMAC domain [43].",0
"Due to communication limitations during execution and computational feasibility during training, centralised training with decentralised execution is a crucial environment for cooperative deep multi-agent reinforcement learning. In this study, we examine value-based techniques renowned for their superior performance in intricate situations [43]. We concentrate specifically on QMIX [40], which is presently the leading method in this field. We demonstrate that QMIX and comparable methods' representational restrictions on joint action-values lead to inadequate exploration and suboptimality. Moreover, we suggest a new strategy called MAVEN that blends value and policy-based methods by introducing a latent space for hierarchical control. The value-based agents rely on the shared latent variable regulated by a hierarchical policy to govern their behavior. This enables MAVEN to achieve committed, temporally extended exploration, which is crucial for solving complicated multi-agent tasks. Our experimental findings reveal that MAVEN produces significant performance improvements in the challenging SMAC domain [43].",1
"The Markov decision process (MDP) formulation used to model many real-world sequential decision making problems does not efficiently capture the setting where the set of available decisions (actions) at each time step is stochastic. Recently, the stochastic action set Markov decision process (SAS-MDP) formulation has been proposed, which better captures the concept of a stochastic action set. In this paper we argue that existing RL algorithms for SAS-MDPs can suffer from potential divergence issues, and present new policy gradient algorithms for SAS-MDPs that incorporate variance reduction techniques unique to this setting, and provide conditions for their convergence. We conclude with experiments that demonstrate the practicality of our approaches on tasks inspired by real-life use cases wherein the action set is stochastic.",0
"Sequential decision-making problems in the real world are often modeled using the Markov decision process (MDP) formulation. However, it does not accurately capture situations where the available decisions (actions) at each time step are stochastic. To address this, a new formulation called stochastic action set Markov decision process (SAS-MDP) has been introduced. While existing reinforcement learning (RL) algorithms for SAS-MDPs may encounter divergence issues, this paper presents new policy gradient algorithms that incorporate variance reduction techniques specific to this setting. The study also includes conditions for their convergence and experiments that demonstrate the practicality of the proposed approaches in real-life situations where the action set is stochastic.",1
"We present Decentralized Distributed Proximal Policy Optimization (DD-PPO), a method for distributed reinforcement learning in resource-intensive simulated environments. DD-PPO is distributed (uses multiple machines), decentralized (lacks a centralized server), and synchronous (no computation is ever stale), making it conceptually simple and easy to implement. In our experiments on training virtual robots to navigate in Habitat-Sim, DD-PPO exhibits near-linear scaling -- achieving a speedup of 107x on 128 GPUs over a serial implementation. We leverage this scaling to train an agent for 2.5 Billion steps of experience (the equivalent of 80 years of human experience) -- over 6 months of GPU-time training in under 3 days of wall-clock time with 64 GPUs.   This massive-scale training not only sets the state of art on Habitat Autonomous Navigation Challenge 2019, but essentially solves the task --near-perfect autonomous navigation in an unseen environment without access to a map, directly from an RGB-D camera and a GPS+Compass sensor. Fortuitously, error vs computation exhibits a power-law-like distribution; thus, 90% of peak performance is obtained relatively early (at 100 million steps) and relatively cheaply (under 1 day with 8 GPUs). Finally, we show that the scene understanding and navigation policies learned can be transferred to other navigation tasks -- the analog of ImageNet pre-training + task-specific fine-tuning for embodied AI. Our model outperforms ImageNet pre-trained CNNs on these transfer tasks and can serve as a universal resource (all models and code are publicly available).",0
"DD-PPO is a decentralized, distributed, and synchronous method for reinforcement learning in resource-intensive simulated environments. This approach is easy to implement and exhibits near-linear scaling, achieving a remarkable speedup of 107x on 128 GPUs over a serial implementation. With this scalability, we trained an agent for 2.5 Billion steps of experience, equivalent to 80 years of human experience, in just under 3 days of wall-clock time with 64 GPUs. This training achieved near-perfect autonomous navigation in an unseen environment without access to a map, using an RGB-D camera and a GPS+Compass sensor. Furthermore, we found that 90% of peak performance can be obtained relatively early and cheaply, within 100 million steps and under 1 day with 8 GPUs. We also demonstrated that our scene understanding and navigation policies can be transferred to other navigation tasks, outperforming ImageNet pre-trained CNNs on these transfer tasks. All models and code are publicly available.",1
"Balancing exploration and exploitation remains a key challenge in reinforcement learning (RL). State-of-the-art RL algorithms suffer from high sample complexity, particularly in the sparse reward case, where they can do no better than to explore in all directions until the first positive rewards are found. To mitigate this, we propose Rapidly Randomly-exploring Reinforcement Learning (R3L). We formulate exploration as a search problem and leverage widely-used planning algorithms such as Rapidly-exploring Random Tree (RRT) to find initial solutions. These solutions are used as demonstrations to initialize a policy, then refined by a generic RL algorithm, leading to faster and more stable convergence. We provide theoretical guarantees of R3L exploration finding successful solutions, as well as bounds for its sampling complexity. We experimentally demonstrate the method outperforms classic and intrinsic exploration techniques, requiring only a fraction of exploration samples and achieving better asymptotic performance.",0
"The challenge of balancing exploration and exploitation is still a major issue in reinforcement learning (RL). Current RL algorithms have a high sample complexity, particularly in cases where rewards are sparse, causing exploration to continue in all directions until positive rewards are found. To address this problem, we introduce Rapidly Randomly-exploring Reinforcement Learning (R3L). We present exploration as a search problem and use popular planning algorithms like Rapidly-exploring Random Tree (RRT) to find initial solutions. These solutions are then utilized as demonstrations to initiate a policy which is then refined by a generic RL algorithm, resulting in faster and more reliable convergence. We offer theoretical assurances that R3L exploration can find successful solutions, as well as boundaries for its sampling complexity. Our experiments show that our approach outperforms traditional and intrinsic exploration methods, requiring just a fraction of exploration samples while achieving better asymptotic performance.",1
"Generative adversarial networks (GANs) are a hot research topic recently. GANs have been widely studied since 2014, and a large number of algorithms have been proposed. However, there is few comprehensive study explaining the connections among different GANs variants, and how they have evolved. In this paper, we attempt to provide a review on various GANs methods from the perspectives of algorithms, theory, and applications. Firstly, the motivations, mathematical representations, and structure of most GANs algorithms are introduced in details. Furthermore, GANs have been combined with other machine learning algorithms for specific applications, such as semi-supervised learning, transfer learning, and reinforcement learning. This paper compares the commonalities and differences of these GANs methods. Secondly, theoretical issues related to GANs are investigated. Thirdly, typical applications of GANs in image processing and computer vision, natural language processing, music, speech and audio, medical field, and data science are illustrated. Finally, the future open research problems for GANs are pointed out.",0
"Recently, Generative Adversarial Networks (GANs) have become a popular area of research. Since 2014, many GAN algorithms have been proposed and studied. However, there is a lack of a comprehensive overview of how these different GAN variants are connected and have evolved. This paper aims to provide a review of various GAN methods from different perspectives, including algorithms, theory, and applications. The paper begins with a detailed introduction of the motivations, mathematical representations, and structures of most GAN algorithms. It also explores how GANs have been combined with other machine learning algorithms for specific applications, such as semi-supervised learning, transfer learning, and reinforcement learning. The paper compares the similarities and differences of these GAN methods. Theoretical issues related to GANs are also investigated. Additionally, the paper illustrates typical applications of GANs in image processing and computer vision, natural language processing, music, speech and audio, medical field, and data science. Finally, the paper highlights future open research problems for GANs.",1
"We present a method for learning options from segmented demonstration trajectories. The trajectories are first segmented into skills using nonparametric Bayesian clustering and a reward function for each segment is then learned using inverse reinforcement learning. From this, a set of inferred trajectories for the demonstration are generated. Option initiation sets and termination conditions are learned from these trajectories using the one-class support vector machine clustering algorithm. We demonstrate our method in the four rooms domain, where an agent is able to autonomously discover usable options from human demonstration. Our results show that these inferred options can then be used to improve learning and planning.",0
"Our approach involves acquiring knowledge of options from segmented demonstration trajectories. Initially, the trajectories are divided into skills using a nonparametric Bayesian clustering method, and subsequently, an incentive function is learned for each segment via inverse reinforcement learning. This helps in creating a series of assumed trajectories for the demonstration. To determine option initiation sets and termination conditions, we utilize the one-class support vector machine clustering algorithm on these trajectories. We implemented this technique in the four rooms domain, where an agent could learn functional options from human demonstration. Our results demonstrate that these derived options can enhance the learning and planning process.",1
"Actor critic methods with sparse rewards in model-based deep reinforcement learning typically require a deterministic binary reward function that reflects only two possible outcomes: if, for each step, the goal has been achieved or not. Our hypothesis is that we can influence an agent to learn faster by applying an external environmental pressure during training, which adversely impacts its ability to get higher rewards. As such, we deviate from the classical paradigm of sparse rewards and add a uniformly sampled reward value to the baseline reward to show that (1) sample efficiency of the training process can be correlated to the adversity experienced during training, (2) it is possible to achieve higher performance in less time and with less resources, (3) we can reduce the performance variability experienced seed over seed, (4) there is a maximum point after which more pressure will not generate better results, and (5) that random positive incentives have an adverse effect when using a negative reward strategy, making an agent under those conditions learn poorly and more slowly. These results have been shown to be valid for Deep Deterministic Policy Gradients using Hindsight Experience Replay in a well known Mujoco environment, but we argue that they could be generalized to other methods and environments as well.",0
"In deep reinforcement learning, actor critic methods that involve sparse rewards and a model-based approach often rely on a binary reward function that indicates whether the goal has been achieved or not at each step. Our theory is that applying external pressure during training that hinders an agent's ability to obtain higher rewards can actually accelerate the learning process. This approach deviates from the traditional paradigm of sparse rewards by adding a uniformly sampled reward value to the baseline reward. Our research has demonstrated that this method can lead to several benefits, including improved sample efficiency, quicker and more resource-efficient performance, reduced performance variability, and a maximum pressure threshold beyond which further pressure does not yield better results. However, we also found that random positive incentives can be counterproductive when using a negative reward strategy, leading to slower and less effective learning. While our experiments were conducted using Deep Deterministic Policy Gradients and Hindsight Experience Replay in the Mujoco environment, we believe that these findings could be applicable to other methods and environments as well.",1
"Temporally language grounding in untrimmed videos is a newly-raised task in video understanding. Most of the existing methods suffer from inferior efficiency, lacking interpretability, and deviating from the human perception mechanism. Inspired by human's coarse-to-fine decision-making paradigm, we formulate a novel Tree-Structured Policy based Progressive Reinforcement Learning (TSP-PRL) framework to sequentially regulate the temporal boundary by an iterative refinement process. The semantic concepts are explicitly represented as the branches in the policy, which contributes to efficiently decomposing complex policies into an interpretable primitive action. Progressive reinforcement learning provides correct credit assignment via two task-oriented rewards that encourage mutual promotion within the tree-structured policy. We extensively evaluate TSP-PRL on the Charades-STA and ActivityNet datasets, and experimental results show that TSP-PRL achieves competitive performance over existing state-of-the-art methods.",0
"The task of grounding language in untrimmed videos over time is a new challenge in video comprehension. Unfortunately, many current methods are inefficient, lack interpretability, and do not align with human perception. To address this, we created a Tree-Structured Policy based Progressive Reinforcement Learning (TSP-PRL) framework, inspired by the human decision-making process. Our approach sequentially refines temporal boundaries, representing semantic concepts as branches in the policy to efficiently break down complex policies into understandable actions. Our method uses two task-oriented rewards to provide correct credit assignment and encourage mutual promotion within the policy. We evaluated TSP-PRL on Charades-STA and ActivityNet datasets and found that it outperforms existing state-of-the-art methods.",1
"This paper introduces a hybrid algorithm of deep reinforcement learning (RL) and Force-based motion planning (FMP) to solve distributed motion planning problem in dense and dynamic environments. Individually, RL and FMP algorithms each have their own limitations. FMP is not able to produce time-optimal paths and existing RL solutions are not able to produce collision-free paths in dense environments. Therefore, we first tried improving the performance of recent RL approaches by introducing a new reward function that not only eliminates the requirement of a pre supervised learning (SL) step but also decreases the chance of collision in crowded environments. That improved things, but there were still a lot of failure cases. So, we developed a hybrid approach to leverage the simpler FMP approach in stuck, simple and high-risk cases, and continue using RL for normal cases in which FMP can't produce optimal path. Also, we extend GA3C-CADRL algorithm to 3D environment. Simulation results show that the proposed algorithm outperforms both deep RL and FMP algorithms and produces up to 50% more successful scenarios than deep RL and up to 75% less extra time to reach goal than FMP.",0
"The article presents a hybrid algorithm combining deep reinforcement learning (RL) and Force-based motion planning (FMP) to tackle the distributed motion planning challenge in dense and dynamic environments. While both RL and FMP have their own drawbacks, FMP cannot generate time-optimal paths and existing RL approaches cannot ensure collision-free paths in crowded areas. To address these limitations, the authors first improved RL performance by introducing a novel reward function that eliminates the need for a pre supervised learning (SL) stage and reduces the likelihood of collisions. However, this approach still had many failure cases. Therefore, the authors developed a hybrid solution that utilizes FMP in simple, high-risk, and stuck situations while relying on RL in normal cases when FMP cannot produce optimal paths. Moreover, the authors extended the GA3C-CADRL algorithm to 3D environments. According to simulations, the proposed algorithm outperforms both deep RL and FMP approaches, producing up to 50% more successful scenarios than deep RL and up to 75% less extra time to reach the goal than FMP.",1
"Deep reinforcement learning algorithms have shown an impressive ability to learn complex control policies in high-dimensional tasks. However, despite the ever-increasing performance on popular benchmarks, policies learned by deep reinforcement learning algorithms can struggle to generalize when evaluated in remarkably similar environments. In this paper we propose a protocol to evaluate generalization in reinforcement learning through different modes of Atari 2600 games. With that protocol we assess the generalization capabilities of DQN, one of the most traditional deep reinforcement learning algorithms, and we provide evidence suggesting that DQN overspecializes to the training environment. We then comprehensively evaluate the impact of dropout and $\ell_2$ regularization, as well as the impact of reusing learned representations to improve the generalization capabilities of DQN. Despite regularization being largely underutilized in deep reinforcement learning, we show that it can, in fact, help DQN learn more general features. These features can be reused and fine-tuned on similar tasks, considerably improving DQN's sample efficiency.",0
"The ability of deep reinforcement learning algorithms to learn intricate control policies in high-dimensional tasks is impressive. However, even with their exceptional performance on popular benchmarks, the policies learned by these algorithms can face difficulties in generalizing when tested in similar environments. This article proposes a method to evaluate generalization in reinforcement learning by using various modes of Atari 2600 games. The generalization capabilities of DQN, one of the traditional deep reinforcement learning algorithms, are assessed using this protocol, revealing that DQN tends to specialize too much in the training environment. The impact of dropout and $\ell_2$ regularization, as well as the reuse of learned representations to enhance DQN's generalization capabilities, is thoroughly evaluated. Although regularization is not commonly used in deep reinforcement learning, this research demonstrates that it can help DQN learn more general features. These features can be reused and fine-tuned on similar tasks, greatly improving DQN's sample efficiency.",1
"Reinforcement learning is well suited for optimizing policies of recommender systems. Current solutions mostly focus on model-free approaches, which require frequent interactions with the real environment, and thus are expensive in model learning. Offline evaluation methods, such as importance sampling, can alleviate such limitations, but usually request a large amount of logged data and do not work well when the action space is large. In this work, we propose a model-based reinforcement learning solution which models user-agent interaction for offline policy learning via a generative adversarial network. To reduce bias in the learned model and policy, we use a discriminator to evaluate the quality of generated data and scale the generated rewards. Our theoretical analysis and empirical evaluations demonstrate the effectiveness of our solution in learning policies from the offline and generated data.",0
"Reinforcement learning is ideal for improving policies in recommender systems. However, current approaches mainly concentrate on model-free methods that necessitate frequent interactions with the actual environment, leading to high model learning costs. Although offline evaluation methods, such as importance sampling, can address these limitations, they typically require a considerable amount of logged data and are ineffective when dealing with a vast action space. To overcome these challenges, we propose a model-based reinforcement learning approach that employs a generative adversarial network to model user-agent interaction for offline policy learning. To minimize bias in the learned model and policy, we employ a discriminator to evaluate the quality of the generated data and scale the generated rewards. Our theoretical analysis and empirical evaluations demonstrate the effectiveness of our solution in learning policies from offline and generated data.",1
"We introduce a deep neural network based method for solving a class of elliptic partial differential equations. We approximate the solution of the PDE with a deep neural network which is trained under the guidance of a probabilistic representation of the PDE in the spirit of the Feynman-Kac formula. The solution is given by an expectation of a martingale process driven by a Brownian motion. As Brownian walkers explore the domain, the deep neural network is iteratively trained using a form of reinforcement learning. Our method is a 'Derivative-Free Loss Method' since it does not require the explicit calculation of the derivatives of the neural network with respect to the input neurons in order to compute the training loss. The advantages of our method are showcased in a series of test problems: a corner singularity problem, an interface problem, and an application to a chemotaxis population model.",0
"We present a new approach utilizing deep neural networks to tackle a certain type of elliptic partial differential equations. Our method involves an approximation of the PDE solution through a deep neural network. The network is trained through reinforcement learning, with guidance from a probabilistic representation of the PDE, similar to the Feynman-Kac formula. The solution is obtained from the expectation of a Brownian motion driven martingale process. By avoiding the explicit calculation of derivatives of the neural network, our method is a 'Derivative-Free Loss Method'. We demonstrate the effectiveness of our approach through various test problems, including a corner singularity problem, an interface problem, and a chemotaxis population model.",1
"Traffic signal control is an important and challenging real-world problem, which aims to minimize the travel time of vehicles by coordinating their movements at the road intersections. Current traffic signal control systems in use still rely heavily on oversimplified information and rule-based methods, although we now have richer data, more computing power and advanced methods to drive the development of intelligent transportation. With the growing interest in intelligent transportation using machine learning methods like reinforcement learning, this survey covers the widely acknowledged transportation approaches and a comprehensive list of recent literature on reinforcement for traffic signal control. We hope this survey can foster interdisciplinary research on this important topic.",0
"The issue of traffic signal control presents a significant and demanding obstacle that seeks to reduce the duration of vehicle travel by harmonizing their movements at road intersections. Despite having greater data, more computing capabilities and innovative techniques to facilitate the progress of intelligent transportation, current traffic signal control systems continue to rely heavily on simplified data and rule-based procedures. With the accelerating curiosity in machine learning approaches such as reinforcement learning for intelligent transportation, this examination encompasses the generally recognized transportation procedures and an inclusive compilation of recent literature on reinforcement for traffic signal control. Our aspiration is that this study will stimulate interdisciplinary exploration on this vital subject matter.",1
"We show that reinforcement learning agents that learn by surprise (surprisal) get stuck at abrupt environmental transition boundaries because these transitions are difficult to learn. We propose a counter-intuitive solution that we call Mutual Information Minimising Exploration (MIME) where an agent learns a latent representation of the environment without trying to predict the future states. We show that our agent performs significantly better over sharp transition boundaries while matching the performance of surprisal driven agents elsewhere. In particular, we show state-of-the-art performance on difficult learning games such as Gravitar, Montezuma's Revenge and Doom.",0
"Our research reveals that reinforcement learning agents relying on surprisal encounter a problem at sudden shifts in the environment, as they struggle to comprehend these changes. To address this issue, we propose a novel approach called Mutual Information Minimising Exploration (MIME), which involves an agent learning about the environment's latent representation without attempting to predict future states. Our experiments demonstrate that MIME outperforms surprisal-based agents at abrupt transition boundaries, while producing comparable results in other scenarios. Notably, our approach achieves state-of-the-art performance in challenging games like Gravitar, Montezuma's Revenge, and Doom.",1
"Adversarial attacks have exposed a significant security vulnerability in state-of-the-art machine learning models. Among these models include deep reinforcement learning agents. The existing methods for attacking reinforcement learning agents assume the adversary either has access to the target agent's learned parameters or the environment that the agent interacts with. In this work, we propose a new class of threat models, called snooping threat models, that are unique to reinforcement learning. In these snooping threat models, the adversary does not have the ability to interact with the target agent's environment, and can only eavesdrop on the action and reward signals being exchanged between agent and environment. We show that adversaries operating in these highly constrained threat models can still launch devastating attacks against the target agent by training proxy models on related tasks and leveraging the transferability of adversarial examples.",0
"State-of-the-art machine learning models, including deep reinforcement learning agents, have been found to have a significant security vulnerability through adversarial attacks. The current methods of attacking reinforcement learning agents assume that the adversary can access the learned parameters of the target agent or the environment it interacts with. This study proposes a new class of threat models unique to reinforcement learning, called snooping threat models. In these models, the adversary cannot interact with the target agent's environment but only observe the action and reward signals exchanged between agent and environment. Despite these constraints, the study demonstrates that adversaries using these models can still launch devastating attacks against the target agent by training proxy models on related tasks and leveraging the transferability of adversarial examples.",1
"We present a new model-based reinforcement learning algorithm, Cooperative Prioritized Sweeping, for efficient learning in multi-agent Markov decision processes. The algorithm allows for sample-efficient learning on large problems by exploiting a factorization to approximate the value function. Our approach only requires knowledge about the structure of the problem in the form of a dynamic decision network. Using this information, our method learns a model of the environment and performs temporal difference updates which affect multiple joint states and actions at once. Batch updates are additionally performed which efficiently back-propagate knowledge throughout the factored Q-function. Our method outperforms the state-of-the-art algorithm sparse cooperative Q-learning algorithm, both on the well-known SysAdmin benchmark and randomized environments.",0
"A new algorithm called Cooperative Prioritized Sweeping is being introduced for model-based reinforcement learning. This approach is designed to facilitate efficient learning in multi-agent Markov decision processes. The algorithm is able to handle large problems in a sample-efficient manner through the use of a factorization technique that approximates the value function. The only requirement for this approach is knowledge about the problem's structure in the form of a dynamic decision network. Using this information, the algorithm learns an environment model and performs temporal difference updates that impact multiple joint states and actions simultaneously. Batch updates are also performed to efficiently back-propagate knowledge throughout the factored Q-function. This method has been tested on the well-known SysAdmin benchmark and randomized environments, outperforming the current state-of-the-art sparse cooperative Q-learning algorithm.",1
"Intelligent Object manipulation for grasping is a challenging problem for robots. Unlike robots, humans almost immediately know how to manipulate objects for grasping due to learning over the years. A grown woman can grasp objects more skilfully than a child because of learning skills developed over years, the absence of which in the present day robotic grasping compels it to perform well below the human object grasping benchmarks. In this paper we have taken up the challenge of developing learning based pose estimation by decomposing the problem into both position and orientation learning. More specifically, for grasp position estimation, we explore three different methods - a Genetic Algorithm (GA) based optimization method to minimize error between calculated image points and predicted end-effector (EE) position, a regression based method (RM) where collected data points of robot EE and image points have been regressed with a linear model, a PseudoInverse (PI) model which has been formulated in the form of a mapping matrix with robot EE position and image points for several observations. Further for grasp orientation learning, we develop a deep reinforcement learning (DRL) model which we name as Grasp Deep Q-Network (GDQN) and benchmarked our results with Modified VGG16 (MVGG16). Rigorous experimentations show that due to inherent capability of producing very high-quality solutions for optimization problems and search problems, GA based predictor performs much better than the other two models for position estimation. For orientation learning results indicate that off policy learning through GDQN outperforms MVGG16, since GDQN architecture is specially made suitable for the reinforcement learning. Based on our proposed architectures and algorithms, the robot is capable of grasping all rigid body objects having regular shapes.",0
"Robots face a difficult task when it comes to intelligently manipulating objects for grasping. In contrast, humans have acquired this skill over time and can manipulate objects with greater dexterity. Robotic grasping falls short of human benchmarks due to the absence of learning skills. This paper aims to tackle this challenge by developing a learning-based pose estimation method that breaks down the problem into position and orientation learning. Three methods are explored for grasp position estimation: a Genetic Algorithm (GA) optimization method, a regression-based method (RM), and a PseudoInverse (PI) model. For grasp orientation learning, a deep reinforcement learning (DRL) model called Grasp Deep Q-Network (GDQN) is developed and benchmarked against Modified VGG16 (MVGG16). Experimentations demonstrate that the GA-based predictor outperforms the other two models for position estimation due to its ability to produce high-quality solutions for optimization and search problems. Additionally, off-policy learning through GDQN outperforms MVGG16 for orientation learning. Based on these proposed architectures and algorithms, the robot is capable of grasping all rigid body objects with regular shapes.",1
"This work tackles the problem of robust zero-shot planning in non-stationary stochastic environments. We study Markov Decision Processes (MDPs) evolving over time and consider Model-Based Reinforcement Learning algorithms in this setting. We make two hypotheses: 1) the environment evolves continuously with a bounded evolution rate; 2) a current model is known at each decision epoch but not its evolution. Our contribution can be presented in four points. 1) we define a specific class of MDPs that we call Non-Stationary MDPs (NSMDPs). We introduce the notion of regular evolution by making an hypothesis of Lipschitz-Continuity on the transition and reward functions w.r.t. time; 2) we consider a planning agent using the current model of the environment but unaware of its future evolution. This leads us to consider a worst-case method where the environment is seen as an adversarial agent; 3) following this approach, we propose the Risk-Averse Tree-Search (RATS) algorithm, a zero-shot Model-Based method similar to Minimax search; 4) we illustrate the benefits brought by RATS empirically and compare its performance with reference Model-Based algorithms.",0
"The objective of this research is to address the issue of robust zero-shot planning in non-stationary stochastic environments. The study focuses on Markov Decision Processes (MDPs) that evolve over time and uses Model-Based Reinforcement Learning algorithms. The research makes two hypotheses: 1) the environment changes continuously with a limited evolution rate; 2) the model of the environment is known at each decision epoch but not its evolution. The contribution of the study can be summarized in four points. Firstly, the research introduces a class of MDPs that are called Non-Stationary MDPs (NSMDPs) and proposes the notion of regular evolution by hypothesizing Lipschitz-Continuity on the transition and reward functions with respect to time. Secondly, the research considers a planning agent that uses the current model of the environment but is unaware of its future evolution. Therefore, the research proposes a worst-case method where the environment is seen as an adversarial agent. Thirdly, the research presents the Risk-Averse Tree-Search (RATS) algorithm, which is a zero-shot Model-Based method similar to Minimax search. Finally, the research demonstrates the effectiveness of RATS empirically and compares its performance with reference Model-Based algorithms.",1
"In this paper, a novel racing environment for OpenAI Gym is introduced. This environment operates with continuous action- and state-spaces and requires agents to learn to control the acceleration and steering of a car while navigating a randomly generated racetrack. Different versions of two actor-critic learning algorithms are tested on this environment: Sampled Policy Gradient (SPG) and Proximal Policy Optimization (PPO). An extension of SPG is introduced that aims to improve learning performance by weighting action samples during the policy update step. The effect of using experience replay (ER) is also investigated. To this end, a modification to PPO is introduced that allows for training using old action samples by optimizing the actor in log space. Finally, a new technique for performing ER is tested that aims to improve learning speed without sacrificing performance by splitting the training into two parts, whereby networks are first trained using state transitions from the replay buffer, and then using only recent experiences. The results indicate that experience replay is not beneficial to PPO in continuous action spaces. The training of SPG seems to be more stable when actions are weighted. All versions of SPG outperform PPO when ER is used. The ER trick is effective at improving training speed on a computationally less intensive version of SPG.",0
"This paper presents a new racing environment for OpenAI Gym that involves controlling a car's acceleration and steering while navigating a randomly generated racetrack. The environment operates with continuous action- and state-spaces, and two actor-critic learning algorithms, Sampled Policy Gradient (SPG) and Proximal Policy Optimization (PPO), are tested on it. An extension to SPG that prioritizes action samples during the policy update is introduced, along with a modification to PPO that allows training using old action samples. Additionally, a new technique for experience replay is tested, which involves splitting the training into two parts. The results show that experience replay is not helpful for PPO in continuous action spaces, and that SPG performs better with weighted actions when using ER. Furthermore, all versions of SPG outperform PPO when ER is employed, and the ER trick is effective in speeding up training on a less computationally intensive version of SPG.",1
"The recent success of deep neural networks (DNNs) for function approximation in reinforcement learning has triggered the development of Deep Reinforcement Learning (DRL) algorithms in various fields, such as robotics, computer games, natural language processing, computer vision, sensing systems, and wireless networking. Unfortunately, DNNs suffer from high computational cost and memory consumption, which limits the use of DRL algorithms in systems with limited hardware resources. In recent years, pruning algorithms have demonstrated considerable success in reducing the redundancy of DNNs in classification tasks. However, existing algorithms suffer from a significant performance reduction in the DRL domain. In this paper, we develop the first effective solution to the performance reduction problem of pruning in the DRL domain, and establish a working algorithm, named Policy Pruning and Shrinking (PoPS), to train DRL models with strong performance while achieving a compact representation of the DNN. The framework is based on a novel iterative policy pruning and shrinking method that leverages the power of transfer learning when training the DRL model. We present an extensive experimental study that demonstrates the strong performance of PoPS using the popular Cartpole, Lunar Lander, Pong, and Pacman environments. Finally, we develop an open source software for the benefit of researchers and developers in related fields.",0
"The success of deep neural networks (DNNs) in function approximation for reinforcement learning has led to the emergence of Deep Reinforcement Learning (DRL) algorithms across various fields, including robotics, computer games, natural language processing, computer vision, sensing systems, and wireless networking. However, DNNs are computationally expensive and require high memory consumption, which limits the use of DRL algorithms in systems with limited hardware resources. Although pruning algorithms have been successful in reducing DNN redundancy in classification tasks, they have not been effective in the DRL domain. In this study, we introduce Policy Pruning and Shrinking (PoPS), the first algorithm to solve the pruning performance reduction issue in the DRL domain. PoPS is a framework based on an iterative policy pruning and shrinking method that takes advantage of transfer learning when training the DRL model. Our extensive experimental study on popular environments such as Cartpole, Lunar Lander, Pong, and Pacman demonstrates the strong performance of PoPS. We have also developed open-source software for researchers and developers in related fields.",1
"In this paper we consider self-supervised representation learning to improve sample efficiency in reinforcement learning (RL). We propose a forward prediction objective for simultaneously learning embeddings of states and action sequences. These embeddings capture the structure of the environment's dynamics, enabling efficient policy learning. We demonstrate that our action embeddings alone improve the sample efficiency and peak performance of model-free RL on control from low-dimensional states. By combining state and action embeddings, we achieve efficient learning of high-quality policies on goal-conditioned continuous control from pixel observations in only 1-2 million environment steps.",0
"This article discusses the usage of self-supervised representation learning to enhance the efficiency of reinforcement learning (RL). The authors suggest a forward prediction objective, which enables the learning of embeddings for both states and action sequences simultaneously. These embeddings effectively capture the environment's dynamics and facilitate policy learning. The authors also show that their action embeddings alone can enhance the sample efficiency and peak performance of model-free RL on low-dimensional state control. By combining state and action embeddings, they achieve efficient learning of high-quality policies on goal-conditioned continuous control from pixel observations within a short span of 1-2 million environment steps.",1
"Proximal policy optimization (PPO) is one of the most successful deep reinforcement-learning methods, achieving state-of-the-art performance across a wide range of challenging tasks. However, its optimization behavior is still far from being fully understood. In this paper, we show that PPO could neither strictly restrict the likelihood ratio as it attempts to do nor enforce a well-defined trust region constraint, which means that it may still suffer from the risk of performance instability. To address this issue, we present an enhanced PPO method, named Truly PPO. Two critical improvements are made in our method: 1) it adopts a new clipping function to support a rollback behavior to restrict the difference between the new policy and the old one; 2) the triggering condition for clipping is replaced with a trust region-based one, such that optimizing the resulted surrogate objective function provides guaranteed monotonic improvement of the ultimate policy performance. It seems, by adhering more truly to making the algorithm proximal - confining the policy within the trust region, the new algorithm improves the original PPO on both sample efficiency and performance.",0
"PPO, a popular deep reinforcement-learning method, has achieved remarkable success in accomplishing challenging tasks. Despite this, its optimization mechanism is not yet fully comprehended. The likelihood ratio restriction and trust region constraint that PPO aims to impose cannot be strictly enforced, which leads to a risk of performance instability. In this study, we introduce an enhanced version of PPO, called Truly PPO, which overcomes this issue. It incorporates two critical improvements: a new clipping function that enables rollback behavior to control the difference between the old and new policies, and a trust region-based triggering condition that guarantees that the surrogate objective function's optimization leads to monotonous improvement in policy performance. By truly adhering to the algorithm's proximal nature and confining the policy within the trust region, the new method enhances both sample efficiency and performance compared to the original PPO.",1
"We propose a novel data augmentation method `GridMask' in this paper. It utilizes information removal to achieve state-of-the-art results in a variety of computer vision tasks. We analyze the requirement of information dropping. Then we show limitation of existing information dropping algorithms and propose our structured method, which is simple and yet very effective. It is based on the deletion of regions of the input image. Our extensive experiments show that our method outperforms the latest AutoAugment, which is way more computationally expensive due to the use of reinforcement learning to find the best policies. On the ImageNet dataset for recognition, COCO2017 object detection, and on Cityscapes dataset for semantic segmentation, our method all notably improves performance over baselines. The extensive experiments manifest the effectiveness and generality of the new method.",0
"In this paper, we introduce a new technique called `GridMask' for data augmentation. By removing certain information from input images, we were able to achieve superior results in various computer vision tasks. We conducted an analysis of the necessity for information removal and found that existing algorithms had limitations. To address this, we propose a structured approach that is both simple and highly effective, involving the deletion of specific regions within the image. Our experiments show that our method performs better than the more computationally expensive AutoAugment, which uses reinforcement learning to determine the best policies. We tested our technique on datasets for recognition, object detection, and semantic segmentation, and it consistently outperformed baselines. These extensive experiments demonstrate the versatility and efficacy of our approach.",1
"Modern deep reinforcement learning methods have departed from the incremental learning required for eligibility traces, rendering the implementation of the $\lambda$-return difficult in this context. In particular, off-policy methods that utilize experience replay remain problematic because their random sampling of minibatches is not conducive to the efficient calculation of $\lambda$-returns. Yet replay-based methods are often the most sample efficient, and incorporating $\lambda$-returns into them is a viable way to achieve new state-of-the-art performance. Towards this, we propose the first method to enable practical use of $\lambda$-returns in arbitrary replay-based methods without relying on other forms of decorrelation such as asynchronous gradient updates. By promoting short sequences of past transitions into a small cache within the replay memory, adjacent $\lambda$-returns can be efficiently precomputed by sharing Q-values. Computation is not wasted on experiences that are never sampled, and stored $\lambda$-returns behave as stable temporal-difference (TD) targets that replace the target network. Additionally, our method grants the unique ability to observe TD errors prior to sampling; for the first time, transitions can be prioritized by their true significance rather than by a proxy to it. Furthermore, we propose the novel use of the TD error to dynamically select $\lambda$-values that facilitate faster learning. We show that these innovations can enhance the performance of DQN when playing Atari 2600 games, even under partial observability. While our work specifically focuses on $\lambda$-returns, these ideas are applicable to any multi-step return estimator.",0
"The latest advancements in deep reinforcement learning have moved away from the gradual learning needed for eligibility traces, making it challenging to implement the $\lambda$-return in this setting. This is particularly problematic for off-policy approaches that utilize experience replay since their random selection of minibatches does not lend itself to the efficient computation of $\lambda$-returns. However, these replay-based methods are often the most efficient in terms of sample usage, and integrating $\lambda$-returns into them is a practical method for achieving new state-of-the-art results. To this end, we present a novel approach that allows for the practical application of $\lambda$-returns in any replay-based method without relying on other methods of decorrelation, such as asynchronous gradient updates. By creating a small cache within the replay memory that contains recent transitions, adjacent $\lambda$-returns can be precomputed efficiently by sharing Q-values. This approach prevents computation from being wasted on experiences that will never be sampled, and stored $\lambda$-returns act as stable temporal-difference (TD) targets that replace the target network. Additionally, our method allows for TD errors to be observed before sampling, enabling transitions to be prioritized based on their true significance rather than a proxy measure. Furthermore, we introduce the use of TD errors to dynamically select $\lambda$-values that facilitate faster learning. Our experiments show that these innovations can improve the performance of the DQN when playing Atari 2600 games, even when the agent has limited observation capabilities. While our focus is on $\lambda$-returns, our ideas can be applied to any multi-step return estimator.",1
"In this work, we consider the popular tree-based search strategy within the framework of reinforcement learning, the Monte Carlo Tree Search (MCTS), in the context of infinite-horizon discounted cost Markov Decision Process (MDP). While MCTS is believed to provide an approximate value function for a given state with enough simulations, the claimed proof in the seminal works is incomplete. This is due to the fact that the variant, the Upper Confidence Bound for Trees (UCT), analyzed in prior works utilizes ""logarithmic"" bonus term for balancing exploration and exploitation within the tree-based search, following the insights from stochastic multi-arm bandit (MAB) literature. In effect, such an approach assumes that the regret of the underlying recursively dependent non-stationary MABs concentrates around their mean exponentially in the number of steps, which is unlikely to hold as pointed out in literature, even for stationary MABs. As the key contribution of this work, we establish polynomial concentration property of regret for a class of non-stationary MAB. This in turn establishes that the MCTS with appropriate polynomial rather than logarithmic bonus term in UCB has the claimed property. Using this as a building block, we argue that MCTS, combined with nearest neighbor supervised learning, acts as a ""policy improvement"" operator: it iteratively improves value function approximation for all states, due to combining with supervised learning, despite evaluating at only finitely many states. In effect, we establish that to learn an $\varepsilon$ approximation of the value function with respect to $\ell_\infty$ norm, MCTS combined with nearest neighbor requires a sample size scaling as $\widetilde{O}\big(\varepsilon^{-(d+4)}\big)$, where $d$ is the dimension of the state space. This is nearly optimal due to a minimax lower bound of $\widetilde{\Omega}\big(\varepsilon^{-(d+2)}\big)$.",0
"This study examines the Monte Carlo Tree Search (MCTS), a popular tree-based search strategy in reinforcement learning, specifically in the context of infinite-horizon discounted cost Markov Decision Process (MDP). Previous works have shown that MCTS can provide an approximate value function for a given state with enough simulations, but the proof is incomplete due to the use of the Upper Confidence Bound for Trees (UCT) variant, which assumes that regret concentrates around the mean exponentially in the number of steps. However, this assumption is unlikely to hold for non-stationary MABs, as pointed out in the literature. The authors establish a polynomial concentration property of regret for a class of non-stationary MABs, which shows that MCTS with an appropriate polynomial bonus term in UCB has the claimed property. They also argue that MCTS, combined with nearest neighbor supervised learning, acts as a ""policy improvement"" operator, improving value function approximation for all states despite evaluating at only finitely many states. The authors establish that to learn an $\varepsilon$ approximation of the value function with respect to $\ell_\infty$ norm, MCTS combined with nearest neighbor requires a sample size scaling as $\widetilde{O}\big(\varepsilon^{-(d+4)}\big)$, which is nearly optimal based on a minimax lower bound of $\widetilde{\Omega}\big(\varepsilon^{-(d+2)}\big)$.",1
"Designing effective architectures is one of the key factors behind the success of deep neural networks. Existing deep architectures are either manually designed or automatically searched by some Neural Architecture Search (NAS) methods. However, even a well-searched architecture may still contain many non-significant or redundant modules or operations (e.g., convolution or pooling), which may not only incur substantial memory consumption and computation cost but also deteriorate the performance. Thus, it is necessary to optimize the operations inside an architecture to improve the performance without introducing extra computation cost. Unfortunately, such a constrained optimization problem is NP-hard. To make the problem feasible, we cast the optimization problem into a Markov decision process (MDP) and seek to learn a Neural Architecture Transformer (NAT) to replace the redundant operations with the more computationally efficient ones (e.g., skip connection or directly removing the connection). Based on MDP, we learn NAT by exploiting reinforcement learning to obtain the optimization policies w.r.t. different architectures. To verify the effectiveness of the proposed strategies, we apply NAT on both hand-crafted architectures and NAS based architectures. Extensive experiments on two benchmark datasets, i.e., CIFAR-10 and ImageNet, demonstrate that the transformed architecture by NAT significantly outperforms both its original form and those architectures optimized by existing methods.",0
"The success of deep neural networks relies heavily on effective architecture design. Deep architectures are typically either manually crafted or auto-discovered through Neural Architecture Search (NAS). However, even well-designed architectures may contain redundant or insignificant modules, leading to high memory consumption, computation costs and poor performance. Therefore, optimizing architecture operations is crucial to improving performance without adding extra costs. This optimization problem is NP-hard. To address this, we use a Markov decision process (MDP) to learn a Neural Architecture Transformer (NAT) that replaces redundant operations with more efficient ones. We apply reinforcement learning to obtain optimization policies for different architectures. We tested NAT on both hand-crafted and NAS-based architectures on CIFAR-10 and ImageNet. Results demonstrate that the transformed architecture significantly outperforms the original forms and those optimized by existing methods.",1
"A reflex is a simple closed loop control approach which tries to minimise an error but fails to do so because it will always react too late. An adaptive algorithm can use this error to learn a forward model with the help of predictive cues. For example a driver learns to improve their steering by looking ahead to avoid steering in the last minute. In order to process complex cues such as the road ahead deep learning is a natural choice. However, this is usually only achieved indirectly by employing deep reinforcement learning having a discrete state space. Here, we show how this can be directly achieved by embedding deep learning into a closed loop system and preserving its continuous processing. We show specifically how error back-propagation can be achieved in z-space and in general how gradient based approaches can be analysed in such closed loop scenarios. The performance of this learning paradigm is demonstrated using a line-follower both in simulation and on a real robot that show very fast and continuous learning.",0
"The reflex mechanism is a control system that seeks to minimize errors but is ineffective because it always reacts too late. On the other hand, an adaptive algorithm can use errors to learn a forward model with the aid of predictive cues. For instance, a driver improves their steering skills by anticipating the road ahead and avoiding last-minute adjustments. When processing complex cues such as the road ahead, deep learning is the most effective choice. However, this is often achieved indirectly by employing deep reinforcement learning with a discrete state space. We demonstrate how this can be directly achieved by incorporating deep learning into a closed loop system while maintaining its continuous processing. We show how error back-propagation can be achieved in z-space and how gradient-based approaches can be analyzed in closed loop scenarios. We demonstrate the effectiveness of this learning method using a line-follower both in simulation and on a real robot, which shows rapid and continuous learning.",1
"Learning optimal policies from sparse feedback is a known challenge in reinforcement learning. Hindsight Experience Replay (HER) is a multi-goal reinforcement learning algorithm that comes to solve such tasks. The algorithm treats every failure as a success for an alternative (virtual) goal that has been achieved in the episode and then generalizes from that virtual goal to real goals. HER has known flaws and is limited to relatively simple tasks. In this thesis, we present three algorithms based on the existing HER algorithm that improves its performances. First, we prioritize virtual goals from which the agent will learn more valuable information. We call this property the \textit{instructiveness} of the virtual goal and define it by a heuristic measure, which expresses how well the agent will be able to generalize from that virtual goal to actual goals. Secondly, we designed a filtering process that detects and removes misleading samples that may induce bias throughout the learning process. Lastly, we enable the learning of complex, sequential, tasks using a form of curriculum learning combined with HER. We call this algorithm \textit{Curriculum HER}. To test our algorithms, we built three challenging manipulation environments with sparse reward functions. Each environment has three levels of complexity. Our empirical results show vast improvement in the final success rate and sample efficiency when compared to the original HER algorithm.",0
"Reinforcement learning faces a well-known hurdle of learning optimal policies from limited feedback. Hindsight Experience Replay (HER) is a multi-goal reinforcement learning technique that aims to overcome such challenges. However, HER has limitations, and it may not be suitable for more intricate tasks. This thesis introduces three algorithms based on HER that enhance its performance. Firstly, we prioritize virtual goals that provide more informative feedback to the agent by defining a heuristic measure called the ""instructiveness"" of the virtual goal. Secondly, we developed a filtering process to remove misleading samples that may create bias during the learning process. Lastly, we combine HER with curriculum learning to enable the agent to learn complex, sequential tasks. This algorithm, named ""Curriculum HER,"" was tested on three challenging manipulation environments with sparse reward functions of varying complexity levels. The results indicate significant improvements in both sample efficiency and final success rate compared to the original HER algorithm.",1
"Generative Adversarial Imitation Learning (GAIL) is a powerful and practical approach for learning sequential decision-making policies. Different from Reinforcement Learning (RL), GAIL takes advantage of demonstration data by experts (e.g., human), and learns both the policy and reward function of the unknown environment. Despite the significant empirical progresses, the theory behind GAIL is still largely unknown. The major difficulty comes from the underlying temporal dependency of the demonstration data and the minimax computational formulation of GAIL without convex-concave structure. To bridge such a gap between theory and practice, this paper investigates the theoretical properties of GAIL. Specifically, we show: (1) For GAIL with general reward parameterization, the generalization can be guaranteed as long as the class of the reward functions is properly controlled; (2) For GAIL, where the reward is parameterized as a reproducing kernel function, GAIL can be efficiently solved by stochastic first order optimization algorithms, which attain sublinear convergence to a stationary solution. To the best of our knowledge, these are the first results on statistical and computational guarantees of imitation learning with reward/policy function approximation. Numerical experiments are provided to support our analysis.",0
"The Generative Adversarial Imitation Learning (GAIL) technique is an effective and practical method for acquiring policies for sequential decision-making. Unlike Reinforcement Learning (RL), GAIL employs demonstration data from experts, such as humans, to learn the policy and reward function of an unknown environment. However, while there have been significant practical advancements, the theoretical foundations of GAIL remain largely unexplored. This is due to the complexity of the demonstration data's temporal dependency and the minimax computational formulation of GAIL, which lacks a convex-concave structure. Our study aims to fill this gap by investigating the theoretical properties of GAIL. Our findings reveal that by controlling the class of reward functions, GAIL can ensure generalization for general reward parameterization. Additionally, GAIL with reward parameterized as a reproducing kernel function can be efficiently solved using stochastic first-order optimization algorithms, which achieve sublinear convergence to a stationary solution. These findings represent the first results on statistical and computational guarantees of imitation learning with reward/policy function approximation. Our analysis is supported by numerical experiments.",1
"We propose a novel hardware and software co-exploration framework for efficient neural architecture search (NAS). Different from existing hardware-aware NAS which assumes a fixed hardware design and explores the neural architecture search space only, our framework simultaneously explores both the architecture search space and the hardware design space to identify the best neural architecture and hardware pairs that maximize both test accuracy and hardware efficiency. Such a practice greatly opens up the design freedom and pushes forward the Pareto frontier between hardware efficiency and test accuracy for better design tradeoffs. The framework iteratively performs a two-level (fast and slow) exploration. Without lengthy training, the fast exploration can effectively fine-tune hyperparameters and prune inferior architectures in terms of hardware specifications, which significantly accelerates the NAS process. Then, the slow exploration trains candidates on a validation set and updates a controller using the reinforcement learning to maximize the expected accuracy together with the hardware efficiency. Experiments on ImageNet show that our co-exploration NAS can find the neural architectures and associated hardware design with the same accuracy, 35.24% higher throughput, 54.05% higher energy efficiency and 136x reduced search time, compared with the state-of-the-art hardware-aware NAS.",0
"Our proposed framework for efficient neural architecture search (NAS) involves both hardware and software co-exploration. Unlike existing hardware-aware NAS methods that only explore the neural architecture search space, our framework explores both the architecture and hardware design spaces concurrently, seeking the best neural architecture and hardware pairs that maximize both test accuracy and hardware efficiency. This approach provides greater design freedom and improves the tradeoff between hardware efficiency and test accuracy. The framework uses a two-level exploration process, with a fast exploration that fine-tunes hyperparameters and prunes inferior architectures based on hardware specifications without lengthy training, and a slow exploration that trains candidates on a validation set. The slow exploration also updates a controller using reinforcement learning to maximize expected accuracy and hardware efficiency. Our co-exploration NAS approach was tested on ImageNet and showed significant improvements in accuracy, throughput, energy efficiency, and search time compared to state-of-the-art hardware-aware NAS methods.",1
"Reinforcement learning usually uses the feedback rewards of environmental to train agents. But the rewards in the actual environment are sparse, and even some environments will not rewards. Most of the current methods are difficult to get good performance in sparse reward or non-reward environments. Although using shaped rewards is effective when solving sparse reward tasks, it is limited to specific problems and learning is also susceptible to local optima. We propose a model-free method that does not rely on environmental rewards to solve the problem of sparse rewards in the general environment. Our method use the minimum number of transitions between states as the distance to replace the rewards of environmental, and proposes a goal-distance gradient to achieve policy improvement. We also introduce a bridge point planning method based on the characteristics of our method to improve exploration efficiency, thereby solving more complex tasks. Experiments show that our method performs better on sparse reward and local optimal problems in complex environments than previous work.",0
"Traditionally, reinforcement learning trains agents using feedback rewards from the environment. However, rewards in the real environment are often sparse, and some environments do not provide rewards at all. This presents a challenge for current methods, which struggle to achieve good performance in such conditions. Although shaped rewards can be effective for solving sparse reward tasks, they are limited in their applicability and are prone to local optima. To address this issue, we propose a model-free method that can solve the problem of sparse rewards in a general environment without relying on environmental rewards. Our method replaces rewards with the minimum number of transitions between states and employs a goal-distance gradient to achieve policy improvement. Additionally, we introduce a bridge point planning method to improve exploration efficiency and solve more complex tasks. Experimental results demonstrate that our method outperforms previous approaches in dealing with sparse reward and local optimal problems in complex environments.",1
"Temporal difference (TD) learning is a popular algorithm for policy evaluation in reinforcement learning, but the vanilla TD can substantially suffer from the inherent optimization variance. A variance reduced TD (VRTD) algorithm was proposed by Korda and La (2015), which applies the variance reduction technique directly to the online TD learning with Markovian samples. In this work, we first point out the technical errors in the analysis of VRTD in Korda and La (2015), and then provide a mathematically solid analysis of the non-asymptotic convergence of VRTD and its variance reduction performance. We show that VRTD is guaranteed to converge to a neighborhood of the fixed-point solution of TD at a linear convergence rate. Furthermore, the variance error (for both i.i.d.\ and Markovian sampling) and the bias error (for Markovian sampling) of VRTD are significantly reduced by the batch size of variance reduction in comparison to those of vanilla TD. As a result, the overall computational complexity of VRTD to attain a given accurate solution outperforms that of TD under Markov sampling and outperforms that of TD under i.i.d.\ sampling for a sufficiently small conditional number.",0
"Temporal difference (TD) learning is a commonly used algorithm for assessing policies in reinforcement learning. However, the original TD can face significant optimization variance. To address this issue, Korda and La (2015) proposed a variance reduced TD (VRTD) algorithm that applies variance reduction directly to online TD learning with Markovian samples. In this study, we identify technical errors in the analysis of VRTD in Korda and La (2015). We then provide a rigorous mathematical analysis of the non-asymptotic convergence of VRTD and its variance reduction performance. Our analysis shows that VRTD converges to a neighborhood of the fixed-point solution of TD at a linear convergence rate. Additionally, VRTD significantly reduces the variance error (for both i.i.d. and Markovian sampling) and the bias error (for Markovian sampling) compared to vanilla TD, as the batch size of variance reduction increases. Consequently, VRTD outperforms TD in terms of overall computational complexity for achieving a given accurate solution under Markov sampling and for a sufficiently small conditional number under i.i.d. sampling.",1
"In this paper, we consider the problem of large scale multi agent reinforcement learning. Firstly, we studied the representation problem of the pairwise value function to reduce the complexity of the interactions among agents. Secondly, we adopt a l2-norm trick to ensure the trivial term of the approximated value function is bounded. Thirdly, experimental results on battle game demonstrate the effectiveness of the proposed approach.",0
"The focus of this paper is on tackling the challenge of multi agent reinforcement learning on a large scale. Our approach involves addressing the issue of complexity in the interactions among agents by examining the pairwise value function's representation. We also implement a l2-norm trick to bound the trivial term of the approximated value function. To validate our method, we conducted experiments on a battle game, which yielded positive results.",1
"Hypotension in critical care settings is a life-threatening emergency that must be recognized and treated early. While fluid bolus therapy and vasopressors are common treatments, it is often unclear which interventions to give, in what amounts, and for how long. Observational data in the form of electronic health records can provide a source for helping inform these choices from past events, but often it is not possible to identify a single best strategy from observational data alone. In such situations, we argue it is important to expose the collection of plausible options to a provider. To this end, we develop SODA-RL: Safely Optimized, Diverse, and Accurate Reinforcement Learning, to identify distinct treatment options that are supported in the data. We demonstrate SODA-RL on a cohort of 10,142 ICU stays where hypotension presented. Our learned policies perform comparably to the observed physician behaviors, while providing different, plausible alternatives for treatment decisions.",0
"Early recognition and treatment of hypotension in critical care settings is essential as it can be life-threatening. Although fluid bolus therapy and vasopressors are commonly used treatments, determining the appropriate interventions, their quantities, and duration is often difficult. Electronic health records containing observational data can help inform treatment choices based on past events, but it may not always be possible to identify the best strategy using this data alone. Therefore, it is crucial to provide healthcare providers with a range of plausible options. In this study, we present SODA-RL, a system that uses Safely Optimized, Diverse, and Accurate Reinforcement Learning to identify distinct and supported treatment options. We applied SODA-RL to a cohort of 10,142 ICU stays where hypotension was observed. Our results show that our learned policies compare well to physician behaviors while also providing different and plausible alternatives for treatment decisions.",1
"Motivated by their broad applications in reinforcement learning, we study the linear two-time-scale stochastic approximation, an iterative method using two different step sizes for finding the solutions of a system of two equations. Our main focus is to characterize the finite-time complexity of this method under time-varying step sizes and Markovian noise. In particular, we show that the mean square errors of the variables generated by the method converge to zero at a sublinear rate $\Ocal(k^{2/3})$, where $k$ is the number of iterations. We then improve the performance of this method by considering the restarting scheme, where we restart the algorithm after every predetermined number of iterations. We show that using this restarting method the complexity of the algorithm under time-varying step sizes is as good as the one using constant step sizes, but still achieving an exact converge to the desired solution. Moreover, the restarting scheme also helps to prevent the step sizes from getting too small, which is useful for the practical implementation of the linear two-time-scale stochastic approximation.",0
"In this study, we examine the linear two-time-scale stochastic approximation method due to its widespread use in reinforcement learning. The method involves using two distinct step sizes to solve a system of equations iteratively. Our objective is to investigate the finite-time complexity of this method in the presence of Markovian noise and time-varying step sizes. Our research shows that the mean square errors of the variables converge to zero at a sublinear rate of $\Ocal(k^{2/3})$ after k iterations. To enhance the method's performance, we introduce a restarting scheme that restarts the algorithm after a set number of iterations, demonstrating that the complexity under time-varying step sizes is comparable to that of constant step sizes. Additionally, the restarting scheme prevents the step sizes from becoming too small, making the linear two-time-scale stochastic approximation more practical.",1
"We review basic concepts of convex duality, focusing on the very general and supremely useful Fenchel-Rockafellar duality. We summarize how this duality may be applied to a variety of reinforcement learning (RL) settings, including policy evaluation or optimization, online or offline learning, and discounted or undiscounted rewards. The derivations yield a number of intriguing results, including the ability to perform policy evaluation and on-policy policy gradient with behavior-agnostic offline data and methods to learn a policy via max-likelihood optimization. Although many of these results have appeared previously in various forms, we provide a unified treatment and perspective on these results, which we hope will enable researchers to better use and apply the tools of convex duality to make further progress in RL.",0
"The article discusses convex duality and its relevance in reinforcement learning (RL), with a focus on the widely applicable Fenchel-Rockafellar duality. It explores how this duality can be utilized in various RL scenarios such as policy evaluation or optimization, online or offline learning, and discounted or undiscounted rewards. The research yields several interesting findings, including the ability to perform policy evaluation and on-policy policy gradient using behavior-agnostic offline data, as well as techniques for learning a policy via max-likelihood optimization. While some of these results have been previously published, the article offers a comprehensive treatment and perspective on the subject matter to facilitate further progress in RL research.",1
"We propose a novel method for fact-checking on knowledge graphs based on debate dynamics. The underlying idea is to frame the task of triple classification as a debate game between two reinforcement learning agents which extract arguments -- paths in the knowledge graph -- with the goal to justify the fact being true (thesis) or the fact being false (antithesis), respectively. Based on these arguments, a binary classifier, referred to as the judge, decides whether the fact is true or false. The two agents can be considered as sparse feature extractors that present interpretable evidence for either the thesis or the antithesis. In contrast to black-box methods, the arguments enable the user to gain an understanding for the decision of the judge. Moreover, our method allows for interactive reasoning on knowledge graphs where the users can raise additional arguments or evaluate the debate taking common sense reasoning and external information into account. Such interactive systems can increase the acceptance of various AI applications based on knowledge graphs and can further lead to higher efficiency, robustness, and fairness.",0
"Our proposed approach for fact-checking on knowledge graphs is innovative and utilizes debate dynamics. Our methodology involves a game between two reinforcement learning agents that extract arguments, or paths in the knowledge graph. Their objective is to justify whether a fact is true (thesis) or false (antithesis). A binary classifier, known as the judge, makes the final decision based on these arguments. These agents act as sparse feature extractors, providing interpretable evidence for either the thesis or antithesis. Unlike black-box methods, our approach provides a clear understanding of the judge's decision-making process. Additionally, our interactive reasoning system allows for users to incorporate external information and common sense reasoning when evaluating the debate or presenting additional arguments. This system can improve the acceptance of various AI applications based on knowledge graphs, while also enhancing efficiency, robustness, and fairness.",1
"MushroomRL is an open-source Python library developed to simplify the process of implementing and running Reinforcement Learning (RL) experiments. Compared to other available libraries, MushroomRL has been created with the purpose of providing a comprehensive and flexible framework to minimize the effort in implementing and testing novel RL methodologies. Indeed, the architecture of MushroomRL is built in such a way that every component of an RL problem is already provided, and most of the time users can only focus on the implementation of their own algorithms and experiments. The result is a library from which RL researchers can significantly benefit in the critical phase of the empirical analysis of their works. MushroomRL stable code, tutorials and documentation can be found at https://github.com/MushroomRL/mushroom-rl.",0
"MushroomRL is a Python library that is available in open-source. It is designed to make the process of conducting Reinforcement Learning (RL) experiments easier. MushroomRL is unique in that it is a comprehensive and flexible framework that reduces the amount of effort required to implement and test new RL methodologies. The MushroomRL architecture provides all the necessary components for an RL problem, allowing users to focus solely on implementing their own algorithms and experiments. As a result, RL researchers can benefit greatly from using this library during the empirical analysis phase of their work. Stable code, tutorials, and documentation for MushroomRL can be accessed at https://github.com/MushroomRL/mushroom-rl.",1
"In this paper, a new population-guided parallel learning scheme is proposed to enhance the performance of off-policy reinforcement learning (RL). In the proposed scheme, multiple identical learners with their own value-functions and policies share a common experience replay buffer, and search a good policy in collaboration with the guidance of the best policy information. The key point is that the information of the best policy is fused in a soft manner by constructing an augmented loss function for policy update to enlarge the overall search region by the multiple learners. The guidance by the previous best policy and the enlarged range enable faster and better policy search. Monotone improvement of the expected cumulative return by the proposed scheme is proved theoretically. Working algorithms are constructed by applying the proposed scheme to the twin delayed deep deterministic (TD3) policy gradient algorithm. Numerical results show that the constructed algorithm outperforms most of the current state-of-the-art RL algorithms, and the gain is significant in the case of sparse reward environment.",0
"This paper suggests a novel approach to improve the performance of off-policy reinforcement learning (RL) by introducing a population-guided parallel learning scheme. The proposed scheme involves multiple identical learners, each with their own value-functions and policies, collaborating to search for a good policy using a common experience replay buffer. The best policy information guides the learners towards a better policy through a soft fusion mechanism, which enlarges the overall search region. The proposed scheme guarantees a monotone improvement of the expected cumulative return, as proved theoretically. The twin delayed deep deterministic (TD3) policy gradient algorithm is used to implement the proposed scheme, and the results indicate that the constructed algorithm outperforms most state-of-the-art RL algorithms, particularly in sparse reward environments.",1
"Value function learning plays a central role in many state-of-the-art reinforcement-learning algorithms. Many popular algorithms like Q-learning do not optimize any objective function, but are fixed-point iterations of some variant of Bellman operator that is not necessarily a contraction. As a result, they may easily lose convergence guarantees, as can be observed in practice. In this paper, we propose a novel loss function, which can be optimized using standard gradient-based methods without risking divergence. The key advantage is that its gradient can be easily approximated using sampled transitions, avoiding the need for double samples required by prior algorithms like residual gradient. Our approach may be combined with general function classes such as neural networks, on either on- or off-policy data, and is shown to work reliably and effectively in several benchmarks.",0
"Many modern reinforcement-learning algorithms rely heavily on value function learning. Although popular algorithms, such as Q-learning, do not optimize any objective function, they are still widely used as fixed-point iterations of a variation of the Bellman operator, which may not be a contraction. This characteristic can lead to a lack of convergence guarantees and practical issues. To address this problem, we propose a new loss function that can be optimized using standard gradient-based methods without the risk of divergence. Our approach has a significant advantage in that it can easily approximate the gradient using sampled transitions, which eliminates the need for double samples, a requirement of previous algorithms like residual gradient. Our technique can be combined with general function classes, including neural networks, and can be applied to both on- and off-policy data. Furthermore, our approach has been proven to be reliable and effective in a variety of benchmarks.",1
"Distributional reinforcement learning (DRL) is a recent reinforcement learning framework whose success has been supported by various empirical studies. It relies on the key idea of replacing the expected return with the return distribution, which captures the intrinsic randomness of the long term rewards. Most of the existing literature on DRL focuses on problems with discrete action space and value based methods. In this work, motivated by applications in robotics with continuous action space control settings, we propose sample-based distributional policy gradient (SDPG) algorithm. It models the return distribution using samples via a reparameterization technique widely used in generative modeling and inference. We compare SDPG with the state-of-art policy gradient method in DRL, distributed distributional deterministic policy gradients (D4PG), which has demonstrated state-of-art performance. We apply SDPG and D4PG to multiple OpenAI Gym environments and observe that our algorithm shows better sample efficiency as well as higher reward for most tasks.",0
"Numerous empirical studies have supported the success of Distributional Reinforcement Learning (DRL), a recent framework for reinforcement learning. DRL replaces the anticipated return with the return distribution, which captures the inherent randomness of long-term rewards. Past research on DRL has primarily focused on value-based methods and problems with discrete action spaces. In this study, we propose the Sample-Based Distributional Policy Gradient (SDPG) algorithm for robotics applications with continuous action space controls. SDPG models the return distribution using samples through a widely used reparameterization technique in generative modeling and inference. We compared SDPG with the Distributed Distributional Deterministic Policy Gradients (D4PG), which has demonstrated state-of-the-art performance in policy gradient methods in DRL. We applied both algorithms to multiple OpenAI Gym environments and found that our algorithm showed better sample efficiency and higher rewards for most tasks.",1
"Empowerment is an information-theoretic method that can be used to intrinsically motivate learning agents. It attempts to maximize an agent's control over the environment by encouraging visiting states with a large number of reachable next states. Empowered learning has been shown to lead to complex behaviors, without requiring an explicit reward signal. In this paper, we investigate the use of empowerment in the presence of an extrinsic reward signal. We hypothesize that empowerment can guide reinforcement learning (RL) agents to find good early behavioral solutions by encouraging highly empowered states. We propose a unified Bellman optimality principle for empowered reward maximization. Our empowered reward maximization approach generalizes both Bellman's optimality principle as well as recent information-theoretical extensions to it. We prove uniqueness of the empowered values and show convergence to the optimal solution. We then apply this idea to develop off-policy actor-critic RL algorithms which we validate in high-dimensional continuous robotics domains (MuJoCo). Our methods demonstrate improved initial and competitive final performance compared to model-free state-of-the-art techniques.",0
"The method of empowerment is a means of motivating learning agents through information theory. It strives to enhance an agent's control over its surroundings by encouraging exploration of states with many possible next states. Empowered learning has been found to result in complex behaviors, even without explicit reward signals. This study explores the use of empowerment in conjunction with external reward signals. It proposes that empowerment can guide reinforcement learning (RL) agents towards effective early behavioral solutions by promoting highly empowered states. The study presents a unified Bellman optimality principle for empowered reward maximization, which extends both Bellman's optimality principle and recent information-theoretical extensions to it. The empowered values are proven to be unique, and convergence to the optimal solution is shown. The idea is then applied to develop off-policy actor-critic RL algorithms, which are tested in high-dimensional continuous robotics domains (MuJoCo). The results demonstrate improved initial and competitive final performance in comparison to state-of-the-art model-free techniques.",1
"Reinforcement learning can greatly benefit from the use of options as a way of encoding recurring behaviours and to foster exploration. An important open problem is how can an agent autonomously learn useful options when solving particular distributions of related tasks. We investigate some of the conditions that influence optimality of options, in settings where agents have a limited time budget for learning each task and the task distribution might involve problems with different levels of similarity. We directly search for optimal option sets and show that the discovered options significantly differ depending on factors such as the available learning time budget and that the found options outperform popular option-generation heuristics.",0
The utilization of options in reinforcement learning can provide significant advantages for encoding repetitive actions and promoting exploration. A crucial unresolved issue is how an agent can independently acquire valuable options while tackling various related tasks. Our research examines the factors that impact the effectiveness of options when agents face time constraints for learning each task and encounter problems with varying degrees of similarity. We directly search for the most optimal set of options and demonstrate that the identified options differ considerably based on elements such as the assigned learning time and that the discovered options surpass commonly used option-generation heuristics.,1
"Conservative Policy Iteration (CPI) is a founding algorithm of Approximate Dynamic Programming (ADP). Its core principle is to stabilize greediness through stochastic mixtures of consecutive policies. It comes with strong theoretical guarantees, and inspired approaches in deep Reinforcement Learning (RL). However, CPI itself has rarely been implemented, never with neural networks, and only experimented on toy problems. In this paper, we show how CPI can be practically combined with deep RL with discrete actions. We also introduce adaptive mixture rates inspired by the theory. We experiment thoroughly the resulting algorithm on the simple Cartpole problem, and validate the proposed method on a representative subset of Atari games. Overall, this work suggests that revisiting classic ADP may lead to improved and more stable deep RL algorithms.",0
"Conservative Policy Iteration (CPI) is a fundamental algorithm in Approximate Dynamic Programming (ADP) that aims to stabilize greediness by using stochastic mixtures of consecutive policies. Despite its strong theoretical foundations and influence on deep Reinforcement Learning (RL), CPI has not been widely implemented, especially with neural networks, and has only been tested on simple problems. Our study demonstrates how CPI can be practically combined with deep RL with discrete actions and introduces adaptive mixture rates based on theory. We extensively test this resulting algorithm on the Cartpole problem and a representative subset of Atari games, confirming that revisiting classic ADP may lead to improved and more stable deep RL algorithms.",1
"Transfer in Reinforcement Learning (RL) refers to the idea of applying knowledge gained from previous tasks to solving related tasks. Learning a universal value function (Schaul et al., 2015), which generalizes over goals and states, has previously been shown to be useful for transfer. However, successor features are believed to be more suitable than values for transfer (Dayan, 1993; Barreto et al.,2017), even though they cannot directly generalize to new goals. In this paper, we propose (1) Universal Successor Features (USFs) to capture the underlying dynamics of the environment while allowing generalization to unseen goals and (2) a flexible end-to-end model of USFs that can be trained by interacting with the environment. We show that learning USFs is compatible with any RL algorithm that learns state values using a temporal difference method. Our experiments in a simple gridworld and with two MuJoCo environments show that USFs can greatly accelerate training when learning multiple tasks and can effectively transfer knowledge to new tasks.",0
"In Reinforcement Learning (RL), the concept of Transfer involves utilizing the knowledge acquired from prior tasks to solve related tasks. Prior research has demonstrated that learning a universal value function, which generalizes over goals and states, can aid in transfer. However, successor features are deemed more appropriate than values for transfer, despite their inability to directly generalize to new goals. This paper presents (1) Universal Successor Features (USFs), which capture the underlying environment dynamics while allowing for generalization to unseen goals, and (2) a customizable end-to-end model of USFs that can train by interacting with the environment. We prove that learning USFs is compatible with any RL algorithm that learns state values using a temporal difference method. Our experiments in a basic gridworld and two MuJoCo environments indicate that USFs can significantly expedite training in multiple tasks and efficiently transfer knowledge to new tasks.",1
"We consider a settings of hierarchical reinforcement learning, in which the reward is a sum of components. For each component we are given a policy that maximizes it and our goal is to assemble a policy from the individual policies that maximizes the sum of the components. We provide theoretical guarantees for assembling such policies in deterministic MDPs with collectible rewards. Our approach builds on formulating this problem as a traveling salesman problem with discounted reward. We focus on local solutions, i.e., policies that only use information from the current state; thus, they are easy to implement and do not require substantial computational resources. We propose three local stochastic policies and prove that they guarantee better performance than any deterministic local policy in the worst case; experimental results suggest that they also perform better on average.",0
"Our study examines hierarchical reinforcement learning where the reward comprises multiple components. Each component has its own policy that maximizes it, and our objective is to combine these policies to create a policy that maximizes the overall reward. We offer theoretical assurances for assembling such policies in deterministic MDPs with collectible rewards, using a method that treats the problem as a discounted reward traveling salesman problem. Our emphasis is on local solutions that solely rely on the current state for information, making them uncomplicated to implement and not computationally demanding. We propose three local stochastic policies and demonstrate that they outperform any deterministic local policy in the worst-case scenario, while experimental findings suggest that they also outperform them on average.",1
"Resource allocation and transceivers in wireless networks are usually designed by solving optimization problems subject to specific constraints, which can be formulated as variable or functional optimization. If the objective and constraint functions of a variable optimization problem can be derived, standard numerical algorithms can be applied for finding the optimal solution, which however incur high computational cost when the dimension of the variable is high. To reduce the on-line computational complexity, learning the optimal solution as a function of the environment's status by deep neural networks (DNNs) is an effective approach. DNNs can be trained under the supervision of optimal solutions, which however, is not applicable to the scenarios without models or for functional optimization where the optimal solutions are hard to obtain. If the objective and constraint functions are unavailable, reinforcement learning can be applied to find the solution of a functional optimization problem, which is however not tailored to optimization problems in wireless networks. In this article, we introduce unsupervised and reinforced-unsupervised learning frameworks for solving both variable and functional optimization problems without the supervision of the optimal solutions. When the mathematical model of the environment is completely known and the distribution of environment's status is known or unknown, we can invoke unsupervised learning algorithm. When the mathematical model of the environment is incomplete, we introduce reinforced-unsupervised learning algorithms that learn the model by interacting with the environment. Our simulation results confirm the applicability of these learning frameworks by taking a user association problem as an example.",0
"In wireless networks, resource allocation and transceivers are designed through optimization problems that have specific constraints. These problems can be formulated as variable or functional optimization. If the objective and constraint functions of a variable optimization problem are known, finding the optimal solution using numerical algorithms incurs high computational costs when the variable dimension is high. To reduce this complexity, deep neural networks (DNNs) can be trained to learn the optimal solution as a function of the environment's status. However, this approach is not applicable in scenarios without models or for functional optimization where optimal solutions are difficult to obtain. Reinforcement learning can be used to find the solution of a functional optimization problem when objective and constraint functions are unavailable, but this approach is not tailored to optimization problems in wireless networks. In this article, we present unsupervised and reinforced-unsupervised learning frameworks for solving both variable and functional optimization problems without supervision of optimal solutions. Unsupervised learning algorithms can be used when the mathematical model of the environment is completely known and the distribution of environment's status is known or unknown. Reinforced-unsupervised learning algorithms are introduced when the mathematical model of the environment is incomplete, and the model is learned by interacting with the environment. We demonstrate the applicability of these learning frameworks through a user association problem simulation.",1
"Simulation-to-simulation and simulation-to-real world transfer of neural network models have been a difficult problem. To close the reality gap, prior methods to simulation-to-real world transfer focused on domain adaptation, decoupling perception and dynamics and solving each problem separately, and randomization of agent parameters and environment conditions to expose the learning agent to a variety of conditions. While these methods provide acceptable performance, the computational complexity required to capture a large variation of parameters for comprehensive scenarios on a given task such as autonomous driving or robotic manipulation is high. Our key contribution is to theoretically prove and empirically demonstrate that a deep attention convolutional neural network (DACNN) with specific visual sensor configuration performs as well as training on a dataset with high domain and parameter variation at lower computational complexity. Specifically, the attention network weights are learned through policy optimization to focus on local dependencies that lead to optimal actions, and does not require tuning in real-world for generalization. Our new architecture adapts perception with respect to the control objective, resulting in zero-shot learning without pre-training a perception network. To measure the impact of our new deep network architecture on domain adaptation, we consider autonomous driving as a use case. We perform an extensive set of experiments in simulation-to-simulation and simulation-to-real scenarios to compare our approach to several baselines including the current state-of-art models.",0
"The transfer of neural network models between simulations and the real world has been a challenging issue. Previous methods for addressing this problem involved domain adaptation by separating perception and dynamics and tackling each issue separately, as well as randomizing agent parameters and environmental conditions to expose the learning agent to various situations. However, these techniques require high computational complexity to capture a wide range of parameters for comprehensive scenarios, such as autonomous driving or robotic manipulation. Our contribution is to prove theoretically and demonstrate empirically that a deep attention convolutional neural network (DACNN) with a specific visual sensor configuration can deliver comparable performance to training on a dataset with high domain and parameter variation but with lower computational complexity. The attention network weights are learned through policy optimization to focus on local dependencies, leading to optimal actions and eliminating the need for tuning in the real world for generalization. Our new architecture adapts perception with respect to the control objective, resulting in zero-shot learning without the need for pre-training a perception network. To assess the impact of our deep network architecture on domain adaptation, we use autonomous driving as a use case and conduct extensive experiments in simulation-to-simulation and simulation-to-real scenarios, comparing our approach to several baselines, including the current state-of-the-art models.",1
"We propose a novel method for automatic reasoning on knowledge graphs based on debate dynamics. The main idea is to frame the task of triple classification as a debate game between two reinforcement learning agents which extract arguments -- paths in the knowledge graph -- with the goal to promote the fact being true (thesis) or the fact being false (antithesis), respectively. Based on these arguments, a binary classifier, called the judge, decides whether the fact is true or false. The two agents can be considered as sparse, adversarial feature generators that present interpretable evidence for either the thesis or the antithesis. In contrast to other black-box methods, the arguments allow users to get an understanding of the decision of the judge. Since the focus of this work is to create an explainable method that maintains a competitive predictive accuracy, we benchmark our method on the triple classification and link prediction task. Thereby, we find that our method outperforms several baselines on the benchmark datasets FB15k-237, WN18RR, and Hetionet. We also conduct a survey and find that the extracted arguments are informative for users.",0
"Our proposal introduces a unique approach to automatically reason on knowledge graphs using debate dynamics. We suggest that the task of triple classification can be transformed into a debate game between two reinforcement learning agents. These agents extract arguments, which are essentially paths in the knowledge graph, to support either the truth or falsity of the fact at hand. A binary classifier, referred to as the judge, evaluates these arguments to determine the fact's accuracy. The agents act as sparse, adversarial feature generators who provide evidence for their respective positions. Our method stands apart from other black-box techniques as the arguments are interpretable, offering users insight into the judge's decision. We aim to create an explainable method that maintains competitive predictive accuracy, and thus we benchmark our approach on triple classification and link prediction tasks. Our results demonstrate that our model outperforms several baselines on benchmark datasets FB15k-237, WN18RR, and Hetionet. We also conducted a survey, which revealed that users find the extracted arguments informative.",1
"Many real-world control problems involve both discrete decision variables - such as the choice of control modes, gear switching or digital outputs - as well as continuous decision variables - such as velocity setpoints, control gains or analogue outputs. However, when defining the corresponding optimal control or reinforcement learning problem, it is commonly approximated with fully continuous or fully discrete action spaces. These simplifications aim at tailoring the problem to a particular algorithm or solver which may only support one type of action space. Alternatively, expert heuristics are used to remove discrete actions from an otherwise continuous space. In contrast, we propose to treat hybrid problems in their 'native' form by solving them with hybrid reinforcement learning, which optimizes for discrete and continuous actions simultaneously. In our experiments, we first demonstrate that the proposed approach efficiently solves such natively hybrid reinforcement learning problems. We then show, both in simulation and on robotic hardware, the benefits of removing possibly imperfect expert-designed heuristics. Lastly, hybrid reinforcement learning encourages us to rethink problem definitions. We propose reformulating control problems, e.g. by adding meta actions, to improve exploration or reduce mechanical wear and tear.",0
"Numerous control problems in reality require both discrete and continuous decision variables. However, these problems are often simplified to fully continuous or fully discrete action spaces when defining optimal control or reinforcement learning problems. This is done to tailor the problem to a specific algorithm or solver that may only support one type of action space or to eliminate discrete actions using expert heuristics. Our proposal, on the other hand, is to solve these hybrid problems in their ""native"" form using hybrid reinforcement learning, which optimizes both discrete and continuous actions simultaneously. Our experiments demonstrate that this approach efficiently solves such natively hybrid reinforcement learning problems. In addition, we show the advantages of eliminating possibly imperfect expert-designed heuristics in simulation and on robotic hardware. Finally, hybrid reinforcement learning motivates us to reconsider problem definitions by suggesting the addition of meta actions to enhance exploration or reduce mechanical wear and tear.",1
"We develop a normative framework for hierarchical model-based policy optimization based on applying second-order methods in the space of all possible state-action paths. The resulting natural path gradient performs policy updates in a manner which is sensitive to the long-range correlational structure of the induced stationary state-action densities. We demonstrate that the natural path gradient can be computed exactly given an environment dynamics model and depends on expressions akin to higher-order successor representations. In simulation, we show that the priorization of local policy updates in the resulting policy flow indeed reflects the intuitive state-space hierarchy in several toy problems.",0
"A normative framework for hierarchical model-based policy optimization is created by utilizing second-order methods in the space of all potential state-action paths. The natural path gradient that arises from this approach updates policies in a way that accounts for the long-range correlation structure of the stationary state-action densities induced by the framework. By utilizing expressions similar to higher-order successor representations, the natural path gradient can be accurately computed with the aid of an environment dynamics model. Our simulation results demonstrate that the resulting policy flow prioritizes local policy updates that align with the intuitive state-space hierarchy in various toy problems.",1
"We present a reduction from reinforcement learning (RL) to no-regret online learning based on the saddle-point formulation of RL, by which ""any"" online algorithm with sublinear regret can generate policies with provable performance guarantees. This new perspective decouples the RL problem into two parts: regret minimization and function approximation. The first part admits a standard online-learning analysis, and the second part can be quantified independently of the learning algorithm. Therefore, the proposed reduction can be used as a tool to systematically design new RL algorithms. We demonstrate this idea by devising a simple RL algorithm based on mirror descent and the generative-model oracle. For any $\gamma$-discounted tabular RL problem, with probability at least $1-\delta$, it learns an $\epsilon$-optimal policy using at most $\tilde{O}\left(\frac{|\mathcal{S}||\mathcal{A}|\log(\frac{1}{\delta})}{(1-\gamma)^4\epsilon^2}\right)$ samples. Furthermore, this algorithm admits a direct extension to linearly parameterized function approximators for large-scale applications, with computation and sample complexities independent of $|\mathcal{S}|$,$|\mathcal{A}|$, though at the cost of potential approximation bias.",0
"The saddle-point formulation of reinforcement learning (RL) enables us to reduce it to no-regret online learning. This reduction divides the RL problem into two parts: regret minimization and function approximation. The first part follows a standard online-learning analysis, while the second part can be quantified independently of the learning algorithm. Consequently, the proposed reduction can be used to systematically design novel RL algorithms. We illustrate this idea by introducing an RL algorithm that utilizes mirror descent and the generative-model oracle. For a $\gamma$-discounted tabular RL problem, it learns an $\epsilon$-optimal policy using at most $\tilde{O}\left(\frac{|\mathcal{S}||\mathcal{A}|\log(\frac{1}{\delta})}{(1-\gamma)^4\epsilon^2}\right)$ samples with a probability of at least $1-\delta$. Moreover, this algorithm can be directly extended to linearly parameterized function approximators for large-scale applications, and its computation and sample complexities do not depend on $|\mathcal{S}|$,$|\mathcal{A}|$, albeit with the potential for approximation bias.",1
"There are great interests as well as many challenges in applying reinforcement learning (RL) to recommendation systems. In this setting, an online user is the environment; neither the reward function nor the environment dynamics are clearly defined, making the application of RL challenging. In this paper, we propose a novel model-based reinforcement learning framework for recommendation systems, where we develop a generative adversarial network to imitate user behavior dynamics and learn her reward function. Using this user model as the simulation environment, we develop a novel Cascading DQN algorithm to obtain a combinatorial recommendation policy which can handle a large number of candidate items efficiently. In our experiments with real data, we show this generative adversarial user model can better explain user behavior than alternatives, and the RL policy based on this model can lead to a better long-term reward for the user and higher click rate for the system.",0
"The application of reinforcement learning (RL) to recommendation systems presents both significant interests and challenges. In this context, the online user serves as the environment and the definition of the reward function and environment dynamics is not clear, which poses a challenge for RL. To address this, we propose a model-based RL framework for recommendation systems. Our approach involves using a generative adversarial network to simulate user behavior dynamics and learn the reward function. With this user model as the simulation environment, we introduce the Cascading DQN algorithm to derive an efficient combinatorial recommendation policy that can handle a large number of candidate items. Through experiments with actual data, we demonstrate that our generative adversarial user model is superior to other alternatives in explaining user behavior, and the RL policy based on this model achieves a better long-term reward for the user and higher click rate for the system.",1
"Reinforcement learning with sparse rewards is still an open challenge. Classic methods rely on getting feedback via extrinsic rewards to train the agent, and in situations where this occurs very rarely the agent learns slowly or cannot learn at all. Similarly, if the agent receives also rewards that create suboptimal modes of the objective function, it will likely prematurely stop exploring. More recent methods add auxiliary intrinsic rewards to encourage exploration. However, auxiliary rewards lead to a non-stationary target for the Q-function. In this paper, we present a novel approach that (1) plans exploration actions far into the future by using a long-term visitation count, and (2) decouples exploration and exploitation by learning a separate function assessing the exploration value of the actions. Contrary to existing methods which use models of reward and dynamics, our approach is off-policy and model-free. We further propose new tabular environments for benchmarking exploration in reinforcement learning. Empirical results on classic and novel benchmarks show that the proposed approach outperforms existing methods in environments with sparse rewards, especially in the presence of rewards that create suboptimal modes of the objective function. Results also suggest that our approach scales gracefully with the size of the environment. Source code is available at https://github.com/sparisi/visit-value-explore",0
"The challenge of reinforcement learning with sparse rewards remains unresolved. Traditional methods depend on receiving feedback through extrinsic rewards to train the agent, but if this happens infrequently, the agent learns slowly or not at all. Furthermore, if the agent receives rewards that generate suboptimal modes of the objective function, it is likely to stop exploring prematurely. More recent approaches use auxiliary intrinsic rewards to encourage exploration, but these lead to a non-stationary target for the Q-function. In this study, we propose a new strategy that (1) plans exploration actions far into the future based on a long-term visitation count, and (2) separates exploration and exploitation by learning a distinct function that assesses the exploration value of the actions. Our approach is off-policy and model-free, in contrast to existing methods that use models of reward and dynamics. We also suggest new tabular environments for benchmarking exploration in reinforcement learning. Our empirical results on classic and novel benchmarks show that our approach outperforms existing methods in environments with sparse rewards, especially when there are rewards that create suboptimal modes of the objective function. Our approach also appears to scale effectively with the size of the environment. The source code for this approach is available at https://github.com/sparisi/visit-value-explore.",1
"Open-domain dialog generation is a challenging problem; maximum likelihood training can lead to repetitive outputs, models have difficulty tracking long-term conversational goals, and training on standard movie or online datasets may lead to the generation of inappropriate, biased, or offensive text. Reinforcement Learning (RL) is a powerful framework that could potentially address these issues, for example by allowing a dialog model to optimize for reducing toxicity and repetitiveness. However, previous approaches which apply RL to open-domain dialog generation do so at the word level, making it difficult for the model to learn proper credit assignment for long-term conversational rewards. In this paper, we propose a novel approach to hierarchical reinforcement learning, VHRL, which uses policy gradients to tune the utterance-level embedding of a variational sequence model. This hierarchical approach provides greater flexibility for learning long-term, conversational rewards. We use self-play and RL to optimize for a set of human-centered conversation metrics, and show that our approach provides significant improvements -- in terms of both human evaluation and automatic metrics -- over state-of-the-art dialog models, including Transformers.",0
"Generating open-domain dialog is a complex task that poses several challenges. For instance, training models using maximum likelihood may result in repetitive outputs, and models may face difficulties in keeping track of long-term conversational goals. Additionally, training on standard datasets may lead to biased or offensive text. To tackle these issues, Reinforcement Learning (RL) is a promising approach that can help models optimize for reducing toxicity and repetitiveness. However, previous RL-based approaches have applied it at the word level, making it tough for models to learn proper credit assignment for long-term conversational rewards. In this paper, we propose a novel hierarchical approach called VHRL that uses policy gradients to tune the utterance-level embedding of a variational sequence model. This method enables the model to learn long-term conversational rewards more flexibly. We use self-play and RL to optimize for a set of human-centered conversation metrics and show that our approach outperforms state-of-the-art dialog models, including Transformers, in terms of both human evaluation and automatic metrics.",1
"Reinforcement learning offers the promise of automating the acquisition of complex behavioral skills. However, compared to commonly used and well-understood supervised learning methods, reinforcement learning algorithms can be brittle, difficult to use and tune, and sensitive to seemingly innocuous implementation decisions. In contrast, imitation learning utilizes standard and well-understood supervised learning methods, but requires near-optimal expert data. Can we learn effective policies via supervised learning without demonstrations? The main idea that we explore in this work is that non-expert trajectories collected from sub-optimal policies can be viewed as optimal supervision, not for maximizing the reward, but for matching the reward of the given trajectory. By then conditioning the policy on the numerical value of the reward, we can obtain a policy that generalizes to larger returns. We show how such an approach can be derived as a principled method for policy search, discuss several variants, and compare the method experimentally to a variety of current reinforcement learning methods on standard benchmarks.",0
"The potential of reinforcement learning to automate complex behavioral skills is hindered by its brittleness, difficulty in use and tuning, and sensitivity to minor implementation decisions, unlike the commonly used and well-understood supervised learning methods. However, imitation learning requires optimal expert data. This work explores the possibility of learning effective policies through supervised learning without demonstrations. The approach involves considering non-expert trajectories from sub-optimal policies as optimal supervision for matching the reward of the trajectory, rather than maximizing it. This allows for the derivation of a principled method for policy search that can generalize to larger returns. The paper presents several variants of this approach and compares it experimentally to various reinforcement learning methods on standard benchmarks.",1
"Building systems that autonomously create temporal abstractions from data is a key challenge in scaling learning and planning in reinforcement learning. One popular approach for addressing this challenge is the options framework (Sutton et al., 1999). However, only recently in (Bacon et al., 2017) was a policy gradient theorem derived for online learning of general purpose options in an end to end fashion. In this work, we extend previous work on this topic that only focuses on learning a two-level hierarchy including options and primitive actions to enable learning simultaneously at multiple resolutions in time. We achieve this by considering an arbitrarily deep hierarchy of options where high level temporally extended options are composed of lower level options with finer resolutions in time. We extend results from (Bacon et al., 2017) and derive policy gradient theorems for a deep hierarchy of options. Our proposed hierarchical option-critic architecture is capable of learning internal policies, termination conditions, and hierarchical compositions over options without the need for any intrinsic rewards or subgoals. Our empirical results in both discrete and continuous environments demonstrate the efficiency of our framework.",0
"A significant challenge in scaling learning and planning in reinforcement learning is creating systems that can autonomously generate temporal abstractions from data. One popular method for addressing this challenge is using the options framework, as proposed by Sutton et al. in 1999. However, it was only recently in 2017 that Bacon et al. derived a policy gradient theorem for online learning of general purpose options in an end-to-end manner. Our work builds on this by extending previous research that only focused on learning a two-level hierarchy of options and primitive actions. We enable learning at multiple resolutions in time by considering an arbitrarily deep hierarchy of options, where higher-level temporally extended options are composed of lower-level options with finer time resolutions. We derive policy gradient theorems for this extended hierarchy and propose a hierarchical option-critic architecture that can learn internal policies, termination conditions, and hierarchical compositions over options without requiring intrinsic rewards or subgoals. Our empirical results in both discrete and continuous environments demonstrate the efficiency of our framework.",1
"Robustness to out-of-distribution (OOD) data is an important goal in building reliable machine learning systems. Especially in autonomous systems, wrong predictions for OOD inputs can cause safety critical situations. As a first step towards a solution, we consider the problem of detecting such data in a value-based deep reinforcement learning (RL) setting. Modelling this problem as a one-class classification problem, we propose a framework for uncertainty-based OOD classification: UBOOD. It is based on the effect that an agent's epistemic uncertainty is reduced for situations encountered during training (in-distribution), and thus lower than for unencountered (OOD) situations. Being agnostic towards the approach used for estimating epistemic uncertainty, combinations with different uncertainty estimation methods, e.g. approximate Bayesian inference methods or ensembling techniques are possible. We further present a first viable solution for calculating a dynamic classification threshold, based on the uncertainty distribution of the training data. Evaluation shows that the framework produces reliable classification results when combined with ensemble-based estimators, while the combination with concrete dropout-based estimators fails to reliably detect OOD situations. In summary, UBOOD presents a viable approach for OOD classification in deep RL settings by leveraging the epistemic uncertainty of the agent's value function.",0
"Creating dependable machine learning systems that can handle out-of-distribution (OOD) data is crucial for ensuring safety, particularly in autonomous systems where erroneous predictions can lead to hazardous situations. To address this problem, we focus on detecting OOD data in a value-based deep reinforcement learning (RL) context. Our proposed solution, UBOOD, models this problem as a one-class classification issue and relies on the agent's epistemic uncertainty, which is lower for in-distribution situations and higher for OOD situations. We can use various uncertainty estimation methods, such as approximate Bayesian inference or ensembling techniques, with UBOOD. Additionally, we propose a dynamic classification threshold based on the uncertainty distribution of the training data. Our experimental results reveal that UBOOD combined with ensemble-based estimators produces dependable classification outcomes, while concrete dropout-based estimators fail to identify OOD situations. In summary, UBOOD leverages the epistemic uncertainty of an agent's value function, making it a practical choice for OOD classification in deep RL settings.",1
"The composition of elementary behaviors to solve challenging transfer learning problems is one of the key elements in building intelligent machines. To date, there has been plenty of work on learning task-specific policies or skills but almost no focus on composing necessary, task-agnostic skills to find a solution to new problems. In this paper, we propose a novel deep reinforcement learning-based skill transfer and composition method that takes the agent's primitive policies to solve unseen tasks. We evaluate our method in difficult cases where training policy through standard reinforcement learning (RL) or even hierarchical RL is either not feasible or exhibits high sample complexity. We show that our method not only transfers skills to new problem settings but also solves the challenging environments requiring both task planning and motion control with high data efficiency.",0
"Constructing fundamental behaviors for tackling intricate transfer learning problems is a crucial aspect of developing intelligent machines. Although there has been a considerable amount of research on acquiring policies or abilities tailored to specific tasks, there has been little emphasis on assembling fundamental, task-agnostic abilities to address new problems. This study proposes an innovative deep reinforcement learning-based methodology for transferring and combining an agent's primary policies to resolve previously unseen tasks. We assess our approach in complex scenarios where training a policy through standard reinforcement learning or hierarchical reinforcement learning is either not viable or involves a high degree of sample complexity. Our results demonstrate that our approach can transfer abilities to novel problem contexts and resolve challenging environments that necessitate both task planning and motion control with high data efficiency.",1
"We seek to align agent policy with human expert behavior in a reinforcement learning (RL) setting, without any prior knowledge about dynamics, reward function, and unsafe states. There is a human expert knowing the rewards and unsafe states based on his preference and objective, but querying that human expert is expensive. To address this challenge, we propose a new framework for imitation learning (IL) algorithm that actively and interactively learns a model of the user's reward function with efficient queries. We build an adversarial generative model of states and a successor feature (SR) model trained over transition experience collected by learning policy. Our method uses these models to select state-action pairs, asking the user to comment on the optimality or safety, and trains a adversarial neural network to predict the rewards. Different from previous papers, which are almost all based on uncertainty sampling, the key idea is to actively and efficiently select state-action pairs from both on-policy and off-policy experience, by discriminating the queried (expert) and unqueried (generated) data and maximizing the efficiency of value function learning. We call this method adversarial reward query with successor representation. We evaluate the proposed method with simulated human on a state-based 2D navigation task, robotic control tasks and the image-based video games, which have high-dimensional observation and complex state dynamics. The results show that the proposed method significantly outperforms uncertainty-based methods on learning reward models, achieving better query efficiency, where the adversarial discriminator can make the agent learn human behavior more efficiently and the SR can select states which have stronger impact on value function. Moreover, the proposed method can also learn to avoid unsafe states when training the reward model.",0
"In reinforcement learning (RL), our goal is to match the behavior of agents with that of human experts, even when we lack prior knowledge about dynamics, reward function, and unsafe states. Although a human expert knows about rewards and unsafe states based on their preferences and objectives, it is expensive to query them. To overcome this challenge, we propose a new framework for imitation learning (IL) algorithm that actively and interactively learns the user's reward function with efficient queries. Our method involves an adversarial generative model of states and a successor feature (SR) model, which is trained over transition experience collected by learning policy. We use these models to select state-action pairs and ask the user to comment on the optimality or safety, and then train an adversarial neural network to predict rewards. Unlike previous papers that rely on uncertainty sampling, we actively and efficiently select state-action pairs from both on-policy and off-policy experience. We call it adversarial reward query with successor representation. We evaluate our method on various tasks, including state-based 2D navigation, robotic control tasks, and image-based video games, and show that it outperforms uncertainty-based methods in terms of learning reward models, achieving better query efficiency. Our proposed method can also learn to avoid unsafe states when training the reward model, making it more efficient in learning human behavior.",1
"Hierarchical reinforcement learning has demonstrated significant success at solving difficult reinforcement learning (RL) tasks. Previous works have motivated the use of hierarchy by appealing to a number of intuitive benefits, including learning over temporally extended transitions, exploring over temporally extended periods, and training and exploring in a more semantically meaningful action space, among others. However, in fully observed, Markovian settings, it is not immediately clear why hierarchical RL should provide benefits over standard ""shallow"" RL architectures. In this work, we isolate and evaluate the claimed benefits of hierarchical RL on a suite of tasks encompassing locomotion, navigation, and manipulation. Surprisingly, we find that most of the observed benefits of hierarchy can be attributed to improved exploration, as opposed to easier policy learning or imposed hierarchical structures. Given this insight, we present exploration techniques inspired by hierarchy that achieve performance competitive with hierarchical RL while at the same time being much simpler to use and implement.",0
"The effectiveness of hierarchical reinforcement learning in tackling challenging tasks has been proven. Previous studies have shown that hierarchy offers advantages such as learning over extended time periods, exploring over prolonged periods, and training in a more meaningful action space. However, it is not clear why hierarchical RL is superior to standard RL in fully observed Markovian settings. In this study, we assess the benefits of hierarchical RL on various tasks such as locomotion, navigation, and manipulation. The results show that most of the benefits attributed to hierarchy are due to improved exploration rather than easier policy learning or hierarchical structures. Based on this finding, we propose exploration techniques inspired by hierarchy that achieve comparable performance to hierarchical RL but are simpler to implement and use.",1
"Some of the most important tasks take place in environments which lack cheap and perfect simulators, thus hampering the application of model-free reinforcement learning (RL). While model-based RL aims to learn a dynamics model, in a more general case the learner does not know a priori what the action space is. Here we propose a formalism where the learner induces a world program by learning a dynamics model and the actions in graph-based compositional environments by observing state-state transition examples. Then, the learner can perform RL with the world program as the simulator for complex planning tasks. We highlight a recent application, and propose a challenge for the community to assess world program-based planning.",0
"The absence of inexpensive and flawless simulators in certain settings obstructs the utilization of model-free reinforcement learning (RL) in performing crucial tasks. Although model-based RL allows for the acquisition of a dynamics model, it may not be applicable in situations where the learner lacks knowledge about the action space. Our proposal is to use a formalism in which the learner learns a dynamics model and actions in graph-based compositional environments by observing state-state transition examples, thereby inducing a world program. Subsequently, RL can be performed with the world program serving as the simulator for intricate planning tasks. We present a recent application and a challenge to the community to evaluate planning based on world programs.",1
"The motivation of this study is to leverage recent breakthroughs in artificial intelligence research to unlock novel solutions to important scientific problems encountered in computational science. To address the human intelligence limitations in discovering reduced-order models, we propose to supplement human thinking with artificial intelligence. Our three-pronged strategy consists of learning (i) models expressed in analytical form, (ii) which are evaluated a posteriori, and iii) using exclusively integral quantities from the reference solution as prior knowledge. In point (i), we pursue interpretable models expressed symbolically as opposed to black-box neural networks, the latter only being used during learning to efficiently parameterize the large search space of possible models. In point (ii), learned models are dynamically evaluated a posteriori in the computational solver instead of based on a priori information from preprocessed high-fidelity data, thereby accounting for the specificity of the solver at hand such as its numerics. Finally in point (iii), the exploration of new models is solely guided by predefined integral quantities, e.g., averaged quantities of engineering interest in Reynolds-averaged or large-eddy simulations (LES). We use a coupled deep reinforcement learning framework and computational solver to concurrently achieve these objectives. The combination of reinforcement learning with objectives (i), (ii) and (iii) differentiate our work from previous modeling attempts based on machine learning. In this report, we provide a high-level description of the model discovery framework with reinforcement learning. The method is detailed for the application of discovering missing terms in differential equations. An elementary instantiation of the method is described that discovers missing terms in the Burgers' equation.",0
"The aim of this study is to apply advances in artificial intelligence research to computational science in order to find innovative solutions to significant scientific issues. To overcome human limitations in discovering reduced-order models, we suggest using artificial intelligence techniques alongside human thinking. Our approach involves three strategies: (i) developing interpretable models that are expressed in analytical form rather than using black-box neural networks, which are only employed to parameterize the search space during the learning process; (ii) dynamically evaluating models in the computational solver instead of using a priori information from preprocessed high-fidelity data, which accounts for the solver's numerics; and (iii) exploring new models exclusively guided by predefined integral quantities, such as averaged quantities of engineering interest in Reynolds-averaged or large-eddy simulations. We use a coupled deep reinforcement learning framework and computational solver to achieve these objectives simultaneously, which differentiates our work from previous modeling attempts based on machine learning. In this report, we provide a high-level overview of the model discovery process using reinforcement learning, focusing on discovering missing terms in differential equations. An elementary instantiation of the method is described, which discovers missing terms in the Burgers' equation.",1
"Online reinforcement learning agents are currently able to process an increasing amount of data by converting it into a higher order value functions. This expansion of the information collected from the environment increases the agent's state space enabling it to scale up to a more complex problems but also increases the risk of forgetting by learning on redundant or conflicting data. To improve the approximation of a large amount of data, a random mini-batch of the past experiences that are stored in the replay memory buffer is often replayed at each learning step. The proposed work takes inspiration from a biological mechanism which act as a protective layer of human brain higher cognitive functions: active memory consolidation mitigates the effect of forgetting of previous memories by dynamically processing the new ones. The similar dynamics are implemented by a proposed augmented memory replay AMR capable of optimizing the replay of the experiences from the agent's memory structure by altering or augmenting their relevance. Experimental results show that an evolved AMR augmentation function capable of increasing the significance of the specific memories is able to further increase the stability and convergence speed of the learning algorithms dealing with the complexity of continuous action domains.",0
"Currently, reinforcement learning agents operating online can handle an increasing amount of data by converting it into higher order value functions. This allows the agents to expand their state space, which enables them to tackle more complex problems. However, this expansion also poses a risk of forgetting by learning on redundant or conflicting data. To improve the approximation of a large amount of data, past experiences stored in the replay memory buffer are often replayed at each learning step. The proposed work draws inspiration from a biological mechanism that protects human brain higher cognitive functions: active memory consolidation. This mechanism mitigates the effect of forgetting previous memories by dynamically processing new ones. The proposed augmented memory replay (AMR) implements similar dynamics by optimizing the replay of experiences from the agent's memory structure through alterations or augmentations of their relevance. Experimental results show that an evolved AMR augmentation function, capable of increasing the significance of specific memories, can further increase the stability and convergence speed of learning algorithms dealing with the complexity of continuous action domains.",1
"Policy distillation in deep reinforcement learning provides an effective way to transfer control policies from a larger network to a smaller untrained network without a significant degradation in performance. However, policy distillation is underexplored in deep reinforcement learning, and existing approaches are computationally inefficient, resulting in a long distillation time. In addition, the effectiveness of the distillation process is still limited to the model capacity. We propose a new distillation mechanism, called real-time policy distillation, in which training the teacher model and distilling the policy to the student model occur simultaneously. Accordingly, the teacher's latest policy is transferred to the student model in real time. This reduces the distillation time to half the original time or even less and also makes it possible for extremely small student models to learn skills at the expert level. We evaluated the proposed algorithm in the Atari 2600 domain. The results show that our approach can achieve full distillation in most games, even with compression ratios up to 1.7%.",0
"Deep reinforcement learning's policy distillation technique is effective in transferring control policies from larger to smaller untrained networks without any significant performance degradation. Nonetheless, this method remains underexplored with current approaches being computationally inefficient, leading to prolonged distillation time. Moreover, the distillation process's effectiveness remains limited to the model's capacity. In response, we propose a new distillation mechanism, known as real-time policy distillation, where the teacher and student models train simultaneously. This approach transfers the teacher's latest policy to the student model in real-time, reducing distillation time by half or more. Our algorithm also enables small student models to learn expert-level skills. We tested our method on Atari 2600 games and found that it can achieve full distillation despite compression ratios of up to 1.7%.",1
"When playing video-games we immediately detect which entity we control and we center the attention towards it to focus the learning and reduce its dimensionality. Reinforcement Learning (RL) has been able to deal with big state spaces, including states derived from pixel images in Atari games, but the learning is slow, depends on the brute force mapping from the global state to the action values (Q-function), thus its performance is severely affected by the dimensionality of the state and cannot be transferred to other games or other parts of the same game. We propose different transformations of the input state that combine attention and agency detection mechanisms which both have been addressed separately in RL but not together to our knowledge. We propose and benchmark different architectures including both global and local agency centered versions of the state and also including summaries of the surroundings. Results suggest that even a redundant global-local state network can learn faster than the global alone. Summarized versions of the state look promising to achieve input-size independence learning.",0
"While playing video games, we quickly identify the entity we control and focus our attention on it to facilitate learning and reduce complexity. Reinforcement Learning (RL) can handle large state spaces, such as those derived from pixel images in Atari games, but its learning is slow and heavily dependent on the brute force mapping from the global state to the action values (Q-function), which limits its performance and transferability to other games or parts of the same game. Our proposed approach combines attention and agency detection mechanisms, which have been addressed separately in RL, to transform the input state. We tested different architectures, including global and local agency-centered versions of the state, as well as summaries of the surroundings. The results indicate that even a redundant global-local state network can learn faster than the global state alone, and summarized versions of the state hold promise for input-size independent learning.",1
"We introduce SLM Lab, a software framework for reproducible reinforcement learning (RL) research. SLM Lab implements a number of popular RL algorithms, provides synchronous and asynchronous parallel experiment execution, hyperparameter search, and result analysis. RL algorithms in SLM Lab are implemented in a modular way such that differences in algorithm performance can be confidently ascribed to differences between algorithms, not between implementations. In this work we present the design choices behind SLM Lab and use it to produce a comprehensive single-codebase RL algorithm benchmark. In addition, as a consequence of SLM Lab's modular design, we introduce and evaluate a discrete-action variant of the Soft Actor-Critic algorithm (Haarnoja et al., 2018) and a hybrid synchronous/asynchronous training method for RL agents.",0
"SLM Lab is a framework for conducting reproducible research in reinforcement learning (RL). It includes various RL algorithms, enables parallel experiment execution, allows for hyperparameter search, and facilitates result analysis. The modular implementation of RL algorithms in SLM Lab ensures that any differences in performance can be attributed to algorithmic variations and not implementation discrepancies. This paper details SLM Lab's design choices and demonstrates its effectiveness by presenting a benchmark of RL algorithms that use a single-codebase. Additionally, the modular design of SLM Lab allows for the development and evaluation of a discrete-action version of the Soft Actor-Critic algorithm (Haarnoja et al., 2018) and a hybrid training method for RL agents that combines synchronous and asynchronous approaches.",1
"We present an algorithm based on the \emph{Optimism in the Face of Uncertainty} (OFU) principle which is able to learn Reinforcement Learning (RL) modeled by Markov decision process (MDP) with finite state-action space efficiently. By evaluating the state-pair difference of the optimal bias function $h^{*}$, the proposed algorithm achieves a regret bound of $\tilde{O}(\sqrt{SAHT})$\footnote{The symbol $\tilde{O}$ means $O$ with log factors ignored. } for MDP with $S$ states and $A$ actions, in the case that an upper bound $H$ on the span of $h^{*}$, i.e., $sp(h^{*})$ is known. This result outperforms the best previous regret bounds $\tilde{O}(S\sqrt{AHT}) $\citep{fruit2019improved} by a factor of $\sqrt{S}$. Furthermore, this regret bound matches the lower bound of $\Omega(\sqrt{SAHT}) $\citep{jaksch2010near} up to a logarithmic factor. As a consequence, we show that there is a near optimal regret bound of $\tilde{O}(\sqrt{SADT})$ for MDPs with a finite diameter $D$ compared to the lower bound of $\Omega(\sqrt{SADT}) $\citep{jaksch2010near}.",0
"We introduce an algorithm that utilizes the \emph{Optimism in the Face of Uncertainty} (OFU) principle to efficiently learn Reinforcement Learning (RL) modeled by Markov decision process (MDP) with finite state-action space. By assessing the difference in state-pair of the optimal bias function $h^{*}$, our proposed algorithm achieves a regret bound of $\tilde{O}(\sqrt{SAHT})$\footnote{The symbol $\tilde{O}$ means $O$ with log factors ignored. } for MDP with $S$ states and $A$ actions, provided that an upper bound $H$ on the span of $h^{*}$, i.e., $sp(h^{*})$ is known. This result surpasses the previous best regret bounds of $\tilde{O}(S\sqrt{AHT}) $\citep{fruit2019improved} by a factor of $\sqrt{S}$. Additionally, our regret bound matches the lower bound of $\Omega(\sqrt{SAHT}) $\citep{jaksch2010near} up to a logarithmic factor. As a result, we prove that MDPs with a finite diameter $D$ have a near optimal regret bound of $\tilde{O}(\sqrt{SADT})$ compared to the lower bound of $\Omega(\sqrt{SADT}) $\citep{jaksch2010near}.",1
"A major component of overfitting in model-free reinforcement learning (RL) involves the case where the agent may mistakenly correlate reward with certain spurious features from the observations generated by the Markov Decision Process (MDP). We provide a general framework for analyzing this scenario, which we use to design multiple synthetic benchmarks from only modifying the observation space of an MDP. When an agent overfits to different observation spaces even if the underlying MDP dynamics is fixed, we term this observational overfitting. Our experiments expose intriguing properties especially with regards to implicit regularization, and also corroborate results from previous works in RL generalization and supervised learning (SL).",0
"Overfitting in model-free reinforcement learning (RL) can occur when the agent incorrectly associates reward with irrelevant features in the observations generated by the Markov Decision Process (MDP). To analyze this issue, we developed a general framework and created several synthetic benchmarks by modifying the observation space of an MDP. This phenomenon, which we call observational overfitting, occurs when an agent overfits to different observation spaces despite the MDP dynamics remaining constant. Our experimental results reveal fascinating properties related to implicit regularization and support findings from prior research in RL generalization and supervised learning (SL).",1
"The task of Visual Commonsense Reasoning is extremely challenging in the sense that the model has to not only be able to answer a question given an image, but also be able to learn to reason. The baselines introduced in this task are quite limiting because two networks are trained for predicting answers and rationales separately. Question and image is used as input to train answer prediction network while question, image and correct answer are used as input in the rationale prediction network. As rationale is conditioned on the correct answer, it is based on the assumption that we can solve Visual Question Answering task without any error - which is over ambitious. Moreover, such an approach makes both answer and rationale prediction two completely independent VQA tasks rendering cognition task meaningless. In this paper, we seek to address these issues by proposing an end-to-end trainable model which considers both answers and their reasons jointly. Specifically, we first predict the answer for the question and then use the chosen answer to predict the rationale. However, a trivial design of such a model becomes non-differentiable which makes it difficult to train. We solve this issue by proposing four approaches - softmax, gumbel-softmax, reinforcement learning based sampling and direct cross entropy against all pairs of answers and rationales. We demonstrate through experiments that our model performs competitively against current state-of-the-art. We conclude with an analysis of presented approaches and discuss avenues for further work.",0
"Visual Commonsense Reasoning is a highly difficult task because the model must not only answer questions based on an image, but also reason. The current baselines for this task have limitations because they use separate networks to predict answers and rationales. The answer prediction network takes the question and image as input, while the rationale prediction network takes the question, image, and correct answer as input. This approach assumes that the Visual Question Answering task can be solved without error, which is unrealistic and renders cognition meaningless. To address these issues, we propose an end-to-end trainable model that considers both answers and their reasons jointly. We first predict the answer for the question and then use that answer to predict the rationale. However, designing such a model is challenging because it becomes non-differentiable, making it difficult to train. We propose four approaches to solve this issue: softmax, gumbel-softmax, reinforcement learning-based sampling, and direct cross-entropy against all pairs of answers and rationales. Our experiments show that our model performs competitively against the current state-of-the-art. We conclude with an analysis of our approaches and suggest avenues for further research.",1
"Recent years have witnessed the increasing interests in research of crowdfunding mechanism. In this area, dynamics tracking is a significant issue but is still under exploration. Existing studies either fit the fluctuations of time-series or employ regularization terms to constrain learned tendencies. However, few of them take into account the inherent decision-making process between investors and crowdfunding dynamics. To address the problem, in this paper, we propose a Trajectory-based Continuous Control for Crowdfunding (TC3) algorithm to predict the funding progress in crowdfunding. Specifically, actor-critic frameworks are employed to model the relationship between investors and campaigns, where all of the investors are viewed as an agent that could interact with the environment derived from the real dynamics of campaigns. Then, to further explore the in-depth implications of patterns (i.e., typical characters) in funding series, we propose to subdivide them into $\textit{fast-growing}$ and $\textit{slow-growing}$ ones. Moreover, for the purpose of switching from different kinds of patterns, the actor component of TC3 is extended with a structure of options, which comes to the TC3-Options. Finally, extensive experiments on the Indiegogo dataset not only demonstrate the effectiveness of our methods, but also validate our assumption that the entire pattern learned by TC3-Options is indeed the U-shaped one.",0
"In recent years, there has been an increasing interest in researching crowdfunding mechanisms. One significant issue in this area is dynamics tracking, which is still being explored. Previous studies have either fitted time-series fluctuations or used regularization terms to constrain learned tendencies. However, they have not considered the decision-making process between investors and crowdfunding dynamics. To address this problem, we propose the Trajectory-based Continuous Control for Crowdfunding (TC3) algorithm to predict crowdfunding progress. Our approach employs actor-critic frameworks to model the relationship between investors and campaigns, with all investors viewed as agents interacting with the environment derived from the real dynamics of campaigns. We further categorize funding patterns into ""fast-growing"" and ""slow-growing"" and extend the actor component of TC3 with a structure of options to switch between different patterns, creating the TC3-Options. Our extensive experiments on the Indiegogo dataset demonstrate the effectiveness of our methods and validate our assumption that the entire pattern learned by TC3-Options is U-shaped.",1
"We propose a trust region method for policy optimization that employs Quasi-Newton approximation for the Hessian, called Quasi-Newton Trust Region Policy Optimization QNTRPO. Gradient descent is the de facto algorithm for reinforcement learning tasks with continuous controls. The algorithm has achieved state-of-the-art performance when used in reinforcement learning across a wide range of tasks. However, the algorithm suffers from a number of drawbacks including: lack of stepsize selection criterion, and slow convergence. We investigate the use of a trust region method using dogleg step and a Quasi-Newton approximation for the Hessian for policy optimization. We demonstrate through numerical experiments over a wide range of challenging continuous control tasks that our particular choice is efficient in terms of number of samples and improves performance",0
"Our proposed method for policy optimization, called Quasi-Newton Trust Region Policy Optimization (QNTRPO), utilizes a Quasi-Newton approximation for the Hessian and employs a trust region approach. Gradient descent is commonly used for reinforcement learning tasks with continuous controls and has been successful in achieving state-of-the-art performance across various tasks. However, it has limitations such as lacking a stepsize selection criterion and slow convergence. To address these drawbacks, we explore the use of a trust region method with a dogleg step and Quasi-Newton approximation for the Hessian. Our numerical experiments on challenging continuous control tasks show that our approach is efficient in terms of the number of samples required and leads to improved performance.",1
"Predicting distant future trajectories of agents in a dynamic scene is not an easy problem because the future trajectory of an agent is affected by not only his/her past trajectory but also the scene contexts. To tackle this problem, we propose a model based on recurrent neural networks (RNNs) and a novel method for training the model. The proposed model is based on an encoder-decoder architecture where the encoder encodes inputs (past trajectories and scene context information) while the decoder produces a trajectory from the context vector given by the encoder. We train the networks of the proposed model to produce a future trajectory, which is the closest to the true trajectory, while maximizing a reward from a reward function. The reward function is also trained at the same time to maximize the margin between the rewards from the ground-truth trajectory and its estimate. The reward function plays the role of a regularizer for the proposed model so the trained networks are able to better utilize the scene context information for the prediction task. We evaluated the proposed model on several public datasets. Experimental results show that the prediction performance of the proposed model is much improved by the regularization, which outperforms the-state-of-the-arts in terms of accuracy. The implementation codes are available at https://github.com/d1024choi/traj-pred-irl/.",0
"It is a challenging task to predict the future trajectories of agents in a dynamic scene, as their movements are influenced not only by their past trajectory but also by the surrounding context. In order to address this issue, we have developed a recurrent neural network (RNN)-based model and a unique training approach. Our model follows an encoder-decoder architecture, with the encoder encoding input data (past trajectories and scene context), and the decoder producing a trajectory based on the context vector from the encoder. Our training process trains the networks to generate a future trajectory that is closest to the true trajectory and maximizes a reward from a reward function. This function is also trained simultaneously to increase the margin between the rewards from the ground-truth trajectory and its estimate. The reward function acts as a regularizer for the model, allowing the networks to better utilize contextual information for prediction. We have evaluated our model on multiple public datasets, with results showing improved prediction performance and higher accuracy compared to state-of-the-art methods. Implementation codes are available at https://github.com/d1024choi/traj-pred-irl/.",1
"Trajectory optimization using a learned model of the environment is one of the core elements of model-based reinforcement learning. This procedure often suffers from exploiting inaccuracies of the learned model. We propose to regularize trajectory optimization by means of a denoising autoencoder that is trained on the same trajectories as the model of the environment. We show that the proposed regularization leads to improved planning with both gradient-based and gradient-free optimizers. We also demonstrate that using regularized trajectory optimization leads to rapid initial learning in a set of popular motor control tasks, which suggests that the proposed approach can be a useful tool for improving sample efficiency.",0
"One of the fundamental components of model-based reinforcement learning is trajectory optimization utilizing a learned model of the environment. However, this process is frequently hindered by the exploitation of inaccuracies within the learned model. To address this issue, we suggest regularizing trajectory optimization with a denoising autoencoder that is trained on the same trajectories as the environment model. Our research shows that this regularization method results in improved planning using both gradient-based and gradient-free optimizers. Additionally, we demonstrate that utilizing regularized trajectory optimization results in rapid initial learning in several popular motor control tasks, indicating that our proposal can be a valuable tool in enhancing sample efficiency.",1
"Exploration efficiency is a challenging problem in multi-agent reinforcement learning (MARL), as the policy learned by confederate MARL depends on the collaborative approach among multiple agents. Another important problem is the less informative reward restricts the learning speed of MARL compared with the informative label in supervised learning. In this work, we leverage on a novel communication method to guide MARL to accelerate exploration and propose a predictive network to forecast the reward of current state-action pair and use the guidance learned by the predictive network to modify the reward function. An improved prioritized experience replay is employed to better take advantage of the different knowledge learned by different agents which utilizes Time-difference (TD) error more effectively. Experimental results demonstrates that the proposed algorithm outperforms existing methods in cooperative multi-agent environments. We remark that this algorithm can be extended to supervised learning to speed up its training.",0
"Efficient exploration in multi-agent reinforcement learning (MARL) presents a challenge, as the policy developed through collaborative efforts among multiple agents is dependent upon each other. Additionally, the limited information provided by rewards slows down the learning process of MARL, in comparison to supervised learning which uses informative labels. This study introduces a novel communication method to improve exploration efficiency in MARL, and a predictive network is proposed to forecast the reward of state-action pairs. This network is used to modify the reward function based on the guidance it has learned. To better utilize the different knowledge acquired by different agents, an improved prioritized experience replay method is utilized, which effectively employs Time-difference (TD) error. Results show that this approach outperforms existing methods in cooperative multi-agent situations. It should be noted that this algorithm can also be applied to supervised learning to hasten its training.",1
"In partially observable (PO) environments, deep reinforcement learning (RL) agents often suffer from unsatisfactory performance, since two problems need to be tackled together: how to extract information from the raw observations to solve the task, and how to improve the policy. In this study, we propose an RL algorithm for solving PO tasks. Our method comprises two parts: a variational recurrent model (VRM) for modeling the environment, and an RL controller that has access to both the environment and the VRM. The proposed algorithm was tested in two types of PO robotic control tasks, those in which either coordinates or velocities were not observable and those that require long-term memorization. Our experiments show that the proposed algorithm achieved better data efficiency and/or learned more optimal policy than other alternative approaches in tasks in which unobserved states cannot be inferred from raw observations in a simple manner.",0
"Deep reinforcement learning agents often struggle with poor performance in partially observable (PO) environments, as they must simultaneously address two challenges: extracting information from raw observations to solve the task and improving the policy. To address these issues, we present an RL algorithm for PO tasks consisting of a variational recurrent model (VRM) for environment modeling and an RL controller with access to both the environment and VRM. We evaluated the algorithm's effectiveness in two types of PO robotic control tasks: those without observable coordinates or velocities and those requiring long-term memorization. Our experiments demonstrate that the proposed algorithm outperforms other approaches in tasks where unobserved states cannot be easily inferred from raw observations, achieving better data efficiency and/or learning more optimal policies.",1
"Despite its potential to improve sample complexity versus model-free approaches, model-based reinforcement learning can fail catastrophically if the model is inaccurate. An algorithm should ideally be able to trust an imperfect model over a reasonably long planning horizon, and only rely on model-free updates when the model errors get infeasibly large. In this paper, we investigate techniques for choosing the planning horizon on a state-dependent basis, where a state's planning horizon is determined by the maximum cumulative model error around that state. We demonstrate that these state-dependent model errors can be learned with Temporal Difference methods, based on a novel approach of temporally decomposing the cumulative model errors. Experimental results show that the proposed method can successfully adapt the planning horizon to account for state-dependent model accuracy, significantly improving the efficiency of policy learning compared to model-based and model-free baselines.",0
"Model-based reinforcement learning has the potential to enhance sample complexity, but it can result in catastrophic failure if the model is inaccurate. It is essential for an algorithm to rely on an imperfect model for an extended planning horizon and switch to model-free updates only when the model errors become unmanageable. This study explores ways to determine the planning horizon on a state-by-state basis, where the maximum cumulative model error around a state determines its planning horizon. We employ Temporal Difference methods to learn state-dependent model errors, utilizing a new technique that breaks down cumulative model errors temporally. The experimental results show that our method can adjust the planning horizon to reflect state-dependent model accuracy, resulting in a significant improvement in the efficiency of policy learning compared to both model-based and model-free baselines.",1
"How much does having visual priors about the world (e.g. the fact that the world is 3D) assist in learning to perform downstream motor tasks (e.g. navigating a complex environment)? What are the consequences of not utilizing such visual priors in learning? We study these questions by integrating a generic perceptual skill set (a distance estimator, an edge detector, etc.) within a reinforcement learning framework (see Fig. 1). This skill set (""mid-level vision"") provides the policy with a more processed state of the world compared to raw images.   Our large-scale study demonstrates that using mid-level vision results in policies that learn faster, generalize better, and achieve higher final performance, when compared to learning from scratch and/or using state-of-the-art visual and non-visual representation learning methods. We show that conventional computer vision objectives are particularly effective in this regard and can be conveniently integrated into reinforcement learning frameworks. Finally, we found that no single visual representation was universally useful for all downstream tasks, hence we computationally derive a task-agnostic set of representations optimized to support arbitrary downstream tasks.",0
"The extent to which possessing pre-existing visual knowledge of the world, such as its three-dimensional nature, aids in acquiring motor skills for navigating complex environments is explored. The potential drawbacks of neglecting such visual knowledge in the learning process are also considered. To investigate these questions, we have incorporated a set of generic perceptual skills, referred to as ""mid-level vision"", including distance estimation and edge detection, into a reinforcement learning framework (refer to Fig. 1). This mid-level vision provides the policy with a more refined view of the world compared to raw images. Our comprehensive study showcases that utilizing mid-level vision leads to faster learning, better generalization, and superior final performance when contrasted with learning from scratch or using current visual and non-visual representation learning methods. We have observed that conventional computer vision objectives are especially effective and can be conveniently integrated into reinforcement learning frameworks. Furthermore, we have discovered that no individual visual representation is universally beneficial for all downstream tasks. Therefore, we have developed a task-agnostic set of representations that are optimized to support diverse downstream tasks using computational methods.",1
"While most current research in Reinforcement Learning (RL) focuses on improving the performance of the algorithms in controlled environments, the use of RL under constraints like those met in the video game industry is rarely studied. Operating under such constraints, we propose Hybrid SAC, an extension of the Soft Actor-Critic algorithm able to handle discrete, continuous and parameterized actions in a principled way. We show that Hybrid SAC can successfully solve a highspeed driving task in one of our games, and is competitive with the state-of-the-art on parameterized actions benchmark tasks. We also explore the impact of using normalizing flows to enrich the expressiveness of the policy at minimal computational cost, and identify a potential undesired effect of SAC when used with normalizing flows, that may be addressed by optimizing a different objective.",0
"Although most current research in Reinforcement Learning (RL) is geared towards improving algorithm performance in controlled environments, there is minimal exploration of using RL in constraint-heavy industries such as video games. To address this, we introduce Hybrid SAC - an extension of Soft Actor-Critic algorithm that can handle discrete, continuous, and parameterized actions in a systematic manner. Our experiments demonstrate that Hybrid SAC can effectively solve a high-speed driving task in our game and perform competitively with state-of-the-art on parameterized actions benchmark tasks. Additionally, we investigate the use of normalizing flows to increase policy expressiveness without compromising computational efficiency and highlight a potential unintended effect of SAC used with normalizing flows, which can be corrected by optimizing a different objective.",1
"In an effort to better understand the different ways in which the discount factor affects the optimization process in reinforcement learning, we designed a set of experiments to study each effect in isolation. Our analysis reveals that the common perception that poor performance of low discount factors is caused by (too) small action-gaps requires revision. We propose an alternative hypothesis that identifies the size-difference of the action-gap across the state-space as the primary cause. We then introduce a new method that enables more homogeneous action-gaps by mapping value estimates to a logarithmic space. We prove convergence for this method under standard assumptions and demonstrate empirically that it indeed enables lower discount factors for approximate reinforcement-learning methods. This in turn allows tackling a class of reinforcement-learning problems that are challenging to solve with traditional methods.",0
"To gain a better understanding of how the discount factor impacts the optimization process in reinforcement learning, we conducted experiments to isolate each effect. Our findings challenge the conventional belief that low discount factors lead to poor performance due to small action-gaps. Instead, we propose a new hypothesis that highlights the size-difference of the action-gap as the primary cause. To address this issue, we developed a new method that utilizes logarithmic space to achieve more uniform action-gaps. We prove that this method converges under standard assumptions and demonstrate through empirical evidence that it allows for lower discount factors in approximate reinforcement-learning methods. This breakthrough enables the resolution of a class of reinforcement-learning problems that were previously difficult to solve with traditional techniques.",1
"Gradient-based methods are often used for policy optimization in deep reinforcement learning, despite being vulnerable to local optima and saddle points. Although gradient-free methods (e.g., genetic algorithms or evolution strategies) help mitigate these issues, poor initialization and local optima are still concerns in highly nonconvex spaces. This paper presents a method for policy optimization based on Monte-Carlo tree search and gradient-free optimization. Our method, called Monte-Carlo tree search for policy optimization (MCTSPO), provides a better exploration-exploitation trade-off through the use of the upper confidence bound heuristic. We demonstrate improved performance on reinforcement learning tasks with deceptive or sparse reward functions compared to popular gradient-based and deep genetic algorithm baselines.",0
"Despite their vulnerability to local optima and saddle points, gradient-based methods are commonly employed for policy optimization in deep reinforcement learning. While gradient-free methods like genetic algorithms or evolution strategies can alleviate these issues, initialization quality and local optima remain problematic in highly nonconvex spaces. This paper introduces a technique for policy optimization that is based on Monte-Carlo tree search and gradient-free optimization, referred to as Monte-Carlo tree search for policy optimization (MCTSPO). By utilizing the upper confidence bound heuristic, our MCTSPO method provides a superior balance between exploration and exploitation. We demonstrate that our technique outperforms popular gradient-based and deep genetic algorithm baselines on reinforcement learning tasks with deceptive or sparse reward functions.",1
New methodologies will be needed to ensure the airspace remains safe and efficient as traffic densities rise to accommodate new unmanned operations. This paper explores how unmanned free-flight traffic may operate in dense airspace. We develop and analyze autonomous collision avoidance systems for aircraft operating in dense airspace where traditional collision avoidance systems fail. We propose a metric for quantifying the decision burden on a collision avoidance system as well as a metric for measuring the impact of the collision avoidance system on airspace. We use deep reinforcement learning to compute corrections for an existing collision avoidance approach to account for dense airspace. The results show that a corrected collision avoidance system can operate more efficiently than traditional methods in dense airspace while maintaining high levels of safety.,0
"To accommodate new unmanned operations and increasing traffic densities, innovative methodologies must be implemented to ensure the safety and efficiency of airspace. This study delves into the potential of unmanned free-flight traffic in congested airspace and develops and assesses independent collision avoidance systems for aircraft. We propose a metric to measure the workload of a collision avoidance system and its impact on airspace. Using deep reinforcement learning, we calculate corrections for an existing collision avoidance method in response to dense airspace. Our findings demonstrate that a revised collision avoidance system operates more effectively in dense airspace, while still maintaining a high standard of safety.",1
"Neural Networks are being integrated into safety critical systems, e.g., perception systems for autonomous vehicles, which require trained networks to perform safely in novel scenarios. It is challenging to verify neural networks because their decisions are not explainable, they cannot be exhaustively tested, and finite test samples cannot capture the variation across all operating conditions. Existing work seeks to train models robust to new scenarios via domain adaptation, style transfer, or few-shot learning. But these techniques fail to predict how a trained model will perform when the operating conditions differ from the testing conditions. We propose a metric, Machine Learning (ML) Dependability, that measures the network's probability of success in specified operating conditions which need not be the testing conditions. In addition, we propose the metrics Task Undependability and Harmful Undependability to distinguish network failures by their consequences. We evaluate the performance of a Neural Network agent trained using Reinforcement Learning in a simulated robot manipulation task. Our results demonstrate that we can accurately predict the ML Dependability, Task Undependability, and Harmful Undependability for operating conditions that are significantly different from the testing conditions. Finally, we design a Safety Function, using harmful failures identified during testing, that reduces harmful failures, in one example, by a factor of 700 while maintaining a high probability of success.",0
"Safety critical systems, such as perception systems for autonomous vehicles, are incorporating Neural Networks that necessitate trained networks to operate safely in new situations. However, verifying these networks is difficult because their decisions cannot be explained, exhaustive testing is not possible, and limited test samples cannot account for all operational conditions. Previous methods have tried to train models that are robust to new situations, but they are unable to anticipate how a trained model will behave under different operating conditions. To address this issue, we propose Machine Learning (ML) Dependability, a metric that estimates the network's probability of success under specified operating conditions, regardless of whether they correspond to the testing conditions. We also introduce the metrics Task Undependability and Harmful Undependability to differentiate network failures based on their outcomes. We assess the performance of a Neural Network agent trained using Reinforcement Learning in a simulated robot manipulation task. Our findings show that we can accurately anticipate ML Dependability, Task Undependability, and Harmful Undependability for operating conditions that are considerably different from the testing conditions. Finally, we create a Safety Function that reduces harmful failures by a factor of 700 while maintaining a high probability of success, using harmful failures detected during testing.",1
"This paper presents a framework for efficiently learning feature selection policies which use less features to reach a high classification precision on large unstructured data. It uses a Deep Convolutional Autoencoder (DCAE) for learning compact feature spaces, in combination with recently-proposed Reinforcement Learning (RL) algorithms as Double DQN and Retrace.",0
"The focus of this article is to introduce a structure for effectively acquiring feature selection policies that employ minimal features in achieving precise classification outcomes when dealing with vast unorganized data. The methodology integrates the usage of a Deep Convolutional Autoencoder (DCAE) for acquiring compressed feature spaces, along with newly suggested Reinforcement Learning (RL) algorithms such as Double DQN and Retrace.",1
"What is a good exploration strategy for an agent that interacts with an environment in the absence of external rewards? Ideally, we would like to get a policy driving towards a uniform state-action visitation (highly exploring) in a minimum number of steps (fast mixing), in order to ease efficient learning of any goal-conditioned policy later on. Unfortunately, it is remarkably arduous to directly learn an optimal policy of this nature. In this paper, we propose a novel surrogate objective for learning highly exploring and fast mixing policies, which focuses on maximizing a lower bound to the entropy of the steady-state distribution induced by the policy. In particular, we introduce three novel lower bounds, that lead to as many optimization problems, that tradeoff the theoretical guarantees with computational complexity. Then, we present a model-based reinforcement learning algorithm, IDE$^{3}$AL, to learn an optimal policy according to the introduced objective. Finally, we provide an empirical evaluation of this algorithm on a set of hard-exploration tasks.",0
"What exploration strategy should an agent use when it interacts with an environment without external rewards? The goal is to achieve a policy that encourages frequent visits to various state-action pairs while minimizing the number of steps required for efficient learning of any goal-conditioned policy. However, it is challenging to directly learn an optimal policy of this nature. This paper proposes a new surrogate objective for developing highly exploring and fast mixing policies that focuses on maximizing a lower bound to the entropy of the steady-state distribution induced by the policy. Three novel lower bounds are introduced, each with its own optimization problem, which balance theoretical guarantees with computational complexity. The IDE$^{3}$AL model-based reinforcement learning algorithm is then presented to learn an optimal policy based on the proposed objective. Finally, the algorithm's performance is evaluated empirically on a set of challenging exploration tasks.",1
"In this article, we explore the feasibility of applying proximal policy optimization, a state-of-the-art deep reinforcement learning algorithm for continuous control tasks, on the dual-objective problem of controlling an underactuated autonomous surface vehicle to follow an a priori known path while avoiding collisions with non-moving obstacles along the way. The artificial intelligent agent, which is equipped with multiple rangefinder sensors for obstacle detection, is trained and evaluated in a challenging, stochastically generated simulation environment based on the OpenAI gym python toolkit. Notably, the agent is provided with real-time insight into its own reward function, allowing it to dynamically adapt its guidance strategy. Depending on its strategy, which ranges from radical path-adherence to radical obstacle avoidance, the trained agent achieves an episodic success rate between 84 and 100%.",0
"This article delves into the possibility of utilizing proximal policy optimization, a cutting-edge deep reinforcement learning algorithm for continuous control tasks, to tackle the dual-objective challenge of directing an underactuated autonomous surface vehicle to track a predetermined course while circumventing stationary obstructions en route. The AI agent, which employs numerous rangefinder sensors for detecting obstacles, is educated and assessed in a complex, randomly produced simulation environment founded on the OpenAI gym python toolkit. Notably, the agent obtains ongoing comprehension of its own reward mechanism, empowering it to adjust its guidance technique dynamically. Based on its approach, which ranges from stringent path adherence to rigorous obstacle avoidance, the trained agent accomplishes an episodic triumph rate of 84 to 100%.",1
"Global Autoregressive Models (GAMs) are a recent proposal [Parshakova et al., CoNLL 2019] for exploiting global properties of sequences for data-efficient learning of seq2seq models. In the first phase of training, an Energy-Based model (EBM) over sequences is derived. This EBM has high representational power, but is unnormalized and cannot be directly exploited for sampling. To address this issue [Parshakova et al., CoNLL 2019] proposes a distillation technique, which can only be applied under limited conditions. By relating this problem to Policy Gradient techniques in RL, but in a \emph{distributional} rather than \emph{optimization} perspective, we propose a general approach applicable to any sequential EBM. Its effectiveness is illustrated on GAM-based experiments.",0
"Recently proposed by Parshakova et al. (CoNLL 2019), Global Autoregressive Models (GAMs) are a method that takes advantage of global properties of sequences to improve data-efficient learning of seq2seq models. During the first stage of training, an Energy-Based Model (EBM) is derived, which has significant representational power but cannot be used for sampling due to its lack of normalization. To address this limitation, Parshakova et al. (CoNLL 2019) proposed a distillation technique that has limited applicability. In this work, we propose a general approach that can be applied to any sequential EBM by framing the problem in a distributional perspective, drawing on Policy Gradient techniques in RL. We demonstrate the effectiveness of this approach through experiments conducted using GAMs.",1
"In this work, we introduce a new method for imitation learning from video demonstrations. Our method, Relational Mimic (RM), improves on previous visual imitation learning methods by combining generative adversarial networks and relational learning. RM is flexible and can be used in conjunction with other recent advances in generative adversarial imitation learning to better address the need for more robust and sample-efficient approaches. In addition, we introduce a new neural network architecture that improves upon the previous state-of-the-art in reinforcement learning and illustrate how increasing the relational reasoning capabilities of the agent enables the latter to achieve increasingly higher performance in a challenging locomotion task with pixel inputs. Finally, we study the effects and contributions of relational learning in policy evaluation, policy improvement and reward learning through ablation studies.",0
"This paper presents a novel technique for learning by imitation from videos called Relational Mimic (RM). The proposed approach is an advancement over existing techniques by combining generative adversarial networks with relational learning to improve the robustness and efficiency of the learning process. The flexibility of RM allows it to be used in conjunction with other recent developments in generative adversarial imitation learning. Moreover, a new neural network architecture is introduced that surpasses the previous state-of-the-art in reinforcement learning. By enhancing the agent's relational reasoning abilities, it achieves better performance in a challenging locomotion task with pixel inputs. The paper concludes with ablation studies that examine the role of relational learning in policy evaluation, policy improvement, and reward learning.",1
"The neural linear model is a simple adaptive Bayesian linear regression method that has recently been used in a number of problems ranging from Bayesian optimization to reinforcement learning. Despite its apparent successes in these settings, to the best of our knowledge there has been no systematic exploration of its capabilities on simple regression tasks. In this work we characterize these on the UCI datasets, a popular benchmark for Bayesian regression models, as well as on the recently introduced UCI ""gap"" datasets, which are better tests of out-of-distribution uncertainty. We demonstrate that the neural linear model is a simple method that shows generally good performance on these tasks, but at the cost of requiring good hyperparameter tuning.",0
"The neural linear model is a Bayesian regression technique that has been employed in various domains, such as Bayesian optimization and reinforcement learning. Although it has exhibited positive results in these domains, there has not been a comprehensive investigation of its ability to perform on straightforward regression tasks. This study aims to evaluate its performance on the widely-used UCI datasets, which are a standard metric for Bayesian regression models, as well as on the UCI ""gap"" datasets, which are more challenging for out-of-distribution uncertainty. Our findings reveal that the neural linear model is an uncomplicated method that performs well on these tasks, but it requires meticulous hyperparameter tuning.",1
"Goal-oriented reinforcement learning has recently been a practical framework for robotic manipulation tasks, in which an agent is required to reach a certain goal defined by a function on the state space. However, the sparsity of such reward definition makes traditional reinforcement learning algorithms very inefficient. Hindsight Experience Replay (HER), a recent advance, has greatly improved sample efficiency and practical applicability for such problems. It exploits previous replays by constructing imaginary goals in a simple heuristic way, acting like an implicit curriculum to alleviate the challenge of sparse reward signal. In this paper, we introduce Hindsight Goal Generation (HGG), a novel algorithmic framework that generates valuable hindsight goals which are easy for an agent to achieve in the short term and are also potential for guiding the agent to reach the actual goal in the long term. We have extensively evaluated our goal generation algorithm on a number of robotic manipulation tasks and demonstrated substantially improvement over the original HER in terms of sample efficiency.",0
"Recently, goal-oriented reinforcement learning has become a practical approach for robotic manipulation tasks that require an agent to achieve a specific goal defined by a function in the state space. However, traditional reinforcement learning algorithms are inefficient due to the sparse reward definition. To address this problem, a recent advancement called Hindsight Experience Replay (HER) has significantly improved the sample efficiency and practical applicability of such tasks by constructing imaginary goals in a simple heuristic way. In this paper, we introduce a novel algorithmic framework called Hindsight Goal Generation (HGG), which generates valuable hindsight goals that are easy for an agent to achieve in the short term and have the potential to guide the agent to reach the actual goal in the long term. We extensively evaluated our goal generation algorithm on various robotic manipulation tasks and demonstrated a substantial improvement over the original HER in terms of sample efficiency.",1
"The main goal of this thesis is to point out that the bias-variance tradeoff is not always true (e.g. in neural networks). We advocate for this lack of universality to be acknowledged in textbooks and taught in introductory courses that cover the tradeoff. We first review the history of the bias-variance tradeoff, its prevalence in textbooks, and some of the main claims made about the bias-variance tradeoff. Through extensive experiments and analysis, we show a lack of a bias-variance tradeoff in neural networks when increasing network width. Our findings seem to contradict the claims of the landmark work by Geman et al. (1992). Motivated by this contradiction, we revisit the experimental measurements in Geman et al. (1992). We discuss that there was never strong evidence for a tradeoff in neural networks when varying the number of parameters. We observe a similar phenomenon beyond supervised learning, with a set of deep reinforcement learning experiments. We argue that textbook and lecture revisions are in order to convey this nuanced modern understanding of the bias-variance tradeoff.",0
"The primary objective of this thesis is to challenge the idea that the bias-variance tradeoff is always applicable, particularly in neural networks. We believe that this perspective should be recognized in introductory courses and textbooks that cover this concept. Initially, we explore the history of the bias-variance tradeoff, its prevalence in educational materials, and the principal arguments made about it. By conducting extensive experiments and analysis, we demonstrate that there is no bias-variance tradeoff in neural networks when network width is increased. Our findings appear to contradict the assertions made in Geman et al.'s seminal work (1992). As a result, we review the experimental measurements in Geman et al. (1992) and conclude that there was never significant evidence for a tradeoff in neural networks when varying the number of parameters. We also note a similar trend in deep reinforcement learning experiments, indicating that a nuanced understanding of the bias-variance tradeoff should be conveyed in textbooks and lectures.",1
"This paper tackles unpaired image enhancement, a task of learning a mapping function which transforms input images into enhanced images in the absence of input-output image pairs. Our method is based on generative adversarial networks (GANs), but instead of simply generating images with a neural network, we enhance images utilizing image editing software such as Adobe Photoshop for the following three benefits: enhanced images have no artifacts, the same enhancement can be applied to larger images, and the enhancement is interpretable. To incorporate image editing software into a GAN, we propose a reinforcement learning framework where the generator works as the agent that selects the software's parameters and is rewarded when it fools the discriminator. Our framework can use high-quality non-differentiable filters present in image editing software, which enables image enhancement with high performance. We apply the proposed method to two unpaired image enhancement tasks: photo enhancement and face beautification. Our experimental results demonstrate that the proposed method achieves better performance, compared to the performances of the state-of-the-art methods based on unpaired learning.",0
"The main focus of this paper is on improving unpaired images by developing a mapping function without any input-output image pairs. The method employed for this task is based on generative adversarial networks (GANs) which use image editing software like Adobe Photoshop to enhance images. This approach has several advantages such as the absence of any artifacts, the ability to apply enhancement to larger images, and the interpretability of the enhancements. To integrate image editing software into GANs, a reinforcement learning framework is proposed where the generator acts as an agent that selects software parameters and receives rewards when it fools the discriminator. This framework uses high-quality non-differentiable filters present in image editing software for high-performance image enhancement. The proposed method is applied to two unpaired image enhancement tasks: photo enhancement and face beautification. The experimental results show that this method outperforms the state-of-the-art methods based on unpaired learning.",1
"Recently, dynamic inference has emerged as a promising way to reduce the computational cost of deep convolutional neural network (CNN). In contrast to static methods (e.g. weight pruning), dynamic inference adaptively adjusts the inference process according to each input sample, which can considerably reduce the computational cost on ""easy"" samples while maintaining the overall model performance. In this paper, we introduce a general framework, S2DNAS, which can transform various static CNN models to support dynamic inference via neural architecture search. To this end, based on a given CNN model, we first generate a CNN architecture space in which each architecture is a multi-stage CNN generated from the given model using some predefined transformations. Then, we propose a reinforcement learning based approach to automatically search for the optimal CNN architecture in the generated space. At last, with the searched multi-stage network, we can perform dynamic inference by adaptively choosing a stage to evaluate for each sample. Unlike previous works that introduce irregular computations or complex controllers in the inference or re-design a CNN model from scratch, our method can generalize to most of the popular CNN architectures and the searched dynamic network can be directly deployed using existing deep learning frameworks in various hardware devices.",0
"Dynamic inference has emerged as a cost-effective method for deep convolutional neural network (CNN). Unlike static methods such as weight pruning, dynamic inference adjusts the inference process based on input samples, reducing computational cost on ""easy"" samples while maintaining model performance. The S2DNAS framework transforms static CNN models to support dynamic inference through neural architecture search. The framework generates a multi-stage CNN architecture space from a given model using predefined transformations and uses a reinforcement learning approach to identify the optimal architecture. The searched multi-stage network enables adaptive selection of evaluation stages for each sample. Unlike previous approaches that introduce irregular computations or complex controllers, S2DNAS can generalize to most popular CNN architectures and be directly deployed using existing deep learning frameworks across various hardware devices.",1
"Model-free deep reinforcement learning (RL) algorithms have been widely used for a range of complex control tasks. However, slow convergence and sample inefficiency remain challenging problems in RL, especially when handling continuous and high-dimensional state spaces. To tackle this problem, we propose a general acceleration method for model-free, off-policy deep RL algorithms by drawing the idea underlying regularized Anderson acceleration (RAA), which is an effective approach to accelerating the solving of fixed point problems with perturbations. Specifically, we first explain how policy iteration can be applied directly with Anderson acceleration. Then we extend RAA to the case of deep RL by introducing a regularization term to control the impact of perturbation induced by function approximation errors. We further propose two strategies, i.e., progressive update and adaptive restart, to enhance the performance. The effectiveness of our method is evaluated on a variety of benchmark tasks, including Atari 2600 and MuJoCo. Experimental results show that our approach substantially improves both the learning speed and final performance of state-of-the-art deep RL algorithms.",0
"Deep reinforcement learning (RL) algorithms without models have been extensively utilized for intricate control responsibilities. Nonetheless, RL encounters difficulties with slow convergence and sample inefficiency, particularly in the handling of continuous and high-dimensional state spaces. We present a general acceleration procedure for off-policy, model-free deep RL algorithms to confront this issue. The method is based on the idea of regularized Anderson acceleration (RAA), which is an efficient means of accelerating the resolution of fixed point problems with perturbations. We clarify how policy iteration can be instantly applied with Anderson acceleration and extend RAA to the deep RL situation by adding a regularization term that controls the influence of perturbation caused by function approximation inaccuracies. We further suggest two strategies, i.e., progressive update and adaptive restart, to enhance performance. We tested the efficacy of our approach on a variety of benchmark tasks, including Atari 2600 and MuJoCo. Experimentation results demonstrate that our approach significantly enhances the learning speed and final performance of state-of-the-art deep RL algorithms.",1
"This paper investigates the intriguing question of whether we can create learning algorithms that automatically generate training data, learning environments, and curricula in order to help AI agents rapidly learn. We show that such algorithms are possible via Generative Teaching Networks (GTNs), a general approach that is, in theory, applicable to supervised, unsupervised, and reinforcement learning, although our experiments only focus on the supervised case. GTNs are deep neural networks that generate data and/or training environments that a learner (e.g. a freshly initialized neural network) trains on for a few SGD steps before being tested on a target task. We then differentiate through the entire learning process via meta-gradients to update the GTN parameters to improve performance on the target task. GTNs have the beneficial property that they can theoretically generate any type of data or training environment, making their potential impact large. This paper introduces GTNs, discusses their potential, and showcases that they can substantially accelerate learning. We also demonstrate a practical and exciting application of GTNs: accelerating the evaluation of candidate architectures for neural architecture search (NAS), which is rate-limited by such evaluations, enabling massive speed-ups in NAS. GTN-NAS improves the NAS state of the art, finding higher performing architectures when controlling for the search proposal mechanism. GTN-NAS also is competitive with the overall state of the art approaches, which achieve top performance while using orders of magnitude less computation than typical NAS methods. Speculating forward, GTNs may represent a first step toward the ambitious goal of algorithms that generate their own training data and, in doing so, open a variety of interesting new research questions and directions.",0
"The paper explores the possibility of developing learning algorithms that can automatically generate training data, learning environments, and curricula to aid in the rapid learning of AI agents. Using Generative Teaching Networks (GTNs), a general method applicable to supervised, unsupervised, and reinforcement learning, the paper shows that such algorithms are feasible. GTNs are deep neural networks that can generate data and/or training environments for a learner to train on before being tested on a target task. The paper showcases that GTNs can substantially accelerate learning and can be used to improve the state of the art in neural architecture search. The potential impact of GTNs is significant, as they can theoretically generate any type of data or training environment. GTNs represent a promising step towards algorithms that generate their own training data, which could lead to further research questions and directions.",1
"We show by counterexample that policy-gradient algorithms have no guarantees of even local convergence to Nash equilibria in continuous action and state space multi-agent settings. To do so, we analyze gradient-play in N-player general-sum linear quadratic games, a classic game setting which is recently emerging as a benchmark in the field of multi-agent learning. In such games the state and action spaces are continuous and global Nash equilibria can be found be solving coupled Ricatti equations. Further, gradient-play in LQ games is equivalent to multi agent policy-gradient. We first show that these games are surprisingly not convex games. Despite this, we are still able to show that the only critical points of the gradient dynamics are global Nash equilibria. We then give sufficient conditions under which policy-gradient will avoid the Nash equilibria, and generate a large number of general-sum linear quadratic games that satisfy these conditions. In such games we empirically observe the players converging to limit cycles for which the time average does not coincide with a Nash equilibrium. The existence of such games indicates that one of the most popular approaches to solving reinforcement learning problems in the classic reinforcement learning setting has no local guarantee of convergence in multi-agent settings. Further, the ease with which we can generate these counterexamples suggests that such situations are not mere edge cases and are in fact quite common.",0
"Through a counterexample, we demonstrate that policy-gradient algorithms do not offer assurances of even local convergence to Nash equilibria in multi-agent settings with continuous state and action spaces. Our analysis focuses on N-player general-sum linear quadratic games, which have recently become a standard benchmark in the field of multi-agent learning. Despite the fact that these games have continuous state and action spaces, global Nash equilibria can be found by solving coupled Ricatti equations. Gradient-play in LQ games is equivalent to multi-agent policy-gradient. We prove that these games are not convex, but we show that the only critical points of the gradient dynamics are global Nash equilibria. We also provide sufficient conditions under which policy-gradient avoids Nash equilibria, and we generate numerous general-sum linear quadratic games that meet these criteria. In these games, we observe players converging to limit cycles where the time average does not align with a Nash equilibrium. The existence of such games implies that one of the most popular methods for solving reinforcement learning problems in the classic reinforcement learning setting lacks a local guarantee of convergence in multi-agent settings. Furthermore, our ability to generate these counterexamples with ease implies that such situations are not rare and may be quite common.",1
"This paper tackles a new problem setting: reinforcement learning with pixel-wise rewards (pixelRL) for image processing. After the introduction of the deep Q-network, deep RL has been achieving great success. However, the applications of deep reinforcement learning (RL) for image processing are still limited. Therefore, we extend deep RL to pixelRL for various image processing applications. In pixelRL, each pixel has an agent, and the agent changes the pixel value by taking an action. We also propose an effective learning method for pixelRL that significantly improves the performance by considering not only the future states of the own pixel but also those of the neighbor pixels. The proposed method can be applied to some image processing tasks that require pixel-wise manipulations, where deep RL has never been applied. Besides, it is possible to visualize what kind of operation is employed for each pixel at each iteration, which would help us understand why and how such an operation is chosen. We also believe that our technology can enhance the explainability and interpretability of the deep neural networks. In addition, because the operations executed at each pixels are visualized, we can change or modify the operations if necessary. We apply the proposed method to a variety of image processing tasks: image denoising, image restoration, local color enhancement, and saliency-driven image editing. Our experimental results demonstrate that the proposed method achieves comparable or better performance, compared with the state-of-the-art methods based on supervised learning. The source code is available on https://github.com/rfuruta/pixelRL.",0
"This paper introduces a new problem setting for image processing, namely reinforcement learning with pixel-wise rewards (pixelRL). While deep reinforcement learning (RL) has been successful since the introduction of the deep Q-network, its applications in image processing remain limited. Thus, we propose extending deep RL to pixelRL for various image processing tasks. In this approach, every pixel has its agent, which modifies the pixel value through actions. We also present an effective learning method for pixelRL that enhances performance by considering not only the future states of the pixel but also those of the neighboring pixels. This proposed method can be applied to image processing tasks that require pixel-wise manipulations, which have never been attempted through deep RL. Moreover, the proposed technology can enhance the interpretability and explainability of deep neural networks. By visualizing the operations employed for each pixel at each iteration, we can understand why and how such operations are chosen and modify them as needed. We apply the proposed method to various image processing tasks, such as image denoising, image restoration, local color enhancement, and saliency-driven image editing. The experimental results show that the proposed method outperforms the state-of-the-art methods based on supervised learning. The source code is available on https://github.com/rfuruta/pixelRL.",1
"Determining what experience to generate to best facilitate learning (i.e. exploration) is one of the distinguishing features and open challenges in reinforcement learning. The advent of distributed agents that interact with parallel instances of the environment has enabled larger scales and greater flexibility, but has not removed the need to tune exploration to the task, because the ideal data for the learning algorithm necessarily depends on its process of learning. We propose to dynamically adapt the data generation by using a non-stationary multi-armed bandit to optimize a proxy of the learning progress. The data distribution is controlled by modulating multiple parameters of the policy (such as stochasticity, consistency or optimism) without significant overhead. The adaptation speed of the bandit can be increased by exploiting the factored modulation structure. We demonstrate on a suite of Atari 2600 games how this unified approach produces results comparable to per-task tuning at a fraction of the cost.",0
"Reinforcement learning faces the challenge of determining the best experience to facilitate exploration and learning. Though distributed agents have increased scalability and flexibility, exploration still needs to be tailored to the task. To address this, we suggest dynamically adjusting data generation with a non-stationary multi-armed bandit to optimize learning progress. The policy's parameters, such as stochasticity and consistency, can modulate the data distribution without significant overhead. The bandit's adaptation speed can be increased by leveraging its factored modulation structure. Our approach achieves comparable results to per-task tuning at a lower cost, as demonstrated on a set of Atari 2600 games.",1
"Multi-agent reinforcement learning has been successfully applied to a number of challenging problems. Despite these empirical successes, theoretical understanding of different algorithms is lacking, primarily due to the curse of dimensionality caused by the exponential growth of the state-action space with the number of agents. We study a fundamental problem of multi-agent linear quadratic regulator in a setting where the agents are partially exchangeable. In this setting, we develop a hierarchical actor-critic algorithm, whose computational complexity is independent of the total number of agents, and prove its global linear convergence to the optimal policy. As linear quadratic regulators are often used to approximate general dynamic systems, this paper provided an important step towards better understanding of general hierarchical mean-field multi-agent reinforcement learning.",0
"Although multi-agent reinforcement learning has proven successful in solving challenging problems, a lack of theoretical understanding of different algorithms persists, mainly due to the exponential increase of the state-action space as more agents are added. To address this issue, we focused on the multi-agent linear quadratic regulator problem, where agents are partially exchangeable. We developed a hierarchical actor-critic algorithm that achieves global linear convergence to the optimal policy and has computational complexity that is independent of the number of agents. Our work is significant because linear quadratic regulators are commonly used to approximate general dynamic systems, and it provides an essential contribution to the understanding of general hierarchical mean-field multi-agent reinforcement learning.",1
"A simple approach to obtaining uncertainty-aware neural networks for regression is to do Bayesian linear regression (BLR) on the representation from the last hidden layer. Recent work [Riquelme et al., 2018, Azizzadenesheli et al., 2018] indicates that the method is promising, though it has been limited to homoscedastic noise. In this paper, we propose a novel variation that enables the method to flexibly model heteroscedastic noise. The method is benchmarked against two prominent alternative methods on a set of standard datasets, and finally evaluated as an uncertainty-aware model in model-based reinforcement learning. Our experiments indicate that the method is competitive with standard ensembling, and ensembles of BLR outperforms the methods we compared to.",0
"To achieve neural networks that are aware of uncertainty for regression, a straightforward approach is to perform Bayesian linear regression (BLR) on the representation derived from the last hidden layer. Although recent studies by Riquelme et al. (2018) and Azizzadenesheli et al. (2018) have shown the method to be promising, it has been limited to handling homoscedastic noise. In this research paper, we introduce a new variation that allows the method to flexibly accommodate heteroscedastic noise. We benchmarked the proposed method against two other prominent alternatives using standard datasets and evaluated it as an uncertainty-aware model in model-based reinforcement learning. Our experiments reveal that the method is on a par with standard ensembling, and ensembles of BLR outperform the methods we compared to.",1
"On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.",0
"In April 2019, OpenAI Five made history by defeating the esports world champions in the game of Dota 2. This game presents unique obstacles for AI systems, such as long-term planning, incomplete information, and complicated and uninterrupted state-action spaces, all of which will become more significant for more advanced AI systems. To achieve its success, OpenAI Five utilized existing reinforcement learning techniques, which were scaled to learn from batches of around 2 million frames every 2 seconds. A distributed training system and continual training tools were created to enable OpenAI Five to be trained for a period of 10 months. By beating the Dota 2 world champion (Team OG), OpenAI Five has shown that self-play reinforcement learning can attain superhuman performance on a challenging task.",1
"Reinforcement learning, evolutionary algorithms and imitation learning are three principal methods to deal with continuous control tasks. Reinforcement learning is sample efficient, yet sensitive to hyper-parameters setting and needs efficient exploration; Evolutionary algorithms are stable, but with low sample efficiency; Imitation learning is both sample efficient and stable, however it requires the guidance of expert data. In this paper, we propose Recruitment-imitation Mechanism (RIM) for evolutionary reinforcement learning, a scalable framework that combines advantages of the three methods mentioned above. The core of this framework is a dual-actors and single critic reinforcement learning agent. This agent can recruit high-fitness actors from the population of evolutionary algorithms, which instructs itself to learn from experience replay buffer. At the same time, low-fitness actors in the evolutionary population can imitate behavior patterns of the reinforcement learning agent and improve their adaptability. Reinforcement and imitation learners in this framework can be replaced with any off-policy actor-critic reinforcement learner or data-driven imitation learner. We evaluate RIM on a series of benchmarks for continuous control tasks in Mujoco. The experimental results show that RIM outperforms prior evolutionary or reinforcement learning methods. The performance of RIM's components is significantly better than components of previous evolutionary reinforcement learning algorithm, and the recruitment using soft update enables reinforcement learning agent to learn faster than that using hard update.",0
"Three primary methods exist to address continuous control tasks, which include reinforcement learning, evolutionary algorithms, and imitation learning. Reinforcement learning is efficient in sampling, but it is sensitive to hyper-parameter adjustment and necessitates efficient exploration. Evolutionary algorithms are stable, but they exhibit low sample efficiency. Imitation learning is stable and efficient in sampling; however, it requires expert data guidance. This paper proposes the Recruitment-imitation Mechanism (RIM) for evolutionary reinforcement learning, a scalable framework that combines the strengths of the three methods mentioned above. A dual-actors and single critic reinforcement learning agent forms the core of this framework, which can recruit high-fitness actors from the population of evolutionary algorithms and learn from an experience replay buffer. Meanwhile, low-fitness actors in the evolutionary population can imitate the reinforcement learning agent's behavior patterns to enhance their adaptability. The reinforcement and imitation learners in this framework can be replaced with any off-policy actor-critic reinforcement learner or data-driven imitation learner. We evaluated RIM on various Mujoco benchmarks for continuous control tasks. The experimental results demonstrate that RIM outperforms previous evolutionary or reinforcement learning methods. Additionally, the performance of RIM's components is significantly better than that of the previous evolutionary reinforcement learning algorithm, and the recruitment using soft update enables the reinforcement learning agent to learn faster than that using hard update.",1
"We study the problem of off-policy evaluation (OPE) in Reinforcement Learning (RL), where the aim is to estimate the performance of a new policy given historical data that may have been generated by a different policy, or policies. In particular, we introduce a novel doubly-robust estimator for the OPE problem in RL, based on the Targeted Maximum Likelihood Estimation principle from the statistical causal inference literature. We also introduce several variance reduction techniques that lead to impressive performance gains in off-policy evaluation. We show empirically that our estimator uniformly wins over existing off-policy evaluation methods across multiple RL environments and various levels of model misspecification. Finally, we further the existing theoretical analysis of estimators for the RL off-policy estimation problem by showing their $O_P(1/\sqrt{n})$ rate of convergence and characterizing their asymptotic distribution.",0
"Our research delves into the issue of off-policy evaluation (OPE) in Reinforcement Learning (RL), which involves estimating the effectiveness of a new policy using historical data that may have been produced by different policies. We propose a new doubly-robust estimator for the OPE problem in RL, which is based on the Targeted Maximum Likelihood Estimation principle from the statistical causal inference literature. Additionally, we introduce multiple techniques for reducing variance that result in impressive performance improvements in off-policy evaluation. Our empirical results demonstrate that our estimator outperforms existing OPE methods consistently across various RL environments and model misspecification levels. Furthermore, we enhance the current theoretical analysis of estimators for the RL off-policy estimation problem by demonstrating their $O_P(1/\sqrt{n})$ convergence rate and characterizing their asymptotic distribution.",1
"The exploration mechanism used by a Deep Reinforcement Learning (RL) agent plays a key role in determining its sample efficiency. Thus, improving over random exploration is crucial to solve long-horizon tasks with sparse rewards. We propose to leverage an ensemble of partial solutions as teachers that guide the agent's exploration with action suggestions throughout training. While the setup of learning with teachers has been previously studied, our proposed approach - Actor-Critic with Teacher Ensembles (AC-Teach) - is the first to work with an ensemble of suboptimal teachers that may solve only part of the problem or contradict other each other, forming a unified algorithmic solution that is compatible with a broad range of teacher ensembles. AC-Teach leverages a probabilistic representation of the expected outcome of the teachers' and student's actions to direct exploration, reduce dithering, and adapt to the dynamically changing quality of the learner. We evaluate a variant of AC-Teach that guides the learning of a Bayesian DDPG agent on three tasks - path following, robotic pick and place, and robotic cube sweeping using a hook - and show that it improves largely on sampling efficiency over a set of baselines, both for our target scenario of unconstrained suboptimal teachers and for easier setups with optimal or single teachers. Additional results and videos at https://sites.google.com/view/acteach/home.",0
"The effectiveness of a Deep Reinforcement Learning (RL) agent relies heavily on its exploration mechanism, particularly when dealing with long-term tasks with sparse rewards. To address this, we propose a method called Actor-Critic with Teacher Ensembles (AC-Teach), which utilizes a group of imperfect partial solutions as teachers to guide the agent's exploration during training. Unlike previous learning-with-teachers approaches, AC-Teach is the first to work with a collection of suboptimal teachers that may only solve a part of the problem or even contradict each other, resulting in a comprehensive algorithmic solution that can accommodate a wide range of teacher ensembles. AC-Teach employs a probabilistic representation to direct exploration and reduce dithering, as well as to adapt to the learner's changing quality. We evaluated a version of AC-Teach on three tasks - path following, robotic pick and place, and robotic cube sweeping using a hook - and demonstrated that it significantly enhances sampling efficiency compared to several baselines, not only for our intended scenario of unconstrained suboptimal teachers but also for simpler setups with optimal or single teachers. For more information and videos, please visit https://sites.google.com/view/acteach/home.",1
"We propose a new benchmark environment for evaluating Reinforcement Learning (RL) algorithms: the PlayStation Learning Environment (PSXLE), a PlayStation emulator modified to expose a simple control API that enables rich game-state representations. We argue that the PlayStation serves as a suitable progression for agent evaluation and propose a framework for such an evaluation. We build an action-driven abstraction for a PlayStation game with support for the OpenAI Gym interface and demonstrate its use by running OpenAI Baselines.",0
"Our proposal is to introduce a novel benchmark environment, the PlayStation Learning Environment (PSXLE), which modifies a PlayStation emulator to provide a straightforward control API that facilitates intricate game-state representations for evaluating Reinforcement Learning (RL) algorithms. Our position is that the PlayStation constitutes a fitting advancement for agent evaluation, and we suggest a framework for such an evaluation. We create an action-oriented abstraction for a PlayStation game that supports the OpenAI Gym interface and exemplify its usage through the execution of OpenAI Baselines.",1
"Trust region policy optimization (TRPO) is a popular and empirically successful policy search algorithm in Reinforcement Learning (RL) in which a surrogate problem, that restricts consecutive policies to be 'close' to one another, is iteratively solved. Nevertheless, TRPO has been considered a heuristic algorithm inspired by Conservative Policy Iteration (CPI). We show that the adaptive scaling mechanism used in TRPO is in fact the natural ""RL version"" of traditional trust-region methods from convex analysis. We first analyze TRPO in the planning setting, in which we have access to the model and the entire state space. Then, we consider sample-based TRPO and establish $\tilde O(1/\sqrt{N})$ convergence rate to the global optimum. Importantly, the adaptive scaling mechanism allows us to analyze TRPO in regularized MDPs for which we prove fast rates of $\tilde O(1/N)$, much like results in convex optimization. This is the first result in RL of better rates when regularizing the instantaneous cost or reward.",0
"Trust region policy optimization (TRPO) is a well-known and effective algorithm for policy search in Reinforcement Learning (RL). It involves solving a surrogate problem that enforces policies to be 'close' to one another. Despite being considered a heuristic algorithm inspired by Conservative Policy Iteration (CPI), we demonstrate that the adaptive scaling mechanism used in TRPO is actually the natural ""RL version"" of trust-region methods from convex analysis. Our analysis first considers TRPO in the planning setting, where the model and entire state space are available. We then extend our analysis to sample-based TRPO and establish a convergence rate of $\tilde O(1/\sqrt{N})$ to the global optimum. The adaptive scaling mechanism also enables us to analyze TRPO in regularized MDPs, where we prove fast convergence rates of $\tilde O(1/N)$. This is a groundbreaking result in RL as it demonstrates better rates when regularizing the instantaneous cost or reward, which is similar to results in convex optimization.",1
"Markov Decision Processes (MDPs), the mathematical framework underlying most algorithms in Reinforcement Learning (RL), are often used in a way that wrongfully assumes that the state of an agent's environment does not change during action selection. As RL systems based on MDPs begin to find application in real-world safety critical situations, this mismatch between the assumptions underlying classical MDPs and the reality of real-time computation may lead to undesirable outcomes. In this paper, we introduce a new framework, in which states and actions evolve simultaneously and show how it is related to the classical MDP formulation. We analyze existing algorithms under the new real-time formulation and show why they are suboptimal when used in real-time. We then use those insights to create a new algorithm Real-Time Actor-Critic (RTAC) that outperforms the existing state-of-the-art continuous control algorithm Soft Actor-Critic both in real-time and non-real-time settings. Code and videos can be found at https://github.com/rmst/rtrl.",0
"Most algorithms in Reinforcement Learning (RL) rely on Markov Decision Processes (MDPs) as their mathematical foundation. However, MDPs are often used incorrectly by assuming that the environment's state remains constant during action selection. This assumption can lead to undesirable outcomes when applying RL systems based on MDPs in safety-critical real-world scenarios. This paper presents a new framework that accounts for the simultaneous evolution of states and actions, which is related to the traditional MDP formulation. By analyzing existing algorithms under this new real-time formulation, we demonstrate their suboptimal performance and use this knowledge to create a superior algorithm, Real-Time Actor-Critic (RTAC). Our new algorithm outperforms the state-of-the-art continuous control algorithm Soft Actor-Critic in both real-time and non-real-time settings. For code and videos, please visit https://github.com/rmst/rtrl.",1
"An efficient inverse reinforcement learning for generating trajectories is proposed based of 2D and 3D activity forecasting. We modify reward function with $L_p$ norm and propose convolution into value iteration steps, which is called convolutional value iteration. Experimental results with seabird trajectories (43 for training and 10 for test), our method is best in terms of MHD error and performs fastest. Generated trajectories for interpolating missing parts of trajectories look much similar to real seabird trajectories than those by the previous works.",0
"A novel approach to generate trajectories through efficient inverse reinforcement learning is presented, utilizing 2D and 3D activity forecasting. Our proposed method involves modifying the reward function using $L_p$ norm and implementing convolution into value iteration steps, referred to as convolutional value iteration. Our experimental results using seabird trajectories, which comprised of 43 for training and 10 for testing, demonstrate that our approach outperforms other methods in terms of MHD error and speed. Moreover, the generated trajectories for interpolating missing parts of the original trajectories exhibit greater similarity to real seabird trajectories compared to previous works.",1
"With the rapidly growing expansion in the use of UAVs, the ability to autonomously navigate in varying environments and weather conditions remains a highly desirable but as-of-yet unsolved challenge. In this work, we use Deep Reinforcement Learning to continuously improve the learning and understanding of a UAV agent while exploring a partially observable environment, which simulates the challenges faced in a real-life scenario. Our innovative approach uses a double state-input strategy that combines the acquired knowledge from the raw image and a map containing positional information. This positional data aids the network understanding of where the UAV has been and how far it is from the target position, while the feature map from the current scene highlights cluttered areas that are to be avoided. Our approach is extensively tested using variants of Deep Q-Network adapted to cope with double state input data. Further, we demonstrate that by altering the reward and the Q-value function, the agent is capable of consistently outperforming the adapted Deep Q-Network, Double Deep Q- Network and Deep Recurrent Q-Network. Our results demonstrate that our proposed Extended Double Deep Q-Network (EDDQN) approach is capable of navigating through multiple unseen environments and under severe weather conditions.",0
"The use of UAVs is rapidly expanding, but they still face the challenge of autonomously navigating through diverse environments and weather conditions. To address this challenge, we employ Deep Reinforcement Learning to enhance the learning and understanding of a UAV agent in a partially observable environment, which presents real-life challenges. Our innovative approach uses a double state-input strategy that combines positional data from a map with knowledge acquired from raw images. This approach helps the network understand the UAV's location and proximity to the target position, while also highlighting areas to avoid. We extensively test our approach using variants of Deep Q-Network and show that our proposed Extended Double Deep Q-Network (EDDQN) outperforms other networks in various scenarios. Our approach demonstrates the ability to navigate through multiple unfamiliar environments and harsh weather conditions.",1
"We study model-based reinforcement learning in an unknown finite communicating Markov decision process. We propose a simple algorithm that leverages a variance based confidence interval. We show that the proposed algorithm, UCRL-V, achieves the optimal regret $\tilde{\mathcal{O}}(\sqrt{DSAT})$ up to logarithmic factors, and so our work closes a gap with the lower bound without additional assumptions on the MDP. We perform experiments in a variety of environments that validates the theoretical bounds as well as prove UCRL-V to be better than the state-of-the-art algorithms.",0
"Our focus is on the investigation of model-based reinforcement learning in a finite Markov decision process with unknown communication. Our proposed approach employs a simple algorithm that takes advantage of a confidence interval based on variance. By demonstrating that our proposed algorithm, UCRL-V, attains the optimal regret $\tilde{\mathcal{O}}(\sqrt{DSAT})$ (with logarithmic factors) without any extra assumptions on the MDP, we have bridged a gap with the lower bound. Through experiments conducted in various settings, we have confirmed our theoretical findings and demonstrated that UCRL-V outperforms other leading algorithms.",1
"Entropy regularization is used to get improved optimization performance in reinforcement learning tasks. A common form of regularization is to maximize policy entropy to avoid premature convergence and lead to more stochastic policies for exploration through action space. However, this does not ensure exploration in the state space. In this work, we instead consider the distribution of discounted weighting of states, and propose to maximize the entropy of a lower bound approximation to the weighting of a state, based on latent space state representation. We propose entropy regularization based on the marginal state distribution, to encourage the policy to have a more uniform distribution over the state space for exploration. Our approach based on marginal state distribution achieves superior state space coverage on complex gridworld domains, that translate into empirical gains in sparse reward 3D maze navigation and continuous control domains compared to entropy regularization with stochastic policies.",0
"Reinforcement learning tasks can benefit from entropy regularization to improve optimization performance. While maximizing policy entropy is a common form of regularization that can lead to more exploratory policies, it only applies to action space and not state space. This study proposes an alternative method that considers the distribution of discounted weighting of states and suggests maximizing the entropy of a lower bound approximation of the weighting of a state based on a latent space state representation. To encourage more exploration in the state space, they propose entropy regularization based on the marginal state distribution, which promotes a more uniform distribution over the state space. This approach results in superior state space coverage when tested on complex gridworld domains and shows empirical gains in sparse reward 3D maze navigation and continuous control domains compared to traditional stochastic policies with entropy regularization.",1
"We study the problem of off-policy critic evaluation in several variants of value-based off-policy actor-critic algorithms. Off-policy actor-critic algorithms require an off-policy critic evaluation step, to estimate the value of the new policy after every policy gradient update. Despite enormous success of off-policy policy gradients on control tasks, existing general methods suffer from high variance and instability, partly because the policy improvement depends on gradient of the estimated value function. In this work, we present a new way of off-policy policy evaluation in actor-critic, based on the doubly robust estimators. We extend the doubly robust estimator from off-policy policy evaluation (OPE) to actor-critic algorithms that consist of a reward estimator performance model. We find that doubly robust estimation of the critic can significantly improve performance in continuous control tasks. Furthermore, in cases where the reward function is stochastic that can lead to high variance, doubly robust critic estimation can improve performance under corrupted, stochastic reward signals, indicating its usefulness for robust and safe reinforcement learning.",0
"The focus of our study is the assessment of off-policy critics in different variations of value-based off-policy actor-critic algorithms. In off-policy actor-critic algorithms, an off-policy critic evaluation is necessary to determine the value of the new policy after each policy gradient update. While off-policy policy gradients have been highly successful in control tasks, existing methods have limitations due to the high variance and instability that result from policy improvement's dependence on the estimated value function gradient. To address this issue, we introduce a novel approach to off-policy policy evaluation in actor-critic, which relies on doubly robust estimators. Our method extends the doubly robust estimator from off-policy policy evaluation to actor-critic algorithms that feature a reward estimator performance model. Our findings demonstrate that the doubly robust estimation of the critic significantly enhances performance in continuous control tasks. Moreover, in situations where the reward function is stochastic and may result in high variance, doubly robust critic estimation can enhance performance even when the reward signals are corrupted or stochastic, indicating its potential for safe and robust reinforcement learning.",1
"The policy gradient theorem is defined based on an objective with respect to the initial distribution over states. In the discounted case, this results in policies that are optimal for one distribution over initial states, but may not be uniformly optimal for others, no matter where the agent starts from. Furthermore, to obtain unbiased gradient estimates, the starting point of the policy gradient estimator requires sampling states from a normalized discounted weighting of states. However, the difficulty of estimating the normalized discounted weighting of states, or the stationary state distribution, is quite well-known. Additionally, the large sample complexity of policy gradient methods is often attributed to insufficient exploration, and to remedy this, it is often assumed that the restart distribution provides sufficient exploration in these algorithms. In this work, we propose exploration in policy gradient methods based on maximizing entropy of the discounted future state distribution. The key contribution of our work includes providing a practically feasible algorithm to estimate the normalized discounted weighting of states, i.e, the \textit{discounted future state distribution}. We propose that exploration can be achieved by entropy regularization with the discounted state distribution in policy gradients, where a metric for maximal coverage of the state space can be based on the entropy of the induced state distribution. The proposed approach can be considered as a three time-scale algorithm and under some mild technical conditions, we prove its convergence to a locally optimal policy. Experimentally, we demonstrate usefulness of regularization with the discounted future state distribution in terms of increased state space coverage and faster learning on a range of complex tasks.",0
"The policy gradient theorem is defined based on an objective that considers the initial distribution over states. In the case of discounting, this leads to policies that are optimal for one initial state distribution but not uniformly optimal for others, regardless of the agent's starting point. To obtain unbiased gradient estimates, the policy gradient estimator needs to sample states from a normalized discounted weighting of states. However, estimating this weighting, or the stationary state distribution, is challenging due to its complexity. Insufficient exploration is often blamed for the large sample complexity of policy gradient methods, and it is assumed that the restart distribution provides sufficient exploration. In this study, we propose exploration based on maximizing the entropy of the discounted future state distribution in policy gradient methods. Our contribution is a feasible algorithm to estimate the discounted future state distribution, which we propose can be used to achieve exploration through entropy regularization. We suggest that maximal coverage of the state space can be achieved by maximizing the entropy of the induced state distribution. Our approach is a three time-scale algorithm, which, under mild technical assumptions, converges to a locally optimal policy. We demonstrate the usefulness of our approach with experiments on a range of complex tasks, showing increased state space coverage and faster learning.",1
"When performing imitation learning from expert demonstrations, distribution matching is a popular approach, in which one alternates between estimating distribution ratios and then using these ratios as rewards in a standard reinforcement learning (RL) algorithm. Traditionally, estimation of the distribution ratio requires on-policy data, which has caused previous work to either be exorbitantly data-inefficient or alter the original objective in a manner that can drastically change its optimum. In this work, we show how the original distribution ratio estimation objective may be transformed in a principled manner to yield a completely off-policy objective. In addition to the data-efficiency that this provides, we are able to show that this objective also renders the use of a separate RL optimization unnecessary.Rather, an imitation policy may be learned directly from this objective without the use of explicit rewards. We call the resulting algorithm ValueDICE and evaluate it on a suite of popular imitation learning benchmarks, finding that it can achieve state-of-the-art sample efficiency and performance.",0
"Distribution matching is a common technique used in imitation learning from expert demonstrations. This involves estimating distribution ratios and using them as rewards in a standard reinforcement learning algorithm. However, traditionally, distribution ratio estimation requires on-policy data, which can be data-inefficient or alter the original objective. In this work, we present a method to transform the original distribution ratio estimation objective into a completely off-policy objective, providing data efficiency and eliminating the need for a separate reinforcement learning optimization. We call this algorithm ValueDICE, and it allows for direct learning of an imitation policy without explicit rewards. We evaluated ValueDICE on several imitation learning benchmarks and found that it achieved state-of-the-art sample efficiency and performance.",1
"With Reinforcement Learning we assume that a model of the world does exist. We assume furthermore that the model in question is perfect (i.e. it describes the world completely and unambiguously). This article will demonstrate that it does not make sense to search for the perfect model because this model is too complicated and practically impossible to find. We will show that we should abandon the pursuit of perfection and pursue Event-Driven (ED) models instead. These models are generalization of Markov Decision Process (MDP) models. This generalization is essential because nothing can be found without it. Rather than a single MDP, we will aim to find a raft of neat simple ED models each one describing a simple dependency or property. In other words, we will replace the search for a singular and complex perfect model with a search for a large number of simple models.",0
"The article argues against the notion that a perfect and all-encompassing model of the world exists in Reinforcement Learning. Instead, it suggests pursuing Event-Driven (ED) models, which are a more generalized version of Markov Decision Process (MDP) models. The article contends that the pursuit of a perfect model is impractical and futile, and it is advisable to aim for a multitude of simple ED models that describe specific dependencies or properties. In summary, the article proposes a shift in focus from a complex and singular perfect model to a collection of straightforward models.",1
"By integrating dynamics models into model-free reinforcement learning (RL) methods, model-based value expansion (MVE) algorithms have shown a significant advantage in sample efficiency as well as value estimation. However, these methods suffer from higher function approximation errors than model-free methods in stochastic environments due to a lack of modeling the environmental randomness. As a result, their performance lags behind the best model-free algorithms in some challenging scenarios. In this paper, we propose a novel Hybrid-RL method that builds on MVE, namely the Risk Averse Value Expansion (RAVE). With imaginative rollouts generated by an ensemble of probabilistic dynamics models, we further introduce the aversion of risks by seeking the lower confidence bound of the estimation. Experiments on a range of challenging environments show that by modeling the uncertainty completely, RAVE substantially enhances the robustness of previous model-based methods, and yields state-of-the-art performance. With this technique, our solution gets the first place in NeurIPS 2019: Learn to Move.",0
"MVE algorithms have demonstrated improved sample efficiency and value estimation by incorporating dynamic models into model-free RL methods. However, in stochastic environments, these methods have higher function approximation errors due to the absence of environmental randomness modeling. Consequently, they may not perform as well as the best model-free algorithms in challenging scenarios. This paper introduces a novel Hybrid-RL approach, RAVE, which builds on MVE and incorporates imaginative rollouts generated by a probabilistic dynamics model ensemble. The technique further introduces risk aversion by seeking lower confidence bounds of the estimation. Experiments on various challenging environments indicate that RAVE enhances the robustness of previous model-based methods and achieves state-of-the-art performance by fully modeling uncertainty. Our solution took first place in NeurIPS 2019: Learn to Move.",1
"Lifelong learning is a very important step toward realizing robust autonomous artificial agents. Neural networks are the main engine of deep learning, which is the current state-of-the-art technique in formulating adaptive artificial intelligent systems. However, neural networks suffer from catastrophic forgetting when stressed with the challenge of continual learning. We investigate how to exploit modular topology in neural networks in order to dynamically balance the information load between different modules by routing inputs based on the information content in each module so that information interference is minimized. Our dynamic information balancing (DIB) technique adapts a reinforcement learning technique to guide the routing of different inputs based on a reward signal derived from a measure of the information load in each module. Our empirical results show that DIB combined with elastic weight consolidation (EWC) regularization outperforms models with similar capacity and EWC regularization across different task formulations and datasets.",0
"To achieve robust autonomous artificial agents, it is crucial to engage in lifelong learning. Deep learning relies heavily on neural networks, which are currently the most advanced approach to creating adaptive artificial intelligence systems. However, neural networks are prone to catastrophic forgetting when subjected to continual learning. Our research focuses on utilizing modular topology in neural networks to distribute the information load between modules dynamically. By routing inputs based on their information content, we can minimize information interference. Our dynamic information balancing (DIB) technique, which is based on reinforcement learning, guides input routing based on a reward signal that gauges the information load in each module. Empirical evidence suggests that combining DIB with elastic weight consolidation (EWC) regularization produces better results than similarly equipped models across various task formulations and datasets.",1
"Bayesian inverse reinforcement learning (IRL) methods are ideal for safe imitation learning, as they allow a learning agent to reason about reward uncertainty and the safety of a learned policy. However, Bayesian IRL is computationally intractable for high-dimensional problems because each sample from the posterior requires solving an entire Markov Decision Process (MDP). While there exist non-Bayesian deep IRL methods, these methods typically infer point estimates of reward functions, precluding rigorous safety and uncertainty analysis. We propose Bayesian Reward Extrapolation (B-REX), a highly efficient, preference-based Bayesian reward learning algorithm that scales to high-dimensional, visual control tasks. Our approach uses successor feature representations and preferences over demonstrations to efficiently generate samples from the posterior distribution over the demonstrator's reward function without requiring an MDP solver. Using samples from the posterior, we demonstrate how to calculate high-confidence bounds on policy performance in the imitation learning setting, in which the ground-truth reward function is unknown. We evaluate our proposed approach on the task of learning to play Atari games via imitation learning from pixel inputs, with no access to the game score. We demonstrate that B-REX learns imitation policies that are competitive with a state-of-the-art deep imitation learning method that only learns a point estimate of the reward function. Furthermore, we demonstrate that samples from the posterior generated via B-REX can be used to compute high-confidence performance bounds for a variety of evaluation policies. We show that high-confidence performance bounds are useful for accurately ranking different evaluation policies when the reward function is unknown. We also demonstrate that high-confidence performance bounds may be useful for detecting reward hacking.",0
"Safe imitation learning can benefit from Bayesian inverse reinforcement learning (IRL) methods, which enable a learning agent to consider reward uncertainty and policy safety. However, high-dimensional problems make Bayesian IRL computationally infeasible since each sample from the posterior requires solving a Markov Decision Process (MDP). While non-Bayesian deep IRL methods exist, they usually estimate reward functions as point values, limiting safety and uncertainty analysis. Our proposed solution is Bayesian Reward Extrapolation (B-REX), which efficiently learns preferences-based Bayesian reward with successor feature representations. B-REX generates posterior distribution samples without an MDP solver, allowing high-confidence bounds on policy performance in imitation learning when the reward function is unknown. We evaluate B-REX in Atari games for imitation learning and show that it performs comparably to a deep imitation learning method that only estimates the reward function's point value. Moreover, B-REX generates high-confidence performance bounds that accurately rank different evaluation policies and detect reward hacking.",1
"Inverse reinforcement learning has proved its ability to explain state-action trajectories of expert agents by recovering their underlying reward functions in increasingly challenging environments. Recent advances in adversarial learning have allowed extending inverse RL to applications with non-stationary environment dynamics unknown to the agents, arbitrary structures of reward functions and improved handling of the ambiguities inherent to the ill-posed nature of inverse RL. This is particularly relevant in real time applications on stochastic environments involving risk, like volatile financial markets. Moreover, recent work on simulation of complex environments enable learning algorithms to engage with real market data through simulations of its latent space representations, avoiding a costly exploration of the original environment. In this paper, we explore whether adversarial inverse RL algorithms can be adapted and trained within such latent space simulations from real market data, while maintaining their ability to recover agent rewards robust to variations in the underlying dynamics, and transfer them to new regimes of the original environment.",0
"The ability of inverse reinforcement learning to explain the actions of expert agents by uncovering their reward functions in challenging environments has been demonstrated. Recent advancements in adversarial learning have made it possible to apply inverse RL to situations with non-stationary environment dynamics, diverse reward function structures, and better management of the uncertainties inherent in inverse RL's ill-defined nature. This is especially important in real-time applications in stochastic environments involving risk, such as volatile financial markets. Furthermore, recent research on the simulation of complex environments has enabled learning algorithms to work with real market data by simulating its latent space representations, avoiding the expensive exploration of the original environment. In this study, we investigate whether adversarial inverse RL algorithms can be adapted and trained within such latent space simulations using real market data while maintaining their capacity to recover agent rewards that are resilient to variations in the underlying dynamics and transfer them to new environments.",1
"Model-based reinforcement learning methods typically learn models for high-dimensional state spaces by aiming to reconstruct and predict the original observations. However, drawing inspiration from model-free reinforcement learning, we propose learning a latent dynamics model directly from rewards. In this work, we introduce a model-based planning framework which learns a latent reward prediction model and then plans in the latent state-space. The latent representation is learned exclusively from multi-step reward prediction which we show to be the only necessary information for successful planning. With this framework, we are able to benefit from the concise model-free representation, while still enjoying the data-efficiency of model-based algorithms. We demonstrate our framework in multi-pendulum and multi-cheetah environments where several pendulums or cheetahs are shown to the agent but only one of which produces rewards. In these environments, it is important for the agent to construct a concise latent representation to filter out irrelevant observations. We find that our method can successfully learn an accurate latent reward prediction model in the presence of the irrelevant information while existing model-based methods fail. Planning in the learned latent state-space shows strong performance and high sample efficiency over model-free and model-based baselines.",0
"Usually, model-based reinforcement learning methods learn models for high-dimensional state spaces by reconstructing and predicting original observations. However, our proposed approach is inspired by model-free reinforcement learning. We aim to learn a latent dynamics model solely from rewards. Our method involves learning a latent reward prediction model and planning in the latent state-space. The latent representation is learned from multi-step reward prediction, which we prove to be the only necessary information for successful planning. Our approach allows us to use the concise model-free representation while also benefiting from the data-efficiency of model-based algorithms. We demonstrate the effectiveness of our method in multi-pendulum and multi-cheetah environments, where the agent must filter out irrelevant observations and focus on the rewarding ones. Our approach is successful in learning an accurate latent reward prediction model, while existing model-based methods fail. Planning in the learned latent state-space proves to be highly sample-efficient and outperforms model-free and model-based baselines.",1
"We design a new provably efficient algorithm for episodic reinforcement learning with generalized linear function approximation. We analyze the algorithm under a new expressivity assumption that we call ""optimistic closure,"" which is strictly weaker than assumptions from prior analyses for the linear setting. With optimistic closure, we prove that our algorithm enjoys a regret bound of $\tilde{O}(\sqrt{d^3 T})$ where $d$ is the dimensionality of the state-action features and $T$ is the number of episodes. This is the first statistically and computationally efficient algorithm for reinforcement learning with generalized linear functions.",0
"Our team has created an innovative algorithm for episodic reinforcement learning that utilizes generalized linear function approximation. We have conducted a thorough analysis of the algorithm, employing a new expressivity assumption known as ""optimistic closure."" This assumption is less restrictive than those used in prior analyses for linear settings. By utilizing optimistic closure, we have proven that our algorithm has a regret bound of $\tilde{O}(\sqrt{d^3 T})$, where $d$ represents the dimensionality of state-action features and $T$ is the number of episodes. Our algorithm is the first of its kind to offer both statistical and computational efficiency in reinforcement learning with generalized linear functions.",1
"Neural Machine Translation has lately gained a lot of ""attention"" with the advent of more and more sophisticated but drastically improved models. Attention mechanism has proved to be a boon in this direction by providing weights to the input words, making it easy for the decoder to identify words representing the present context. But by and by, as newer attention models with more complexity came into development, they involved large computation, making inference slow. In this paper, we have modelled the attention network using techniques resonating with social choice theory. Along with that, the attention mechanism, being a Markov Decision Process, has been represented by reinforcement learning techniques. Thus, we propose to use an election method ($k$-Borda), fine-tuned using Q-learning, as a replacement for attention networks. The inference time for this network is less than a standard Bahdanau translator, and the results of the translation are comparable. This not only experimentally verifies the claims stated above but also helped provide a faster inference.",0
"With the emergence of more advanced and effective models, Neural Machine Translation has recently become a prominent topic. The introduction of the attention mechanism has been particularly beneficial, assigning weights to input words and simplifying the identification of words relevant to the current context. However, as more complex attention models were developed, computation requirements increased, resulting in slower inference. In this study, we utilized social choice theory to model the attention network and represented the Markov Decision Process of the attention mechanism using reinforcement learning techniques. Our proposed approach involves using an election method, specifically the $k$-Borda method, fine-tuned with Q-learning, as a substitute for attention networks. Our network's inference time is faster than a standard Bahdanau translator, and the translation results are comparable. This not only validates our previous claims but also results in quicker inference time.",1
"Sparse representations have been shown to be useful in deep reinforcement learning for mitigating catastrophic interference and improving the performance of agents in terms of cumulative reward. Previous results were based on a two step process were the representation was learned offline and the action-value function was learned online afterwards. In this paper, we investigate if it is possible to learn a sparse representation and the action-value function simultaneously and incrementally. We investigate this question by employing several regularization techniques and observing how they affect sparsity of the representation learned by a DQN agent in two different benchmark domains. Our results show that with appropriate regularization it is possible to increase the sparsity of the representations learned by DQN agents. Moreover, we found that learning sparse representations also resulted in improved performance in terms of cumulative reward. Finally, we found that the performance of the agents that learned a sparse representation was more robust to the size of the experience replay buffer. This last finding supports the long standing hypothesis that the overlap in representations learned by deep neural networks is the leading cause of catastrophic interference.",0
"Sparse representations have been proven to be effective in enhancing the performance of agents in deep reinforcement learning, reducing catastrophic interference, and increasing cumulative reward. Previous research involved a two-step process where the representation was learned offline and the action-value function was learned online subsequently. This paper aims to explore the possibility of simultaneous and incremental learning of a sparse representation and the action-value function. We employed various regularization techniques to observe their effect on the sparsity of the representation learned by a DQN agent in two benchmark domains. Our findings suggest that with appropriate regularization, the sparsity of representations learned by DQN agents can be increased, leading to improved performance in terms of cumulative reward. Additionally, we found that agents that learned a sparse representation exhibited more robust performance in relation to the size of the experience replay buffer. This supports the widely held hypothesis that overlap in representations learned by deep neural networks is the primary cause of catastrophic interference.",1
"Despite of the recent progress in agents that learn through interaction, there are several challenges in terms of sample efficiency and generalization across unseen behaviors during training. To mitigate these problems, we propose and apply a first-order Meta-Learning algorithm called Bottom-Up Meta-Policy Search (BUMPS), which works with two-phase optimization procedure: firstly, in a meta-training phase, it distills few expert policies to create a meta-policy capable of generalizing knowledge to unseen tasks during training; secondly, it applies a fast adaptation strategy named Policy Filtering, which evaluates few policies sampled from the meta-policy distribution and selects which best solves the task. We conducted all experiments in the RoboCup 3D Soccer Simulation domain, in the context of kick motion learning. We show that, given our experimental setup, BUMPS works in scenarios where simple multi-task Reinforcement Learning does not. Finally, we performed experiments in a way to evaluate each component of the algorithm.",0
"Although there have been advances in interactive learning agents, challenges remain regarding their efficiency with sample size and ability to generalize to new behaviors during training. In order to address these issues, a Meta-Learning algorithm called Bottom-Up Meta-Policy Search (BUMPS) has been proposed and implemented, utilizing a two-phase optimization process. During the meta-training phase, expert policies are distilled to create a meta-policy that can generalize information to new tasks. The second phase, Policy Filtering, involves evaluating a small sample of policies from the meta-policy distribution to determine the best solution for the task at hand. All experiments were conducted in the context of kick motion learning within the RoboCup 3D Soccer Simulation domain. Results indicate that BUMPS is effective in scenarios where simple multi-task Reinforcement Learning is not. Additionally, each component of the algorithm was evaluated through experimentation.",1
"Recent times have witnessed sharp improvements in reinforcement learning tasks using deep reinforcement learning techniques like Deep Q Networks, Policy Gradients, Actor Critic methods which are based on deep learning based models and back-propagation of gradients to train such models. An active area of research in reinforcement learning is about training agents to play complex video games, which so far has been something accomplished only by human intelligence. Some state of the art performances in video game playing using deep reinforcement learning are obtained by processing the sequence of frames from video games, passing them through a convolutional network to obtain features and then using recurrent neural networks to figure out the action leading to optimal rewards. The recurrent neural network will learn to extract the meaningful signal out of the sequence of such features. In this work, we propose a method utilizing a transformer network which have recently replaced RNNs in Natural Language Processing (NLP), and perform experiments to compare with existing methods.",0
"Deep reinforcement learning techniques like Deep Q Networks, Policy Gradients, and Actor Critic methods have led to significant improvements in reinforcement learning tasks in recent times. These techniques rely on deep learning models and back-propagation of gradients to train the models. An area of active research in reinforcement learning involves training agents to play complex video games, a feat that has so far only been accomplished by human intelligence. State-of-the-art performances in video game playing using deep reinforcement learning are achieved by processing video game frames, using a convolutional network to obtain features, and then using recurrent neural networks to determine the optimal action leading to rewards. The recurrent neural network extracts meaningful signals from the feature sequence. In this study, we propose using a transformer network, which has replaced RNNs in NLP, and conduct experiments to compare our method with existing ones.",1
"In the last few years, deep multi-agent reinforcement learning (RL) has become a highly active area of research. A particularly challenging class of problems in this area is partially observable, cooperative, multi-agent learning, in which teams of agents must learn to coordinate their behaviour while conditioning only on their private observations. This is an attractive research area since such problems are relevant to a large number of real-world systems and are also more amenable to evaluation than general-sum problems. Standardised environments such as the ALE and MuJoCo have allowed single-agent RL to move beyond toy domains, such as grid worlds. However, there is no comparable benchmark for cooperative multi-agent RL. As a result, most papers in this field use one-off toy problems, making it difficult to measure real progress. In this paper, we propose the StarCraft Multi-Agent Challenge (SMAC) as a benchmark problem to fill this gap. SMAC is based on the popular real-time strategy game StarCraft II and focuses on micromanagement challenges where each unit is controlled by an independent agent that must act based on local observations. We offer a diverse set of challenge maps and recommendations for best practices in benchmarking and evaluations. We also open-source a deep multi-agent RL learning framework including state-of-the-art algorithms. We believe that SMAC can provide a standard benchmark environment for years to come. Videos of our best agents for several SMAC scenarios are available at: https://youtu.be/VZ7zmQ_obZ0.",0
"The field of deep multi-agent reinforcement learning (RL) has gained significant attention in recent years. One of the most difficult types of problems in this area is partially observable, cooperative, multi-agent learning, which requires teams of agents to coordinate their behavior based on private observations. This research area is attractive because it is relevant to many real-world systems and is easier to evaluate than general-sum problems. While standardized environments like ALE and MuJoCo have allowed single-agent RL to move beyond simple domains, there is currently no comparable benchmark for cooperative multi-agent RL. To address this gap, we introduce the StarCraft Multi-Agent Challenge (SMAC), a benchmark problem based on the popular real-time strategy game StarCraft II. SMAC focuses on micromanagement challenges where each unit is controlled by an independent agent that must act based on local observations. We provide a diverse set of challenge maps, best practices for benchmarking and evaluations, and an open-source deep multi-agent RL learning framework with state-of-the-art algorithms. We believe that SMAC can serve as a standard benchmark environment for years to come and offer videos of our best agents for several SMAC scenarios at: https://youtu.be/VZ7zmQ_obZ0.",1
"Multi-agent reinforcement learning (MARL) has long been a significant and everlasting research topic in both machine learning and control. With the recent development of (single-agent) deep RL, there is a resurgence of interests in developing new MARL algorithms, especially those that are backed by theoretical analysis. In this paper, we review some recent advances a sub-area of this topic: decentralized MARL with networked agents. Specifically, multiple agents perform sequential decision-making in a common environment, without the coordination of any central controller. Instead, the agents are allowed to exchange information with their neighbors over a communication network. Such a setting finds broad applications in the control and operation of robots, unmanned vehicles, mobile sensor networks, and smart grid. This review is built upon several our research endeavors in this direction, together with some progresses made by other researchers along the line. We hope this review to inspire the devotion of more research efforts to this exciting yet challenging area.",0
"The study of Multi-agent reinforcement learning (MARL) has always been a significant area of research in machine learning and control. Recently, with the advancement of deep RL, there has been renewed interest in developing new MARL algorithms, especially those with theoretical backing. This paper focuses on reviewing the progress made in the sub-field of decentralized MARL with networked agents. This involves multiple agents making sequential decisions in a shared environment, without any central controller. Instead, the agents communicate with neighboring agents over a network. This particular setup has applications in controlling and operating robots, unmanned vehicles, mobile sensor networks, and smart grids. The review is based on our own research and that of other researchers in this area, and we hope it encourages more research into this exciting and challenging field.",1
"Reinforcement learning is an appropriate and successful method to robustly perform low-level robot control under noisy conditions. Symbolic action planning is useful to resolve causal dependencies and to break a causally complex problem down into a sequence of simpler high-level actions. A problem with the integration of both approaches is that action planning is based on discrete high-level action- and state spaces, whereas reinforcement learning is usually driven by a continuous reward function. However, recent advances in reinforcement learning, specifically, universal value function approximators and hindsight experience replay, have focused on goal-independent methods based on sparse rewards. In this article, we build on these novel methods to facilitate the integration of action planning with reinforcement learning by exploiting the reward-sparsity as a bridge between the high-level and low-level state- and control spaces. As a result, we demonstrate that the integrated neuro-symbolic method is able to solve object manipulation problems that involve tool use and non-trivial causal dependencies under noisy conditions, exploiting both data and knowledge.",0
"The successful use of reinforcement learning for low-level robot control in noisy environments is well-established. Symbolic action planning is also valuable for breaking down complex problems into simpler high-level actions and resolving causal dependencies. However, a challenge with combining these approaches is that action planning relies on discrete high-level action and state spaces, while reinforcement learning uses a continuous reward function. Recent advancements in reinforcement learning, such as universal value function approximators and hindsight experience replay, have addressed this issue by focusing on goal-independent methods with sparse rewards. In this article, we leverage these developments to integrate action planning with reinforcement learning by utilizing reward-sparsity as a bridge between high-level and low-level state and control spaces. We demonstrate that this integrated neuro-symbolic method effectively solves object manipulation problems with tool use and complex causal dependencies in noisy environments, utilizing both data and knowledge.",1
"Aiming at a comprehensive and concise tutorial survey, recap of variational inference and reinforcement learning with Probabilistic Graphical Models are given with detailed derivations. Reviews and comparisons on recent advances in deep reinforcement learning are made from various aspects. We offer detailed derivations to a taxonomy of Probabilistic Graphical Model and Variational Inference methods in deep reinforcement learning, which serves as a complementary material on top of the original contributions.",0
"In order to provide a comprehensive and succinct tutorial, this article presents a summary of variational inference and reinforcement learning using Probabilistic Graphical Models, accompanied by detailed derivations. Additionally, it includes reviews and comparisons of recent advancements in deep reinforcement learning from various perspectives. Furthermore, we provide detailed derivations for a classification of Probabilistic Graphical Model and Variational Inference techniques in deep reinforcement learning, which serves as supplementary material to the original contributions.",1
"There have been numerous attempts in explaining the general learning behaviours using model-based and model-free methods. While the model-based control is flexible yet computationally expensive in planning, the model-free control is quick but inflexible. The model-based control is therefore immune from reward devaluation and contingency degradation. Multiple arbitration schemes have been suggested to achieve the data efficiency and computational efficiency of model-based and model-free control respectively. In this context, we propose a quantitative 'value of information' based arbitration between both the controllers in order to establish a general computational framework for skill learning. The interacting model-based and model-free reinforcement learning processes are arbitrated using an uncertainty-based value of information. We further show that our algorithm performs better than Q-learning as well as Q-learning with experience replay.",0
"Several attempts have been made to explain general learning behaviors through the use of model-based and model-free methods. While model-based control is flexible, it is computationally expensive in planning. Conversely, model-free control is quick but lacks flexibility. Model-based control is immune to reward devaluation and contingency degradation. To achieve data and computational efficiency in both types of control, multiple arbitration schemes have been proposed. We suggest a quantitative value-of-information-based arbitration between the two controllers to create a general computational framework for skill learning. The interacting model-based and model-free reinforcement learning processes are arbitrated using an uncertainty-based value of information. Our algorithm outperforms Q-learning and Q-learning with experience replay.",1
"The measurement of time is central to intelligent behavior. We know that both animals and artificial agents can successfully use temporal dependencies to select actions. In artificial agents, little work has directly addressed (1) which architectural components are necessary for successful development of this ability, (2) how this timing ability comes to be represented in the units and actions of the agent, and (3) whether the resulting behavior of the system converges on solutions similar to those of biology. Here we studied interval timing abilities in deep reinforcement learning agents trained end-to-end on an interval reproduction paradigm inspired by experimental literature on mechanisms of timing. We characterize the strategies developed by recurrent and feedforward agents, which both succeed at temporal reproduction using distinct mechanisms, some of which bear specific and intriguing similarities to biological systems. These findings advance our understanding of how agents come to represent time, and they highlight the value of experimentally inspired approaches to characterizing agent abilities.",0
"Intelligent behavior relies heavily on the ability to measure time. Both animals and artificial agents are capable of utilizing temporal dependencies to make decisions. However, there is little research on the necessary architectural components for developing this ability in artificial agents, how timing is represented in the units and actions of the agent, and whether the resulting behavior converges with that of biology. To address these gaps, we conducted a study on interval timing in deep reinforcement learning agents trained on an interval reproduction paradigm inspired by experimental literature. Our findings show that both recurrent and feedforward agents can successfully reproduce intervals, using distinct mechanisms that share some intriguing similarities with biological systems. These results provide insight into how agents represent time and demonstrate the value of experimentally-inspired approaches to understanding agent abilities.",1
"VALAN is a lightweight and scalable software framework for deep reinforcement learning based on the SEED RL architecture. The framework facilitates the development and evaluation of embodied agents for solving grounded language understanding tasks, such as Vision-and-Language Navigation and Vision-and-Dialog Navigation, in photo-realistic environments, such as Matterport3D and Google StreetView. We have added a minimal set of abstractions on top of SEED RL allowing us to generalize the architecture to solve a variety of other RL problems. In this article, we will describe VALAN's software abstraction and architecture, and also present an example of using VALAN to design agents for instruction-conditioned indoor navigation.",0
"VALAN is a software framework that is both lightweight and scalable and is based on the SEED RL architecture. With VALAN, developers are able to create and assess embodied agents for solving grounded language understanding tasks in photo-realistic environments, such as Matterport3D and Google StreetView. By adding a few abstractions on top of SEED RL, VALAN is able to address a range of other RL problems. The software abstraction and architecture of VALAN are discussed in this article, along with an example of how to use VALAN to create agents for instruction-conditioned indoor navigation.",1
"In real-world decision-making problems, for instance in the fields of finance, robotics or autonomous driving, keeping uncertainty under control is as important as maximizing expected returns. Risk aversion has been addressed in the reinforcement learning literature through risk measures related to the variance of returns. However, in many cases, the risk is measured not only on a long-term perspective, but also on the step-wise rewards (e.g., in trading, to ensure the stability of the investment bank, it is essential to monitor the risk of portfolio positions on a daily basis). In this paper, we define a novel measure of risk, which we call reward volatility, consisting of the variance of the rewards under the state-occupancy measure. We show that the reward volatility bounds the return variance so that reducing the former also constrains the latter. We derive a policy gradient theorem with a new objective function that exploits the mean-volatility relationship, and develop an actor-only algorithm. Furthermore, thanks to the linearity of the Bellman equations defined under the new objective function, it is possible to adapt the well-known policy gradient algorithms with monotonic improvement guarantees such as TRPO in a risk-averse manner. Finally, we test the proposed approach in two simulated financial environments.",0
"In decision-making scenarios, such as finance, robotics, and autonomous driving, managing uncertainty is just as crucial as maximizing anticipated returns. The reinforcement learning literature has tackled risk avoidance by utilizing risk measures linked to return variance. However, numerous cases necessitate assessing risk not only over the long term but also in terms of step-wise rewards. For example, monitoring portfolio position risk on a daily basis is critical for stabilizing investment banks. This paper presents a new risk measure called reward volatility, which is the variance of rewards under the state-occupancy measure. We prove that reward volatility sets boundaries for return variance, such that reducing the former also restricts the latter. We introduce a policy gradient theorem with a new objective function that utilizes the mean-volatility relationship and create an actor-only algorithm. Additionally, due to the linear nature of the Bellman equations defined under the new objective function, we can adapt policy gradient algorithms with monotonic improvement guarantees, such as TRPO, in a risk-averse manner. Finally, we test the proposed methodology in two simulated financial environments.",1
"Animals excel at adapting their intentions, attention, and actions to the environment, making them remarkably efficient at interacting with a rich, unpredictable and ever-changing external world, a property that intelligent machines currently lack. Such an adaptation property relies heavily on cellular neuromodulation, the biological mechanism that dynamically controls intrinsic properties of neurons and their response to external stimuli in a context-dependent manner. In this paper, we take inspiration from cellular neuromodulation to construct a new deep neural network architecture that is specifically designed to learn adaptive behaviours. The network adaptation capabilities are tested on navigation benchmarks in a meta-reinforcement learning context and compared with state-of-the-art approaches. Results show that neuromodulation is capable of adapting an agent to different tasks and that neuromodulation-based approaches provide a promising way of improving adaptation of artificial systems.",0
"The ability of animals to adapt their intentions, attention, and actions to their surroundings makes them highly efficient in dealing with a complex and ever-changing external world, a quality that intelligent machines currently lack. This adaptability is largely dependent on cellular neuromodulation, a biological mechanism that dynamically regulates the intrinsic properties of neurons and their response to external stimuli in a context-dependent manner. In this study, we draw inspiration from cellular neuromodulation to create a novel deep neural network architecture tailored to learning adaptive behaviors. The network's adaptability is tested in a meta-reinforcement learning context using navigation benchmarks and compared to state-of-the-art techniques. Results demonstrate that neuromodulation can help an agent adapt to various tasks and that neuromodulation-based approaches offer a promising means of enhancing artificial systems' adaptability.",1
"Most microscopic pedestrian navigation models use the concept of ""forces"" applied to the pedestrian agents to replicate the navigation environment. While the approach could provide believable results in regular situations, it does not always resemble natural pedestrian navigation behaviour in many typical settings. In our research, we proposed a novel approach using reinforcement learning for simulation of pedestrian agent path planning and collision avoidance problem. The primary focus of this approach is using human perception of the environment and danger awareness of interferences. The implementation of our model has shown that the path planned by the agent shares many similarities with a human pedestrian in several aspects such as following common walking conventions and human behaviours.",0
"Microscopic pedestrian navigation models often utilize the idea of applying ""forces"" to pedestrian agents to replicate the navigation environment. However, this approach may not always accurately reflect natural pedestrian navigation behavior in various settings. Our research introduces a new approach that employs reinforcement learning to simulate pedestrian agent path planning and collision avoidance. This approach prioritizes incorporating human perception of the environment and awareness of potential dangers. Our model's implementation has demonstrated that the agent's planned path aligns closely with human pedestrian behavior, including adhering to common walking conventions and exhibiting human behaviors.",1
"Humans and animals solve a difficult problem much more easily when they are presented with a sequence of problems that starts simple and slowly increases in difficulty. We explore this idea in the context of reinforcement learning. Rather than providing the agent with an externally provided curriculum of progressively more difficult tasks, the agent solves a single task utilizing a decreasingly constrained policy space. The algorithm we propose first learns to categorize features into positive and negative before gradually learning a more refined policy. Experimental results in Tetris demonstrate superior learning rate of our approach when compared to existing algorithms.",0
"The concept of gradually increasing difficulty levels in problem-solving is found to be beneficial for both humans and animals. This notion is applied in the realm of reinforcement learning where we suggest an alternative approach to providing the agent with a curriculum of challenging tasks. Instead, the agent tackles a single task by gradually narrowing down its policy space. Our proposed algorithm initially learns to differentiate between positive and negative features, and then gradually improves its policy. Our experimental results on Tetris showcase the superior learning rate of our method compared to other existing algorithms.",1
"We consider the problem of efficient credit assignment in reinforcement learning. In order to efficiently and meaningfully utilize new data, we propose to explicitly assign credit to past decisions based on the likelihood of them having led to the observed outcome. This approach uses new information in hindsight, rather than employing foresight. Somewhat surprisingly, we show that value functions can be rewritten through this lens, yielding a new family of algorithms. We study the properties of these algorithms, and empirically show that they successfully address important credit assignment challenges, through a set of illustrative tasks.",0
"Our focus is on solving the issue of credit assignment in reinforcement learning in an effective manner. To make the most of fresh data, we suggest that credit be assigned to previous decisions based on the probability of them resulting in the observed outcome. This method involves using hindsight to analyze new information instead of relying on foresight. Strikingly, we demonstrate that by adopting this approach, value functions can be redefined, leading to a novel collection of algorithms. We explore the features of these algorithms and provide evidence that they effectively tackle significant credit assignment problems by carrying out a series of illustrative tasks.",1
"Several imaging applications (vessels, retina, plant roots, road networks from satellites) require the accurate segmentation of thin structures for subsequent analysis. Discontinuities (gaps) in the extracted foreground may hinder down-stream image-based analysis of biomarkers, organ structure and topology. In this paper, we propose a general post-processing technique to recover such gaps in large-scale segmentation masks. We cast this problem as a blind inpainting task, where the regions of missing lines in the segmentation masks are not known to the algorithm, which we solve with an adversarially trained neural network. One challenge of using large images is the memory capacity of current GPUs. The typical approach of dividing a large image into smaller patches to train the network does not guarantee global coherence of the reconstructed image that preserves structure and topology. We use adversarial training and reinforcement learning (Policy Gradient) to endow the model with both global context and local details. We evaluate our method in several datasets in medical imaging, plant science, and remote sensing. Our experiments demonstrate that our model produces the most realistic and complete inpainted results, outperforming other approaches. In a dedicated study on plant roots we find that our approach is also comparable to human performance. Implementation available at \url{https://github.com/Hhhhhhhhhhao/Thin-Structure-Inpainting}.",0
"Accurate segmentation of thin structures is crucial for various imaging applications, including vessels, retina, plant roots, and road networks from satellites. However, gaps in the extracted foreground may disrupt downstream image-based analysis of organ structure, topology, and biomarkers. To address this issue, we present a general post-processing technique that recovers gaps in large-scale segmentation masks. Our approach involves using an adversarially trained neural network to solve a blind inpainting task where the missing regions of the segmentation masks are unknown. One challenge we face when working with large images is the memory capacity of current GPUs. Dividing a large image into smaller patches can result in a lack of global coherence in the reconstructed image, compromising structure and topology. We overcome this challenge by using adversarial training and reinforcement learning (Policy Gradient) to provide the model with both global context and local details. We evaluate our method on datasets from medical imaging, plant science, and remote sensing, and demonstrate that our model produces the most realistic and complete inpainted results, outperforming other approaches. In a dedicated study on plant roots, our method is also comparable to human performance. Our implementation is available at \url{https://github.com/Hhhhhhhhhhao/Thin-Structure-Inpainting}.",1
"Information-theoretic bounded rationality describes utility-optimizing decision-makers whose limited information-processing capabilities are formalized by information constraints. One of the consequences of bounded rationality is that resource-limited decision-makers can join together to solve decision-making problems that are beyond the capabilities of each individual. Here, we study an information-theoretic principle that drives division of labor and specialization when decision-makers with information constraints are joined together. We devise an on-line learning rule of this principle that learns a partitioning of the problem space such that it can be solved by specialized linear policies. We demonstrate the approach for decision-making problems whose complexity exceeds the capabilities of individual decision-makers, but can be solved by combining the decision-makers optimally. The strength of the model is that it is abstract and principled, yet has direct applications in classification, regression, reinforcement learning and adaptive control.",0
"The concept of information-theoretic bounded rationality pertains to decision-makers who optimize utility but are limited in their capacity to process information, which is formalized through information constraints. One implication of this theory is that decision-makers with limited resources can collaborate in order to solve problems that are beyond their individual capabilities. In this study, we examine an information-theoretic principle that drives the division of labor and specialization when decision-makers with information constraints work together. We have developed an online learning rule that employs this principle to learn a partitioning of the problem space, which can then be solved using specialized linear policies. We have demonstrated the effectiveness of this approach for decision-making problems that are too complex for individual decision-makers to solve but can be tackled through optimal collaboration. This model is valuable because it is both abstract and principled, while also having practical applications in areas such as classification, regression, reinforcement learning, and adaptive control.",1
"Optimal policies in Markov decision processes (MDPs) are very sensitive to model misspecification. This raises serious concerns about deploying them in high-stake domains. Robust MDPs (RMDP) provide a promising framework to mitigate vulnerabilities by computing policies with worst-case guarantees in reinforcement learning. The solution quality of an RMDP depends on the ambiguity set, which is a quantification of model uncertainties. In this paper, we propose a new approach for optimizing the shape of the ambiguity sets for RMDPs. Our method departs from the conventional idea of constructing a norm-bounded uniform and symmetric ambiguity set. We instead argue that the structure of a near-optimal ambiguity set is problem specific. Our proposed method computes a weight parameter from the value functions, and these weights then drive the shape of the ambiguity sets. Our theoretical analysis demonstrates the rationale of the proposed idea. We apply our method to several different problem domains, and the empirical results further furnish the practical promise of weighted near-optimal ambiguity sets.",0
"The sensitivity of Markov decision processes (MDPs) to model misspecification is a significant concern when utilizing them in high-stakes situations. To address this, Robust MDPs (RMDP) have emerged as a promising solution, as they compute policies with worst-case guarantees in reinforcement learning. However, the quality of RMDP solutions relies on the ambiguity set, which quantifies model uncertainties. In this study, we propose a new approach for optimizing the shape of ambiguity sets for RMDPs. Rather than using a norm-bounded uniform and symmetric ambiguity set, we suggest that the structure of a near-optimal ambiguity set should be tailored to the specific problem. Our method computes a weight parameter from value functions to shape the ambiguity sets, and our theoretical analysis supports this approach. We apply our method to various problem domains and demonstrate its practical promise in creating near-optimal ambiguity sets with weighted parameters.",1
"In many real-world applications of reinforcement learning (RL), interactions with the environment are limited due to cost or feasibility. This presents a challenge to traditional RL algorithms since the max-return objective involves an expectation over on-policy samples. We introduce a new formulation of max-return optimization that allows the problem to be re-expressed by an expectation over an arbitrary behavior-agnostic and off-policy data distribution. We first derive this result by considering a regularized version of the dual max-return objective before extending our findings to unregularized objectives through the use of a Lagrangian formulation of the linear programming characterization of Q-values. We show that, if auxiliary dual variables of the objective are optimized, then the gradient of the off-policy objective is exactly the on-policy policy gradient, without any use of importance weighting. In addition to revealing the appealing theoretical properties of this approach, we also show that it delivers good practical performance.",0
"The use of reinforcement learning (RL) in real-world applications is often limited by the expense or feasibility of interactions with the environment. This can pose a challenge for traditional RL algorithms that rely on an expectation over on-policy samples to achieve the max-return objective. To address this issue, we propose a new approach to max-return optimization that involves an expectation over an off-policy data distribution that is behavior-agnostic. Our method is derived by first considering a regularized version of the dual max-return objective and then extending the findings to unregularized objectives using a Lagrangian formulation of Q-values. We demonstrate that by optimizing auxiliary dual variables of the objective, the off-policy objective gradient is equivalent to the on-policy policy gradient, without requiring importance weighting. Our approach has excellent theoretical properties and achieves good practical performance.",1
"Policy evaluation in reinforcement learning is often conducted using two-timescale stochastic approximation, which results in various gradient temporal difference methods such as GTD(0), GTD2, and TDC. Here, we provide convergence rate bounds for this suite of algorithms. Algorithms such as these have two iterates, $\theta_n$ and $w_n,$ which are updated using two distinct stepsize sequences, $\alpha_n$ and $\beta_n,$ respectively. Assuming $\alpha_n = n^{-\alpha}$ and $\beta_n = n^{-\beta}$ with $1 > \alpha > \beta > 0,$ we show that, with high probability, the two iterates converge to their respective solutions $\theta^*$ and $w^*$ at rates given by $\|\theta_n - \theta^*\| = \tilde{O}( n^{-\alpha/2})$ and $\|w_n - w^*\| = \tilde{O}(n^{-\beta/2});$ here, $\tilde{O}$ hides logarithmic terms. Via comparable lower bounds, we show that these bounds are, in fact, tight. To the best of our knowledge, ours is the first finite-time analysis which achieves these rates. While it was known that the two timescale components decouple asymptotically, our results depict this phenomenon more explicitly by showing that it in fact happens from some finite time onwards. Lastly, compared to existing works, our result applies to a broader family of stepsizes, including non-square summable ones.",0
"Two-timescale stochastic approximation is often used to evaluate policies in reinforcement learning, resulting in gradient temporal difference methods like GTD(0), GTD2, and TDC. This suite of algorithms involves two iterates, $\theta_n$ and $w_n,$ updated with distinct stepsize sequences, $\alpha_n$ and $\beta_n,$ respectively. We provide convergence rate bounds for these algorithms assuming $\alpha_n = n^{-\alpha}$ and $\beta_n = n^{-\beta}$ with $1 > \alpha > \beta > 0$. With high probability, we show that the iterates converge to their respective solutions, $\theta^*$ and $w^*$, at rates of $\|\theta_n - \theta^*\| = \tilde{O}( n^{-\alpha/2})$ and $\|w_n - w^*\| = \tilde{O}(n^{-\beta/2});$ where $\tilde{O}$ hides logarithmic terms. Our result applies to a broader family of stepsizes, including non-square summable ones. We also depict the decoupling phenomenon of the two timescale components explicitly by showing that it happens from some finite time onwards. Our analysis achieves these rates, which to the best of our knowledge, is the first finite-time analysis to do so. We also establish comparable lower bounds, which demonstrate that our bounds are tight.",1
"The self-driving based on deep reinforcement learning, as the most important application of artificial intelligence, has become a popular topic. Most of the current self-driving methods focus on how to directly learn end-to-end self-driving control strategy from the raw sensory data. Essentially, this control strategy can be considered as a mapping between images and driving behavior, which usually faces a problem of low generalization ability. To improve the generalization ability for the driving behavior, the reinforcement learning method requires extrinsic reward from the real environment, which may damage the car. In order to obtain a good generalization ability in safety, a virtual simulation environment that can be constructed different driving scene is designed by Unity. A theoretical model is established and analyzed in the virtual simulation environment, and it is trained by double Deep Q-network. Then, the trained model is migrated to a scale car in real world. This process is also called a sim2real method. The sim2real training method efficiently handle the these two problems. The simulations and experiments are carried out to evaluate the performance and effectiveness of the proposed algorithm. Finally, it is demonstrated that the scale car in real world obtain the capability for autonomous driving.",0
"The prevalence of self-driving vehicles, which rely on deep reinforcement learning, is a noteworthy application of artificial intelligence. Presently, most self-driving techniques prioritize the direct acquisition of a self-driving control strategy from raw sensory data. However, this strategy's capacity for generalization is often restricted. To enhance the control strategy's generalization ability while ensuring safety, Unity designed a virtual simulation environment with varied driving scenarios. A theoretical model was established and analyzed in this environment, and then trained using a double Deep Q-network. After training, the model was transferred to a real car, a process referred to as the ""sim2real"" method. This method successfully managed both generalization and safety issues. To evaluate the algorithm's performance and effectiveness, simulations and experiments were conducted. Ultimately, the real car showed a capability for autonomous driving.",1
"In many environments, only a relatively small subset of the complete state space is necessary in order to accomplish a given task. We develop a simple technique using emergency stops (e-stops) to exploit this phenomenon. Using e-stops significantly improves sample complexity by reducing the amount of required exploration, while retaining a performance bound that efficiently trades off the rate of convergence with a small asymptotic sub-optimality gap. We analyze the regret behavior of e-stops and present empirical results in discrete and continuous settings demonstrating that our reset mechanism can provide order-of-magnitude speedups on top of existing reinforcement learning methods.",0
"A task can often be achieved using only a small portion of the state space in certain environments. To take advantage of this, we introduce a straightforward method that utilizes emergency stops (e-stops). This approach reduces the amount of exploration required, leading to a substantial improvement in sample complexity while maintaining a performance limit that balances convergence speed with a negligible sub-optimal gap. We evaluate the regret behavior of e-stops and provide evidence in both discrete and continuous scenarios that our reset mechanism can achieve order-of-magnitude acceleration in conjunction with existing reinforcement learning techniques.",1
"Posterior sampling for reinforcement learning (PSRL) is an effective method for balancing exploration and exploitation in reinforcement learning. Randomised value functions (RVF) can be viewed as a promising approach to scaling PSRL. However, we show that most contemporary algorithms combining RVF with neural network function approximation do not possess the properties which make PSRL effective, and provably fail in sparse reward problems. Moreover, we find that propagation of uncertainty, a property of PSRL previously thought important for exploration, does not preclude this failure. We use these insights to design Successor Uncertainties (SU), a cheap and easy to implement RVF algorithm that retains key properties of PSRL. SU is highly effective on hard tabular exploration benchmarks. Furthermore, on the Atari 2600 domain, it surpasses human performance on 38 of 49 games tested (achieving a median human normalised score of 2.09), and outperforms its closest RVF competitor, Bootstrapped DQN, on 36 of those.",0
"The technique of posterior sampling for reinforcement learning (PSRL) is useful for balancing the exploration and exploitation aspects of reinforcement learning. Randomised value functions (RVF) have the potential to enhance PSRL scalability, but many of the current algorithms that merge RVF with neural network function approximation lack the critical features that make PSRL effective, and they are unable to handle problems with sparse rewards. Even the propagation of uncertainty, which was once believed to be crucial for exploration, does not prevent this failure. With this knowledge, we created Successor Uncertainties (SU), an inexpensive and straightforward RVF algorithm that retains the essential properties of PSRL. SU performs exceptionally well on challenging tabular exploration benchmarks. Additionally, in the Atari 2600 domain, SU outperforms humans in 38 out of 49 games tested (with a median human normalised score of 2.09) and surpasses its closest RVF competitor, Bootstrapped DQN, in 36 of those games.",1
"We propose a technique for producing ""visual explanations"" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo at http://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.",0
"The production of ""visual explanations"" for decisions from a broad range of CNN-based models is made possible and more transparent through our proposed technique. Our method, Gradient-weighted Class Activation Mapping (Grad-CAM), involves utilizing the gradients of any target concept that flows into the final convolutional layer to create a course localization map that highlights significant image regions for predicting the concept. Grad-CAM can be applied to various CNN model-families without any architectural changes or re-training, including CNNs with fully-connected layers, those used for structured outputs, and those used in tasks with multimodal inputs or reinforcement learning. We combine Grad-CAM with fine-grained visualizations to generate a high-resolution class-discriminative visualization, which we apply to off-the-shelf image classification, captioning, and visual question answering (VQA) models, such as ResNet-based architectures. Our visualizations provide insight into image classification model failure modes, are robust to adversarial images, outperform previous methods on localization, are more faithful to the underlying model, and help identify dataset bias to achieve generalization. We also show that even non-attention based models can localize inputs for captioning and VQA. Through Grad-CAM, we identify important neurons and combine them with neuron names to provide textual explanations for model decisions. Furthermore, we conduct human studies to determine if Grad-CAM helps users establish appropriate trust in model predictions and successfully discern a 'stronger' model from a 'weaker' one. Our code is available at https://github.com/ramprs/grad-cam/, and we provide a demo at http://gradcam.cloudcv.org, as well as a video at youtu.be/COjUB9Izk6E.",1
"Neural architecture search (NAS) is proposed to automate the architecture design process and attracts overwhelming interest from both academia and industry. However, it is confronted with overfitting issue due to the high-dimensional search space composed by operator selection and skip connection of each layer. This paper explores the architecture overfitting issue in depth based on the reinforcement learning-based NAS framework. We show that the policy gradient method has deep correlations with the cross entropy minimization. Based on this correlation, we further demonstrate that, though the reward of NAS is sparse, the policy gradient method implicitly assign the reward to all operations and skip connections based on the sampling frequency. However, due to the inaccurate reward estimation, curse of dimensionality problem and the hierachical structure of neural networks, reward charateristics for operators and skip connections have intrinsic differences, the assigned rewards for the skip connections are extremely noisy and inaccurate. To alleviate this problem, we propose a neural architecture refinement approach that working with an initial state-of-the-art network structure and only refining its operators. Extensive experiments have demonstrated that the proposed method can achieve fascinated results, including classification, face recognition etc.",0
"The process of designing neural architecture can be automated through Neural Architecture Search (NAS), which has garnered significant attention from academia and industry. However, NAS faces challenges with overfitting due to the complex search space that comprises operator selection and skip connection of each layer. This paper delves into the overfitting issue of architecture in the reinforcement learning-based NAS framework. We establish a connection between the policy gradient method and cross-entropy minimization and reveal that the policy gradient method implicitly assigns rewards to all operations and skip connections based on the sampling frequency, even though the reward of NAS is sparse. However, due to the curse of dimensionality problem, inaccurate reward estimation, and hierarchical structure of neural networks, the rewards assigned to skip connections are highly inaccurate and noisy. To overcome this challenge, we propose a neural architecture refinement approach that refines the operators of an initial state-of-the-art network structure. The proposed method has been extensively tested and has achieved impressive results in classification, face recognition, and other applications.",1
"We present a new model-based algorithm for reinforcement learning (RL) which consists of explicit exploration and exploitation phases, and is applicable in large or infinite state spaces. The algorithm maintains a set of dynamics models consistent with current experience and explores by finding policies which induce high disagreement between their state predictions. It then exploits using the refined set of models or experience gathered during exploration. We show that under realizability and optimal planning assumptions, our algorithm provably finds a near-optimal policy with a number of samples that is polynomial in a structural complexity measure which we show to be low in several natural settings. We then give a practical approximation using neural networks and demonstrate its performance and sample efficiency in practice.",0
"A novel algorithm for reinforcement learning (RL) is introduced in this study. The algorithm is model-based and can be implemented in state spaces that are either large or infinite. It involves two phases, namely explicit exploration and exploitation. The exploration phase entails identifying policies that result in high disagreement between their state predictions. The algorithm maintains a set of dynamics models that are consistent with current experience. During the exploitation phase, the refined set of models or experience gathered during exploration is utilized. The algorithm is shown to identify a near-optimal policy using a polynomial number of samples, when realizability and optimal planning assumptions are met. The structural complexity measure is found to be low in several natural settings. A practical approximation using neural networks is presented, and its performance and sample efficiency are demonstrated in practice.",1
"Many real world tasks require multiple agents to work together. Multi-agent reinforcement learning (RL) methods have been proposed in recent years to solve these tasks, but current methods often fail to efficiently learn policies. We thus investigate the presence of a common weakness in single-agent RL, namely value function overestimation bias, in the multi-agent setting. Based on our findings, we propose an approach that reduces this bias by using double centralized critics. We evaluate it on six mixed cooperative-competitive tasks, showing a significant advantage over current methods. Finally, we investigate the application of multi-agent methods to high-dimensional robotic tasks and show that our approach can be used to learn decentralized policies in this domain.",0
"Collaboration among multiple agents is often necessary for many real-world tasks. Although multi-agent reinforcement learning (RL) methods have been developed to address this, they frequently struggle to efficiently learn policies. Therefore, we explore the prevalence of value function overestimation bias, a weakness commonly found in single-agent RL, in the multi-agent setting. Our research led us to propose a solution that mitigates this bias through the use of double centralized critics. We tested this approach on six mixed cooperative-competitive tasks and discovered that it outperforms existing methods. Furthermore, we applied our findings to high-dimensional robotic tasks and demonstrated that our approach can effectively learn decentralized policies.",1
"In many reinforcement learning tasks, the goal is to learn a policy to manipulate an agent, whose design is fixed, to maximize some notion of cumulative reward. The design of the agent's physical structure is rarely optimized for the task at hand. In this work, we explore the possibility of learning a version of the agent's design that is better suited for its task, jointly with the policy. We propose an alteration to the popular OpenAI Gym framework, where we parameterize parts of an environment, and allow an agent to jointly learn to modify these environment parameters along with its policy. We demonstrate that an agent can learn a better structure of its body that is not only better suited for the task, but also facilitates policy learning. Joint learning of policy and structure may even uncover design principles that are useful for assisted-design applications. Videos of results at https://designrl.github.io/",0
"The aim of many reinforcement learning tasks is to acquire a policy that enables manipulation of an agent, whose construction is fixed, to attain maximum cumulative reward. The agent's physical structure is usually not optimized for the task. This study delves into the possibility of learning an agent's design that is more suitable for the task, along with the policy. To achieve this, we propose an amendment to the widely used OpenAI Gym framework. We parameterize parts of the environment and allow the agent to learn how to modify these parameters jointly with its policy. The results show that the agent can learn a better body structure that is not only better suited for the task but also facilitates policy learning. Learning the policy and structure together can even reveal design principles that are beneficial for assisted-design applications. Check out the videos of the results on https://designrl.github.io/.",1
"Reinforcement Learning (RL) has achieved state-of-the-art results in domains such as robotics and games. We build on this previous work by applying RL algorithms to a selection of canonical online stochastic optimization problems with a range of practical applications: Bin Packing, Newsvendor, and Vehicle Routing. While there is a nascent literature that applies RL to these problems, there are no commonly accepted benchmarks which can be used to compare proposed approaches rigorously in terms of performance, scale, or generalizability. This paper aims to fill that gap. For each problem we apply both standard approaches as well as newer RL algorithms and analyze results. In each case, the performance of the trained RL policy is competitive with or superior to the corresponding baselines, while not requiring much in the way of domain knowledge. This highlights the potential of RL in real-world dynamic resource allocation problems.",0
"RL has proven to be highly effective in domains like robotics and games, and our research builds upon this by utilizing RL algorithms to solve various online stochastic optimization problems with practical applications, including Bin Packing, Newsvendor, and Vehicle Routing. While there have been some studies that have applied RL to these problems, there is a lack of widely accepted benchmarks that can be used to compare different approaches in terms of performance, scale, and generalizability. Our paper aims to address this gap by implementing standard and newer RL algorithms to these problems and analyzing the results. In all cases, the performance of the trained RL policy rivals or surpasses the corresponding baselines, without requiring extensive domain knowledge. This indicates the potential of RL in solving real-world dynamic resource allocation problems.",1
"This article reviews recent advances in multi-agent reinforcement learning algorithms for large-scale control systems and communication networks, which learn to communicate and cooperate. We provide an overview of this emerging field, with an emphasis on the decentralized setting under different coordination protocols. We highlight the evolution of reinforcement learning algorithms from single-agent to multi-agent systems, from a distributed optimization perspective, and conclude with future directions and challenges, in the hope to catalyze the growing synergy among distributed optimization, signal processing, and reinforcement learning communities.",0
"The focus of this article is on the latest developments in multi-agent reinforcement learning algorithms that are designed for managing large-scale control systems and communication networks. These algorithms are capable of learning to work together and communicate effectively. The article provides an overview of this emerging field, with a particular emphasis on the decentralized setting and various coordination protocols. It covers the evolution of reinforcement learning algorithms from single-agent to multi-agent systems from a distributed optimization perspective. The article concludes by discussing future directions and challenges, with the aim of fostering collaboration among the distributed optimization, signal processing, and reinforcement learning communities.",1
"Greenhouse environment is the key to influence crops production. However, it is difficult for classical control methods to give precise environment setpoints, such as temperature, humidity, light intensity and carbon dioxide concentration for greenhouse because it is uncertain nonlinear system. Therefore, an intelligent close loop control framework based on model embedded deep reinforcement learning (MEDRL) is designed for greenhouse environment control. Specifically, computer vision algorithms are used to recognize growing periods and sex of crops, followed by the crop growth models, which can be trained with different growing periods and sex. These model outputs combined with the cost factor provide the setpoints for greenhouse and feedback to the control system in real-time. The whole MEDRL system has capability to conduct optimization control precisely and conveniently, and costs will be greatly reduced compared with traditional greenhouse control approaches.",0
"The production of crops is heavily influenced by the greenhouse environment, but controlling the temperature, humidity, light intensity, and carbon dioxide concentration can be challenging with traditional methods due to the uncertain and nonlinear nature of the system. To address this, an intelligent closed-loop control framework has been developed based on model-embedded deep reinforcement learning (MEDRL) for greenhouse environment control. Computer vision algorithms are used to recognize the crop's growth periods and sex, and combined with crop growth models that have been trained with various growing periods and sexes. The resulting model outputs, along with cost factors, are used to set the greenhouse's parameters, which are continually monitored and adjusted in real-time by the control system. The MEDRL system provides more precise and convenient optimization control, as well as significant cost savings compared to traditional greenhouse control methods.",1
"Asynchronous Advantage Actor Critic (A3C) is an effective Reinforcement Learning (RL) algorithm for a wide range of tasks, such as Atari games and robot control. The agent learns policies and value function through trial-and-error interactions with the environment until converging to an optimal policy. Robustness and stability are critical in RL; however, neural network can be vulnerable to noise from unexpected sources and is not likely to withstand very slight disturbances. We note that agents generated from mild environment using A3C are not able to handle challenging environments. Learning from adversarial examples, we proposed an algorithm called Adversary Robust A3C (AR-A3C) to improve the agent's performance under noisy environments. In this algorithm, an adversarial agent is introduced to the learning process to make it more robust against adversarial disturbances, thereby making it more adaptive to noisy environments. Both simulations and real-world experiments are carried out to illustrate the stability of the proposed algorithm. The AR-A3C algorithm outperforms A3C in both clean and noisy environments.",0
"A3C is a successful RL algorithm that can be applied to various tasks, including Atari games and robot control. The agent acquires policies and value function by interacting with the environment until reaching an optimal policy. Nonetheless, RL requires robustness and stability, which can be undermined by unexpected noise that neural networks are susceptible to. A3C-generated agents struggle with challenging environments stemming from mild disturbances. To enhance the agent's performance under noisy conditions, we have developed a new algorithm, AR-A3C, by introducing an adversarial agent to the learning process. The AR-A3C algorithm increases the agent's adaptability to noisy environments and resilience against adversarial disturbances. Both simulations and real-world experiments demonstrate the proposed algorithm's stability. The AR-A3C algorithm surpasses A3C in both clean and noisy environments.",1
"Off-policy deep reinforcement learning (RL) algorithms are incapable of learning solely from batch offline data without online interactions with the environment, due to the phenomenon known as \textit{extrapolation error}. This is often due to past data available in the replay buffer that may be quite different from the data distribution under the current policy. We argue that most off-policy learning methods fundamentally suffer from a \textit{state distribution shift} due to the mismatch between the state visitation distribution of the data collected by the behavior and target policies. This data distribution shift between current and past samples can significantly impact the performance of most modern off-policy based policy optimization algorithms. In this work, we first do a systematic analysis of state distribution mismatch in off-policy learning, and then develop a novel off-policy policy optimization method to constraint the state distribution shift. To do this, we first estimate the state distribution based on features of the state, using a density estimator and then develop a novel constrained off-policy gradient objective that minimizes the state distribution shift. Our experimental results on continuous control tasks show that minimizing this distribution mismatch can significantly improve performance in most popular practical off-policy policy gradient algorithms.",0
"The phenomenon of extrapolation error makes it impossible for off-policy deep reinforcement learning (RL) algorithms to solely rely on batch offline data for learning, without any online interactions with the environment. This is because the data available in the replay buffer may differ greatly from the current policy’s data distribution. Our argument is that most off-policy learning methods are fundamentally impacted by a state distribution shift, which is caused by a mismatch between the state visitation distribution of the data collected by the behavior and target policies. This distribution shift between current and past samples can have a significant impact on the performance of modern off-policy based policy optimization algorithms. Therefore, we conduct a systematic analysis of state distribution mismatch in off-policy learning and develop a new off-policy policy optimization method that limits the state distribution shift. We use a density estimator to estimate the state distribution based on state features and then create a novel constrained off-policy gradient objective to minimize the state distribution shift. Our experimental results on continuous control tasks demonstrate that minimizing distribution mismatch can substantially enhance the performance of most popular practical off-policy policy gradient algorithms.",1
"Reinforcement Learning algorithms have recently been proposed to learn time-sequential control policies in the field of autonomous driving. Direct applications of Reinforcement Learning algorithms with discrete action space will yield unsatisfactory results at the operational level of driving where continuous control actions are actually required. In addition, the design of neural networks often fails to incorporate the domain knowledge of the targeting problem such as the classical control theories in our case. In this paper, we propose a hybrid model by combining Q-learning and classic PID (Proportion Integration Differentiation) controller for handling continuous vehicle control problems under dynamic driving environment. Particularly, instead of using a big neural network as Q-function approximation, we design a Quadratic Q-function over actions with multiple simple neural networks for finding optimal values within a continuous space. We also build an action network based on the domain knowledge of the control mechanism of a PID controller to guide the agent to explore optimal actions more efficiently.We test our proposed approach in simulation under two common but challenging driving situations, the lane change scenario and ramp merge scenario. Results show that the autonomous vehicle agent can successfully learn a smooth and efficient driving behavior in both situations.",0
"Recently, Reinforcement Learning algorithms have been suggested for learning time-sequential control policies in the realm of autonomous driving. However, directly applying these algorithms with a discrete action space would lead to unsatisfactory outcomes at the operational level of driving, which requires continuous control actions. Additionally, the design of neural networks often overlooks the domain knowledge of the targeted problem, such as classical control theories. To tackle these issues, this paper proposes a hybrid model that integrates Q-learning with a classic PID (Proportion Integration Differentiation) controller for managing continuous vehicle control problems in a dynamic driving environment. Instead of a large neural network for Q-function approximation, a Quadratic Q-function is designed over actions with multiple simple neural networks to find optimal values in a continuous space. Additionally, an action network is created based on the domain knowledge of the control mechanism of a PID controller to guide the agent in exploring optimal actions more efficiently. The proposed approach is tested in simulation for two common yet challenging driving scenarios: the lane change and ramp merge scenarios. The results indicate that the autonomous vehicle agent can successfully learn a smooth and efficient driving behavior in both situations.",1
"In this work we present ISA, a novel approach for learning and exploiting subgoals in reinforcement learning (RL). Our method relies on inducing an automaton whose transitions are subgoals expressed as propositional formulas over a set of observable events. A state-of-the-art inductive logic programming system is used to learn the automaton from observation traces perceived by the RL agent. The reinforcement learning and automaton learning processes are interleaved: a new refined automaton is learned whenever the RL agent generates a trace not recognized by the current automaton. We evaluate ISA in several gridworld problems and show that it performs similarly to a method for which automata are given in advance. We also show that the learned automata can be exploited to speed up convergence through reward shaping and transfer learning across multiple tasks. Finally, we analyze the running time and the number of traces that ISA needs to learn an automata, and the impact that the number of observable events has on the learner's performance.",0
"The work presented here introduces ISA, a new approach to reinforcement learning (RL) that focuses on learning and utilizing subgoals. Our method involves creating an automaton that uses propositional formulas to express subgoals based on observable events. This automaton is learned through the use of a state-of-the-art inductive logic programming system, which analyzes traces observed by the RL agent. The RL and automaton learning processes are intertwined, with the automaton being refined whenever the agent generates an unrecognized trace. We demonstrate the effectiveness of ISA in various gridworld problems, and show that it performs comparably to a method that relies on pre-existing automata. Furthermore, we illustrate how the learned automata can be leveraged to hasten convergence through reward shaping and transfer learning across multiple tasks. Lastly, we evaluate the running time of ISA, the number of traces required to learn an automaton, and the impact of the number of observable events on the learner's performance.",1
"Deploying trained convolutional neural networks (CNNs) to mobile devices is a challenging task because of the simultaneous requirements of the deployed model to be fast, lightweight and accurate. Designing and training a CNN architecture that does well on all three metrics is highly non-trivial and can be very time-consuming if done by hand. One way to solve this problem is to compress the trained CNN models before deploying to mobile devices. This work asks and answers three questions on compressing CNN models automatically: a) How to control the trade-off between speed, memory and accuracy during model compression? b) In practice, a deployed model may not see all classes and/or may not need to produce all class labels. Can this fact be used to improve the trade-off? c) How to scale the compression algorithm to execute within a reasonable amount of time for many deployments? The paper demonstrates that a model compression algorithm utilizing reinforcement learning with architecture search and knowledge distillation can answer these questions in the affirmative. Experimental results are provided for current state-of-the-art CNN model families for image feature extraction like VGG and ResNet with CIFAR datasets.",0
"The task of deploying trained convolutional neural networks (CNNs) onto mobile devices is a difficult undertaking because the model must be quick, lightweight, and precise. It is a challenging and time-consuming process to design and train a CNN architecture that can meet all three requirements. To address this issue, compressing the trained CNN models before deployment is a possible solution. This research investigates three key questions about compressing CNN models automatically: 1) How to balance the trade-off between speed, memory, and accuracy during model compression? 2) Can the fact that a deployed model may not see all classes or may not need to produce all class labels be used to improve the trade-off? 3) How can the compression algorithm be scaled to execute within a reasonable amount of time for multiple deployments? The paper showcases that a model compression algorithm that utilizes reinforcement learning with architecture search and knowledge distillation can provide affirmative answers to these questions. The experimental results for the current state-of-the-art CNN model families for image feature extraction, such as VGG and ResNet with CIFAR datasets, are presented.",1
"The deep Q-network (DQN) and return-based reinforcement learning are two promising algorithms proposed in recent years. DQN brings advances to complex sequential decision problems, while return-based algorithms have advantages in making use of sample trajectories. In this paper, we propose a general framework to combine DQN and most of the return-based reinforcement learning algorithms, named R-DQN. We show the performance of traditional DQN can be improved effectively by introducing return-based reinforcement learning. In order to further improve the R-DQN, we design a strategy with two measurements which can qualitatively measure the policy discrepancy. Moreover, we give the two measurements' bounds in the proposed R-DQN framework. We show that algorithms with our strategy can accurately express the trace coefficient and achieve a better approximation to return. The experiments, conducted on several representative tasks from the OpenAI Gym library, validate the effectiveness of the proposed measurements. The results also show that the algorithms with our strategy outperform the state-of-the-art methods.",0
"In recent years, the deep Q-network (DQN) and return-based reinforcement learning have emerged as promising algorithms. DQN is adept at solving complex sequential decision problems, while return-based algorithms excel at utilizing sample trajectories. With this in mind, we introduce R-DQN, a general framework that combines DQN with most return-based reinforcement learning algorithms. Our research suggests that the performance of traditional DQN can be significantly enhanced by integrating return-based reinforcement learning. To further improve R-DQN, we have devised a strategy that utilizes two measurements to qualitatively assess policy discrepancies. We have also provided the bounds for these measurements within the proposed R-DQN framework. Our experimental results, conducted on various tasks from OpenAI Gym library, demonstrate the effectiveness of our measurements and the superiority of algorithms that employ our strategy over existing state-of-the-art methods.",1
"We address the problem of active visual exploration of large 360{\deg} inputs. In our setting an active agent with a limited camera bandwidth explores its 360{\deg} environment by changing its viewing direction at limited discrete time steps. As such, it observes the world as a sequence of narrow field-of-view 'glimpses', deciding for itself where to look next. Our proposed method exceeds previous works' performance by a significant margin without the need for deep reinforcement learning or training separate networks as sidekicks. A key component of our system are the spatial memory maps that make the system aware of the glimpses' orientations (locations in the 360{\deg} image). Further, we stress the advantages of retina-like glimpses when the agent's sensor bandwidth and time-steps are limited. Finally, we use our trained model to do classification of the whole scene using only the information observed in the glimpses.",0
"The issue of actively exploring large 360{\deg} inputs is our focus. In our scenario, a limited camera bandwidth active agent navigates its 360{\deg} environment by altering its viewing direction at discrete time intervals. Consequently, it perceives the environment as a sequence of narrow field-of-view 'glimpses' and determines where to direct its gaze next. Our proposed approach performs substantially better than previous methods and does not require deep reinforcement learning or training separate networks as sidekicks. Our system's spatial memory maps are a crucial component that enables it to be conscious of the glimpses' orientations (locations in the 360{\deg} image). Moreover, we highlight the advantages of retina-like glimpses when the agent's sensor bandwidth and time-steps are limited. Finally, we utilize our trained model to classify the entire scene using only the glimpses' observed data.",1
"Panoramic video provides immersive and interactive experience by enabling humans to control the field of view (FoV) through head movement (HM). Thus, HM plays a key role in modeling human attention on panoramic video. This paper establishes a database collecting subjects' HM in panoramic video sequences. From this database, we find that the HM data are highly consistent across subjects. Furthermore, we find that deep reinforcement learning (DRL) can be applied to predict HM positions, via maximizing the reward of imitating human HM scanpaths through the agent's actions. Based on our findings, we propose a DRL-based HM prediction (DHP) approach with offline and online versions, called offline-DHP and online-DHP. In offline-DHP, multiple DRL workflows are run to determine potential HM positions at each panoramic frame. Then, a heat map of the potential HM positions, named the HM map, is generated as the output of offline-DHP. In online-DHP, the next HM position of one subject is estimated given the currently observed HM position, which is achieved by developing a DRL algorithm upon the learned offline-DHP model. Finally, the experiments validate that our approach is effective in both offline and online prediction of HM positions for panoramic video, and that the learned offline-DHP model can improve the performance of online-DHP.",0
"The use of panoramic video allows for a more engaging and interactive experience as it allows individuals to control the field of view through head movement. Therefore, head movement plays a significant role in determining human attention in panoramic video. This study has created a database of head movement data from subjects watching panoramic video sequences, which has shown to be consistent across different individuals. Additionally, the study has found that deep reinforcement learning can be used to predict head movement positions by imitating human head movement patterns. As a result, the study proposes an approach called DHP, which has both offline and online versions. In the offline version, multiple DRL workflows are used to determine potential head movement positions, which are then used to create a heat map called the HM map. In the online version, a DRL algorithm is used to estimate the next head movement position based on the currently observed position. The results of the experiments show that the DHP approach is effective in both offline and online prediction of head movement positions, and that the learned offline-DHP model can improve the performance of online-DHP.",1
"Model-based reinforcement learning algorithms tend to achieve higher sample efficiency than model-free methods. However, due to the inevitable errors of learned models, model-based methods struggle to achieve the same asymptotic performance as model-free methods.   In this paper, We propose a Policy Optimization method with Model-Based Uncertainty (POMBU)---a novel model-based approach---that can effectively improve the asymptotic performance using the uncertainty in Q-values. We derive an upper bound of the uncertainty, based on which we can approximate the uncertainty accurately and efficiently for model-based methods. We further propose an uncertainty-aware policy optimization algorithm that optimizes the policy conservatively to encourage performance improvement with high probability. This can significantly alleviate the overfitting of policy to inaccurate models.   Experiments show POMBU can outperform existing state-of-the-art policy optimization algorithms in terms of sample efficiency and asymptotic performance. Moreover, the experiments demonstrate the excellent robustness of POMBU compared to previous model-based approaches.",0
"The sample efficiency of model-based reinforcement learning algorithms is typically higher than that of model-free methods, but the learned models are prone to errors, limiting the model-based methods' asymptotic performance. This paper introduces a novel model-based approach called Policy Optimization method with Model-Based Uncertainty (POMBU). POMBU utilizes the uncertainty in Q-values to improve asymptotic performance. An upper bound of the uncertainty is derived, allowing for accurate and efficient approximation for model-based methods. An uncertainty-aware policy optimization algorithm is proposed to optimize the policy conservatively, encouraging performance improvement with high probability and alleviating overfitting of policy to inaccurate models. Experiments demonstrate that POMBU surpasses existing state-of-the-art policy optimization algorithms in terms of sample efficiency and asymptotic performance while maintaining excellent robustness compared to previous model-based approaches.",1
"We propose a novel approach to address one aspect of the non-stationarity problem in multi-agent reinforcement learning (RL), where the other agents may alter their policies due to environment changes during execution. This violates the Markov assumption that governs most single-agent RL methods and is one of the key challenges in multi-agent RL. To tackle this, we propose to train multiple policies for each agent and postpone the selection of the best policy at execution time. Specifically, we model the environment non-stationarity with a finite set of scenarios and train policies fitting each scenario. In addition to multiple policies, each agent also learns a policy predictor to determine which policy is the best with its local information. By doing so, each agent is able to adapt its policy when the environment changes and consequentially the other agents alter their policies during execution. We empirically evaluated our method on a variety of common benchmark problems proposed for multi-agent deep RL in the literature. Our experimental results show that the agents trained by our algorithm have better adaptiveness in changing environments and outperform the state-of-the-art methods in all the tested environments.",0
"Our innovative approach addresses a significant aspect of the non-stationarity issue in multi-agent reinforcement learning (RL). In this scenario, alterations in policies by other agents due to changes in the environment during execution disrupt the Markov assumption, which is a primary challenge in multi-agent RL. To combat this issue, we suggest training multiple policies for each agent and delaying the selection of the best policy until execution time. To model the environment non-stationarity, we utilize a finite set of scenarios and train policies that fit each scenario. Additionally, each agent learns a policy predictor to determine the best policy based on its local information. As a result, each agent can adjust its policy when the environment changes, and other agents modify their policies during execution. We tested our approach on various common benchmark problems proposed for multi-agent deep RL in the literature, and our experimental results demonstrate that our algorithm produces agents with superior adaptability in dynamic environments, outperforming the state-of-the-art methods in all tested environments.",1
"Model-based reinforcement learning strategies are believed to exhibit more significant sample complexity than model-free strategies to control dynamical systems,such as quadcopters.This belief that Model-based strategies that involve the use of well-trained neural networks for making such high-level decisions always give better performance can be dispelled by making use of Model-free policy search methods.This paper proposes the use of a model-free random searching strategy,called Augmented Random Search(ARS),which is a better and faster approach of linear policy training for continuous control tasks like controlling a Quadcopters flight.The method achieves state-of-the-art accuracy by eliminating the use of too much data for the training of neural networks that are present in the previous approaches to the task of Quadcopter control.The paper also highlights the performance results of the searching strategy used for this task in a strategically designed task environment with the help of simulations.Reward collection performance over 1000 episodes and agents behavior in flight for augmented random search is compared with that of the behavior for reinforcement learning state-of-the-art algorithm,called Deep Deterministic policy gradient(DDPG).Our simulations and results manifest that a high variability in performance is observed in commonly used strategies for sample efficiency of such tasks but the built policy network of ARS-Quad can react relatively accurately to step response providing a better performing alternative to reinforcement learning strategies.",0
"It is commonly believed that using model-based reinforcement learning strategies to control dynamic systems like quadcopters requires more samples and is more complex than using model-free strategies. However, this belief can be challenged by using model-free policy search methods such as Augmented Random Search (ARS). This paper proposes the use of ARS, a model-free random searching strategy, for linear policy training in continuous control tasks like quadcopter flights. ARS achieves state-of-the-art accuracy and eliminates the need for excessive data training of neural networks used in previous methods. Performance results of ARS in a strategically designed task environment are presented through simulations, and compared to the Deep Deterministic Policy Gradient algorithm. The simulations show that ARS-Quad has a higher variability in performance but can react accurately to step response, providing a better alternative to reinforcement learning strategies.",1
"Fictitious play with reinforcement learning is a general and effective framework for zero-sum games. However, using the current deep neural network models, the implementation of fictitious play faces crucial challenges. Neural network model training employs gradient descent approaches to update all connection weights, and thus is easy to forget the old opponents after training to beat the new opponents. Existing approaches often maintain a pool of historical policy models to avoid the forgetting. However, learning to beat a pool in stochastic games, i.e., a wide distribution over policy models, is either sample-consuming or insufficient to exploit all models with limited amount of samples. In this paper, we propose a learning process with neural fictitious play to alleviate the above issues. We train a single model as our policy model, which consists of sub-models and a selector. Everytime facing a new opponent, the model is expanded by adding a new sub-model, where only the new sub-model is updated instead of the whole model. At the same time, the selector is also updated to mix up the new sub-model with the previous ones at the state-level, so that the model is maintained as a behavior strategy instead of a wide distribution over policy models. Experiments on Kuhn poker, a grid-world Treasure Hunting game, and Mini-RTS environments show that the proposed approach alleviates the forgetting problem, and consequently improves the learning efficiency and the robustness of neural fictitious play.",0
"The utilization of fictitious play with reinforcement learning is an effective and encompassing framework for zero-sum games. However, its implementation faces significant challenges when using current deep neural network models. The training of neural network models involves the use of gradient descent approaches to modify all connection weights, which often leads to forgetting old opponents after training to defeat new ones. To combat this issue, many approaches employ a pool of historical policy models. However, learning to surpass a pool in stochastic games, which have a wide distribution over policy models, is either sample-intensive or insufficient to exploit all models with limited samples. This paper presents a learning process with neural fictitious play that mitigates these issues. A single model is trained as our policy model, which consists of sub-models and a selector. Every time a new opponent is encountered, the model is expanded by adding a new sub-model, and only the new sub-model is updated rather than the entire model. Moreover, the selector is updated to mix the new sub-model with the previous ones at the state-level, ensuring that the model is maintained as a behavior strategy, not a broad distribution over policy models. Experiments conducted on Kuhn poker, a grid-world Treasure Hunting game, and Mini-RTS environments demonstrate that the proposed approach alleviates the forgetting problem and enhances the learning efficiency and robustness of neural fictitious play.",1
In this extended abstract we introduce a novel control-tutored Q-learning approach (CTQL) as part of the ongoing effort in developing model-based and safe RL for continuous state spaces. We validate our approach by applying it to a challenging multi-agent herding control problem.,0
"This extended abstract presents the CTQL (Control-Tutored Q-Learning) approach, which is a new technique for developing safe and model-based RL in continuous state spaces. To demonstrate the effectiveness of our method, we apply it to a difficult multi-agent herding control scenario.",1
"This paper investigates the anti-jamming performance of a cognitive radar under a partially observable Markov decision process (POMDP) model. First, we obtain an explicit expression for uncertainty of jammer dynamics, which paves the way for illuminating the performance metric of probability of being jammed for the radar beyond a conventional signal-to-noise ratio ($\mathsf{SNR}$) based analysis. Considering two frequency hopping strategies developed in the framework of reinforcement learning (RL), this performance metric is analyzed with deep Q-network (DQN) and long short term memory (LSTM) networks under various uncertainty values. Finally, the requirement of the target network in the RL algorithm for both network architectures is replaced with a softmax operator. Simulation results show that this operator improves upon the performance of the traditional target network.",0
"The objective of this research is to examine how well a cognitive radar can resist jamming in a partially observable Markov decision process (POMDP) model. Initially, the study focuses on determining the level of uncertainty in jammer dynamics, which enables the probability of radar jamming to be evaluated beyond the conventional signal-to-noise ratio ($\mathsf{SNR}$) approach. Two frequency hopping strategies created through reinforcement learning (RL) are evaluated using deep Q-network (DQN) and long short term memory (LSTM) networks across various uncertainty levels. Finally, the traditional target network requirement for both network architectures in the RL algorithm is replaced by a softmax operator, leading to improved performance as indicated by simulation results.",1
"Pseudo-rehearsal allows neural networks to learn a sequence of tasks without forgetting how to perform in earlier tasks. Preventing forgetting is achieved by introducing a generative network which can produce data from previously seen tasks so that it can be rehearsed along side learning the new task. This has been found to be effective in both supervised and reinforcement learning. Our current work aims to further prevent forgetting by encouraging the generator to accurately generate features important for task retention. More specifically, the generator is improved by introducing a second discriminator into the Generative Adversarial Network which learns to classify between real and fake items from the intermediate activation patterns that they produce when fed through a continual learning agent. Using Atari 2600 games, we experimentally find that improving the generator can considerably reduce catastrophic forgetting compared to the standard pseudo-rehearsal methods used in deep reinforcement learning. Furthermore, we propose normalising the Q-values taught to the long-term system as we observe this substantially reduces catastrophic forgetting by minimising the interference between tasks' reward functions.",0
"The technique of pseudo-rehearsal allows neural networks to learn a series of tasks without losing the ability to perform earlier tasks. This is achieved by incorporating a generative network that can produce data from previous tasks, allowing for rehearsal while learning new tasks. This method has been proven effective in both supervised and reinforcement learning. Our research aims to enhance this method by improving the generator's ability to generate important task features. To achieve this, a second discriminator is introduced into the Generative Adversarial Network to classify real and fake items based on their intermediate activation patterns. We conducted experiments using Atari 2600 games and found that this improved generator significantly reduces catastrophic forgetting compared to standard pseudo-rehearsal methods used in deep reinforcement learning. Additionally, we suggest normalizing the Q-values taught to the long-term system to minimize interference between tasks' reward functions and further reduce catastrophic forgetting.",1
"Reinforcement learning has attracted great attention recently, especially policy gradient algorithms, which have been demonstrated on challenging decision making and control tasks. In this paper, we propose an active multi-step TD algorithm with adaptive stepsizes to learn actor and critic. Specifically, our model consists of two components: active stepsize learning and adaptive multi-step TD algorithm. Firstly, we divide the time horizon into chunks and actively select state and action inside each chunk. Then given the selected samples, we propose the adaptive multi-step TD, which generalizes TD($\lambda$), but adaptively switch on/off the backups from future returns of different steps. Particularly, the adaptive multi-step TD introduces a context-aware mechanism, here a binary classifier, which decides whether or not to turn on its future backups based on the context changes. Thus, our model is kind of combination of active learning and multi-step TD algorithm, which has the capacity for learning off-policy without the need of importance sampling. We evaluate our approach on both discrete and continuous space tasks in an off-policy setting respectively, and demonstrate competitive results compared to other reinforcement learning baselines.",0
"Recently, policy gradient algorithms have garnered significant attention in the field of reinforcement learning, owing to their successful performance in complex decision-making and control tasks. This study proposes an adaptive multi-step TD algorithm with active stepsizes to learn both the actor and critic components. The model comprises two parts, namely active stepsize learning and adaptive multi-step TD algorithm. The authors first divide the time horizon into chunks and actively select the state and action within each chunk. Then, using the selected samples, they suggest the adaptive multi-step TD method, which generalizes TD($\lambda$) and adapts to switch on/off the backups from future returns of different steps. The adaptive multi-step TD algorithm incorporates a context-aware mechanism, a binary classifier, which decides whether to turn on future backups based on the context changes. Hence, the proposed model combines active learning and multi-step TD algorithms and can learn off-policy without the need for importance sampling. The authors evaluate their approach on discrete and continuous space tasks in an off-policy setting, demonstrating competitive results compared to other reinforcement learning baselines.",1
"Most previous studies on multi-agent reinforcement learning focus on deriving decentralized and cooperative policies to maximize a common reward and rarely consider the transferability of trained policies to new tasks. This prevents such policies from being applied to more complex multi-agent tasks. To resolve these limitations, we propose a model that conducts both representation learning for multiple agents using hierarchical graph attention network and policy learning using multi-agent actor-critic. The hierarchical graph attention network is specially designed to model the hierarchical relationships among multiple agents that either cooperate or compete with each other to derive more advanced strategic policies. Two attention networks, the inter-agent and inter-group attention layers, are used to effectively model individual and group level interactions, respectively. The two attention networks have been proven to facilitate the transfer of learned policies to new tasks with different agent compositions and allow one to interpret the learned strategies. Empirically, we demonstrate that the proposed model outperforms existing methods in several mixed cooperative and competitive tasks.",0
"Previous research on multi-agent reinforcement learning has primarily focused on developing decentralized and cooperative policies that maximize a common reward. However, little attention has been given to the transferability of these policies to new tasks, which limits their application to more complex multi-agent tasks. To address this issue, we propose a model that incorporates representation learning for multiple agents using a hierarchical graph attention network and policy learning using multi-agent actor-critic. The hierarchical graph attention network is designed to model the hierarchical relationships between agents that cooperate or compete to derive advanced strategic policies. The model includes two attention networks, inter-agent and inter-group attention layers, which effectively model individual and group level interactions. These attention networks have been shown to facilitate the transfer of learned policies to new tasks with different agent compositions and allow for interpretation of the learned strategies. Our empirical results demonstrate that this model outperforms existing methods in various mixed cooperative and competitive tasks.",1
"Value iteration networks (VINs) have been demonstrated to have a good generalization ability for reinforcement learning tasks across similar domains. However, based on our experiments, a policy learned by VINs still fail to generalize well on the domain whose action space and feature space are not identical to those in the domain where it is trained. In this paper, we propose a transfer learning approach on top of VINs, termed Transfer VINs (TVINs), such that a learned policy from a source domain can be generalized to a target domain with only limited training data, even if the source domain and the target domain have domain-specific actions and features. We empirically verify that our proposed TVINs outperform VINs when the source and the target domains have similar but not identical action and feature spaces. Furthermore, we show that the performance improvement is consistent across different environments, maze sizes, dataset sizes as well as different values of hyperparameters such as number of iteration and kernel size.",0
"Although Value Iteration Networks (VINs) have demonstrated good generalization capabilities for reinforcement learning tasks in similar domains, our experiments have shown that policies learned by VINs struggle to generalize in domains with different action and feature spaces than those in which they were trained. To address this, we introduce Transfer VINs (TVINs), a transfer learning approach built on top of VINs that enables a policy learned in one domain to be transferred to a target domain with limited training data, even if the domains have different actions and features. Our empirical results show that TVINs outperform VINs when the source and target domains have similar but not identical action and feature spaces. Additionally, we demonstrate that this performance improvement is consistent across different environments, maze sizes, dataset sizes, and hyperparameters such as iteration number and kernel size.",1
"In this paper we introduce a simple approach for exploration in reinforcement learning (RL) that allows us to develop theoretically justified algorithms in the tabular case but that is also extendable to settings where function approximation is required. Our approach is based on the successor representation (SR), which was originally introduced as a representation defining state generalization by the similarity of successor states. Here we show that the norm of the SR, while it is being learned, can be used as a reward bonus to incentivize exploration. In order to better understand this transient behavior of the norm of the SR we introduce the substochastic successor representation (SSR) and we show that it implicitly counts the number of times each state (or feature) has been observed. We use this result to introduce an algorithm that performs as well as some theoretically sample-efficient approaches. Finally, we extend these ideas to a deep RL algorithm and show that it achieves state-of-the-art performance in Atari 2600 games when in a low sample-complexity regime.",0
"This article presents a straightforward method for exploration in reinforcement learning (RL) that permits the creation of theoretically grounded algorithms in the tabular case and can be adapted to settings that require function approximation. The approach is centered around the successor representation (SR), which was originally introduced to define state generalization by the similarity of successor states. The paper shows that the norm of the SR can be employed as a reward bonus during learning to encourage exploration. To gain insight into the norm's transient behavior during learning, the substochastic successor representation (SSR) is introduced, which implicitly counts the number of times each state (or feature) is observed. This discovery is used to develop an algorithm that performs as well as some theoretically sample-efficient approaches. Finally, the ideas are extended to a deep RL algorithm, which achieves state-of-the-art performance in Atari 2600 games when in a low sample-complexity regime.",1
"In environments with continuous state and action spaces, state-of-the-art actor-critic reinforcement learning algorithms can solve very complex problems, yet can also fail in environments that seem trivial, but the reason for such failures is still poorly understood. In this paper, we contribute a formal explanation of these failures in the particular case of sparse reward and deterministic environments. First, using a very elementary control problem, we illustrate that the learning process can get stuck into a fixed point corresponding to a poor solution. Then, generalizing from the studied example, we provide a detailed analysis of the underlying mechanisms which results in a new understanding of one of the convergence regimes of these algorithms. The resulting perspective casts a new light on already existing solutions to the issues we have highlighted, and suggests other potential approaches.",0
"State-of-the-art actor-critic reinforcement learning algorithms can tackle highly complex problems in environments with continuous state and action spaces. However, they may encounter difficulties in seemingly straightforward environments, and the reasons for these failures are not well understood. This paper offers a formal explanation of such failures in the case of deterministic environments with sparse rewards. We begin by demonstrating how the learning process can become trapped in a suboptimal solution using a simple control problem. We then extend our analysis to provide a comprehensive examination of the underlying mechanisms that lead to convergence issues in these algorithms. This new perspective sheds light on existing solutions and highlights alternative approaches that could prove fruitful.",1
"Sample inefficiency is a long-lasting problem in reinforcement learning (RL). The state-of-the-art estimates the optimal action values while it usually involves an extensive search over the state-action space and unstable optimization. Towards the sample-efficient RL, we propose ranking policy gradient (RPG), a policy gradient method that learns the optimal rank of a set of discrete actions. To accelerate the learning of policy gradient methods, we establish the equivalence between maximizing the lower bound of return and imitating a near-optimal policy without accessing any oracles. These results lead to a general off-policy learning framework, which preserves the optimality, reduces variance, and improves the sample-efficiency. Furthermore, the sample complexity of RPG does not depend on the dimension of state space, which enables RPG for large-scale problems. We conduct extensive experiments showing that when consolidating with the off-policy learning framework, RPG substantially reduces the sample complexity, comparing to the state-of-the-art.",0
"Inefficient sampling has been a persistent issue in reinforcement learning (RL) for a considerable amount of time. The current best practice for estimating optimal action values involves a search of the state-action space, which can be unstable and time-consuming. To address this problem, we introduce the ranking policy gradient (RPG), a policy gradient method that learns the optimal rank of a set of discrete actions. We demonstrate that maximizing the lower bound of return is equivalent to imitating a near-optimal policy without any oracles. This leads us to a general off-policy learning framework that improves optimality, reduces variance, and enhances sample efficiency. Additionally, the sample complexity of RPG is independent of the state space dimension, making it suitable for large-scale problems. Our experiments show that, when combined with the off-policy learning framework, RPG significantly reduces sample complexity compared to the current state-of-the-art.",1
"Continual learning is the problem of learning new tasks or knowledge while protecting old knowledge and ideally generalizing from old experience to learn new tasks faster. Neural networks trained by stochastic gradient descent often degrade on old tasks when trained successively on new tasks with different data distributions. This phenomenon, referred to as catastrophic forgetting, is considered a major hurdle to learning with non-stationary data or sequences of new tasks, and prevents networks from continually accumulating knowledge and skills. We examine this issue in the context of reinforcement learning, in a setting where an agent is exposed to tasks in a sequence. Unlike most other work, we do not provide an explicit indication to the model of task boundaries, which is the most general circumstance for a learning agent exposed to continuous experience. While various methods to counteract catastrophic forgetting have recently been proposed, we explore a straightforward, general, and seemingly overlooked solution - that of using experience replay buffers for all past events - with a mixture of on- and off-policy learning, leveraging behavioral cloning. We show that this strategy can still learn new tasks quickly yet can substantially reduce catastrophic forgetting in both Atari and DMLab domains, even matching the performance of methods that require task identities. When buffer storage is constrained, we confirm that a simple mechanism for randomly discarding data allows a limited size buffer to perform almost as well as an unbounded one.",0
"The challenge of continual learning involves acquiring new knowledge or skills while retaining previous ones and applying them to learn new tasks more efficiently. However, neural networks trained by stochastic gradient descent tend to deteriorate when trained on new tasks with different data distributions, leading to catastrophic forgetting. This impedes the accumulation of knowledge and skills, making it difficult to learn with non-stationary data or sequences of new tasks. In this study, we investigate this issue in the context of reinforcement learning, where an agent is exposed to tasks in succession without explicit task boundaries. Instead of using existing methods to counteract catastrophic forgetting, we propose a straightforward yet overlooked solution - using experience replay buffers for all past events, combined with a mix of on- and off-policy learning and behavioral cloning. Our results show that this approach can still facilitate quick learning of new tasks while significantly reducing catastrophic forgetting in both Atari and DMLab domains, even without task identities. In situations where buffer storage is limited, we demonstrate that randomly discarding data can enable a limited-size buffer to perform almost as well as an unbounded one.",1
"Reinforcement Learning (RL) has demonstrated state-of-the-art results in a number of autonomous system applications, however many of the underlying algorithms rely on black-box predictions. This results in poor explainability of the behaviour of these systems, raising concerns as to their use in safety-critical applications. Recent work has demonstrated that uncertainty-aware models exhibit more cautious behaviours through the incorporation of model uncertainty estimates. In this work, we build on Probabilistic Backpropagation to introduce a fully Bayesian Recurrent Neural Network architecture. We apply this within a Safe RL scenario, and demonstrate that the proposed method significantly outperforms a popular approach for obtaining model uncertainties in collision avoidance tasks. Furthermore, we demonstrate that the proposed approach requires less training and is far more efficient than the current leading method, both in terms of compute resource and memory footprint.",0
"Although Reinforcement Learning (RL) has achieved impressive results in various autonomous system applications, its underlying algorithms often rely on opaque predictions. Consequently, these systems' behavior lacks transparency, raising concerns about their suitability for safety-critical applications. Recent research has shown that models that account for uncertainty tend to exhibit more cautious behavior by incorporating model uncertainty estimates. In this study, we enhance the Probabilistic Backpropagation by introducing a fully Bayesian Recurrent Neural Network architecture. We apply this approach to a Safe RL scenario, and our results demonstrate that our method outperforms a popular technique for obtaining model uncertainties in collision avoidance tasks. Furthermore, our approach requires less training and is more efficient than the current leading method in terms of compute resource and memory footprint.",1
"Recurrent neural networks (RNNs) for reinforcement learning (RL) have shown distinct advantages, e.g., solving memory-dependent tasks and meta-learning. However, little effort has been spent on improving RNN architectures and on understanding the underlying neural mechanisms for performance gain. In this paper, we propose a novel, multiple-timescale, stochastic RNN for RL. Empirical results show that the network can autonomously learn to abstract sub-goals and can self-develop an action hierarchy using internal dynamics in a challenging continuous control task. Furthermore, we show that the self-developed compositionality of the network enhances faster re-learning when adapting to a new task that is a re-composition of previously learned sub-goals, than when starting from scratch. We also found that improved performance can be achieved when neural activities are subject to stochastic rather than deterministic dynamics.",0
"Recurrent neural networks (RNNs) have demonstrated significant advantages in reinforcement learning (RL), such as solving memory-dependent tasks and meta-learning. Nevertheless, little effort has been made toward enhancing RNN architectures and understanding the neural mechanisms that lead to performance improvements. In this study, we introduce a novel stochastic RNN with multiple timescales for RL. The empirical results indicate that the network can learn sub-goals independently and develop an action hierarchy using internal dynamics in a challenging continuous control task. Additionally, we show that the network's self-developed compositional structure facilitates faster re-learning when adapting to a new task that consists of previously learned sub-goals, compared to starting from scratch. We also discovered that introducing stochasticity to neural activities improves the overall performance of the network compared to deterministic dynamics.",1
"In reinforcement learning (RL) research, it is common to assume access to direct online interactions with the environment. However in many real-world applications, access to the environment is limited to a fixed offline dataset of logged experience. In such settings, standard RL algorithms have been shown to diverge or otherwise yield poor performance. Accordingly, recent work has suggested a number of remedies to these issues. In this work, we introduce a general framework, behavior regularized actor critic (BRAC), to empirically evaluate recently proposed methods as well as a number of simple baselines across a variety of offline continuous control tasks. Surprisingly, we find that many of the technical complexities introduced in recent methods are unnecessary to achieve strong performance. Additional ablations provide insights into which design choices matter most in the offline RL setting.",0
"It is often assumed in reinforcement learning (RL) research that direct online interactions with the environment are readily available. However, this is not the case in many real-world applications where access to the environment is restricted to a fixed offline dataset of logged experience. This limitation has resulted in poor performance or divergence of standard RL algorithms. To address this issue, recent studies have proposed several solutions. Our study introduces the behavior regularized actor critic (BRAC) framework, which evaluates recently proposed methods and simple baselines across various offline continuous control tasks. Surprisingly, we discovered that many of the technical complexities introduced in recent methods are unnecessary to achieve strong performance. Furthermore, we provide additional insights into which design choices matter most in the offline RL setting through ablations.",1
"This paper studies the problem of spectrum shortage in an unmanned aerial vehicle (UAV) network during critical missions such as wildfire monitoring, search and rescue, and disaster monitoring. Such applications involve a high demand for high-throughput data transmissions such as real-time video-, image-, and voice- streaming where the assigned spectrum to the UAV network may not be adequate to provide the desired Quality of Service (QoS). In these scenarios, the aerial network can borrow an additional spectrum from the available terrestrial networks in the trade of a relaying service for them. We propose a spectrum sharing model in which the UAVs are grouped into two classes of relaying UAVs that service the spectrum owner and the sensing UAVs that perform the disaster relief mission using the obtained spectrum. The operation of the UAV network is managed by a hierarchical mechanism in which a central controller assigns the tasks of the UAVs based on their resources and determine their operation region based on the level of priority of impacted areas and then the UAVs autonomously fine-tune their position using a model-free reinforcement learning algorithm to maximize the individual throughput and prolong their lifetime. We analyze the performance and the convergence for the proposed method analytically and with extensive simulations in different scenarios.",0
"The focus of this research is the issue of limited spectrum availability in unmanned aerial vehicle (UAV) networks during critical missions such as search and rescue, wildfire monitoring, and disaster relief. These missions require high throughput data transmission, including real-time video, image, and voice streaming, which may not be possible with the assigned spectrum. To address this, the UAV network can borrow additional spectrum from terrestrial networks in exchange for a relaying service. The paper proposes a spectrum sharing model with two classes of UAVs - relaying UAVs that provide service to the spectrum owner and sensing UAVs that perform the disaster relief mission. A hierarchical mechanism managed by a central controller assigns tasks to UAVs based on their resources, determines their operational areas based on priority, and enables UAVs to optimize their position and maximize throughput using a model-free reinforcement learning algorithm. The proposed method's performance and convergence are analyzed through analytical and simulation-based approaches in various scenarios.",1
"Deep reinforcement learning requires a heavy price in terms of sample efficiency and overparameterization in the neural networks used for function approximation. In this work, we use tensor factorization in order to learn more compact representation for reinforcement learning policies. We show empirically that in the low-data regime, it is possible to learn online policies with 2 to 10 times less total coefficients, with little to no loss of performance. We also leverage progress in second order optimization, and use the theory of wavelet scattering to further reduce the number of learned coefficients, by foregoing learning the topmost convolutional layer filters altogether. We evaluate our results on the Atari suite against recent baseline algorithms that represent the state-of-the-art in data efficiency, and get comparable results with an order of magnitude gain in weight parsimony.",0
"The use of deep reinforcement learning can lead to reduced sample efficiency and a need for overparameterization in neural networks. In this study, we employ tensor factorization to obtain more concise representations for reinforcement learning policies. Our findings reveal that it is possible to achieve online policies with significantly fewer coefficients, without compromising performance, particularly in situations where data is scarce. We also implement second-order optimization and the wavelet scattering theory to further reduce the number of learned coefficients. By omitting the topmost convolutional layer filters, we demonstrate an order of magnitude improvement in weight parsimony while still achieving comparable results to recent baseline algorithms in the Atari suite.",1
"Order dispatching and driver repositioning (also known as fleet management) in the face of spatially and temporally varying supply and demand are central to a ride-sharing platform marketplace. Hand-crafting heuristic solutions that account for the dynamics in these resource allocation problems is difficult, and may be better handled by an end-to-end machine learning method. Previous works have explored machine learning methods to the problem from a high-level perspective, where the learning method is responsible for either repositioning the drivers or dispatching orders, and as a further simplification, the drivers are considered independent agents maximizing their own reward functions. In this paper we present a deep reinforcement learning approach for tackling the full fleet management and dispatching problems. In addition to treating the drivers as individual agents, we consider the problem from a system-centric perspective, where a central fleet management agent is responsible for decision-making for all drivers.",0
"The successful operation of a ride-sharing platform marketplace heavily relies on effective order dispatching and driver repositioning, also known as fleet management, in response to the constantly changing supply and demand. Developing manual heuristic solutions that take into account the dynamic nature of these resource allocation challenges is a challenging task, and a complete machine learning approach may be more suitable. Prior research has investigated machine learning methods for solving this issue, focusing on either driver repositioning or order dispatching, while considering drivers as independent agents with their own reward functions. The present study introduces a deep reinforcement learning method that addresses the complete fleet management and dispatching problems. In addition to treating drivers as individual agents, the approach takes a system-centric view, with a central fleet management agent making decisions for all drivers.",1
"Learning transferable knowledge across similar but different settings is a fundamental component of generalized intelligence. In this paper, we approach the transfer learning challenge from a causal theory perspective. Our agent is endowed with two basic yet general theories for transfer learning: (i) a task shares a common abstract structure that is invariant across domains, and (ii) the behavior of specific features of the environment remain constant across domains. We adopt a Bayesian perspective of causal theory induction and use these theories to transfer knowledge between environments. Given these general theories, the goal is to train an agent by interactively exploring the problem space to (i) discover, form, and transfer useful abstract and structural knowledge, and (ii) induce useful knowledge from the instance-level attributes observed in the environment. A hierarchy of Bayesian structures is used to model abstract-level structural causal knowledge, and an instance-level associative learning scheme learns which specific objects can be used to induce state changes through interaction. This model-learning scheme is then integrated with a model-based planner to achieve a task in the OpenLock environment, a virtual ``escape room'' with a complex hierarchy that requires agents to reason about an abstract, generalized causal structure. We compare performances against a set of predominate model-free reinforcement learning(RL) algorithms. RL agents showed poor ability transferring learned knowledge across different trials. Whereas the proposed model revealed similar performance trends as human learners, and more importantly, demonstrated transfer behavior across trials and learning situations.",0
"The acquisition of transferable knowledge in diverse contexts is a crucial aspect of general intelligence. Our study tackles the challenge of transfer learning through the lens of causal theory. Our approach equips our agent with two fundamental theories for transfer learning: (i) tasks possess a common abstract structure that remains unchanged across domains, and (ii) specific environmental features maintain consistency across domains. Employing a Bayesian perspective on causal theory induction, we employ these theories to transfer knowledge between domains. Our objective is to train an agent by interactively exploring the problem space to (i) identify, create, and transfer valuable abstract and structural knowledge, and (ii) elicit useful knowledge from the instance-level characteristics observed in the environment. We leverage a hierarchy of Bayesian structures to model abstract-level structural causal knowledge, and an instance-level associative learning scheme to learn which specific objects can cause state changes through interaction. We integrate this model-learning scheme with a model-based planner to accomplish a task in the OpenLock environment, a virtual ""escape room"" with a complicated hierarchy that necessitates agents to reason about an abstract, generalized causal structure. We compare the performance of our approach to a set of preeminent model-free reinforcement learning algorithms. Reinforcement learning agents demonstrated poor ability to transfer learned knowledge across different trials. Conversely, our proposed model exhibited comparable performance trends to human learners, and more significantly, demonstrated transfer behavior across trials and learning situations.",1
"Off-policy reinforcement learning aims to leverage experience collected from prior policies for sample-efficient learning. However, in practice, commonly used off-policy approximate dynamic programming methods based on Q-learning and actor-critic methods are highly sensitive to the data distribution, and can make only limited progress without collecting additional on-policy data. As a step towards more robust off-policy algorithms, we study the setting where the off-policy experience is fixed and there is no further interaction with the environment. We identify bootstrapping error as a key source of instability in current methods. Bootstrapping error is due to bootstrapping from actions that lie outside of the training data distribution, and it accumulates via the Bellman backup operator. We theoretically analyze bootstrapping error, and demonstrate how carefully constraining action selection in the backup can mitigate it. Based on our analysis, we propose a practical algorithm, bootstrapping error accumulation reduction (BEAR). We demonstrate that BEAR is able to learn robustly from different off-policy distributions, including random and suboptimal demonstrations, on a range of continuous control tasks.",0
"The goal of off-policy reinforcement learning is to improve efficiency by using past policies as a basis for learning. However, commonly used methods such as Q-learning and actor-critic techniques are highly sensitive to data distribution, making only limited progress without additional on-policy data. To address this, we explore a scenario where off-policy experience is fixed, and there is no further interaction with the environment. We identify bootstrapping error as a major source of instability and propose a practical algorithm, BEAR, to mitigate it. Our theoretical analysis shows that carefully constraining action selection during the backup can reduce bootstrapping error accumulation. We demonstrate that BEAR can learn robustly from various off-policy distributions, including random and suboptimal demonstrations, on a range of continuous control tasks.",1
"Quantum computing is a computational paradigm with the potential to outperform classical methods for a variety of problems. Proposed recently, the Quantum Approximate Optimization Algorithm (QAOA) is considered as one of the leading candidates for demonstrating quantum advantage in the near term. QAOA is a variational hybrid quantum-classical algorithm for approximately solving combinatorial optimization problems. The quality of the solution obtained by QAOA for a given problem instance depends on the performance of the classical optimizer used to optimize the variational parameters. In this paper, we formulate the problem of finding optimal QAOA parameters as a learning task in which the knowledge gained from solving training instances can be leveraged to find high-quality solutions for unseen test instances. To this end, we develop two machine-learning-based approaches. Our first approach adopts a reinforcement learning (RL) framework to learn a policy network to optimize QAOA circuits. Our second approach adopts a kernel density estimation (KDE) technique to learn a generative model of optimal QAOA parameters. In both approaches, the training procedure is performed on small-sized problem instances that can be simulated on a classical computer; yet the learned RL policy and the generative model can be used to efficiently solve larger problems. Extensive simulations using the IBM Qiskit Aer quantum circuit simulator demonstrate that our proposed RL- and KDE-based approaches reduce the optimality gap by factors up to 30.15 when compared with other commonly used off-the-shelf optimizers.",0
"Quantum computing has the potential to surpass classical methods for various problems, and the Quantum Approximate Optimization Algorithm (QAOA) is a promising contender for demonstrating quantum advantage in the near future. QAOA is a hybrid quantum-classical algorithm designed to solve combinatorial optimization problems approximately. The quality of the solution depends on the performance of the classical optimizer used to optimize the variational parameters. In this study, we present two machine-learning-based approaches to find optimal QAOA parameters by formulating the problem as a learning task. The first approach uses reinforcement learning (RL) to train a policy network that optimizes QAOA circuits, while the second approach uses kernel density estimation (KDE) to learn a generative model of optimal QAOA parameters. Both approaches are trained on small-sized problem instances that can be simulated on a classical computer, but the learned policy and generative model can efficiently solve larger problems. We demonstrate through extensive simulations using the IBM Qiskit Aer quantum circuit simulator that our proposed approaches reduce the optimality gap by factors up to 30.15 compared to commonly used off-the-shelf optimizers.",1
"Biological intelligence can learn to solve many diverse tasks in a data efficient manner by re-using basic knowledge and skills from one task to another. Furthermore, many of such skills are acquired without explicit supervision in an intrinsically driven fashion. This is in contrast to the state-of-the-art reinforcement learning agents, which typically start learning each new task from scratch and struggle with knowledge transfer. In this paper we propose a principled way to learn a basis set of policies, which, when recombined through generalised policy improvement, come with guarantees on the coverage of the final task space. In particular, we concentrate on solving goal-based downstream tasks where the execution order of actions is not important. We demonstrate both theoretically and empirically that learning a small number of policies that reach intrinsically specified goal regions in a disentangled latent space can be re-used to quickly achieve a high level of performance on an exponentially larger number of externally specified, often significantly more complex downstream tasks. Our learning pipeline consists of two stages. First, the agent learns to perform intrinsically generated, goal-based tasks in the total absence of environmental rewards. Second, the agent leverages this experience to quickly achieve a high level of performance on numerous diverse externally specified tasks.",0
"By reusing basic knowledge and skills from one task to another, biological intelligence can efficiently solve a variety of tasks. Additionally, many of these skills are learned without explicit instruction in an intrinsically driven manner. Unlike current reinforcement learning agents that struggle with knowledge transfer and start learning each new task from scratch, we propose a systematic approach to learn a set of policies that can be recombined to cover the final task space. Our focus is on goal-based downstream tasks where the order of actions is insignificant. We show that by learning a small number of policies that reach intrinsically specified goal regions in a disentangled latent space, we can achieve high performance on a significantly larger number of externally specified tasks. Our learning process comprises two stages: first, the agent learns to perform goal-based tasks without environmental rewards, and second, it leverages this experience to excel in diverse externally specified tasks.",1
"Racial equality is an important theme of international human rights law, but it has been largely obscured when the overall face recognition accuracy is pursued blindly. More facts indicate racial bias indeed degrades the fairness of recognition system and the error rates on non-Caucasians are usually much higher than Caucasians. To encourage fairness, we introduce the idea of adaptive margin to learn balanced performance for different races based on large margin losses. A reinforcement learning based race balance network (RL-RBN) is proposed. We formulate the process of finding the optimal margins for non-Caucasians as a Markov decision process and employ deep Q-learning to learn policies for an agent to select appropriate margin by approximating the Q-value function. Guided by the agent, the skewness of feature scatter between races can be reduced. Besides, we provide two ethnicity aware training datasets, called BUPT-Globalface and BUPT-Balancedface dataset, which can be utilized to study racial bias from both data and algorithm aspects. Extensive experiments on RFW database show that RL-RBN successfully mitigates racial bias and learns more balanced performance for different races.",0
"The principle of racial equality is a crucial aspect of international human rights law. However, it is often overlooked in the pursuit of overall facial recognition accuracy. Numerous studies have demonstrated that racial bias can significantly compromise the fairness of recognition systems, with higher error rates observed in non-Caucasian individuals. To promote fairness, we propose the use of adaptive margins to achieve balanced performance across different races, employing a reinforcement learning-based race balance network (RL-RBN). Specifically, we utilize deep Q-learning to approximate the Q-value function and enable the agent to select the most appropriate margin for non-Caucasians, with the aim of reducing the skewness of feature scatter between races. We also introduce two ethnicity-aware training datasets, BUPT-Globalface and BUPT-Balancedface, to facilitate research on racial bias from both data and algorithmic perspectives. Extensive experiments on the RFW database confirm the efficacy of RL-RBN in mitigating racial bias and promoting more equitable performance across different races.",1
"Deep reinforcement learning for high dimensional, hierarchical control tasks usually requires the use of complex neural networks as functional approximators, which can lead to inefficiency, instability and even divergence in the training process. Here, we introduce stacked deep Q learning (SDQL), a flexible modularized deep reinforcement learning architecture, that can enable finding of optimal control policy of control tasks consisting of multiple linear stages in a stable and efficient way. SDQL exploits the linear stage structure by approximating the Q function via a collection of deep Q sub-networks stacking along an axis marking the stage-wise progress of the whole task. By back-propagating the learned state values from later stages to earlier stages, all sub-networks co-adapt to maximize the total reward of the whole task, although each sub-network is responsible for learning optimal control policy for its own stage. This modularized architecture offers considerable flexibility in terms of environment and policy modeling, as it allows choices of different state spaces, action spaces, reward structures, and Q networks for each stage, Further, the backward stage-wise training procedure of SDQL can offers additional transparency, stability, and flexibility to the training process, thus facilitating model fine-tuning and hyper-parameter search. We demonstrate that SDQL is capable of learning competitive strategies for problems with characteristics of high-dimensional state space, heterogeneous action space(both discrete and continuous), multiple scales, and sparse and delayed rewards.",0
"To successfully tackle high dimensional, hierarchical control tasks using deep reinforcement learning, it is common to employ complex neural networks as functional approximators. However, this approach can lead to inefficiency, instability and even divergence in the training process. To address these issues, we have developed stacked deep Q learning (SDQL), a modularized deep reinforcement learning architecture that enables the discovery of optimal control policies for tasks consisting of multiple linear stages in a stable and efficient manner. SDQL leverages the linear stage structure by approximating the Q function through a collection of deep Q sub-networks that stack along an axis representing the stage-wise progress of the whole task. By back-propagating the learned state values from later stages to earlier ones, all sub-networks co-adapt to maximize the total reward of the whole task, while each sub-network is responsible for learning optimal control policies for its own stage. This modularized architecture offers great flexibility in terms of environment and policy modeling, as it allows for different state spaces, action spaces, reward structures, and Q networks for each stage. Furthermore, the backward stage-wise training procedure of SDQL provides additional transparency, stability, and flexibility to the training process, facilitating model fine-tuning and hyper-parameter search. We demonstrate that SDQL can effectively learn competitive strategies for problems with characteristics such as high-dimensional state space, heterogeneous action space (both discrete and continuous), multiple scales, and sparse and delayed rewards.",1
"In reinforcement learning (RL), agents often operate in partially observed and uncertain environments. Model-based RL suggests that this is best achieved by learning and exploiting a probabilistic model of the world. 'Active inference' is an emerging normative framework in cognitive and computational neuroscience that offers a unifying account of how biological agents achieve this. On this framework, inference, learning and action emerge from a single imperative to maximize the Bayesian evidence for a niched model of the world. However, implementations of this process have thus far been restricted to low-dimensional and idealized situations. Here, we present a working implementation of active inference that applies to high-dimensional tasks, with proof-of-principle results demonstrating efficient exploration and an order of magnitude increase in sample efficiency over strong model-free baselines. Our results demonstrate the feasibility of applying active inference at scale and highlight the operational homologies between active inference and current model-based approaches to RL.",0
"Reinforcement learning (RL) is often used by agents in environments that are partially observed and uncertain. To achieve this, model-based RL suggests that agents should learn and exploit a probabilistic model of the world. An emerging framework in cognitive and computational neuroscience called 'active inference' offers a comprehensive explanation of how biological agents do this. This framework suggests that inference, learning, and action come from one imperative to maximize the Bayesian evidence for a niched model of the world. However, active inference has only been implemented in low-dimensional and idealized situations. Our study presents a working implementation of active inference that applies to high-dimensional tasks and demonstrates efficient exploration and a significant increase in sample efficiency over strong model-free baselines. These results show that active inference can be applied at scale and highlight the similarities between active inference and current model-based approaches to RL.",1
"Customer services are critical to all companies, as they may directly connect to the brand reputation. Due to a great number of customers, e-commerce companies often employ multiple communication channels to answer customers' questions, for example, chatbot and hotline. On one hand, each channel has limited capacity to respond to customers' requests, on the other hand, customers have different preferences over these channels. The current production systems are mainly built based on business rules, which merely considers tradeoffs between resources and customers' satisfaction. To achieve the optimal tradeoff between resources and customers' satisfaction, we propose a new framework based on deep reinforcement learning, which directly takes both resources and user model into account. In addition to the framework, we also propose a new deep-reinforcement-learning based routing method-double dueling deep Q-learning with prioritized experience replay (PER-DoDDQN). We evaluate our proposed framework and method using both synthetic and a real customer service log data from a large financial technology company. We show that our proposed deep-reinforcement-learning based framework is superior to the existing production system. Moreover, we also show our proposed PER-DoDDQN is better than all other deep Q-learning variants in practice, which provides a more optimal routing plan. These observations suggest that our proposed method can seek the trade-off where both channel resources and customers' satisfaction are optimal.",0
"All companies rely heavily on customer services that can significantly impact their brand reputation. E-commerce companies, with a large customer base, employ various communication channels like chatbots and hotlines to promptly address customer queries. However, each channel has a limited capacity to handle requests, and customers have varying preferences for these channels. The current production systems prioritize business rules and resource allocation, ignoring customer satisfaction. To address this issue, we propose a new framework based on deep reinforcement learning that considers both resources and customers' preferences. Additionally, we propose a deep-reinforcement-learning-based routing method called PER-DoDDQN, which outperforms all other deep Q-learning variants. We evaluated our approach using both synthetic and real customer service data from a financial technology company, and our results demonstrate that our proposed framework and method provide a more optimal routing plan that considers both channel resources and customers' satisfaction.",1
"This work studies the problem of batch off-policy evaluation for Reinforcement Learning in partially observable environments. Off-policy evaluation under partial observability is inherently prone to bias, with risk of arbitrarily large errors. We define the problem of off-policy evaluation for Partially Observable Markov Decision Processes (POMDPs) and establish what we believe is the first off-policy evaluation result for POMDPs. In addition, we formulate a model in which observed and unobserved variables are decoupled into two dynamic processes, called a Decoupled POMDP. We show how off-policy evaluation can be performed under this new model, mitigating estimation errors inherent to general POMDPs. We demonstrate the pitfalls of off-policy evaluation in POMDPs using a well-known off-policy method, Importance Sampling, and compare it with our result on synthetic medical data.",0
"The focus of this research is on addressing the challenge of evaluating off-policy in Reinforcement Learning within environments that are only partially observable. This type of evaluation is prone to bias and can result in large errors. The study defines the off-policy evaluation problem for Partially Observable Markov Decision Processes (POMDPs) and presents what is believed to be the first off-policy evaluation outcome for POMDPs. To address the inherent errors in off-policy evaluation in POMDPs, the research introduces a new model, called a Decoupled POMDP, which separates observed and unobserved variables into two dynamic processes. The study demonstrates how off-policy evaluation can be performed under this model and compares the results with a well-known off-policy method, Importance Sampling, using synthetic medical data.",1
"In this paper, we study Reinforcement Learning from Demonstrations (RLfD) that improves the exploration efficiency of Reinforcement Learning (RL) by providing expert demonstrations. Most of existing RLfD methods require demonstrations to be perfect and sufficient, which yet is unrealistic to meet in practice. To work on imperfect demonstrations, we first define an imperfect expert setting for RLfD in a formal way, and then point out that previous methods suffer from two issues in terms of optimality and convergence, respectively. Upon the theoretical findings we have derived, we tackle these two issues by regarding the expert guidance as a soft constraint on regulating the policy exploration of the agent, which eventually leads to a constrained optimization problem. We further demonstrate that such problem is able to be addressed efficiently by performing a local linear search on its dual form. Considerable empirical evaluations on a comprehensive collection of benchmarks indicate our method attains consistent improvement over other RLfD counterparts.",0
"The focus of this paper is Reinforcement Learning from Demonstrations (RLfD) and how it can enhance the exploration efficiency of Reinforcement Learning (RL) by incorporating expert demonstrations. However, current RLfD methods typically require perfect and sufficient demonstrations, which is not realistic in practical settings. To address this issue, we establish a formal framework for an imperfect expert setting in RLfD and identify two problems with existing methods regarding optimality and convergence. Based on our theoretical analysis, we propose a solution that treats expert guidance as a soft constraint for regulating the agent's policy exploration. This approach results in a constrained optimization problem that can be efficiently solved using a local linear search on its dual form. Our method is evaluated using a broad range of benchmarks, demonstrating consistent performance improvements over other RLfD approaches.",1
"The retail banking services are one of the pillars of the modern economic growth. However, the evolution of the client's habits in modern societies and the recent European regulations promoting more competition mean the retail banks will encounter serious challenges for the next few years, endangering their activities. They now face an impossible compromise: maximizing the satisfaction of their hyper-connected clients while avoiding any risk of default and being regulatory compliant. Therefore, advanced and novel research concepts are a serious game-changer to gain a competitive advantage. In this context, we investigate in this thesis different concepts bridging the gap between persistent homology, neural networks, recommender engines and reinforcement learning with the aim of improving the quality of the retail banking services. Our contribution is threefold. First, we highlight how to overcome insufficient financial data by generating artificial data using generative models and persistent homology. Then, we present how to perform accurate financial recommendations in multi-dimensions. Finally, we underline a reinforcement learning model-free approach to determine the optimal policy of money management based on the aggregated financial transactions of the clients. Our experimental data sets, extracted from well-known institutions where the privacy and the confidentiality of the clients were not put at risk, support our contributions. In this work, we provide the motivations of our retail banking research project, describe the theory employed to improve the financial services quality and evaluate quantitatively and qualitatively our methodologies for each of the proposed research scenarios.",0
"Modern economic growth relies heavily on retail banking services. However, changes in clients' habits and European regulations promoting competition pose significant challenges to retail banks, threatening their operations. These challenges require banks to balance hyper-connected clients' satisfaction with compliance and risk avoidance. To gain a competitive edge, advanced research concepts are necessary. This thesis investigates bridging the gap between persistent homology, neural networks, recommender engines, and reinforcement learning to improve retail banking services. Our threefold contribution includes overcoming insufficient financial data through generative models and persistent homology, performing accurate multi-dimensional financial recommendations, and determining optimal money management policies using a model-free reinforcement learning approach. We use experimental data sets from reputable institutions without compromising client privacy or confidentiality to support our contributions. This work outlines the motivations behind our retail banking research project, describes the theory used to enhance financial services quality, and evaluates our methodologies quantitatively and qualitatively in each proposed research scenario.",1
"Existing automatic 3D image segmentation methods usually fail to meet the clinic use. Many studies have explored an interactive strategy to improve the image segmentation performance by iteratively incorporating user hints. However, the dynamic process for successive interactions is largely ignored. We here propose to model the dynamic process of iterative interactive image segmentation as a Markov decision process (MDP) and solve it with reinforcement learning (RL). Unfortunately, it is intractable to use single-agent RL for voxel-wise prediction due to the large exploration space. To reduce the exploration space to a tractable size, we treat each voxel as an agent with a shared voxel-level behavior strategy so that it can be solved with multi-agent reinforcement learning. An additional advantage of this multi-agent model is to capture the dependency among voxels for segmentation task. Meanwhile, to enrich the information of previous segmentations, we reserve the prediction uncertainty in the state space of MDP and derive an adjustment action space leading to a more precise and finer segmentation. In addition, to improve the efficiency of exploration, we design a relative cross-entropy gain-based reward to update the policy in a constrained direction. Experimental results on various medical datasets have shown that our method significantly outperforms existing state-of-the-art methods, with the advantage of fewer interactions and a faster convergence.",0
"The current automatic 3D image segmentation methods are not suitable for clinical use. Many studies have attempted to improve the segmentation performance by incorporating user hints through an interactive strategy. However, these studies have neglected the dynamic process for successive interactions. In this paper, we propose a solution to model the iterative interactive image segmentation as a Markov decision process (MDP) and use reinforcement learning (RL) to solve it. Unfortunately, using single-agent RL for voxel-wise prediction is impractical due to the large exploration space. To make it more manageable, we treat each voxel as an agent with a shared voxel-level behavior strategy, which can be solved with multi-agent reinforcement learning. This approach also allows for capturing the dependency among voxels for segmentation tasks. Additionally, we retain the prediction uncertainty in the state space of MDP and derive an adjustment action space to achieve a more precise and finer segmentation. To improve exploration efficiency, we design a relative cross-entropy gain-based reward to update the policy in a constrained direction. Our method outperforms existing state-of-the-art methods on various medical datasets, with fewer interactions and faster convergence.",1
"In many settings, as for example wind farms, multiple machines are instantiated to perform the same task, which is called a fleet. The recent advances with respect to the Internet of Things allow control devices and/or machines to connect through cloud-based architectures in order to share information about their status and environment. Such an infrastructure allows seamless data sharing between fleet members, which could greatly improve the sample-efficiency of reinforcement learning techniques. However in practice, these machines, while almost identical in design, have small discrepancies due to production errors or degradation, preventing control algorithms to simply aggregate and employ all fleet data. We propose a novel reinforcement learning method that learns to transfer knowledge between similar fleet members and creates member-specific dynamics models for control. Our algorithm uses Gaussian processes to establish cross-member covariances. This is significantly different from standard transfer learning methods, as the focus is not on sharing information over tasks, but rather over system specifications. We demonstrate our approach on two benchmarks and a realistic wind farm setting. Our method significantly outperforms two baseline approaches, namely individual learning and joint learning where all samples are aggregated, in terms of the median and variance of the results.",0
"A group of machines performing the same task is referred to as a fleet, commonly seen in settings like wind farms. With the advancement of the Internet of Things, machines can use cloud-based architectures to connect and share information about their status and environment. This infrastructure allows for seamless data sharing between fleet members, potentially improving the efficiency of reinforcement learning techniques. However, due to small discrepancies in production or degradation, control algorithms cannot simply aggregate all fleet data. To solve this, we propose a novel reinforcement learning method that transfers knowledge between similar fleet members and creates member-specific dynamics models for control. Our algorithm uses Gaussian processes to establish cross-member covariances, which differs significantly from standard transfer learning methods. Our approach outperforms individual and joint learning methods in terms of median and variance on two benchmarks and a realistic wind farm setting.",1
"The ability to transfer knowledge to novel environments and tasks is a sensible desiderata for general learning agents. Despite the apparent promises, transfer in RL is still an open and little exploited research area. In this paper, we take a brand-new perspective about transfer: we suggest that the ability to assign credit unveils structural invariants in the tasks that can be transferred to make RL more sample-efficient. Our main contribution is SECRET, a novel approach to transfer learning for RL that uses a backward-view credit assignment mechanism based on a self-attentive architecture. Two aspects are key to its generality: it learns to assign credit as a separate offline supervised process and exclusively modifies the reward function. Consequently, it can be supplemented by transfer methods that do not modify the reward function and it can be plugged on top of any RL algorithm.",0
"A desirable trait for learning agents is the capacity to apply knowledge to new tasks and environments. However, the research area of transfer in RL remains largely unexplored, despite its potential benefits. This paper presents a new perspective on transfer, proposing that the ability to assign credit can reveal underlying task structures that facilitate more efficient RL. Our contribution is a novel approach called SECRET, which employs a backward-view credit assignment mechanism using a self-attentive architecture. Its generality lies in its ability to learn credit assignment separately from the RL process and modify only the reward function, allowing it to be used alongside non-reward-modifying transfer methods and any RL algorithm.",1
"We propose an actor-critic, model-free, and online Reinforcement Learning (RL) framework for continuous-state continuous-action Markov Decision Processes (MDPs) when the reward is highly sparse but encompasses a high-level temporal structure. We represent this temporal structure by a finite-state machine and construct an on-the-fly synchronised product with the MDP and the finite machine. The temporal structure acts as a guide for the RL agent within the product, where a modular Deep Deterministic Policy Gradient (DDPG) architecture is proposed to generate a low-level control policy. We evaluate our framework in a Mars rover experiment and we present the success rate of the synthesised policy.",0
"Our proposal is a Reinforcement Learning (RL) framework for Markov Decision Processes (MDPs) with continuous states and actions. The framework is actor-critic, model-free, and online. It is designed to handle MDPs with sparse rewards that have a high-level temporal structure, which we represent using a finite-state machine. We create an on-the-fly synchronised product between the MDP and the finite machine to guide the RL agent. We propose a modular Deep Deterministic Policy Gradient (DDPG) architecture to generate a low-level control policy. We evaluate our framework in a Mars rover experiment and report the success rate of the synthesised policy.",1
"In deep reinforcement learning (RL), adversarial attacks can trick an agent into unwanted states and disrupt training. We propose a system called Robust Student-DQN (RS-DQN), which permits online robustness training alongside Q networks, while preserving competitive performance. We show that RS-DQN can be combined with (i) state-of-the-art adversarial training and (ii) provably robust training to obtain an agent that is resilient to strong attacks during training and evaluation.",0
"The training of an agent in deep reinforcement learning can be disrupted by adversarial attacks, causing it to end up in undesired states. To address this, we suggest a solution known as Robust Student-DQN (RS-DQN), which enables the simultaneous training of Q networks and online robustness. This system maintains competitive performance while incorporating (i) the latest adversarial training and (ii) provably robust training, resulting in an agent that can withstand powerful attacks during both training and evaluation.",1
Consider an imitation learning problem that the imitator and the expert have different dynamics models. Most of the current imitation learning methods fail because they focus on imitating actions. We propose a novel state alignment-based imitation learning method to train the imitator to follow the state sequences in expert demonstrations as much as possible. The state alignment comes from both local and global perspectives and we combine them into a reinforcement learning framework by a regularized policy update objective. We show the superiority of our method on standard imitation learning settings and imitation learning settings where the expert and imitator have different dynamics models.,0
"If the dynamics models of the imitator and the expert in an imitation learning problem are different, most current imitation learning methods are unsuccessful in their attempt to imitate actions. To address this challenge, we present a new approach to imitation learning that leverages state alignment. Our method focuses on training the imitator to closely follow the state sequences of expert demonstrations, incorporating both local and global perspectives. This is achieved by integrating the state alignment into a reinforcement learning framework through a regularized policy update objective. Our approach demonstrates superior performance in both standard imitation learning scenarios and those in which the expert and imitator have distinct dynamics models.",1
"Human ratings are currently the most accurate way to assess the quality of an image captioning model, yet most often the only used outcome of an expensive human rating evaluation is a few overall statistics over the evaluation dataset. In this paper, we show that the signal from instance-level human caption ratings can be leveraged to improve captioning models, even when the amount of caption ratings is several orders of magnitude less than the caption training data. We employ a policy gradient method to maximize the human ratings as rewards in an off-policy reinforcement learning setting, where policy gradients are estimated by samples from a distribution that focuses on the captions in a caption ratings dataset. Our empirical evidence indicates that the proposed method learns to generalize the human raters' judgments to a previously unseen set of images, as judged by a different set of human judges, and additionally on a different, multi-dimensional side-by-side human evaluation procedure.",0
"Although human ratings are the most precise way to evaluate the quality of an image captioning model, they are often used only to generate a few general statistics for the evaluation dataset. In this study, we demonstrate that the signal from individual-level human caption ratings can be utilized to enhance captioning models, even when the number of caption ratings is significantly lower than the caption training data. We employ a policy gradient approach, using human ratings as rewards in an off-policy reinforcement learning environment, with policy gradients estimated from a distribution that concentrates on captions in a caption ratings dataset. Our results demonstrate that the proposed technique learns to extrapolate human raters' judgments to previously unseen images, as assessed by a distinct set of human judges and a distinct, multi-dimensional side-by-side human evaluation process.",1
"We integrate information-theoretic concepts into the design and analysis of optimistic algorithms and Thompson sampling. By making a connection between information-theoretic quantities and confidence bounds, we obtain results that relate the per-period performance of the agent with its information gain about the environment, thus explicitly characterizing the exploration-exploitation tradeoff. The resulting cumulative regret bound depends on the agent's uncertainty over the environment and quantifies the value of prior information. We show applicability of this approach to several environments, including linear bandits, tabular MDPs, and factored MDPs. These examples demonstrate the potential of a general information-theoretic approach for the design and analysis of reinforcement learning algorithms.",0
"Our approach to designing and analyzing optimistic algorithms and Thompson sampling involves the integration of information-theoretic concepts. By linking information-theoretic quantities to confidence bounds, we are able to establish a relationship between the agent's performance per period and its acquisition of knowledge about the environment. This connection allows us to explicitly define the exploration-exploitation tradeoff. The cumulative regret bound that results is dependent on the agent's level of uncertainty about the environment and measures the value of prior information. We have applied this approach to different environments, such as linear bandits, tabular MDPs, and factored MDPs, demonstrating the potential of this general information-theoretic approach for the design and analysis of reinforcement learning algorithms.",1
"Recent research on reinforcement learning (RL) has suggested that trained agents are vulnerable to maliciously crafted adversarial samples. In this work, we show how such samples can be generalised from White-box and Grey-box attacks to a strong Black-box case, where the attacker has no knowledge of the agents, their training parameters and their training methods. We use sequence-to-sequence models to predict a single action or a sequence of future actions that a trained agent will make. First, we show our approximation model, based on time-series information from the agent, consistently predicts RL agents' future actions with high accuracy in a Black-box setup on a wide range of games and RL algorithms. Second, we find that although adversarial samples are transferable from the target model to our RL agents, they often outperform random Gaussian noise only marginally. This highlights a serious methodological deficiency in previous work on such agents; random jamming should have been taken as the baseline for evaluation. Third, we propose a novel use for adversarial samplesin Black-box attacks of RL agents: they can be used to trigger a trained agent to misbehave after a specific time delay. This appears to be a genuinely new type of attack. It potentially enables an attacker to use devices controlled by RL agents as time bombs.",0
"Recent studies in reinforcement learning have indicated that trained agents are susceptible to adversarial samples crafted with malicious intent. Our work demonstrates how such samples can be extended from White-box and Grey-box attacks to a formidable Black-box scenario, where the attacker possesses no knowledge of the agents, their training parameters or methods. To achieve this, we use sequence-to-sequence models to forecast a single or sequence of future actions that a trained agent will execute. Firstly, our model accurately predicts RL agents' future actions in a Black-box setup for a wide range of games and RL algorithms. Secondly, we observe that adversarial samples are transferable from the target model to our RL agents, but they barely outperform random Gaussian noise. This exposes a significant methodological flaw in previous research on such agents, where random jamming should have been utilized as the evaluation baseline. Finally, we suggest a new approach to using adversarial samples in Black-box attacks of RL agents, whereby they can be used to trigger a trained agent to misbehave after a specific time delay. This represents a groundbreaking type of attack which could be used to turn devices controlled by RL agents into time bombs.",1
"Deep networks have enabled reinforcement learning to scale to more complex and challenging domains, but these methods typically require large quantities of training data. An alternative is to use sample-efficient episodic control methods: neuro-inspired algorithms which use non-/semi-parametric models that predict values based on storing and retrieving previously experienced transitions. One way to further improve the sample efficiency of these approaches is to use more principled exploration strategies. In this work, we therefore propose maximum entropy mellowmax episodic control (MEMEC), which samples actions according to a Boltzmann policy with a state-dependent temperature. We demonstrate that MEMEC outperforms other uncertainty- and softmax-based exploration methods on classic reinforcement learning environments and Atari games, achieving both more rapid learning and higher final rewards.",0
"Reinforcement learning has become more adaptable to complex and demanding domains thanks to the use of deep networks, but these methods often require a significant amount of training data. Another option is to utilize sample-efficient episodic control methods which utilize neuro-inspired algorithms and non-/semi-parametric models to predict values based on stored and recalled transitions. To improve the sample efficiency of these techniques, more principled exploration strategies can be employed. This study introduces maximum entropy mellowmax episodic control (MEMEC), which selects actions based on a Boltzmann policy with a temperature dependent on the state. Results show that MEMEC performs better than other exploration methods based on uncertainty and softmax on traditional reinforcement learning environments and Atari games, leading to faster learning and higher rewards in the end.",1
"Recently, neuro-inspired episodic control (EC) methods have been developed to overcome the data-inefficiency of standard deep reinforcement learning approaches. Using non-/semi-parametric models to estimate the value function, they learn rapidly, retrieving cached values from similar past states. In realistic scenarios, with limited resources and noisy data, maintaining meaningful representations in memory is essential to speed up the learning and avoid catastrophic forgetting. Unfortunately, EC methods have a large space and time complexity. We investigate different solutions to these problems based on prioritising and ranking stored states, as well as online clustering techniques. We also propose a new dynamic online k-means algorithm that is both computationally-efficient and yields significantly better performance at smaller memory sizes; we validate this approach on classic reinforcement learning environments and Atari games.",0
"Neuro-inspired episodic control (EC) techniques have emerged as a solution to the inefficiency of standard deep reinforcement learning methods. These approaches employ non-/semi-parametric models for value function estimation and leverage cached values from similar past states to learn quickly. In resource-limited and noisy environments, maintaining meaningful memory representations is crucial for accelerating learning and avoiding catastrophic forgetting. However, EC methods are computationally complex and time-consuming. To address these issues, we explore various solutions such as prioritizing and ranking stored states and using online clustering techniques. Additionally, we introduce a novel dynamic online k-means algorithm that is both efficient and offers superior performance at smaller memory sizes. We validate our approach on classic reinforcement learning scenarios and Atari games.",1
"Reinforcement Learning in domains with sparse rewards is a difficult problem, and a large part of the training process is often spent searching the state space in a more or less random fashion for any learning signals. For control problems, we often have some controller readily available which might be suboptimal but nevertheless solves the problem to some degree. This controller can be used to guide the initial exploration phase of the learning controller towards reward yielding states, reducing the time before refinement of a viable policy can be initiated. In our work, the agent is guided through an auxiliary behaviour cloning loss which is made conditional on a Q-filter, i.e. it is only applied in situations where the critic deems the guiding controller to be better than the agent. The Q-filter provides a natural way to adjust the guidance throughout the training process, allowing the agent to exceed the guiding controller in a manner that is adaptive to the task at hand and the proficiency of the guiding controller. The contribution of this paper lies in identifying shortcomings in previously proposed implementations of the Q-filter concept, and in suggesting some ways these issues can be mitigated. These modifications are tested on the OpenAI Gym Fetch environments, showing clear improvements in adaptivity and yielding increased performance in all robotic environments tested.",0
"It is challenging to implement Reinforcement Learning in sparse reward domains, as a significant portion of the training process involves randomly searching for learning signals in the state space. However, in control problems, we may have an existing suboptimal controller that can guide the learning controller towards reward-generating states during the initial exploration phase. This approach reduces the time required to refine a viable policy. In this study, we use an auxiliary behaviour cloning loss conditional on a Q-filter to guide the agent. The Q-filter adjusts the guidance during the training process, enabling the agent to surpass the guiding controller in an adaptive manner. Our paper addresses the shortcomings of previous Q-filter implementations and suggests ways to mitigate these issues. We test our modifications on the OpenAI Gym Fetch environments, demonstrating enhanced adaptivity and improved performance in all robotic environments tested.",1
"Traditional model-based reinforcement learning approaches learn a model of the environment dynamics without explicitly considering how it will be used by the agent. In the presence of misspecified model classes, this can lead to poor estimates, as some relevant available information is ignored. In this paper, we introduce a novel model-based policy search approach that exploits the knowledge of the current agent policy to learn an approximate transition model, focusing on the portions of the environment that are most relevant for policy improvement. We leverage a weighting scheme, derived from the minimization of the error on the model-based policy gradient estimator, in order to define a suitable objective function that is optimized for learning the approximate transition model. Then, we integrate this procedure into a batch policy improvement algorithm, named Gradient-Aware Model-based Policy Search (GAMPS), which iteratively learns a transition model and uses it, together with the collected trajectories, to compute the new policy parameters. Finally, we empirically validate GAMPS on benchmark domains analyzing and discussing its properties.",0
"Conventional reinforcement learning methods based on models learn the dynamics of the environment without considering the agent's use of it. This approach can lead to poor estimates when there are misspecified model classes, as important information is ignored. This study proposes a new model-based policy search approach that utilizes the current agent policy to learn an approximate transition model, focusing on the relevant aspects of the environment to improve policy. The model-based policy gradient estimator is used to derive a weighting scheme that creates an objective function suitable for optimizing the approximate transition model. This method is integrated into a batch policy improvement algorithm named Gradient-Aware Model-based Policy Search (GAMPS), which iteratively learns a transition model and uses it with collected trajectories to compute new policy parameters. Finally, GAMPS is empirically validated on benchmark domains, and its properties are discussed.",1
"Network slicing is a key technology in 5G communications system. Its purpose is to dynamically and efficiently allocate resources for diversified services with distinct requirements over a common underlying physical infrastructure. Therein, demand-aware resource allocation is of significant importance to network slicing. In this paper, we consider a scenario that contains several slices in a radio access network with base stations that share the same physical resources (e.g., bandwidth or slots). We leverage deep reinforcement learning (DRL) to solve this problem by considering the varying service demands as the environment state and the allocated resources as the environment action. In order to reduce the effects of the annoying randomness and noise embedded in the received service level agreement (SLA) satisfaction ratio (SSR) and spectrum efficiency (SE), we primarily propose generative adversarial network-powered deep distributional Q network (GAN-DDQN) to learn the action-value distribution driven by minimizing the discrepancy between the estimated action-value distribution and the target action-value distribution. We put forward a reward-clipping mechanism to stabilize GAN-DDQN training against the effects of widely-spanning utility values. Moreover, we further develop Dueling GAN-DDQN, which uses a specially designed dueling generator, to learn the action-value distribution by estimating the state-value distribution and the action advantage function. Finally, we verify the performance of the proposed GAN-DDQN and Dueling GAN-DDQN algorithms through extensive simulations.",0
"Network slicing is an essential technology in the 5G communication system that allocates resources dynamically and efficiently to diverse services with varying requirements over a shared physical infrastructure. Efficient resource allocation is crucial in network slicing, and this paper explores a scenario with multiple slices in a radio access network that share the same physical resources. To address the randomness and noise in the received service level agreement satisfaction ratio and spectrum efficiency, we propose the use of generative adversarial network-powered deep distributional Q network (GAN-DDQN) to learn the action-value distribution. We also introduce a reward-clipping mechanism to stabilize GAN-DDQN training against the effects of widely spanning utility values. Additionally, we develop Dueling GAN-DDQN, which estimates the state-value distribution and the action advantage function. Through extensive simulations, we demonstrate the effectiveness of the proposed GAN-DDQN and Dueling GAN-DDQN algorithms.",1
"Sparse reward is one of the biggest challenges in reinforcement learning (RL). In this paper, we propose a novel method called Generative Exploration and Exploitation (GENE) to overcome sparse reward. GENE automatically generates start states to encourage the agent to explore the environment and to exploit received reward signals. GENE can adaptively tradeoff between exploration and exploitation according to the varying distributions of states experienced by the agent as the learning progresses. GENE relies on no prior knowledge about the environment and can be combined with any RL algorithm, no matter on-policy or off-policy, single-agent or multi-agent. Empirically, we demonstrate that GENE significantly outperforms existing methods in three tasks with only binary rewards, including Maze, Maze Ant, and Cooperative Navigation. Ablation studies verify the emergence of progressive exploration and automatic reversing.",0
"Reinforcement learning (RL) faces a major challenge in the form of sparse reward. To tackle this issue, our paper introduces a new technique called Generative Exploration and Exploitation (GENE). GENE generates initial states, enabling the agent to explore the environment and exploit reward signals. The method balances exploration and exploitation based on the state distributions that the agent experiences during learning, without prior knowledge of the environment. GENE can be used with any RL algorithm, from on-policy to off-policy and single-agent to multi-agent. We demonstrate the effectiveness of GENE in three tasks with binary rewards (Maze, Maze Ant, and Cooperative Navigation), outperforming existing methods. Ablation studies confirm the emergence of progressive exploration and automatic reversing.",1
"Option-critic learning is a general-purpose reinforcement learning (RL) framework that aims to address the issue of long term credit assignment by leveraging temporal abstractions. However, when dealing with extended timescales, discounting future rewards can lead to incorrect credit assignments. In this work, we address this issue by extending the hierarchical option-critic policy gradient theorem for the average reward criterion. Our proposed framework aims to maximize the long-term reward obtained in the steady-state of the Markov chain defined by the agent's policy. Furthermore, we use an ordinary differential equation based approach for our convergence analysis and prove that the parameters of the intra-option policies, termination functions, and value functions, converge to their corresponding optimal values, with probability one. Finally, we illustrate the competitive advantage of learning options, in the average reward setting, on a grid-world environment with sparse rewards.",0
"The Option-critic learning framework is a general-purpose approach to reinforcement learning that utilizes temporal abstractions to tackle the issue of long term credit assignment. However, the discounting of future rewards can result in incorrect credit assignments when dealing with extended timescales. To overcome this problem, we have extended the hierarchical option-critic policy gradient theorem for the average reward criterion. Our framework aims to maximize the long-term reward obtained in the steady-state of the Markov chain defined by the agent's policy. In addition, we have utilized an ordinary differential equation based approach for convergence analysis and have proven that the parameters of the intra-option policies, termination functions, and value functions converge to their respective optimal values with probability one. Finally, we have demonstrated the competitive advantage of learning options in the average reward setting using a sparse rewards grid-world environment.",1
"Prevalent theories in cognitive science propose that humans understand and represent the knowledge of the world through causal relationships. In making sense of the world, we build causal models in our mind to encode cause-effect relations of events and use these to explain why new events happen. In this paper, we use causal models to derive causal explanations of behaviour of reinforcement learning agents. We present an approach that learns a structural causal model during reinforcement learning and encodes causal relationships between variables of interest. This model is then used to generate explanations of behaviour based on counterfactual analysis of the causal model. We report on a study with 120 participants who observe agents playing a real-time strategy game (Starcraft II) and then receive explanations of the agents' behaviour. We investigated: 1) participants' understanding gained by explanations through task prediction; 2) explanation satisfaction and 3) trust. Our results show that causal model explanations perform better on these measures compared to two other baseline explanation models.",0
"Theories in cognitive science suggest that humans comprehend and represent knowledge of the world by establishing causal connections. To make sense of the world, our minds construct causal models that encode cause-and-effect relationships of events, which we utilize to explain why new events occur. In this paper, we employ causal models to obtain causal explanations of reinforcement learning agents' behavior. We present a technique that learns a structural causal model during reinforcement learning and records causal relationships between the variables of interest. This model is then employed to create explanations of behavior based on counterfactual analysis of the causal model. Our study involved 120 participants who observed agents playing a real-time strategy game (Starcraft II) and then received explanations of the agents' behavior. We evaluated the participants' understanding gained through explanations, explanation satisfaction, and trust. Our results indicate that causal model explanations outperform two other baseline explanation models in these measures.",1
"Balancing exploration and exploitation is a fundamental part of reinforcement learning, yet most state-of-the-art algorithms use a naive exploration protocol like $\epsilon$-greedy. This contributes to the problem of high sample complexity, as the algorithm wastes effort by repeatedly visiting parts of the state space that have already been explored. We introduce a novel method based on Bayesian linear regression and latent space embedding to generate an intrinsic reward signal that encourages the learning agent to seek out unexplored parts of the state space. This method is computationally efficient, simple to implement, and can extend any state-of-the-art reinforcement learning algorithm. We evaluate the method on a range of algorithms and challenging control tasks, on both simulated and physical robots, demonstrating how the proposed method can significantly improve sample complexity.",0
"The act of balancing exploration and exploitation is a crucial aspect of reinforcement learning. However, many advanced algorithms still utilize a simplistic exploration protocol such as $\epsilon$-greedy, leading to high sample complexity as the algorithm revisits areas of the state space that have already been explored. To address this issue, we present a new technique that employs Bayesian linear regression and latent space embedding to create an intrinsic reward system that encourages the learning agent to search for uncharted regions in the state space. This technique is straightforward to implement and computationally efficient, and can be integrated into any state-of-the-art reinforcement learning algorithm. We tested this technique on a variety of challenging control tasks, both on simulated and real-world robots, and found that it significantly enhances sample complexity.",1
"Deep reinforcement learning has been shown to solve challenging tasks where large amounts of training experience is available, usually obtained online while learning the task. Robotics is a significant potential application domain for many of these algorithms, but generating robot experience in the real world is expensive, especially when each task requires a lengthy online training procedure. Off-policy algorithms can in principle learn arbitrary tasks from a diverse enough fixed dataset. In this work, we evaluate popular exploration methods by generating robotics datasets for the purpose of learning to solve tasks completely offline without any further interaction in the real world. We present results on three popular continuous control tasks in simulation, as well as continuous control of a high-dimensional real robot arm. Code documenting all algorithms, experiments, and hyper-parameters is available at https://github.com/qutrobotlearning/batchlearning.",0
"The effectiveness of deep reinforcement learning has been demonstrated in tackling challenging tasks that require a substantial amount of training experience, often obtained online while learning the task. Robotics is an application domain with great potential for many of these algorithms. However, collecting robot experience in the real world can be a costly endeavor, particularly when each task necessitates an extended online training process. Off-policy algorithms have the potential to learn any task from a diverse and fixed dataset. In this study, we assess various exploration techniques by creating robotics datasets for learning how to solve tasks entirely offline without any additional interaction in the real world. We present the outcomes of our research on three well-known continuous control tasks in simulation, as well as continuous control of a real robot arm with high dimensions. Code containing all algorithms, experiments, and hyper-parameters is accessible at https://github.com/qutrobotlearning/batchlearning.",1
"The study of object representations in computer vision has primarily focused on developing representations that are useful for image classification, object detection, or semantic segmentation as downstream tasks. In this work we aim to learn object representations that are useful for control and reinforcement learning (RL). To this end, we introduce Transporter, a neural network architecture for discovering concise geometric object representations in terms of keypoints or image-space coordinates. Our method learns from raw video frames in a fully unsupervised manner, by transporting learnt image features between video frames using a keypoint bottleneck. The discovered keypoints track objects and object parts across long time-horizons more accurately than recent similar methods. Furthermore, consistent long-term tracking enables two notable results in control domains -- (1) using the keypoint co-ordinates and corresponding image features as inputs enables highly sample-efficient reinforcement learning; (2) learning to explore by controlling keypoint locations drastically reduces the search space, enabling deep exploration (leading to states unreachable through random action exploration) without any extrinsic rewards.",0
"The primary focus of computer vision's study on object representations has been on developing representations that benefit downstream tasks like image classification, object detection, or semantic segmentation. However, our objective is to learn object representations that are useful for control and reinforcement learning (RL). To achieve this goal, we present Transporter, a neural network architecture that discovers concise geometric object representations in terms of keypoints or image-space coordinates. Our unsupervised method utilizes raw video frames, transporting learnt image features between them using a keypoint bottleneck. Compared to similar methods, our approach accurately tracks objects and object parts across long time-horizons. This consistent long-term tracking leads to two notable results in control domains: (1) inputting the keypoint co-ordinates and corresponding image features enables highly sample-efficient reinforcement learning; (2) learning to explore by controlling keypoint locations significantly reduces the search space, allowing for deep exploration without extrinsic rewards (leading to states unreachable through random action exploration).",1
"We propose Cluster Pruning (CUP) for compressing and accelerating deep neural networks. Our approach prunes similar filters by clustering them based on features derived from both the incoming and outgoing weight connections. With CUP, we overcome two limitations of prior work-(1) non-uniform pruning: CUP can efficiently determine the ideal number of filters to prune in each layer of a neural network. This is in contrast to prior methods that either prune all layers uniformly or otherwise use resource-intensive methods such as manual sensitivity analysis or reinforcement learning to determine the ideal number. (2) Single-shot operation: We extend CUP to CUP-SS (for CUP single shot) whereby pruning is integrated into the initial training phase itself. This leads to large savings in training time compared to traditional pruning pipelines. Through extensive evaluation on multiple datasets (MNIST, CIFAR-10, and Imagenet) and models(VGG-16, Resnets-18/34/56) we show that CUP outperforms recent state of the art. Specifically, CUP-SS achieves 2.2x flops reduction for a Resnet-50 model trained on Imagenet while staying within 0.9% top-5 accuracy. It saves over 14 hours in training time with respect to the original Resnet-50. The code to reproduce results is available.",0
"Our proposed method for compressing and accelerating deep neural networks is Cluster Pruning (CUP). By clustering similar filters based on both incoming and outgoing weight connections, our approach addresses two limitations of previous methods. Firstly, CUP efficiently determines the ideal number of filters to prune in each layer, unlike previous methods that either prune uniformly or require manual sensitivity analysis or reinforcement learning. Secondly, we extend CUP to CUP-SS, where pruning is integrated into the initial training phase, leading to significant savings in training time compared to traditional pruning pipelines. We conducted extensive evaluations on multiple datasets and models, including MNIST, CIFAR-10, and Imagenet, and VGG-16, Resnets-18/34/56. Our results demonstrate that CUP outperforms current state of the art techniques. In particular, CUP-SS achieved a 2.2x reduction in flops for a Resnet-50 model trained on Imagenet while maintaining a top-5 accuracy of 0.9%, saving over 14 hours in training time compared to the original Resnet-50. We provide the code to reproduce these results.",1
"Recently, the Deep Planning Network (PlaNet) approach was introduced as a model-based reinforcement learning method that learns environment dynamics directly from pixel observations. This architecture is useful for learning tasks in which either the agent does not have access to meaningful states (like position/velocity of robotic joints) or where the observed states significantly deviate from the physical state of the agent (which is commonly the case in low-cost robots in the form of backlash or noisy joint readings). PlaNet, by design, interleaves phases of training the dynamics model with phases of collecting more data on the target environment, leading to long training times. In this work, we introduce Robo-PlaNet, an asynchronous version of PlaNet. This algorithm consistently reaches higher performance in the same amount of time, which we demonstrate in both a simulated and a real robotic experiment.",0
"A new approach called the Deep Planning Network (PlaNet) has recently been developed as a model-based reinforcement learning technique that directly learns environment dynamics from pixel observations. This method is particularly useful for learning tasks where the agent does not have access to relevant states, such as position or velocity of robotic joints, or when the observed states deviate significantly from the agent's physical state, as is often the case with low-cost robots that have noisy joint readings or backlash. However, PlaNet requires long training times as it alternates between training the dynamics model and collecting more data on the target environment. To address this issue, we introduce Robo-PlaNet, an asynchronous version of PlaNet that achieves higher performance in the same amount of time, as demonstrated in both simulated and real robotic experiments.",1
"Learning good representations is a long standing problem in reinforcement learning (RL). One of the conventional ways to achieve this goal in the supervised setting is through regularization of the parameters. Extending some of these ideas to the RL setting has not yielded similar improvements in learning. In this paper, we develop an online regularization framework for decorrelating features in RL and demonstrate its utility in several test environments. We prove that the proposed algorithm converges in the linear function approximation setting and does not change the main objective of maximizing cumulative reward. We demonstrate how to scale the approach to deep RL using the Gramian of the features achieving linear computational complexity in the number of features and squared complexity in size of the batch. We conduct an extensive empirical study of the new approach on Atari 2600 games and show a significant improvement in sample efficiency in 40 out of 49 games.",0
"Reinforcement learning (RL) has long struggled with the challenge of developing effective representations. While supervised learning has traditionally used parameter regularization to address this issue, efforts to apply this technique to RL have not been successful. This paper presents an online regularization framework designed to decorrelate features in RL, with promising results in multiple test environments. The algorithm preserves the main objective of maximizing cumulative reward and is proven to converge in the linear function approximation setting. The technique can be scaled to deep RL using Gramian features, with linear computational complexity in the number of features and squared complexity in batch size. Empirical testing on Atari 2600 games demonstrates a significant improvement in sample efficiency for 40 out of 49 games.",1
"This paper considers the problem of resource allocation in stream processing, where continuous data flows must be processed in real time in a large distributed system. To maximize system throughput, the resource allocation strategy that partitions the computation tasks of a stream processing graph onto computing devices must simultaneously balance workload distribution and minimize communication. Since this problem of graph partitioning is known to be NP-complete yet crucial to practical streaming systems, many heuristic-based algorithms have been developed to find reasonably good solutions. In this paper, we present a graph-aware encoder-decoder framework to learn a generalizable resource allocation strategy that can properly distribute computation tasks of stream processing graphs unobserved from training data. We, for the first time, propose to leverage graph embedding to learn the structural information of the stream processing graphs. Jointly trained with the graph-aware decoder using deep reinforcement learning, our approach can effectively find optimized solutions for unseen graphs. Our experiments show that the proposed model outperforms both METIS, a state-of-the-art graph partitioning algorithm, and an LSTM-based encoder-decoder model, in about 70% of the test cases.",0
"The article examines the challenge of distributing resources in stream processing, where a vast distributed system must handle real-time data flows. To optimize system efficiency, the resource allocation method must effectively divide the computation tasks of a stream processing chart across computing devices while balancing the workload distribution and minimizing communication. Although this graph partitioning problem is crucial for operational streaming systems, it is NP-complete, so many heuristic algorithms have been developed to find satisfactory solutions. This paper introduces a novel graph-aware encoder-decoder framework that uses graph embedding to learn the structural information of stream processing charts. The approach applies deep reinforcement learning to jointly train with the graph-aware decoder and find optimized solutions for unseen graphs. The proposed model surpasses both METIS, a state-of-the-art graph partitioning algorithm, and an LSTM-based encoder-decoder model in approximately 70% of test cases, as demonstrated by experiments.",1
"Planning methods can solve temporally extended sequential decision making problems by composing simple behaviors. However, planning requires suitable abstractions for the states and transitions, which typically need to be designed by hand. In contrast, model-free reinforcement learning (RL) can acquire behaviors from low-level inputs directly, but often struggles with temporally extended tasks. Can we utilize reinforcement learning to automatically form the abstractions needed for planning, thus obtaining the best of both approaches? We show that goal-conditioned policies learned with RL can be incorporated into planning, so that a planner can focus on which states to reach, rather than how those states are reached. However, with complex state observations such as images, not all inputs represent valid states. We therefore also propose using a latent variable model to compactly represent the set of valid states for the planner, so that the policies provide an abstraction of actions, and the latent variable model provides an abstraction of states. We compare our method with planning-based and model-free methods and find that our method significantly outperforms prior work when evaluated on image-based robot navigation and manipulation tasks that require non-greedy, multi-staged behavior.",0
"The utilization of planning techniques can solve problems related to sequential decision-making that occur over a period of time by combining uncomplicated actions. However, it is vital to design appropriate abstractions for the states and transitions, which is usually done manually. On the other hand, model-free reinforcement learning (RL) can directly acquire behaviors from basic inputs, but it may face difficulties with tasks that require an extended period of time. Is it possible to combine reinforcement learning and planning to obtain the advantages of both methods? We demonstrate that by using RL, we can develop goal-based policies that can be integrated into planning, enabling the planner to concentrate on the states that need to be reached. However, in situations where state observations are complex, not all inputs may be valid states. Therefore, we suggest using a latent variable model to represent the set of valid states in a condensed format, allowing the policies to provide an outline of actions, and the latent variable model to provide an outline of states. Our approach was compared with planning-based and model-free methods, and it was discovered that our technique outperformed previous methods when tested on image-based robot navigation and manipulation tasks that require multi-staged, non-greedy behavior.",1
"In this paper we target the problem of transferring policies across multiple environments with different dynamics parameters and motor noise variations, by introducing a framework that decouples the processes of policy learning and system identification. Efficiently transferring learned policies to an unknown environment with changes in dynamics configurations in the presence of motor noise is very important for operating robots in the real world, and our work is a novel attempt in that direction. We introduce MANGA: Method Agnostic Neural-policy Generalization and Adaptation, that trains dynamics conditioned policies and efficiently learns to estimate the dynamics parameters of the environment given off-policy state-transition rollouts in the environment. Our scheme is agnostic to the type of training method used - both reinforcement learning (RL) and imitation learning (IL) strategies can be used. We demonstrate the effectiveness of our approach by experimenting with four different MuJoCo agents and comparing against previously proposed transfer baselines.",0
"The aim of this paper is to address the challenge of transferring policies between multiple environments that have varying dynamics parameters and motor noise variations. To accomplish this, we have devised a framework that separates the processes of policy learning and system identification. The ability to effectively transfer learned policies to unfamiliar environments with altered dynamics configurations and motor noise is crucial for deploying robots in the real world. Our framework, called MANGA (Method Agnostic Neural-policy Generalization and Adaptation), trains dynamics conditioned policies and can accurately estimate the dynamics parameters of the environment using off-policy state-transition rollouts. Our approach is not limited to any particular training method and can accommodate both reinforcement learning (RL) and imitation learning (IL) strategies. We conducted experiments on four different MuJoCo agents and compared our results with existing transfer baselines to demonstrate the efficacy of our approach.",1
"SARSA is an on-policy algorithm to learn a Markov decision process policy in reinforcement learning. We investigate the SARSA algorithm with linear function approximation under the non-i.i.d.\ data, where a single sample trajectory is available. With a Lipschitz continuous policy improvement operator that is smooth enough, SARSA has been shown to converge asymptotically \cite{perkins2003convergent,melo2008analysis}. However, its non-asymptotic analysis is challenging and remains unsolved due to the non-i.i.d. samples and the fact that the behavior policy changes dynamically with time. In this paper, we develop a novel technique to explicitly characterize the stochastic bias of a type of stochastic approximation procedures with time-varying Markov transition kernels. Our approach enables non-asymptotic convergence analyses of this type of stochastic approximation algorithms, which may be of independent interest. Using our bias characterization technique and a gradient descent type of analysis, we provide the finite-sample analysis on the mean square error of the SARSA algorithm. We then further study a fitted SARSA algorithm, which includes the original SARSA algorithm and its variant in \cite{perkins2003convergent} as special cases. This fitted SARSA algorithm provides a more general framework for \textit{iterative} on-policy fitted policy iteration, which is more memory and computationally efficient. For this fitted SARSA algorithm, we also provide its finite-sample analysis.",0
"The SARSA algorithm is an on-policy method used to learn a policy in reinforcement learning for Markov decision processes. Our focus is on investigating the SARSA algorithm with linear function approximation under non-i.i.d. data, where only a single sample trajectory is available. Previous research has shown that SARSA converges asymptotically with a Lipschitz continuous policy improvement operator that is smooth enough. However, non-asymptotic analysis is still a challenge due to the non-i.i.d. samples and the dynamic changes in the behavior policy over time. In this study, we develop a novel technique to explicitly characterize the stochastic bias of a specific type of stochastic approximation procedures with time-varying Markov transition kernels. Our approach enables non-asymptotic convergence analyses of this type of stochastic approximation algorithms. Through our bias characterization technique and a gradient descent analysis, we provide finite-sample analysis on the mean square error of the SARSA algorithm. Moreover, we explore a fitted SARSA algorithm, which is more generalized and efficient than the original SARSA algorithm and its variant in \cite{perkins2003convergent}. This fitted SARSA algorithm presents an iterative on-policy fitted policy iteration and is computationally and memory efficient. Lastly, we provide the finite-sample analysis of the fitted SARSA algorithm.",1
"The reinforcement learning (RL) research area is very active, with an important number of new contributions; especially considering the emergent field of deep RL (DRL). However a number of scientific and technical challenges still need to be addressed, amongst which we can mention the ability to abstract actions or the difficulty to explore the environment which can be addressed by intrinsic motivation (IM). In this article, we provide a survey on the role of intrinsic motivation in DRL. We categorize the different kinds of intrinsic motivations and detail for each category, its advantages and limitations with respect to the mentioned challenges. Additionnally, we conduct an in-depth investigation of substantial current research questions, that are currently under study or not addressed at all in the considered research area of DRL. We choose to survey these research works, from the perspective of learning how to achieve tasks. We suggest then, that solving current challenges could lead to a larger developmental architecture which may tackle most of the tasks. We describe this developmental architecture on the basis of several building blocks composed of a RL algorithm and an IM module compressing information.",0
"The field of reinforcement learning (RL) is highly active and has seen numerous advancements, particularly in deep RL (DRL). However, there are still scientific and technical hurdles that need to be overcome, such as the challenge of abstracting actions and exploring the environment, which can be addressed by intrinsic motivation (IM). This article presents a survey on the role of IM in DRL, categorizing the different types of IM and discussing their advantages and limitations in tackling the aforementioned challenges. Additionally, we delve into current research questions related to task achievement in DRL, highlighting areas that require further investigation. By resolving these challenges, we can potentially develop a comprehensive architecture that can handle most tasks. This architecture is based on various building blocks, including a RL algorithm and an IM module that compresses information.",1
"Robustness of Deep Reinforcement Learning (DRL) algorithms towards adversarial attacks in real world applications such as those deployed in cyber-physical systems (CPS) are of increasing concern. Numerous studies have investigated the mechanisms of attacks on the RL agent's state space. Nonetheless, attacks on the RL agent's action space (AS) (corresponding to actuators in engineering systems) are equally perverse; such attacks are relatively less studied in the ML literature. In this work, we first frame the problem as an optimization problem of minimizing the cumulative reward of an RL agent with decoupled constraints as the budget of attack. We propose a white-box Myopic Action Space (MAS) attack algorithm that distributes the attacks across the action space dimensions. Next, we reformulate the optimization problem above with the same objective function, but with a temporally coupled constraint on the attack budget to take into account the approximated dynamics of the agent. This leads to the white-box Look-ahead Action Space (LAS) attack algorithm that distributes the attacks across the action and temporal dimensions. Our results shows that using the same amount of resources, the LAS attack deteriorates the agent's performance significantly more than the MAS attack. This reveals the possibility that with limited resource, an adversary can utilize the agent's dynamics to malevolently craft attacks that causes the agent to fail. Additionally, we leverage these attack strategies as a possible tool to gain insights on the potential vulnerabilities of DRL agents.",0
"There is a growing concern about the ability of Deep Reinforcement Learning (DRL) algorithms to withstand adversarial attacks in real-world applications, particularly in Cyber-Physical Systems (CPS). Previous research has studied attacks on the RL agent's state space, but less attention has been given to attacks on the agent's action space, which corresponds to the actuators in engineering systems. In this study, we present an optimization problem that minimizes the cumulative reward of an RL agent while considering the budget of attack as a decoupled constraint. We propose two white-box attack algorithms: Myopic Action Space (MAS) and Look-ahead Action Space (LAS), which distribute attacks across the action space dimensions and both the action and temporal dimensions, respectively. Our results show that the LAS attack deteriorates the agent's performance significantly more than the MAS attack, indicating that an adversary can use the agent's dynamics to craft malevolent attacks that cause it to fail. Furthermore, these attack strategies can be used to identify potential vulnerabilities of DRL agents.",1
"Efficient exploration for automatic subgoal discovery is a challenging problem in Hierarchical Reinforcement Learning (HRL). In this paper, we show that intrinsic motivation learning increases the efficiency of exploration, leading to successful subgoal discovery. We introduce a model-free subgoal discovery method based on unsupervised learning over a limited memory of agent's experiences during intrinsic motivation. Additionally, we offer a unified approach to learning representations in model-free HRL.",0
"Discovering automatic subgoals through effective exploration is a difficult task in HRL. In this study, we demonstrate that the use of intrinsic motivation learning improves exploration efficiency and subgoal discovery outcomes. We present an unsupervised, model-free method for subgoal discovery that relies on the agent's intrinsic motivation and a limited memory of its experiences. Furthermore, we offer a comprehensive technique for learning representations in model-free HRL.",1
"Solving complex, temporally-extended tasks is a long-standing problem in reinforcement learning (RL). We hypothesize that one critical element of solving such problems is the notion of compositionality. With the ability to learn concepts and sub-skills that can be composed to solve longer tasks, i.e. hierarchical RL, we can acquire temporally-extended behaviors. However, acquiring effective yet general abstractions for hierarchical RL is remarkably challenging. In this paper, we propose to use language as the abstraction, as it provides unique compositional structure, enabling fast learning and combinatorial generalization, while retaining tremendous flexibility, making it suitable for a variety of problems. Our approach learns an instruction-following low-level policy and a high-level policy that can reuse abstractions across tasks, in essence, permitting agents to reason using structured language. To study compositional task learning, we introduce an open-source object interaction environment built using the MuJoCo physics engine and the CLEVR engine. We find that, using our approach, agents can learn to solve to diverse, temporally-extended tasks such as object sorting and multi-object rearrangement, including from raw pixel observations. Our analysis reveals that the compositional nature of language is critical for learning diverse sub-skills and systematically generalizing to new sub-skills in comparison to non-compositional abstractions that use the same supervision.",0
"For quite some time, reinforcement learning (RL) has struggled to solve intricate, long-lasting tasks. We believe that one key aspect of solving such tasks is compositionality. By mastering concepts and sub-skills that can be combined to solve longer tasks, hierarchical RL can lead to the acquisition of temporally-extended behaviors. However, developing effective yet general abstractions for hierarchical RL is quite challenging. In this paper, we suggest using language as the abstraction, which offers unique compositional structure, allowing for swift learning and combinatorial generalization while maintaining significant flexibility suitable for various problems. Our method involves training a low-level policy that follows instructions and a high-level policy that can reuse abstractions across tasks, thus enabling agents to reason using structured language. To study compositional task learning, we introduce an open-source object interaction environment created using the MuJoCo physics engine and the CLEVR engine. Using our approach, we discovered that agents can learn to solve various temporally-extended tasks, such as object sorting and multi-object rearrangement, even from raw pixel observations. Our analysis shows that the compositional nature of language is critical in learning diverse sub-skills and systematically generalizing to new sub-skills compared to non-compositional abstractions that use the same supervision.",1
"This paper extends off-policy reinforcement learning to the multi-agent case in which a set of networked agents communicating with their neighbors according to a time-varying graph collaboratively evaluates and improves a target policy while following a distinct behavior policy. To this end, the paper develops a multi-agent version of emphatic temporal difference learning for off-policy policy evaluation, and proves convergence under linear function approximation. The paper then leverages this result, in conjunction with a novel multi-agent off-policy policy gradient theorem and recent work in both multi-agent on-policy and single-agent off-policy actor-critic methods, to develop and give convergence guarantees for a new multi-agent off-policy actor-critic algorithm.",0
"This article expands the use of off-policy reinforcement learning to the multi-agent scenario. This involves a group of networked agents who communicate with their neighbors based on a time-varying graph to cooperatively assess and improve a target policy while following a separate behavior policy. The article introduces a multi-agent version of emphatic temporal difference learning for off-policy policy evaluation, which is proven to converge with linear function approximation. Using this result, along with a novel multi-agent off-policy policy gradient theorem and recent research on both multi-agent on-policy and single-agent off-policy actor-critic methods, a new multi-agent off-policy actor-critic algorithm is developed, with convergence guarantees.",1
"Du, Kakade, Wang, and Yang recently established intriguing lower bounds on sample complexity, which suggest that reinforcement learning with a misspecified representation is intractable. Another line of work, which centers around a statistic called the eluder dimension, establishes tractability of problems similar to those considered in the Du-Kakade-Wang-Yang paper. We compare these results and reconcile interpretations.",0
"In a recent study, Kakade, Du, Wang, and Yang presented fascinating lower limits on sample complexity that imply the impracticality of reinforcement learning with a misrepresented representation. Meanwhile, a separate area of research, focused on the eluder dimension statistic, demonstrates the feasibility of solving problems similar to those examined in the Du-Kakade-Wang-Yang study. We compare these findings and harmonize their interpretations.",1
"We investigate the combination of actor-critic reinforcement learning algorithms with uniform large-scale experience replay and propose solutions for two challenges: (a) efficient actor-critic learning with experience replay (b) stability of off-policy learning where agents learn from other agents behaviour. We employ those insights to accelerate hyper-parameter sweeps in which all participating agents run concurrently and share their experience via a common replay module. To this end we analyze the bias-variance tradeoffs in V-trace, a form of importance sampling for actor-critic methods. Based on our analysis, we then argue for mixing experience sampled from replay with on-policy experience, and propose a new trust region scheme that scales effectively to data distributions where V-trace becomes unstable. We provide extensive empirical validation of the proposed solution. We further show the benefits of this setup by demonstrating state-of-the-art data efficiency on Atari among agents trained up until 200M environment frames.",0
"Our research focuses on integrating actor-critic reinforcement learning algorithms with uniform large-scale experience replay. We address two challenges: (a) enhancing the efficiency of actor-critic learning with experience replay, and (b) ensuring stability of off-policy learning in situations where agents learn from other agents' behavior. Our findings enable us to accelerate hyper-parameter sweeps by running participating agents concurrently and sharing their experience through a common replay module. We assess the bias-variance tradeoffs in V-trace, which is a form of importance sampling for actor-critic methods, and recommend mixing replayed experience with on-policy experience. In addition, we propose a trust region scheme that is effective for data distributions where V-trace becomes unstable. We provide extensive empirical validation of our proposed solution, which shows state-of-the-art data efficiency on Atari among agents trained up to 200M environment frames.",1
"The data efficiency of learning-based algorithms is more and more important since high-quality and clean data is expensive as well as hard to collect. In order to achieve high model performance with the least number of samples, active learning is a technique that queries the most important subset of data from the original dataset. In active learning domain, one of the mainstream research is the heuristic uncertainty-based method which is useful for the learning-based system. Recently, a few works propose to apply policy reinforcement learning (PRL) for querying important data. It seems more general than heuristic uncertainty-based method owing that PRL method depends on data feature which is reliable than human prior. However, there have two problems - sample inefficiency of policy learning and overconfidence, when applying PRL on active learning. To be more precise, sample inefficiency of policy learning occurs when sampling within a large action space, in the meanwhile, class imbalance can lead to the overconfidence. In this paper, we propose a bias-aware policy network called Heapified Active Learning (HAL), which prevents overconfidence, and improves sample efficiency of policy learning by heapified structure without ignoring global inforamtion(overview of the whole unlabeled set). In our experiment, HAL outperforms other baseline methods on MNIST dataset and duplicated MNIST. Last but not least, we investigate the generalization of the HAL policy learned on MNIST dataset by directly applying it on MNIST-M. We show that the agent can generalize and outperform directly-learned policy under constrained labeled sets.",0
"As high-quality and clean data is expensive and difficult to obtain, the data efficiency of learning-based algorithms is becoming increasingly important. Active learning is a technique that aims to achieve high model performance with the least number of samples by querying the most important subset of data from the original dataset. In the active learning domain, one of the mainstream research areas is the heuristic uncertainty-based method, which is useful for learning-based systems. However, recent works propose to apply policy reinforcement learning (PRL) for querying important data, which seems more general than the heuristic uncertainty-based method since the PRL method depends on reliable data features rather than human prior. Nonetheless, applying PRL on active learning poses two problems: sample inefficiency of policy learning and overconfidence. Sample inefficiency of policy learning occurs when sampling within a large action space, while class imbalance can lead to overconfidence. In this paper, we propose a bias-aware policy network called Heapified Active Learning (HAL), which prevents overconfidence and improves sample efficiency of policy learning by using a heapified structure that does not ignore the global information of the entire unlabeled set. Our experiments show that HAL outperforms other baseline methods on the MNIST dataset and duplicated MNIST. Furthermore, we investigate the generalization of the HAL policy learned on the MNIST dataset by directly applying it on MNIST-M. We demonstrate that the agent can generalize and outperform directly-learned policy under constrained labeled sets.",1
"We observe that several existing policy gradient methods (such as vanilla policy gradient, PPO, A2C) may suffer from overly large gradients when the current policy is close to deterministic (even in some very simple environments), leading to an unstable training process. To address this issue, we propose a new method, called \emph{target distribution learning} (TDL), for policy improvement in reinforcement learning. TDL alternates between proposing a target distribution and training the policy network to approach the target distribution. TDL is more effective in constraining the KL divergence between updated policies, and hence leads to more stable policy improvements over iterations. Our experiments show that TDL algorithms perform comparably to (or better than) state-of-the-art algorithms for most continuous control tasks in the MuJoCo environment while being more stable in training.",0
"It has been noticed that certain present policy gradient methods, including PPO, vanilla policy gradient, and A2C, can encounter problems with excessively large gradients when the current policy is almost deterministic, even in some basic environments, resulting in an unsteady training process. We have developed a new approach, known as ""target distribution learning"" (TDL), to address this problem in reinforcement learning policy improvement. TDL involves proposing a target distribution and then training the policy network to approximate it. TDL is more successful in restricting the KL divergence between updated policies, leading to more consistent policy improvements over time. Our experiments have demonstrated that TDL methods perform similarly to, or surpass, cutting-edge algorithms for most continuous control tasks in the MuJoCo environment, while being more stable during training.",1
"In this work, we study semi-supervised multi-label node classification problem in attributed graphs. Classic solutions to multi-label node classification follow two steps, first learn node embedding and then build a node classifier on the learned embedding. To improve the discriminating power of the node embedding, we propose a novel collaborative graph walk, named Multi-Label-Graph-Walk, to finely tune node representations with the available label assignments in attributed graphs via reinforcement learning. The proposed method formulates the multi-label node classification task as simultaneous graph walks conducted by multiple label-specific agents. Furthermore, policies of the label-wise graph walks are learned in a cooperative way to capture first the predictive relation between node labels and structural attributes of graphs; and second, the correlation among the multiple label-specific classification tasks. A comprehensive experimental study demonstrates that the proposed method can achieve significantly better multi-label classification performance than the state-of-the-art approaches and conduct more efficient graph exploration.",0
"The focus of our research is on the semi-supervised multi-label node classification issue in attributed graphs. The traditional approach to multi-label node classification involves learning node embedding and then constructing a node classifier based on the learned embedding. To enhance the discriminating ability of the node embedding, we suggest a new collaborative graph walk, called Multi-Label-Graph-Walk, which fine-tunes node representations with the available label assignments in attributed graphs via reinforcement learning. The proposed approach frames the multi-label node classification task as numerous graph walks carried out by various label-specific agents. Additionally, policies of the label-wise graph walks are acquired in a cooperative way to capture firstly the predictive relationship between node labels and structural attributes of graphs, and secondly, the correlation among the multiple label-specific classification tasks. Our extensive experimental analysis confirms that the proposed method outperforms state-of-the-art techniques in multi-label classification performance and is more effective in graph exploration.",1
"In this paper, we derive a new model of synaptic plasticity, based on recent algorithms for reinforcement learning (in which an agent attempts to learn appropriate actions to maximize its long-term average reward). We show that these direct reinforcement learning algorithms also give locally optimal performance for the problem of reinforcement learning with multiple agents, without any explicit communication between agents. By considering a network of spiking neurons as a collection of agents attempting to maximize the long-term average of a reward signal, we derive a synaptic update rule that is qualitatively similar to Hebb's postulate. This rule requires only simple computations, such as addition and leaky integration, and involves only quantities that are available in the vicinity of the synapse. Furthermore, it leads to synaptic connection strengths that give locally optimal values of the long term average reward. The reinforcement learning paradigm is sufficiently broad to encompass many learning problems that are solved by the brain. We illustrate, with simulations, that the approach is effective for simple pattern classification and motor learning tasks.",0
"This paper introduces a novel model of synaptic plasticity that draws on recent reinforcement learning algorithms. These algorithms enable agents to learn suitable actions that maximize long-term average rewards. We demonstrate that these direct reinforcement learning algorithms yield locally optimal performance for the problem of reinforcement learning with multiple agents, even without explicit communication between agents. By treating a network of spiking neurons as a group of agents attempting to maximize the long-term average of a reward signal, we derive a synaptic update rule that is akin to Hebb's postulate. This rule involves basic computations, such as addition and leaky integration, and utilizes only quantities that are available near the synapse. Additionally, it leads to synaptic connection strengths that produce locally optimal values of the long-term average reward. The reinforcement learning framework is broad enough to encompass many learning problems solved by the brain. Using simulations, we demonstrate the effectiveness of this approach for simple pattern classification and motor learning tasks.",1
"There is an emerging trend in the reinforcement learning for healthcare literature. In order to prepare longitudinal, irregularly sampled, clinical datasets for reinforcement learning algorithms, many researchers will resample the time series data to short, regular intervals and use last-observation-carried-forward (LOCF) imputation to fill in these gaps. Typically, they will not maintain any explicit information about which values were imputed. In this work, we (1) call attention to this practice and discuss its potential implications; (2) propose an alternative representation of the patient state that addresses some of these issues; and (3) demonstrate in a novel but representative clinical dataset that our alternative representation yields consistently better results for achieving optimal control, as measured by off-policy policy evaluation, compared to representations that do not incorporate missingness information.",0
"The healthcare literature on reinforcement learning is witnessing a developing trend. To ready clinical datasets with longitudinal, irregular sampling for reinforcement learning algorithms, many researchers resample the time series data to brief, regular intervals and use last-observation-carried-forward (LOCF) imputation to fill in the gaps. They usually do not keep any explicit record of the imputed values. In this study, we (1) highlight this practice and its potential repercussions; (2) suggest an alternative approach to representing the patient state that addresses some of these issues; and (3) demonstrate, using a new but representative clinical dataset, that our alternative representation consistently yields better results for achieving optimal control, as measured by off-policy policy evaluation, compared to representations that do not account for missingness information.",1
"Imitation learning trains a policy from expert demonstrations. Imitation learning approaches have been designed from various principles, such as behavioral cloning via supervised learning, apprenticeship learning via inverse reinforcement learning, and GAIL via generative adversarial learning. In this paper, we propose a framework to analyze the theoretical property of imitation learning approaches based on discrepancy propagation analysis. Under the infinite-horizon setting, the framework leads to the value discrepancy of behavioral cloning in an order of O((1-\gamma)^{-2}). We also show that the framework leads to the value discrepancy of GAIL in an order of O((1-\gamma)^{-1}). It implies that GAIL has less compounding errors than behavioral cloning, which is also verified empirically in this paper. To the best of our knowledge, we are the first one to analyze GAIL's performance theoretically. The above results indicate that the proposed framework is a general tool to analyze imitation learning approaches. We hope our theoretical results can provide insights for future improvements in imitation learning algorithms.",0
"Imitation learning involves training a policy using demonstrations from experts. There are different approaches to imitation learning, such as behavioral cloning, apprenticeship learning, and GAIL. In this study, we present a framework to analyze the theoretical properties of these approaches using discrepancy propagation analysis. Our results show that behavioral cloning has a value discrepancy of O((1-\gamma)^{-2}) under the infinite-horizon setting, while GAIL has a value discrepancy of O((1-\gamma)^{-1}). This suggests that GAIL has fewer compounding errors than behavioral cloning, which was also empirically verified in this study. This is the first study to analyze GAIL's performance theoretically. Our framework provides a general tool for analyzing imitation learning approaches and our results can guide future improvements in these algorithms.",1
"We consider the problem of recovering an expert's reward function with inverse reinforcement learning (IRL) when there are missing/incomplete state-action pairs or observations in the demonstrated trajectories. This issue of missing trajectory data or information occurs in many situations, e.g., GPS signals from vehicles moving on a road network are intermittent. In this paper, we propose a tractable approach to directly compute the log-likelihood of demonstrated trajectories with incomplete/missing data. Our algorithm is efficient in handling a large number of missing segments in the demonstrated trajectories, as it performs the training with incomplete data by solving a sequence of systems of linear equations, and the number of such systems to be solved does not depend on the number of missing segments. Empirical evaluation on a real-world dataset shows that our training algorithm outperforms other conventional techniques.",0
"The problem we address in this paper is the recovery of an expert's reward function using inverse reinforcement learning (IRL), even if there are missing or incomplete state-action pairs or observations in the demonstrated trajectories. This issue is prevalent in situations such as intermittent GPS signals from moving vehicles on a road network. Our proposed approach offers a feasible solution to directly calculate the log-likelihood of the incomplete/missing data in the demonstrated trajectories. Moreover, our algorithm is efficient in managing a large number of missing segments in the demonstrated trajectories, as it uses a sequence of systems of linear equations to solve the training with incomplete data, and the number of such systems to solve is independent of the number of missing segments. Our empirical evaluation on a real-world dataset demonstrates that our training algorithm surpasses other conventional techniques.",1
"This paper investigates methods for estimating the optimal stochastic control policy for a Markov Decision Process with unknown transition dynamics and an unknown reward function. This form of model-free reinforcement learning comprises many real world systems such as playing video games, simulated control tasks, and real robot locomotion. Existing methods for estimating the optimal stochastic control policy rely on high variance estimates of the policy descent. However, these methods are not guaranteed to find the optimal stochastic policy, and the high variance gradient estimates make convergence unstable. In order to resolve these problems, we propose a technique using Markov Chain Monte Carlo to generate samples from the posterior distribution of the parameters conditioned on being optimal. Our method provably converges to the globally optimal stochastic policy, and empirically similar variance compared to the policy gradient.",0
"The objective of this study is to explore various approaches to determine the best stochastic control policy for a Markov Decision Process with an unknown reward function and transition dynamics. This type of model-free reinforcement learning is commonly used in video games, simulated control tasks, and robot locomotion. Despite the availability of methods that estimate the optimal stochastic control policy, these techniques rely on high variance estimates of the policy descent. Additionally, these methods are not always effective in identifying the best stochastic policy, and the high variance gradient estimates can lead to unstable convergence. To address these issues, we present a new technique that employs Markov Chain Monte Carlo to generate samples from the posterior distribution of the parameters, given that they are optimal. Our method is proven to converge to the globally optimal stochastic policy and has a variance comparable to the policy gradient according to empirical data.",1
"Model-free reinforcement learning algorithms such as Deep Deterministic Policy Gradient (DDPG) often require additional exploration strategies, especially if the actor is of deterministic nature. This work evaluates the use of model-based trajectory optimization methods used for exploration in Deep Deterministic Policy Gradient when trained on a latent image embedding. In addition, an extension of DDPG is derived using a value function as critic, making use of a learned deep dynamics model to compute the policy gradient. This approach leads to a symbiotic relationship between the deep reinforcement learning algorithm and the latent trajectory optimizer. The trajectory optimizer benefits from the critic learned by the RL algorithm and the latter from the enhanced exploration generated by the planner. The developed methods are evaluated on two continuous control tasks, one in simulation and one in the real world. In particular, a Baxter robot is trained to perform an insertion task, while only receiving sparse rewards and images as observations from the environment.",0
"The exploration of deterministic actor-based model-free reinforcement learning algorithms, like Deep Deterministic Policy Gradient (DDPG), often requires additional strategies. This study examines the use of model-based trajectory optimization methods for exploration in DDPG, which is trained on a latent image embedding. Furthermore, an extension of DDPG is proposed that utilizes a value function as a critic, utilizing a learned deep dynamics model for policy gradient computation. This approach fosters a mutually beneficial relationship between the deep reinforcement learning algorithm and the latent trajectory optimizer. The planner benefits from the critic learned by the RL algorithm, while the latter benefits from the improved exploration generated by the planner. The efficacy of these methods is evaluated on two continuous control tasks, one in simulation and one in the real world, where a Baxter robot is trained to perform an insertion task using sparse rewards and images as observations from the environment.",1
"Humans and animals are capable of quickly learning new behaviours to solve new tasks. Yet, we often forget that they also rely on a highly specialized morphology that co-adapted with motor control throughout thousands of years. Although compelling, the idea of co-adapting morphology and behaviours in robots is often unfeasible because of the long manufacturing times, and the need to re-design an appropriate controller for each morphology. In this paper, we propose a novel approach to automatically and efficiently co-adapt a robot morphology and its controller. Our approach is based on recent advances in deep reinforcement learning, and specifically the soft actor critic algorithm. Key to our approach is the possibility of leveraging previously tested morphologies and behaviors to estimate the performance of new candidate morphologies. As such, we can make full use of the information available for making more informed decisions, with the ultimate goal of achieving a more data-efficient co-adaptation (i.e., reducing the number of morphologies and behaviors tested). Simulated experiments show that our approach requires drastically less design prototypes to find good morphology-behaviour combinations, making this method particularly suitable for future co-adaptation of robot designs in the real world.",0
"Although humans and animals are adept at quickly learning new behaviors to solve novel challenges, we often overlook their reliance on specialized morphology that has evolved over millennia to work in tandem with motor control. However, replicating this co-adaptation of morphology and behavior in robots is often impractical due to lengthy manufacturing times and the need to create a new controller for each morphology. In this paper, we introduce a fresh approach to automatically and efficiently co-adapt a robot's morphology and controller using recent advancements in deep reinforcement learning, specifically the soft actor critic algorithm. Our approach leverages previously tested morphologies and behaviors to estimate the performance of new candidate morphologies, allowing us to make more informed decisions and achieve a more data-efficient co-adaptation while reducing the number of morphologies and behaviors tested. Simulated experiments show that our method drastically reduces the number of design prototypes required to find optimal morphology-behavior combinations, making it ideal for real-world robot co-adaptation.",1
"Traffic sign identification using camera images from vehicles plays a critical role in autonomous driving and path planning. However, the front camera images can be distorted due to blurriness, lighting variations and vandalism which can lead to degradation of detection performances. As a solution, machine learning models must be trained with data from multiple domains, and collecting and labeling more data in each new domain is time consuming and expensive. In this work, we present an end-to-end framework to augment traffic sign training data using optimal reinforcement learning policies and a variety of Generative Adversarial Network (GAN) models, that can then be used to train traffic sign detector modules. Our automated augmenter enables learning from transformed nightime, poor lighting, and varying degrees of occlusions using the LISA Traffic Sign and BDD-Nexar dataset. The proposed method enables mapping training data from one domain to another, thereby improving traffic sign detection precision/recall from 0.70/0.66 to 0.83/0.71 for nighttime images.",0
"The identification of traffic signs through camera images from vehicles is crucial for autonomous driving and path planning. However, detection performance may be compromised due to distorted front camera images caused by blurriness, lighting changes, and vandalism. To address this issue, machine learning models need to be trained with data from multiple domains, which can be time-consuming and expensive. This study introduces an end-to-end framework that utilizes optimal reinforcement learning policies and various Generative Adversarial Network (GAN) models to augment traffic sign training data. The automated augmenter enables learning from nighttime, poor lighting, and occluded images using the LISA Traffic Sign and BDD-Nexar dataset. The proposed method facilitates the mapping of training data from one domain to another, improving traffic sign detection precision/recall from 0.70/0.66 to 0.83/0.71 for nighttime images.",1
"This paper introduces ASCAI, a novel adaptive sampling methodology that can learn how to effectively compress Deep Neural Networks (DNNs) for accelerated inference on resource-constrained platforms. Modern DNN compression techniques comprise various hyperparameters that require per-layer customization to ensure high accuracy. Choosing such hyperparameters is cumbersome as the pertinent search space grows exponentially with the number of model layers. To effectively traverse this large space, we devise an intelligent sampling mechanism that adapts the sampling strategy using customized operations inspired by genetic algorithms. As a special case, we consider the space of model compression as a vector space. The adaptively selected samples enable ASCAI to automatically learn how to tune per-layer compression hyperparameters to optimize the accuracy/model-size trade-off. Our extensive evaluations show that ASCAI outperforms rule-based and reinforcement learning methods in terms of compression rate and/or accuracy",0
"ASCAI, a new approach to adaptive sampling, is presented in this paper. Its purpose is to efficiently compress Deep Neural Networks (DNNs) for faster inference on platforms with limited resources. Traditional DNN compression methods require customization of various hyperparameters for each layer to achieve high accuracy. This can be a difficult task due to the large search space that grows exponentially with the number of layers. To address this issue, we have developed a sampling mechanism that adapts its strategy using genetic algorithms. By considering the model compression space as a vector space, ASCAI can learn how to optimize the accuracy and model size trade-off for each layer's compression hyperparameters. Our results show that ASCAI performs better than rule-based and reinforcement learning methods in terms of compression rate and/or accuracy.",1
"We introduce a new neural network-based continual learning algorithm, dubbed as Uncertainty-regularized Continual Learning (UCL), which builds on traditional Bayesian online learning framework with variational inference. We focus on two significant drawbacks of the recently proposed regularization-based methods: a) considerable additional memory cost for determining the per-weight regularization strengths and b) the absence of gracefully forgetting scheme, which can prevent performance degradation in learning new tasks. In this paper, we show UCL can solve these two problems by introducing a fresh interpretation on the Kullback-Leibler (KL) divergence term of the variational lower bound for Gaussian mean-field approximation. Based on the interpretation, we propose the notion of node-wise uncertainty, which drastically reduces the number of additional parameters for implementing per-weight regularization. Moreover, we devise two additional regularization terms that enforce stability by freezing important parameters for past tasks and allow plasticity by controlling the actively learning parameters for a new task. Through extensive experiments, we show UCL convincingly outperforms most of recent state-of-the-art baselines not only on popular supervised learning benchmarks, but also on challenging lifelong reinforcement learning tasks. The source code of our algorithm is available at https://github.com/csm9493/UCL.",0
"Our paper presents a novel neural network-based continual learning algorithm called Uncertainty-regularized Continual Learning (UCL), which is an extension of the traditional Bayesian online learning framework with variational inference. We address two significant issues with existing regularization-based methods: the high memory cost of determining per-weight regularization strengths and the lack of a graceful forgetting scheme to prevent performance degradation when learning new tasks. We demonstrate that UCL can overcome these problems by offering a new interpretation of the Kullback-Leibler (KL) divergence term of the variational lower bound for Gaussian mean-field approximation. This new interpretation introduces the concept of node-wise uncertainty, which reduces the number of additional parameters required for per-weight regularization. Additionally, we introduce two new regularization terms that promote stability by freezing important parameters for past tasks and allow plasticity by controlling the actively learning parameters for new tasks. Our experiments show that UCL outperforms state-of-the-art baselines on both supervised learning benchmarks and lifelong reinforcement learning tasks. The source code for UCL is available on our GitHub repository (https://github.com/csm9493/UCL).",1
"We present a new general approach to modeling research problems as Atari-like videogames to make them amenable to recent groundbreaking solution methods from the deep reinforcement learning community. The approach is flexible, applicable to a wide range of problems. We demonstrate its application on a well known vehicle routing problem. Our preliminary results on this problem, though not transformative, show signs of success and suggest that Atari-fication may be a useful modeling approach for researchers studying problems involving sequential decision making under uncertainty.",0
"Our study introduces a versatile method for converting research problems into Atari-like videogames, which can be effectively addressed through advanced reinforcement learning techniques. This approach is adaptable to various problem domains, and we illustrate its effectiveness in solving a popular vehicle routing problem. Although our initial findings do not bring about a significant transformation, they indicate promising outcomes and highlight the potential of Atari-fication as a modeling strategy for researchers investigating decision-making challenges in uncertain environments.",1
"We present an algorithm, HOMER, for exploration and reinforcement learning in rich observation environments that are summarizable by an unknown latent state space. The algorithm interleaves representation learning to identify a new notion of kinematic state abstraction with strategic exploration to reach new states using the learned abstraction. The algorithm provably explores the environment with sample complexity scaling polynomially in the number of latent states and the time horizon, and, crucially, with no dependence on the size of the observation space, which could be infinitely large. This exploration guarantee further enables sample-efficient global policy optimization for any reward function. On the computational side, we show that the algorithm can be implemented efficiently whenever certain supervised learning problems are tractable. Empirically, we evaluate HOMER on a challenging exploration problem, where we show that the algorithm is exponentially more sample efficient than standard reinforcement learning baselines.",0
"An algorithm called HOMER is introduced for exploration and reinforcement learning in observation environments that can be summarized by an unknown latent state space. The algorithm uses representation learning to identify a new kinematic state abstraction, which is combined with strategic exploration to reach new states. The algorithm guarantees exploration with sample complexity that scales polynomially in the number of latent states and the time horizon, without being dependent on the size of the observation space. This guarantees sample-efficient global policy optimization for any reward function. The algorithm can be implemented efficiently when certain supervised learning problems are tractable. The empirical evaluation shows that HOMER is exponentially more sample efficient than standard reinforcement learning baselines when applied to a challenging exploration problem.",1
"Importance sampling (IS) is a common reweighting strategy for off-policy prediction in reinforcement learning. While it is consistent and unbiased, it can result in high variance updates to the weights for the value function. In this work, we explore a resampling strategy as an alternative to reweighting. We propose Importance Resampling (IR) for off-policy prediction, which resamples experience from a replay buffer and applies standard on-policy updates. The approach avoids using importance sampling ratios in the update, instead correcting the distribution before the update. We characterize the bias and consistency of IR, particularly compared to Weighted IS (WIS). We demonstrate in several microworlds that IR has improved sample efficiency and lower variance updates, as compared to IS and several variance-reduced IS strategies, including variants of WIS and V-trace which clips IS ratios. We also provide a demonstration showing IR improves over IS for learning a value function from images in a racing car simulator.",0
"In reinforcement learning, Importance Sampling (IS) is a popular method for reweighting in off-policy prediction. However, while it is unbiased and consistent, it can lead to significant variance in the weight updates for the value function. This study proposes an alternative approach to reweighting, known as Importance Resampling (IR), which involves resampling experience from a replay buffer and applying standard on-policy updates. Instead of using importance sampling ratios in the update, IR corrects the distribution before the update. The study investigates the consistency and bias of IR, particularly in comparison to Weighted IS (WIS). The results of several microworlds experiments show that IR has better sample efficiency and low variance updates than IS and various variance-reduced IS strategies, such as WIS and V-trace, which clips IS ratios. Additionally, a demonstration shows that IR outperforms IS when learning a value function from images in a racing car simulator.",1
"Learning the value function of a given policy (target policy) from the data samples obtained from a different policy (behavior policy) is an important problem in Reinforcement Learning (RL). This problem is studied under the setting of off-policy prediction. Temporal Difference (TD) learning algorithms are a popular class of algorithms for solving the prediction problem. TD algorithms with linear function approximation are shown to be convergent when the samples are generated from the target policy (known as on-policy prediction). However, it has been well established in the literature that off-policy TD algorithms under linear function approximation diverge. In this work, we propose a convergent on-line off-policy TD algorithm under linear function approximation. The main idea is to penalize the updates of the algorithm in a way as to ensure convergence of the iterates. We provide a convergence analysis of our algorithm. Through numerical evaluations, we further demonstrate the effectiveness of our algorithm.",0
"In Reinforcement Learning (RL), it is crucial to learn the value function of a target policy from data samples, even if they were obtained from a different behavior policy. This off-policy prediction problem is commonly solved using Temporal Difference (TD) learning algorithms. While TD algorithms with linear function approximation are known to converge when the samples are generated from the target policy (on-policy prediction), they tend to diverge when dealing with off-policy prediction. To address this issue, we introduce an on-line off-policy TD algorithm under linear function approximation that guarantees convergence by penalizing the algorithm's updates. We also conduct numerical evaluations to prove the effectiveness of our proposed algorithm and provide a comprehensive convergence analysis.",1
"We consider the issue of multiple agents learning to communicate through reinforcement learning within partially observable environments, with a focus on information asymmetry in the second part of our work. We provide a review of the recent algorithms developed to improve the agents' policy by allowing the sharing of information between agents and the learning of communication strategies, with a focus on Deep Recurrent Q-Network-based models. We also describe recent efforts to interpret the languages generated by these agents and study their properties in an attempt to generate human-language-like sentences. We discuss the metrics used to evaluate the generated communication strategies and propose a novel entropy-based evaluation metric. Finally, we address the issue of the cost of communication and introduce the idea of an experimental setup to expose this cost in cooperative-competitive game.",0
"In our work, we examine the concept of reinforcement learning in partially observable environments, where multiple agents learn to communicate. Our focus is on information asymmetry in the latter part of our research. We provide a detailed overview of the latest algorithms developed to enhance agents' policies by enabling the sharing of information between them and learning communication strategies. Deep Recurrent Q-Network-based models are the main emphasis of our study. Additionally, we discuss recent attempts to interpret the languages generated by these agents and analyze their properties to create sentences that resemble human language. We also discuss the metrics used to evaluate the generated communication strategies and suggest a novel entropy-based evaluation metric. Finally, we address the issue of communication cost and introduce an experimental setup to expose it in a cooperative-competitive game.",1
"Reinforcement learning algorithms such as the deep deterministic policy gradient algorithm (DDPG) has been widely used in continuous control tasks. However, the model-free DDPG algorithm suffers from high sample complexity. In this paper we consider the deterministic value gradients to improve the sample efficiency of deep reinforcement learning algorithms. Previous works consider deterministic value gradients with the finite horizon, but it is too myopic compared with infinite horizon. We firstly give a theoretical guarantee of the existence of the value gradients in this infinite setting. Based on this theoretical guarantee, we propose a class of the deterministic value gradient algorithm (DVG) with infinite horizon, and different rollout steps of the analytical gradients by the learned model trade off between the variance of the value gradients and the model bias. Furthermore, to better combine the model-based deterministic value gradient estimators with the model-free deterministic policy gradient estimator, we propose the deterministic value-policy gradient (DVPG) algorithm. We finally conduct extensive experiments comparing DVPG with state-of-the-art methods on several standard continuous control benchmarks. Results demonstrate that DVPG substantially outperforms other baselines.",0
"The deep deterministic policy gradient algorithm (DDPG) is commonly used for continuous control tasks in reinforcement learning. However, the DDPG algorithm without a model suffers from a high sample complexity. This paper aims to improve the sample efficiency of deep reinforcement learning algorithms by considering deterministic value gradients. Previous studies have only examined deterministic value gradients with finite horizons, which is too short-sighted compared to infinite horizons. The authors provide a theoretical guarantee for the existence of value gradients in infinite settings and propose a new class of deterministic value gradient algorithm (DVG) with infinite horizons. The proposed algorithm uses different rollout steps of analytical gradients learned by the model to balance the variance of value gradients and model bias. Additionally, the authors propose the deterministic value-policy gradient (DVPG) algorithm to combine model-based deterministic value gradient estimators with model-free deterministic policy gradient estimators. Extensive experiments on standard continuous control benchmarks show that DVPG outperforms other state-of-the-art methods.",1
"The Pommerman simulation was recently developed to mimic the classic Japanese game Bomberman, and focuses on competitive gameplay in a multi-agent setting. We focus on the 2$\times$2 team version of Pommerman, developed for a competition at NeurIPS 2018. Our methodology involves training an agent initially through imitation learning on a noisy expert policy, followed by a proximal-policy optimization (PPO) reinforcement learning algorithm. The basic PPO approach is modified for stable transition from the imitation learning phase through reward shaping, action filters based on heuristics, and curriculum learning. The proposed methodology is able to beat heuristic and pure reinforcement learning baselines with a combined 100,000 training games, significantly faster than other non-tree-search methods in literature. We present results against multiple agents provided by the developers of the simulation, including some that we have enhanced. We include a sensitivity analysis over different parameters, and highlight undesirable effects of some strategies that initially appear promising. Since Pommerman is a complex multi-agent competitive environment, the strategies developed here provide insights into several real-world problems with characteristics such as partial observability, decentralized execution (without communication), and very sparse and delayed rewards.",0
"Recently, a simulation called Pommerman was created to replicate the Japanese game Bomberman, with a focus on competitive gameplay in a setting with multiple agents. Our research concentrates on the 2$\times$2 team version of Pommerman, which was developed for the NeurIPS 2018 competition. The methodology we propose involves training an agent using imitation learning with a noisy expert policy, followed by a reinforcement learning algorithm called proximal-policy optimization (PPO). We modify the basic PPO approach to ensure stable transition from the imitation learning phase, incorporating reward shaping, action filters based on heuristics, and curriculum learning. Our proposed methodology can outperform heuristic and pure reinforcement learning baselines with a combined 100,000 training games, which is significantly faster than other non-tree-search methods documented in literature. We present results against multiple agents provided by the simulation developers, including some that we have improved. We also conduct a sensitivity analysis on different parameters and highlight the negative effects of some strategies that seem promising at first. As Pommerman is a complex competitive environment with multiple agents, the strategies we develop can provide insights into several real-world problems with characteristics such as partial observability, decentralized execution (without communication), and very sparse and delayed rewards.",1
"We introduce two tactics to attack agents trained by deep reinforcement learning algorithms using adversarial examples, namely the strategically-timed attack and the enchanting attack. In the strategically-timed attack, the adversary aims at minimizing the agent's reward by only attacking the agent at a small subset of time steps in an episode. Limiting the attack activity to this subset helps prevent detection of the attack by the agent. We propose a novel method to determine when an adversarial example should be crafted and applied. In the enchanting attack, the adversary aims at luring the agent to a designated target state. This is achieved by combining a generative model and a planning algorithm: while the generative model predicts the future states, the planning algorithm generates a preferred sequence of actions for luring the agent. A sequence of adversarial examples is then crafted to lure the agent to take the preferred sequence of actions. We apply the two tactics to the agents trained by the state-of-the-art deep reinforcement learning algorithm including DQN and A3C. In 5 Atari games, our strategically timed attack reduces as much reward as the uniform attack (i.e., attacking at every time step) does by attacking the agent 4 times less often. Our enchanting attack lures the agent toward designated target states with a more than 70% success rate. Videos are available at http://yenchenlin.me/adversarial_attack_RL/",0
"The article presents two methods for attacking agents trained by deep reinforcement learning algorithms using adversarial examples. The first method is called the strategically-timed attack, which involves attacking the agent only at specific time steps in an episode to minimize its reward. This approach helps evade detection by the agent. The authors propose a new way to determine when to create and apply an adversarial example. The second method is the enchanting attack, which aims to lure the agent to a designated target state. This is accomplished by combining a generative model and a planning algorithm to predict future states and generate a preferred sequence of actions to lead the agent to the target state. The authors test these methods on agents trained using DQN and A3C, and find that the strategically-timed attack reduces as much reward as the uniform attack while attacking the agent less often. Additionally, the enchanting attack successfully lures the agent to the target state in over 70% of cases. Videos of the experiments can be found at http://yenchenlin.me/adversarial_attack_RL/.",1
"Drawing an inspiration from behavioral studies of human decision making, we propose here a general parametric framework for a reinforcement learning problem, which extends the standard Q-learning approach to incorporate a two-stream framework of reward processing with biases biologically associated with several neurological and psychiatric conditions, including Parkinson's and Alzheimer's diseases, attention-deficit/hyperactivity disorder (ADHD), addiction, and chronic pain. For AI community, the development of agents that react differently to different types of rewards can enable us to understand a wide spectrum of multi-agent interactions in complex real-world socioeconomic systems. Moreover, from the behavioral modeling perspective, our parametric framework can be viewed as a first step towards a unifying computational model capturing reward processing abnormalities across multiple mental conditions and user preferences in long-term recommendation systems.",0
"We propose a general parametric framework for a reinforcement learning problem that draws inspiration from behavioral studies of human decision making. This approach extends the standard Q-learning approach by incorporating a two-stream framework of reward processing. This framework includes biases that are biologically associated with several neurological and psychiatric conditions, such as Parkinson's and Alzheimer's diseases, attention-deficit/hyperactivity disorder (ADHD), addiction, and chronic pain. The development of agents that react differently to different types of rewards can enable the AI community to understand a wide spectrum of multi-agent interactions in complex real-world socioeconomic systems. Our parametric framework can also be viewed as a first step towards a unifying computational model that captures reward processing abnormalities across multiple mental conditions and user preferences in long-term recommendation systems from a behavioral modeling perspective.",1
"In this work, we introduce Graph Pointer Networks (GPNs) trained using reinforcement learning (RL) for tackling the traveling salesman problem (TSP). GPNs build upon Pointer Networks by introducing a graph embedding layer on the input, which captures relationships between nodes. Furthermore, to approximate solutions to constrained combinatorial optimization problems such as the TSP with time windows, we train hierarchical GPNs (HGPNs) using RL, which learns a hierarchical policy to find an optimal city permutation under constraints. Each layer of the hierarchy is designed with a separate reward function, resulting in stable training. Our results demonstrate that GPNs trained on small-scale TSP50/100 problems generalize well to larger-scale TSP500/1000 problems, with shorter tour lengths and faster computational times. We verify that for constrained TSP problems such as the TSP with time windows, the feasible solutions found via hierarchical RL training outperform previous baselines. In the spirit of reproducible research we make our data, models, and code publicly available.",0
"The aim of this study is to present Graph Pointer Networks (GPNs) that have been trained using reinforcement learning (RL) to solve the traveling salesman problem (TSP). GPNs are an extension of Pointer Networks, with a graph embedding layer added to the input layer to capture relationships between nodes. To address constrained combinatorial optimization problems, hierarchical GPNs (HGPNs) were trained using RL to learn a hierarchical policy. Each layer of the hierarchy has a separate reward function, resulting in stable training. Our experiments demonstrate that GPNs trained on small-scale TSP50/100 problems generalize well to larger-scale TSP500/1000 problems, with shorter tour lengths and faster computational times. Furthermore, we confirm that for constrained TSP problems, feasible solutions from hierarchical RL training outperform previous baselines. We provide our data, models, and code publicly for reproducible research.",1
"Optical Earth observation satellites acquire images worldwide , covering up to several million square kilometers every day. The complexity of scheduling acquisitions for such systems increases exponentially when considering the interoperabil-ity of several satellite constellations together with the uncertainties from weather forecasts. In order to deliver valid images to customers as fast as possible, it is crucial to acquire cloud-free images. Depending on weather forecasts, up to 50% of images acquired by operational satellites can be trashed due to excessive cloud covers, showing there is room for improvement. We propose an acquisition scheduling approach based on Deep Reinforcement Learning and experiment on a simplified environment. We find that it challenges classical methods relying on human-expert heuristic.",0
"Every day, optical Earth observation satellites capture images from across the world, covering millions of square kilometers. However, scheduling acquisitions for these systems becomes increasingly complex when considering the interoperability of multiple satellite constellations and weather uncertainties. To deliver valid images quickly, cloud-free images are essential. Unfortunately, weather forecasts can lead to up to 50% of operational satellite images being discarded due to excessive cloud coverage, highlighting the need for improvement. Our proposed acquisition scheduling approach is based on Deep Reinforcement Learning, and we tested it in a simplified environment. Our findings suggest that it outperforms classical methods that rely on human-expert heuristic.",1
"We propose Episodic Backward Update (EBU) - a novel deep reinforcement learning algorithm with a direct value propagation. In contrast to the conventional use of the experience replay with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state to its previous states. Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate directly through all transitions of the sampled episode. We theoretically prove the convergence of the EBU method and experimentally demonstrate its performance in both deterministic and stochastic environments. Especially in 49 games of Atari 2600 domain, EBU achieves the same mean and median human normalized performance of DQN by using only 5% and 10% of samples, respectively.",0
"The authors introduce a new deep reinforcement learning strategy called Episodic Backward Update (EBU) which involves propagating state values directly through a whole episode, as opposed to uniformly sampling experiences. Using a computationally efficient recursive algorithm, EBU allows for sparse and delayed rewards to propagate effectively throughout all transitions of the episode. The authors prove EBU's convergence theoretically and demonstrate its strong performance in both deterministic and stochastic environments. In fact, EBU achieves the same mean and median human normalized performance of DQN in 49 Atari 2600 games while using only 5% and 10% of samples, respectively.",1
"The information bottleneck principle is an elegant and useful approach to representation learning. In this paper, we investigate the problem of representation learning in the context of reinforcement learning using the information bottleneck framework, aiming at improving the sample efficiency of the learning algorithms. %by accelerating the process of discarding irrelevant information when the %input states are extremely high-dimensional. We analytically derive the optimal conditional distribution of the representation, and provide a variational lower bound. Then, we maximize this lower bound with the Stein variational (SV) gradient method. We incorporate this framework in the advantageous actor critic algorithm (A2C) and the proximal policy optimization algorithm (PPO). Our experimental results show that our framework can improve the sample efficiency of vanilla A2C and PPO significantly. Finally, we study the information bottleneck (IB) perspective in deep RL with the algorithm called mutual information neural estimation(MINE) . We experimentally verify that the information extraction-compression process also exists in deep RL and our framework is capable of accelerating this process. We also analyze the relationship between MINE and our method, through this relationship, we theoretically derive an algorithm to optimize our IB framework without constructing the lower bound.",0
"The principle of the information bottleneck is a sophisticated and valuable method for acquiring representation skills. In this research, we explore the challenge of representation learning within the context of reinforcement learning, utilizing the information bottleneck framework to enhance the learning algorithm's sample efficiency. Our focus is on expediting the elimination of irrelevant information when input states become exceedingly high-dimensional. We mathematically derive the optimal conditional distribution of the representation and present a variational lower bound. This lower bound is then maximized using the Stein variational gradient method. Our framework is integrated into the advantageous actor critic algorithm and the proximal policy optimization algorithm. Our experimental findings demonstrate that our framework can considerably improve the sample efficiency of vanilla A2C and PPO. Additionally, we examine the information bottleneck perspective in deep RL using the mutual information neural estimation algorithm. We experimentally confirm that the process of information extraction and compression exists in deep RL, and our framework can accelerate it. We also analyze the relationship between MINE and our method and theoretically derive an algorithm for optimizing our IB framework without constructing the lower bound.",1
"Quantum computing exploits basic quantum phenomena such as state superposition and entanglement to perform computations. The Quantum Approximate Optimization Algorithm (QAOA) is arguably one of the leading quantum algorithms that can outperform classical state-of-the-art methods in the near term. QAOA is a hybrid quantum-classical algorithm that combines a parameterized quantum state evolution with a classical optimization routine to approximately solve combinatorial problems. The quality of the solution obtained by QAOA within a fixed budget of calls to the quantum computer depends on the performance of the classical optimization routine used to optimize the variational parameters. In this work, we propose an approach based on reinforcement learning (RL) to train a policy network that can be used to quickly find high-quality variational parameters for unseen combinatorial problem instances. The RL agent is trained on small problem instances which can be simulated on a classical computer, yet the learned RL policy is generalizable and can be used to efficiently solve larger instances. Extensive simulations using the IBM Qiskit Aer quantum circuit simulator demonstrate that our trained RL policy can reduce the optimality gap by a factor up to 8.61 compared with other off-the-shelf optimizers tested.",0
"By utilizing fundamental quantum principles like state superposition and entanglement, quantum computing is capable of performing computations. The Quantum Approximate Optimization Algorithm (QAOA) is considered to be one of the prominent quantum algorithms that can surpass conventional state-of-the-art techniques in the short term. QAOA is a combination of a parameterized quantum state evolution and a classical optimization routine, making it a hybrid quantum-classical algorithm that can solve combinatorial problems approximately. The effectiveness of the QAOA solution within a fixed quantum computer call budget is dependent on the classical optimization routine's performance, which optimizes the variational parameters. This study proposes a reinforcement learning (RL) approach to train a policy network that can efficiently discover high-quality variational parameters for new combinatorial problem instances. The RL agent is trained on small-scale problem instances that can be simulated on a classical computer, but the learned RL policy is transferable and can be used to solve larger instances efficiently. IBM Qiskit Aer quantum circuit simulator extensively tested our trained RL policy, demonstrating that it can decrease the optimality gap by up to 8.61 times compared to other pre-existing optimizers.",1
"In standard reinforcement learning (RL), a learning agent seeks to optimize the overall reward. However, many key aspects of a desired behavior are more naturally expressed as constraints. For instance, the designer may want to limit the use of unsafe actions, increase the diversity of trajectories to enable exploration, or approximate expert trajectories when rewards are sparse. In this paper, we propose an algorithmic scheme that can handle a wide class of constraints in RL tasks: specifically, any constraints that require expected values of some vector measurements (such as the use of an action) to lie in a convex set. This captures previously studied constraints (such as safety and proximity to an expert), but also enables new classes of constraints (such as diversity). Our approach comes with rigorous theoretical guarantees and only relies on the ability to approximately solve standard RL tasks. As a result, it can be easily adapted to work with any model-free or model-based RL. In our experiments, we show that it matches previous algorithms that enforce safety via constraints, but can also enforce new properties that these algorithms do not incorporate, such as diversity.",0
"The traditional goal of reinforcement learning (RL) is for the learning agent to optimize the overall reward. However, certain desired behaviors are better expressed as constraints. Examples include limiting unsafe actions, increasing trajectory diversity for exploration, or approximating expert trajectories when rewards are scarce. This paper introduces an algorithm that can handle a broad range of constraints in RL tasks. Specifically, it can handle constraints that require expected values of certain vector measurements (such as action usage) to be within a convex set. This covers previously studied constraints like safety and expert proximity, as well as new constraints like diversity. The approach is theoretically sound and can be adapted to work with any model-free or model-based RL. Experiments show that it can match previous algorithms that enforce safety via constraints, but can also enforce new properties, such as diversity.",1
"Successfully navigating a complex environment to obtain a desired outcome is a difficult task, that up to recently was believed to be capable only by humans. This perception has been broken down over time, especially with the introduction of deep reinforcement learning, which has greatly increased the difficulty of tasks that can be automated. However, for traditional reinforcement learning agents this requires an environment to be able to provide frequent extrinsic rewards, which are not known or accessible for many real-world environments. This project aims to explore and contrast existing reinforcement learning solutions that circumnavigate the difficulties of an environment that provide sparse rewards. Different reinforcement solutions will be implemented over a several video game environments with varying difficulty and varying frequency of rewards, as to properly investigate the applicability of these solutions. This project introduces a novel reinforcement learning solution by combining aspects of two existing state of the art sparse reward solutions, curiosity driven exploration and unsupervised auxiliary tasks.",0
"The task of successfully achieving a desired outcome in a complex environment has long been thought to be achievable only by humans. However, this perception has been challenged with the introduction of deep reinforcement learning, which has expanded the range of tasks that can be automated. Nonetheless, conventional reinforcement learning relies on frequent extrinsic rewards, which are often absent in many real-world environments. Therefore, this project aims to examine and compare different reinforcement learning approaches that can overcome the challenge of sparse rewards in environments. Multiple video game environments with varying levels of difficulty and reward frequency will be utilized to assess the applicability of these approaches. Additionally, this project proposes a new solution to reinforcement learning that combines elements of the curiosity-driven exploration and unsupervised auxiliary tasks methods.",1
"In this work we investigate on the concept of ""restraining bolt"", envisioned in Science Fiction. Specifically we introduce a novel problem in AI. We have two distinct sets of features extracted from the world, one by the agent and one by the authority imposing restraining specifications (the ""restraining bolt""). The two sets are apparently unrelated since of interest to independent parties, however they both account for (aspects of) the same world. We consider the case in which the agent is a reinforcement learning agent on the first set of features, while the restraining bolt is specified logically using linear time logic on finite traces LTLf/LDLf over the second set of features. We show formally, and illustrate with examples, that, under general circumstances, the agent can learn while shaping its goals to suitably conform (as much as possible) to the restraining bolt specifications.",0
"The focus of our study is the ""restraining bolt"" concept portrayed in Science Fiction. Our research explores a new problem in AI where we examine two distinct sets of features, one extracted by the agent and the other by the authority imposing the specifications for the restraining bolt. Although the two sets appear unrelated, they represent different aspects of the same world. We investigate a scenario where the agent is a reinforcement learning agent operating on the first set of features, while the restraining bolt is specified logically using linear time logic on finite traces LTLf/LDLf over the second set of features. We provide formal evidence and examples that demonstrate how, under typical circumstances, the agent can learn while adjusting its goals to adhere to the restraining bolt specifications as closely as possible.",1
"The capability to learn and adapt to changes in the driving environment is crucial for developing autonomous driving systems that are scalable beyond geo-fenced operational design domains. Deep Reinforcement Learning (RL) provides a promising and scalable framework for developing adaptive learning based solutions. Deep RL methods usually model the problem as a (Partially Observable) Markov Decision Process in which an agent acts in a stationary environment to learn an optimal behavior policy. However, driving involves complex interaction between multiple, intelligent (artificial or human) agents in a highly non-stationary environment. In this paper, we propose the use of Partially Observable Markov Games(POSG) for formulating the connected autonomous driving problems with realistic assumptions. We provide a taxonomy of multi-agent learning environments based on the nature of tasks, nature of agents and the nature of the environment to help in categorizing various autonomous driving problems that can be addressed under the proposed formulation. As our main contributions, we provide MACAD-Gym, a Multi-Agent Connected, Autonomous Driving agent learning platform for furthering research in this direction. Our MACAD-Gym platform provides an extensible set of Connected Autonomous Driving (CAD) simulation environments that enable the research and development of Deep RL- based integrated sensing, perception, planning and control algorithms for CAD systems with unlimited operational design domain under realistic, multi-agent settings. We also share the MACAD-Agents that were trained successfully using the MACAD-Gym platform to learn control policies for multiple vehicle agents in a partially observable, stop-sign controlled, 3-way urban intersection environment with raw (camera) sensor observations.",0
"Developing autonomous driving systems that can operate beyond designated areas requires the ability to learn and adapt to changes in the driving environment. Deep Reinforcement Learning (RL) is a scalable framework for creating adaptive learning solutions, but traditional RL methods do not account for the complex interactions between multiple agents in a non-stationary driving environment. To address this, we propose using Partially Observable Markov Games (POSG) to formulate connected autonomous driving problems. We provide a taxonomy of multi-agent learning environments to categorize various autonomous driving problems and introduce MACAD-Gym, a Multi-Agent Connected, Autonomous Driving agent learning platform. MACAD-Gym provides a set of simulation environments to develop Deep RL-based sensing, perception, planning, and control algorithms for CAD systems under realistic, multi-agent settings. We also share the MACAD-Agents that were successfully trained to learn control policies for multiple vehicle agents in a partially observable, stop-sign controlled, 3-way urban intersection environment using raw sensor observations.",1
"We consider differentially private algorithms for reinforcement learning in continuous spaces, such that neighboring reward functions are indistinguishable. This protects the reward information from being exploited by methods such as inverse reinforcement learning. Existing studies that guarantee differential privacy are not extendable to infinite state spaces, as the noise level to ensure privacy will scale accordingly to infinity. Our aim is to protect the value function approximator, without regard to the number of states queried to the function. It is achieved by adding functional noise to the value function iteratively in the training. We show rigorous privacy guarantees by a series of analyses on the kernel of the noise space, the probabilistic bound of such noise samples, and the composition over the iterations. We gain insight into the utility analysis by proving the algorithm's approximate optimality when the state space is discrete. Experiments corroborate our theoretical findings and show improvement over existing approaches.",0
"Our focus is on differential privacy algorithms for reinforcement learning in continuous spaces, ensuring that similar reward functions cannot be distinguished. This prevents the exploitation of reward information by inverse reinforcement learning and other methods. However, current studies that guarantee differential privacy cannot be extended to infinite state spaces due to the scaling of noise levels. Our goal is to safeguard the value function approximator, regardless of the number of states it is queried for. This is accomplished by iteratively adding functional noise to the value function during training. We provide rigorous privacy guarantees by analyzing the kernel of the noise space, the probabilistic bound of noise samples, and the composition over iterations. In addition, we demonstrate the algorithm's approximate optimality in discrete state spaces through utility analysis. Experimental results confirm our theoretical findings and show improvement over existing methods.",1
"Meta-Reinforcement learning approaches aim to develop learning procedures that can adapt quickly to a distribution of tasks with the help of a few examples. Developing efficient exploration strategies capable of finding the most useful samples becomes critical in such settings. Existing approaches towards finding efficient exploration strategies add auxiliary objectives to promote exploration by the pre-update policy, however, this makes the adaptation using a few gradient steps difficult as the pre-update (exploration) and post-update (exploitation) policies are often quite different. Instead, we propose to explicitly model a separate exploration policy for the task distribution. Having two different policies gives more flexibility in training the exploration policy and also makes adaptation to any specific task easier. We show that using self-supervised or supervised learning objectives for adaptation allows for more efficient inner-loop updates and also demonstrate the superior performance of our model compared to prior works in this domain.",0
"The goal of Meta-Reinforcement learning approaches is to develop learning methods that can quickly adapt to a variety of tasks with minimal examples. In this context, it is crucial to have efficient exploration strategies that can identify the most valuable examples. Existing methods have attempted to promote exploration by adding auxiliary objectives to the pre-update policy. However, this approach can make adapting using few gradient steps challenging, as the pre-update (exploration) and post-update (exploitation) policies are often quite different. Instead, we propose the explicit modeling of a separate exploration policy for the task distribution. This approach provides greater flexibility in training the exploration policy and makes adapting to specific tasks more manageable. We demonstrate the effectiveness of our model, which utilizes self-supervised or supervised learning objectives for adaptation, and outperforms prior works in this field.",1
"Neuroscientific theory suggests that dopaminergic neurons broadcast global reward prediction errors to large areas of the brain influencing the synaptic plasticity of the neurons in those regions. We build on this theory to propose a multi-agent learning framework with spiking neurons in the generalized linear model (GLM) formulation as agents, to solve reinforcement learning (RL) tasks. We show that a network of GLM spiking agents connected in a hierarchical fashion, where each spiking agent modulates its firing policy based on local information and a global prediction error, can learn complex action representations to solve RL tasks. We further show how leveraging principles of modularity and population coding inspired from the brain can help reduce variance in the learning updates making it a viable optimization technique.",0
"According to neuroscientific theory, dopaminergic neurons transmit reward prediction errors to various regions of the brain, which affects the synaptic plasticity of those neurons. We have expanded on this theory by proposing a multi-agent learning framework that uses spiking neurons in the generalized linear model (GLM) formulation as agents to address reinforcement learning (RL) tasks. Our study demonstrates that a hierarchical network of GLM spiking agents, wherein each agent adjusts its firing policy based on local information and a global prediction error, can learn intricate action representations to solve RL tasks. Additionally, we have determined that incorporating principles of modularity and population coding, which are inspired by the brain, can minimize variance in the learning updates, making it a viable optimization technique.",1
"Recent advances in deep reinforcement learning have demonstrated the capability of learning complex control policies from many types of environments. When learning policies for safety-critical applications, it is essential to be sensitive to risks and avoid catastrophic events. Towards this goal, we propose an actor-critic framework that models the uncertainty of the future and simultaneously learns a policy based on that uncertainty model. Specifically, given a distribution of the future return for any state and action, we optimize policies for varying levels of conditional Value-at-Risk. The learned policy can map the same state to different actions depending on the propensity for risk. We demonstrate the effectiveness of our approach in the domain of driving simulations, where we learn maneuvers in two scenarios. Our learned controller can dynamically select actions along a continuous axis, where safe and conservative behaviors are found at one end while riskier behaviors are found at the other. Finally, when testing with very different simulation parameters, our risk-averse policies generalize significantly better compared to other reinforcement learning approaches.",0
"Recent developments in deep reinforcement learning have exhibited its ability to learn intricate control strategies from diverse environments. However, when dealing with safety-critical applications, it is crucial to be mindful of potential hazards and prevent disastrous outcomes. To address this, we suggest an actor-critic framework that takes into account the probability of future events and learns a policy based on this uncertainty model. Specifically, we optimize policies for different levels of conditional Value-at-Risk by considering the distribution of future returns for any given state and action. The learned policy can select different actions for the same state based on the level of risk. We demonstrate the effectiveness of our method in driving simulations, where we train maneuvers in two different scenarios. Our trained controller can choose actions along a continuous spectrum, with safe and cautious actions at one end and riskier actions at the other. Furthermore, our risk-averse policies perform better in generalization when tested with varying simulation parameters than other reinforcement learning techniques.",1
"We introduce TextWorld, a sandbox learning environment for the training and evaluation of RL agents on text-based games. TextWorld is a Python library that handles interactive play-through of text games, as well as backend functions like state tracking and reward assignment. It comes with a curated list of games whose features and challenges we have analyzed. More significantly, it enables users to handcraft or automatically generate new games. Its generative mechanisms give precise control over the difficulty, scope, and language of constructed games, and can be used to relax challenges inherent to commercial text games like partial observability and sparse rewards. By generating sets of varied but similar games, TextWorld can also be used to study generalization and transfer learning. We cast text-based games in the Reinforcement Learning formalism, use our framework to develop a set of benchmark games, and evaluate several baseline agents on this set and the curated list.",0
"TextWorld is a learning environment designed for training and evaluating RL agents on text-based games. This environment is built as a Python library that facilitates interactive gameplay and tracks states and rewards. It features a range of games that have been meticulously analyzed for their features and challenges. Additionally, TextWorld offers the flexibility of creating new games either manually or automatically using its generative mechanisms. The generative mechanisms allow for precise control over the difficulty, scope, and language of games created, and can also overcome challenges commonly found in commercial text games such as sparse rewards and partial observability. TextWorld is also useful for studying generalization and transfer learning by generating sets of similar but varied games. We have cast text-based games in the Reinforcement Learning formalism, developed benchmark games using our framework, and evaluated several baseline agents on both the curated list and benchmark games.",1
"Autonomous reinforcement learning agents, like children, do not have access to predefined goals and reward functions. They must discover potential goals, learn their own reward functions and engage in their own learning trajectory. Children, however, benefit from exposure to language, helping to organize and mediate their thought. We propose LE2 (Language Enhanced Exploration), a learning algorithm leveraging intrinsic motivations and natural language (NL) interactions with a descriptive social partner (SP). Using NL descriptions from the SP, it can learn an NL-conditioned reward function to formulate goals for intrinsically motivated goal exploration and learn a goal-conditioned policy. By exploring, collecting descriptions from the SP and jointly learning the reward function and the policy, the agent grounds NL descriptions into real behavioral goals. From simple goals discovered early to more complex goals discovered by experimenting on simpler ones, our agent autonomously builds its own behavioral repertoire. This naturally occurring curriculum is supplemented by an active learning curriculum resulting from the agent's intrinsic motivations. Experiments are presented with a simulated robotic arm that interacts with several objects including tools.",0
"Like children, autonomous reinforcement learning agents must discover potential goals, learn their own reward functions, and engage in their own learning trajectory, without predefined goals or reward functions. However, children benefit from exposure to language, which helps to organize and mediate their thoughts. To address this, we propose LE2, a learning algorithm that leverages intrinsic motivations and natural language interactions with a descriptive social partner. By using NL descriptions from the social partner, LE2 can learn an NL-conditioned reward function to formulate goals for intrinsically motivated goal exploration and learn a goal-conditioned policy. The agent explores, collects descriptions, and jointly learns both the reward function and policy to ground NL descriptions into real behavioral goals. Our agent autonomously builds its own behavioral repertoire, starting with simple goals and gradually experimenting with more complex ones. This naturally occurring curriculum is supplemented by an active learning curriculum resulting from the agent's intrinsic motivations. We demonstrate the effectiveness of our approach with experiments using a simulated robotic arm that interacts with several objects, including tools.",1
"Proximal policy optimization (PPO) is one of the most popular deep reinforcement learning (RL) methods, achieving state-of-the-art performance across a wide range of challenging tasks. However, as a model-free RL method, the success of PPO relies heavily on the effectiveness of its exploratory policy search. In this paper, we give an in-depth analysis on the exploration behavior of PPO, and show that PPO is prone to suffer from the risk of lack of exploration especially under the case of bad initialization, which may lead to the failure of training or being trapped in bad local optima. To address these issues, we proposed a novel policy optimization method, named Trust Region-Guided PPO (TRGPPO), which adaptively adjusts the clipping range within the trust region. We formally show that this method not only improves the exploration ability within the trust region but enjoys a better performance bound compared to the original PPO as well. Extensive experiments verify the advantage of the proposed method.",0
"PPO is a widely used method in deep reinforcement learning that has demonstrated exceptional performance in various challenging tasks. Nonetheless, being a model-free technique, the success of PPO depends significantly on its exploratory policy search. In this paper, we provide a thorough examination of PPO's exploration behavior and reveal its susceptibility to a lack of exploration, particularly when the initialization is poor, resulting in training failure or being stuck in unfavorable local optima. To address these concerns, we introduce a new policy optimization technique called Trust Region-Guided PPO (TRGPPO), which adjusts the clipping range within the trust region. We prove that this approach not only enhances the exploration ability within the trust region but also yields better performance results than the original PPO. Our extensive experiments verify the effectiveness of the proposed method.",1
"Exposure bias refers to the train-test discrepancy that seemingly arises when an autoregressive generative model uses only ground-truth contexts at training time but generated ones at test time. We separate the contributions of the model and the learning framework to clarify the debate on consequences and review proposed counter-measures. In this light, we argue that generalization is the underlying property to address and propose unconditional generation as its fundamental benchmark. Finally, we combine latent variable modeling with a recent formulation of exploration in reinforcement learning to obtain a rigorous handling of true and generated contexts. Results on language modeling and variational sentence auto-encoding confirm the model's generalization capability.",0
"The concept of exposure bias pertains to the apparent disparity between the training and testing phases of an autoregressive generative model. During training, the model solely relies on authentic contexts, whereas during testing, it employs generated ones. To clarify the discussion on the outcomes and suggest potential remedies, we distinguish between the model and the learning framework's contributions. From this perspective, we posit that generalization is the fundamental characteristic to focus on and suggest unconditional generation as the principal benchmark. Lastly, we combine latent variable modeling with a novel approach to exploration in reinforcement learning to handle authentic and generated contexts with precision. The findings from language modeling and variational sentence auto-encoding support the model's ability to generalize.",1
"In many settings, it is desirable to learn decision-making and control policies through learning or bootstrapping from expert demonstrations. The most common approaches under this Imitation Learning (IL) framework are Behavioural Cloning (BC), and Inverse Reinforcement Learning (IRL). Recent methods for IRL have demonstrated the capacity to learn effective policies with access to a very limited set of demonstrations, a scenario in which BC methods often fail. Unfortunately, due to multiple factors of variation, directly comparing these methods does not provide adequate intuition for understanding this difference in performance. In this work, we present a unified probabilistic perspective on IL algorithms based on divergence minimization. We present $f$-MAX, an $f$-divergence generalization of AIRL [Fu et al., 2018], a state-of-the-art IRL method. $f$-MAX enables us to relate prior IRL methods such as GAIL [Ho & Ermon, 2016] and AIRL [Fu et al., 2018], and understand their algorithmic properties. Through the lens of divergence minimization we tease apart the differences between BC and successful IRL approaches, and empirically evaluate these nuances on simulated high-dimensional continuous control domains. Our findings conclusively identify that IRL's state-marginal matching objective contributes most to its superior performance. Lastly, we apply our new understanding of IL methods to the problem of state-marginal matching, where we demonstrate that in simulated arm pushing environments we can teach agents a diverse range of behaviours using simply hand-specified state distributions and no reward functions or expert demonstrations. For datasets and reproducing results please refer to https://github.com/KamyarGh/rl_swiss/blob/master/reproducing/fmax_paper.md .",0
"Learning decision-making and control policies from expert demonstrations is often desired in various settings. Imitation Learning (IL) is a commonly used framework that includes Behavioural Cloning (BC) and Inverse Reinforcement Learning (IRL) approaches. Although IRL methods have demonstrated their effectiveness with limited demonstrations, BC methods often fail in such scenarios. However, comparing these methods directly is not straightforward due to various factors of variation. This study presents a unified probabilistic perspective on IL algorithms based on divergence minimization. The authors introduce $f$-MAX, a generalization of AIRL that enables the understanding of algorithmic properties of prior IRL methods such as GAIL and AIRL. The study investigates the differences between BC and successful IRL approaches through the lens of divergence minimization and evaluates these nuances on high-dimensional continuous control domains. The results suggest that IRL's state-marginal matching objective contributes most to its superior performance. Finally, the authors demonstrate that they can teach agents a diverse range of behaviours in simulated arm pushing environments using hand-specified state distributions without reward functions or expert demonstrations. Datasets and results can be found at https://github.com/KamyarGh/rl_swiss/blob/master/reproducing/fmax_paper.md.",1
"We introduce a new algorithm for multi-objective reinforcement learning (MORL) with linear preferences, with the goal of enabling few-shot adaptation to new tasks. In MORL, the aim is to learn policies over multiple competing objectives whose relative importance (preferences) is unknown to the agent. While this alleviates dependence on scalar reward design, the expected return of a policy can change significantly with varying preferences, making it challenging to learn a single model to produce optimal policies under different preference conditions. We propose a generalized version of the Bellman equation to learn a single parametric representation for optimal policies over the space of all possible preferences. After an initial learning phase, our agent can execute the optimal policy under any given preference, or automatically infer an underlying preference with very few samples. Experiments across four different domains demonstrate the effectiveness of our approach.",0
"Our new algorithm for multi-objective reinforcement learning (MORL) with linear preferences has been developed to facilitate quick adaptation to new tasks. In MORL, policies are learned for multiple objectives that compete with each other, and the agent is unaware of their relative importance. Although this eliminates the need for scalar reward design, it is difficult to learn a single model that produces the best policies under different preference conditions, as the expected return of a policy can vary greatly. To tackle this issue, we have proposed a generalized version of the Bellman equation that allows the learning of a single parametric representation for optimal policies across all possible preferences. Once trained, our agent can execute the optimal policy under any preference or deduce an underlying preference using a small number of samples. We have conducted experiments in four different domains, and our approach has proven to be highly effective.",1
"The idea of experience sharing between cooperative agents naturally emerges from our understanding of how humans learn. Our evolution as a species is tightly linked to the ability to exchange learned knowledge with one another. It follows that experience sharing (ES) between autonomous and independent agents could become the key to accelerate learning in cooperative multiagent settings. We investigate if randomly selecting experiences to share can increase the performance of deep reinforcement learning agents, and propose three new methods for selecting experiences to accelerate the learning process. Firstly, we introduce Focused ES, which prioritizes unexplored regions of the state space. Secondly, we present Prioritized ES, in which temporal-difference error is used as a measure of priority. Finally, we devise Focused Prioritized ES, which combines both previous approaches. The methods are empirically validated in a control problem. While sharing randomly selected experiences between two Deep Q-Network agents shows no improvement over a single agent baseline, we show that the proposed ES methods can successfully outperform the baseline. In particular, the Focused ES accelerates learning by a factor of 2, reducing by 51% the number of episodes required to complete the task.",0
"Learning through experience sharing among cooperative agents is a natural concept derived from our understanding of human learning, which has evolved with the exchange of knowledge. In multi-agent settings, experience sharing (ES) between independent agents can significantly boost learning. This study explores the effectiveness of randomly selecting experiences for sharing among deep reinforcement learning agents and proposes three novel methods for selecting experiences to expedite the learning process. The first method, Focused ES, prioritizes unexplored areas of the state space, while the second method, Prioritized ES, uses temporal-difference error as a measure of priority. The third method, Focused Prioritized ES, combines both the approaches. We evaluate these methods in a control problem and show that while sharing randomly selected experiences between two Deep Q-Network agents does not enhance performance, the proposed ES methods can significantly outperform the baseline. Specifically, Focused ES accelerates learning by a factor of 2, thereby reducing the number of episodes required to complete the task by 51%.",1
"In this paper, we propose an inverse reinforcement learning method for architecture search (IRLAS), which trains an agent to learn to search network structures that are topologically inspired by human-designed network. Most existing architecture search approaches totally neglect the topological characteristics of architectures, which results in complicated architecture with a high inference latency. Motivated by the fact that human-designed networks are elegant in topology with a fast inference speed, we propose a mirror stimuli function inspired by biological cognition theory to extract the abstract topological knowledge of an expert human-design network (ResNeXt). To avoid raising a too strong prior over the search space, we introduce inverse reinforcement learning to train the mirror stimuli function and exploit it as a heuristic guidance for architecture search, easily generalized to different architecture search algorithms. On CIFAR-10, the best architecture searched by our proposed IRLAS achieves 2.60% error rate. For ImageNet mobile setting, our model achieves a state-of-the-art top-1 accuracy 75.28%, while being 2~4x faster than most auto-generated architectures. A fast version of this model achieves 10% faster than MobileNetV2, while maintaining a higher accuracy.",0
"The purpose of this paper is to introduce a novel method for architecture search called inverse reinforcement learning method for architecture search (IRLAS). This method utilizes a mirror stimuli function, based on biological cognition theory, to extract abstract topological knowledge from a ResNeXt network which serves as an expert human-designed network. Unlike existing architecture search approaches that neglect topological characteristics of architectures and result in complicated architectures with high inference latency, IRLAS is motivated by the elegant topology and fast inference speed of human-designed networks. To avoid being too restrictive in the search space, inverse reinforcement learning is introduced to train the mirror stimuli function, which then serves as heuristic guidance for architecture search that can be easily generalized to different search algorithms. The best architecture found by IRLAS on CIFAR-10 has an error rate of 2.60%, and for ImageNet mobile setting, the model achieved a state-of-the-art top-1 accuracy of 75.28%, while being 2-4 times faster than most auto-generated architectures. Furthermore, a faster version of the model achieves a 10% faster speed than MobileNetV2, while maintaining higher accuracy.",1
"Many reinforcement learning (RL) tasks have specific properties that can be leveraged to modify existing RL algorithms to adapt to those tasks and further improve performance, and a general class of such properties is the multiple reward channel. In those environments the full reward can be decomposed into sub-rewards obtained from different channels. Existing work on reward decomposition either requires prior knowledge of the environment to decompose the full reward, or decomposes reward without prior knowledge but with degraded performance. In this paper, we propose Distributional Reward Decomposition for Reinforcement Learning (DRDRL), a novel reward decomposition algorithm which captures the multiple reward channel structure under distributional setting. Empirically, our method captures the multi-channel structure and discovers meaningful reward decomposition, without any requirements on prior knowledge. Consequently, our agent achieves better performance than existing methods on environments with multiple reward channels.",0
"RL tasks often possess unique features that can be exploited to modify current RL algorithms and enhance performance. One such feature is the multiple reward channel, where the complete reward can be divided into sub-rewards obtained from distinct channels. Prior research on reward decomposition either necessitates prior knowledge of the environment or decomposes reward without prior knowledge but with suboptimal performance. In this study, we present Distributional Reward Decomposition for Reinforcement Learning (DRDRL), a fresh reward decomposition approach that captures the multi-reward channel structure in a distributional environment. Our approach captures the multi-channel structure and identifies meaningful reward decomposition without any prior knowledge requirements. As a result, our agent performs better than existing methods in environments with multiple reward channels.",1
"We present an algorithm for learning an approximate action-value soft Q-function in the relative entropy regularised reinforcement learning setting, for which an optimal improved policy can be recovered in closed form. We use recent advances in normalising flows for parametrising the policy together with a learned value-function; and show how this combination can be used to implicitly represent Q-values of an arbitrary policy in continuous action space. Using simple temporal difference learning on the Q-values then leads to a unified objective for policy and value learning. We show how this approach considerably simplifies standard Actor-Critic off-policy algorithms, removing the need for a policy optimisation step. We perform experiments on a range of established reinforcement learning benchmarks, demonstrating that our approach allows for complex, multimodal policy distributions in continuous action spaces, while keeping the process of sampling from the policy both fast and exact.",0
"Our study introduces an algorithm that can learn an approximate action-value soft Q-function in the context of reinforced learning with relative entropy regularization. This algorithm enables the retrieval of an optimal improved policy through a closed form. We leverage recent advancements in normalizing flows to parametrize the policy, alongside a learned value-function to implicitly represent Q-values of any policy in a continuous action space. By using simple temporal difference learning on the Q-values, we create a unified objective for policy and value learning. Our approach simplifies standard Actor-Critic off-policy algorithms and eliminates the need for a policy optimization step. We conducted experiments on various reinforcement learning benchmarks, which demonstrate the potential for complex, multimodal policy distributions in continuous action spaces, while maintaining fast and accurate sampling from the policy.",1
"DeepRacer is a platform for end-to-end experimentation with RL and can be used to systematically investigate the key challenges in developing intelligent control systems. Using the platform, we demonstrate how a 1/18th scale car can learn to drive autonomously using RL with a monocular camera. It is trained in simulation with no additional tuning in physical world and demonstrates: 1) formulation and solution of a robust reinforcement learning algorithm, 2) narrowing the reality gap through joint perception and dynamics, 3) distributed on-demand compute architecture for training optimal policies, and 4) a robust evaluation method to identify when to stop training. It is the first successful large-scale deployment of deep reinforcement learning on a robotic control agent that uses only raw camera images as observations and a model-free learning method to perform robust path planning. We open source our code and video demo on GitHub: https://git.io/fjxoJ.",0
"DeepRacer serves as a platform for comprehensive experimentation with RL and allows for a methodical examination of the primary obstacles involved in developing intelligent control systems. Our use of the platform showcases how a miniature car, at a scale of 1/18th, can acquire the ability to self-drive through the implementation of RL with the assistance of a single camera. The vehicle is trained solely in a simulated environment without adjustments in the physical world. This demonstration highlights: 1) the development and implementation of a reliable reinforcement learning algorithm, 2) the reduction of the reality gap through a combination of perception and dynamics, 3) a distributed on-demand compute infrastructure for the training of optimal policies, and 4) a dependable evaluation process for determining when to conclude training. This marks the initial large-scale utilization of deep reinforcement learning on a robotic control agent that solely relies on raw camera images as observations and a model-free learning method for effective path planning. Our code and video demo on GitHub have been made available to the public.",1
"Designing effective model-based reinforcement learning algorithms is difficult because the ease of data generation must be weighed against the bias of model-generated data. In this paper, we study the role of model usage in policy optimization both theoretically and empirically. We first formulate and analyze a model-based reinforcement learning algorithm with a guarantee of monotonic improvement at each step. In practice, this analysis is overly pessimistic and suggests that real off-policy data is always preferable to model-generated on-policy data, but we show that an empirical estimate of model generalization can be incorporated into such analysis to justify model usage. Motivated by this analysis, we then demonstrate that a simple procedure of using short model-generated rollouts branched from real data has the benefits of more complicated model-based algorithms without the usual pitfalls. In particular, this approach surpasses the sample efficiency of prior model-based methods, matches the asymptotic performance of the best model-free algorithms, and scales to horizons that cause other model-based methods to fail entirely.",0
"Developing effective reinforcement learning algorithms based on models is a challenging task as it requires balancing the ease of generating data with the bias of model-generated data. The purpose of this study is to examine the role of model usage in policy optimization both theoretically and practically. Our initial focus is on formulating and analyzing a model-based reinforcement learning algorithm that guarantees monotonic improvement at each step. However, this analysis is overly pessimistic, indicating that real off-policy data is always preferable to model-generated on-policy data. Nonetheless, we demonstrate that incorporating an empirical estimate of model generalization into the analysis can justify the use of models. Based on this analysis, we propose a simple approach that utilizes short model-generated rollouts branched from real data. This approach outperforms previous model-based methods in terms of sample efficiency and matches the asymptotic performance of the best model-free algorithms. Moreover, it scales to horizons that cause other model-based methods to fail entirely.",1
"In many real-world reinforcement learning applications, access to the environment is limited to a fixed dataset, instead of direct (online) interaction with the environment. When using this data for either evaluation or training of a new policy, accurate estimates of discounted stationary distribution ratios -- correction terms which quantify the likelihood that the new policy will experience a certain state-action pair normalized by the probability with which the state-action pair appears in the dataset -- can improve accuracy and performance. In this work, we propose an algorithm, DualDICE, for estimating these quantities. In contrast to previous approaches, our algorithm is agnostic to knowledge of the behavior policy (or policies) used to generate the dataset. Furthermore, it eschews any direct use of importance weights, thus avoiding potential optimization instabilities endemic of previous methods. In addition to providing theoretical guarantees, we present an empirical study of our algorithm applied to off-policy policy evaluation and find that our algorithm significantly improves accuracy compared to existing techniques.",0
"Access to the environment is often restricted to a fixed dataset in many real-world reinforcement learning applications, rather than direct interaction with the environment. Accurate estimates of discounted stationary distribution ratios can enhance the precision and efficacy of using this data for evaluating or training a new policy. These ratios measure the probability of a new policy encountering a certain state-action pair, normalized by the probability of the state-action pair appearing in the dataset. Our algorithm, DualDICE, proposes a solution for estimating these ratios. It doesn't require any knowledge of the behavior policy used to generate the dataset, and avoids optimization instabilities that plagued previous methods by disregarding importance weights. We also provide theoretical guarantees and empirical evidence that our algorithm significantly improves accuracy compared to existing techniques when applied to off-policy policy evaluation.",1
"Despite an ever growing literature on reinforcement learning algorithms and applications, much less is known about their statistical inference. In this paper, we investigate the large sample behaviors of the Q-value estimates with closed-form characterizations of the asymptotic variances. This allows us to efficiently construct confidence regions for Q-value and optimal value functions, and to develop policies to minimize their estimation errors. This also leads to a policy exploration strategy that relies on estimating the relative discrepancies among the Q estimates. Numerical experiments show superior performances of our exploration strategy than other benchmark approaches.",0
"Although there is a vast amount of literature regarding the algorithms and applications of reinforcement learning, there is a lack of knowledge regarding their statistical inference. The purpose of this paper is to examine the behaviors of Q-value estimates in large samples and provide closed-form descriptions of their asymptotic variances. By doing so, we can create confidence regions for Q-value and optimal value functions and create policies that reduce estimation errors. Additionally, this strategy allows us to explore policies by estimating the discrepancies among Q estimates. Our numerical experiments demonstrate that our exploration strategy outperforms other benchmark approaches.",1
"$Q$-learning with function approximation is one of the most popular methods in reinforcement learning. Though the idea of using function approximation was proposed at least 60 years ago, even in the simplest setup, i.e, approximating $Q$-functions with linear functions, it is still an open problem on how to design a provably efficient algorithm that learns a near-optimal policy. The key challenges are how to efficiently explore the state space and how to decide when to stop exploring in conjunction with the function approximation scheme.   The current paper presents a provably efficient algorithm for $Q$-learning with linear function approximation. Under certain regularity assumptions, our algorithm, Difference Maximization $Q$-learning (DMQ), combined with linear function approximation, returns a near-optimal policy using a polynomial number of trajectories. Our algorithm introduces a new notion, the Distribution Shift Error Checking (DSEC) oracle. This oracle tests whether there exists a function in the function class that predicts well on a distribution $\mathcal{D}_1$, but predicts poorly on another distribution $\mathcal{D}_2$, where $\mathcal{D}_1$ and $\mathcal{D}_2$ are distributions over states induced by two different exploration policies. For the linear function class, this oracle is equivalent to solving a top eigenvalue problem. We believe our algorithmic insights, especially the DSEC oracle, are also useful in designing and analyzing reinforcement learning algorithms with general function approximation.",0
"One of the most popular techniques in reinforcement learning is to use $Q$-learning with function approximation. Despite the fact that the idea of using function approximation has been around for at least 60 years, designing an algorithm that can efficiently learn a near-optimal policy remains an open problem, even when linear functions are used to approximate $Q$-functions in the simplest setup. The main challenges are determining how to explore the state space in an efficient manner and when to stop exploring while using function approximation. In this paper, we present a provably efficient algorithm called Difference Maximization $Q$-learning (DMQ) that uses linear function approximation. Under certain regularity assumptions, our algorithm returns a near-optimal policy using a polynomial number of trajectories. Our algorithm introduces a new concept called the Distribution Shift Error Checking (DSEC) oracle. This oracle tests whether a function in the function class can accurately predict outcomes on one distribution while failing to do so on another distribution induced by a different exploration policy. For the linear function class, the DSEC oracle is equivalent to solving a top eigenvalue problem. We believe that the insights gained from our algorithm, particularly the DSEC oracle, can be applied to the design and analysis of reinforcement learning algorithms that use general function approximation.",1
"In this paper, we propose a Deep Reinforcement Learning (RL) framework for task arrangement, which is a critical problem for the success of crowdsourcing platforms. Previous works conduct the personalized recommendation of tasks to workers via supervised learning methods. However, the majority of them only consider the benefit of either workers or requesters independently. In addition, they cannot handle the dynamic environment and may produce sub-optimal results. To address these issues, we utilize Deep Q-Network (DQN), an RL-based method combined with a neural network to estimate the expected long-term return of recommending a task. DQN inherently considers the immediate and future reward simultaneously and can be updated in real-time to deal with evolving data and dynamic changes. Furthermore, we design two DQNs that capture the benefit of both workers and requesters and maximize the profit of the platform. To learn value functions in DQN effectively, we also propose novel state representations, carefully design the computation of Q values, and predict transition probabilities and future states. Experiments on synthetic and real datasets demonstrate the superior performance of our framework.",0
"This paper proposes a Deep Reinforcement Learning (RL) approach to address the crucial issue of task arrangement in crowdsourcing platforms. Prior studies have used supervised learning methods to personalize task recommendations to workers, but they only focus on the interests of either the workers or the requesters and fail to adapt to changing environments. To overcome these shortcomings, this paper employs the Deep Q-Network (DQN), an RL-based method that integrates a neural network to predict the expected long-term return of suggesting a task. DQN considers both immediate and future rewards and can be updated in real-time to handle evolving data and dynamic changes. Moreover, this paper introduces two DQNs that maximize the profit of the platform and capture the benefits of both workers and requesters. To improve the learning of value functions in DQN, this paper also proposes innovative state representations, Q value computations, and predictions of transition probabilities and future states. Experimental results on synthetic and real datasets demonstrate the superior performance of this framework.",1
"The aim of multi-task reinforcement learning is two-fold: (1) efficiently learn by training against multiple tasks and (2) quickly adapt, using limited samples, to a variety of new tasks. In this work, the tasks correspond to reward functions for environments with the same (or similar) dynamical models. We propose to learn a dynamical model during the training process and use this model to perform sample-efficient adaptation to new tasks at test time. We use significantly fewer samples by performing policy optimization only in a ""virtual"" environment whose transitions are given by our learned dynamical model. Our algorithm sequentially trains against several tasks. Upon encountering a new task, we first warm-up a policy on our learned dynamical model, which requires no new samples from the environment. We then adapt the dynamical model with samples from this policy in the real environment. We evaluate our approach on several continuous control benchmarks and demonstrate its efficacy over MAML, a state-of-the-art meta-learning algorithm, on these tasks.",0
"Multi-task reinforcement learning has two objectives: (1) to efficiently learn by training against multiple tasks and (2) to quickly adapt to various new tasks using limited samples. The tasks in this study correspond to reward functions in environments with similar or the same dynamical models. We suggest learning a dynamical model during the training process and utilizing it to perform sample-efficient adaptation to new tasks at test time. To use fewer samples, we optimize policy only in a ""virtual"" environment, which has transitions given by our learned dynamical model. Our algorithm trains sequentially against multiple tasks. Upon encountering a new task, we first warm-up a policy on our learned dynamical model, which requires no new samples from the environment. We then adapt the dynamical model with samples from this policy in the real environment. We assess our approach on various continuous control benchmarks and demonstrate its effectiveness over MAML, a leading meta-learning algorithm, for these tasks.",1
"In order to make good decision under uncertainty an agent must learn from observations. To do so, two of the most common frameworks are Contextual Bandits and Markov Decision Processes (MDPs). In this paper, we study whether there exist algorithms for the more general framework (MDP) which automatically provide the best performance bounds for the specific problem at hand without user intervention and without modifying the algorithm. In particular, it is found that a very minor variant of a recently proposed reinforcement learning algorithm for MDPs already matches the best possible regret bound $\tilde O (\sqrt{SAT})$ in the dominant term if deployed on a tabular Contextual Bandit problem despite the agent being agnostic to such setting.",0
"Learning from observations is crucial for making sound decisions in uncertain situations. Contextual Bandits and Markov Decision Processes (MDPs) are two commonly used frameworks for this purpose. This study explores whether algorithms for the more general MDP framework can automatically offer the best performance limits for a given problem without user intervention or algorithm modification. Remarkably, a slight modification of a reinforcement learning algorithm for MDPs already achieves the best possible regret bound of $\tilde O (\sqrt{SAT})$ in the dominant term when applied to a tabular Contextual Bandit problem, even though the agent has no knowledge of the setting.",1
"Two hitherto disconnected threads of research, diverse exploration (DE) and maximum entropy RL have addressed a wide range of problems facing reinforcement learning algorithms via ostensibly distinct mechanisms. In this work, we identify a connection between these two approaches. First, a discriminator-based diversity objective is put forward and connected to commonly used divergence measures. We then extend this objective to the maximum entropy framework and propose an algorithm Maximum Entropy Diverse Exploration (MEDE) which provides a principled method to learn diverse behaviors. A theoretical investigation shows that the set of policies learned by MEDE capture the same modalities as the optimal maximum entropy policy. In effect, the proposed algorithm disentangles the maximum entropy policy into its diverse, constituent policies. Experiments show that MEDE is superior to the state of the art in learning high performing and diverse policies.",0
"Previously unrelated research areas, diverse exploration (DE) and maximum entropy RL, have each independently tackled a variety of challenges faced by reinforcement learning algorithms through seemingly different means. This study aims to establish a connection between these two approaches. We first introduce a diversity objective based on a discriminator and link it to commonly used divergence measures. We then expand this objective to the maximum entropy framework, resulting in the Maximum Entropy Diverse Exploration (MEDE) algorithm, which provides a systematic approach to learning diverse behaviors. Our theoretical analysis confirms that the policies learned by MEDE encompass the same characteristics as the optimal maximum entropy policy. Essentially, our proposed algorithm deconstructs the maximum entropy policy into its various constituent policies. Our experiments demonstrate that MEDE surpasses existing techniques in learning high-performance and diverse policies.",1
"Learning to solve complex goal-oriented tasks with sparse terminal-only rewards often requires an enormous number of samples. In such cases, using a set of expert trajectories could help to learn faster. However, Imitation Learning (IL) via supervised pre-training with these trajectories may not perform as well and generally requires additional finetuning with expert-in-the-loop. In this paper, we propose an approach which uses the expert trajectories and learns to decompose the complex main task into smaller sub-goals. We learn a function which partitions the state-space into sub-goals, which can then be used to design an extrinsic reward function. We follow a strategy where the agent first learns from the trajectories using IL and then switches to Reinforcement Learning (RL) using the identified sub-goals, to alleviate the errors in the IL step. To deal with states which are under-represented by the trajectory set, we also learn a function to modulate the sub-goal predictions. We show that our method is able to solve complex goal-oriented tasks, which other RL, IL or their combinations in literature are not able to solve.",0
"Acquiring the ability to tackle intricate target-driven assignments with limited terminal rewards necessitates an extensive amount of data. To expedite the learning process, incorporating a collection of expert paths may prove useful. However, pre-training through Imitation Learning (IL) using these trajectories may not yield optimal results, and typically requires expert intervention for further refinement. This document presents a new technique that employs the expert trajectories to break down the primary task into smaller sub-tasks. We develop a function that segments the state-space into sub-goals, which can be utilized to devise an extrinsic reward function. Our approach involves the agent learning from the trajectories using IL initially, and then transitioning to Reinforcement Learning (RL) with the identified sub-goals to mitigate any errors in the earlier IL phase. To address states that are under-represented in the trajectory set, we also create a function to adjust the sub-goal predictions. Our method demonstrates its capability to solve challenging target-driven tasks that other literature-based RL, IL, or their amalgamation, cannot achieve.",1
"Recent advances in contextual bandit optimization and reinforcement learning have garnered interest in applying these methods to real-world sequential decision making problems. Real-world applications frequently have constraints with respect to a currently deployed policy. Many of the existing constraint-aware algorithms consider problems with a single objective (the reward) and a constraint on the reward with respect to a baseline policy. However, many important applications involve multiple competing objectives and auxiliary constraints. In this paper, we propose a novel Thompson sampling algorithm for multi-outcome contextual bandit problems with auxiliary constraints. We empirically evaluate our algorithm on a synthetic problem. Lastly, we apply our method to a real world video transcoding problem and provide a practical way for navigating the trade-off between safety and performance using Bayesian optimization.",0
"The interest in implementing contextual bandit optimization and reinforcement learning in real-world sequential decision making problems has increased due to recent advancements. However, real-world applications usually have constraints related to the current policy in place. Most constraint-aware algorithms focus on a single objective, such as the reward, and a constraint on the reward concerning a baseline policy. Although, several significant applications involve multiple objectives and auxiliary constraints. This paper presents a new Thompson sampling algorithm that addresses multi-outcome contextual bandit problems with auxiliary constraints. We test our algorithm on a synthetic problem and evaluate it empirically. Lastly, we use Bayesian optimization to apply our method to a real-world video transcoding problem, providing a practical approach for balancing safety and performance.",1
"Strong worst-case performance bounds for episodic reinforcement learning exist but fortunately in practice RL algorithms perform much better than such bounds would predict. Algorithms and theory that provide strong problem-dependent bounds could help illuminate the key features of what makes a RL problem hard and reduce the barrier to using RL algorithms in practice. As a step towards this we derive an algorithm for finite horizon discrete MDPs and associated analysis that both yields state-of-the art worst-case regret bounds in the dominant terms and yields substantially tighter bounds if the RL environment has small environmental norm, which is a function of the variance of the next-state value functions. An important benefit of our algorithmic is that it does not require apriori knowledge of a bound on the environmental norm. As a result of our analysis, we also help address an open learning theory question~\cite{jiang2018open} about episodic MDPs with a constant upper-bound on the sum of rewards, providing a regret bound with no $H$-dependence in the leading term that scales a polynomial function of the number of episodes.",0
"Although there are strong worst-case performance bounds for episodic reinforcement learning, RL algorithms tend to perform much better than expected in practical applications. However, developing algorithms and theories that provide problem-dependent bounds could aid in identifying the key features that make an RL problem difficult and reduce the obstacles to using RL algorithms in real-world scenarios. With this goal in mind, we have developed an algorithm for finite horizon discrete MDPs and associated analysis that can yield state-of-the-art worst-case regret bounds in the dominant terms. Furthermore, if the RL environment has a small environmental norm, which is a function of the variance of the next-state value functions, our algorithm can provide substantially tighter bounds. A significant advantage of our algorithm is that it does not require prior knowledge of a bound on the environmental norm. Additionally, our analysis addresses an open learning theory question concerning episodic MDPs with a constant upper-bound on the sum of rewards, providing a regret bound with no H-dependence in the leading term that scales as a polynomial function of the number of episodes.",1
"Learning reward functions from data is a promising path towards achieving scalable Reinforcement Learning (RL) for robotics. However, a major challenge in training agents from learned reward models is that the agent can learn to exploit errors in the reward model to achieve high reward behaviors that do not correspond to the intended task. These reward delusions can lead to unintended and even dangerous behaviors. On the other hand, adversarial imitation learning frameworks tend to suffer the opposite problem, where the discriminator learns to trivially distinguish agent and expert behavior, resulting in reward models that produce low reward signal regardless of the input state. In this paper, we connect these two classes of reward learning methods to positive-unlabeled (PU) learning, and we show that by applying a large-scale PU learning algorithm to the reward learning problem, we can address both the reward under- and over-estimation problems simultaneously. Our approach drastically improves both GAIL and supervised reward learning, without any additional assumptions.",0
"Achieving scalable Reinforcement Learning (RL) for robotics through learning reward functions from data is a promising approach. However, training agents from learned reward models poses a major challenge as the agent may exploit errors in the reward model to achieve high reward behaviors that do not align with the intended task, leading to unintended and potentially harmful actions. On the other hand, adversarial imitation learning frameworks suffer from the opposite problem, resulting in low reward models that produce low reward signals regardless of the input state. In this paper, we bridge these two types of reward learning methods through positive-unlabeled (PU) learning and demonstrate that applying a large-scale PU learning algorithm to the reward learning problem can simultaneously address both reward under- and over-estimation issues. Our approach significantly improves both GAIL and supervised reward learning without additional assumptions.",1
"Multi-agent reinforcement learning (MARL) has recently received considerable attention due to its applicability to a wide range of real-world applications. However, achieving efficient communication among agents has always been an overarching problem in MARL. In this work, we propose Variance Based Control (VBC), a simple yet efficient technique to improve communication efficiency in MARL. By limiting the variance of the exchanged messages between agents during the training phase, the noisy component in the messages can be eliminated effectively, while the useful part can be preserved and utilized by the agents for better performance. Our evaluation using a challenging set of StarCraft II benchmarks indicates that our method achieves $2-10\times$ lower in communication overhead than state-of-the-art MARL algorithms, while allowing agents to better collaborate by developing sophisticated strategies.",0
"MARL has become a popular research area lately, due to its adaptability to various real-world scenarios. Nevertheless, one of the major difficulties in MARL is establishing efficient communication among agents. In this study, we suggest a straightforward yet effective approach called Variance Based Control (VBC) to enhance communication efficiency in MARL. By controlling the variance of messages exchanged between agents during the training phase, we can effectively eliminate the noisy component of the messages while retaining the useful part for better performance. Our evaluation on a difficult collection of StarCraft II benchmarks reveals that our method significantly reduces communication overhead (2-10 times lower) compared to the state-of-the-art MARL algorithms. Moreover, it enables agents to collaborate better by developing advanced strategies.",1
"Generative adversarial imitation learning (GAIL) has attracted increasing attention in the field of robot learning. It enables robots to learn a policy to achieve a task demonstrated by an expert while simultaneously estimating the reward function behind the expert's behaviors. However, this framework is limited to learning a single task with a single reward function. This study proposes an extended framework called situated GAIL (S-GAIL), in which a task variable is introduced to both the discriminator and generator of the GAIL framework. The task variable has the roles of discriminating different contexts and making the framework learn different reward functions and policies for multiple tasks. To achieve the early convergence of learning and robustness during reward estimation, we introduce a term to adjust the entropy regularization coefficient in the generator's objective function. Our experiments using two setups (navigation in a discrete grid world and arm reaching in a continuous space) demonstrate that the proposed framework can acquire multiple reward functions and policies more effectively than existing frameworks. The task variable enables our framework to differentiate contexts while sharing common knowledge among multiple tasks.",0
"The concept of generative adversarial imitation learning (GAIL) has garnered attention in the realm of robot learning. It allows robots to learn how to perform a task that an expert has demonstrated while simultaneously estimating the underlying reward function behind the expert's actions. However, GAIL is limited to learning only one task with one reward function. In response, this study introduces a new framework called situated GAIL (S-GAIL), which includes a task variable in both the discriminator and generator of the GAIL framework. The task variable serves to differentiate various contexts and enable the framework to learn multiple reward functions and policies for various tasks. To ensure early convergence of learning and robust reward estimation, we adjust the entropy regularization coefficient in the generator's objective function. Our experiments, which include navigation in a discrete grid world and arm reaching in a continuous space, demonstrate that our proposed framework is more effective at acquiring multiple reward functions and policies than existing frameworks. By leveraging the task variable, our framework can differentiate between contexts while sharing common knowledge among multiple tasks.",1
"Traditional sequence-to-sequence (seq2seq) models and other variations of the attention-mechanism such as hierarchical attention have been applied to the text summarization problem. Though there is a hierarchy in the way humans use language by forming paragraphs from sentences and sentences from words, hierarchical models have usually not worked that much better than their traditional seq2seq counterparts. This effect is mainly because either the hierarchical attention mechanisms are too sparse using hard attention or noisy using soft attention. In this paper, we propose a method based on extracting the highlights of a document; a key concept that is conveyed in a few sentences. In a typical text summarization dataset consisting of documents that are 800 tokens in length (average), capturing long-term dependencies is very important, e.g., the last sentence can be grouped with the first sentence of a document to form a summary. LSTMs (Long Short-Term Memory) proved useful for machine translation. However, they often fail to capture long-term dependencies while modeling long sequences. To address these issues, we have adapted Neural Semantic Encoders (NSE) to text summarization, a class of memory-augmented neural networks by improving its functionalities and proposed a novel hierarchical NSE that outperforms similar previous models significantly. The quality of summarization was improved by augmenting linguistic factors, namely lemma, and Part-of-Speech (PoS) tags, to each word in the dataset for improved vocabulary coverage and generalization. The hierarchical NSE model on factored dataset outperformed the state-of-the-art by nearly 4 ROUGE points. We further designed and used the first GPU-based self-critical Reinforcement Learning model.",0
"Various versions of attention mechanisms, including traditional seq2seq models and hierarchical attention, have been utilized in text summarization. However, hierarchical models have not demonstrated significant improvement over traditional seq2seq models due to sparse hard attention or noisy soft attention. To address these issues, the authors propose a new method based on identifying key concepts in a document through extracting highlights. Capturing long-term dependencies is crucial in text summarization, and LSTMs have been used in the past but struggle with long sequences. To improve upon previous models, the authors adapt Neural Semantic Encoders and introduce a novel hierarchical NSE. Additionally, linguistic factors like lemma and Part-of-Speech (PoS) tags are incorporated to improve vocabulary coverage and generalization. The hierarchical NSE model outperformed the state-of-the-art by nearly 4 ROUGE points, and the authors also designed the first GPU-based self-critical Reinforcement Learning model.",1
"We consider model-based reinforcement learning (MBRL) in 2-agent, high-fidelity continuous control problems -- an important domain for robots interacting with other agents in the same workspace. For non-trivial dynamical systems, MBRL typically suffers from accumulating errors. Several recent studies have addressed this problem by learning latent variable models for trajectory segments and optimizing over behavior in the latent space. In this work, we investigate whether this approach can be extended to 2-agent competitive and cooperative settings. The fundamental challenge is how to learn models that capture interactions between agents, yet are disentangled to allow for optimization of each agent behavior separately. We propose such models based on a disentangled variational auto-encoder, and demonstrate our approach on a simulated 2-robot manipulation task, where one robot can either help or distract the other. We show that our approach has better sample efficiency than a strong model-free RL baseline, and can learn both cooperative and adversarial behavior from the same data.",0
"Our focus is on the use of model-based reinforcement learning (MBRL) in high-fidelity continuous control problems involving two agents, which is a crucial area for robots operating alongside other agents in the same workspace. However, MBRL can encounter difficulties with accumulating errors when dealing with complex dynamic systems. To overcome this issue, recent studies have explored learning latent variable models for trajectory segments and optimizing behavior within the latent space. Our research delves into whether this method can be expanded to encompass 2-agent competitive and cooperative scenarios. The primary challenge is discovering models that can capture the interactions between agents while remaining disentangled to allow for separate optimization of each agent's behavior. We propose models based on a disentangled variational auto-encoder and apply them to a simulated 2-robot manipulation task where one robot can cooperate or hinder the other. Our approach demonstrates better sample efficiency than a strong model-free RL baseline and is capable of learning both cooperative and adversarial behavior from the same data.",1
"Solving goal-oriented tasks is an important but challenging problem in reinforcement learning (RL). For such tasks, the rewards are often sparse, making it difficult to learn a policy effectively. To tackle this difficulty, we propose a new approach called Policy Continuation with Hindsight Inverse Dynamics (PCHID). This approach learns from Hindsight Inverse Dynamics based on Hindsight Experience Replay, enabling the learning process in a self-imitated manner and thus can be trained with supervised learning. This work also extends it to multi-step settings with Policy Continuation. The proposed method is general, which can work in isolation or be combined with other on-policy and off-policy algorithms. On two multi-goal tasks GridWorld and FetchReach, PCHID significantly improves the sample efficiency as well as the final performance.",0
"Reinforcement learning (RL) faces a challenging task of solving goal-oriented tasks where sparse rewards make it difficult to learn an effective policy. To overcome this challenge, we introduce a new approach, Policy Continuation with Hindsight Inverse Dynamics (PCHID), which utilizes Hindsight Inverse Dynamics based on Hindsight Experience Replay to learn in a self-imitated manner. This approach can be trained with supervised learning and extended to multi-step settings with Policy Continuation. PCHID is a general method that can operate independently or in combination with other on-policy and off-policy algorithms. In experiments on GridWorld and FetchReach multi-goal tasks, PCHID significantly improves sample efficiency and final performance.",1
"A model used for velocity control during car following was proposed based on deep reinforcement learning (RL). To fulfil the multi-objectives of car following, a reward function reflecting driving safety, efficiency, and comfort was constructed. With the reward function, the RL agent learns to control vehicle speed in a fashion that maximizes cumulative rewards, through trials and errors in the simulation environment. A total of 1,341 car-following events extracted from the Next Generation Simulation (NGSIM) dataset were used to train the model. Car-following behavior produced by the model were compared with that observed in the empirical NGSIM data, to demonstrate the model's ability to follow a lead vehicle safely, efficiently, and comfortably. Results show that the model demonstrates the capability of safe, efficient, and comfortable velocity control in that it 1) has small percentages (8\%) of dangerous minimum time to collision values (\textless\ 5s) than human drivers in the NGSIM data (35\%); 2) can maintain efficient and safe headways in the range of 1s to 2s; and 3) can follow the lead vehicle comfortably with smooth acceleration. The results indicate that reinforcement learning methods could contribute to the development of autonomous driving systems.",0
"Using deep reinforcement learning (RL), a model was proposed to control velocity during car following. In order to achieve the multi-objectives of car following, a reward function was created that considered driving safety, efficiency, and comfort. Through trial and error in a simulation environment, the RL agent learns to adjust vehicle speed to maximize cumulative rewards. The model was trained using 1,341 car-following events from the Next Generation Simulation (NGSIM) dataset. The car-following behavior generated by the model was compared to that observed in the NGSIM data to demonstrate its ability to safely, efficiently, and comfortably follow a lead vehicle. Results indicate that the model can control velocity in a safe, efficient, and comfortable manner, with a lower percentage (8%) of dangerous minimum time to collision values (\textless\ 5s) than human drivers in the NGSIM data (35%). Additionally, the model can maintain safe and efficient headways in the range of 1s to 2s and follow the lead vehicle comfortably with smooth acceleration. Overall, these results suggest that reinforcement learning methods could be useful in developing autonomous driving systems.",1
"Automatic machine learning (AutoML) is an area of research aimed at automating machine learning (ML) activities that currently require human experts. One of the most challenging tasks in this field is the automatic generation of end-to-end ML pipelines: combining multiple types of ML algorithms into a single architecture used for end-to-end analysis of previously-unseen data. This task has two challenging aspects: the first is the need to explore a large search space of algorithms and pipeline architectures. The second challenge is the computational cost of training and evaluating multiple pipelines. In this study we present DeepLine, a reinforcement learning based approach for automatic pipeline generation. Our proposed approach utilizes an efficient representation of the search space and leverages past knowledge gained from previously-analyzed datasets to make the problem more tractable. Additionally, we propose a novel hierarchical-actions algorithm that serves as a plugin, mediating the environment-agent interaction in deep reinforcement learning problems. The plugin significantly speeds up the training process of our model. Evaluation on 56 datasets shows that DeepLine outperforms state-of-the-art approaches both in accuracy and in computational cost.",0
"AutoML is a field of study focused on automating ML tasks that currently require human expertise. Generating end-to-end ML pipelines, which combine multiple ML algorithms into a single architecture for analyzing previously-unseen data, is a particularly challenging task due to the need to explore a large search space of algorithms and pipeline architectures and the high computational cost of training and evaluating multiple pipelines. In this research, we introduce DeepLine, a reinforcement learning-based approach that efficiently represents the search space and leverages past knowledge to make the task more manageable. Our proposed hierarchical-actions algorithm serves as a plugin that speeds up the model's training process. We evaluated DeepLine on 56 datasets and found that it outperforms state-of-the-art methods in both accuracy and computational cost.",1
"Sample efficiency and scalability to a large number of agents are two important goals for multi-agent reinforcement learning systems. Recent works got us closer to those goals, addressing non-stationarity of the environment from a single agent's perspective by utilizing a deep net critic which depends on all observations and actions. The critic input concatenates agent observations and actions in a user-specified order. However, since deep nets aren't permutation invariant, a permuted input changes the critic output despite the environment remaining identical. To avoid this inefficiency, we propose a 'permutation invariant critic' (PIC), which yields identical output irrespective of the agent permutation. This consistent representation enables our model to scale to 30 times more agents and to achieve improvements of test episode reward between 15% to 50% on the challenging multi-agent particle environment (MPE).",0
"Achieving sample efficiency and scalability to a large number of agents are crucial objectives for multi-agent reinforcement learning systems. Recent studies have made progress towards these goals by addressing non-stationarity in the environment from a single agent's perspective using a deep net critic that depends on all observations and actions. However, since deep nets are not permutation invariant, a permuted input can alter the critic output even though the environment remains unchanged. To overcome this inefficiency, we propose a 'permutation invariant critic' (PIC) that produces identical outputs regardless of the agent permutation. This consistent representation allows our model to scale up to 30 times more agents and improve test episode rewards by 15% to 50% on the challenging multi-agent particle environment (MPE).",1
"Continual learning aims to improve the ability of modern learning systems to deal with non-stationary distributions, typically by attempting to learn a series of tasks sequentially. Prior art in the field has largely considered supervised or reinforcement learning tasks, and often assumes full knowledge of task labels and boundaries. In this work, we propose an approach (CURL) to tackle a more general problem that we will refer to as unsupervised continual learning. The focus is on learning representations without any knowledge about task identity, and we explore scenarios when there are abrupt changes between tasks, smooth transitions from one task to another, or even when the data is shuffled. The proposed approach performs task inference directly within the model, is able to dynamically expand to capture new concepts over its lifetime, and incorporates additional rehearsal-based techniques to deal with catastrophic forgetting. We demonstrate the efficacy of CURL in an unsupervised learning setting with MNIST and Omniglot, where the lack of labels ensures no information is leaked about the task. Further, we demonstrate strong performance compared to prior art in an i.i.d setting, or when adapting the technique to supervised tasks such as incremental class learning.",0
"The goal of continual learning is to enhance the capability of contemporary learning systems to handle non-stationary distributions, typically by learning a sequence of tasks one after the other. Previous research in the field has primarily focused on supervised or reinforcement learning tasks, assuming complete knowledge of task labels and boundaries. In this study, we introduce a novel approach (CURL) that addresses a more broad issue, referred to as unsupervised continual learning. The emphasis is on acquiring representations without any prior knowledge of task identity, while accommodating abrupt task changes, smooth transitions, or shuffled data scenarios. The proposed technique conducts task inference within the model and can dynamically expand to incorporate new concepts over time. Additionally, rehearsal-based techniques are employed to tackle catastrophic forgetting. We demonstrate the effectiveness of CURL on unsupervised learning with MNIST and Omniglot datasets, where no information about the task is leaked due to the absence of labels. Moreover, we show superior performance compared to prior research in an i.i.d setting and when adapting the method for supervised tasks such as incremental class learning.",1
"Fairness is essential for human society, contributing to stability and productivity. Similarly, fairness is also the key for many multi-agent systems. Taking fairness into multi-agent learning could help multi-agent systems become both efficient and stable. However, learning efficiency and fairness simultaneously is a complex, multi-objective, joint-policy optimization. To tackle these difficulties, we propose FEN, a novel hierarchical reinforcement learning model. We first decompose fairness for each agent and propose fair-efficient reward that each agent learns its own policy to optimize. To avoid multi-objective conflict, we design a hierarchy consisting of a controller and several sub-policies, where the controller maximizes the fair-efficient reward by switching among the sub-policies that provides diverse behaviors to interact with the environment. FEN can be trained in a fully decentralized way, making it easy to be deployed in real-world applications. Empirically, we show that FEN easily learns both fairness and efficiency and significantly outperforms baselines in a variety of multi-agent scenarios.",0
"The importance of fairness cannot be overstated, as it is crucial for promoting stability and productivity in human society. Additionally, fairness is also a critical factor for the success of many multi-agent systems. By incorporating fairness into multi-agent learning, these systems can achieve both efficiency and stability. However, achieving both fairness and efficiency simultaneously poses a challenge, as it requires complex joint-policy optimization. To address this issue, we propose a novel hierarchical reinforcement learning model called FEN. Our approach involves breaking down fairness for each agent and creating a fair-efficient reward system that enables each agent to optimize its own policy. To avoid conflicting objectives, we develop a hierarchy with a controller and multiple sub-policies. The controller maximizes the fair-efficient reward by switching between sub-policies that offer different behaviors for interacting with the environment. FEN can be trained in a fully decentralized manner, making it easy to implement in real-world applications. Our empirical results show that FEN learns fairness and efficiency effectively and outperforms other methods in various multi-agent scenarios.",1
"State-of-the-art efficient model-based Reinforcement Learning (RL) algorithms typically act by iteratively solving empirical models, i.e., by performing \emph{full-planning} on Markov Decision Processes (MDPs) built by the gathered experience. In this paper, we focus on model-based RL in the finite-state finite-horizon MDP setting and establish that exploring with \emph{greedy policies} -- act by \emph{1-step planning} -- can achieve tight minimax performance in terms of regret, $\tilde{\mathcal{O}}(\sqrt{HSAT})$. Thus, full-planning in model-based RL can be avoided altogether without any performance degradation, and, by doing so, the computational complexity decreases by a factor of $S$. The results are based on a novel analysis of real-time dynamic programming, then extended to model-based RL. Specifically, we generalize existing algorithms that perform full-planning to such that act by 1-step planning. For these generalizations, we prove regret bounds with the same rate as their full-planning counterparts.",0
"Cutting-edge model-based Reinforcement Learning (RL) algorithms operate by repeatedly solving empirical models, using \emph{full-planning} on Markov Decision Processes (MDPs) created from gathered experience. In this work, we concentrate on model-based RL in the finite-state finite-horizon MDP environment, demonstrating that by using \emph{greedy policies} and \emph{1-step planning}, we can achieve a tight minimax performance in regret, at $\tilde{\mathcal{O}}(\sqrt{HSAT})$. Thus, it is possible to avoid full-planning altogether in model-based RL without affecting performance, and this approach reduces computational complexity by a factor of $S$. Our results are based on a new analysis of real-time dynamic programming, which we expand to model-based RL. We generalize existing algorithms that use full-planning to those that use 1-step planning, and show that these generalizations have regret bounds with the same rate as their full-planning equivalents.",1
"We present an efficient algorithm for model-free episodic reinforcement learning on large (potentially continuous) state-action spaces. Our algorithm is based on a novel $Q$-learning policy with adaptive data-driven discretization. The central idea is to maintain a finer partition of the state-action space in regions which are frequently visited in historical trajectories, and have higher payoff estimates. We demonstrate how our adaptive partitions take advantage of the shape of the optimal $Q$-function and the joint space, without sacrificing the worst-case performance. In particular, we recover the regret guarantees of prior algorithms for continuous state-action spaces, which additionally require either an optimal discretization as input, and/or access to a simulation oracle. Moreover, experiments demonstrate how our algorithm automatically adapts to the underlying structure of the problem, resulting in much better performance compared both to heuristics and $Q$-learning with uniform discretization.",0
"Our algorithm for model-free episodic reinforcement learning on large state-action spaces is highly effective. It is based on an innovative $Q$-learning policy that uses data-driven discretization. The key concept is to maintain a more detailed partition of the state-action space in regions that are frequently visited and have higher payoff estimates. Our adaptive partitions take advantage of the optimal $Q$-function and the joint space without sacrificing worst-case performance. We have achieved regret guarantees similar to prior algorithms for continuous state-action spaces, which require optimal discretization or access to a simulation oracle. Experiments have shown that our algorithm automatically adjusts to the problem's underlying structure, resulting in better performance than heuristics and $Q$-learning with uniform discretization.",1
"Exploration in environments with continuous control and sparse rewards remains a key challenge in reinforcement learning (RL). Recently, surprise has been used as an intrinsic reward that encourages systematic and efficient exploration. We introduce a new definition of surprise and its RL implementation named Variational Assorted Surprise Exploration (VASE). VASE uses a Bayesian neural network as a model of the environment dynamics and is trained using variational inference, alternately updating the accuracy of the agent's model and policy. Our experiments show that in continuous control sparse reward environments VASE outperforms other surprise-based exploration techniques.",0
"Reinforcement learning (RL) faces a significant obstacle in the form of exploration in environments with continuous control and sparse rewards. To tackle this issue, surprise has been employed as an intrinsic reward to promote methodical and effective exploration. Our study introduces a novel definition of surprise and its RL application, known as Variational Assorted Surprise Exploration (VASE). VASE utilizes a Bayesian neural network to model environment dynamics and is trained using variational inference, with periodic updates of the agent's model and policy accuracy. Our findings reveal that VASE surpasses other surprise-based exploration approaches in environments with sparse rewards and continuous control.",1
"We explore the impact of learning paradigms on training deep neural networks for the Travelling Salesman Problem. We design controlled experiments to train supervised learning (SL) and reinforcement learning (RL) models on fixed graph sizes up to 100 nodes, and evaluate them on variable sized graphs up to 500 nodes. Beyond not needing labelled data, our results reveal favorable properties of RL over SL: RL training leads to better emergent generalization to variable graph sizes and is a key component for learning scale-invariant solvers for novel combinatorial problems.",0
"Our study delves into the effects that learning paradigms have on the training of deep neural networks for the Travelling Salesman Problem. We conduct systematic tests to train supervised learning (SL) and reinforcement learning (RL) models on fixed graphs with a maximum of 100 nodes, and assess their performance on graphs with varying sizes up to 500 nodes. Our findings demonstrate that RL presents more advantageous characteristics than SL, not requiring labeled data and leading to superior emergent generalization to variable graph sizes. Furthermore, RL is an essential element in constructing solvers that are scale-invariant for new combinatorial problems.",1
"User identity linkage is a task of recognizing the identities of the same user across different social networks (SN). Previous works tackle this problem via estimating the pairwise similarity between identities from different SN, predicting the label of identity pairs or selecting the most relevant identity pair based on the similarity scores. However, most of these methods ignore the results of previously matched identities, which could contribute to the linkage in following matching steps. To address this problem, we convert user identity linkage into a sequence decision problem and propose a reinforcement learning model to optimize the linkage strategy from the global perspective. Our method makes full use of both the social network structure and the history matched identities, and explores the long-term influence of current matching on subsequent decisions. We conduct experiments on different types of datasets, the results show that our method achieves better performance than other state-of-the-art methods.",0
"Recognizing the same user across different social networks is the main objective of user identity linkage. In the past, this task was achieved by estimating similarity between identities from different SN, predicting identity pair labels, or selecting the most relevant pair based on similarity scores. However, these methods overlooked the importance of previously matched identities, which could aid in subsequent matching steps. To address this issue, we propose a reinforcement learning model that transforms user identity linkage into a sequence decision problem. Our approach utilizes both the social network structure and the history of matched identities to optimize the linkage strategy from a global perspective. Furthermore, we explore the long-term impact of current matching on future decisions. Our experimental results on different datasets demonstrate that our method outperforms other state-of-the-art methods.",1
"We study a security threat to batch reinforcement learning and control where the attacker aims to poison the learned policy. The victim is a reinforcement learner / controller which first estimates the dynamics and the rewards from a batch data set, and then solves for the optimal policy with respect to the estimates. The attacker can modify the data set slightly before learning happens, and wants to force the learner into learning a target policy chosen by the attacker. We present a unified framework for solving batch policy poisoning attacks, and instantiate the attack on two standard victims: tabular certainty equivalence learner in reinforcement learning and linear quadratic regulator in control. We show that both instantiation result in a convex optimization problem on which global optimality is guaranteed, and provide analysis on attack feasibility and attack cost. Experiments show the effectiveness of policy poisoning attacks.",0
"In our research, we investigate a security threat to batch reinforcement learning and control. The focus is on an attacker who intends to corrupt the policy learned by the victim. The victim in this scenario is a reinforcement learner/controller that estimates the dynamics and rewards from a batch data set and then derives the optimal policy based on these estimates. The attacker can modify the data set slightly before the learning process, with the aim of forcing the victim to learn a policy of their choice. To solve the problem of batch policy poisoning attacks, we propose a unified framework. We apply this framework to two standard victims, namely the tabular certainty equivalence learner in reinforcement learning and the linear quadratic regulator in control. Both cases result in a convex optimization problem, where global optimality is guaranteed. We provide an analysis of the attack feasibility and cost and demonstrate the effectiveness of policy poisoning attacks through experiments.",1
"We propose a new approach to visualize saliency maps for deep neural network models and apply it to deep reinforcement learning agents trained on Atari environments. Our method adds an attention module that we call FLS (Free Lunch Saliency) to the feature extractor from an established baseline (Mnih et al., 2015). This addition results in a trainable model that can produce saliency maps, i.e., visualizations of the importance of different parts of the input for the agent's current decision making. We show experimentally that a network with an FLS module exhibits performance similar to the baseline (i.e., it is ""free"", with no performance cost) and can be used as a drop-in replacement for reinforcement learning agents. We also design another feature extractor that scores slightly lower but provides higher-fidelity visualizations. In addition to attained scores, we report saliency metrics evaluated on the Atari-HEAD dataset of human gameplay.",0
"A novel technique for displaying saliency maps of deep neural network models has been proposed and demonstrated on deep reinforcement learning agents trained on Atari environments. The approach involves introducing an attention module, named FLS (Free Lunch Saliency), to the feature extractor of an established baseline (Mnih et al., 2015). This enables the creation of saliency maps that illustrate the significance of different input parts in the agent's decision-making process. Experimental results show that the FLS module does not impact performance and can replace reinforcement learning agents without any additional cost. Furthermore, an alternative feature extractor that produces more detailed visualizations but yields slightly lower scores has been developed. Along with achieved scores, saliency metrics evaluated on the Atari-HEAD dataset of human gameplay are also reported.",1
"The growth of autonomous vehicles, ridesharing systems, and self driving technology will bring a shift in the way ride hailing platforms plan out their services. However, these advances in technology coupled with road congestion, environmental concerns, fuel usage, vehicles emissions, and the high cost of the vehicle usage have brought more attention to better utilize the use of vehicles and their capacities. In this paper, we propose a novel multi-hop ride-sharing (MHRS) algorithm that uses deep reinforcement learning to learn optimal vehicle dispatch and matching decisions by interacting with the external environment. By allowing customers to transfer between vehicles, i.e., ride with one vehicle for sometime and then transfer to another one, MHRS helps in attaining 30\% lower cost and 20\% more efficient utilization of fleets, as compared to the ride-sharing algorithms. This flexibility of multi-hop feature gives a seamless experience to customers and ride-sharing companies, and thus improves ride-sharing services.",0
"The development of self-driving technology, autonomous vehicles, and ridesharing systems will result in a transformation of the way ride-hailing companies organize their services. Despite the potential benefits of these technological advancements, issues such as traffic congestion, environmental concerns, fuel consumption, vehicle emissions, and high costs have highlighted the need for more efficient vehicle usage. This research proposes a new algorithm, called multi-hop ride-sharing (MHRS), which utilizes deep reinforcement learning to optimize vehicle dispatch and matching decisions. By enabling customers to transfer between vehicles, MHRS can decrease costs by 30% and improve fleet utilization by 20% compared to traditional ride-sharing algorithms. The multi-hop feature of MHRS provides a seamless experience for customers and ride-sharing companies, ultimately enhancing ride-sharing services.",1
"Motivated by recent advancements in Deep Reinforcement Learning (RL), we have developed an RL agent to manage the operation of storage devices in a household and is designed to maximize demand-side cost savings. The proposed technique is data-driven, and the RL agent learns from scratch how to efficiently use the energy storage device given variable tariff structures. In most of the studies, the RL agent is considered as a black box, and how the agent has learned is often ignored. We explain the learning progression of the RL agent, and the strategies it follows based on the capacity of the storage device.",0
"Our development of an RL agent for household storage device management is inspired by recent advancements in Deep Reinforcement Learning. The focus is on maximizing cost savings on the demand side, and our proposed technique is data-driven. The RL agent is trained from scratch to effectively utilize the energy storage device under variable tariff structures. Typically, the RL agent is treated as a black box in most studies, with little attention paid to how it learned. However, we provide insight into the learning progression of the RL agent and its strategies, which are based on the storage device's capacity.",1
"We introduce a framework for dynamic adversarial discovery of information (DADI), motivated by a scenario where information (a feature set) is used by third parties with unknown objectives. We train a reinforcement learning agent to sequentially acquire a subset of the information while balancing accuracy and fairness of predictors downstream. Based on the set of already acquired features, the agent decides dynamically to either collect more information from the set of available features or to stop and predict using the information that is currently available. Building on previous work exploring adversarial representation learning, we attain group fairness (demographic parity) by rewarding the agent with the adversary's loss, computed over the final feature set. Importantly, however, the framework provides a more general starting point for fair or private dynamic information discovery. Finally, we demonstrate empirically, using two real-world datasets, that we can trade-off fairness and predictive performance",0
"Our framework, called Dynamic Adversarial Discovery of Information (DADI), is designed to address the challenge of third parties using feature sets with unknown objectives. The framework trains a reinforcement learning agent to acquire a subset of the information while ensuring downstream predictors remain accurate and fair. The agent decides whether to collect more information based on the already acquired features or to stop and use the available information for prediction. We achieve group fairness by rewarding the agent with the adversary's loss computed over the final feature set. This framework is a general starting point for fair or private dynamic information discovery. We demonstrate the effectiveness of our approach on two real-world datasets, showing that it allows for a trade-off between fairness and predictive performance.",1
"Recent policy optimization approaches have achieved substantial empirical success by constructing surrogate optimization objectives. The Approximate Policy Iteration objective (Schulman et al., 2015a; Kakade and Langford, 2002) has become a standard optimization target for reinforcement learning problems. Using this objective in practice requires an estimator of the advantage function. Policy optimization methods such as those proposed in Schulman et al. (2015b) estimate the advantages using a parametric critic. In this work we establish conditions under which the parametric approximation of the critic does not introduce bias to the updates of surrogate objective. These results hold for a general class of parametric policies, including deep neural networks. We obtain a result analogous to the compatible features derived for the original Policy Gradient Theorem (Sutton et al., 1999). As a result, we also identify a previously unknown bias that current state-of-the-art policy optimization algorithms (Schulman et al., 2015a, 2017) have introduced by not employing these compatible features.",0
"Constructing surrogate optimization objectives has led to significant empirical success in recent policy optimization approaches. The Approximate Policy Iteration objective, which has become a standard target for reinforcement learning problems, requires an advantage function estimator for practical use. Parametric critics are used in policy optimization methods like those proposed in Schulman et al. (2015b) to estimate advantages. We establish conditions for when the parametric approximation of the critic does not introduce bias to surrogate objective updates. These conditions apply to a general class of parametric policies, including deep neural networks, and are analogous to the compatible features derived for the original Policy Gradient Theorem. We identify a previously unknown bias that results from current state-of-the-art policy optimization algorithms (Schulman et al., 2015a, 2017) not employing these compatible features.",1
"Search-based methods for hard combinatorial optimization are often guided by heuristics. Tuning heuristics in various conditions and situations is often time-consuming. In this paper, we propose NeuRewriter that learns a policy to pick heuristics and rewrite the local components of the current solution to iteratively improve it until convergence. The policy factorizes into a region-picking and a rule-picking component, each parameterized by a neural network trained with actor-critic methods in reinforcement learning. NeuRewriter captures the general structure of combinatorial problems and shows strong performance in three versatile tasks: expression simplification, online job scheduling and vehicle routing problems. NeuRewriter outperforms the expression simplification component in Z3; outperforms DeepRM and Google OR-tools in online job scheduling; and outperforms recent neural baselines and Google OR-tools in vehicle routing problems.",0
"Heuristic-guided search-based methods are commonly utilized for solving complex combinatorial optimization problems. However, modifying heuristics to suit diverse conditions can be a tedious process. This study introduces NeuRewriter, which employs a policy to select appropriate heuristics and rewrites the local elements of the current solution to improve it gradually until it reaches convergence. The policy is divided into two components: region-picking and rule-picking, and is represented by neural networks trained with actor-critic algorithms in reinforcement learning. NeuRewriter has demonstrated remarkable results in three versatile tasks: expression simplification, online job scheduling, and vehicle routing problems by capturing the fundamental structure of combinatorial problems. It outperforms Z3's expression simplification, DeepRM, and Google OR-tools in online job scheduling, and recent neural baselines and Google OR-tools in vehicle routing problems.",1
"Model-agnostic meta-learners aim to acquire meta-learned parameters from similar tasks to adapt to novel tasks from the same distribution with few gradient updates. With the flexibility in the choice of models, those frameworks demonstrate appealing performance on a variety of domains such as few-shot image classification and reinforcement learning. However, one important limitation of such frameworks is that they seek a common initialization shared across the entire task distribution, substantially limiting the diversity of the task distributions that they are able to learn from. In this paper, we augment MAML with the capability to identify the mode of tasks sampled from a multimodal task distribution and adapt quickly through gradient updates. Specifically, we propose a multimodal MAML (MMAML) framework, which is able to modulate its meta-learned prior parameters according to the identified mode, allowing more efficient fast adaptation. We evaluate the proposed model on a diverse set of few-shot learning tasks, including regression, image classification, and reinforcement learning. The results not only demonstrate the effectiveness of our model in modulating the meta-learned prior in response to the characteristics of tasks but also show that training on a multimodal distribution can produce an improvement over unimodal training.",0
"The objective of model-agnostic meta-learners is to obtain meta-learned parameters from similar tasks so that they can adapt to new tasks from the same distribution with minimal gradient updates. Due to the flexibility in selecting models, these frameworks have promising performance in various domains like few-shot image classification and reinforcement learning. Nevertheless, a significant limitation is that they search for a common initialization shared across all tasks, which considerably restricts the range of task distributions they can learn from. This research proposes a solution to this limitation by introducing the capability to identify the mode of tasks selected from a multimodal task distribution and adapt swiftly through gradient updates. A multimodal MAML (MMAML) framework is introduced, which can modulate its meta-learned prior parameters based on the identified mode, allowing for faster adaptation. The proposed model is evaluated on a variety of few-shot learning tasks, including regression, image classification, and reinforcement learning. The results exhibit the effectiveness of the model in adjusting the meta-learned prior in response to task characteristics, and also indicate that training on a multimodal distribution can lead to an enhancement over unimodal training.",1
"Autonomous agents must often deal with conflicting requirements, such as completing tasks using the least amount of time/energy, learning multiple tasks, or dealing with multiple opponents. In the context of reinforcement learning~(RL), these problems are addressed by (i)~designing a reward function that simultaneously describes all requirements or (ii)~combining modular value functions that encode them individually. Though effective, these methods have critical downsides. Designing good reward functions that balance different objectives is challenging, especially as the number of objectives grows. Moreover, implicit interference between goals may lead to performance plateaus as they compete for resources, particularly when training on-policy. Similarly, selecting parameters to combine value functions is at least as hard as designing an all-encompassing reward, given that the effect of their values on the overall policy is not straightforward. The later is generally addressed by formulating the conflicting requirements as a constrained RL problem and solved using Primal-Dual methods. These algorithms are in general not guaranteed to converge to the optimal solution since the problem is not convex. This work provides theoretical support to these approaches by establishing that despite its non-convexity, this problem has zero duality gap, i.e., it can be solved exactly in the dual domain, where it becomes convex. Finally, we show this result basically holds if the policy is described by a good parametrization~(e.g., neural networks) and we connect this result with primal-dual algorithms present in the literature and we establish the convergence to the optimal solution.",0
"Autonomous agents frequently encounter conflicting requirements, such as completing tasks quickly and efficiently, learning multiple tasks, or dealing with multiple opponents. In reinforcement learning, these issues can be addressed by either designing a reward function that encompasses all requirements or combining modular value functions that encode each requirement individually. However, these methods have drawbacks. Creating a well-balanced reward function for multiple objectives is challenging, and implicit interference between goals can lead to performance plateaus. Selecting parameters for combining value functions is equally challenging as it affects the overall policy. To address these issues, the conflicting requirements can be formulated as a constrained RL problem and solved using Primal-Dual methods, though convergence to the optimal solution is not guaranteed due to the non-convexity of the problem. This work establishes that despite its non-convexity, the problem has a zero duality gap and can be solved exactly in the dual domain, which is convex. Theoretical support is provided for these approaches, and the connection between primal-dual algorithms and good parametrization of the policy, such as neural networks, is established. Convergence to the optimal solution is also demonstrated.",1
"Inverse reinforcement learning (IRL) enables an agent to learn complex behavior by observing demonstrations from a (near-)optimal policy. The typical assumption is that the learner's goal is to match the teacher's demonstrated behavior. In this paper, we consider the setting where the learner has its own preferences that it additionally takes into consideration. These preferences can for example capture behavioral biases, mismatched worldviews, or physical constraints. We study two teaching approaches: learner-agnostic teaching, where the teacher provides demonstrations from an optimal policy ignoring the learner's preferences, and learner-aware teaching, where the teacher accounts for the learner's preferences. We design learner-aware teaching algorithms and show that significant performance improvements can be achieved over learner-agnostic teaching.",0
"The concept of Inverse Reinforcement Learning (IRL) allows an agent to acquire intricate behavior by observing demonstrations from a policy that is near-optimal. The traditional assumption is that the learner aims to emulate the teacher's demonstrated behavior. However, this study considers a scenario where the learner has its own inclinations that it takes into account. These inclinations can incorporate behavioral biases, conflicting worldviews, or physical limitations. The study evaluates two teaching methods: learner-agnostic teaching, where the teacher provides demonstrations from an optimal policy neglecting the learner's preferences, and learner-aware teaching, where the teacher factors in the learner's preferences. The study designs learner-aware teaching algorithms and proves that they produce significant enhancements in performance compared to learner-agnostic teaching.",1
"Deep reinforcement learning algorithms have recently been used to train multiple interacting agents in a centralised manner whilst keeping their execution decentralised. When the agents can only acquire partial observations and are faced with tasks requiring coordination and synchronisation skills, inter-agent communication plays an essential role. In this work, we propose a framework for multi-agent training using deep deterministic policy gradients that enables concurrent, end-to-end learning of an explicit communication protocol through a memory device. During training, the agents learn to perform read and write operations enabling them to infer a shared representation of the world. We empirically demonstrate that concurrent learning of the communication device and individual policies can improve inter-agent coordination and performance in small-scale systems. Our experimental results show that the proposed method achieves superior performance in scenarios with up to six agents. We illustrate how different communication patterns can emerge on six different tasks of increasing complexity. Furthermore, we study the effects of corrupting the communication channel, provide a visualisation of the time-varying memory content as the underlying task is being solved and validate the building blocks of the proposed memory device through ablation studies.",0
"Recently, deep reinforcement learning algorithms have been employed to train multiple agents in a centralised manner while maintaining their decentralised execution. When these agents are faced with tasks requiring coordination and synchronisation skills and can only obtain partial observations, inter-agent communication becomes essential. This study introduces a framework for multi-agent training using deep deterministic policy gradients that allows for concurrent, end-to-end learning of an explicit communication protocol via a memory device. The agents learn to perform read and write operations, which enables them to deduce a shared representation of the world. The proposed method enhances inter-agent coordination and performance in small-scale systems by concurrently learning the communication device and individual policies. Empirical data indicates that the proposed approach performs better in scenarios with up to six agents, and communication patterns emerge in six tasks of increasing complexity. The research also examines the effects of corrupting the communication channel, provides a visualisation of the time-varying memory content as the underlying task is being solved, and validates the building blocks of the proposed memory device through ablation studies.",1
"Dynamic Programming (DP) provides standard algorithms to solve Markov Decision Processes. However, these algorithms generally do not optimize a scalar objective function. In this paper, we draw connections between DP and (constrained) convex optimization. Specifically, we show clear links in the algorithmic structure between three DP schemes and optimization algorithms. We link Conservative Policy Iteration to Frank-Wolfe, Mirror-Descent Modified Policy Iteration to Mirror Descent, and Politex (Policy Iteration Using Expert Prediction) to Dual Averaging. These abstract DP schemes are representative of a number of (deep) Reinforcement Learning (RL) algorithms. By highlighting these connections (most of which have been noticed earlier, but in a scattered way), we would like to encourage further studies linking RL and convex optimization, that could lead to the design of new, more efficient, and better understood RL algorithms.",0
"Markov Decision Processes can be solved using Dynamic Programming (DP) algorithms, but these typically do not optimize a scalar objective function. This paper aims to connect DP with (constrained) convex optimization by identifying similarities in algorithmic structure between three DP schemes and optimization algorithms. Specifically, the connections between Conservative Policy Iteration and Frank-Wolfe, Mirror-Descent Modified Policy Iteration and Mirror Descent, and Politex to Dual Averaging are highlighted. These DP schemes are representative of various (deep) Reinforcement Learning (RL) algorithms. By emphasizing these connections and encouraging further research, new and more effective RL algorithms can be developed.",1
"Millions of blind and visually-impaired (BVI) people navigate urban environments every day, using smartphones for high-level path-planning and white canes or guide dogs for local information. However, many BVI people still struggle to travel to new places. In our endeavor to create a navigation assistant for the BVI, we found that existing Reinforcement Learning (RL) environments were unsuitable for the task. This work introduces SEVN, a sidewalk simulation environment and a neural network-based approach to creating a navigation agent. SEVN contains panoramic images with labels for house numbers, doors, and street name signs, and formulations for several navigation tasks. We study the performance of an RL algorithm (PPO) in this setting. Our policy model fuses multi-modal observations in the form of variable resolution images, visible text, and simulated GPS data to navigate to a goal door. We hope that this dataset, simulator, and experimental results will provide a foundation for further research into the creation of agents that can assist members of the BVI community with outdoor navigation.",0
"Every day, millions of individuals who are blind or visually impaired (BVI) utilize smartphones for high-level path-planning and rely on white canes or guide dogs for local information to navigate urban environments. Nonetheless, traveling to new places remains a challenge for many BVI individuals. Our aim to develop a navigation assistant for the BVI community led us to discover that current Reinforcement Learning (RL) environments were not suitable for the task. This research paper introduces SEVN, a sidewalk simulation environment, and a neural network-based approach for creating a navigation agent. SEVN encompasses panoramic images that provide labels for house numbers, doors, and street name signs, as well as formulations for various navigation tasks. Our study focuses on the performance of an RL algorithm (PPO) in this environment. Our policy model integrates multi-modal observations, including variable resolution images, visible text, and simulated GPS data, to guide the agent to a goal door. We anticipate that this dataset, simulator, and experimental outcomes will serve as a foundation for further research into the development of agents that can assist members of the BVI community with outdoor navigation.",1
"This paper considers the problem of efficient exploration of unseen environments, a key challenge in AI. We propose a `learning to explore' framework where we learn a policy from a distribution of environments. At test time, presented with an unseen environment from the same distribution, the policy aims to generalize the exploration strategy to visit the maximum number of unique states in a limited number of steps. We particularly focus on environments with graph-structured state-spaces that are encountered in many important real-world applications like software testing and map building. We formulate this task as a reinforcement learning problem where the `exploration' agent is rewarded for transitioning to previously unseen environment states and employ a graph-structured memory to encode the agent's past trajectory. Experimental results demonstrate that our approach is extremely effective for exploration of spatial maps; and when applied on the challenging problems of coverage-guided software-testing of domain-specific programs and real-world mobile applications, it outperforms methods that have been hand-engineered by human experts.",0
"Exploring unknown environments is a significant obstacle in AI. This article presents a ""learning to explore"" framework that teaches a policy from a variety of environments. During testing, the policy attempts to generalize the exploration strategy to visit the maximum number of unique states in a limited number of steps when presented with an unseen environment from the same distribution. We concentrate on graph-structured state-spaces, which are prevalent in essential real-world applications like software testing and map building. We present this task as a reinforcement learning problem, where the ""exploration"" agent is rewarded for transitioning to previously unseen environment states and uses a graph-structured memory to encode the agent's past trajectory. Our experimental findings suggest that our approach is highly effective for exploring spatial maps. Furthermore, when applied to difficult challenges such as coverage-guided software testing of domain-specific programs and real-world mobile applications, our approach outperforms methods that have been hand-engineered by human experts.",1
"The ability for policies to generalize to new environments is key to the broad application of RL agents. A promising approach to prevent an agent's policy from overfitting to a limited set of training environments is to apply regularization techniques originally developed for supervised learning. However, there are stark differences between supervised learning and RL. We discuss those differences and propose modifications to existing regularization techniques in order to better adapt them to RL. In particular, we focus on regularization techniques relying on the injection of noise into the learned function, a family that includes some of the most widely used approaches such as Dropout and Batch Normalization. To adapt them to RL, we propose Selective Noise Injection (SNI), which maintains the regularizing effect the injected noise has, while mitigating the adverse effects it has on the gradient quality. Furthermore, we demonstrate that the Information Bottleneck (IB) is a particularly well suited regularization technique for RL as it is effective in the low-data regime encountered early on in training RL agents. Combining the IB with SNI, we significantly outperform current state of the art results, including on the recently proposed generalization benchmark Coinrun.",0
"The ability of RL agents to apply policies broadly to new environments is crucial. To prevent an agent's policy from overfitting to a limited set of training environments, regularization techniques originally designed for supervised learning can be applied. However, there are significant differences between supervised learning and RL. In this paper, we discuss these differences and suggest modifications to existing regularization techniques to better adapt them to RL. Specifically, we focus on regularization techniques that involve injecting noise into the learned function, such as Dropout and Batch Normalization. To adapt these techniques to RL, we propose Selective Noise Injection (SNI), which maintains the regularizing effect of injected noise while mitigating its adverse effects on gradient quality. Additionally, we show that the Information Bottleneck (IB) is an effective regularization technique for RL, particularly in the low-data regime encountered early on in training RL agents. By combining SNI with IB, we achieve significant improvements over current state-of-the-art results, even on the recently proposed generalization benchmark Coinrun.",1
"Actor-critic methods, a type of model-free Reinforcement Learning, have been successfully applied to challenging tasks in continuous control, often achieving state-of-the art performance. However, wide-scale adoption of these methods in real-world domains is made difficult by their poor sample efficiency. We address this problem both theoretically and empirically. On the theoretical side, we identify two phenomena preventing efficient exploration in existing state-of-the-art algorithms such as Soft Actor Critic. First, combining a greedy actor update with a pessimistic estimate of the critic leads to the avoidance of actions that the agent does not know about, a phenomenon we call pessimistic underexploration. Second, current algorithms are directionally uninformed, sampling actions with equal probability in opposite directions from the current mean. This is wasteful, since we typically need actions taken along certain directions much more than others. To address both of these phenomena, we introduce a new algorithm, Optimistic Actor Critic, which approximates a lower and upper confidence bound on the state-action value function. This allows us to apply the principle of optimism in the face of uncertainty to perform directed exploration using the upper bound while still using the lower bound to avoid overestimation. We evaluate OAC in several challenging continuous control tasks, achieving state-of the art sample efficiency.",0
"Successful application of model-free Reinforcement Learning, specifically Actor-critic methods, has been observed in challenging continuous control tasks, resulting in state-of-the-art performance. However, the adoption of these methods in real-world domains is hindered due to their poor sample efficiency. To address this issue, we have identified two phenomena that prevent efficient exploration in existing algorithms, namely pessimistic underexploration and directionally uninformed sampling. To counter these phenomena, we have introduced a new algorithm called Optimistic Actor Critic, which approximates a lower and upper confidence bound on the state-action value function. This allows directed exploration using the upper bound while avoiding overestimation with the lower bound. We have evaluated OAC in multiple challenging continuous control tasks, achieving state-of-the-art sample efficiency.",1
"Manipulating data, such as weighting data examples or augmenting with new instances, has been increasingly used to improve model training. Previous work has studied various rule- or learning-based approaches designed for specific types of data manipulation. In this work, we propose a new method that supports learning different manipulation schemes with the same gradient-based algorithm. Our approach builds upon a recent connection of supervised learning and reinforcement learning (RL), and adapts an off-the-shelf reward learning algorithm from RL for joint data manipulation learning and model training. Different parameterization of the ""data reward"" function instantiates different manipulation schemes. We showcase data augmentation that learns a text transformation network, and data weighting that dynamically adapts the data sample importance. Experiments show the resulting algorithms significantly improve the image and text classification performance in low data regime and class-imbalance problems.",0
"Model training has been enhanced by the usage of data manipulation techniques such as weighting data examples or introducing new instances. Previous research has focused on developing specific rule- or learning-based methodologies for data manipulation. Our study proposes a novel approach that employs a gradient-based algorithm for learning various manipulation schemes. This technique leverages the connection between supervised learning and reinforcement learning, adapting an off-the-shelf reward learning algorithm from RL for joint data manipulation learning and model training. Different manipulation schemes are achieved through different parameterizations of the ""data reward"" function. Our experiments demonstrate the effectiveness of our approach in enhancing image and text classification performance for low data and class-imbalance scenarios. Our approach showcases data augmentation that learns a text transformation network and data weighting that dynamically adjusts the data sample importance.",1
"Many decision-making problems naturally exhibit pronounced structures inherited from the characteristics of the underlying environment. In a Markov decision process model, for example, two distinct states can have inherently related semantics or encode resembling physical state configurations. This often implies locally correlated transition dynamics among the states. In order to complete a certain task in such environments, the operating agent usually needs to execute a series of temporally and spatially correlated actions. Though there exists a variety of approaches to capture these correlations in continuous state-action domains, a principled solution for discrete environments is missing. In this work, we present a Bayesian learning framework based on P\'olya-Gamma augmentation that enables an analogous reasoning in such cases. We demonstrate the framework on a number of common decision-making related problems, such as imitation learning, subgoal extraction, system identification and Bayesian reinforcement learning. By explicitly modeling the underlying correlation structures of these problems, the proposed approach yields superior predictive performance compared to correlation-agnostic models, even when trained on data sets that are an order of magnitude smaller in size.",0
"Decision-making problems often have inherent structures that are influenced by the underlying environment. For instance, in a Markov decision process model, two states may have related meanings or similar physical configurations, leading to locally correlated transitions. To accomplish a task in such environments, agents must execute a series of actions that are temporally and spatially correlated. While several approaches exist to capture these correlations in continuous state-action domains, there is no principled solution for discrete environments. This study introduces a Bayesian learning framework that uses P\'olya-Gamma augmentation to enable analogous reasoning in such cases. The proposed approach models the underlying correlation structures of decision-making related problems, such as imitation learning, subgoal extraction, system identification, and Bayesian reinforcement learning. By explicitly modeling these correlations, the approach outperforms correlation-agnostic models, even when trained on smaller datasets.",1
"We propose a new objective, the counterfactual objective, unifying existing objectives for off-policy policy gradient algorithms in the continuing reinforcement learning (RL) setting. Compared to the commonly used excursion objective, which can be misleading about the performance of the target policy when deployed, our new objective better predicts such performance. We prove the Generalized Off-Policy Policy Gradient Theorem to compute the policy gradient of the counterfactual objective and use an emphatic approach to get an unbiased sample from this policy gradient, yielding the Generalized Off-Policy Actor-Critic (Geoff-PAC) algorithm. We demonstrate the merits of Geoff-PAC over existing algorithms in Mujoco robot simulation tasks, the first empirical success of emphatic algorithms in prevailing deep RL benchmarks.",0
"A new objective called the counterfactual objective has been proposed to unify existing objectives in the continuing reinforcement learning setting for off-policy policy gradient algorithms. The commonly used excursion objective can be misleading about the performance of the target policy when deployed, whereas the new objective is a better predictor of its performance. The Generalized Off-Policy Policy Gradient Theorem is proven to compute the policy gradient of the counterfactual objective and an emphatic approach is used to obtain an unbiased sample. This results in the Generalized Off-Policy Actor-Critic (Geoff-PAC) algorithm, which outperforms existing algorithms in Mujoco robot simulation tasks. This marks the first empirical success of emphatic algorithms in prevailing deep RL benchmarks.",1
"Significant progress has been made in the area of model-based reinforcement learning. State-of-the-art algorithms are now able to match the asymptotic performance of model-free methods while being significantly more data efficient. However, this success has come at a price: state-of-the-art model-based methods require significant computation interleaved with data collection, resulting in run times that take days, even if the amount of agent interaction might be just hours or even minutes. When considering the goal of learning in real-time on real robots, this means these state-of-the-art model-based algorithms still remain impractical. In this work, we propose an asynchronous framework for model-based reinforcement learning methods that brings down the run time of these algorithms to be just the data collection time. We evaluate our asynchronous framework on a range of standard MuJoCo benchmarks. We also evaluate our asynchronous framework on three real-world robotic manipulation tasks. We show how asynchronous learning not only speeds up learning w.r.t wall-clock time through parallelization, but also further reduces the sample complexity of model-based approaches by means of improving the exploration and by means of effectively avoiding the policy overfitting to the deficiencies of learned dynamics models.",0
"The area of model-based reinforcement learning has seen significant progress, with state-of-the-art algorithms now able to match the performance of model-free methods while being more efficient with data. However, these methods require significant computation and result in long run times, making them impractical for learning in real-time on real robots. To address this issue, we propose an asynchronous framework for model-based reinforcement learning that reduces run times to just data collection time. We evaluate our framework on MuJoCo benchmarks and real-world robotic manipulation tasks, demonstrating that asynchronous learning not only speeds up learning through parallelization but also improves exploration and avoids policy overfitting to learned dynamics models.",1
"We study the safe reinforcement learning problem with nonlinear function approximation, where policy optimization is formulated as a constrained optimization problem with both the objective and the constraint being nonconvex functions. For such a problem, we construct a sequence of surrogate convex constrained optimization problems by replacing the nonconvex functions locally with convex quadratic functions obtained from policy gradient estimators. We prove that the solutions to these surrogate problems converge to a stationary point of the original nonconvex problem. Furthermore, to extend our theoretical results, we apply our algorithm to examples of optimal control and multi-agent reinforcement learning with safety constraints.",0
"Our focus is on the secure implementation of reinforcement learning, which involves the use of nonlinear function approximation. In this context, the optimization of policies is established as a problem of constrained optimization with nonconvex functions for both the objective and the constraint. To overcome this challenge, we have developed a series of surrogate convex constrained optimization problems that replace the nonconvex functions locally using convex quadratic functions based on policy gradient estimators. Our research indicates that the solutions to these surrogate problems can be used to converge to a stationary point of the original nonconvex problem. Additionally, we have applied this algorithm to optimal control and multi-agent reinforcement learning examples, incorporating safety constraints to extend our theoretical findings.",1
"Learning from demonstrations is a popular tool for accelerating and reducing the exploration requirements of reinforcement learning. When providing expert demonstrations to human students, we know that the demonstrations must fall within a particular range of difficulties called the ""Zone of Proximal Development (ZPD)"". If they are too easy the student learns nothing, but if they are too difficult the student is unable to follow along. This raises the question: Given a set of potential demonstrators, which among them is best suited for teaching any particular learner? Prior work, such as the popular Deep Q-learning from Demonstrations (DQfD) algorithm has generally focused on single demonstrators. In this work we consider the problem of choosing among multiple demonstrators of varying skill levels. Our results align with intuition from human learners: it is not always the best policy to draw demonstrations from the best performing demonstrator (in terms of reward). We show that careful selection of teaching strategies can result in sample efficiency gains in the learner's environment across nine Atari games",0
"Demonstration-based learning is a widely used technique to expedite and diminish the exploration prerequisites of reinforcement learning. When employing skilled demonstrations to human learners, it is crucial to ensure that the demonstrations fall within a specific range of challenges referred to as the ""Zone of Proximal Development (ZPD)."" If the demonstrations are too effortless, the student gains no knowledge, and if they are too demanding, the student cannot keep up. This raises a question: Among a group of potential demonstrators, which one is the most suitable to instruct a specific learner? Previous studies, such as the Deep Q-learning from Demonstrations (DQfD) algorithm, have primarily focused on a single demonstrator. In our research, we address the issue of selecting among various demonstrators with varying levels of proficiency. Our findings align with the intuition of human learners: It is not always best to draw demonstrations from the top-performing demonstrator in terms of reward. We demonstrate that by carefully selecting teaching strategies, we can substantially enhance the learner's sample efficiency across nine Atari games.",1
"Providing a suitable reward function to reinforcement learning can be difficult in many real world applications. While inverse reinforcement learning (IRL) holds promise for automatically learning reward functions from demonstrations, several major challenges remain. First, existing IRL methods learn reward functions from scratch, requiring large numbers of demonstrations to correctly infer the reward for each task the agent may need to perform. Second, existing methods typically assume homogeneous demonstrations for a single behavior or task, while in practice, it might be easier to collect datasets of heterogeneous but related behaviors. To this end, we propose a deep latent variable model that is capable of learning rewards from demonstrations of distinct but related tasks in an unsupervised way. Critically, our model can infer rewards for new, structurally-similar tasks from a single demonstration. Our experiments on multiple continuous control tasks demonstrate the effectiveness of our approach compared to state-of-the-art imitation and inverse reinforcement learning methods.",0
"In many real-world scenarios, it can be challenging to provide a suitable reward function for reinforcement learning. Although inverse reinforcement learning (IRL) shows promise in automatically learning reward functions from demonstrations, it still faces significant challenges. Firstly, current IRL methods require a large number of demonstrations to correctly infer the reward for each task that the agent may need to perform. Secondly, existing methods typically assume homogeneous demonstrations for a single behavior or task, whereas it may be easier to collect datasets of heterogeneous but related behaviors. To overcome these challenges, we propose a deep latent variable model that can learn rewards from demonstrations of distinct but related tasks in an unsupervised manner. Importantly, our model can infer rewards for new, structurally-similar tasks from a single demonstration. Our experiments on multiple continuous control tasks demonstrate the effectiveness of our approach compared to state-of-the-art imitation and inverse reinforcement learning methods.",1
"We present relay policy learning, a method for imitation and reinforcement learning that can solve multi-stage, long-horizon robotic tasks. This general and universally-applicable, two-phase approach consists of an imitation learning stage that produces goal-conditioned hierarchical policies, and a reinforcement learning phase that finetunes these policies for task performance. Our method, while not necessarily perfect at imitation learning, is very amenable to further improvement via environment interaction, allowing it to scale to challenging long-horizon tasks. We simplify the long-horizon policy learning problem by using a novel data-relabeling algorithm for learning goal-conditioned hierarchical policies, where the low-level only acts for a fixed number of steps, regardless of the goal achieved. While we rely on demonstration data to bootstrap policy learning, we do not assume access to demonstrations of every specific tasks that is being solved, and instead leverage unstructured and unsegmented demonstrations of semantically meaningful behaviors that are not only less burdensome to provide, but also can greatly facilitate further improvement using reinforcement learning. We demonstrate the effectiveness of our method on a number of multi-stage, long-horizon manipulation tasks in a challenging kitchen simulation environment. Videos are available at https://relay-policy-learning.github.io/",0
"A technique called relay policy learning is presented in this paper, which uses both imitation and reinforcement learning to tackle multi-stage, long-horizon robotic tasks. The approach is divided into two phases: an imitation learning stage that generates goal-conditioned hierarchical policies and a reinforcement learning phase that fine-tunes these policies for optimal task performance. Although the imitation learning stage may not be perfect, the method can be improved through environment interaction, making it suitable for complex tasks. A data-relabeling algorithm is used to simplify the long-horizon policy learning problem for goal-conditioned hierarchical policies. The method relies on unstructured and unsegmented demonstrations of semantically meaningful behaviors, rather than demonstrations of specific tasks. The effectiveness of our technique is demonstrated on various multi-stage, long-horizon manipulation tasks in a challenging kitchen simulation environment. Videos of the experiments are available at https://relay-policy-learning.github.io/.",1
"This work studies reinforcement learning in the Sim-to-Real setting, in which an agent is first trained on a number of simulators before being deployed in the real world, with the aim of decreasing the real-world sample complexity requirement. Using a dynamic model known as a rich observation Markov decision process (ROMDP), we formulate a theoretical framework for Sim-to-Real in the situation where feedback in the real world is not available. We establish real-world sample complexity guarantees that are smaller than what is currently known for directly (i.e., without access to simulators) learning a ROMDP with feedback.",0
"The focus of this study is on reinforcement learning within the Sim-to-Real environment, where the agent undergoes preliminary training on numerous simulators before being deployed in the real world. This approach aims to minimize the sample complexity requirement for real-world applications. By utilizing a dynamic model called a rich observation Markov decision process (ROMDP), we outline a theoretical framework for Sim-to-Real scenarios where real-world feedback is unavailable. Our research establishes smaller real-world sample complexity guarantees compared to conventional ROMDP learning methods that rely on feedback without access to simulators.",1
"Most common navigation tasks in human environments require auxiliary arm interactions, e.g. opening doors, pressing buttons and pushing obstacles away. This type of navigation tasks, which we call Interactive Navigation, requires the use of mobile manipulators: mobile bases with manipulation capabilities. Interactive Navigation tasks are usually long-horizon and composed of heterogeneous phases of pure navigation, pure manipulation, and their combination. Using the wrong part of the embodiment is inefficient and hinders progress. We propose HRL4IN, a novel Hierarchical RL architecture for Interactive Navigation tasks. HRL4IN exploits the exploration benefits of HRL over flat RL for long-horizon tasks thanks to temporally extended commitments towards subgoals. Different from other HRL solutions, HRL4IN handles the heterogeneous nature of the Interactive Navigation task by creating subgoals in different spaces in different phases of the task. Moreover, HRL4IN selects different parts of the embodiment to use for each phase, improving energy efficiency. We evaluate HRL4IN against flat PPO and HAC, a state-of-the-art HRL algorithm, on Interactive Navigation in two environments - a 2D grid-world environment and a 3D environment with physics simulation. We show that HRL4IN significantly outperforms its baselines in terms of task performance and energy efficiency. More information is available at https://sites.google.com/view/hrl4in.",0
"Interacting with human environments often requires the use of auxiliary arm movements such as opening doors, pushing obstacles, and pressing buttons. This type of navigation, which we refer to as Interactive Navigation, necessitates the use of mobile manipulators, which are mobile bases equipped with manipulation capabilities. Interactive Navigation is typically a long-term task that involves various phases of pure navigation, pure manipulation, and a combination of the two. Using the incorrect part of the embodiment can be inefficient and impede progress. To address this, we propose HRL4IN, a new Hierarchical RL architecture that takes advantage of the exploration benefits of HRL over flat RL for long-term tasks. HRL4IN handles the heterogeneous nature of the Interactive Navigation task by creating subgoals in different spaces during different phases of the task. Additionally, HRL4IN selects different parts of the embodiment for each phase, improving energy efficiency. We evaluate HRL4IN against flat PPO and HAC, a state-of-the-art HRL algorithm, in two environments - a 2D grid-world environment and a 3D environment with physics simulation. Our results indicate that HRL4IN outperforms its baselines in terms of task performance and energy efficiency. For further information, visit https://sites.google.com/view/hrl4in.",1
"Reinforcement Learning enables to train an agent via interaction with the environment. However, in the majority of real-world scenarios, the extrinsic feedback is sparse or not sufficient, thus intrinsic reward formulations are needed to successfully train the agent. This work investigates and extends the paradigm of curiosity-driven exploration. First, a probabilistic approach is taken to exploit the advantages of the attention mechanism, which is successfully applied in other domains of Deep Learning. Combining them, we propose new methods, such as AttA2C, an extension of the Actor-Critic framework. Second, another curiosity-based approach - ICM - is extended. The proposed model utilizes attention to emphasize features for the dynamic models within ICM, moreover, we also modify the loss function, resulting in a new curiosity formulation, which we call rational curiosity. The corresponding implementation can be found at https://github.com/rpatrik96/AttA2C/.",0
"Reinforcement Learning allows an agent to be trained by interacting with its environment. However, in many real-world situations, there is a lack of extrinsic feedback, which makes it necessary to use intrinsic reward formulations for successful training of the agent. This study investigates and expands upon the concept of curiosity-driven exploration. To achieve this, we use a probabilistic approach that leverages the benefits of the attention mechanism, which has been successful in other areas of Deep Learning. We propose new methods, such as AttA2C, which is an extension of the Actor-Critic framework. Additionally, we extend the ICM approach, which is also based on curiosity. Our proposed model utilizes attention to highlight features for the dynamic models within ICM, and we modify the loss function to create a new curiosity formulation, which we call rational curiosity. The corresponding implementation can be found at https://github.com/rpatrik96/AttA2C/.",1
"In this paper, we propose a novel Reinforcement Learning approach for solving the Active Information Acquisition problem, which requires an agent to choose a sequence of actions in order to acquire information about a process of interest using on-board sensors. The classic challenges in the information acquisition problem are the dependence of a planning algorithm on known models and the difficulty of computing information-theoretic cost functions over arbitrary distributions. In contrast, the proposed framework of reinforcement learning does not require any knowledge on models and alleviates the problems during an extended training stage. It results in policies that are efficient to execute online and applicable for real-time control of robotic systems. Furthermore, the state-of-the-art planning methods are typically restricted to short horizons, which may become problematic with local minima. Reinforcement learning naturally handles the issue of planning horizon in information problems as it maximizes a discounted sum of rewards over a long finite or infinite time horizon. We discuss the potential benefits of the proposed framework and compare the performance of the novel algorithm to an existing information acquisition method for multi-target tracking scenarios.",0
"Our paper presents a fresh approach to the Active Information Acquisition problem using Reinforcement Learning. This problem involves an agent selecting a series of actions to gather information about a process by using on-board sensors. Existing planning algorithms face challenges such as relying on known models and computing cost functions for arbitrary distributions. However, our proposed framework of reinforcement learning eliminates the need for model knowledge and overcomes these issues through an extended training phase. This results in policies that are efficient for real-time control of robotic systems. Additionally, traditional planning methods have limited horizons, which can lead to local minima. In contrast, reinforcement learning maximizes rewards over a long, finite, or infinite time horizon, tackling the problem of planning horizon in information acquisition. We explore the benefits of our framework and compare it to an existing method for multi-target tracking scenarios.",1
"Reinforcement learning (RL) methods have been shown to be capable of learning intelligent behavior in rich domains. However, this has largely been done in simulated domains without adequate focus on the process of building the simulator. In this paper, we consider a setting where we have access to an ensemble of pre-trained and possibly inaccurate simulators (models). We approximate the real environment using a state-dependent linear combination of the ensemble, where the coefficients are determined by the given state features and some unknown parameters. Our proposed algorithm provably learns a near-optimal policy with a sample complexity polynomial in the number of unknown parameters, and incurs no dependence on the size of the state (or action) space. As an extension, we also consider the more challenging problem of model selection, where the state features are unknown and can be chosen from a large candidate set. We provide exponential lower bounds that illustrate the fundamental hardness of this problem, and develop a provably efficient algorithm under additional natural assumptions.",0
"Although reinforcement learning (RL) methods have been successful in teaching intelligent behavior in complex domains, they have mainly been tested in simulated environments without much emphasis on the simulator development process. This article explores a scenario where multiple pre-trained simulators (models) are available, which may not be entirely accurate. We propose a method of approximating the actual environment by using a state-dependent linear combination of the ensemble, with the coefficients determined by the state features and unknown parameters. Our algorithm can learn a nearly optimal policy with sample complexity that is polynomial in the number of unknown parameters, without being affected by the size of the state or action space. Furthermore, we investigate the difficult issue of model selection when the state features are unknown and can be chosen from a vast collection. We provide exponential lower bounds to demonstrate the fundamental difficulty of this problem and introduce a provably efficient algorithm that relies on natural assumptions.",1
"From a young age humans learn to use grammatical principles to hierarchically combine words into sentences. Action grammars is the parallel idea, that there is an underlying set of rules (a ""grammar"") that govern how we hierarchically combine actions to form new, more complex actions. We introduce the Action Grammar Reinforcement Learning (AG-RL) framework which leverages the concept of action grammars to consistently improve the sample efficiency of Reinforcement Learning agents. AG-RL works by using a grammar inference algorithm to infer the ""action grammar"" of an agent midway through training. The agent's action space is then augmented with macro-actions identified by the grammar. We apply this framework to Double Deep Q-Learning (AG-DDQN) and a discrete action version of Soft Actor-Critic (AG-SAC) and find that it improves performance in 8 out of 8 tested Atari games (median +31%, max +668%) and 19 out of 20 tested Atari games (median +96%, maximum +3,756%) respectively without substantive hyperparameter tuning. We also show that AG-SAC beats the model-free state-of-the-art for sample efficiency in 17 out of the 20 tested Atari games (median +62%, maximum +13,140%), again without substantive hyperparameter tuning.",0
"Humans acquire the skill of creating sentences by combining words in a hierarchical order according to grammatical principles. Similarly, Action grammars propose that there are certain rules that govern the hierarchical combination of actions to form more complex ones. In this context, we present the Action Grammar Reinforcement Learning (AG-RL) framework that utilizes the concept of action grammars to enhance the sample efficiency of Reinforcement Learning agents. During training, AG-RL uses a grammar inference algorithm to deduce the ""action grammar"" of an agent. The agent's action space is then expanded by incorporating macro-actions identified by the grammar. We apply this framework to AG-DDQN and AG-SAC and observe a significant improvement in performance across all tested Atari games without requiring extensive hyperparameter tuning. Additionally, AG-SAC outperforms the model-free state-of-the-art for sample efficiency in most of the tested Atari games.",1
"We propose a novel neural architecture search algorithm via reinforcement learning by decoupling structure and operation search processes. Our approach samples candidate models from the multinomial distribution on the policy vectors defined on the two search spaces independently. The proposed technique improves the efficiency of architecture search process significantly compared to the conventional methods based on reinforcement learning with the RNN controllers while achieving competitive accuracy and model size in target tasks. Our policy vectors are easily interpretable throughout the training procedure, which allows to analyze the search progress and the discovered architectures; the black-box characteristics of the RNN controllers hamper understanding training progress in terms of policy parameter updates. Our experiments demonstrate outstanding performance compared to the state-of-the-art methods with a fraction of search cost.",0
"We suggest a new way of finding optimal neural architecture through reinforcement learning by separating the search for structure and operation processes. Our method involves selecting potential models from the multinomial distribution on policy vectors defined on the two search spaces independently. This approach is more efficient than conventional reinforcement learning methods using RNN controllers, while still achieving comparable accuracy and model size in target tasks. Our policy vectors are easily understandable throughout the training process, which is useful in analyzing search progress and discovered architectures. RNN controllers have black-box characteristics that make it difficult to understand training progress based on policy parameter updates. Our experiments showed exceptional performance compared to state-of-the-art methods with lower search costs.",1
"A major challenge in reinforcement learning (RL) is the design of agents that are able to generalize across tasks that share common dynamics. A viable solution is meta-reinforcement learning, which identifies common structures among past tasks to be then generalized to new tasks (meta-test). In meta-training, the RL agent learns state representations that encode prior information from a set of tasks, used to generalize the value function approximation. This has been proposed in the literature as successor representation approximators. While promising, these methods do not generalize well across optimal policies, leading to sampling-inefficiency during meta-test phases. In this paper, we propose state2vec, an efficient and low-complexity framework for learning successor features which (i) generalize across policies, (ii) ensure sample-efficiency during meta-test. We extend the well known node2vec framework to learn state embeddings that account for the discounted future state transitions in RL. The proposed off-policy state2vec captures the geometry of the underlying state space, making good basis functions for linear value function approximation.",0
"Reinforcement learning (RL) faces a significant obstacle in developing agents that can generalize across tasks that share common dynamics. Meta-reinforcement learning offers a solution by identifying common structures among past tasks and applying them to new tasks (meta-test). During meta-training, the RL agent learns state representations that encode prior information from a set of tasks, which can be used to generalize the value function approximation. However, current methods such as successor representation approximators do not generalize well across optimal policies, resulting in sampling-inefficiency during meta-test phases. To address this issue, we propose state2vec, a low-complexity framework for learning successor features that can generalize across policies and ensure sample-efficiency during meta-test. Our off-policy state2vec builds on the node2vec framework and captures the geometry of the underlying state space, making it a good basis for linear value function approximation.",1
"In this paper, we study the graph-based semi-supervised learning for classifying nodes in attributed networks, where the nodes and edges possess content information. Recent approaches like graph convolution networks and attention mechanisms have been proposed to ensemble the first-order neighbors and incorporate the relevant neighbors. However, it is costly (especially in memory) to consider all neighbors without a prior differentiation. We propose to explore the neighborhood in a reinforcement learning setting and find a walk path well-tuned for classifying the unlabelled target nodes. We let an agent (of node classification task) walk over the graph and decide where to direct to maximize classification accuracy. We define the graph walk as a partially observable Markov decision process (POMDP). The proposed method is flexible for working in both transductive and inductive setting. Extensive experiments on four datasets demonstrate that our proposed method outperforms several state-of-the-art methods. Several case studies also illustrate the meaningful movement trajectory made by the agent.",0
"The focus of this paper is on investigating graph-based semi-supervised learning for the purpose of classifying nodes in attributed networks. These networks contain content information for both the nodes and edges. Previous methods, such as graph convolution networks and attention mechanisms, have been introduced to incorporate relevant neighbors and first-order neighbors. However, considering all neighbors is expensive, particularly in terms of memory, and therefore, we propose a reinforcement learning approach to explore the neighborhood and determine a path that is well-suited for classifying the unlabelled target nodes. The node classification task agent traverses the graph and determines the most appropriate direction to maximize classification accuracy. Our method defines the graph walk as a partially observable Markov decision process (POMDP) and can be used in both transductive and inductive settings. Our experiments on four datasets demonstrate that our proposed method is superior to several state-of-the-art methods. Additionally, several case studies demonstrate the meaningful movement trajectory made by the agent.",1
"Humans achieve efficient learning by relying on prior knowledge about the structure of naturally occurring tasks. There is considerable interest in designing reinforcement learning (RL) algorithms with similar properties. This includes proposals to learn the learning algorithm itself, an idea also known as meta learning. One formal interpretation of this idea is as a partially observable multi-task RL problem in which task information is hidden from the agent. Such unknown task problems can be reduced to Markov decision processes (MDPs) by augmenting an agent's observations with an estimate of the belief about the task based on past experience. However estimating the belief state is intractable in most partially-observed MDPs. We propose a method that separately learns the policy and the task belief by taking advantage of various kinds of privileged information. Our approach can be very effective at solving standard meta-RL environments, as well as a complex continuous control environment with sparse rewards and requiring long-term memory.",0
"To efficiently learn, humans utilize their prior knowledge of the structure of tasks that occur naturally. There is a significant interest in creating reinforcement learning (RL) algorithms with similar capabilities, including the concept of meta-learning or learning the learning algorithm itself. This can be interpreted as a partially observable multi-task RL problem where task information is concealed from the agent. However, estimating the belief state is challenging in most partially-observed MDPs. Our proposed method solves this by separately learning the policy and the task belief through privileged information. It has proven to be successful in solving both standard meta-RL environments and a complex continuous control environment with sparse rewards and requiring long-term memory.",1
"In this paper, we develop a multi-agent reinforcement learning (MARL) framework to obtain online power control policies for a large energy harvesting (EH) multiple access channel, when only causal information about the EH process and wireless channel is available. In the proposed framework, we model the online power control problem as a discrete-time mean-field game (MFG), and analytically show that the MFG has a unique stationary solution. Next, we leverage the fictitious play property of the mean-field games, and the deep reinforcement learning technique to learn the stationary solution of the game, in a completely distributed fashion. We analytically show that the proposed procedure converges to the unique stationary solution of the MFG. This, in turn, ensures that the optimal policies can be learned in a completely distributed fashion. In order to benchmark the performance of the distributed policies, we also develop a deep neural network (DNN) based centralized as well as distributed online power control schemes. Our simulation results show the efficacy of the proposed power control policies. In particular, the DNN based centralized power control policies provide a very good performance for large EH networks for which the design of optimal policies is intractable using the conventional methods such as Markov decision processes. Further, performance of both the distributed policies is close to the throughput achieved by the centralized policies.",0
"This article presents a framework utilizing multi-agent reinforcement learning (MARL) to develop online power control policies for an energy harvesting (EH) multiple access channel with limited causal information. The proposed framework models the power control problem as a discrete-time mean-field game (MFG) and identifies a unique stationary solution. The researchers use the fictitious play property of MFGs and the deep reinforcement learning technique to learn the stationary solution in a completely distributed manner, which ensures the learning of optimal policies. Additionally, the study develops deep neural network (DNN) based centralized and distributed online power control schemes to benchmark the performance of the distributed policies. Simulation results demonstrate the effectiveness of the proposed power control policies, with the DNN based centralized policies performing well for large EH networks where conventional methods struggle, and both distributed policies achieving a throughput close to that of the centralized policies.",1
"Aiming to produce sufficient and diverse training samples, data augmentation has been demonstrated for its effectiveness in training deep models. Regarding that the criterion of the best augmentation is challenging to define, we in this paper present a novel learning-based augmentation method termed as DeepAugNet, which formulates the final augmented data as a collection of several sequentially augmented subsets. Specifically, the current augmented subset is required to maximize the performance improvement compared with the last augmented subset by learning the deterministic augmentation policy using deep reinforcement learning. By introducing an unified optimization goal, DeepAugNet intends to combine the data augmentation and the deep model training in an end-to-end training manner which is realized by simultaneously training a hybrid architecture of dueling deep Q-learning algorithm and a surrogate deep model. We extensively evaluated our proposed DeepAugNet on various benchmark datasets including Fashion MNIST, CUB, CIFAR-100 and WebCaricature. Compared with the current state-of-the-arts, our method can achieve a significant improvement in small-scale datasets, and a comparable performance in large-scale datasets. Code will be available soon.",0
"In order to effectively train deep models, data augmentation has proven to be useful in generating sufficient and diverse training samples. However, determining the best augmentation method is difficult. This paper presents a novel learning-based augmentation technique called DeepAugNet, which creates the final augmented data by combining several sequentially augmented subsets. Deep reinforcement learning is used to learn the deterministic augmentation policy, requiring the current subset to outperform the previous subset. DeepAugNet aims to integrate data augmentation and deep model training in an end-to-end manner by simultaneously training a hybrid architecture of dueling deep Q-learning algorithm and a surrogate deep model with a unified optimization goal. We evaluated our method on several benchmark datasets including Fashion MNIST, CUB, CIFAR-100 and WebCaricature, achieving significant improvement in small-scale datasets and comparable performance in large-scale datasets. Code for DeepAugNet will be available soon.",1
"Urban autonomous driving decision making is challenging due to complex road geometry and multi-agent interactions. Current decision making methods are mostly manually designing the driving policy, which might result in sub-optimal solutions and is expensive to develop, generalize and maintain at scale. On the other hand, with reinforcement learning (RL), a policy can be learned and improved automatically without any manual designs. However, current RL methods generally do not work well on complex urban scenarios. In this paper, we propose a framework to enable model-free deep reinforcement learning in challenging urban autonomous driving scenarios. We design a specific input representation and use visual encoding to capture the low-dimensional latent states. Several state-of-the-art model-free deep RL algorithms are implemented into our framework, with several tricks to improve their performance. We evaluate our method in a challenging roundabout task with dense surrounding vehicles in a high-definition driving simulator. The result shows that our method can solve the task well and is significantly better than the baseline.",0
"The intricate road layout and interactions among various entities make decision-making in urban autonomous driving a formidable task. The present methods of driving policy design mostly involve manual input, which can lead to suboptimal outcomes and incur high costs in terms of development, scalability, and maintenance. However, reinforcement learning (RL) can help learn and enhance driving policies automatically, without any manual involvement. Unfortunately, current RL methods are not effective in complex urban scenarios. In this paper, we propose a framework that enables model-free deep reinforcement learning in challenging urban autonomous driving scenarios. We design a specific input representation and use visual encoding to capture the low-dimensional latent states. We have implemented several state-of-the-art model-free deep RL algorithms in our framework, along with some tricks to enhance their efficacy. We have tested our approach in a challenging roundabout task, involving numerous vehicles in a high-definition driving simulator. The results indicate that our method can resolve the task effectively and is significantly better than the baseline.",1
"In this paper, we study reinforcement learning (RL) algorithms to solve real-world decision problems with the objective of maximizing the long-term reward as well as satisfying cumulative constraints. We propose a novel first-order policy optimization method, Interior-point Policy Optimization (IPO), which augments the objective with logarithmic barrier functions, inspired by the interior-point method. Our proposed method is easy to implement with performance guarantees and can handle general types of cumulative multiconstraint settings. We conduct extensive evaluations to compare our approach with state-of-the-art baselines. Our algorithm outperforms the baseline algorithms, in terms of reward maximization and constraint satisfaction.",0
"The focus of this paper is on using reinforcement learning (RL) algorithms to address decision-making problems in the real world. Our goal is to maximize long-term rewards while adhering to cumulative constraints. To achieve this, we introduce a novel first-order policy optimization approach called Interior-point Policy Optimization (IPO). This method utilizes logarithmic barrier functions, inspired by the interior-point method, to supplement the objective. Our method is user-friendly and has performance guarantees, making it suitable for various cumulative multiconstraint scenarios. We conducted extensive assessments to compare our approach with state-of-the-art baselines. Our algorithm proves superior to the baseline algorithms in terms of reward maximization and constraint satisfaction.",1
"Deep reinforcement learning (DRL) is a booming area of artificial intelligence. Many practical applications of DRL naturally involve more than one collaborative learners, making it important to study DRL in a multi-agent context. Previous research showed that effective learning in complex multi-agent systems demands for highly coordinated environment exploration among all the participating agents. Many researchers attempted to cope with this challenge through learning centralized value functions. However, the common strategy for every agent to learn their local policies directly often fail to nurture strong inter-agent collaboration and can be sample inefficient whenever agents alter their communication channels. To address these issues, we propose a new framework known as centralized training and exploration with decentralized execution via policy distillation. Guided by this framework and the maximum-entropy learning technique, we will first train agents' policies with shared global component to foster coordinated and effective learning. Locally executable policies will be derived subsequently from the trained global policies via policy distillation. Experiments show that our new framework and algorithm can achieve significantly better performance and higher sample efficiency than a cutting-edge baseline on several multi-agent DRL benchmarks.",0
"The field of artificial intelligence is experiencing a surge in the area of deep reinforcement learning (DRL). In practical applications, DRL often involves collaboration among multiple learners, making it essential to study it in a multi-agent context. Previous studies have demonstrated that effective learning in complex multi-agent systems requires highly coordinated exploration among all participating agents. While some researchers have tried to overcome this challenge by learning centralized value functions, the conventional approach of each agent learning their local policies directly can fall short in promoting inter-agent collaboration. It can also be inefficient when agents change their communication channels. To address these issues, we propose a new framework called centralized training and exploration with decentralized execution via policy distillation. Using this framework and the maximum-entropy learning technique, we first train agents' policies with a shared global component to facilitate coordinated and effective learning. We then derive locally executable policies from the trained global policies via policy distillation. Our experiments show that this approach outperforms a cutting-edge baseline on several multi-agent DRL benchmarks, achieving significantly better performance and higher sample efficiency.",1
"While often stated as an instance of the likelihood ratio trick [Rubinstein, 1989], the original policy gradient theorem [Sutton, 1999] involves an integral over the action space. When this integral can be computed, the resulting ""all-action"" estimator [Sutton, 2001] provides a conditioning effect [Bratley, 1987] reducing the variance significantly compared to the REINFORCE estimator [Williams, 1992]. In this paper, we adopt a numerical integration perspective to broaden the applicability of the all-action estimator to general spaces and to any function class for the policy or critic components, beyond the Gaussian case considered by [Ciosek, 2018]. In addition, we provide a new theoretical result on the effect of using a biased critic which offers more guidance than the previous ""compatible features"" condition of [Sutton, 1999]. We demonstrate the benefit of our approach in continuous control tasks with nonlinear function approximation. Our results show improved performance and sample efficiency.",0
"Although commonly referred to as an example of the likelihood ratio trick proposed by Rubinstein in 1989, the original policy gradient theorem developed by Sutton in 1999 involves an integration process across the action space. If this integration can be calculated, the resulting ""all-action"" estimator introduced by Sutton in 2001 has a conditioning effect that significantly reduces variance compared to the REINFORCE estimator put forth by Williams in 1992. In this article, we take a numerical integration approach to expand the use of the all-action estimator to general spaces and any function class for the policy or critic components, surpassing the Gaussian case studied by Ciosek in 2018. Additionally, we offer a new theoretical finding on the effects of employing a biased critic, which provides more direction than the ""compatible features"" condition outlined in Sutton's 1999 research. By conducting continuous control tasks with nonlinear function approximation, we demonstrate the advantages of our methodology, as it enhances performance and sample efficiency.",1
"Slimmable networks are a family of neural networks that can instantly adjust the runtime width. The width can be chosen from a predefined widths set to adaptively optimize accuracy-efficiency trade-offs at runtime. In this work, we propose a systematic approach to train universally slimmable networks (US-Nets), extending slimmable networks to execute at arbitrary width, and generalizing to networks both with and without batch normalization layers. We further propose two improved training techniques for US-Nets, named the sandwich rule and inplace distillation, to enhance training process and boost testing accuracy. We show improved performance of universally slimmable MobileNet v1 and MobileNet v2 on ImageNet classification task, compared with individually trained ones and 4-switch slimmable network baselines. We also evaluate the proposed US-Nets and improved training techniques on tasks of image super-resolution and deep reinforcement learning. Extensive ablation experiments on these representative tasks demonstrate the effectiveness of our proposed methods. Our discovery opens up the possibility to directly evaluate FLOPs-Accuracy spectrum of network architectures. Code and models are available at: https://github.com/JiahuiYu/slimmable_networks",0
"The concept of slimming networks involves neural networks that have the ability to adjust their runtime width immediately. This width can be selected from a predetermined set of widths to optimize accuracy and efficiency during runtime. Our study introduces a comprehensive approach to train universally slimmable networks (US-Nets), which expands on the concept of slimmable networks to work with networks of varying widths and batch normalization layers. We also introduce two training techniques, the sandwich rule and inplace distillation, to improve the training process and testing accuracy. Our experiments show that the universally slimmable MobileNet v1 and MobileNet v2 outperform individually trained networks and 4-switch slimmable network baselines on the ImageNet classification task. We also test our proposed methods on image super-resolution and deep reinforcement learning tasks, and demonstrate their effectiveness through extensive experimentation. Our work provides a way to evaluate the FLOPs-Accuracy spectrum of network architectures. To access our code and models, please visit: https://github.com/JiahuiYu/slimmable_networks",1
"We propose and study a general framework for regularized Markov decision processes (MDPs) where the goal is to find an optimal policy that maximizes the expected discounted total reward plus a policy regularization term. The extant entropy-regularized MDPs can be cast into our framework. Moreover, under our framework, many regularization terms can bring multi-modality and sparsity, which are potentially useful in reinforcement learning. In particular, we present sufficient and necessary conditions that induce a sparse optimal policy. We also conduct a full mathematical analysis of the proposed regularized MDPs, including the optimality condition, performance error, and sparseness control. We provide a generic method to devise regularization forms and propose off-policy actor critic algorithms in complex environment settings. We empirically analyze the numerical properties of optimal policies and compare the performance of different sparse regularization forms in discrete and continuous environments.",0
"A general framework for regularized Markov decision processes (MDPs) is proposed and examined in this study. The aim is to find an optimal policy that maximizes the expected discounted total reward and includes a policy regularization term. Our framework includes the extant entropy-regularized MDPs and has the potential to bring multi-modality and sparsity through various regularization terms, which can be useful in reinforcement learning. We present sufficient and necessary conditions that induce a sparse optimal policy and perform a complete mathematical analysis of the proposed regularized MDPs, including optimality conditions, performance error, and sparseness control. We also suggest a generic method for developing regularization forms and propose off-policy actor critic algorithms for complex environment settings. Optimal policy numerical properties are empirically analyzed, and the performance of different sparse regularization forms is compared in both discrete and continuous environments.",1
"Sequential decision making is a typical problem in reinforcement learning with plenty of algorithms to solve it. However, only a few of them can work effectively with a very small number of observations. In this report, we introduce the progress to learn the policy for Malaria Control as a Reinforcement Learning problem in the KDD Cup Challenge 2019 and propose diverse solutions to deal with the limited observations problem. We apply the Genetic Algorithm, Bayesian Optimization, Q-learning with sequence breaking to find the optimal policy for five years in a row with only 20 episodes/100 evaluations. We evaluate those algorithms and compare their performance with Random Search as a baseline. Among these algorithms, Q-Learning with sequence breaking has been submitted to the challenge and got ranked 7th in KDD Cup.",0
"Reinforcement learning involves solving sequential decision-making problems, for which numerous algorithms have been developed. However, only a handful of them are effective when there are limited observations available. This report discusses how the problem of Malaria Control was approached as a Reinforcement Learning challenge in the KDD Cup Challenge 2019. Various methods were proposed to address the challenge of limited observations, including Genetic Algorithm, Bayesian Optimization, and Q-learning with sequence breaking. These algorithms were evaluated and compared to the baseline Random Search. Q-learning with sequence breaking was ultimately submitted to the challenge and achieved a ranking of 7th place in KDD Cup. Our approach involved finding the optimal policy for five years in a row, with only 20 episodes/100 evaluations.",1
"In this work, we explore how a strategic selection of camera movements can facilitate the task of 6D multi-object pose estimation in cluttered scenarios while respecting real-world constraints important in robotics and augmented reality applications, such as time and distance traveled. In the proposed framework, a set of multiple object hypotheses is given to an agent, which is inferred by an object pose estimator and subsequently spatio-temporally selected by a fusion function that makes use of a verification score that circumvents the need of ground-truth annotations. The agent reasons about these hypotheses, directing its attention to the object which it is most uncertain about, moving the camera towards such an object. Unlike previous works that propose short-sighted policies, our agent is trained in simulated scenarios using reinforcement learning, attempting to learn the camera moves that produce the most accurate object poses hypotheses for a given temporal and spatial budget, without the need of viewpoints rendering during inference. Our experiments show that the proposed approach successfully estimates the 6D object pose of a stack of objects in both challenging cluttered synthetic and real scenarios, showing superior performance compared to strong baselines.",0
"This study examines how carefully chosen camera movements can aid in the accurate estimation of the 6D multi-object pose in complex surroundings, while taking into account significant real-world constraints in robotics and augmented reality applications, such as time and distance. The proposed framework provides an agent with multiple object hypotheses inferred by an object pose estimator, which are then spatio-temporally chosen by a fusion function that utilizes a verification score, eliminating the need for ground-truth annotations. The agent evaluates these hypotheses and focuses on the object that it is most uncertain about, maneuvering the camera towards it. Unlike previous research that suggests short-sighted policies, our agent is trained using reinforcement learning in simulated scenarios, aiming to learn camera movements that provide the most precise object pose hypotheses for a given space and time budget, without requiring viewpoints rendering during inference. Our findings reveal that the proposed method successfully estimates the 6D object pose of a stack of objects in both challenging synthetic and real scenarios, outperforming strong baselines.",1
"Effective coordination is crucial to solve multi-agent collaborative (MAC) problems. While centralized reinforcement learning methods can optimally solve small MAC instances, they do not scale to large problems and they fail to generalize to scenarios different from those seen during training. In this paper, we consider MAC problems with some intrinsic notion of locality (e.g., geographic proximity) such that interactions between agents and tasks are locally limited. By leveraging this property, we introduce a novel structured prediction approach to assign agents to tasks. At each step, the assignment is obtained by solving a centralized optimization problem (the inference procedure) whose objective function is parameterized by a learned scoring model. We propose different combinations of inference procedures and scoring models able to represent coordination patterns of increasing complexity. The resulting assignment policy can be efficiently learned on small problem instances and readily reused in problems with more agents and tasks (i.e., zero-shot generalization). We report experimental results on a toy search and rescue problem and on several target selection scenarios in StarCraft: Brood War, in which our model significantly outperforms strong rule-based baselines on instances with 5 times more agents and tasks than those seen during training.",0
"The effective coordination of multiple agents is crucial to solve collaborative problems. Although centralized reinforcement learning methods can tackle small problems, they are inadequate for larger ones and do not adapt well to new scenarios. In this study, we focus on MAC problems with an intrinsic sense of locality, such as geographic proximity. By leveraging this feature, we present a fresh structured prediction approach to allocate agents to tasks. At each step, we solve a centralized optimization problem to obtain the allocation, and the objective function is based on a learned scoring model. We propose various combinations of inference procedures and scoring models that can reflect increasingly complex coordination patterns. The resulting allocation policy can be efficiently learned on small problems and applied to larger ones with zero-shot generalization. We tested our model on a toy search and rescue problem and several target selection scenarios in StarCraft: Brood War. Our model outperformed strong rule-based baselines on instances with five times more agents and tasks than those encountered during training.",1
"Deep learning is attracting significant interest in the neuroimaging community as a means to diagnose psychiatric and neurological disorders from structural magnetic resonance images. However, there is a tendency amongst researchers to adopt architectures optimized for traditional computer vision tasks, rather than design networks customized for neuroimaging data. We address this by introducing NEURO-DRAM, a 3D recurrent visual attention model tailored for neuroimaging classification. The model comprises an agent which, trained by reinforcement learning, learns to navigate through volumetric images, selectively attending to the most informative regions for a given task. When applied to Alzheimer's disease prediction, NEURODRAM achieves state-of-the-art classification accuracy on an out-of-sample dataset, significantly outperforming a baseline convolutional neural network. When further applied to the task of predicting which patients with mild cognitive impairment will be diagnosed with Alzheimer's disease within two years, the model achieves state-of-the-art accuracy with no additional training. Encouragingly, the agent learns, without explicit instruction, a search policy in agreement with standardized radiological hallmarks of Alzheimer's disease, suggesting a route to automated biomarker discovery for more poorly understood disorders.",0
"The neuroimaging community is highly interested in using deep learning to diagnose psychiatric and neurological disorders from structural magnetic resonance images. However, many researchers tend to use architectures optimized for traditional computer vision tasks instead of developing networks specifically designed for neuroimaging data. To address this issue, we have created NEURO-DRAM, a 3D recurrent visual attention model catering to neuroimaging classification. The model includes an agent trained by reinforcement learning, which learns to navigate through volumetric images and attend to the most informative regions for a given task. NEURO-DRAM has achieved state-of-the-art classification accuracy on an out-of-sample dataset for Alzheimer's disease prediction, surpassing a baseline convolutional neural network. Additionally, the model has achieved state-of-the-art accuracy for predicting which patients with mild cognitive impairment will be diagnosed with Alzheimer's disease within two years, without further training. The agent has learned a search policy that matches the standardized radiological hallmarks of Alzheimer's disease, providing a potential means for automated biomarker discovery for poorly understood disorders.",1
"Reinforcement learning, mathematically described by Markov Decision Problems, may be approached either through dynamic programming or policy search. Actor-critic algorithms combine the merits of both approaches by alternating between steps to estimate the value function and policy gradient updates. Due to the fact that the updates exhibit correlated noise and biased gradient updates, only the asymptotic behavior of actor-critic is known by connecting its behavior to dynamical systems. This work puts forth a new variant of actor-critic that employs Monte Carlo rollouts during the policy search updates, which results in controllable bias that depends on the number of critic evaluations. As a result, we are able to provide for the first time the convergence rate of actor-critic algorithms when the policy search step employs policy gradient, agnostic to the choice of policy evaluation technique. In particular, we establish conditions under which the sample complexity is comparable to stochastic gradient method for non-convex problems or slower as a result of the critic estimation error, which is the main complexity bottleneck. These results hold for in continuous state and action spaces with linear function approximation for the value function. We then specialize these conceptual results to the case where the critic is estimated by Temporal Difference, Gradient Temporal Difference, and Accelerated Gradient Temporal Difference. These learning rates are then corroborated on a navigation problem involving an obstacle, which suggests that learning more slowly may lead to improved limit points, providing insight into the interplay between optimization and generalization in reinforcement learning.",0
"Markov Decision Problems provide a mathematical framework for reinforcement learning, which can be approached through either dynamic programming or policy search. Actor-critic algorithms combine these approaches by alternating between estimating the value function and updating the policy gradient. However, these updates may exhibit correlated noise and biased gradient updates, limiting our understanding of actor-critic's behavior as a dynamical system. This paper proposes a new variant of actor-critic that employs Monte Carlo rollouts during policy search updates, resulting in controllable bias dependent on critic evaluations. This enables us to establish for the first time the convergence rate of actor-critic algorithms, regardless of the policy evaluation technique used. Our results show that sample complexity is comparable to the stochastic gradient method for non-convex problems, but may be slower due to critic estimation error. These findings hold for continuous state and action spaces with linear function approximation for the value function. We further specialize these results to the use of Temporal Difference, Gradient Temporal Difference, and Accelerated Gradient Temporal Difference. We apply these learning rates to a navigation problem with an obstacle, discovering that learning more slowly may lead to better limit points, providing insight into the relationship between optimization and generalization in reinforcement learning.",1
"Soft Actor-Critic is a state-of-the-art reinforcement learning algorithm for continuous action settings that is not applicable to discrete action settings. Many important settings involve discrete actions, however, and so here we derive an alternative version of the Soft Actor-Critic algorithm that is applicable to discrete action settings. We then show that, even without any hyperparameter tuning, it is competitive with the tuned model-free state-of-the-art on a selection of games from the Atari suite.",0
"Soft Actor-Critic is an advanced reinforcement learning algorithm designed for continuous action settings, but it cannot be used in settings that involve discrete actions. Nonetheless, we have created an alternative version of Soft Actor-Critic that can be applied to discrete action settings. We have demonstrated that our modified algorithm can compete with state-of-the-art model-free approaches, even without any hyperparameter adjustments, by testing it on a variety of games from the Atari suite.",1
"This paper is concerned with multi-view reinforcement learning (MVRL), which allows for decision making when agents share common dynamics but adhere to different observation models. We define the MVRL framework by extending partially observable Markov decision processes (POMDPs) to support more than one observation model and propose two solution methods through observation augmentation and cross-view policy transfer. We empirically evaluate our method and demonstrate its effectiveness in a variety of environments. Specifically, we show reductions in sample complexities and computational time for acquiring policies that handle multi-view environments.",0
"The focus of this article is on multi-view reinforcement learning (MVRL), which enables decision making in situations where agents have common dynamics but follow different observation models. Our definition of the MVRL framework extends partially observable Markov decision processes (POMDPs) to accommodate more than one observation model. We present two solution approaches utilizing observation augmentation and cross-view policy transfer. Our method is assessed empirically and proven effective in various settings, with reduced sample complexities and computational time in developing policies for multi-view environments.",1
"Currently, many applications in Machine Learning are based on define new models to extract more information about data, In this case Deep Reinforcement Learning with the most common application in video games like Atari, Mario, and others causes an impact in how to computers can learning by himself with only information called rewards obtained from any action. There is a lot of algorithms modeled and implemented based on Deep Recurrent Q-Learning proposed by DeepMind used in AlphaZero and Go. In this document, We proposed Deep Recurrent Double Q-Learning that is an implementation of Deep Reinforcement Learning using Double Q-Learning algorithms and Recurrent Networks like LSTM and DRQN.",0
"The current trend in Machine Learning involves creating new models to gain more insight into data. Deep Reinforcement Learning, which is commonly used in video games such as Atari and Mario, is a prime example of this trend. This approach allows computers to learn independently using rewards obtained from actions. A variety of algorithms have been developed based on Deep Recurrent Q-Learning, which was proposed by DeepMind and used in AlphaZero and Go. In this paper, we propose Deep Recurrent Double Q-Learning, which uses Double Q-Learning algorithms and Recurrent Networks such as LSTM and DRQN to implement Deep Reinforcement Learning.",1
"Meta-learning has emerged as an important framework for learning new tasks from just a few examples. The success of any meta-learning model depends on (i) its fast adaptation to new tasks, as well as (ii) having a shared representation across similar tasks. Here we extend the model-agnostic meta-learning (MAML) framework introduced by Finn et al. (2017) to achieve improved performance by analyzing the temporal dynamics of the optimization procedure via the Runge-Kutta method. This method enables us to gain fine-grained control over the optimization and helps us achieve both the adaptation and representation goals across tasks. By leveraging this refined control, we demonstrate that there are multiple principled ways to update MAML and show that the classic MAML optimization is simply a special case of second-order Runge-Kutta method that mainly focuses on fast-adaptation. Experiments on benchmark classification, regression and reinforcement learning tasks show that this refined control helps attain improved results.",0
"The framework of meta-learning has become significant in the ability to learn new tasks with limited examples. In order for a meta-learning model to be successful, it must be able to quickly adapt to new tasks and have a shared representation for similar tasks. Our extension of the model-agnostic meta-learning (MAML) framework, originally introduced by Finn et al. in 2017, achieves enhanced performance by examining the optimization procedure's temporal dynamics using the Runge-Kutta method. This method allows us to have precise control over the optimization process and accomplish both adaptation and representation goals for various tasks. By utilizing this refined control, we demonstrate that there are multiple systematic approaches to update MAML and prove that the classic MAML optimization is a specific instance of the second-order Runge-Kutta method that mainly focuses on fast adaptation. Our experiments on benchmark classification, regression, and reinforcement learning tasks showcase that this refined control leads to better results.",1
"Off-policy temporal difference (TD) methods are a powerful class of reinforcement learning (RL) algorithms. Intriguingly, deep off-policy TD algorithms are not commonly used in combination with feature normalization techniques, despite positive effects of normalization in other domains. We show that naive application of existing normalization techniques is indeed not effective, but that well-designed normalization improves optimization stability and removes the necessity of target networks. In particular, we introduce a normalization based on a mixture of on- and off-policy transitions, which we call cross-normalization. It can be regarded as an extension of batch normalization that re-centers data for two different distributions, as present in off-policy learning. Applied to DDPG and TD3, cross-normalization improves over the state of the art across a range of MuJoCo benchmark tasks.",0
"Temporal difference (TD) methods that are off-policy are powerful algorithms in reinforcement learning (RL). Interestingly, deep off-policy TD algorithms are not commonly combined with feature normalization techniques, even though normalization has had positive effects in other domains. Our research demonstrates that simply applying existing normalization techniques is not effective, but well-designed normalization can enhance optimization stability and remove the need for target networks. We introduce a normalization method based on a combination of on- and off-policy transitions, which we refer to as cross-normalization. It is an extension of batch normalization that re-centers data for two different distributions, as found in off-policy learning. When applied to DDPG and TD3, cross-normalization outperforms the state of the art across various MuJoCo benchmark tasks.",1
"Deep Reinforcement Learning (DRL) has emerged as a powerful control technique in robotic science. In contrast to control theory, DRL is more robust in the thorough exploration of the environment. This capability of DRL generates more human-like behaviour and intelligence when applied to the robots. To explore this capability, we designed challenging manipulation tasks to observe robots strategy to handle complex scenarios. We observed that robots not only perform tasks successfully, but also transpire a creative and non intuitive solution. We also observed robot's persistence in tasks that are close to success and its striking ability in discerning to continue or give up.",0
"The use of Deep Reinforcement Learning (DRL) has become a potent control method in the field of robotics. DRL proves to be more resilient in exploring the environment compared to control theory. This feature of DRL results in robots exhibiting behavior and intelligence that is more akin to humans. To test this ability, we devised difficult manipulation tasks to gauge how robots handle complex situations. Our observations showed that the robots not only accomplish the tasks at hand, but also devise innovative and unconventional solutions. Furthermore, we noticed the persistence of robots in tasks that are almost complete and their impressive aptitude in deciding whether to continue or abandon them.",1
"We consider the problem of how a teacher algorithm can enable an unknown Deep Reinforcement Learning (DRL) student to become good at a skill over a wide range of diverse environments. To do so, we study how a teacher algorithm can learn to generate a learning curriculum, whereby it sequentially samples parameters controlling a stochastic procedural generation of environments. Because it does not initially know the capacities of its student, a key challenge for the teacher is to discover which environments are easy, difficult or unlearnable, and in what order to propose them to maximize the efficiency of learning over the learnable ones. To achieve this, this problem is transformed into a surrogate continuous bandit problem where the teacher samples environments in order to maximize absolute learning progress of its student. We present a new algorithm modeling absolute learning progress with Gaussian mixture models (ALP-GMM). We also adapt existing algorithms and provide a complete study in the context of DRL. Using parameterized variants of the BipedalWalker environment, we study their efficiency to personalize a learning curriculum for different learners (embodiments), their robustness to the ratio of learnable/unlearnable environments, and their scalability to non-linear and high-dimensional parameter spaces. Videos and code are available at https://github.com/flowersteam/teachDeepRL.",0
"The problem addressed in this study is how to help an unknown student of Deep Reinforcement Learning (DRL) become proficient in a skill across a variety of different environments. The focus is on how a teacher algorithm can generate a learning curriculum by sequentially creating different environments using stochastic procedural generation. Since the teacher algorithm is unaware of the student's abilities, it must determine which environments are easy, difficult, or impossible to learn and in what order they should be presented to optimize the learning process. To accomplish this, the problem is transformed into a continuous bandit problem in which the teacher algorithm selects environments to maximize the student's absolute learning progress. A new algorithm is presented, called the Absolute Learning Progress with Gaussian Mixture Models (ALP-GMM), and existing algorithms are adapted to address the problem in the context of DRL. The study examines the effectiveness of personalized learning curricula for different learners and their scalability in non-linear and high-dimensional parameter spaces using parameterized variants of the BipedalWalker environment. The videos and code are available at https://github.com/flowersteam/teachDeepRL.",1
"Integration of reinforcement learning and imitation learning is an important problem that has been studied for a long time in the field of intelligent robotics. Reinforcement learning optimizes policies to maximize the cumulative reward, whereas imitation learning attempts to extract general knowledge about the trajectories demonstrated by experts, i.e., demonstrators. Because each of them has their own drawbacks, methods combining them and compensating for each set of drawbacks have been explored thus far. However, many of the methods are heuristic and do not have a solid theoretical basis. In this paper, we present a new theory for integrating reinforcement and imitation learning by extending the probabilistic generative model framework for reinforcement learning, {\it plan by inference}. We develop a new probabilistic graphical model for reinforcement learning with multiple types of rewards and a probabilistic graphical model for Markov decision processes with multiple optimality emissions (pMDP-MO). Furthermore, we demonstrate that the integrated learning method of reinforcement learning and imitation learning can be formulated as a probabilistic inference of policies on pMDP-MO by considering the output of the discriminator in generative adversarial imitation learning as an additional optimal emission observation. We adapt the generative adversarial imitation learning and task-achievement reward to our proposed framework, achieving significantly better performance than agents trained with reinforcement learning or imitation learning alone. Experiments demonstrate that our framework successfully integrates imitation and reinforcement learning even when the number of demonstrators is only a few.",0
"For a long time, researchers in intelligent robotics have been studying the important problem of integrating reinforcement learning and imitation learning. While reinforcement learning aims to optimize policies to maximize cumulative reward, imitation learning seeks to extract general knowledge from expert demonstrations. However, both methods have their drawbacks, and previous attempts to combine them have often been heuristic and lacked a solid theoretical foundation. This paper presents a new theory for integrating reinforcement and imitation learning using a probabilistic generative model framework called ""plan by inference."" We introduce a new probabilistic graphical model for reinforcement learning with multiple types of rewards and a probabilistic graphical model for Markov decision processes with multiple optimality emissions. We show that the integrated learning method can be formulated as a probabilistic inference of policies on the pMDP-MO model by incorporating the output of the discriminator in generative adversarial imitation learning as an additional optimal emission observation. Our proposed framework adapts generative adversarial imitation learning and task-achievement reward, resulting in significantly better performance than agents trained with reinforcement learning or imitation learning alone. Our experiments demonstrate that our framework successfully integrates imitation and reinforcement learning even when there are only a few demonstrators.",1
"Conventional neural architecture search (NAS) approaches are based on reinforcement learning or evolutionary strategy, which take more than 3000 GPU hours to find a good model on CIFAR-10. We propose an efficient NAS approach learning to search by gradient descent. Our approach represents the search space as a directed acyclic graph (DAG). This DAG contains billions of sub-graphs, each of which indicates a kind of neural architecture. To avoid traversing all the possibilities of the sub-graphs, we develop a differentiable sampler over the DAG. This sampler is learnable and optimized by the validation loss after training the sampled architecture. In this way, our approach can be trained in an end-to-end fashion by gradient descent, named Gradient-based search using Differentiable Architecture Sampler (GDAS). In experiments, we can finish one searching procedure in four GPU hours on CIFAR-10, and the discovered model obtains a test error of 2.82\% with only 2.5M parameters, which is on par with the state-of-the-art. Code is publicly available on GitHub: https://github.com/D-X-Y/NAS-Projects.",0
"The traditional methods for neural architecture search (NAS) involve reinforcement learning or evolutionary strategy and require more than 3000 GPU hours to find a suitable model for CIFAR-10. We introduce an efficient NAS technique that uses gradient descent for learning to search. Our method employs a directed acyclic graph (DAG) to represent the search space, which comprises billions of sub-graphs, each indicating a distinct neural architecture. To prevent exploring all the sub-graph possibilities, we implement a differentiable sampler on the DAG, which is trainable and optimized by the validation loss after training the sampled architecture. This enables us to end-to-end train our approach using gradient descent, which we call Gradient-based search using Differentiable Architecture Sampler (GDAS). Our experiments reveal that we can complete a search in only four GPU hours on CIFAR-10, and the discovered model has a test error of 2.82% with only 2.5M parameters, which is comparable to the state-of-the-art. The code is publicly available on GitHub: https://github.com/D-X-Y/NAS-Projects.",1
"There is a growing demand for planning redirected walking techniques and applying them to physical environments with obstacles. Such techniques are mainly managed using three kinds of methods: direct scripting, generalized controller, and physical- or virtual-environment analysis to determine user redirection. The first approach is effective when a user's path and both physical and virtual environments are fixed; however, it is difficult to handle irregular movements and reuse other environments. The second approach has the potential of reusing any environment but is less optimized. The last approach is highly anticipated and versatile, although it has not been sufficiently developed. In this study, we propose a novel redirection controller using reinforcement learning with advanced plannability/versatility. Our simulation experiments show that the proposed strategy can reduce the number of resets by 20.3% for physical-space conditions with multiple obstacles.",0
"The demand for planning redirected walking techniques and implementing them in environments with obstacles is increasing. These techniques are typically managed using three methods: direct scripting, generalized controller, and physical or virtual environment analysis to determine user redirection. The first method is effective when the user's path and environments are fixed, but it struggles with irregular movements and reusing other environments. The second method has potential for reusing any environment but is less optimized. The third method is highly anticipated but not yet developed enough. This study proposes a novel redirection controller using reinforcement learning with advanced plannability and versatility. Simulation experiments demonstrate that the proposed strategy can reduce the number of resets by 20.3% for physical-space conditions with multiple obstacles.",1
"According to a mainstream position in contemporary cognitive science and philosophy, the use of abstract compositional concepts is both a necessary and a sufficient condition for the presence of genuine thought. In this article, we show how the ability to develop and utilise abstract conceptual structures can be achieved by a particular kind of learning agents. More specifically, we provide and motivate a concrete operational definition of what it means for these agents to be in possession of abstract concepts, before presenting an explicit example of a minimal architecture that supports this capability. We then proceed to demonstrate how the existence of abstract conceptual structures can be operationally useful in the process of employing previously acquired knowledge in the face of new experiences, thereby vindicating the natural conjecture that the cognitive functions of abstraction and generalisation are closely related.   Keywords: concept formation, projective simulation, reinforcement learning, transparent artificial intelligence, theory formation, explainable artificial intelligence (XAI)",0
"Contemporary cognitive science and philosophy hold that the use of abstract compositional concepts is necessary and sufficient for genuine thought. This article explores how learning agents can acquire and utilize abstract conceptual structures. The authors provide a clear definition of possession of abstract concepts and offer a minimal architecture example that supports this capability. They then demonstrate how abstract conceptual structures are operationally useful in employing previous knowledge in new situations, establishing a connection between abstraction, generalization, and cognitive functions. The article covers various topics, including concept formation, projective simulation, reinforcement learning, transparent artificial intelligence, theory formation, and explainable artificial intelligence (XAI).",1
"In statistics and machine learning, approximation of an intractable integration is often achieved by using the unbiased Monte Carlo estimator, but the variances of the estimation are generally high in many applications. Control variates approaches are well-known to reduce the variance of the estimation. These control variates are typically constructed by employing predefined parametric functions or polynomials, determined by using those samples drawn from the relevant distributions. Instead, we propose to construct those control variates by learning neural networks to handle the cases when test functions are complex. In many applications, obtaining a large number of samples for Monte Carlo estimation is expensive, which may result in overfitting when training a neural network. We thus further propose to employ auxiliary random variables induced by the original ones to extend data samples for training the neural networks. We apply the proposed control variates with augmented variables to thermodynamic integration and reinforcement learning. Experimental results demonstrate that our method can achieve significant variance reduction compared with other alternatives.",0
"The Monte Carlo estimator is commonly used in statistics and machine learning to approximate intractable integrations, but its estimations often have high variances in various applications. To reduce the variance, control variates are employed, typically constructed using predefined parametric functions or polynomials based on drawn samples from relevant distributions. However, for complex test functions, we suggest constructing control variates by training neural networks. Obtaining many Monte Carlo samples can be costly, leading to overfitting during neural network training. Therefore, we propose employing auxiliary random variables from the original data to expand the sample size for training. Our approach was applied to thermodynamic integration and reinforcement learning, demonstrating significant variance reduction compared to other methods.",1
"A plethora of problems in AI, engineering and the sciences are naturally formalized as inference in discrete probabilistic models. Exact inference is often prohibitively expensive, as it may require evaluating the (unnormalized) target density on its entire domain. Here we consider the setting where only a limited budget of calls to the unnormalized density oracle is available, raising the challenge of where in the domain to allocate these function calls in order to construct a good approximate solution. We formulate this problem as an instance of sequential decision-making under uncertainty and leverage methods from reinforcement learning for probabilistic inference with budget constraints. In particular, we propose the TreeSample algorithm, an adaptation of Monte Carlo Tree Search to approximate inference. This algorithm caches all previous queries to the density oracle in an explicit search tree, and dynamically allocates new queries based on a ""best-first"" heuristic for exploration, using existing upper confidence bound methods. Our non-parametric inference method can be effectively combined with neural networks that compile approximate conditionals of the target, which are then used to guide the inference search and enable generalization across multiple target distributions. We show empirically that TreeSample outperforms standard approximate inference methods on synthetic factor graphs.",0
"Numerous issues in AI, engineering, and sciences can be represented as inference in discrete probabilistic models. However, exact inference can be too expensive since it requires evaluating the target density on its entire domain. This creates a problem when there is only a limited budget for calling the unnormalized density oracle. To solve this, we use reinforcement learning methods to address the challenge of allocating function calls in the domain effectively. Our proposed algorithm, TreeSample, is an adaptation of Monte Carlo Tree Search that dynamically allocates new queries using existing upper confidence bound methods. This algorithm is useful for non-parametric inference and can be combined with neural networks to compile approximate conditionals of the target for generalization across multiple target distributions. We demonstrate that TreeSample outperforms standard approximate inference methods on synthetic factor graphs.",1
"Navigating complex urban environments safely is a key to realize fully autonomous systems. Predicting future locations of vulnerable road users, such as pedestrians and cyclists, thus, has received a lot of attention in the recent years. While previous works have addressed modeling interactions with the static (obstacles) and dynamic (humans) environment agents, we address an important gap in trajectory prediction. We propose SafeCritic, a model that synergizes generative adversarial networks for generating multiple ""real"" trajectories with reinforcement learning to generate ""safe"" trajectories. The Discriminator evaluates the generated candidates on whether they are consistent with the observed inputs. The Critic network is environmentally aware to prune trajectories that are in collision or are in violation with the environment. The auto-encoding loss stabilizes training and prevents mode-collapse. We demonstrate results on two large scale data sets with a considerable improvement over state-of-the-art. We also show that the Critic is able to classify the safety of trajectories.",0
"Ensuring safe navigation in complex urban areas is crucial for the successful implementation of autonomous systems. In recent years, much attention has been given to predicting the future movements of vulnerable road users like pedestrians and cyclists. While previous research has focused on modeling interactions with stationary obstacles and dynamic human agents, we aim to fill a significant gap in trajectory prediction. Our proposed model, SafeCritic, combines generative adversarial networks with reinforcement learning to generate both ""real"" and ""safe"" trajectories. The Discriminator assesses the generated trajectories for consistency with observed inputs, while the Critic network prunes trajectories that could result in collisions or violate environmental constraints. The use of auto-encoding loss stabilizes training and prevents mode-collapse. Our results, based on two large-scale datasets, demonstrate significant improvement over state-of-the-art methods. Furthermore, we show that the Critic network can accurately classify the safety of trajectories.",1
"Reparameterization (RP) and likelihood ratio (LR) gradient estimators are used throughout machine and reinforcement learning; however, they are usually explained as simple mathematical tricks without providing any insight into their nature. We use a first principles approach to explain LR and RP, and show a connection between the two via the divergence theorem. The theory motivated us to derive optimal importance sampling schemes to reduce LR gradient variance. Our newly derived distributions have analytic probability densities and can be directly sampled from. The improvement for Gaussian target distributions was modest, but for other distributions such as a Beta distribution, our method could lead to arbitrarily large improvements, and was crucial to obtain competitive performance in evolution strategies experiments.",0
"Machine and reinforcement learning commonly employ the Reparameterization (RP) and likelihood ratio (LR) gradient estimators. However, these techniques are often presented as mere mathematical tricks, lacking any depth of explanation. Through a first principles approach, we aim to elucidate the nature of LR and RP and establish a connection between the two via the divergence theorem. Our theoretical findings drive the development of optimal importance sampling schemes to reduce LR gradient variance. We introduce newly derived distributions with analytic probability densities that can be directly sampled from. While the improvement for Gaussian target distributions is modest, our method proves particularly valuable for other distributions, such as the Beta distribution, where it could lead to arbitrarily large improvements and was crucial in achieving competitive performance in evolution strategies experiments.",1
"A significant challenge for the practical application of reinforcement learning in the real world is the need to specify an oracle reward function that correctly defines a task. Inverse reinforcement learning (IRL) seeks to avoid this challenge by instead inferring a reward function from expert behavior. While appealing, it can be impractically expensive to collect datasets of demonstrations that cover the variation common in the real world (e.g. opening any type of door). Thus in practice, IRL must commonly be performed with only a limited set of demonstrations where it can be exceedingly difficult to unambiguously recover a reward function. In this work, we exploit the insight that demonstrations from other tasks can be used to constrain the set of possible reward functions by learning a ""prior"" that is specifically optimized for the ability to infer expressive reward functions from limited numbers of demonstrations. We demonstrate that our method can efficiently recover rewards from images for novel tasks and provide intuition as to how our approach is analogous to learning a prior.",0
"The practical application of reinforcement learning faces a significant challenge in the form of specifying an oracle reward function that accurately defines a task. Inverse reinforcement learning (IRL) offers a solution to this obstacle by inferring a reward function from expert behavior. However, collecting datasets of demonstrations that cover real-world variations can be prohibitively expensive, making it difficult to recover a reward function with only a limited set of demonstrations. To tackle this issue, we propose using demonstrations from other tasks to restrict the possible reward functions by developing a ""prior"" that is optimized for inferring expressive reward functions from a small number of demonstrations. Our method efficiently recovers rewards from images for new tasks and is similar to learning a prior.",1
"We present foundations for using Model Predictive Control (MPC) as a differentiable policy class for reinforcement learning in continuous state and action spaces. This provides one way of leveraging and combining the advantages of model-free and model-based approaches. Specifically, we differentiate through MPC by using the KKT conditions of the convex approximation at a fixed point of the controller. Using this strategy, we are able to learn the cost and dynamics of a controller via end-to-end learning. Our experiments focus on imitation learning in the pendulum and cartpole domains, where we learn the cost and dynamics terms of an MPC policy class. We show that our MPC policies are significantly more data-efficient than a generic neural network and that our method is superior to traditional system identification in a setting where the expert is unrealizable.",0
"In this article, we introduce a new approach to reinforcement learning in continuous state and action spaces using Model Predictive Control (MPC) as a differentiable policy class. By combining the benefits of model-based and model-free approaches, we demonstrate the ability to differentiate through MPC by using the KKT conditions of the convex approximation at a fixed point of the controller. This method allows us to learn the cost and dynamics of a controller through end-to-end learning, and we apply it to imitation learning in the pendulum and cartpole domains. Our results indicate that our MPC policies are more data-efficient than a generic neural network and outperform traditional system identification when the expert is unrealizable.",1
"The performance of imitation learning is typically upper-bounded by the performance of the demonstrator. While recent empirical results demonstrate that ranked demonstrations allow for better-than-demonstrator performance, preferences over demonstrations may be difficult to obtain, and little is known theoretically about when such methods can be expected to successfully extrapolate beyond the performance of the demonstrator. To address these issues, we first contribute a sufficient condition for better-than-demonstrator imitation learning and provide theoretical results showing why preferences over demonstrations can better reduce reward function ambiguity when performing inverse reinforcement learning. Building on this theory, we introduce Disturbance-based Reward Extrapolation (D-REX), a ranking-based imitation learning method that injects noise into a policy learned through behavioral cloning to automatically generate ranked demonstrations. These ranked demonstrations are used to efficiently learn a reward function that can then be optimized using reinforcement learning. We empirically validate our approach on simulated robot and Atari imitation learning benchmarks and show that D-REX outperforms standard imitation learning approaches and can significantly surpass the performance of the demonstrator. D-REX is the first imitation learning approach to achieve significant extrapolation beyond the demonstrator's performance without additional side-information or supervision, such as rewards or human preferences. By generating rankings automatically, we show that preference-based inverse reinforcement learning can be applied in traditional imitation learning settings where only unlabeled demonstrations are available.",0
"Usually, imitation learning's success is limited to the performance of the entity being imitated. However, recent research suggests that ranked demonstrations can lead to better results than the original performer. Obtaining preferences over demonstrations can be challenging, and it is unclear when these methods can exceed the demonstrator's performance. To address these problems, we establish a condition for better-than-demonstrator imitation learning and show why ranked demonstrations can reduce ambiguity when performing inverse reinforcement learning. Using this theory, we propose Disturbance-based Reward Extrapolation (D-REX), a ranking-based imitation learning technique that adds noise to a policy learned through behavioral cloning to generate ranked demonstrations automatically. These ranked demonstrations are used to learn a reward function efficiently, which can then be optimized using reinforcement learning. We validate our approach using simulated robot and Atari imitation learning benchmarks and demonstrate that D-REX outperforms standard imitation learning techniques and can surpass the demonstrator's performance significantly. D-REX is the first technique to achieve substantial extrapolation beyond the performer's performance without additional supervision or side-information, such as rewards or human preferences. By generating rankings automatically, we demonstrate that preference-based inverse reinforcement learning can be applied in traditional imitation learning settings where only unlabeled demonstrations are available.",1
"Entropy regularized algorithms such as Soft Q-learning and Soft Actor-Critic, recently showed state-of-the-art performance on a number of challenging reinforcement learning (RL) tasks. The regularized formulation modifies the standard RL objective and thus generally converges to a policy different from the optimal greedy policy of the original RL problem. Practically, it is important to control the sub-optimality of the regularized optimal policy. In this paper, we establish sufficient conditions for convergence of a large class of regularized dynamic programming algorithms, unified under regularized modified policy iteration (MPI) and conservative value iteration (VI) schemes. We provide explicit convergence rates to the optimality depending on the decrease rate of the regularization parameter. Our experiments show that the empirical error closely follows the established theoretical convergence rates. In addition to optimality, we demonstrate two desirable behaviours of the regularized algorithms even in the absence of approximations: robustness to stochasticity of environment and safety of trajectories induced by the policy iterates.",0
"Recently, entropy regularized algorithms like Soft Q-learning and Soft Actor-Critic have demonstrated exceptional performance on challenging reinforcement learning (RL) tasks. These algorithms modify the standard RL objective, resulting in a policy that differs from the optimal greedy policy of the original RL problem. It is crucial to manage the sub-optimality of the regularized optimal policy. This paper establishes adequate conditions for convergence of a broad array of regularized dynamic programming algorithms, which are unified under regularized modified policy iteration (MPI) and conservative value iteration (VI) schemes. The paper provides explicit convergence rates to optimality based on the decrease rate of the regularization parameter. Empirical experiments demonstrate that the established theoretical convergence rates are closely followed by the empirical error. Moreover, in addition to optimality, the paper shows that the regularized algorithms exhibit two desirable traits even in the absence of approximations: robustness to the environment's stochasticity and safety of trajectories generated by the policy iterates.",1
"Reinforcement learning (RL) is widely used in autonomous driving tasks and training RL models typically involves in a multi-step process: pre-training RL models on simulators, uploading the pre-trained model to real-life robots, and fine-tuning the weight parameters on robot vehicles. This sequential process is extremely time-consuming and more importantly, knowledge from the fine-tuned model stays local and can not be re-used or leveraged collaboratively. To tackle this problem, we present an online federated RL transfer process for real-time knowledge extraction where all the participant agents make corresponding actions with the knowledge learned by others, even when they are acting in very different environments. To validate the effectiveness of the proposed approach, we constructed a real-life collision avoidance system with Microsoft Airsim simulator and NVIDIA JetsonTX2 car agents, which cooperatively learn from scratch to avoid collisions in indoor environment with obstacle objects. We demonstrate that with the proposed framework, the simulator car agents can transfer knowledge to the RC cars in real-time, with 27% increase in the average distance with obstacles and 42% decrease in the collision counts.",0
"Autonomous driving tasks frequently utilize reinforcement learning (RL) which involves a multi-step process of pre-training RL models on simulators, transferring the pre-trained model to real-life robots, and fine-tuning the weight parameters on robot vehicles. This process is time-consuming and limits the re-use of fine-tuned model knowledge. To address this issue, we propose an online federated RL transfer process that allows participant agents to share knowledge learned from others, even in diverse environments. We validate our approach by constructing a collision avoidance system with Microsoft Airsim simulator and NVIDIA JetsonTX2 car agents. We demonstrate that our framework allows simulator car agents to transfer knowledge to RC cars in real-time, resulting in a 27% increase in average distance from obstacles and a 42% decrease in collision counts.",1
"This paper makes one step forward towards characterizing a new family of \textit{model-free} Deep Reinforcement Learning (DRL) algorithms. The aim of these algorithms is to jointly learn an approximation of the state-value function ($V$), alongside an approximation of the state-action value function ($Q$). Our analysis starts with a thorough study of the Deep Quality-Value Learning (DQV) algorithm, a DRL algorithm which has been shown to outperform popular techniques such as Deep-Q-Learning (DQN) and Double-Deep-Q-Learning (DDQN) \cite{sabatelli2018deep}. Intending to investigate why DQV's learning dynamics allow this algorithm to perform so well, we formulate a set of research questions which help us characterize a new family of DRL algorithms. Among our results, we present some specific cases in which DQV's performance can get harmed and introduce a novel \textit{off-policy} DRL algorithm, called DQV-Max, which can outperform DQV. We then study the behavior of the $V$ and $Q$ functions that are learned by DQV and DQV-Max and show that both algorithms might perform so well on several DRL test-beds because they are less prone to suffer from the overestimation bias of the $Q$ function.",0
"This article takes a step towards characterizing a new group of Deep Reinforcement Learning (DRL) algorithms that do not rely on models. These algorithms aim to learn both state-value function ($V$) and state-action value function ($Q$) approximations simultaneously. The study begins by examining the Deep Quality-Value Learning (DQV) algorithm, which has proven more effective than popular techniques like Deep-Q-Learning (DQN) and Double-Deep-Q-Learning (DDQN) \cite{sabatelli2018deep}. To understand why DQV performs so well, the researchers ask a set of research questions that help them define a new category of DRL algorithms. The researchers present specific cases in which DQV's performance can be compromised and introduce a new off-policy DRL algorithm called DQV-Max, which surpasses DQV's performance. They then examine the behavior of the $V$ and $Q$ functions learned by both algorithms and show that both algorithms perform well on many DRL test-beds because they are less susceptible to the overestimation bias of the $Q$ function.",1
"The breakthrough of deep Q-Learning on different types of environments revolutionized the algorithmic design of Reinforcement Learning to introduce more stable and robust algorithms, to that end many extensions to deep Q-Learning algorithm have been proposed to reduce the variance of the target values and the overestimation phenomena. In this paper, we examine new methodology to solve these issues, we propose using Dropout techniques on deep Q-Learning algorithm as a way to reduce variance and overestimation. We further present experiments on some of the benchmark environments that demonstrate significant improvement of the stability of the performance and a reduction in variance and overestimation.",0
"The algorithmic design of Reinforcement Learning has been transformed by the success of deep Q-Learning across various environments. To improve the stability and robustness of these algorithms, numerous extensions to the deep Q-Learning algorithm have been suggested to decrease the variance of target values and overestimation. This study proposes a new approach to address these problems by utilizing Dropout techniques on the deep Q-Learning algorithm. Our experiments on benchmark environments show that this method significantly enhances performance stability while reducing overestimation and variance.",1
"Reinforcement learning algorithms are known to be sample inefficient, and often performance on one task can be substantially improved by leveraging information (e.g., via pre-training) on other related tasks. In this work, we propose a technique to achieve such knowledge transfer in cases where agent trajectories contain sensitive or private information, such as in the healthcare domain. Our approach leverages a differentially private policy evaluation algorithm to initialize an actor-critic model and improve the effectiveness of learning in downstream tasks. We empirically show this technique increases sample efficiency in resource-constrained control problems while preserving the privacy of trajectories collected in an upstream task.",0
"The inefficiency of reinforcement learning algorithms is widely recognized, and it is often possible to enhance performance on a task by utilizing information gained from related tasks, for instance, via pre-training. In this study, we suggest a method for achieving knowledge transfer in situations where agent trajectories contain sensitive or confidential information, such as in the healthcare field. Our method employs a policy evaluation algorithm that is differentially private to initiate an actor-critic model and enhance learning efficiency in subsequent tasks. We demonstrate empirically that this approach improves sample efficiency in resource-limited control problems while maintaining the confidentiality of trajectories obtained in an initial task.",1
"Neural inductive program synthesis is a task generating instructions that can produce desired outputs from given inputs. In this paper, we focus on the generation of a chunk of assembly code that can be executed to match a state change inside the CPU and RAM. We develop a neural program synthesis algorithm, AutoAssemblet, learned via self-learning reinforcement learning that explores the large code space efficiently. Policy networks and value networks are learned to reduce the breadth and depth of the Monte Carlo Tree Search, resulting in better synthesis performance. We also propose an effective multi-entropy policy sampling technique to alleviate online update correlations. We apply AutoAssemblet to basic programming tasks and show significant higher success rates compared to several competing baselines.",0
"The objective of neural inductive program synthesis is to create instructions that can produce desired outputs given specific inputs. The focus of this study is on generating a block of assembly code that can execute and produce a state change in the CPU and RAM. To achieve this, we developed a neural program synthesis algorithm called AutoAssemblet. This algorithm uses self-learning reinforcement learning to efficiently explore the vast code space. We trained policy networks and value networks to reduce the Monte Carlo Tree Search's breadth and depth, resulting in improved synthesis performance. Additionally, we proposed a technique called multi-entropy policy sampling to address online update correlations. We tested AutoAssemblet on basic programming tasks and found that it outperformed several competing baselines.",1
"Owing to their ability to both effectively integrate information over long time horizons and scale to massive amounts of data, self-attention architectures have recently shown breakthrough success in natural language processing (NLP), achieving state-of-the-art results in domains such as language modeling and machine translation. Harnessing the transformer's ability to process long time horizons of information could provide a similar performance boost in partially observable reinforcement learning (RL) domains, but the large-scale transformers used in NLP have yet to be successfully applied to the RL setting. In this work we demonstrate that the standard transformer architecture is difficult to optimize, which was previously observed in the supervised learning setting but becomes especially pronounced with RL objectives. We propose architectural modifications that substantially improve the stability and learning speed of the original Transformer and XL variant. The proposed architecture, the Gated Transformer-XL (GTrXL), surpasses LSTMs on challenging memory environments and achieves state-of-the-art results on the multi-task DMLab-30 benchmark suite, exceeding the performance of an external memory architecture. We show that the GTrXL, trained using the same losses, has stability and performance that consistently matches or exceeds a competitive LSTM baseline, including on more reactive tasks where memory is less critical. GTrXL offers an easy-to-train, simple-to-implement but substantially more expressive architectural alternative to the standard multi-layer LSTM ubiquitously used for RL agents in partially observable environments.",0
"Self-attention architectures have proven to be effective in integrating information over long periods and handling large amounts of data, leading to significant progress in natural language processing, particularly in language modeling and machine translation. While the transformer's ability to process long-term information could also benefit partially observable reinforcement learning (RL) domains, large-scale transformers used in NLP have not yet been successfully applied to RL. In this study, we discovered that the standard transformer architecture is challenging to optimize, especially with RL objectives. We proposed architectural modifications that greatly improved the learning speed and stability of the original Transformer and XL variant. The resulting Gated Transformer-XL (GTrXL) outperformed LSTMs in challenging memory environments and achieved state-of-the-art results on the multi-task DMLab-30 benchmark suite, surpassing the performance of an external memory architecture. We demonstrated that GTrXL consistently matched or exceeded a competitive LSTM baseline's stability and performance, even on more reactive tasks where memory is less critical. GTrXL provides a straightforward and expressive architectural alternative to the standard multi-layer LSTM commonly used for RL agents in partially observable environments, which is easy to train and implement.",1
"An algorithmic decision-maker incentivizes people to act in certain ways to receive better decisions. These incentives can dramatically influence subjects' behaviors and lives, and it is important that both decision-makers and decision-recipients have clarity on which actions are incentivized by the chosen model. While for linear functions, the changes a subject is incentivized to make may be clear, we prove that for many non-linear functions (e.g. neural networks, random forests), classical methods for interpreting the behavior of models (e.g. input gradients) provide poor advice to individuals on which actions they should take. In this work, we propose a mathematical framework for understanding algorithmic incentives as the challenge of solving a Markov Decision Process, where the state includes the set of input features, and the reward is a function of the model's output. We can then leverage the many toolkits for solving MDPs (e.g. tree-based planning, reinforcement learning) to identify the optimal actions each individual is incentivized to take to improve their decision under a given model. We demonstrate the utility of our method by estimating the maximally-incentivized actions in two real-world settings: a recidivism risk predictor we train using ProPublica's COMPAS dataset, and an online credit scoring tool published by the Fair Isaac Corporation (FICO).",0
"The use of an algorithmic decision-maker can encourage individuals to behave in certain ways to receive favorable outcomes. These incentives can have a significant impact on a person's life, making it essential for both the decision-makers and the recipients to understand the actions incentivized by the selected model. While it may be straightforward to determine which actions are encouraged for linear functions, this is not always the case for non-linear functions such as neural networks and random forests. Traditional methods for interpreting model behavior, such as input gradients, are often inadequate in providing guidance to individuals on the actions they should take. To address this issue, we propose a mathematical framework for understanding algorithmic incentives as a Markov Decision Process, wherein the state comprises the input features, and the reward is a function of the model output. By utilizing various toolkits for solving MDPs, such as tree-based planning and reinforcement learning, we can identify the optimal actions for individuals to take to improve their decision within a given model. In this study, we demonstrate the efficacy of our approach by estimating the actions maximally incentivized in two real-world scenarios: a recidivism risk predictor trained using ProPublica's COMPAS dataset and an online credit scoring tool developed by the Fair Isaac Corporation (FICO).",1
"Smart and agile drones are fast becoming ubiquitous at the edge of the cloud. The usage of these drones are constrained by their limited power and compute capability. In this paper, we present a Transfer Learning (TL) based approach to reduce on-board computation required to train a deep neural network for autonomous navigation via Deep Reinforcement Learning for a target algorithmic performance. A library of 3D realistic meta-environments is manually designed using Unreal Gaming Engine and the network is trained end-to-end. These trained meta-weights are then used as initializers to the network in a test environment and fine-tuned for the last few fully connected layers. Variation in drone dynamics and environmental characteristics is carried out to show robustness of the approach. Using NVIDIA GPU profiler it was shown that the energy consumption and training latency is reduced by 3.7x and 1.8x respectively without significant degradation in the performance in terms of average distance traveled before crash i.e. Mean Safe Flight (MSF). The approach is also tested on a real environment using DJI Tello drone and similar results were reported.",0
"The prevalence of intelligent and nimble unmanned aerial vehicles (UAVs) is increasing rapidly near the border of the cloud. However, due to their limited computational capacity and power, the application of these drones is restricted. In this study, we propose a technique based on transfer learning (TL) to decrease the on-board computation necessary to teach a deep neural network for independent navigation using deep reinforcement learning to achieve a specified algorithmic performance. A collection of three-dimensional realistic meta-environments is produced using Unreal Gaming Engine, and the network is trained end-to-end. These trained meta-weights are subsequently utilized to initialize the network in a test environment and fine-tuned for the last few completely connected layers. To demonstrate the approach's resilience, drone dynamics and environmental factors are varied. Using NVIDIA GPU profiler, we show that energy consumption and training latency are reduced by 3.7x and 1.8x, respectively, without significant performance degradation in terms of the average distance traveled before a crash, known as Mean Safe Flight (MSF). The method is also tested on a real-world environment using a DJI Tello drone, with comparable results.",1
"Model-based reinforcement learning could enable sample-efficient learning by quickly acquiring rich knowledge about the world and using it to improve behaviour without additional data. Learned dynamics models can be directly used for planning actions but this has been challenging because of inaccuracies in the learned models. In this paper, we focus on planning with learned dynamics models and propose to regularize it using energy estimates of state transitions in the environment. We visually demonstrate the effectiveness of the proposed method and show that off-policy training of an energy estimator can be effectively used to regularize planning with pre-trained dynamics models. Further, we demonstrate that the proposed method enables sample-efficient learning to achieve competitive performance in challenging continuous control tasks such as Half-cheetah and Ant in just a few minutes of experience.",0
"The acquisition of extensive knowledge about the world and utilizing it to enhance behavior without additional data can be achieved through model-based reinforcement learning, leading to sample-efficient learning. However, planning actions through learned dynamics models has been difficult due to inaccuracies in the models. This paper proposes regularization of planning with learned dynamics models through energy estimates of state transitions in the environment. The effectiveness of the proposed method is visually demonstrated, and off-policy training of an energy estimator is shown to be effective in regularizing planning with pre-trained dynamics models. The proposed approach enables sample-efficient learning and can achieve competitive performance in challenging continuous control tasks like Half-cheetah and Ant in just a few minutes of experience.",1
"Intrinsically motivated reinforcement learning aims to address the exploration challenge for sparse-reward tasks. However, the study of exploration methods in transition-dependent multi-agent settings is largely absent from the literature. We aim to take a step towards solving this problem. We present two exploration methods: exploration via information-theoretic influence (EITI) and exploration via decision-theoretic influence (EDTI), by exploiting the role of interaction in coordinated behaviors of agents. EITI uses mutual information to capture influence transition dynamics. EDTI uses a novel intrinsic reward, called Value of Interaction (VoI), to characterize and quantify the influence of one agent's behavior on expected returns of other agents. By optimizing EITI or EDTI objective as a regularizer, agents are encouraged to coordinate their exploration and learn policies to optimize team performance. We show how to optimize these regularizers so that they can be easily integrated with policy gradient reinforcement learning. The resulting update rule draws a connection between coordinated exploration and intrinsic reward distribution. Finally, we empirically demonstrate the significant strength of our method in a variety of multi-agent scenarios.",0
"The exploration challenge for sparse-reward tasks can be addressed through intrinsically motivated reinforcement learning. Sadly, there is not much literature on exploration methods in transition-dependent multi-agent settings. To contribute to solving this issue, we present two exploration methods: exploration via information-theoretic influence (EITI) and exploration via decision-theoretic influence (EDTI). We utilize the role of interaction in coordinated behaviors of agents to achieve this. EITI captures influence transition dynamics using mutual information. EDTI introduces a novel intrinsic reward, Value of Interaction (VoI), to quantify the influence of one agent's behavior on the expected returns of other agents. By optimizing EITI or EDTI objective as a regularizer, agents are motivated to coordinate their exploration and learn policies to optimize team performance. We provide a demonstration of how to easily integrate these regularizers with policy gradient reinforcement learning. The resulting update rule establishes a link between coordinated exploration and intrinsic reward distribution. Finally, we show the remarkable strength of our method in various multi-agent scenarios through empirical evidence.",1
"AI heralds a step-change in the performance and capability of wireless networks and other critical infrastructures. However, it may also cause irreversible environmental damage due to their high energy consumption. Here, we address this challenge in the context of 5G and beyond, where there is a complexity explosion in radio resource management (RRM). On the one hand, deep reinforcement learning (DRL) provides a powerful tool for scalable optimization for high dimensional RRM problems in a dynamic environment. On the other hand, DRL algorithms consume a high amount of energy over time and risk compromising progress made in green radio research. This paper reviews and analyzes how to achieve green DRL for RRM via both architecture and algorithm innovations. Architecturally, a cloud based training and distributed decision-making DRL scheme is proposed, where RRM entities can make lightweight deep local decisions whilst assisted by on-cloud training and updating. On the algorithm level, compression approaches are introduced for both deep neural networks and the underlying Markov Decision Processes, enabling accurate low-dimensional representations of challenges. To scale learning across geographic areas, a spatial transfer learning scheme is proposed to further promote the learning efficiency of distributed DRL entities by exploiting the traffic demand correlations. Together, our proposed architecture and algorithms provide a vision for green and on-demand DRL capability.",0
"The use of AI technology has the potential to greatly enhance the performance and functionality of wireless networks and other crucial infrastructures. However, it is important to consider the potential negative impact on the environment as a result of the high energy consumption required by such systems. This issue is particularly relevant in the context of 5G and other advanced technologies, which involve complex radio resource management challenges. Deep reinforcement learning (DRL) is a promising approach for optimizing these systems in a dynamic environment, but it also consumes a significant amount of energy that can hinder progress in green radio research. To address this challenge, our paper proposes an innovative approach to achieving green DRL for RRM through both architecture and algorithmic innovations. Specifically, we propose a cloud-based training and distributed decision-making scheme that allows RRM entities to make lightweight local decisions while leveraging on-cloud training and updating. Additionally, we introduce compression techniques for deep neural networks and Markov Decision Processes to reduce the energy consumption of DRL algorithms. Finally, we propose a spatial transfer learning scheme to enhance the learning efficiency of distributed DRL entities by leveraging traffic demand correlations. Our proposed approach provides a vision for achieving green and on-demand DRL capability in the context of complex radio resource management challenges.",1
"Learning optimal feedback control laws capable of executing optimal trajectories is essential for many robotic applications. Such policies can be learned using reinforcement learning or planned using optimal control. While reinforcement learning is sample inefficient, optimal control only plans an optimal trajectory from a specific starting configuration. In this paper we propose deep optimal feedback control to learn an optimal feedback policy rather than a single trajectory. By exploiting the inherent structure of the robot dynamics and strictly convex action cost, we can derive principled cost functions such that the optimal policy naturally obeys the action limits, is globally optimal and stable on the training domain given the optimal value function. The corresponding optimal value function is learned end-to-end by embedding a deep differential network in the Hamilton-Jacobi-Bellmann differential equation and minimizing the error of this equality while simultaneously decreasing the discounting from short- to far-sighted to enable the learning. Our proposed approach enables us to learn an optimal feedback control law in continuous time, that in contrast to existing approaches generates an optimal trajectory from any point in state-space without the need of replanning. The resulting approach is evaluated on non-linear systems and achieves optimal feedback control, where standard optimal control methods require frequent replanning.",0
"It is crucial for various robotic applications to acquire the ability to learn the optimal feedback control laws that can execute the optimal trajectories. Reinforcement learning and optimal control are the two methods for learning such policies. However, reinforcement learning is not efficient in terms of the sample, and optimal control only plans a trajectory from a specific starting point. In this study, we propose the deep optimal feedback control method to learn an optimal feedback policy that is not limited to a single trajectory. We utilize the inherent structure of the robot dynamics and strictly convex action cost to derive principled cost functions that ensure the optimal policy adheres to the action limits, is globally optimal, and stable within the training domain. We end-to-end learn the optimal value function by embedding a deep differential network in the Hamilton-Jacobi-Bellmann differential equation and reducing the discounting from short- to far-sighted to enable the learning. Unlike existing methods, our approach allows us to learn an optimal feedback control law in continuous time, enabling us to generate an optimal trajectory without replanning from any state-space point. Our method achieves optimal feedback control in nonlinear systems where standard optimal control methods require frequent replanning.",1
"We present an overview of SURREAL-System, a reproducible, flexible, and scalable framework for distributed reinforcement learning (RL). The framework consists of a stack of four layers: Provisioner, Orchestrator, Protocol, and Algorithms. The Provisioner abstracts away the machine hardware and node pools across different cloud providers. The Orchestrator provides a unified interface for scheduling and deploying distributed algorithms by high-level description, which is capable of deploying to a wide range of hardware from a personal laptop to full-fledged cloud clusters. The Protocol provides network communication primitives optimized for RL. Finally, the SURREAL algorithms, such as Proximal Policy Optimization (PPO) and Evolution Strategies (ES), can easily scale to 1000s of CPU cores and 100s of GPUs. The learning performances of our distributed algorithms establish new state-of-the-art on OpenAI Gym and Robotics Suites tasks.",0
"This article introduces the SURREAL-System, a scalable, flexible, and reproducible framework for distributed reinforcement learning. The framework includes four layers: Provisioner, Orchestrator, Protocol, and Algorithms. The Provisioner simplifies the process of handling machine hardware and node pools across various cloud providers. The Orchestrator offers a single interface for scheduling and deploying distributed algorithms, capable of running on a personal laptop or a cloud cluster. The Protocol is optimized for RL communication. The SURREAL algorithms, including PPO and ES, can scale to thousands of CPU cores and hundreds of GPUs. Our distributed algorithms have achieved new state-of-the-art results on OpenAI Gym and Robotics Suites tasks.",1
"Imitation learning, followed by reinforcement learning algorithms, is a promising paradigm to solve complex control tasks sample-efficiently. However, learning from demonstrations often suffers from the covariate shift problem, which results in cascading errors of the learned policy. We introduce a notion of conservatively-extrapolated value functions, which provably lead to policies with self-correction. We design an algorithm Value Iteration with Negative Sampling (VINS) that practically learns such value functions with conservative extrapolation. We show that VINS can correct mistakes of the behavioral cloning policy on simulated robotics benchmark tasks. We also propose the algorithm of using VINS to initialize a reinforcement learning algorithm, which is shown to outperform significantly prior works in sample efficiency.",0
"The combination of imitation learning and reinforcement learning algorithms is a promising approach for efficiently solving complex control tasks. However, a common issue with learning from demonstrations is the covariate shift problem, which can cause errors in the learned policy. To address this, we introduce the concept of conservatively-extrapolated value functions that lead to self-correcting policies. Our algorithm, Value Iteration with Negative Sampling (VINS), practically learns these value functions with conservative extrapolation and successfully corrects errors in the behavioral cloning policy on simulated robotics benchmark tasks. We also propose using VINS to initialize a reinforcement learning algorithm, which significantly improves sample efficiency compared to previous methods.",1
"This paper addresses the problem of multi-agent inverse reinforcement learning (MIRL) in a two-player general-sum stochastic game framework. Five variants of MIRL are considered: uCS-MIRL, advE-MIRL, cooE-MIRL, uCE-MIRL, and uNE-MIRL, each distinguished by its solution concept. Problem uCS-MIRL is a cooperative game in which the agents employ cooperative strategies that aim to maximize the total game value. In problem uCE-MIRL, agents are assumed to follow strategies that constitute a correlated equilibrium while maximizing total game value. Problem uNE-MIRL is similar to uCE-MIRL in total game value maximization, but it is assumed that the agents are playing a Nash equilibrium. Problems advE-MIRL and cooE-MIRL assume agents are playing an adversarial equilibrium and a coordination equilibrium, respectively. We propose novel approaches to address these five problems under the assumption that the game observer either knows or is able to accurate estimate the policies and solution concepts for players. For uCS-MIRL, we first develop a characteristic set of solutions ensuring that the observed bi-policy is a uCS and then apply a Bayesian inverse learning method. For uCE-MIRL, we develop a linear programming problem subject to constraints that define necessary and sufficient conditions for the observed policies to be correlated equilibria. The objective is to choose a solution that not only minimizes the total game value difference between the observed bi-policy and a local uCS, but also maximizes the scale of the solution. We apply a similar treatment to the problem of uNE-MIRL. The remaining two problems can be solved efficiently by taking advantage of solution uniqueness and setting up a convex optimization problem. Results are validated on various benchmark grid-world games.",0
"This article focuses on the issue of multi-agent inverse reinforcement learning (MIRL) within a two-player general-sum stochastic game framework. Five different MIRL variations are explored, namely uCS-MIRL, advE-MIRL, cooE-MIRL, uCE-MIRL, and uNE-MIRL, each distinguished by its own solution concept. The uCS-MIRL problem deals with a cooperative game in which agents employ cooperative strategies to maximize the total game value. The uCE-MIRL problem assumes agents follow strategies that constitute a correlated equilibrium while maximizing total game value. The uNE-MIRL problem is similar to uCE-MIRL in terms of total game value maximization, but it is assumed that agents are playing a Nash equilibrium. The advE-MIRL and cooE-MIRL problems assume agents are playing an adversarial equilibrium and a coordination equilibrium, respectively. Innovative approaches are proposed to tackle these five problems under the assumption that the game observer knows or is capable of accurately estimating the policies and solution concepts of the players. For uCS-MIRL, a characteristic set of solutions is developed to ensure that the observed bi-policy is a uCS, followed by a Bayesian inverse learning method. For uCE-MIRL, a linear programming problem is developed with constraints defining necessary and sufficient conditions for the observed policies to be correlated equilibria. The objective is to choose a solution that minimizes the total game value difference between the observed bi-policy and a local uCS while maximizing the scale of the solution. A similar treatment is applied to the uNE-MIRL problem. The remaining two problems can be efficiently solved by taking advantage of solution uniqueness and setting up a convex optimization problem. Results are validated on various benchmark grid-world games.",1
"Combining model-based and model-free deep reinforcement learning has shown great promise for improving sample efficiency on complex control tasks while still retaining high performance. Incorporating imagination is a recent effort in this direction inspired by human mental simulation of motor behavior. We propose a learning-adaptive imagination approach which, unlike previous approaches, takes into account the reliability of the learned dynamics model used for imagining the future. Our approach learns an ensemble of disjoint local dynamics models in latent space and derives an intrinsic reward based on learning progress, motivating the controller to take actions leading to data that improves the models. The learned models are used to generate imagined experiences, augmenting the training set of real experiences. We evaluate our approach on learning vision-based robotic grasping and show that it significantly improves sample efficiency and achieves near-optimal performance in a sparse reward environment.",0
"The combination of model-based and model-free deep reinforcement learning has demonstrated potential in enhancing sample efficiency in complex control tasks while maintaining high performance. Recently, the integration of imagination into this approach has been explored, drawing inspiration from human mental simulations of motor behavior. Our proposed learning-adaptive imagination technique takes into consideration the dependability of the learned dynamics model utilized for predicting the future, which differs from previous approaches. Our method involves training an ensemble of separate local dynamics models in latent space and deriving an intrinsic reward based on learning progress, motivating the controller to select actions that generate data to improve the models. The learned models are utilized to generate imagined experiences, thereby supplementing the training set of real experiences. We tested our approach on vision-based robotic grasping and demonstrated that it effectively enhances sample efficiency and attains near-optimal performance in a sparse reward environment.",1
"Hierarchical Reinforcement Learning (HRL) is a promising approach to solving long-horizon problems with sparse and delayed rewards. Many existing HRL algorithms either use pre-trained low-level skills that are unadaptable, or require domain-specific information to define low-level rewards. In this paper, we aim to adapt low-level skills to downstream tasks while maintaining the generality of reward design. We propose an HRL framework which sets auxiliary rewards for low-level skill training based on the advantage function of the high-level policy. This auxiliary reward enables efficient, simultaneous learning of the high-level policy and low-level skills without using task-specific knowledge. In addition, we also theoretically prove that optimizing low-level skills with this auxiliary reward will increase the task return for the joint policy. Experimental results show that our algorithm dramatically outperforms other state-of-the-art HRL methods in Mujoco domains. We also find both low-level and high-level policies trained by our algorithm transferable.",0
"The use of Hierarchical Reinforcement Learning (HRL) shows promise for addressing long-term issues with limited and delayed rewards. However, current HRL algorithms either rely on inflexible pre-trained low-level skills or require specific information to define low-level rewards. This paper aims to adapt low-level skills to new tasks while maintaining a general reward design. To achieve this, we propose an HRL framework that sets auxiliary rewards for low-level skill training based on the high-level policy's advantage function. This enables efficient, simultaneous learning of high-level policy and low-level skills without task-specific knowledge. Furthermore, we prove that optimizing low-level skills with this auxiliary reward increases the task return for the joint policy. Experiments show that our algorithm outperforms other HRL methods in Mujoco domains, with both low-level and high-level policies trained by our algorithm being transferable.",1
"We develop model free PAC performance guarantees for multiple concurrent MDPs, extending recent works where a single learner interacts with multiple non-interacting agents in a noise free environment. Our framework allows noisy and resource limited communication between agents, and develops novel PAC guarantees in this extended setting. By allowing communication between the agents themselves, we suggest improved PAC-exploration algorithms that can overcome the communication noise and lead to improved sample complexity bounds. We provide a theoretically motivated algorithm that optimally combines information from the resource limited agents, thereby analyzing the interaction between noise and communication constraints that are ubiquitous in real-world systems. We present empirical results for a simple task that supports our theoretical formulations and improve upon naive information fusion methods.",0
"Our study expands upon recent research on model-free PAC performance guarantees for multiple concurrent MDPs, which previously focused on a single learner interacting with multiple non-interacting agents in a noise-free environment. Our approach accommodates resource-limited and noisy communication between agents and introduces innovative PAC guarantees in this extended context. By enabling communication between the agents, we propose enhanced PAC-exploration algorithms that can overcome communication noise and result in better sample complexity bounds. Our algorithm combines information optimally from the resource-limited agents, taking into account the interaction between noise and communication constraints that are prevalent in real-world systems. We demonstrate the efficacy of our theoretical formulations through empirical results for a straightforward task and enhance the accuracy of information fusion methods.",1
"Leveraging an equivalence property in the state-space of a Markov Decision Process (MDP) has been investigated in several studies. This paper studies equivalence structure in the reinforcement learning (RL) setup, where transition distributions are no longer assumed to be known. We present a notion of similarity between transition probabilities of various state-action pairs of an MDP, which naturally defines an equivalence structure in the state-action space. We present equivalence-aware confidence sets for the case where the learner knows the underlying structure in advance. These sets are provably smaller than their corresponding equivalence-oblivious counterparts. In the more challenging case of an unknown equivalence structure, we present an algorithm called ApproxEquivalence that seeks to find an (approximate) equivalence structure, and define confidence sets using the approximate equivalence. To illustrate the efficacy of the presented confidence sets, we present C-UCRL, as a natural modification of UCRL2 for RL in undiscounted MDPs. In the case of a known equivalence structure, we show that C-UCRL improves over UCRL2 in terms of regret by a factor of $\sqrt{SA/C}$, in any communicating MDP with $S$ states, $A$ actions, and $C$ classes, which corresponds to a massive improvement when $C \ll SA$. To the best of our knowledge, this is the first work providing regret bounds for RL when an equivalence structure in the MDP is efficiently exploited. In the case of an unknown equivalence structure, we show through numerical experiments that C-UCRL combined with ApproxEquivalence outperforms UCRL2 in ergodic MDPs.",0
"Several studies have investigated the use of an equivalence property in the state-space of a Markov Decision Process (MDP). This paper focuses on studying equivalence structure in the reinforcement learning (RL) setup, where transition distributions are unknown. The authors propose a similarity notion between transition probabilities of different state-action pairs of an MDP, which defines an equivalence structure in the state-action space. They present confidence sets that are aware of the equivalence structure when the learner knows it in advance. These sets are proven to be smaller than their equivalence-oblivious counterparts. In the case of an unknown equivalence structure, the authors propose an algorithm called ApproxEquivalence that seeks to find an approximate equivalence structure and defines confidence sets using the approximate equivalence. The authors demonstrate the effectiveness of these confidence sets by presenting C-UCRL, a modification of UCRL2 for RL in undiscounted MDPs. In the case of a known equivalence structure, C-UCRL improves over UCRL2 in terms of regret by a factor of $\sqrt{SA/C}$ in any communicating MDP with $S$ states, $A$ actions, and $C$ classes. This improvement is significant when $C \ll SA$. This work is the first to provide regret bounds for RL when exploiting an equivalence structure in the MDP efficiently. The authors also show through numerical experiments that C-UCRL combined with ApproxEquivalence outperforms UCRL2 in ergodic MDPs when the equivalence structure is unknown.",1
"The aim of the inverse chemical design is to develop new molecules with given optimized molecular properties or objectives. Recently, generative deep learning (DL) networks are considered as the state-of-the-art in inverse chemical design and have achieved early success in generating molecular structures with desired properties in the pharmaceutical and material chemistry fields. However, satisfying a large number (larger than 10 objectives) of molecular objectives is a limitation of current generative models. To improve the model's ability to handle a large number of molecule design objectives, we developed a Reinforcement Learning (RL) based generative framework to optimize chemical molecule generation. Our use of Curriculum Learning (CL) to fine-tune the pre-trained generative network allowed the model to satisfy up to 21 objectives and increase the generative network's robustness. The experiments show that the proposed multiple-objective RL-based generative model can correctly identify unknown molecules with an 83 to 100 percent success rate, compared to the baseline approach of 0 percent. Additionally, this proposed generative model is not limited to just chemistry research challenges; we anticipate that problems that utilize RL with multiple-objectives will benefit from this framework.",0
"The goal of inverse chemical design is to create new molecules that possess specific molecular properties or objectives. Recently, deep learning networks have emerged as the leading method for inverse chemical design, particularly in the pharmaceutical and material chemistry sectors. However, current generative models have trouble satisfying a large number of molecular objectives, which is typically more than 10. To address this limitation, we developed a Reinforcement Learning (RL) based generative framework that is capable of optimizing chemical molecule generation. By utilizing Curriculum Learning (CL) to fine-tune the pre-trained generative network, we were able to enhance the model's ability to satisfy up to 21 objectives and improve the network's resilience. The experiments conducted demonstrate that our multiple-objective RL-based generative model can accurately identify unknown molecules with a success rate ranging from 83% to 100%. In contrast, the baseline approach had a success rate of 0%. Importantly, this generative model is not limited to the field of chemistry, as we expect it to be applicable to other problems that require RL with multiple-objectives.",1
"When learning behavior, training data is often generated by the learner itself; this can result in unstable training dynamics, and this problem has particularly important applications in safety-sensitive real-world control tasks such as robotics. In this work, we propose a principled and model-agnostic approach to mitigate the issue of unstable learning dynamics by maintaining a history of a reinforcement learning agent over the course of training, and reverting to the parameters of a previous agent whenever performance significantly decreases. We develop techniques for evaluating this performance through statistical hypothesis testing of continued improvement, and evaluate them on a standard suite of challenging benchmark tasks involving continuous control of simulated robots. We show improvements over state-of-the-art reinforcement learning algorithms in performance and robustness to hyperparameters, outperforming DDPG in 5 out of 6 evaluation environments and showing no decrease in performance with TD3, which is known to be relatively stable. In this way, our approach takes an important step towards increasing data efficiency and stability in training for real-world robotic applications.",0
"The generation of training data by the learner can lead to unstable training dynamics, which is a significant problem in safety-sensitive real-world control tasks like robotics. Our study proposes a principled and model-agnostic approach to address this issue by maintaining a history of a reinforcement learning agent during training and returning to the parameters of a prior agent when performance significantly deteriorates. We establish methods for evaluating performance through statistical hypothesis testing of continued improvement and test them on a set of challenging benchmark tasks that involve continuous control of simulated robots. Our approach surpasses state-of-the-art reinforcement learning algorithms in terms of performance and robustness to hyperparameters, outperforming DDPG in 5 out of 6 evaluation environments and showing no decrease in performance with TD3, which is known for being relatively stable. This approach is a significant step towards increasing data efficiency and stability in training for real-world robotic applications.",1
"Current end-to-end deep Reinforcement Learning (RL) approaches require jointly learning perception, decision-making and low-level control from very sparse reward signals and high-dimensional inputs, with little capability of incorporating prior knowledge. This results in prohibitively long training times for use on real-world robotic tasks. Existing algorithms capable of extracting task-level representations from high-dimensional inputs, e.g. object detection, often produce outputs of varying lengths, restricting their use in RL methods due to the need for neural networks to have fixed length inputs. In this work, we propose a framework that combines deep sets encoding, which allows for variable-length abstract representations, with modular RL that utilizes these representations, decoupling high-level decision making from low-level control. We successfully demonstrate our approach on the robot manipulation task of object sorting, showing that this method can learn effective policies within mere minutes of highly simplified simulation. The learned policies can be directly deployed on a robot without further training, and generalize to variations of the task unseen during training.",0
"Current end-to-end deep Reinforcement Learning (RL) methods necessitate learning perception, decision-making, and low-level control simultaneously, using sparse reward signals and high-dimensional inputs, without the ability to integrate prior knowledge. This leads to lengthy training times that are impractical for real-world robotic applications. Although algorithms capable of extracting task-level representations from high-dimensional inputs produce outputs of varying lengths, limiting their use in RL, we propose a framework that combines deep sets encoding, which enables variable-length abstract representations, with modular RL that uses these representations to separate high-level decision-making from low-level control. We demonstrate the effectiveness of our approach in a robot manipulation task of object sorting, where it learns efficient policies within minutes of a basic simulation, and the policies generalize to variations of the task. These learned policies can be directly applied to a robot without further training.",1
"Stochastic games provide a framework for interactions among multiple agents and enable a myriad of applications. In these games, agents decide on actions simultaneously, the state of every agent moves to the next state, and each agent receives a reward. However, finding an equilibrium (if exists) in this game is often difficult when the number of agents becomes large. This paper focuses on finding a mean-field equilibrium (MFE) in an action coupled stochastic game setting in an episodic framework. It is assumed that the impact of the other agents' can be assumed by the empirical distribution of the mean of the actions. All agents know the action distribution and employ lower-myopic best response dynamics to choose the optimal oblivious strategy. This paper proposes a posterior sampling based approach for reinforcement learning in the mean-field game, where each agent samples a transition probability from the previous transitions. We show that the policy and action distributions converge to the optimal oblivious strategy and the limiting distribution, respectively, which constitute an MFE.",0
"Stochastic games offer a versatile tool for modeling interactions among multiple agents. These games involve agents making simultaneous decisions, transitioning to the next state, and receiving rewards. However, as the number of agents increases, finding an equilibrium becomes challenging. To address this, this study aims to identify a mean-field equilibrium (MFE) in an action coupled stochastic game setting within an episodic framework. The approach assumes that agents can estimate the impact of other agents based on the empirical distribution of actions. Additionally, all agents possess knowledge of the action distribution and utilize lower-myopic best response dynamics to select the ideal oblivious strategy. The paper proposes a posterior sampling-based approach for reinforcement learning in the mean-field game, where each agent samples a transition probability from the previous transitions. The research demonstrates that the policy and action distributions converge to the optimal oblivious strategy and the limiting distribution, respectively, which make up an MFE.",1
"TorchBeast is a platform for reinforcement learning (RL) research in PyTorch. It implements a version of the popular IMPALA algorithm for fast, asynchronous, parallel training of RL agents. Additionally, TorchBeast has simplicity as an explicit design goal: We provide both a pure-Python implementation (""MonoBeast"") as well as a multi-machine high-performance version (""PolyBeast""). In the latter, parts of the implementation are written in C++, but all parts pertaining to machine learning are kept in simple Python using PyTorch, with the environments provided using the OpenAI Gym interface. This enables researchers to conduct scalable RL research using TorchBeast without any programming knowledge beyond Python and PyTorch. In this paper, we describe the TorchBeast design principles and implementation and demonstrate that it performs on-par with IMPALA on Atari. TorchBeast is released as an open-source package under the Apache 2.0 license and is available at \url{https://github.com/facebookresearch/torchbeast}.",0
"TorchBeast is a PyTorch-based platform designed for research in reinforcement learning (RL). It incorporates a version of the widely-used IMPALA algorithm that enables fast, asynchronous, and parallel training of RL agents. The platform aims to provide simplicity as a key design objective and offers both a pure-Python version called ""MonoBeast"" and a high-performance multi-machine version called ""PolyBeast."" While parts of the implementation are written in C++, all machine learning-related aspects are in Python using PyTorch, with the OpenAI Gym interface providing the environments. This allows researchers to conduct scalable RL research using TorchBeast without requiring any additional programming knowledge beyond Python and PyTorch. This paper describes the TorchBeast design principles and implementation and demonstrates that it performs comparably to IMPALA on Atari. TorchBeast is available as an open-source package under the Apache 2.0 license at \url{https://github.com/facebookresearch/torchbeast}.",1
"In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.",0
"The objective of this study is to create a reinforcement learning algorithm that is both straightforward and flexible. The algorithm is designed to use conventional supervised learning techniques as subroutines and to employ maximum likelihood loss functions that are simple and convergent. The proposed method, dubbed as advantage-weighted regression (AWR), comprises two standard supervised learning stages: one to regress onto target values for a value function, and the other to regress onto weighted target actions for the policy. AWR is uncomplicated and versatile, works with both continuous and discrete actions, and can be implemented effortlessly on standard supervised learning methods. The study provides a theoretical foundation for AWR and analyzes its properties when incorporating off-policy data from experience replay. The efficiency of AWR is evaluated on a range of OpenAI Gym benchmark tasks, indicating that it performs equally well compared to other well-established state-of-the-art RL algorithms. AWR can also derive more effective policies than most off-policy algorithms when learning from purely static datasets without further environmental interactions. Additionally, the algorithm is demonstrated on challenging continuous control tasks with highly complex simulated characters.",1
"Many (but not all) approaches self-qualifying as ""meta-learning"" in deep learning and reinforcement learning fit a common pattern of approximating the solution to a nested optimization problem. In this paper, we give a formalization of this shared pattern, which we call GIMLI, prove its general requirements, and derive a general-purpose algorithm for implementing similar approaches. Based on this analysis and algorithm, we describe a library of our design, higher, which we share with the community to assist and enable future research into these kinds of meta-learning approaches. We end the paper by showcasing the practical applications of this framework and library through illustrative experiments and ablation studies which they facilitate.",0
"The majority of self-proclaimed ""meta-learning"" approaches in deep learning and reinforcement learning follow a similar method of approximating the solution to a nested optimization problem, though not all approaches do so. This paper presents a formalization of this shared approach, named GIMLI, and proves its general requirements. A versatile algorithm for implementing similar approaches is also derived. Using this analysis and algorithm, a library called higher has been developed and shared with the community to aid future research into meta-learning. The paper concludes by demonstrating the practical applications of this framework and library through illustrative experiments and ablation studies.",1
"Generalization and adaptation of learned skills to novel situations is a core requirement for intelligent autonomous robots. Although contextual reinforcement learning provides a principled framework for learning and generalization of behaviors across related tasks, it generally relies on uninformed sampling of environments from an unknown, uncontrolled context distribution, thus missing the benefits of structured, sequential learning. We introduce a novel relative entropy reinforcement learning algorithm that gives the agent the freedom to control the intermediate task distribution, allowing for its gradual progression towards the target context distribution. Empirical evaluation shows that the proposed curriculum learning scheme drastically improves sample efficiency and enables learning in scenarios with both broad and sharp target context distributions in which classical approaches perform sub-optimally.",0
"A crucial aspect of intelligent autonomous robots is their ability to apply learned skills to new situations. Contextual reinforcement learning is a framework that facilitates the learning and generalization of behaviors across similar tasks. However, this approach often relies on randomly sampling environments from an unknown context distribution, which limits the benefits of structured, sequential learning. Our new algorithm, relative entropy reinforcement learning, allows the agent to control the intermediate task distribution, allowing for a gradual progression towards the desired context distribution. Our evaluation demonstrates that this curriculum learning approach improves sample efficiency and is effective in scenarios with both broad and sharp target context distributions, where traditional methods perform poorly.",1
"With the advents of deep learning, improved image classification with complex discriminative models has been made possible. However, such deep models with increased complexity require a huge set of labeled samples to generalize the training. Such classification models can easily overfit when applied for medical images because of limited training data, which is a common problem in the field of medical image analysis. This paper proposes and investigates a reinforced classifier for improving the generalization under a few available training data. Partially following the idea of reinforcement learning, the proposed classifier uses a generalization-feedback from a subset of the training data to update its parameter instead of only using the conventional cross-entropy loss about the training data. We evaluate the improvement of the proposed classifier by applying it on three different classification problems against the standard deep classifiers equipped with existing overfitting-prevention techniques. Besides an overall improvement in classification performance, the proposed classifier showed remarkable characteristics of generalized learning, which can have great potential in medical classification tasks.",0
"The emergence of deep learning has enabled the development of more sophisticated models for image classification. However, these complex models require a large number of labeled samples to be effective. In the field of medical image analysis, limited training data can cause overfitting, which is a major issue. To address this problem, this paper introduces a reinforced classifier that utilizes generalization-feedback from a subset of the training data to update its parameters. This approach is inspired by reinforcement learning and differs from conventional cross-entropy loss. We evaluate the proposed classifier's effectiveness on three different classification problems and compare it to standard deep classifiers equipped with existing overfitting-prevention techniques. The reinforced classifier demonstrated improved classification performance and generalization capability, which could be advantageous in medical classification tasks.",1
"In recent studies on model-based reinforcement learning (MBRL), incorporating uncertainty in forward dynamics is a state-of-the-art strategy to enhance learning performance, making MBRLs competitive to cutting-edge model free methods, especially in simulated robotics tasks. Probabilistic ensembles with trajectory sampling (PETS) is a leading type of MBRL, which employs Bayesian inference to dynamics modeling and model predictive control (MPC) with stochastic optimization via the cross entropy method (CEM). In this paper, we propose a novel extension to the uncertainty-aware MBRL. Our main contributions are twofold: Firstly, we introduce a variational inference MPC, which reformulates various stochastic methods, including CEM, in a Bayesian fashion. Secondly, we propose a novel instance of the framework, called probabilistic action ensembles with trajectory sampling (PaETS). As a result, our Bayesian MBRL can involve multimodal uncertainties both in dynamics and optimal trajectories. In comparison to PETS, our method consistently improves asymptotic performance on several challenging locomotion tasks.",0
"Recent research on model-based reinforcement learning (MBRL) has shown that incorporating uncertainty in forward dynamics is an effective strategy to enhance learning performance and make MBRLs competitive with cutting-edge model-free methods, particularly in simulated robotics tasks. One of the leading types of MBRL is probabilistic ensembles with trajectory sampling (PETS), which uses Bayesian inference for dynamics modeling and model predictive control (MPC) with stochastic optimization via the cross entropy method (CEM). In this article, we propose a new extension to uncertainty-aware MBRL that has two main contributions. Firstly, we present a variational inference MPC that reformulates various stochastic methods, including CEM, in a Bayesian manner. Secondly, we introduce a new instance of the framework called probabilistic action ensembles with trajectory sampling (PaETS). Consequently, our Bayesian MBRL can involve multimodal uncertainties in both dynamics and optimal trajectories. Our approach consistently improves asymptotic performance on challenging locomotion tasks compared to PETS.",1
"We present a model-based framework for robot locomotion that achieves walking based on only 4.5 minutes (45,000 control steps) of data collected on a quadruped robot. To accurately model the robot's dynamics over a long horizon, we introduce a loss function that tracks the model's prediction over multiple timesteps. We adapt model predictive control to account for planning latency, which allows the learned model to be used for real time control. Additionally, to ensure safe exploration during model learning, we embed prior knowledge of leg trajectories into the action space. The resulting system achieves fast and robust locomotion. Unlike model-free methods, which optimize for a particular task, our planner can use the same learned dynamics for various tasks, simply by changing the reward function. To the best of our knowledge, our approach is more than an order of magnitude more sample efficient than current model-free methods.",0
"Our study presents a framework for robot locomotion that utilizes a model-based approach to achieve walking using only 4.5 minutes (45,000 control steps) of data gathered from a quadruped robot. We introduce a loss function that tracks the model's prediction over multiple timesteps to accurately model the robot's dynamics over a long horizon. We also modify model predictive control to consider planning latency, enabling the learned model to be utilized for real-time control. Furthermore, we integrate prior knowledge of leg trajectories into the action space to ensure safe exploration during model learning. The resulting system achieves fast and robust locomotion and can use the same learned dynamics for various tasks by altering the reward function. Our approach is over ten times more sample efficient than current model-free methods, as far as we know.",1
"We propose a new aggregation framework for approximate dynamic programming, which provides a connection with rollout algorithms, approximate policy iteration, and other single and multistep lookahead methods. The central novel characteristic is the use of a bias function $V$ of the state, which biases the values of the aggregate cost function towards their correct levels. The classical aggregation framework is obtained when $V\equiv0$, but our scheme works best when $V$ is a known reasonably good approximation to the optimal cost function $J^*$.   When $V$ is equal to the cost function $J_{\mu}$ of some known policy $\mu$ and there is only one aggregate state, our scheme is equivalent to the rollout algorithm based on $\mu$ (i.e., the result of a single policy improvement starting with the policy $\mu$). When $V=J_{\mu}$ and there are multiple aggregate states, our aggregation approach can be used as a more powerful form of improvement of $\mu$. Thus, when combined with an approximate policy evaluation scheme, our approach can form the basis for a new and enhanced form of approximate policy iteration.   When $V$ is a generic bias function, our scheme is equivalent to approximation in value space with lookahead function equal to $V$ plus a local correction within each aggregate state. The local correction levels are obtained by solving a low-dimensional aggregate DP problem, yielding an arbitrarily close approximation to $J^*$, when the number of aggregate states is sufficiently large. Except for the bias function, the aggregate DP problem is similar to the one of the classical aggregation framework, and its algorithmic solution by simulation or other methods is nearly identical to one for classical aggregation, assuming values of $V$ are available when needed.",0
"Our proposal is a novel aggregation framework for approximate dynamic programming that establishes a connection with other single and multistep lookahead methods, such as approximate policy iteration and rollout algorithms. The framework uses a bias function $V$ of the state to adjust the values of the aggregate cost function to their correct levels. While the classical aggregation framework is achieved when $V\equiv0$, our approach works best when $V$ approximates the optimal cost function $J^*$.   If $V$ equals the cost function $J_{\mu}$ of a known policy $\mu$ and there is only one aggregate state, our scheme is equivalent to the rollout algorithm based on $\mu$. On the other hand, if $V=J_{\mu}$ and there are multiple aggregate states, our aggregation approach can enhance $\mu$ improvement. When combined with an approximate policy evaluation scheme, our approach can form the basis for a new and enhanced form of approximate policy iteration.   If $V$ is a generic bias function, our framework is similar to approximation in value space. The lookahead function is $V$ plus a local correction within each aggregate state. The local correction levels are obtained by solving a low-dimensional aggregate DP problem, yielding an approximation to $J^*$ when the number of aggregate states is sufficiently large. The aggregate DP problem is similar to the classical aggregation framework's problem, except for the bias function. Our approach's algorithmic solution is nearly identical to the classical aggregation solution, assuming values of $V$ are available when needed.",1
"Efficient dispatching rule in manufacturing industry is key to ensure product on-time delivery and minimum past-due and inventory cost. Manufacturing, especially in the developed world, is moving towards on-demand manufacturing meaning a high mix, low volume product mix. This requires efficient dispatching that can work in dynamic and stochastic environments, meaning it allows for quick response to new orders received and can work over a disparate set of shop floor settings. In this paper we address this problem of dispatching in manufacturing. Using reinforcement learning (RL), we propose a new design to formulate the shop floor state as a 2-D matrix, incorporate job slack time into state representation, and design lateness and tardiness rewards function for dispatching purpose. However, maintaining a separate RL model for each production line on a manufacturing shop floor is costly and often infeasible. To address this, we enhance our deep RL model with an approach for dispatching policy transfer. This increases policy generalization and saves time and cost for model training and data collection. Experiments show that: (1) our approach performs the best in terms of total discounted reward and average lateness, tardiness, (2) the proposed policy transfer approach reduces training time and increases policy generalization.",0
"To ensure timely delivery of products and reduce inventory costs, efficient dispatching rules are crucial in the manufacturing industry. Due to the increasing shift towards on-demand manufacturing with a high mix of low volume products, dispatching must be dynamic and able to respond quickly to new orders in a variety of shop floor settings. This paper addresses the dispatching challenge in manufacturing by proposing a new design using reinforcement learning. The approach involves formulating the shop floor state as a 2-D matrix, including job slack time in the state representation, and designing rewards functions for dispatching purposes. Maintaining separate RL models for each production line is expensive and impractical. To overcome this, the deep RL model is enhanced with a policy transfer approach to increase policy generalization, reduce training time, and save costs. The experiments show that our approach performs the best in terms of total discounted reward and average lateness and tardiness, and the proposed policy transfer approach improves policy generalization and reduces training time.",1
"Experimentally, it has been observed that humans and animals often make decisions that do not maximize their expected utility, but rather choose outcomes randomly, with probability proportional to expected utility. Probability matching, as this strategy is called, is equivalent to maximum entropy reinforcement learning (MaxEnt RL). However, MaxEnt RL does not optimize expected utility. In this paper, we formally show that MaxEnt RL does optimally solve certain classes of control problems with variability in the reward function. In particular, we show (1) that MaxEnt RL can be used to solve a certain class of POMDPs, and (2) that MaxEnt RL is equivalent to a two-player game where an adversary chooses the reward function. These results suggest a deeper connection between MaxEnt RL, robust control, and POMDPs, and provide insight for the types of problems for which we might expect MaxEnt RL to produce effective solutions. Specifically, our results suggest that domains with uncertainty in the task goal may be especially well-suited for MaxEnt RL methods.",0
"Observations have shown that humans and animals often make decisions that are not optimal for their expected utility, instead randomly selecting outcomes with a probability proportional to the expected utility. This approach is known as probability matching and is equivalent to maximum entropy reinforcement learning (MaxEnt RL). However, MaxEnt RL does not optimize expected utility. This paper demonstrates that MaxEnt RL is optimal for solving certain control problems with varying reward functions. Specifically, the study shows that MaxEnt RL can be used to solve a category of POMDPs and is equivalent to a two-player game where an adversary selects the reward function. These findings suggest a deeper association between MaxEnt RL, robust control, and POMDPs, and provide insight into the types of problems that MaxEnt RL is effective in solving. The results suggest that MaxEnt RL methods may be particularly well-suited for domains with ambiguity in the task objective.",1
"Graph representation learning, aiming to learn low-dimensional representations which capture the geometric dependencies between nodes in the original graph, has gained increasing popularity in a variety of graph analysis tasks, including node classification and link prediction. Existing representation learning methods based on graph neural networks and their variants rely on the aggregation of neighborhood information, which makes it sensitive to noises in the graph. In this paper, we propose Graph Denoising Policy Network (short for GDPNet) to learn robust representations from noisy graph data through reinforcement learning. GDPNet first selects signal neighborhoods for each node, and then aggregates the information from the selected neighborhoods to learn node representations for the down-stream tasks. Specifically, in the signal neighborhood selection phase, GDPNet optimizes the neighborhood for each target node by formulating the process of removing noisy neighborhoods as a Markov decision process and learning a policy with task-specific rewards received from the representation learning phase. In the representation learning phase, GDPNet aggregates features from signal neighbors to generate node representations for down-stream tasks, and provides task-specific rewards to the signal neighbor selection phase. These two phases are jointly trained to select optimal sets of neighbors for target nodes with maximum cumulative task-specific rewards, and to learn robust representations for nodes. Experimental results on node classification task demonstrate the effectiveness of GDNet, outperforming the state-of-the-art graph representation learning methods on several well-studied datasets. Additionally, GDPNet is mathematically equivalent to solving the submodular maximizing problem, which theoretically guarantees the best approximation to the optimal solution with GDPNet.",0
"Learning low-dimensional representations that capture the geometric relationships between nodes in the original graph has become increasingly popular for various graph analysis tasks, such as node classification and link prediction. Existing representation learning methods based on graph neural networks and their variants depend on neighborhood information aggregation, which makes them vulnerable to graph noise. This paper introduces the Graph Denoising Policy Network (GDPNet) to learn robust representations from noisy graph data through reinforcement learning. GDPNet selects signal neighborhoods for each node and aggregates the information from these neighborhoods to learn node representations for downstream tasks. The process involves two phases: signal neighborhood selection and representation learning. In the signal neighborhood selection phase, GDPNet optimizes the neighborhood for each target node by formulating the process of removing noisy neighborhoods as a Markov decision process and learning a policy with task-specific rewards. In the representation learning phase, GDPNet aggregates features from signal neighbors to generate node representations for downstream tasks and provides task-specific rewards to the signal neighbor selection phase. These two phases are jointly trained to select optimal sets of neighbors for target nodes with maximum cumulative task-specific rewards and learn robust node representations. The effectiveness of GDPNet is demonstrated through experimental results on node classification tasks, where it outperforms the state-of-the-art graph representation learning methods on several well-studied datasets. Additionally, GDPNet is mathematically equivalent to solving the submodular maximizing problem, which theoretically guarantees the best approximation to the optimal solution with GDPNet.",1
"Widely-used deep reinforcement learning algorithms have been shown to fail in the batch setting--learning from a fixed data set without interaction with the environment. Following this result, there have been several papers showing reasonable performances under a variety of environments and batch settings. In this paper, we benchmark the performance of recent off-policy and batch reinforcement learning algorithms under unified settings on the Atari domain, with data generated by a single partially-trained behavioral policy. We find that under these conditions, many of these algorithms underperform DQN trained online with the same amount of data, as well as the partially-trained behavioral policy. To introduce a strong baseline, we adapt the Batch-Constrained Q-learning algorithm to a discrete-action setting, and show it outperforms all existing algorithms at this task.",0
"Deep reinforcement learning algorithms that are widely used have been found to be ineffective in the batch setting, which involves learning from a fixed dataset without interacting with the environment. However, there have been various papers that have demonstrated reasonable performances across different environments and batch settings. This study aims to evaluate the performance of recent off-policy and batch reinforcement learning algorithms in a unified setting using the Atari domain and data generated by a partially-trained behavioral policy. The results indicate that many of these algorithms perform worse than DQN, which was trained online with the same amount of data, and the partially-trained behavioral policy. To establish a strong baseline, the Batch-Constrained Q-learning algorithm was adapted to a discrete-action setting and shown to outperform all existing algorithms in this task.",1
"Prior work on training generative Visual Dialog models with reinforcement learning(Das et al.) has explored a Qbot-Abot image-guessing game and shown that this 'self-talk' approach can lead to improved performance at the downstream dialog-conditioned image-guessing task. However, this improvement saturates and starts degrading after a few rounds of interaction, and does not lead to a better Visual Dialog model. We find that this is due in part to repeated interactions between Qbot and Abot during self-talk, which are not informative with respect to the image. To improve this, we devise a simple auxiliary objective that incentivizes Qbot to ask diverse questions, thus reducing repetitions and in turn enabling Abot to explore a larger state space during RL ie. be exposed to more visual concepts to talk about, and varied questions to answer. We evaluate our approach via a host of automatic metrics and human studies, and demonstrate that it leads to better dialog, ie. dialog that is more diverse (ie. less repetitive), consistent (ie. has fewer conflicting exchanges), fluent (ie. more human-like),and detailed, while still being comparably image-relevant as prior work and ablations.",0
"Previous research has investigated the use of reinforcement learning to train generative Visual Dialog models in an image-guessing game between Qbot and Abot, also known as self-talk. This approach has shown improvements in downstream dialog-conditioned image-guessing tasks. However, this improvement is limited and declines after a few rounds of interaction, failing to enhance the Visual Dialog model. This is partly due to the repetitive interactions between Qbot and Abot during self-talk, which do not provide any useful information about the image. In response, we have developed a simple auxiliary objective that encourages Qbot to ask diverse questions, reducing repetition and enabling Abot to explore a wider range of visual concepts to discuss and varied questions to answer during RL. We have evaluated our method using automatic metrics and human studies, demonstrating that it leads to better dialog that is more diverse, consistent, fluent, and detailed while remaining comparably image-relevant to prior research and ablations.",1
"We study the variance of the REINFORCE policy gradient estimator in environments with continuous state and action spaces, linear dynamics, quadratic cost, and Gaussian noise. These simple environments allow us to derive bounds on the estimator variance in terms of the environment and noise parameters. We compare the predictions of our bounds to the empirical variance in simulation experiments.",0
"Our focus is on analyzing the variance of the REINFORCE policy gradient estimator in scenarios featuring continuous state and action spaces, linear dynamics, quadratic cost, and Gaussian noise. By examining these basic environments, we can establish limitations on the estimator variance based on the environment and noise parameters. Our analysis is supported by simulation experiments that compare the predicted variance with the actual empirical variance.",1
"Robotics has proved to be an indispensable tool in many industrial as well as social applications, such as warehouse automation, manufacturing, disaster robotics, etc. In most of these scenarios, damage to the agent while accomplishing mission-critical tasks can result in failure. To enable robotic adaptation in such situations, the agent needs to adopt policies which are robust to a diverse set of damages and must do so with minimum computational complexity. We thus propose a damage aware control architecture which diagnoses the damage prior to gait selection while also incorporating domain randomization in the damage space for learning a robust policy. To implement damage awareness, we have used a Long Short Term Memory based supervised learning network which diagnoses the damage and predicts the type of damage. The main novelty of this approach is that only a single policy is trained to adapt against a wide variety of damages and the diagnosis is done in a single trial at the time of damage.",0
"Robotics has become a crucial tool in various industries and social applications, including warehouse automation, manufacturing, and disaster robotics. In most cases, any damage suffered by the robot while performing critical tasks can lead to mission failure. In order to facilitate robotic adaptation in such circumstances, the robot must employ policies that are robust enough to withstand a diverse range of damages while minimizing computational complexity. Our proposed solution is a damage-aware control architecture that diagnoses damage before gait selection and incorporates domain randomization for learning a resilient policy. To achieve damage awareness, we utilized a Long Short Term Memory-based supervised learning network that diagnoses and predicts the type of damage. The uniqueness of our approach lies in training a single policy that can adapt to a wide range of damages and diagnose them in a single trial.",1
"We investigate using reinforcement learning agents as generative models of images (extending arXiv:1804.01118). A generative agent controls a simulated painting environment, and is trained with rewards provided by a discriminator network simultaneously trained to assess the realism of the agent's samples, either unconditional or reconstructions. Compared to prior work, we make a number of improvements to the architectures of the agents and discriminators that lead to intriguing and at times surprising results. We find that when sufficiently constrained, generative agents can learn to produce images with a degree of visual abstraction, despite having only ever seen real photographs (no human brush strokes). And given enough time with the painting environment, they can produce images with considerable realism. These results show that, under the right circumstances, some aspects of human drawing can emerge from simulated embodiment, without the need for external supervision, imitation or social cues. Finally, we note the framework's potential for use in creative applications.",0
"Our study delves into the use of reinforcement learning agents as generative models for images, which builds upon the work discussed in arXiv:1804.01118. In this approach, a generative agent manages a simulated painting environment and receives rewards from a discriminator network that evaluates the realism of the agent's samples, regardless of whether they are unconditional or reconstructions. We improved the agents' and discriminators' architectures, which resulted in intriguing and even unexpected outcomes. Our findings demonstrate that generative agents can learn to create images with a certain level of visual abstraction if adequately constrained, even though they have never seen anything other than actual photographs and not human-made brush strokes. Furthermore, given enough time within the painting environment, these agents can produce images of considerable realism. These results suggest that specific aspects of human drawing can emerge from simulated embodiment without external supervision, imitation, or social cues. Finally, we highlight the potential of this framework for use in creative applications.",1
"Inverse reinforcement learning (IRL) is used to infer the reward function from the actions of an expert running a Markov Decision Process (MDP). A novel approach using variational inference for learning the reward function is proposed in this research. Using this technique, the intractable posterior distribution of the continuous latent variable (the reward function in this case) is analytically approximated to appear to be as close to the prior belief while trying to reconstruct the future state conditioned on the current state and action. The reward function is derived using a well-known deep generative model known as Conditional Variational Auto-encoder (CVAE) with Wasserstein loss function, thus referred to as Conditional Wasserstein Auto-encoder-IRL (CWAE-IRL), which can be analyzed as a combination of the backward and forward inference. This can then form an efficient alternative to the previous approaches to IRL while having no knowledge of the system dynamics of the agent. Experimental results on standard benchmarks such as objectworld and pendulum show that the proposed algorithm can effectively learn the latent reward function in complex, high-dimensional environments.",0
"The research proposes a new method for applying inverse reinforcement learning (IRL) to determine the reward function of a Markov Decision Process (MDP) based on the actions of an expert. This approach utilizes variational inference to approximate the posterior distribution of the continuous latent variable, which is the reward function, and reconstruct the future state based on the current state and action. The reward function is learned using a Conditional Variational Auto-encoder (CVAE) with Wasserstein loss function, which is referred to as Conditional Wasserstein Auto-encoder-IRL (CWAE-IRL). This approach combines backward and forward inference and is an efficient alternative to previous IRL methods. The proposed algorithm has been tested on standard benchmarks and has proven to be effective in learning the latent reward function in complex, high-dimensional environments without prior knowledge of the agent's system dynamics.",1
"In this paper, we study the problem of image recognition with non-differentiable constraints. A lot of real-life recognition applications require a rich output structure with deterministic constraints that are discrete or modeled by a non-differentiable function. A prime example is recognizing digit sequences, which are restricted by such rules (e.g., \textit{container code detection}, \textit{social insurance number recognition}, etc.). We investigate the usefulness of adding non-differentiable constraints in learning for the task of digit sequence recognition. Toward this goal, we synthesize six different datasets from MNIST and Cropped SVHN, with three discrete rules inspired by real-life protocols. To deal with the non-differentiability of these rules, we propose a reinforcement learning approach based on the policy gradient method. We find that incorporating this rule-based reinforcement can effectively increase the accuracy for all datasets and provide a good inductive bias which improves the model even with limited data. On one of the datasets, MNIST\_Rule2, models trained with rule-based reinforcement increase the accuracy by 4.7\% for 2000 samples and 23.6\% for 500 samples. We further test our model against synthesized adversarial examples, e.g., blocking out digits, and observe that adding our rule-based reinforcement increases the model robustness with a relatively smaller performance drop.",0
"The objective of this study is to explore the problem of image recognition when dealing with non-differentiable constraints. Many real-world recognition tasks involve deterministic constraints that are either discrete or modeled by a non-differentiable function. For instance, digit sequences are subject to such rules, as seen in container code detection or social insurance number recognition. To investigate the potential benefits of incorporating non-differentiable constraints in digit sequence recognition, we generate six datasets from MNIST and Cropped SVHN with three discrete rules inspired by real-life protocols. To address the non-differentiability of these rules, we propose a reinforcement learning approach that uses the policy gradient method. Our results show that integrating rule-based reinforcement can significantly improve the accuracy of all datasets and provide a favorable inductive bias that enhances the model's performance even with limited data. For example, on the MNIST\_Rule2 dataset, models trained with rule-based reinforcement improved accuracy by 4.7\% for 2000 samples and 23.6\% for 500 samples. Furthermore, we evaluate our model's robustness against synthesized adversarial examples, such as digit blockouts, and demonstrate that incorporating our rule-based reinforcement enhances the model's robustness while minimizing the performance loss.",1
"Can the success of reinforcement learning methods for simple combinatorial optimization problems be extended to multi-robot sequential assignment planning? In addition to the challenge of achieving near-optimal performance in large problems, transferability to an unseen number of robots and tasks is another key challenge for real-world applications. In this paper, we suggest a method that achieves the first success in both challenges for robot/machine scheduling problems.   Our method comprises of three components. First, we show a robot scheduling problem can be expressed as a random probabilistic graphical model (PGM). We develop a mean-field inference method for random PGM and use it for Q-function inference. Second, we show that transferability can be achieved by carefully designing two-step sequential encoding of problem state. Third, we resolve the computational scalability issue of fitted Q-iteration by suggesting a heuristic auction-based Q-iteration fitting method enabled by transferability we achieved.   We apply our method to discrete-time, discrete space problems (Multi-Robot Reward Collection (MRRC)) and scalably achieve 97% optimality with transferability. This optimality is maintained under stochastic contexts. By extending our method to continuous time, continuous space formulation, we claim to be the first learning-based method with scalable performance among multi-machine scheduling problems; our method scalability achieves comparable performance to popular metaheuristics in Identical parallel machine scheduling (IPMS) problems.",0
"Is it possible to extend the success of reinforcement learning techniques for simple combinatorial optimization problems to multi-robot sequential assignment planning? Achieving near-optimal results in large problems and the ability to transfer to an unknown number of robots and tasks are the key challenges for real-world applications. In this study, we propose a method that successfully addresses both challenges in robot/machine scheduling problems. Our approach consists of three components. Firstly, we demonstrate that a robot scheduling problem can be represented as a random probabilistic graphical model (PGM) and we develop a mean-field inference method for random PGM to use for Q-function inference. Secondly, we demonstrate that transferability can be achieved by designing a two-step sequential encoding of the problem state. Lastly, we solve the computational scalability issue of fitted Q-iteration by proposing a heuristic auction-based Q-iteration fitting method enabled by the transferability we achieved. We apply our method to discrete-time, discrete space problems (Multi-Robot Reward Collection (MRRC)) and achieve 97% optimality with transferability, which remains optimal under stochastic contexts. By extending our method to continuous time, continuous space formulation, we claim to be the first learning-based method with scalable performance among multi-machine scheduling problems. Our method's scalability achieves comparable performance to popular metaheuristics in Identical parallel machine scheduling (IPMS) problems.",1
"This study investigates the use of reinforcement learning to guide a general purpose cache manager decisions. Cache managers directly impact the overall performance of computer systems. They govern decisions about which objects should be cached, the duration they should be cached for, and decides on which objects to evict from the cache if it is full. These three decisions impact both the cache hit rate and size of the storage that is needed to achieve that cache hit rate. An optimal cache manager will avoid unnecessary operations, maximise the cache hit rate which results in fewer round trips to a slower backend storage system, and minimise the size of storage needed to achieve a high hit-rate.   This project investigates using reinforcement learning in cache management by designing three separate agents for each of the cache manager tasks. Furthermore, the project investigates two advanced reinforcement learning architectures for multi-decision problems: a single multi-task agent and a multi-agent. We also introduce a framework to simplify the modelling of computer systems problems as a reinforcement learning task. The framework abstracts delayed experiences observations and reward assignment in computer systems while providing a flexible way to scale to multiple agents.   Simulation results based on an established database benchmark system show that reinforcement learning agents can achieve a higher cache hit rate over heuristic driven algorithms while minimising the needed space. They are also able to adapt to a changing workload and dynamically adjust their caching strategy accordingly. The proposed cache manager model is generic and applicable to other types of caches, such as file system caches. This project is the first, to our knowledge, to model cache manager decisions as a multi-task control problem.",0
"The objective of this research is to investigate the potential of reinforcement learning in guiding the decision-making process of cache managers, which have a direct impact on the overall performance of computer systems. Cache managers are responsible for determining which objects should be cached, for how long, and which ones to evict when the cache is full. These decisions affect both the cache hit rate and the required storage size. An optimal cache manager must minimize unnecessary operations, maximize the cache hit rate, and reduce the storage size required to achieve a high hit-rate. This study proposes the use of reinforcement learning to design three distinct agents to manage each of the cache manager tasks. Moreover, the study explores two advanced reinforcement learning architectures for multi-decision problems, namely a single multi-task agent and a multi-agent. A framework is also introduced to simplify the modeling of computer system problems as a reinforcement learning task, with delayed experience observations and reward assignment. Simulation results demonstrate that reinforcement learning agents can achieve a higher cache hit rate while minimizing the necessary space, and can adapt to changing workloads by dynamically adjusting their caching strategy. This generic cache manager model is applicable to other types of caches, such as file system caches, and is the first to model cache manager decisions as a multi-task control problem.",1
"The common pipeline in autonomous driving systems is highly modular and includes a perception component which extracts lists of surrounding objects and passes these lists to a high-level decision component. In this case, leveraging the benefits of deep reinforcement learning for high-level decision making requires special architectures to deal with multiple variable-length sequences of different object types, such as vehicles, lanes or traffic signs. At the same time, the architecture has to be able to cover interactions between traffic participants in order to find the optimal action to be taken. In this work, we propose the novel Deep Scenes architecture, that can learn complex interaction-aware scene representations based on extensions of either 1) Deep Sets or 2) Graph Convolutional Networks. We present the Graph-Q and DeepScene-Q off-policy reinforcement learning algorithms, both outperforming state-of-the-art methods in evaluations with the publicly available traffic simulator SUMO.",0
"Autonomous driving systems typically have a modular pipeline consisting of a perception component that identifies surrounding objects and a high-level decision component that processes this information. However, implementing deep reinforcement learning for decision making requires specialized architectures that can handle various variable-length sequences of objects, such as vehicles, lanes, and traffic signs, while also considering interactions between traffic participants to determine the best course of action. We propose a new architecture called Deep Scenes, which uses either Deep Sets or Graph Convolutional Networks to learn complex interaction-aware scene representations. Our off-policy reinforcement learning algorithms, Graph-Q and DeepScene-Q, outperform existing methods in evaluations with the SUMO traffic simulator.",1
"Instability and slowness are two main problems in deep reinforcement learning. Even if proximal policy optimization (PPO) is the state of the art, it still suffers from these two problems. We introduce an improved algorithm based on proximal policy optimization, mixed distributed proximal policy optimization (MDPPO), and show that it can accelerate and stabilize the training process. In our algorithm, multiple different policies train simultaneously and each of them controls several identical agents that interact with environments. Actions are sampled by each policy separately as usual, but the trajectories for the training process are collected from all agents, instead of only one policy. We find that if we choose some auxiliary trajectories elaborately to train policies, the algorithm will be more stable and quicker to converge especially in the environments with sparse rewards.",0
"Deep reinforcement learning faces two major issues of instability and sluggishness. Despite being the current state-of-the-art, proximal policy optimization (PPO) algorithm is also prone to these problems. We propose a refined version of PPO, called mixed distributed proximal policy optimization (MDPPO), which enhances the training process by increasing its speed and robustness. Our approach involves training several different policies together, each controlling multiple agents interacting with the environments. While actions are still sampled separately by each policy, the trajectories for training are collected from all agents instead of just one. We observe that carefully selecting auxiliary trajectories for policy training results in greater stability and faster convergence, especially in environments with sparse rewards.",1
"Traffic signal control has long been considered as a critical topic in intelligent transportation systems. Most existing learning methods mainly focus on isolated intersections and suffer from inefficient training. This paper aims at the cooperative control for large scale multi-intersection traffic signal, in which a novel end-to-end learning based model is established and the efficient training method is proposed correspondingly. In the proposed model, the input traffic status in multi-intersections is represented by a tensor, which not only significantly reduces dimensionality than using a single matrix but also avoids information loss. For the output, a multidimensional boolean vector is employed for the control policy to indicate whether the signal state changes or not, which simplifies the representation and abides the practical phase changing rules. In the proposed model, a multi-task learning structure is used to get the cooperative policy by learning. Instead of only using the reinforcement learning to train the model, we employ imitation learning to integrate a rule based model with neural networks to do the pre-training, which provides a reliable and satisfactory stage solution and greatly accelerates the convergence. Afterwards, the reinforcement learning method is adopted to continue the fine training, where proximal policy optimization algorithm is incorporated to solve the policy collapse problem in multi-dimensional output situation. In numerical experiments, the advantages of the proposed model are demonstrated with comparison to the related state-of-the-art methods.",0
"The intelligent transportation system has highlighted the importance of traffic signal control, particularly in dealing with multiple intersections. However, current learning methods are not efficient in training and mainly focus on isolated intersections. To address this issue, this paper presents a novel end-to-end learning model that aims to control large-scale multi-intersection traffic signals. The proposed model utilizes a tensor to represent input traffic status, which reduces dimensionality and prevents information loss. For the output, a multidimensional boolean vector is used to indicate signal state changes, simplifying representation and abiding by practical phase changing rules. A multi-task learning structure is employed to learn the cooperative policy, which integrates a rule-based model with neural networks through imitation learning to provide a reliable pre-training solution that accelerates convergence. Reinforcement learning is then utilized for fine training, incorporating the proximal policy optimization algorithm to solve the policy collapse problem in multi-dimensional output situations. Numerical experiments demonstrate the advantages of the proposed model over related state-of-the-art methods.",1
"As integrated circuits have become progressively more complex, constrained random stimulus has become ubiquitous as a means of stimulating a designs functionality and ensuring it fully meets expectations. In theory, random stimulus allows all possible combinations to be exercised given enough time, but in practice with highly complex designs a purely random approach will have difficulty in exercising all possible combinations in a timely fashion. As a result it is often necessary to steer the Design Verification (DV) environment to generate hard to hit combinations. The resulting constrained-random approach is powerful but often relies on extensive human expertise to guide the DV environment in order to fully exercise the design. As designs become more complex, the guidance aspect becomes progressively more challenging and time consuming often resulting in design schedules in which the verification time to hit all possible design coverage points is the dominant schedule limitation. This paper describes an approach which leverages existing constrained-random DV environment tools but which further enhances them using supervised learning and reinforcement learning techniques. This approach provides better than random results in a highly automated fashion thereby ensuring DV objectives of full design coverage can be achieved on an accelerated timescale and with fewer resources.   Two hardware verification examples are presented, one of a Cache Controller design and one using the open-source RISCV-Ariane design and Google's RISCV Random Instruction Generator. We demonstrate that a machine-learning based approach can perform significantly better on functional coverage and reaching complex hard-to-hit states than a random or constrained-random approach.",0
"With the increasing complexity of integrated circuits, constrained random stimulus has become a common method of ensuring that a design functions as intended. While random stimulus can theoretically cover all possibilities given sufficient time, highly complex designs require a more targeted approach to fully exercise them. This is where Design Verification (DV) comes in, but it often requires human expertise to guide the process. As designs become more complex, this guidance becomes more challenging and time-consuming, causing delays in the design schedule. To overcome this, a new approach that combines existing constrained-random DV tools with supervised and reinforcement learning techniques has been developed. This approach has been tested on two hardware verification examples, demonstrating that it outperforms random and constrained-random approaches in achieving full design coverage on an accelerated timescale with fewer resources.",1
"Graph Neural Networks (GNNs) have boosted the performance of many graph related tasks such as node classification and graph classification. Recent researches show that graph neural networks are vulnerable to adversarial attacks, which deliberately add carefully created unnoticeable perturbation to the graph structure. The perturbation is usually created by adding/deleting a few edges, which might be noticeable even when the number of edges modified is small. In this paper, we propose a graph rewiring operation which affects the graph in a less noticeable way compared to adding/deleting edges. We then use reinforcement learning to learn the attack strategy based on the proposed rewiring operation. Experiments on real world graphs demonstrate the effectiveness of the proposed framework. To understand the proposed framework, we further analyze how its generated perturbation to the graph structure affects the output of the target model.",0
"The use of Graph Neural Networks (GNNs) has led to improved performance in various graph-related tasks, such as node classification and graph classification. However, recent studies have revealed that these networks are prone to adversarial attacks, where minor yet carefully crafted modifications are added to the graph structure. These perturbations are usually achieved by adding or deleting a few edges, which can be noticeable even with minimal changes. To address this issue, we propose a graph rewiring technique that causes less noticeable changes to the graph structure than the addition or deletion of edges. We then employ reinforcement learning to develop an attack strategy based on the proposed rewiring approach. Our experiments on real-world graphs demonstrate the effectiveness of our framework. Additionally, we analyze how the perturbation generated by our approach affects the output of the target model to further comprehend our proposed framework.",1
"In this paper we derive an efficient method for computing the indices associated with an asymptotically optimal upper confidence bound algorithm (MDP-UCB) of Burnetas and Katehakis (1997) that only requires solving a system of two non-linear equations with two unknowns, irrespective of the cardinality of the state space of the Markovian decision process (MDP). In addition, we develop a similar acceleration for computing the indices for the MDP-Deterministic Minimum Empirical Divergence (MDP-DMED) algorithm developed in Cowan et al. (2019), based on ideas from Honda and Takemura (2011), that involves solving a single equation of one variable. We provide experimental results demonstrating the computational time savings and regret performance of these algorithms. In these comparison we also consider the Optimistic Linear Programming (OLP) algorithm (Tewari and Bartlett, 2008) and a method based on Posterior sampling (MDP-PS).",0
"This paper presents a method to efficiently calculate the indices associated with the MDP-UCB algorithm by Burnetas and Katehakis (1997), which only involves solving a system of two non-linear equations with two unknowns. This method can be applied regardless of the state space cardinality of the MDP. Furthermore, we introduce a similar approach for computing the indices of the MDP-DMED algorithm developed in Cowan et al. (2019), inspired by Honda and Takemura (2011), which only requires solving a single equation with one variable. We conduct experiments to compare the computational time and regret performance of these algorithms, alongside the OLP algorithm by Tewari and Bartlett (2008) and the MDP-PS method.",1
"We consider the networked multi-agent reinforcement learning (MARL) problem in a fully decentralized setting, where agents learn to coordinate to achieve the joint success. This problem is widely encountered in many areas including traffic control, distributed control, and smart grids. We assume that the reward function for each agent can be different and observed only locally by the agent itself. Furthermore, each agent is located at a node of a communication network and can exchanges information only with its neighbors. Using softmax temporal consistency and a decentralized optimization method, we obtain a principled and data-efficient iterative algorithm. In the first step of each iteration, an agent computes its local policy and value gradients and then updates only policy parameters. In the second step, the agent propagates to its neighbors the messages based on its value function and then updates its own value function. Hence we name the algorithm value propagation. We prove a non-asymptotic convergence rate 1/T with the nonlinear function approximation. To the best of our knowledge, it is the first MARL algorithm with convergence guarantee in the control, off-policy and non-linear function approximation setting. We empirically demonstrate the effectiveness of our approach in experiments.",0
"The problem of networked multi-agent reinforcement learning (MARL) is examined in a fully decentralized environment, where agents aim to coordinate in order to achieve success together. This issue is commonly encountered in various fields such as traffic control, distributed control, and smart grids. It is assumed that each agent has a unique reward function and can only observe it locally. Additionally, each agent is positioned at a node of a communication network and is only able to exchange information with its neighbors. By utilizing softmax temporal consistency and a decentralized optimization method, an iterative algorithm that is principled and data-efficient is obtained. In the first step of each iteration, an agent calculates its local policy and value gradients and modifies only policy parameters. In the second step, the agent disseminates messages to its neighbors based on its value function and then updates its own value function. This algorithm is called value propagation. With nonlinear function approximation, we establish a non-asymptotic convergence rate of 1/T. As far as we know, this is the first MARL algorithm that has a guarantee of convergence in a control, off-policy, and nonlinear function approximation setting. Through experimentation, we demonstrate the effectiveness of our approach.",1
"Although deep reinforcement learning agents have produced impressive results in many domains, their decision making is difficult to explain to humans. To address this problem, past work has mainly focused on explaining why an action was chosen in a given state. A different type of explanation that is useful is a counterfactual, which deals with ""what if?"" scenarios. In this work, we introduce the concept of a counterfactual state to help humans gain a better understanding of what would need to change (minimally) in an Atari game image for the agent to choose a different action. We introduce a novel method to create counterfactual states from a generative deep learning architecture. In addition, we evaluate the effectiveness of counterfactual states on human participants who are not machine learning experts. Our user study results suggest that our generated counterfactual states are useful in helping non-expert participants gain a better understanding of an agent's decision making process.",0
"Even though deep reinforcement learning agents have achieved impressive results in various fields, their decision-making process is complex for humans to comprehend. To solve this issue, previous studies have primarily focused on explaining why a specific action was chosen in a particular state. However, an additional helpful explanation type is a counterfactual that deals with hypothetical scenarios. This study introduces the concept of a counterfactual state to assist humans in comprehending what minimal modifications would be necessary in an Atari game image for the agent to make a different choice. The authors propose a novel approach to generate counterfactual states using a deep learning architecture. They also assess the efficacy of counterfactual states on non-experts in machine learning. The user study results indicate that the proposed counterfactual states are helpful in aiding non-expert participants to comprehend an agent's decision-making process.",1
"There have been numerous advances in reinforcement learning, but the typically unconstrained exploration of the learning process prevents the adoption of these methods in many safety critical applications. Recent work in safe reinforcement learning uses idealized models to achieve their guarantees, but these models do not easily accommodate the stochasticity or high-dimensionality of real world systems. We investigate how prediction provides a general and intuitive framework to constraint exploration, and show how it can be used to safely learn intersection handling behaviors on an autonomous vehicle.",0
"Although there have been significant improvements in reinforcement learning, the unrestricted exploration of the learning process makes it difficult to apply these methods in safety-critical situations. Safe reinforcement learning has recently used idealized models to ensure safety, but these models do not easily accommodate the real-world systems' stochasticity or high-dimensionality. Our study explores how prediction can offer a comprehensive and easy-to-understand structure to limit exploration. We demonstrate how it can be used to teach an autonomous vehicle intersection handling behaviors safely.",1
"Improving sample efficiency has been a longstanding goal in reinforcement learning. In this paper, we propose the $\mathtt{VRMPO}$: a sample efficient policy gradient method with stochastic mirror descent. A novel variance reduced policy gradient estimator is the key of $\mathtt{VRMPO}$ to improve sample efficiency. Our $\mathtt{VRMPO}$ needs only $\mathcal{O}(\epsilon^{-3})$ sample trajectories to achieve an $\epsilon$-approximate first-order stationary point, which matches the best-known sample complexity. We conduct extensive experiments to show our algorithm outperforms state-of-the-art policy gradient methods in various settings.",0
"For a long time, the reinforcement learning community has aimed to enhance sample efficiency. This article puts forth the $\mathtt{VRMPO}$, which is a policy gradient method with stochastic mirror descent that is efficient in terms of samples. The $\mathtt{VRMPO}$ relies on a new variance reduced policy gradient estimator, which is crucial in improving sample efficiency. Our approach achieves an $\epsilon$-approximate first-order stationary point with just $\mathcal{O}(\epsilon^{-3})$ sample trajectories, matching the best-known sample complexity. We conducted extensive experiments to demonstrate that our algorithm outperforms state-of-the-art policy gradient methods in various scenarios.",1
"Policy gradient based reinforcement learning algorithms coupled with neural networks have shown success in learning complex policies in the model free continuous action space control setting. However, explicitly parameterized policies are limited by the scope of the chosen parametric probability distribution. We show that alternatively to the likelihood based policy gradient, a related objective can be optimized through advantage weighted quantile regression. Our approach models the policy implicitly in the network, which gives the agent the freedom to approximate any distribution in each action dimension, not limiting its capabilities to the commonly used unimodal Gaussian parameterization. This broader spectrum of policies makes our algorithm suitable for problems where Gaussian policies cannot fit the optimal policy. Moreover, our results on the MuJoCo physics simulator benchmarks are comparable or superior to state-of-the-art on-policy methods.",0
"The use of policy gradient based reinforcement learning algorithms in conjunction with neural networks has proven to be effective in acquiring complex policies in the continuous action space control setting without the need for a model. However, explicitly parameterized policies are restricted by the range of the selected parametric probability distribution. Our study reveals that rather than utilizing the likelihood based policy gradient, a parallel objective can be optimized through advantage weighted quantile regression. In our approach, the policy is implicitly modeled in the network, granting the agent the flexibility to approximate any distribution in each action dimension, rather than being restricted to the commonly employed unimodal Gaussian parameterization. This expanded range of policies makes our algorithm appropriate for situations where Gaussian policies are incapable of fitting the optimal policy. Additionally, our results on the MuJoCo physics simulator benchmarks are comparable or superior to state-of-the-art on-policy methods.",1
"In this paper, we aim to tackle the task of semi-supervised video object segmentation across a sequence of frames where only the ground-truth segmentation of the first frame is provided. The challenges lie in how to online update the segmentation model initialized from the first frame adaptively and accurately, even in presence of multiple confusing instances or large object motion. The existing approaches rely on selecting the region of interest for model update, which however, is rough and inflexible, leading to performance degradation. To overcome this limitation, we propose a novel approach which utilizes reinforcement learning to select optimal adaptation areas for each frame, based on the historical segmentation information. The RL model learns to take optimal actions to adjust the region of interest inferred from the previous frame for online model updating. To speed up the model adaption, we further design a novel multi-branch tree based exploration method to fast select the best state action pairs. Our experiments show that our work improves the state-of-the-art of the mean region similarity on DAVIS 2016 dataset to 87.1%.",0
"The aim of this study is to address the challenge of semi-supervised video object segmentation by working with a sequence of frames in which only the first frame has a ground-truth segmentation. The main difficulty lies in updating the segmentation model accurately and adaptively during online operation, even when faced with multiple confusing instances or large object motion. Existing approaches rely on selecting rough and inflexible regions of interest for model updates, which can lead to performance degradation. To overcome this limitation, we present a novel approach that uses reinforcement learning to select optimal adaptation areas for each frame based on historical segmentation information. The RL model learns to take optimal actions to adjust the region of interest inferred from the previous frame for online model updating. We also introduce a multi-branch tree-based exploration method to speed up model adaptation by quickly selecting the best state-action pairs. Our experimental results show that our approach improves the mean region similarity on the DAVIS 2016 dataset to 87.1%, surpassing the current state-of-the-art.",1
"A fundamental question in reinforcement learning is whether model-free algorithms are sample efficient. Recently, Jin et al. \cite{jin2018q} proposed a Q-learning algorithm with UCB exploration policy, and proved it has nearly optimal regret bound for finite-horizon episodic MDP. In this paper, we adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards \emph{without} accessing a generative model. We show that the \textit{sample complexity of exploration} of our algorithm is bounded by $\tilde{O}({\frac{SA}{\epsilon^2(1-\gamma)^7}})$. This improves the previously best known result of $\tilde{O}({\frac{SA}{\epsilon^4(1-\gamma)^8}})$ in this setting achieved by delayed Q-learning \cite{strehl2006pac}, and matches the lower bound in terms of $\epsilon$ as well as $S$ and $A$ except for logarithmic factors.",0
"Reinforcement learning researchers have been questioning whether model-free algorithms are sample efficient. Jin et al. proposed a Q-learning algorithm with UCB exploration policy that has nearly optimal regret bound for finite-horizon episodic MDP. In this paper, the authors have adapted Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards without accessing a generative model. The authors have shown that the sample complexity of exploration of their algorithm is bounded by $\tilde{O}({\frac{SA}{\epsilon^2(1-\gamma)^7}})$, which is an improvement from the previously best-known result achieved by delayed Q-learning. The new result matches the lower bound in terms of $\epsilon$ as well as $S$ and $A$ except for logarithmic factors.",1
"Understanding black-box machine learning models is important towards their widespread adoption. However, developing globally interpretable models that explain the behavior of the entire model is challenging. An alternative approach is to explain black-box models through explaining individual prediction using a locally interpretable model. In this paper, we propose a novel method for locally interpretable modeling - Reinforcement Learning-based Locally Interpretable Modeling (RL-LIM). RL-LIM employs reinforcement learning to select a small number of samples and distill the black-box model prediction into a low-capacity locally interpretable model. Training is guided with a reward that is obtained directly by measuring agreement of the predictions from the locally interpretable model with the black-box model. RL-LIM near-matches the overall prediction performance of black-box models while yielding human-like interpretability, and significantly outperforms state of the art locally interpretable models in terms of overall prediction performance and fidelity.",0
"The comprehension of black-box machine learning models is crucial for their widespread usage. However, it is difficult to develop globally interpretable models that explain the entire model's behavior. Instead, a different approach is to use a locally interpretable model to explain individual predictions. This paper proposes a new method for locally interpretable modeling called Reinforcement Learning-based Locally Interpretable Modeling (RL-LIM). RL-LIM utilizes reinforcement learning to select a few samples and distill the black-box model prediction into a low-capacity locally interpretable model. The training is guided by a reward that measures the agreement of the predictions from the locally interpretable model with the black-box model. RL-LIM provides human-like interpretability while almost matching the overall prediction performance of black-box models. Additionally, it outperforms current state of the art locally interpretable models in terms of overall prediction performance and fidelity.",1
"We propose RecSim, a configurable platform for authoring simulation environments for recommender systems (RSs) that naturally supports sequential interaction with users. RecSim allows the creation of new environments that reflect particular aspects of user behavior and item structure at a level of abstraction well-suited to pushing the limits of current reinforcement learning (RL) and RS techniques in sequential interactive recommendation problems. Environments can be easily configured that vary assumptions about: user preferences and item familiarity; user latent state and its dynamics; and choice models and other user response behavior. We outline how RecSim offers value to RL and RS researchers and practitioners, and how it can serve as a vehicle for academic-industrial collaboration.",0
"RecSim is a platform that can be configured to create simulation environments for recommender systems, enabling natural interaction with users in a sequential manner. This platform is suitable for pushing the boundaries of current reinforcement learning and recommender system techniques in sequential interactive recommendation problems by allowing the creation of new environments that reflect specific aspects of user behavior and item structure in an abstract manner. RecSim offers flexibility by enabling easy configuration of various assumptions including user preferences, item familiarity, user latent state, and choice models. This platform is valuable for both researchers and practitioners in the fields of reinforcement learning and recommender systems and can facilitate academic-industrial collaboration.",1
"Estimating the predictive uncertainty of a Bayesian learning model is critical in various decision-making problems, e.g., reinforcement learning, detecting adversarial attack, self-driving car. As the model posterior is almost always intractable, most efforts were made on finding an accurate approximation the true posterior. Even though a decent estimation of the model posterior is obtained, another approximation is required to compute the predictive distribution over the desired output. A common accurate solution is to use Monte Carlo (MC) integration. However, it needs to maintain a large number of samples, evaluate the model repeatedly and average multiple model outputs. In many real-world cases, this is computationally prohibitive. In this work, assuming that the exact posterior or a decent approximation is obtained, we propose a generic framework to approximate the output probability distribution induced by model posterior with a parameterized model and in an amortized fashion. The aim is to approximate the true uncertainty of a specific Bayesian model, meanwhile alleviating the heavy workload of MC integration at testing time. The proposed method is universally applicable to Bayesian classification models that allow for posterior sampling. Theoretically, we show that the idea of amortization incurs no additional costs on approximation performance. Empirical results validate the strong practical performance of our approach.",0
"Calculating the predictive uncertainty of a Bayesian learning model is crucial for various decision-making situations, such as reinforcement learning, identifying adversarial attacks, and self-driving cars. However, due to the intractability of the model posterior, significant efforts have been made to find an accurate approximation of it. Even when a reliable estimation of the model posterior is obtained, another approximation is necessary to determine the predictive distribution over the desired output. Monte Carlo integration is a commonly used accurate solution for this, but it requires a large number of samples, the repeated evaluation of the model, and multiple model outputs to be averaged. This is computationally prohibitive in many real-world situations. This work proposes a generic framework to approximate the output probability distribution induced by the model posterior with a parameterized model in an amortized fashion, assuming that the exact posterior or a decent approximation has been obtained. The objective is to approximate the true uncertainty of a specific Bayesian model while reducing the heavy workload of MC integration during testing. This method is universally applicable to Bayesian classification models that allow for posterior sampling, and we theoretically demonstrate that the idea of amortization does not incur any additional costs on approximation performance. Empirical results demonstrate the strong practical performance of our approach.",1
"Learning to imitate expert behavior from demonstrations can be challenging, especially in environments with high-dimensional, continuous observations and unknown dynamics. Supervised learning methods based on behavioral cloning (BC) suffer from distribution shift: because the agent greedily imitates demonstrated actions, it can drift away from demonstrated states due to error accumulation. Recent methods based on reinforcement learning (RL), such as inverse RL and generative adversarial imitation learning (GAIL), overcome this issue by training an RL agent to match the demonstrations over a long horizon. Since the true reward function for the task is unknown, these methods learn a reward function from the demonstrations, often using complex and brittle approximation techniques that involve adversarial training. We propose a simple alternative that still uses RL, but does not require learning a reward function. The key idea is to provide the agent with an incentive to match the demonstrations over a long horizon, by encouraging it to return to demonstrated states upon encountering new, out-of-distribution states. We accomplish this by giving the agent a constant reward of r=+1 for matching the demonstrated action in a demonstrated state, and a constant reward of r=0 for all other behavior. Our method, which we call soft Q imitation learning (SQIL), can be implemented with a handful of minor modifications to any standard Q-learning or off-policy actor-critic algorithm. Theoretically, we show that SQIL can be interpreted as a regularized variant of BC that uses a sparsity prior to encourage long-horizon imitation. Empirically, we show that SQIL outperforms BC and achieves competitive results compared to GAIL, on a variety of image-based and low-dimensional tasks in Box2D, Atari, and MuJoCo.",0
"Imitating expert behavior from demonstrations can be difficult, particularly in situations with numerous and ongoing observations, as well as unpredictable dynamics. Behavioral cloning (BC) is a supervised learning method that struggles with distribution shift because it imitates demonstrated actions, potentially leading to deviation from the demonstrated states due to accumulated errors. Reinforcement learning (RL) techniques like inverse RL and generative adversarial imitation learning (GAIL) address this issue by training an RL agent to match the demonstrations over a long horizon. Since the task's true reward function is unknown, these techniques rely on approximations that include adversarial training. We introduce a simpler alternative called soft Q imitation learning (SQIL), which uses RL but does not require learning a reward function. The approach encourages the agent to return to demonstrated states through a constant reward of r=+1 for matching the demonstrated action in a demonstrated state and r=0 for all other behavior. SQIL can be implemented with minor tweaks to any standard Q-learning or off-policy actor-critic algorithm. Theoretically, SQIL can be regarded as a regulated version of BC that utilizes a sparsity prior to encourage long-horizon imitation. Empirically, SQIL surpasses BC and achieves comparable results to GAIL on a range of tasks in Box2D, Atari, and MuJoCo that involve images or low dimensions.",1
"Quantifying the value of data is a fundamental problem in machine learning. Data valuation has multiple important use cases: (1) building insights about the learning task, (2) domain adaptation, (3) corrupted sample discovery, and (4) robust learning. To adaptively learn data values jointly with the target task predictor model, we propose a meta learning framework which we name Data Valuation using Reinforcement Learning (DVRL). We employ a data value estimator (modeled by a deep neural network) to learn how likely each datum is used in training of the predictor model. We train the data value estimator using a reinforcement signal of the reward obtained on a small validation set that reflects performance on the target task. We demonstrate that DVRL yields superior data value estimates compared to alternative methods across different types of datasets and in a diverse set of application scenarios. The corrupted sample discovery performance of DVRL is close to optimal in many regimes (i.e. as if the noisy samples were known apriori), and for domain adaptation and robust learning DVRL significantly outperforms state-of-the-art by 14.6% and 10.8%, respectively.",0
"The problem of determining the worth of data in machine learning is crucial. This issue has various practical applications, including developing insights about the learning task, discovering corrupted samples, adapting to different domains, and achieving robust learning. Our proposed solution to learning data values in conjunction with the target task predictor model is a meta learning framework called Data Valuation using Reinforcement Learning (DVRL). We utilize a deep neural network to estimate the value of each datum and train it using a reinforcement signal obtained from a small validation set that reflects the performance on the target task. Our results demonstrate that DVRL outperforms other methods in estimating data values for different types of datasets and application scenarios. Moreover, our approach excels in discovering corrupted samples and achieves superior performance in domain adaptation and robust learning, surpassing the state-of-the-art by 14.6% and 10.8%, respectively.",1
"Imitation learning seeks to learn an expert policy from sampled demonstrations. However, in the real world, it is often difficult to find a perfect expert and avoiding dangerous behaviors becomes relevant for safety reasons. We present the idea of \textit{learning to avoid}, an objective opposite to imitation learning in some sense, where an agent learns to avoid a demonstrator policy given an environment. We define avoidance learning as the process of optimizing the agent's reward while avoiding dangerous behaviors given by a demonstrator. In this work we develop a framework of avoidance learning by defining a suitable objective function for these problems which involves the \emph{distance} of state occupancy distributions of the expert and demonstrator policies. We use density estimates for state occupancy measures and use the aforementioned distance as the reward bonus for avoiding the demonstrator. We validate our theory with experiments using a wide range of partially observable environments. Experimental results show that we are able to improve sample efficiency during training compared to state of the art policy optimization and safety methods.",0
"Imitation learning aims to learn from demonstrations performed by an expert, but in reality, it can be difficult to find a perfect expert and avoiding hazardous behaviors is critical for safety. Our proposed concept of ""learning to avoid"" is the opposite of imitation learning in some sense, where an agent learns to avoid the behavior of a demonstrator policy in a given environment. We define avoidance learning as the process of maximizing the agent's reward while avoiding dangerous behaviors exhibited by the demonstrator. Our framework for avoidance learning involves defining an appropriate objective function that considers the ""distance"" between the state occupancy distributions of the expert and the demonstrator policies. We estimate state occupancy measures using density estimates and use the distance as a reward bonus for avoiding the demonstrator. We validate our theory through experiments on various partially observable environments and show that our approach improves sample efficiency during training compared to existing policy optimization and safety methods.",1
"The ability to discover approximately optimal policies in domains with sparse rewards is crucial to applying reinforcement learning (RL) in many real-world scenarios. Approaches such as neural density models and continuous exploration (e.g., Go-Explore) have been proposed to maintain the high exploration rate necessary to find high performing and generalizable policies. Soft actor-critic(SAC) is another method for improving exploration that aims to combine efficient learning via off-policy updates while maximizing the policy entropy. In this work, we extend SAC to a richer class of probability distributions (e.g., multimodal) through normalizing flows (NF) and show that this significantly improves performance by accelerating the discovery of good policies while using much smaller policy representations. Our approach, which we call SAC-NF, is a simple, efficient,easy-to-implement modification and improvement to SAC on continuous control baselines such as MuJoCo and PyBullet Roboschool domains. Finally, SAC-NF does this while being significantly parameter efficient, using as few as 5.5% the parameters for an equivalent SAC model.",0
"The ability to find nearly optimal policies for domains with limited rewards is crucial for the practical application of reinforcement learning (RL). To maintain the necessary high exploration rate for discovering high performance and generalizable policies, various approaches such as neural density models and continuous exploration (e.g., Go-Explore) have been proposed. Another method for enhancing exploration is Soft Actor-Critic (SAC), which combines effective learning via off-policy updates with the maximization of policy entropy. In this study, we enhance SAC to a more diverse range of probability distributions (e.g., multimodal) through normalizing flows (NF). Our approach, named SAC-NF, accelerates the discovery of good policies while using smaller policy representations. SAC-NF is an easy-to-implement and effective modification to SAC on continuous control baselines such as MuJoCo and PyBullet Roboschool domains. Finally, SAC-NF is significantly parameter efficient, utilizing as few as 5.5% of the parameters required for an equivalent SAC model.",1
"End-to-end automatic speech recognition (ASR) models are increasingly large and complex to achieve the best possible accuracy. In this paper, we build an AutoML system that uses reinforcement learning (RL) to optimize the per-layer compression ratios when applied to a state-of-the-art attention based end-to-end ASR model composed of several LSTM layers. We use singular value decomposition (SVD) low-rank matrix factorization as the compression method. For our RL-based AutoML system, we focus on practical considerations such as the choice of the reward/punishment functions, the formation of an effective search space, and the creation of a representative but small data set for quick evaluation between search steps. Finally, we present accuracy results on LibriSpeech of the model compressed by our AutoML system, and we compare it to manually-compressed models. Our results show that in the absence of retraining our RL-based search is an effective and practical method to compress a production-grade ASR system. When retraining is possible, we show that our AutoML system can select better highly-compressed seed models compared to manually hand-crafted rank selection, thus allowing for more compression than previously possible.",0
"To achieve high accuracy, automatic speech recognition (ASR) models are becoming increasingly large and complex. This study introduces an AutoML system that utilizes reinforcement learning (RL) to optimize per-layer compression ratios for an attention-based end-to-end ASR model, which includes multiple LSTM layers. Singular value decomposition (SVD) low-rank matrix factorization is used for compression. The study focuses on practical aspects, such as reward/punishment functions, effective search space formation, and creation of a representative but small data set for quick evaluation. The compressed model produced by the RL-based AutoML system is compared to manually compressed models, and accuracy results on LibriSpeech are presented. Results indicate that the RL-based search is a practical and effective method for compressing a production-grade ASR system, and the AutoML system can select better highly-compressed seed models compared to manually crafted rank selection, allowing for more compression than previously possible when retraining is possible.",1
"Since the recent advent of deep reinforcement learning for game play and simulated robotic control, a multitude of new algorithms have flourished. Most are model-free algorithms which can be categorized into three families: deep Q-learning, policy gradients, and Q-value policy gradients. These have developed along separate lines of research, such that few, if any, code bases incorporate all three kinds. Yet these algorithms share a great depth of common deep reinforcement learning machinery. We are pleased to share rlpyt, which implements all three algorithm families on top of a shared, optimized infrastructure, in a single repository. It contains modular implementations of many common deep RL algorithms in Python using PyTorch, a leading deep learning library. rlpyt is designed as a high-throughput code base for small- to medium-scale research in deep RL. This white paper summarizes its features, algorithms implemented, and relation to prior work, and concludes with detailed implementation and usage notes. rlpyt is available at https://github.com/astooke/rlpyt.",0
"In recent times, numerous algorithms have emerged for deep reinforcement learning in gaming and simulated robotic control. These predominantly model-free algorithms can be classified into three groups: deep Q-learning, policy gradients, and Q-value policy gradients. While these algorithms have progressed through separate research pathways, they share common machinery for deep reinforcement learning. The development of rlpyt has made it possible to implement all three algorithm families in a single repository, using a shared and optimized infrastructure. The code base is built using PyTorch, a top deep learning library, and contains modular implementations of various deep RL algorithms in Python. rlpyt is designed for high-throughput research in deep RL, aimed at small- to medium-scale projects. This white paper outlines the features of rlpyt, the algorithms that it implements, its relation to previous work, and provides implementation and usage notes. rlpyt can be accessed at https://github.com/astooke/rlpyt.",1
"One typical assumption in inverse reinforcement learning (IRL) is that human experts act to optimize the expected utility of a stochastic cost with a fixed distribution. This assumption deviates from actual human behaviors under ambiguity. Risk-sensitive inverse reinforcement learning (RS-IRL) bridges such gap by assuming that humans act according to a random cost with respect to a set of subjectively distorted distributions instead of a fixed one. Such assumption provides the additional flexibility to model human's risk preferences, represented by a risk envelope, in safe-critical tasks. However, like other learning from demonstration techniques, RS-IRL could also suffer inefficient learning due to redundant demonstrations. Inspired by the concept of active learning, this research derives a probabilistic disturbance sampling scheme to enable an RS-IRL agent to query expert support that is likely to expose unrevealed boundaries of the expert's risk envelope. Experimental results confirm that our approach accelerates the convergence of RS-IRL algorithms with lower variance while still guaranteeing unbiased convergence.",0
"Inverse reinforcement learning (IRL) typically assumes that human experts optimize the expected utility of a stochastic cost with a fixed distribution. However, this assumption does not accurately reflect human behavior under ambiguity. Risk-sensitive inverse reinforcement learning (RS-IRL) addresses this gap by assuming that humans act according to a random cost with respect to a set of subjectively distorted distributions. This approach allows for the modeling of human risk preferences, represented by a risk envelope, in safe-critical tasks. However, like other learning from demonstration techniques, RS-IRL may suffer from inefficient learning due to redundant demonstrations. To address this issue, this research proposes a probabilistic disturbance sampling scheme inspired by active learning. This scheme enables an RS-IRL agent to query expert support likely to reveal previously unknown boundaries of the expert’s risk envelope. Experimental results demonstrate that this approach accelerates RS-IRL algorithms' convergence, with lower variance, while still ensuring unbiased convergence.",1
"Traffic congestion in metropolitan areas is a world-wide problem that can be ameliorated by traffic lights that respond dynamically to real-time conditions. Recent studies applying deep reinforcement learning (RL) to optimize single traffic lights have shown significant improvement over conventional control. However, optimization of global traffic condition over a large road network fundamentally is a cooperative multi-agent control problem, for which single-agent RL is not suitable due to environment non-stationarity and infeasibility of optimizing over an exponential joint-action space. Motivated by these challenges, we propose QCOMBO, a simple yet effective multi-agent reinforcement learning (MARL) algorithm that combines the advantages of independent and centralized learning. We ensure scalability by selecting actions from individually optimized utility functions, which are shaped to maximize global performance via a novel consistency regularization loss between individual utility and a global action-value function. Experiments on diverse road topologies and traffic flow conditions in the SUMO traffic simulator show competitive performance of QCOMBO versus recent state-of-the-art MARL algorithms. We further show that policies trained on small sub-networks can effectively generalize to larger networks under different traffic flow conditions, providing empirical evidence for the suitability of MARL for intelligent traffic control.",0
"Metropolitan areas worldwide face the issue of traffic congestion, which can be improved by traffic lights that react to real-time conditions. Although studies using deep reinforcement learning (RL) have improved single traffic light optimization, improving global traffic conditions across a vast road network is a cooperative multi-agent control challenge. Single-agent RL is unsuitable due to non-stationarity and the infeasibility of optimizing over a joint-action space. To address these issues, we propose QCOMBO, a multi-agent reinforcement learning (MARL) algorithm that combines independent and centralized learning. Our approach ensures scalability by selecting actions from individually optimized utility functions that maximize global performance via a consistency regularization loss between individual utility and a global action-value function. Our experiments with diverse road topologies and traffic flow conditions in the SUMO traffic simulator show that QCOMBO performs competitively versus recent state-of-the-art MARL algorithms. We also demonstrate that policies trained on small sub-networks can generalize to larger networks under different traffic flow conditions, providing evidence for the suitability of MARL for intelligent traffic control.",1
"An important linear algebra routine, GEneral Matrix Multiplication (GEMM), is a fundamental operator in deep learning. Compilers need to translate these routines into low-level code optimized for specific hardware. Compiler-level optimization of GEMM has significant performance impact on training and executing deep learning models. However, most deep learning frameworks rely on hardware-specific operator libraries in which GEMM optimization has been mostly achieved by manual tuning, which restricts the performance on different target hardware. In this paper, we propose two novel algorithms for GEMM optimization based on the TVM framework, a lightweight Greedy Best First Search (G-BFS) method based on heuristic search, and a Neighborhood Actor Advantage Critic (N-A2C) method based on reinforcement learning. Experimental results show significant performance improvement of the proposed methods, in both the optimality of the solution and the cost of search in terms of time and fraction of the search space explored. Specifically, the proposed methods achieve 24% and 40% savings in GEMM computation time over state-of-the-art XGBoost and RNN methods, respectively, while exploring only 0.1% of the search space. The proposed approaches have potential to be applied to other operator-level optimizations.",0
"Deep learning relies heavily on the fundamental operator of General Matrix Multiplication (GEMM), which is a critical linear algebra routine. Compilers must translate these routines into low-level code optimized for specific hardware to achieve optimal performance in training and executing deep learning models. Compiler-level optimization of GEMM is crucial for achieving significant performance improvements. However, most deep learning frameworks rely on hardware-specific operator libraries that have only achieved GEMM optimization through manual tuning, which limits performance on different target hardware. This paper proposes two novel algorithms for optimizing GEMM based on the TVM framework: a lightweight Greedy Best First Search (G-BFS) method based on heuristic search and a Neighborhood Actor Advantage Critic (N-A2C) method based on reinforcement learning. Experimental results demonstrate that the proposed methods significantly improve performance in terms of both the optimality of the solution and the cost of search in terms of time and fraction of the search space explored. Moreover, the proposed methods achieve 24% and 40% savings in GEMM computation time over state-of-the-art XGBoost and RNN methods, respectively, while exploring only 0.1% of the search space. These approaches have the potential to be applied to other operator-level optimizations.",1
We study the use of randomized value functions to guide deep exploration in reinforcement learning. This offers an elegant means for synthesizing statistically and computationally efficient exploration with common practical approaches to value function learning. We present several reinforcement learning algorithms that leverage randomized value functions and demonstrate their efficacy through computational studies. We also prove a regret bound that establishes statistical efficiency with a tabular representation.,0
"Our focus is on exploring deep reinforcement learning through the use of randomized value functions. This method provides a sophisticated approach to value function learning that combines efficient exploration with practical techniques. Our research includes the development of reinforcement learning algorithms that utilize randomized value functions, which we validate through computational experiments. Additionally, we provide a proof of a regret bound that confirms the statistical efficiency of a tabular representation.",1
"Gradient-based methods for optimisation of objectives in stochastic settings with unknown or intractable dynamics require estimators of derivatives. We derive an objective that, under automatic differentiation, produces low-variance unbiased estimators of derivatives at any order. Our objective is compatible with arbitrary advantage estimators, which allows the control of the bias and variance of any-order derivatives when using function approximation. Furthermore, we propose a method to trade off bias and variance of higher order derivatives by discounting the impact of more distant causal dependencies. We demonstrate the correctness and utility of our objective in analytically tractable MDPs and in meta-reinforcement-learning for continuous control.",0
"In stochastic settings with unknown or intractable dynamics, optimising objectives using gradient-based methods requires derivative estimators. We have created an objective that can produce low-variance unbiased estimators of derivatives at any order when used with automatic differentiation. Our objective is also compatible with arbitrary advantage estimators, which helps control the bias and variance of any-order derivatives when using function approximation. Additionally, we have developed a method to balance the bias and variance of higher order derivatives by reducing the impact of more distant causal dependencies. We have demonstrated the usefulness and accuracy of our objective in both analytically tractable MDPs and meta-reinforcement-learning for continuous control.",1
"Hierarchical Reinforcement Learning algorithms have successfully been applied to temporal credit assignment problems with sparse reward signals. However, state-of-the-art algorithms require manual specification of sub-task structures, a sample inefficient exploration phase or lack semantic interpretability. Humans, on the other hand, efficiently detect hierarchical sub-structures induced by their surroundings. It has been argued that this inference process universally applies to language, logical reasoning as well as motor control. Therefore, we propose a cognitive-inspired Reinforcement Learning architecture which uses grammar induction to identify sub-goal policies. By treating an on-policy trajectory as a sentence sampled from the policy-conditioned language of the environment, we identify hierarchical constituents with the help of unsupervised grammatical inference. The resulting set of temporal abstractions is called action grammar (Pastra & Aloimonos, 2012) and unifies symbolic and connectionist approaches to Reinforcement Learning. It can be used to facilitate efficient imitation, transfer and online learning.",0
"Sparse reward signals in temporal credit assignment problems have been successfully addressed by Hierarchical Reinforcement Learning algorithms. However, the current state-of-the-art algorithms need manual specification of sub-task structures, inefficient exploration, or lack semantic interpretability. Humans, on the other hand, can efficiently detect hierarchical sub-structures in their surroundings, which is believed to be universally applicable to language, logical reasoning, and motor control. To overcome this limitation, we propose a cognitive-inspired Reinforcement Learning architecture that utilizes grammar induction to identify sub-goal policies. By considering an on-policy trajectory as a sentence sampled from the environment's policy-conditioned language, we identify hierarchical constituents using unsupervised grammatical inference. This results in a set of temporal abstractions called action grammar, which integrates symbolic and connectionist Reinforcement Learning approaches. It can facilitate efficient imitation, transfer, and online learning.",1
"Reinforcement learning (RL) makes it possible to train agents capable of achieving sophisticated goals in complex and uncertain environments. A key difficulty in reinforcement learning is specifying a reward function for the agent to optimize. Traditionally, imitation learning in RL has been used to overcome this problem. Unfortunately, hitherto imitation learning methods tend to require that demonstrations are supplied in the first-person: the agent is provided with a sequence of states and a specification of the actions that it should have taken. While powerful, this kind of imitation learning is limited by the relatively hard problem of collecting first-person demonstrations. Humans address this problem by learning from third-person demonstrations: they observe other humans perform tasks, infer the task, and accomplish the same task themselves.   In this paper, we present a method for unsupervised third-person imitation learning. Here third-person refers to training an agent to correctly achieve a simple goal in a simple environment when it is provided a demonstration of a teacher achieving the same goal but from a different viewpoint; and unsupervised refers to the fact that the agent receives only these third-person demonstrations, and is not provided a correspondence between teacher states and student states. Our methods primary insight is that recent advances from domain confusion can be utilized to yield domain agnostic features which are crucial during the training process. To validate our approach, we report successful experiments on learning from third-person demonstrations in a pointmass domain, a reacher domain, and inverted pendulum.",0
"Reinforcement learning (RL) can equip agents with the ability to achieve complex goals in uncertain and complicated environments. One of the major challenges of RL is providing the agent with a reward function to optimize. To overcome this, imitation learning has been traditionally used. However, the method requires first-person demonstrations, where the agent is shown a sequence of states and actions it should have taken. This approach is limited because it is challenging to collect first-person demonstrations. Humans learn from third-person demonstrations, where they observe others performing tasks and infer the process to achieve the same outcome. In this study, we introduce a technique for unsupervised third-person imitation learning. The approach trains an agent to achieve a simple goal in a simple environment by providing demonstrations from a different viewpoint. The agent receives only third-person demonstrations, without any correspondence between teacher and student states. Our method utilizes domain confusion advances to yield domain agnostic features that are critical during training. The approach is successful in learning from third-person demonstrations in various domains, including pointmass, reacher, and inverted pendulum.",1
"In this paper we investigate two hypothesis regarding the use of deep reinforcement learning in multiple tasks. The first hypothesis is driven by the question of whether a deep reinforcement learning algorithm, trained on two similar tasks, is able to outperform two single-task, individually trained algorithms, by more efficiently learning a new, similar task, that none of the three algorithms has encountered before. The second hypothesis is driven by the question of whether the same multi-task deep RL algorithm, trained on two similar tasks and augmented with elastic weight consolidation (EWC), is able to retain similar performance on the new task, as a similar algorithm without EWC, whilst being able to overcome catastrophic forgetting in the two previous tasks. We show that a multi-task Asynchronous Advantage Actor-Critic (GA3C) algorithm, trained on Space Invaders and Demon Attack, is in fact able to outperform two single-tasks GA3C versions, trained individually for each single-task, when evaluated on a new, third task, namely, Phoenix. We also show that, when training two trained multi-task GA3C algorithms on the third task, if one is augmented with EWC, it is not only able to achieve similar performance on the new task, but also capable of overcoming a substantial amount of catastrophic forgetting on the two previous tasks.",0
"The aim of this study is to explore two hypotheses related to the use of deep reinforcement learning in multiple tasks. The first hypothesis examines whether a deep reinforcement learning algorithm, which is trained on two similar tasks, can outperform two individually trained algorithms in efficiently learning a new, similar task that none of the three algorithms has encountered before. The second hypothesis assesses whether the same multi-task deep RL algorithm, when trained on two similar tasks and augmented with elastic weight consolidation (EWC), can maintain comparable performance on the new task as a similar algorithm without EWC, while also overcoming catastrophic forgetting in the two previous tasks. Our findings indicate that a multi-task Asynchronous Advantage Actor-Critic (GA3C) algorithm, trained on Space Invaders and Demon Attack, outperforms two single-task GA3C versions that are independently trained for each single-task when evaluated on a new, third task, Phoenix. Additionally, when two trained multi-task GA3C algorithms are trained on the third task, the one augmented with EWC not only achieves similar performance on the new task but can also overcome a substantial amount of catastrophic forgetting on the two previous tasks.",1
"The ability to navigate from visual observations in unfamiliar environments is a core component of intelligent agents and an ongoing challenge for Deep Reinforcement Learning (RL). Street View can be a sensible testbed for such RL agents, because it provides real-world photographic imagery at ground level, with diverse street appearances; it has been made into an interactive environment called StreetLearn and used for research on navigation. However, goal-driven street navigation agents have not so far been able to transfer to unseen areas without extensive retraining, and relying on simulation is not a scalable solution. Since aerial images are easily and globally accessible, we propose instead to train a multi-modal policy on ground and aerial views, then transfer the ground view policy to unseen (target) parts of the city by utilizing aerial view observations. Our core idea is to pair the ground view with an aerial view and to learn a joint policy that is transferable across views. We achieve this by learning a similar embedding space for both views, distilling the policy across views and dropping out visual modalities. We further reformulate the transfer learning paradigm into three stages: 1) cross-modal training, when the agent is initially trained on multiple city regions, 2) aerial view-only adaptation to a new area, when the agent is adapted to a held-out region using only the easily obtainable aerial view, and 3) ground view-only transfer, when the agent is tested on navigation tasks on unseen ground views, without aerial imagery. Experimental results suggest that the proposed cross-view policy learning enables better generalization of the agent and allows for more effective transfer to unseen environments.",0
"Deep Reinforcement Learning (RL) faces the ongoing challenge of navigating unfamiliar environments based on visual observations, which is a core component of intelligent agents. StreetLearn is an interactive environment that uses Street View images to provide a realistic testbed for RL agents. However, existing goal-driven street navigation agents struggle to transfer to unseen areas without extensive retraining, and simulation-based solutions are not scalable. To address this issue, we propose training a multi-modal policy on both ground and aerial views, which can be easily accessed globally. Our core idea is to pair ground and aerial views to learn a joint policy that can be transferred across views. We achieve this by learning a similar embedding space for both views, distilling the policy across views, and dropping out visual modalities. We also reformulate the transfer learning paradigm into three stages: cross-modal training, aerial view-only adaptation to a new area, and ground view-only transfer. Experimental results show that our proposed cross-view policy learning enables better generalization of the agent and allows for more effective transfer to unseen environments.",1
"We present a novel reinforcement learning-based natural media painting algorithm. Our goal is to reproduce a reference image using brush strokes and we encode the objective through observations. Our formulation takes into account that the distribution of the reward in the action space is sparse and training a reinforcement learning algorithm from scratch can be difficult. We present an approach that combines self-supervised learning and reinforcement learning to effectively transfer negative samples into positive ones and change the reward distribution. We demonstrate the benefits of our painting agent to reproduce reference images with brush strokes. The training phase takes about one hour and the runtime algorithm takes about 30 seconds on a GTX1080 GPU reproducing a 1000x800 image with 20,000 strokes.",0
"We introduce a new painting algorithm that utilizes reinforcement learning and natural media techniques. Our objective is to recreate an image using brush strokes, which we achieve by encoding observations. We take into account the challenge of sparse reward distribution in the action space and the difficulty of training a reinforcement learning model from scratch. To address this, we propose a combination of self-supervised and reinforcement learning, which effectively transforms negative samples into positive ones and alters the reward distribution. Our painting agent demonstrates superior results in reproducing reference images. The training phase takes approximately one hour, while the runtime algorithm takes only 30 seconds on a GTX1080 GPU to replicate a 1000x800 image with 20,000 brush strokes.",1
"The learning rate is one of the most important hyper-parameters for model training and generalization. However, current hand-designed parametric learning rate schedules offer limited flexibility and the predefined schedule may not match the training dynamics of high dimensional and non-convex optimization problems. In this paper, we propose a reinforcement learning based framework that can automatically learn an adaptive learning rate schedule by leveraging the information from past training histories. The learning rate dynamically changes based on the current training dynamics. To validate this framework, we conduct experiments with different neural network architectures on the Fashion MINIST and CIFAR10 datasets. Experimental results show that the auto-learned learning rate controller can achieve better test results. In addition, the trained controller network is generalizable -- able to be trained on one data set and transferred to new problems.",0
"The significance of the hyper-parameter, learning rate, cannot be overstated when it comes to model training and generalization. However, the typical approach of utilizing hand-crafted, parametric learning rate schedules can be restrictive as the pre-defined schedule may not align with the training dynamics of complex optimization problems with high dimensions and non-convexity. To address this issue, we present a novel framework that leverages reinforcement learning to learn an adaptive learning rate schedule based on past training histories. This approach ensures that the learning rate can dynamically adjust according to the current training dynamics. Our experiments on the Fashion MINIST and CIFAR10 datasets with various neural network architectures demonstrate that the auto-learned learning rate controller outperforms traditional methods in terms of test results. Furthermore, the trained controller network is transferable, capable of being trained on one dataset and applied to new problems.",1
"The use of target networks has been a popular and key component of recent deep Q-learning algorithms for reinforcement learning, yet little is known from the theory side. In this work, we introduce a new family of target-based temporal difference (TD) learning algorithms and provide theoretical analysis on their convergences. In contrast to the standard TD-learning, target-based TD algorithms maintain two separate learning parameters-the target variable and online variable. Particularly, we introduce three members in the family, called the averaging TD, double TD, and periodic TD, where the target variable is updated through an averaging, symmetric, or periodic fashion, mirroring those techniques used in deep Q-learning practice.   We establish asymptotic convergence analyses for both averaging TD and double TD and a finite sample analysis for periodic TD. In addition, we also provide some simulation results showing potentially superior convergence of these target-based TD algorithms compared to the standard TD-learning. While this work focuses on linear function approximation and policy evaluation setting, we consider this as a meaningful step towards the theoretical understanding of deep Q-learning variants with target networks.",0
"Recent deep Q-learning algorithms for reinforcement learning have made use of target networks as a popular component, yet the theory behind this remains relatively unknown. This study presents a new family of target-based temporal difference (TD) learning algorithms and provides theoretical analysis on their convergence. Unlike standard TD-learning, these algorithms maintain two separate learning parameters: the target variable and online variable. The family includes three members: the averaging TD, double TD, and periodic TD, which update the target variable through an averaging, symmetric, or periodic fashion, similar to those techniques used in deep Q-learning practice. The study establishes asymptotic convergence analyses for both averaging TD and double TD, as well as a finite sample analysis for periodic TD. Simulation results suggest that these target-based TD algorithms may have superior convergence compared to standard TD-learning. Although this study focuses on linear function approximation and policy evaluation, it is a significant step towards understanding the theoretical basis of deep Q-learning variants using target networks.",1
"We propose a planning and perception mechanism for a robot (agent), that can only observe the underlying environment partially, in order to solve an image classification problem. A three-layer architecture is suggested that consists of a meta-layer that decides the intermediate goals, an action-layer that selects local actions as the agent navigates towards a goal, and a classification-layer that evaluates the reward and makes a prediction. We design and implement these layers using deep reinforcement learning. A generalized policy gradient algorithm is utilized to learn the parameters of these layers to maximize the expected reward. Our proposed methodology is tested on the MNIST dataset of handwritten digits, which provides us with a level of explainability while interpreting the agent's intermediate goals and course of action.",0
"Our proposal involves creating a planning and perception system for a robot with limited environmental observations, aimed at solving an image classification problem. The system comprises three layers: a meta-layer that determines intermediate goals, an action-layer that selects local actions for the agent to move towards the goal, and a classification-layer that evaluates the reward and predicts outcomes. We use deep reinforcement learning to design and implement these layers, using a generalized policy gradient algorithm to optimize the parameters for maximum expected reward. To test our approach, we use the MNIST dataset of handwritten digits, which allows us to interpret the agent's goals and actions.",1
"Practical reinforcement learning problems are often formulated as constrained Markov decision process (CMDP) problems, in which the agent has to maximize the expected return while satisfying a set of prescribed safety constraints. In this study, we propose a novel simulator-based method to approximately solve a CMDP problem without making any compromise on the safety constraints. We achieve this by decomposing the CMDP into a pair of MDPs; reconnaissance MDP and planning MDP. The purpose of reconnaissance MDP is to evaluate the set of actions that are safe, and the purpose of planning MDP is to maximize the return while using the actions authorized by reconnaissance MDP. RMDP can define a set of safe policies for any given set of safety constraint, and this set of safe policies can be used to solve another CMDP problem with different reward. Our method is not only computationally less demanding than the previous simulator-based approaches to CMDP, but also capable of finding a competitive reward-seeking policy in a high dimensional environment, including those involving multiple moving obstacles.",0
"The formulation of practical reinforcement learning problems often involves constrained Markov decision process (CMDP) problems, which require the agent to maximize expected return while adhering to safety constraints. This study proposes a novel simulator-based method to solve CMDP problems without compromising on safety constraints. The CMDP is decomposed into a reconnaissance MDP and a planning MDP, where the former identifies safe actions and the latter maximizes return using those actions. By defining a set of safe policies, the proposed method can solve another CMDP problem with different rewards. Compared to previous simulator-based approaches, our method is computationally less demanding and can handle complex environments with multiple moving obstacles.",1
"In tabular case, when the reward and environment dynamics are known, policy evaluation can be written as $\bm{V}_{\bm{\pi}} = (I - \gamma P_{\bm{\pi}})^{-1} \bm{r}_{\bm{\pi}}$, where $P_{\bm{\pi}}$ is the state transition matrix given policy ${\bm{\pi}}$ and $\bm{r}_{\bm{\pi}}$ is the reward signal given ${\bm{\pi}}$. What annoys us is that $P_{\bm{\pi}}$ and $\bm{r}_{\bm{\pi}}$ are both mixed with ${\bm{\pi}}$, which means every time when we update ${\bm{\pi}}$, they will change together. In this paper, we leverage the notation from \cite{wang2007dual} to disentangle ${\bm{\pi}}$ and environment dynamics which makes optimization over policy more straightforward. We show that policy gradient theorem \cite{sutton2018reinforcement} and TRPO \cite{schulman2015trust} can be put into a more general framework and such notation has good potential to be extended to model-based reinforcement learning.",0
"When the reward and environment dynamics are known in the tabular case, policy evaluation can be expressed as $\bm{V}_{\bm{\pi}} = (I - \gamma P_{\bm{\pi}})^{-1} \bm{r}_{\bm{\pi}}$, where $P_{\bm{\pi}}$ is the state transition matrix given policy ${\bm{\pi}}$, and $\bm{r}_{\bm{\pi}}$ is the reward signal given ${\bm{\pi}}$. However, the mixing of ${\bm{\pi}}$ with $P_{\bm{\pi}}$ and $\bm{r}_{\bm{\pi}}$ means that updating ${\bm{\pi}}$ results in changes to both. To address this issue, we utilize the notation from \cite{wang2007dual} to separate ${\bm{\pi}}$ and environment dynamics, making policy optimization more straightforward. We demonstrate that this notation can be applied to the policy gradient theorem \cite{sutton2018reinforcement} and TRPO \cite{schulman2015trust} in a broader framework, and has the potential to extend to model-based reinforcement learning.",1
"One desirable capability of autonomous cars is to accurately predict the pedestrian motion near intersections for safe and efficient trajectory planning. We are interested in developing transfer learning algorithms that can be trained on the pedestrian trajectories collected at one intersection and yet still provide accurate predictions of the trajectories at another, previously unseen intersection. We first discussed the feature selection for transferable pedestrian motion models in general. Following this discussion, we developed one transferable pedestrian motion prediction algorithm based on Inverse Reinforcement Learning (IRL) that infers pedestrian intentions and predicts future trajectories based on observed trajectory. We evaluated our algorithm on a dataset collected at two intersections, trained at one intersection and tested at the other intersection. We used the accuracy of augmented semi-nonnegative sparse coding (ASNSC), trained and tested at the same intersection as a baseline. The result shows that the proposed algorithm improves the baseline accuracy by 40% in the non-transfer task, and 16% in the transfer task.",0
"Developing transfer learning algorithms that accurately predict pedestrian motion near intersections is a key capability for autonomous cars in terms of safe and efficient trajectory planning. Our objective is to create such algorithms that can be trained on pedestrian trajectories from one intersection and still accurately predict the trajectories at a different, unseen intersection. To do this, we first discussed feature selection for transferable pedestrian motion models and then developed an algorithm based on Inverse Reinforcement Learning (IRL) that predicts future trajectories based on observed pedestrian intentions. We evaluated our algorithm on a dataset collected from two intersections, with training at one intersection and testing at the other. We used the accuracy of augmented semi-nonnegative sparse coding (ASNSC) as a baseline for comparison. Our results show that our proposed algorithm significantly improves accuracy compared to ASNSC, with a 40% improvement in the non-transfer task and a 16% improvement in the transfer task.",1
"In the last decade many different algorithms have been proposed to track a generic object in videos. Their execution on recent large-scale video datasets can produce a great amount of various tracking behaviours. New trends in Reinforcement Learning showed that demonstrations of an expert agent can be efficiently used to speed-up the process of policy learning. Taking inspiration from such works and from the recent applications of Reinforcement Learning to visual tracking, we propose two novel trackers, A3CT, which exploits demonstrations of a state-of-the-art tracker to learn an effective tracking policy, and A3CTD, that takes advantage of the same expert tracker to correct its behaviour during tracking. Through an extensive experimental validation on the GOT-10k, OTB-100, LaSOT, UAV123 and VOT benchmarks, we show that the proposed trackers achieve state-of-the-art performance while running in real-time.",0
"Numerous algorithms have emerged over the past decade for the purpose of tracking objects in videos, resulting in a wide range of tracking behaviors when applied to large-scale video datasets. Recent advancements in Reinforcement Learning have demonstrated the effectiveness of using expert agent demonstrations to accelerate policy learning. Building on this notion and the application of Reinforcement Learning to visual tracking, we introduce two innovative trackers: A3CT, which utilizes an expert tracker's demonstrations to learn a successful tracking policy, and A3CTD, which leverages the same expert tracker to correct its behavior during tracking. Our extensive experimental validation on the GOT-10k, OTB-100, LaSOT, UAV123, and VOT benchmarks showcases the state-of-the-art performance of the proposed trackers while operating in real-time.",1
"Optimization of hyper-parameters in reinforcement learning (RL) algorithms is a key task, because they determine how the agent will learn its policy by interacting with its environment, and thus what data is gathered. In this work, an approach that uses Bayesian optimization to perform a two-step optimization is proposed: first, categorical RL structure hyper-parameters are taken as binary variables and optimized with an acquisition function tailored for such variables. Then, at a lower level of abstraction, solution-level hyper-parameters are optimized by resorting to the expected improvement acquisition function, while using the best categorical hyper-parameters found in the optimization at the upper-level of abstraction. This two-tier approach is validated in a simulated control task. Results obtained are promising and open the way for more user-independent applications of reinforcement learning.",0
"The optimization of hyper-parameters in reinforcement learning algorithms plays a crucial role in determining the agent's policy and the data it gathers from the environment. In this study, a two-step optimization method using Bayesian optimization is introduced. Firstly, binary variables are optimized with a specialized acquisition function for categorical RL structure hyper-parameters. Secondly, at a lower level, the expected improvement acquisition function is used to optimize solution-level hyper-parameters, taking into account the best categorical hyper-parameters found in the upper-level optimization. This method is successfully tested in a simulated control task, showing promising results and paving the way for more user-independent applications of reinforcement learning.",1
"Recently, generating adversarial examples has become an important means of measuring robustness of a deep learning model. Adversarial examples help us identify the susceptibilities of the model and further counter those vulnerabilities by applying adversarial training techniques. In natural language domain, small perturbations in the form of misspellings or paraphrases can drastically change the semantics of the text. We propose a reinforcement learning based approach towards generating adversarial examples in black-box settings. We demonstrate that our method is able to fool well-trained models for (a) IMDB sentiment classification task and (b) AG's news corpus news categorization task with significantly high success rates. We find that the adversarial examples generated are semantics-preserving perturbations to the original text.",0
"Generating adversarial examples is now considered a crucial method for evaluating the resilience of deep learning models. By highlighting the vulnerabilities of a model, we can then apply adversarial training techniques to counter these susceptibilities. In the realm of natural language, even small alterations such as misspellings or paraphrases can have a profound impact on the text's meaning. To address this, we propose a reinforcement learning-based approach for generating adversarial examples in black-box scenarios. Our method successfully deceived well-trained models in both the IMDB sentiment classification and AG's news corpus news categorization tasks with high success rates. Furthermore, we discovered that the adversarial examples generated were subtle changes that preserved the original text's semantics.",1
"This literature review focuses on three important aspects of an autonomous car system: tracking (assessing the identity of the actors such as cars, pedestrians or obstacles in a sequence of observations), prediction (predicting the future motion of surrounding vehicles in order to navigate through various traffic scenarios) and decision making (analyzing the available actions of the ego car and their consequences to the entire driving context). For tracking and prediction, approaches based on (deep) neural networks and other, especially stochastic techniques, are reported. For decision making, deep reinforcement learning algorithms are presented, together with methods used to explore different alternative actions, such as Monte Carlo Tree Search.",0
"In this literature review, the focus is on three crucial elements of an independent vehicle system: evaluating the identification of actors such as cars, pedestrians or obstacles in a series of observations (tracking), anticipating the future movements of surrounding vehicles to navigate through diverse traffic scenarios (prediction), and analyzing the available actions of the ego car and their impact on the whole driving context (decision making). The literature reports approaches based on (deep) neural networks and other stochastic techniques for tracking and prediction. For decision making, deep reinforcement learning algorithms are presented along with methods to explore different alternative actions, including Monte Carlo Tree Search.",1
"Reinforcement learning algorithms, though successful, tend to over-fit to training environments hampering their application to the real-world. This paper proposes $\text{W}\text{R}^{2}\text{L}$ -- a robust reinforcement learning algorithm with significant robust performance on low and high-dimensional control tasks. Our method formalises robust reinforcement learning as a novel min-max game with a Wasserstein constraint for a correct and convergent solver. Apart from the formulation, we also propose an efficient and scalable solver following a novel zero-order optimisation method that we believe can be useful to numerical optimisation in general. We empirically demonstrate significant gains compared to standard and robust state-of-the-art algorithms on high-dimensional MuJuCo environments.",0
"While reinforcement learning algorithms have demonstrated success, they often suffer from over-fitting to training environments, thereby limiting their practical application. To address this issue, this paper introduces a novel reinforcement learning algorithm, $\text{W}\text{R}^{2}\text{L}$, that is both robust and effective in low and high-dimensional control tasks. Our approach formalizes the problem of robust reinforcement learning as a min-max game with a Wasserstein constraint, which ensures convergence and accuracy. Additionally, we present a scalable and efficient solver based on a novel zero-order optimization method that has broad applicability to numerical optimization. Empirical results show that our algorithm outperforms existing state-of-the-art algorithms in high-dimensional MuJuCo environments.",1
"Modern reinforcement learning methods suffer from low sample efficiency and unsafe exploration, making it infeasible to train robotic policies entirely on real hardware. In this work, we propose to address the problem of sim-to-real domain transfer by using meta learning to train a policy that can adapt to a variety of dynamic conditions, and using a task-specific trajectory generation model to provide an action space that facilitates quick exploration. We evaluate the method by performing domain adaptation in simulation and analyzing the structure of the latent space during adaptation. We then deploy this policy on a KUKA LBR 4+ robot and evaluate its performance on a task of hitting a hockey puck to a target. Our method shows more consistent and stable domain adaptation than the baseline, resulting in better overall performance.",0
"Current reinforcement learning techniques have limitations in terms of sample efficiency and the ability to explore safely, making it challenging to fully train robotic policies on actual hardware. This study proposes a solution to the problem of transferring sim-to-real domains by utilizing meta learning to develop a policy that can adapt to various dynamic conditions. Additionally, a task-specific trajectory generation model is used to provide an action space that enables quick exploration. The effectiveness of the approach is assessed by conducting domain adaptation in simulation and analyzing the latent space structure during adaptation. The policy is then deployed on a KUKA LBR 4+ robot to complete a hockey puck hitting task. The experimental results indicate that the proposed method exhibits more reliable and stable domain adaptation compared to the baseline approach, resulting in better overall performance.",1
"When estimating the relevancy between a query and a document, ranking models largely neglect the mutual information among documents. A common wisdom is that if two documents are similar in terms of the same query, they are more likely to have similar relevance score. To mitigate this problem, in this paper, we propose a multi-agent reinforced ranking model, named MarlRank. In particular, by considering each document as an agent, we formulate the ranking process as a multi-agent Markov Decision Process (MDP), where the mutual interactions among documents are incorporated in the ranking process. To compute the ranking list, each document predicts its relevance to a query considering not only its own query-document features but also its similar documents features and actions. By defining reward as a function of NDCG, we can optimize our model directly on the ranking performance measure. Our experimental results on two LETOR benchmark datasets show that our model has significant performance gains over the state-of-art baselines. We also find that the NDCG shows an overall increasing trend along with the step of interactions, which demonstrates that the mutual information among documents helps improve the ranking performance.",0
"Ranking models often overlook the mutual information shared among documents when estimating the relevance between a query and a document. It's widely believed that if two documents are similar regarding the same query, they're likely to have similar relevance scores. To address this issue, we introduce a multi-agent reinforced ranking model called MarlRank. We treat each document as an agent and formulate the ranking process as a multi-agent Markov Decision Process (MDP) to incorporate mutual interactions among documents in the ranking process. To generate the ranking list, every document considers its query-document features, similar documents features, and actions when predicting its relevance to a query. We define rewards as a function of NDCG, which enables us to optimize our model directly on the ranking performance measure. Our experimental results on two LETOR benchmark datasets illustrate that our model outperforms state-of-the-art baselines significantly. Additionally, we observe an overall increase in NDCG as the interaction steps increase, indicating that mutual information among documents enhances ranking performance.",1
"The estimation of advantage is crucial for a number of reinforcement learning algorithms, as it directly influences the choices of future paths. In this work, we propose a family of estimates based on the order statistics over the path ensemble, which allows one to flexibly drive the learning process, towards or against risks. On top of this formulation, we systematically study the impacts of different methods for estimating advantages. Our findings reveal that biased estimates, when chosen appropriately, can result in significant benefits. In particular, for the environments with sparse rewards, optimistic estimates would lead to more efficient exploration of the policy space; while for those where individual actions can have critical impacts, conservative estimates are preferable. On various benchmarks, including MuJoCo continuous control, Terrain locomotion, Atari games, and sparse-reward environments, the proposed biased estimation schemes consistently demonstrate improvement over mainstream methods, not only accelerating the learning process but also obtaining substantial performance gains.",0
"The calculation of advantage is crucial in several reinforcement learning algorithms since it affects the choices made for future paths. In this research, we suggest a set of estimates that are based on the order statistics across the path ensemble, which provides the flexibility to guide the learning process in favor of or against risks. We also examine the effects of different methods for estimating advantages. Our results illustrate that biased estimates, when picked appropriately, can produce significant benefits. Specifically, in situations where rewards are scarce, optimistic estimates lead to more effective exploration of the policy space, whereas conservative estimates are preferable in situations where individual actions can have significant impacts. Across various benchmarks, such as MuJoCo continuous control, Terrain locomotion, Atari games, and environments with sparse rewards, the proposed biased estimation techniques consistently demonstrate an improvement over mainstream methods, enhancing both the learning process and the overall performance.",1
"Reinforcement learning frameworks have introduced abstractions to implement and execute algorithms at scale. They assume standardized simulator interfaces but are not concerned with identifying suitable task representations. We present Wield, a first-of-its kind system to facilitate task design for practical reinforcement learning. Through software primitives, Wield enables practitioners to decouple system-interface and deployment-specific configuration from state and action design. To guide experimentation, Wield further introduces a novel task design protocol and classification scheme centred around staged randomization to incrementally evaluate model capabilities.",0
"Abstractions have been introduced by reinforcement learning frameworks for the implementation and execution of algorithms on a larger scale. These frameworks make use of standardized simulator interfaces, but do not focus on identifying appropriate task representations. To address this, we have developed Wield, a unique system that enables practitioners to design practical reinforcement learning tasks. Wield separates system-interface and deployment-specific configuration from state and action design using software primitives. In addition, Wield introduces a novel task design protocol and classification scheme that revolves around staged randomization to progressively assess model capabilities and guide experimentation.",1
This paper proposes a novel deep reinforcement learning architecture that was inspired by previous tree structured architectures which were only useable in discrete action spaces. Policy Prediction Network offers a way to improve sample complexity and performance on continuous control problems in exchange for extra computation at training time but at no cost in computation at rollout time. Our approach integrates a mix between model-free and model-based reinforcement learning. Policy Prediction Network is the first to introduce implicit model-based learning to Policy Gradient algorithms for continuous action space and is made possible via the empirically justified clipping scheme. Our experiments are focused on the MuJoCo environments so that they can be compared with similar work done in this area.,0
"In this paper, a new deep reinforcement learning architecture is presented. The design is based on previous tree structures that were limited to discrete action spaces. The proposed architecture, called Policy Prediction Network, aims to enhance sample complexity and performance on continuous control problems. This is achieved through a combination of model-free and model-based reinforcement learning, without any additional computation during rollout. The Policy Prediction Network is the first to use implicit model-based learning in Policy Gradient algorithms for continuous action spaces, enabled by a justified clipping scheme. The experiments focus on the MuJoCo environments, allowing for comparison with similar research in the field.",1
"The goal of imitation learning (IL) is to learn a good policy from high-quality demonstrations. However, the quality of demonstrations in reality can be diverse, since it is easier and cheaper to collect demonstrations from a mix of experts and amateurs. IL in such situations can be challenging, especially when the level of demonstrators' expertise is unknown. We propose a new IL method called \underline{v}ariational \underline{i}mitation \underline{l}earning with \underline{d}iverse-quality demonstrations (VILD), where we explicitly model the level of demonstrators' expertise with a probabilistic graphical model and estimate it along with a reward function. We show that a naive approach to estimation is not suitable to large state and action spaces, and fix its issues by using a variational approach which can be easily implemented using existing reinforcement learning methods. Experiments on continuous-control benchmarks demonstrate that VILD outperforms state-of-the-art methods. Our work enables scalable and data-efficient IL under more realistic settings than before.",0
"Imitation learning (IL) aims to acquire a proficient policy from superior demonstrations. However, the actual quality of demonstrations can vary as collecting demonstrations from both experts and amateurs is easier and less expensive. This can make IL challenging, particularly when the level of expertise of the demonstrators is unknown. We suggest VILD, a new IL technique that considers diversely qualified demonstrations. VILD employs a probabilistic graphical model to explicitly model the level of expertise of the demonstrators and estimate it alongside a reward function. We demonstrate that a simple estimation approach is unsuitable for large state and action spaces and employ a variational approach that can be easily implemented utilizing existing reinforcement learning techniques. Experiments on continuous-control benchmarks show that VILD outperforms existing techniques. Our work facilitates scalable and data-efficient IL under more practical conditions.",1
"In class-incremental learning, a model learns continuously from a sequential data stream in which new classes occur. Existing methods often rely on static architectures that are manually crafted. These methods can be prone to capacity saturation because a neural network's ability to generalize to new concepts is limited by its fixed capacity. To understand how to expand a continual learner, we focus on the neural architecture design problem in the context of class-incremental learning: at each time step, the learner must optimize its performance on all classes observed so far by selecting the most competitive neural architecture. To tackle this problem, we propose Continual Neural Architecture Search (CNAS): an autoML approach that takes advantage of the sequential nature of class-incremental learning to efficiently and adaptively identify strong architectures in a continual learning setting. We employ a task network to perform the classification task and a reinforcement learning agent as the meta-controller for architecture search. In addition, we apply network transformations to transfer weights from previous learning step and to reduce the size of the architecture search space, thus saving a large amount of computational resources. We evaluate CNAS on the CIFAR-100 dataset under varied incremental learning scenarios with limited computational power (1 GPU). Experimental results demonstrate that CNAS outperforms architectures that are optimized for the entire dataset. In addition, CNAS is at least an order of magnitude more efficient than naively using existing autoML methods.",0
"The process of class-incremental learning involves a model learning continuously from a sequential data stream containing new classes. However, existing methods often use static architectures that are manually created, causing limitations in the neural network's ability to generalize to new concepts. To address this issue, we aim to expand the continual learner by optimizing its performance on all classes observed so far by selecting the most competitive neural architecture. To achieve this goal, we propose Continual Neural Architecture Search (CNAS), an autoML approach that utilizes the sequential nature of class-incremental learning to identify strong architectures efficiently and adaptively. Our method involves a task network for classification and a reinforcement learning agent as the meta-controller for architecture search. We also apply network transformations to transfer weights from previous learning steps and reduce the architecture search space's size, saving computational resources. Our evaluation of CNAS on the CIFAR-100 dataset demonstrates that it outperforms architectures optimized for the entire dataset, while also being at least an order of magnitude more efficient than existing autoML methods.",1
"Open domain dialog systems face the challenge of being repetitive and producing generic responses. In this paper, we demonstrate that by conditioning the response generation on interpretable discrete dialog attributes and composed attributes, it helps improve the model perplexity and results in diverse and interesting non-redundant responses. We propose to formulate the dialog attribute prediction as a reinforcement learning (RL) problem and use policy gradients methods to optimize utterance generation using long-term rewards. Unlike existing RL approaches which formulate the token prediction as a policy, our method reduces the complexity of the policy optimization by limiting the action space to dialog attributes, thereby making the policy optimization more practical and sample efficient. We demonstrate this with experimental and human evaluations.",0
"The challenge faced by open domain dialog systems is that they tend to produce generic responses and repeat themselves. This study aims to address this issue by utilizing interpretable discrete dialog attributes and composed attributes to enhance the model perplexity and generate diverse and unique responses. The authors suggest using reinforcement learning (RL) to predict dialog attributes and optimize utterance generation with long-term rewards. Unlike previous RL methods that consider token prediction as policy, this study limits the action space to dialog attributes to simplify policy optimization and improve sample efficiency. The effectiveness of this approach is supported by both experimental and human evaluations.",1
"Real-world graph applications, such as advertisements and product recommendations make profits based on accurately classify the label of the nodes. However, in such scenarios, there are high incentives for the adversaries to attack such graph to reduce the node classification performance. Previous work on graph adversarial attacks focus on modifying existing graph structures, which is infeasible in most real-world applications. In contrast, it is more practical to inject adversarial nodes into existing graphs, which can also potentially reduce the performance of the classifier. In this paper, we study the novel node injection poisoning attacks problem which aims to poison the graph. We describe a reinforcement learning based method, namely NIPA, to sequentially modify the adversarial information of the injected nodes. We report the results of experiments using several benchmark data sets that show the superior performance of the proposed method NIPA, relative to the existing state-of-the-art methods.",0
"Accurately labeling nodes is crucial for real-world graph applications like advertisements and product recommendations, as it directly impacts profits. However, adversaries have strong incentives to attack graphs and hinder node classification performance. Previous research on graph adversarial attacks has focused on modifying graph structures, but this is often impractical in real-world scenarios. Instead, injecting adversarial nodes into existing graphs is more feasible, yet still reduces classifier performance. This paper introduces a novel problem of node injection poisoning attacks that aims to poison graphs. We propose NIPA, a reinforcement learning-based approach to sequentially modify adversarial information of injected nodes. Our experiments on various benchmark datasets demonstrate the superior performance of NIPA over existing state-of-the-art methods.",1
"Traditional control methods are inadequate in many deployment settings involving control of Cyber-Physical Systems (CPS). In such settings, CPS controllers must operate and respond to unpredictable interactions, conditions, or failure modes. Dealing with such unpredictability requires the use of executive and cognitive control functions that allow for planning and reasoning. Motivated by the sport of drone racing, this dissertation addresses these concerns for state-of-the-art flight control by investigating the use of deep neural networks to bring essential elements of higher-level cognition for constructing low level flight controllers.   This thesis reports on the development and release of an open source, full solution stack for building neuro-flight controllers. This stack consists of the methodology for constructing a multicopter digital twin for synthesize the flight controller unique to a specific aircraft, a tuning framework for implementing training environments (GymFC), and a firmware for the world's first neural network supported flight controller (Neuroflight). GymFC's novel approach fuses together the digital twinning paradigm for flight control training to provide seamless transfer to hardware. Additionally, this thesis examines alternative reward system functions as well as changes to the software environment to bridge the gap between the simulation and real world deployment environments.   Work summarized in this thesis demonstrates that reinforcement learning is able to be leveraged for training neural network controllers capable, not only of maintaining stable flight, but also precision aerobatic maneuvers in real world settings. As such, this work provides a foundation for developing the next generation of flight control systems.",0
"In many situations where Cyber-Physical Systems (CPS) need to be controlled, traditional methods are insufficient. These settings often involve unpredictable interactions, conditions, or failure modes, which require executive and cognitive control functions for planning and reasoning. This dissertation focuses on drone racing and addresses these concerns by exploring the use of deep neural networks to construct low-level flight controllers with higher-level cognition. The thesis presents an open-source full solution stack for building neuro-flight controllers, including a methodology for constructing a multicopter digital twin, a tuning framework for implementing training environments (GymFC), and a firmware for the world's first neural network supported flight controller (Neuroflight). Additionally, the thesis examines alternative reward system functions and changes to the software environment to bridge the gap between simulation and real-world deployment environments. The work in this thesis shows that reinforcement learning can train neural network controllers capable of maintaining stable flight and precision aerobatic maneuvers in real-world settings, providing a foundation for the next generation of flight control systems.",1
"In this work, we address the problem of tuning communication libraries by using a deep reinforcement learning approach. Reinforcement learning is a machine learning technique incredibly effective in solving game-like situations. In fact, tuning a set of parameters in a communication library in order to get better performance in a parallel application can be expressed as a game: Find the right combination/path that provides the best reward. Even though AITuning has been designed to be utilized with different run-time libraries, we focused this work on applying it to the OpenCoarrays run-time communication library, built on top of MPI-3. This work not only shows the potential of using a reinforcement learning algorithm for tuning communication libraries, but also demonstrates how the MPI Tool Information Interface, introduced by the MPI-3 standard, can be used effectively by run-time libraries to improve the performance without human intervention.",0
"The problem of optimizing communication libraries through deep reinforcement learning is addressed in this study. Reinforcement learning, a powerful machine learning technique for game-like scenarios, is employed to tune a communication library's parameters and enhance parallel application performance. This approach treats the process of finding the optimal combination or path as a game where the best reward is sought. Although AITuning can be used with various run-time libraries, the primary focus of this study is on the OpenCoarrays run-time communication library, which is based on MPI-3. The findings of this study not only demonstrate the potential of reinforcement learning algorithms for optimizing communication libraries, but also highlight how MPI Tool Information Interface, which was introduced in the MPI-3 standard, can be used by run-time libraries to improve performance autonomously.",1
"More and more companies have deployed machine learning (ML) clusters, where deep learning (DL) models are trained for providing various AI-driven services. Efficient resource scheduling is essential for maximal utilization of expensive DL clusters. Existing cluster schedulers either are agnostic to ML workload characteristics, or use scheduling heuristics based on operators' understanding of particular ML framework and workload, which are less efficient or not general enough. In this paper, we show that DL techniques can be adopted to design a generic and efficient scheduler. DL2 is a DL-driven scheduler for DL clusters, targeting global training job expedition by dynamically resizing resources allocated to jobs. DL2 advocates a joint supervised learning and reinforcement learning approach: a neural network is warmed up via offline supervised learning based on job traces produced by the existing cluster scheduler; then the neural network is plugged into the live DL cluster, fine-tuned by reinforcement learning carried out throughout the training progress of the DL jobs, and used for deciding job resource allocation in an online fashion. By applying past decisions made by the existing cluster scheduler in the preparatory supervised learning phase, our approach enables a smooth transition from existing scheduler, and renders a high-quality scheduler in minimizing average training completion time. We implement DL2 on Kubernetes and enable dynamic resource scaling in DL jobs on MXNet. Extensive evaluation shows that DL2 outperforms fairness scheduler (i.e., DRF) by 44.1% and expert heuristic scheduler (i.e., Optimus) by 17.5% in terms of average job completion time.",0
"A growing number of companies are utilizing machine learning (ML) clusters to train deep learning (DL) models that provide various AI services. However, efficient resource scheduling is crucial for optimal use of these costly DL clusters. Current cluster schedulers are either unaware of ML workload characteristics or rely on scheduling heuristics that are less effective or too specific to particular ML frameworks and workloads. To address this issue, we propose a DL-driven scheduler, DL2, that utilizes supervised and reinforcement learning to dynamically resize resources allocated to jobs and expedite global training job completion. DL2 is designed to seamlessly transition from existing schedulers by utilizing past decisions made during the preparatory supervised learning phase. Our implementation of DL2 on Kubernetes enables dynamic resource scaling in DL jobs on MXNet. Extensive evaluations indicate that DL2 outperforms fairness scheduler (DRF) by 44.1% and expert heuristic scheduler (Optimus) by 17.5% in terms of average job completion time.",1
"In this paper we consider the basic version of Reinforcement Learning (RL) that involves computing optimal data driven (adaptive) policies for Markovian decision process with unknown transition probabilities. We provide a brief survey of the state of the art of the area and we compare the performance of the classic UCB policy of \cc{bkmdp97} with a new policy developed herein which we call MDP-Deterministic Minimum Empirical Divergence (MDP-DMED), and a method based on Posterior sampling (MDP-PS).",0
"The focus of this paper is on the fundamental Reinforcement Learning (RL) approach, which seeks to derive adaptable data-driven policies for Markov decision processes with uncertain transition probabilities. A concise overview of this field's current status is presented, followed by a comparison of the traditional UCB policy from \cc{bkmdp97} against two alternative policies: one of our own design, known as MDP-Deterministic Minimum Empirical Divergence (MDP-DMED), and another method based on Posterior sampling (MDP-PS).",1
"Social sensing has emerged as a new sensing paradigm where humans (or devices on their behalf) collectively report measurements about the physical world. This paper focuses on a quality-cost-aware task allocation problem in multi-attribute social sensing applications. The goal is to identify a task allocation strategy (i.e., decide when and where to collect sensing data) to achieve an optimized tradeoff between the data quality and the sensing cost. While recent progress has been made to tackle similar problems, three important challenges have not been well addressed: (i) ""online task allocation"": the task allocation schemes need to respond quickly to the potentially large dynamics of the measured variables in social sensing; (ii) ""multi-attribute constrained optimization"": minimizing the overall sensing error given the dependencies and constraints of multiple attributes of the measured variables is a non-trivial problem to solve; (iii) ""nonuniform task allocation cost"": the task allocation cost in social sensing often has a nonuniform distribution which adds additional complexity to the optimized task allocation problem. This paper develops a Quality-Cost-Aware Online Task Allocation (QCO-TA) scheme to address the above challenges using a principled online reinforcement learning framework. We evaluate the QCO-TA scheme through a real-world social sensing application and the results show that our scheme significantly outperforms the state-of-the-art baselines in terms of both sensing accuracy and cost.",0
"The concept of social sensing involves people or devices collectively reporting measurements of the physical world. This article focuses on solving a task allocation problem for multi-attribute social sensing applications that takes into account both the quality and cost of data collection. Although progress has been made in addressing similar challenges, there are three specific obstacles that require further attention: (i) the need for online task allocation that can quickly respond to changes in measured variables; (ii) the challenge of optimizing sensing accuracy while taking into consideration the dependencies and constraints of multiple attributes; and (iii) the added complexity of nonuniform task allocation costs in social sensing. To overcome these challenges, the authors propose a Quality-Cost-Aware Online Task Allocation (QCO-TA) scheme that utilizes an online reinforcement learning framework. The scheme is evaluated in a real-world social sensing application, and the results demonstrate that it outperforms existing methods in terms of both sensing accuracy and cost.",1
"Cumulative entropy regularization introduces a regulatory signal to the reinforcement learning (RL) problem that encourages policies with high-entropy actions, which is equivalent to enforcing small deviations from a uniform reference marginal policy. This has been shown to improve exploration and robustness, and it tackles the value overestimation problem. It also leads to a significant performance increase in tabular and high-dimensional settings, as demonstrated via algorithms such as soft Q-learning (SQL) and soft actor-critic (SAC). Cumulative entropy regularization has been extended to optimize over the reference marginal policy instead of keeping it fixed, yielding a regularization that minimizes the mutual information between states and actions. While this has been initially proposed for Markov Decision Processes (MDPs) in tabular settings, it was recently shown that a similar principle leads to significant improvements over vanilla SQL in RL for high-dimensional domains with discrete actions and function approximators.   Here, we follow the motivation of mutual-information regularization from an inference perspective and theoretically analyze the corresponding Bellman operator. Inspired by this Bellman operator, we devise a novel mutual-information regularized actor-critic learning (MIRACLE) algorithm for continuous action spaces that optimizes over the reference marginal policy. We empirically validate MIRACLE in the Mujoco robotics simulator, where we demonstrate that it can compete with contemporary RL methods. Most notably, it can improve over the model-free state-of-the-art SAC algorithm which implicitly assumes a fixed reference policy.",0
"The use of cumulative entropy regularization in reinforcement learning (RL) has proven to be effective in encouraging policies with high-entropy actions, resulting in improved exploration and robustness, as well as addressing the value overestimation problem. This method has been successful in tabular and high-dimensional settings, with algorithms such as soft Q-learning (SQL) and soft actor-critic (SAC). Recent developments have extended cumulative entropy regularization to optimize over the reference marginal policy, leading to a regularization that minimizes the mutual information between states and actions. While originally proposed for Markov Decision Processes (MDPs) in tabular settings, this approach has been demonstrated to be beneficial for RL in high-dimensional domains with discrete actions and function approximators. Our work follows the same motivation of mutual-information regularization from an inference perspective, and we present a novel mutual-information regularized actor-critic learning (MIRACLE) algorithm for continuous action spaces that optimizes over the reference marginal policy. We validate MIRACLE in the Mujoco robotics simulator, where we show that it can compete with contemporary RL methods and outperform the state-of-the-art SAC algorithm, which assumes a fixed reference policy.",1
"Structured weight pruning is a representative model compression technique of DNNs to reduce the storage and computation requirements and accelerate inference. An automatic hyperparameter determination process is necessary due to the large number of flexible hyperparameters. This work proposes AutoCompress, an automatic structured pruning framework with the following key performance improvements: (i) effectively incorporate the combination of structured pruning schemes in the automatic process; (ii) adopt the state-of-art ADMM-based structured weight pruning as the core algorithm, and propose an innovative additional purification step for further weight reduction without accuracy loss; and (iii) develop effective heuristic search method enhanced by experience-based guided search, replacing the prior deep reinforcement learning technique which has underlying incompatibility with the target pruning problem. Extensive experiments on CIFAR-10 and ImageNet datasets demonstrate that AutoCompress is the key to achieve ultra-high pruning rates on the number of weights and FLOPs that cannot be achieved before. As an example, AutoCompress outperforms the prior work on automatic model compression by up to 33x in pruning rate (120x reduction in the actual parameter count) under the same accuracy. Significant inference speedup has been observed from the AutoCompress framework on actual measurements on smartphone. We release all models of this work at anonymous link: http://bit.ly/2VZ63dS.",0
"Structured weight pruning is a well-known method for compressing DNNs by reducing storage and computation requirements, thereby speeding up inference. However, due to the large number of flexible hyperparameters involved, an automatic hyperparameter determination process is necessary. This paper presents AutoCompress, an automatic structured pruning framework that improves performance in three key ways: by effectively incorporating structured pruning schemes, by using an innovative additional purification step to reduce weight without accuracy loss, and by developing a heuristic search method enhanced by experience-based guided search. Extensive experiments on CIFAR-10 and ImageNet datasets demonstrate that AutoCompress achieves ultra-high pruning rates on the number of weights and FLOPs that were previously unattainable. For instance, AutoCompress outperforms prior work on automatic model compression by up to 33x in pruning rate (120x reduction in the actual parameter count) under the same accuracy. The framework also produces significant inference speedup on smartphones. All models are available at this anonymous link: http://bit.ly/2VZ63dS.",1
"As reinforcement learning (RL) achieves more success in solving complex tasks, more care is needed to ensure that RL research is reproducible and that algorithms herein can be compared easily and fairly with minimal bias. RL results are, however, notoriously hard to reproduce due to the algorithms' intrinsic variance, the environments' stochasticity, and numerous (potentially unreported) hyper-parameters. In this work we investigate the many issues leading to irreproducible research and how to manage those. We further show how to utilise a rigorous and standardised evaluation approach for easing the process of documentation, evaluation and fair comparison of different algorithms, where we emphasise the importance of choosing the right measurement metrics and conducting proper statistics on the results, for unbiased reporting of the results.",0
"With the increasing success of reinforcement learning (RL) in tackling complex tasks, it is crucial to ensure that RL research is reproducible and that algorithms can be compared fairly with minimal bias. However, reproducing RL results can be challenging due to various factors, such as intrinsic variance of algorithms, stochasticity of environments, and unreported hyper-parameters. In this study, we explore the factors that contribute to irreproducible research and propose solutions to manage them. We also introduce a rigorous and standardized evaluation approach to facilitate documentation, evaluation, and fair comparison of different algorithms. Choosing appropriate measurement metrics and conducting proper statistical analysis are emphasized to ensure unbiased reporting of results.",1
"Proximal policy optimization and trust region policy optimization (PPO and TRPO) with actor and critic parametrized by neural networks achieve significant empirical success in deep reinforcement learning. However, due to nonconvexity, the global convergence of PPO and TRPO remains less understood, which separates theory from practice. In this paper, we prove that a variant of PPO and TRPO equipped with overparametrized neural networks converges to the globally optimal policy at a sublinear rate. The key to our analysis is the global convergence of infinite-dimensional mirror descent under a notion of one-point monotonicity, where the gradient and iterate are instantiated by neural networks. In particular, the desirable representation power and optimization geometry induced by the overparametrization of such neural networks allow them to accurately approximate the infinite-dimensional gradient and iterate.",0
"The effectiveness of deep reinforcement learning has been demonstrated by Proximal Policy Optimization (PPO) and Trust Region Policy Optimization (TRPO) with neural network-based actor and critic parameters. However, the non-convexity of these methods has made their global convergence less clear, creating a gap between theory and practice. This study shows that an altered version of PPO and TRPO with over-parametrized neural networks can converge to the optimal policy at a sublinear pace. The research utilizes the global convergence of infinite-dimensional mirror descent, which relies on one-point monotonicity, and implements neural networks to instantiate the gradient and iterate. The over-parametrization of neural networks results in the desirable representation power and optimization geometry, enabling them to accurately approximate the infinite-dimensional gradient and iterate.",1
"Bandit algorithms have been predominantly analyzed in the convex setting with function-value based stationary regret as the performance measure. In this paper, motivated by online reinforcement learning problems, we propose and analyze bandit algorithms for both general and structured nonconvex problems with nonstationary (or dynamic) regret as the performance measure, in both stochastic and non-stochastic settings. First, for general nonconvex functions, we consider nonstationary versions of first-order and second-order stationary solutions as a regret measure, motivated by similar performance measures for offline nonconvex optimization. In the case of second-order stationary solution based regret, we propose and analyze online and bandit versions of the cubic regularized Newton's method. The bandit version is based on estimating the Hessian matrices in the bandit setting, based on second-order Gaussian Stein's identity. Our nonstationary regret bounds in terms of second-order stationary solutions have interesting consequences for avoiding saddle points in the bandit setting. Next, for weakly quasi convex functions and monotone weakly submodular functions we consider nonstationary regret measures in terms of function-values; such structured classes of nonconvex functions enable one to consider regret measure defined in terms of function values, similar to convex functions. For this case of function-value, and first-order stationary solution based regret measures, we provide regret bounds in both the low- and high-dimensional settings, for some scenarios.",0
"This paper proposes and analyzes bandit algorithms for nonconvex problems with nonstationary regret as the performance measure, in both stochastic and non-stochastic settings. The study focuses on general nonconvex functions, weakly quasi convex functions, and monotone weakly submodular functions. For general nonconvex functions, nonstationary versions of first-order and second-order stationary solutions are considered as a regret measure. The second-order stationary solution-based regret is addressed with the online and bandit versions of the cubic regularized Newton's method. The nonstationary regret bounds for second-order stationary solutions can help avoid saddle points in the bandit setting. For weakly quasi convex functions and monotone weakly submodular functions, nonstationary regret measures in terms of function-values are considered. Regret bounds for both low- and high-dimensional settings are provided for function-value and first-order stationary solution-based regret measures.",1
Reinforcement learning has exceeded human-level performance in game playing AI with deep learning methods according to the experiments from DeepMind on Go and Atari games. Deep learning solves high dimension input problems which stop the development of reinforcement for many years. This study uses both two techniques to create several agents with different algorithms that successfully learn to play T-rex Runner. Deep Q network algorithm and three types of improvements are implemented to train the agent. The results from some of them are far from satisfactory but others are better than human experts. Batch normalization is a method to solve internal covariate shift problems in deep neural network. The positive influence of this on reinforcement learning has also been proved in this study.,0
"Recent experiments by DeepMind on Go and Atari games have shown that deep learning methods have allowed reinforcement learning to surpass human-level performance in game playing AI. The use of deep learning has resolved the high dimension input issues that had previously hindered the development of reinforcement learning for many years. This study utilized both techniques to create multiple agents with various algorithms that successfully learned to play T-rex Runner. The Deep Q network algorithm and three types of improvements were employed to train the agent, with some of the results being better than those of human experts while others were far from satisfactory. In addition, this study also demonstrated the positive impact of batch normalization, a method for solving internal covariate shift issues in deep neural networks, on reinforcement learning.",1
"Generative Adversarial Networks (GANs) have been used widely to generate large volumes of synthetic data. This data is being utilized for augmenting with real examples in order to train deep Convolutional Neural Networks (CNNs). Studies have shown that the generated examples lack sufficient realism to train deep CNNs and are poor in diversity. Unlike previous studies of randomly augmenting the synthetic data with real data, we present our simple, effective and easy to implement synthetic data sampling methods to train deep CNNs more efficiently and accurately. To this end, we propose to maximally utilize the parameters learned during training of the GAN itself. These include discriminator's realism confidence score and the confidence on the target label of the synthetic data. In addition to this, we explore reinforcement learning (RL) to automatically search a subset of meaningful synthetic examples from a large pool of GAN synthetic data. We evaluate our method on two challenging face attribute classification data sets viz. AffectNet and CelebA. Our extensive experiments clearly demonstrate the need of sampling synthetic data before augmentation, which also improves the performance of one of the state-of-the-art deep CNNs in vitro.",0
"The use of Generative Adversarial Networks (GANs) to produce synthetic data has become widespread. However, this data lacks the realism and diversity necessary to effectively train deep Convolutional Neural Networks (CNNs) when used in combination with real examples. Rather than randomly augmenting synthetic data with real data, we propose a straightforward and effective method to sample synthetic data that allows deep CNNs to be trained more efficiently and accurately. Our method involves maximizing the parameters learned during GAN training, including the discriminator's realism confidence score and the confidence of the target label of the synthetic data. Additionally, we explore the use of reinforcement learning (RL) to automatically search for a meaningful subset of synthetic examples from a large pool of GAN synthetic data. Our approach is evaluated on two challenging face attribute classification datasets, AffectNet and CelebA. Our extensive experiments demonstrate the importance of sampling synthetic data before augmentation, which enhances the performance of one of the state-of-the-art deep CNNs.",1
"We propose RUDDER, a novel reinforcement learning approach for delayed rewards in finite Markov decision processes (MDPs). In MDPs the Q-values are equal to the expected immediate reward plus the expected future rewards. The latter are related to bias problems in temporal difference (TD) learning and to high variance problems in Monte Carlo (MC) learning. Both problems are even more severe when rewards are delayed. RUDDER aims at making the expected future rewards zero, which simplifies Q-value estimation to computing the mean of the immediate reward. We propose the following two new concepts to push the expected future rewards toward zero. (i) Reward redistribution that leads to return-equivalent decision processes with the same optimal policies and, when optimal, zero expected future rewards. (ii) Return decomposition via contribution analysis which transforms the reinforcement learning task into a regression task at which deep learning excels. On artificial tasks with delayed rewards, RUDDER is significantly faster than MC and exponentially faster than Monte Carlo Tree Search (MCTS), TD({\lambda}), and reward shaping approaches. At Atari games, RUDDER on top of a Proximal Policy Optimization (PPO) baseline improves the scores, which is most prominent at games with delayed rewards. Source code is available at \url{https://github.com/ml-jku/rudder} and demonstration videos at \url{https://goo.gl/EQerZV}.",0
"RUDDER is a new approach to reinforcement learning that addresses the issue of delayed rewards in finite Markov decision processes (MDPs). In MDPs, the Q-values are comprised of both immediate and future rewards, which can lead to bias problems in temporal difference (TD) learning and high variance problems in Monte Carlo (MC) learning. These issues become even more significant when rewards are delayed. RUDDER aims to simplify Q-value estimation by making expected future rewards zero, which is achieved through two new concepts: reward redistribution and return decomposition via contribution analysis. These concepts push expected future rewards towards zero, transforming the reinforcement learning task into a regression task that deep learning excels at. RUDDER outperforms MC and Monte Carlo Tree Search (MCTS), TD({\lambda}), and reward shaping approaches on artificial tasks with delayed rewards. Additionally, using RUDDER on top of a Proximal Policy Optimization (PPO) baseline improves scores on Atari games, particularly those with delayed rewards. The source code and demonstration videos are available at \url{https://github.com/ml-jku/rudder} and \url{https://goo.gl/EQerZV}, respectively.",1
"We consider undiscounted reinforcement learning in Markov decision processes (MDPs) where both the reward functions and the state-transition probabilities may vary (gradually or abruptly) over time. For this problem setting, we propose an algorithm and provide performance guarantees for the regret evaluated against the optimal non-stationary policy. The upper bound on the regret is given in terms of the total variation in the MDP. This is the first variational regret bound for the general reinforcement learning setting.",0
"Our focus is on undiscounted reinforcement learning within Markov decision processes (MDPs), where the state-transition probabilities and reward functions may change over time, either gradually or suddenly. To tackle this issue, we introduce an algorithm and offer assurance about its performance by measuring regret against the optimal non-stationary policy. Our regret upper bound is expressed in terms of the MDP's total variation, representing the first variational regret bound for the general reinforcement learning context.",1
"Graph neural networks (GNN) has been successfully applied to operate on the graph-structured data. Given a specific scenario, rich human expertise and tremendous laborious trials are usually required to identify a suitable GNN architecture. It is because the performance of a GNN architecture is significantly affected by the choice of graph convolution components, such as aggregate function and hidden dimension. Neural architecture search (NAS) has shown its potential in discovering effective deep architectures for learning tasks in image and language modeling. However, existing NAS algorithms cannot be directly applied to the GNN search problem. First, the search space of GNN is different from the ones in existing NAS work. Second, the representation learning capacity of GNN architecture changes obviously with slight architecture modifications. It affects the search efficiency of traditional search methods. Third, widely used techniques in NAS such as parameter sharing might become unstable in GNN.   To bridge the gap, we propose the automated graph neural networks (AGNN) framework, which aims to find an optimal GNN architecture within a predefined search space. A reinforcement learning based controller is designed to greedily validate architectures via small steps. AGNN has a novel parameter sharing strategy that enables homogeneous architectures to share parameters, based on a carefully-designed homogeneity definition. Experiments on real-world benchmark datasets demonstrate that the GNN architecture identified by AGNN achieves the best performance, comparing with existing handcrafted models and tradistional search methods.",0
"The successful application of graph neural networks (GNN) on graph-structured data typically requires significant human expertise and laborious trial and error to identify an appropriate architecture. This is due to the significant impact that the choice of graph convolution components, such as aggregate function and hidden dimension, has on the performance of a GNN architecture in a given scenario. Although neural architecture search (NAS) has been effective in finding effective architectures for image and language modeling tasks, it cannot be directly applied to the GNN search problem. This is because the search space for GNN is different from those in existing NAS work, and slight architecture modifications can significantly change the representation learning capacity of a GNN architecture, affecting the search efficiency of traditional methods. Additionally, widely used techniques such as parameter sharing may become unstable in GNN. To address these challenges, we propose the automated graph neural networks (AGNN) framework, which uses a reinforcement learning-based controller to search for an optimal GNN architecture within a predefined search space. AGNN also employs a novel parameter sharing strategy based on a carefully-designed homogeneity definition. Our experiments on real-world benchmark datasets show that AGNN can identify the best-performing GNN architecture compared to existing handcrafted models and traditional search methods.",1
"In this paper we propose a deep neural network model with an encoder-decoder architecture that translates images of math formulas into their LaTeX markup sequences. The encoder is a convolutional neural network (CNN) that transforms images into a group of feature maps. To better capture the spatial relationships of math symbols, the feature maps are augmented with 2D positional encoding before being unfolded into a vector. The decoder is a stacked bidirectional long short-term memory (LSTM) model integrated with the soft attention mechanism, which works as a language model to translate the encoder output into a sequence of LaTeX tokens. The neural network is trained in two steps. The first step is token-level training using the Maximum-Likelihood Estimation (MLE) as the objective function. At completion of the token-level training, the sequence-level training objective function is employed to optimize the overall model based on the policy gradient algorithm from reinforcement learning. Our design also overcomes the exposure bias problem by closing the feedback loop in the decoder during sequence-level training, i.e., feeding in the predicted token instead of the ground truth token at every time step. The model is trained and evaluated on the IM2LATEX-100K dataset and shows state-of-the-art performance on both sequence-based and image-based evaluation metrics.",0
"This paper presents a novel deep neural network model that utilizes an encoder-decoder architecture to convert math formula images into LaTeX markup sequences. The encoder component is a convolutional neural network that produces feature maps enhanced with 2D positional encoding to better comprehend the spatial relationships of mathematical symbols. The decoder component is a stacked bidirectional long short-term memory model coupled with the soft attention mechanism, which acts as a language model to generate LaTeX tokens from the encoder output. The neural network is trained in two phases, namely token-level training using Maximum-Likelihood Estimation and sequence-level training using policy gradient algorithm from reinforcement learning. This design also addresses the exposure bias issue by employing a feedback loop in the decoder during sequence-level training. The model is evaluated on the IM2LATEX-100K dataset and shows superior performance on both sequence-based and image-based evaluation metrics.",1
"The use of semantic segmentation for masking and cropping input images has proven to be a significant aid in medical imaging classification tasks by decreasing the noise and variance of the training dataset. However, implementing this approach with classical methods is challenging: the cost of obtaining a dense segmentation is high, and the precise input area that is most crucial to the classification task is difficult to determine a-priori. We propose a novel joint-training deep reinforcement learning framework for image augmentation. A segmentation network, weakly supervised with policy gradient optimization, acts as an agent, and outputs masks as actions given samples as states, with the goal of maximizing reward signals from the classification network. In this way, the segmentation network learns to mask unimportant imaging features. Our method, Adversarial Policy Gradient Augmentation (APGA), shows promising results on Stanford's MURA dataset and on a hip fracture classification task with an increase in global accuracy of up to 7.33% and improved performance over baseline methods in 9/10 tasks evaluated. We discuss the broad applicability of our joint training strategy to a variety of medical imaging tasks.",0
"The utilization of semantic segmentation to mask and crop input images has been found to be highly beneficial in medical imaging classification tasks as it reduces noise and variance in the training dataset. However, implementing this method using classical approaches is challenging due to the high cost of obtaining dense segmentation and the difficulty in determining the most critical input area for the classification task in advance. To address these challenges, we present a new framework for image augmentation using a joint-training deep reinforcement learning approach. Our segmentation network is weakly supervised with policy gradient optimization and acts as an agent that outputs masks as actions given samples as states. The goal is to maximize reward signals from the classification network, thus teaching the segmentation network to mask unimportant imaging features. Our method, Adversarial Policy Gradient Augmentation (APGA), shows promising results in various medical imaging tasks, including a hip fracture classification task and Stanford's MURA dataset. We demonstrate an increase in global accuracy of up to 7.33% and improved performance over baseline methods in 9/10 tasks evaluated. Moreover, we discuss how our joint training strategy can be broadly applied in various medical imaging tasks.",1
"Variational inference with {\alpha}-divergences has been widely used in modern probabilistic machine learning. Compared to Kullback-Leibler (KL) divergence, a major advantage of using {\alpha}-divergences (with positive {\alpha} values) is their mass-covering property. However, estimating and optimizing {\alpha}-divergences require to use importance sampling, which could have extremely large or infinite variances due to heavy tails of importance weights. In this paper, we propose a new class of tail-adaptive f-divergences that adaptively change the convex function f with the tail of the importance weights, in a way that theoretically guarantees finite moments, while simultaneously achieving mass-covering properties. We test our methods on Bayesian neural networks, as well as deep reinforcement learning in which our method is applied to improve a recent soft actor-critic (SAC) algorithm. Our results show that our approach yields significant advantages compared with existing methods based on classical KL and {\alpha}-divergences.",0
"The use of {\alpha}-divergences in variational inference is widespread in contemporary probabilistic machine learning. These divergences possess a mass-covering property that is advantageous over Kullback-Leibler (KL) divergence, particularly when positive {\alpha} values are employed. However, the process of estimating and optimizing {\alpha}-divergences necessitates importance sampling, which may result in exceedingly large or infinite variances due to the tails of importance weights. This paper introduces a novel class of f-divergences that are tail-adaptive and utilize convex function f to tailor to the tail of the importance weights, ensuring finite moments while simultaneously achieving mass-covering properties. We employ our method to Bayesian neural networks and deep reinforcement learning, where we improve a recent soft actor-critic (SAC) algorithm. Our results demonstrate that our approach provides significant advantages over existing methods that rely on classical KL and {\alpha}-divergences.",1
"We consider the Markov Decision Process (MDP) of selecting a subset of items at each step, termed the Select-MDP (S-MDP). The large state and action spaces of S-MDPs make them intractable to solve with typical reinforcement learning (RL) algorithms especially when the number of items is huge. In this paper, we present a deep RL algorithm to solve this issue by adopting the following key ideas. First, we convert the original S-MDP into an Iterative Select-MDP (IS-MDP), which is equivalent to the S-MDP in terms of optimal actions. IS-MDP decomposes a joint action of selecting K items simultaneously into K iterative selections resulting in the decrease of actions at the expense of an exponential increase of states. Second, we overcome this state space explo-sion by exploiting a special symmetry in IS-MDPs with novel weight shared Q-networks, which prov-ably maintain sufficient expressive power. Various experiments demonstrate that our approach works well even when the item space is large and that it scales to environments with item spaces different from those used in training.",0
"The Select-MDP (S-MDP) involves choosing a subset of items at each step, but the large state and action spaces make it difficult to solve with typical reinforcement learning (RL) algorithms, especially for a large number of items. In this paper, we propose a deep RL algorithm to address this issue by converting the S-MDP into an Iterative Select-MDP (IS-MDP) that decomposes the joint action of selecting K items into K iterative selections, resulting in a decrease in actions but an exponential increase in states. To tackle this state space explosion, we use weight shared Q-networks that exploit a special symmetry in IS-MDPs, which maintain sufficient expressive power. Our approach is effective even with a large item space and can scale to different environments. Various experiments demonstrate the effectiveness of our approach.",1
"With the recent evolution of mobile health technologies, health scientists are increasingly interested in developing just-in-time adaptive interventions (JITAIs), typically delivered via notification on mobile device and designed to help the user prevent negative health outcomes and promote the adoption and maintenance of healthy behaviors. A JITAI involves a sequence of decision rules (i.e., treatment policy) that takes the user's current context as input and specifies whether and what type of an intervention should be provided at the moment. In this paper, we develop a Reinforcement Learning (RL) algorithm that continuously learns and improves the treatment policy embedded in the JITAI as the data is being collected from the user. This work is motivated by our collaboration on designing the RL algorithm in HeartSteps V2 based on data from HeartSteps V1. HeartSteps is a physical activity mobile health application. The RL algorithm developed in this paper is being used in HeartSteps V2 to decide, five times per day, whether to deliver a context-tailored activity suggestion.",0
"Health scientists are increasingly interested in developing just-in-time adaptive interventions (JITAIs) to promote healthy behaviors and prevent negative health outcomes. These interventions are usually delivered via mobile device notifications and involve a sequence of decision rules based on the user's current context. This paper presents a Reinforcement Learning (RL) algorithm that continuously learns and improves the treatment policy embedded in the JITAI as data is collected from the user. The algorithm was developed for HeartSteps V2, a physical activity mobile health application, and is used to decide five times a day whether to provide a context-tailored activity suggestion. The research is motivated by collaboration on designing the RL algorithm in HeartSteps V1.",1
"Modeling and prediction of human motion dynamics has long been a challenging problem in computer vision, and most existing methods rely on the end-to-end supervised training of various architectures of recurrent neural networks. Inspired by the recent success of deep reinforcement learning methods, in this paper we propose a new reinforcement learning formulation for the problem of human pose prediction, and develop an imitation learning algorithm for predicting future poses under this formulation through a combination of behavioral cloning and generative adversarial imitation learning. Our experiments show that our proposed method outperforms all existing state-of-the-art baseline models by large margins on the task of human pose prediction in both short-term predictions and long-term predictions, while also enjoying huge advantage in training speed.",0
"For a long time, it has been difficult to model and predict human motion dynamics in computer vision. Most current methods rely on supervised training of various architectures of recurrent neural networks. However, inspired by the success of deep reinforcement learning techniques, this study proposes a new reinforcement learning approach for predicting human poses. An imitation learning algorithm is developed using a combination of behavioral cloning and generative adversarial imitation learning to predict future poses. Experimental results demonstrate that this approach outperforms existing state-of-the-art models by significant margins in both short-term and long-term predictions, and is also faster to train.",1
"Value function estimation is an important task in reinforcement learning, i.e., prediction. The Boltzmann softmax operator is a natural value estimator and can provide several benefits. However, it does not satisfy the non-expansion property, and its direct use may fail to converge even in value iteration. In this paper, we propose to update the value function with dynamic Boltzmann softmax (DBS) operator, which has good convergence property in the setting of planning and learning. Experimental results on GridWorld show that the DBS operator enables better estimation of the value function, which rectifies the convergence issue of the softmax operator. Finally, we propose the DBS-DQN algorithm by applying dynamic Boltzmann softmax updates in deep Q-network, which outperforms DQN substantially in 40 out of 49 Atari games.",0
"Reinforcement learning involves predicting value functions, which is a crucial task. The Boltzmann softmax operator is a natural value estimator that has several advantages. However, it does not meet the non-expansion property, which means that its use may lead to convergence failure even during value iteration. To address this issue, we suggest using the dynamic Boltzmann softmax (DBS) operator to update the value function. This operator has good convergence properties during planning and learning. Through our experiments on GridWorld, we discovered that the DBS operator allows for better value function estimation, which corrects the convergence problem of the softmax operator. Lastly, we created the DBS-DQN algorithm by applying dynamic Boltzmann softmax updates in deep Q-network. This algorithm outperforms DQN substantially in 40 out of 49 Atari games.",1
"Large-scale public datasets have been shown to benefit research in multiple areas of modern artificial intelligence. For decision-making research that requires human data, high-quality datasets serve as important benchmarks to facilitate the development of new methods by providing a common reproducible standard. Many human decision-making tasks require visual attention to obtain high levels of performance. Therefore, measuring eye movements can provide a rich source of information about the strategies that humans use to solve decision-making tasks. Here, we provide a large-scale, high-quality dataset of human actions with simultaneously recorded eye movements while humans play Atari video games. The dataset consists of 117 hours of gameplay data from a diverse set of 20 games, with 8 million action demonstrations and 328 million gaze samples. We introduce a novel form of gameplay, in which the human plays in a semi-frame-by-frame manner. This leads to near-optimal game decisions and game scores that are comparable or better than known human records. We demonstrate the usefulness of the dataset through two simple applications: predicting human gaze and imitating human demonstrated actions. The quality of the data leads to promising results in both tasks. Moreover, using a learned human gaze model to inform imitation learning leads to an 115\% increase in game performance. We interpret these results as highlighting the importance of incorporating human visual attention in models of decision making and demonstrating the value of the current dataset to the research community. We hope that the scale and quality of this dataset can provide more opportunities to researchers in the areas of visual attention, imitation learning, and reinforcement learning.",0
"Large-scale public datasets have proven to be beneficial for various areas of modern artificial intelligence research. When it comes to decision-making research that requires human data, high-quality datasets are a vital benchmark that can facilitate the development of new methods by establishing a common reproducible standard. Given that many human decision-making tasks require visual attention to achieve high levels of performance, measuring eye movements can provide a wealth of information about how humans approach decision-making tasks. This article presents a large-scale, high-quality dataset of human actions and eye movements recorded simultaneously while playing Atari video games. The dataset includes 117 hours of gameplay data from 20 different games, with 8 million action demonstrations and 328 million gaze samples. The article introduces a new form of gameplay that involves playing in a semi-frame-by-frame manner, which leads to near-optimal game decisions and scores that are comparable to or better than human records. The usefulness of the dataset is demonstrated through two simple applications: predicting human gaze and imitating human demonstrated actions. The high-quality data produces promising results in both tasks. Additionally, using a learned human gaze model to inform imitation learning leads to a remarkable 115% increase in game performance. These results highlight the importance of incorporating human visual attention in decision-making models and show the value of the current dataset to the research community. The authors hope that the scale and quality of this dataset will provide more opportunities to researchers in the fields of visual attention, imitation learning, and reinforcement learning.",1
"Maximum entropy deep reinforcement learning (RL) methods have been demonstrated on a range of challenging continuous tasks. However, existing methods either suffer from severe instability when training on large off-policy data or cannot scale to tasks with very high state and action dimensionality such as 3D humanoid locomotion. Besides, the optimality of desired Boltzmann policy set for non-optimal soft value function is not persuasive enough. In this paper, we first derive soft policy gradient based on entropy regularized expected reward objective for RL with continuous actions. Then, we present an off-policy actor-critic, model-free maximum entropy deep RL algorithm called deep soft policy gradient (DSPG) by combining soft policy gradient with soft Bellman equation. To ensure stable learning while eliminating the need of two separate critics for soft value functions, we leverage double sampling approach to making the soft Bellman equation tractable. The experimental results demonstrate that our method outperforms in performance over off-policy prior methods.",0
"Several maximum entropy deep reinforcement learning (RL) techniques have been tried out on various difficult continuous tasks. However, these current methods possess flaws such as severe instability when training on large off-policy data or inability to handle tasks with high state and action dimensionality, such as 3D humanoid locomotion. Additionally, the optimality of the desired Boltzmann policy set for non-optimal soft value function lacks persuasive evidence. This paper introduces a soft policy gradient based on the entropy regularized expected reward objective for RL with continuous actions. We then present a model-free maximum entropy deep RL algorithm called deep soft policy gradient (DSPG), which combines the soft policy gradient with the soft Bellman equation and makes use of a double sampling approach to ensure stable learning without the need for two separate critics for soft value functions. Our experimental results confirm that our method outperforms other off-policy prior methods in performance.",1
"Deep generative models have been praised for their ability to learn smooth latent representation of images, text, and audio, which can then be used to generate new, plausible data. However, current generative models are unable to work with molecular graphs due to their unique characteristics-their underlying structure is not Euclidean or grid-like, they remain isomorphic under permutation of the nodes labels, and they come with a different number of nodes and edges. In this paper, we first propose a novel variational autoencoder for molecular graphs, whose encoder and decoder are specially designed to account for the above properties by means of several technical innovations. Moreover, in contrast with the state of the art, our decoder is able to provide the spatial coordinates of the atoms of the molecules it generates. Then, we develop a gradient-based algorithm to optimize the decoder of our model so that it learns to generate molecules that maximize the value of certain property of interest and, given a molecule of interest, it is able to optimize the spatial configuration of its atoms for greater stability. Experiments reveal that our variational autoencoder can discover plausible, diverse and novel molecules more effectively than several state of the art models. Moreover, for several properties of interest, our optimized decoder is able to identify molecules with property values 121% higher than those identified by several state of the art methods based on Bayesian optimization and reinforcement learning",0
"The ability of deep generative models to learn smooth latent representations of images, text, and audio has been widely praised for their ability to generate plausible new data. However, current generative models are not capable of handling molecular graphs due to their unique characteristics, such as their non-Euclidean structure, isomorphism under permutation of node labels, and varying numbers of nodes and edges. This paper introduces a new variational autoencoder specifically designed to address these challenges, with a decoder that provides the spatial coordinates of generated molecules' atoms. Furthermore, the paper presents a gradient-based algorithm to optimize the decoder to generate molecules with desired properties and optimize the spatial configuration of atoms in a given molecule. Experimental results show that the proposed model outperforms several state-of-the-art models in discovering novel, diverse, and plausible molecules. Additionally, the optimized decoder can identify molecules with property values 121% higher than those identified by Bayesian optimization and reinforcement learning-based methods.",1
"Full-sampling (e.g., Q-learning) and pure-expectation (e.g., Expected Sarsa) algorithms are efficient and frequently used techniques in reinforcement learning. Q$(\sigma,\lambda)$ is the first approach unifies them with eligibility trace through the sampling degree $\sigma$. However, it is limited to the tabular case, for large-scale learning, the Q$(\sigma,\lambda)$ is too expensive to require a huge volume of tables to accurately storage value functions. To address above problem, we propose a GQ$(\sigma,\lambda)$ that extends tabular Q$(\sigma,\lambda)$ with linear function approximation. We prove the convergence of GQ$(\sigma,\lambda)$. Empirical results on some standard domains show that GQ$(\sigma,\lambda)$ with a combination of full-sampling with pure-expectation reach a better performance than full-sampling and pure-expectation methods.",0
"Reinforcement learning commonly employs Full-sampling (e.g., Q-learning) and pure-expectation (e.g., Expected Sarsa) algorithms, which are effective techniques. Q$(\sigma,\lambda)$ unites these methods using eligibility trace through the sampling degree $\sigma$ but is restricted to the tabular case. For large-scale learning, Q$(\sigma,\lambda)$ becomes too expensive as it needs numerous tables to store value functions accurately. To solve this issue, we propose GQ$(\sigma,\lambda)$ that extends tabular Q$(\sigma,\lambda)$ with linear function approximation. We prove the convergence of GQ$(\sigma,\lambda)$, and empirical results on standard domains show that GQ$(\sigma,\lambda)$ with a combination of full-sampling with pure-expectation outperforms full-sampling and pure-expectation methods.",1
"Off-policy learning is powerful for reinforcement learning. However, the high variance of off-policy evaluation is a critical challenge, which causes off-policy learning falls into an uncontrolled instability. In this paper, for reducing the variance, we introduce control variate technique to $\mathtt{Expected}$ $\mathtt{Sarsa}$($\lambda$) and propose a tabular $\mathtt{ES}$($\lambda$)-$\mathtt{CV}$ algorithm. We prove that if a proper estimator of value function reaches, the proposed $\mathtt{ES}$($\lambda$)-$\mathtt{CV}$ enjoys a lower variance than $\mathtt{Expected}$ $\mathtt{Sarsa}$($\lambda$). Furthermore, to extend $\mathtt{ES}$($\lambda$)-$\mathtt{CV}$ to be a convergent algorithm with linear function approximation, we propose the $\mathtt{GES}$($\lambda$) algorithm under the convex-concave saddle-point formulation. We prove that the convergence rate of $\mathtt{GES}$($\lambda$) achieves $\mathcal{O}(1/T)$, which matches or outperforms lots of state-of-art gradient-based algorithms, but we use a more relaxed condition. Numerical experiments show that the proposed algorithm performs better with lower variance than several state-of-art gradient-based TD learning algorithms: $\mathtt{GQ}$($\lambda$), $\mathtt{GTB}$($\lambda$) and $\mathtt{ABQ}$($\zeta$).",0
"Reinforcement learning can benefit greatly from off-policy learning, but the high variance of off-policy evaluation poses a significant challenge, leading to instability. To address this issue, we introduce the control variate technique to $\mathtt{Expected}$ $\mathtt{Sarsa}$($\lambda$) in this study and present a tabular $\mathtt{ES}$($\lambda$)-$\mathtt{CV}$ algorithm that reduces variance. We demonstrate that our proposed $\mathtt{ES}$($\lambda$)-$\mathtt{CV}$ has lower variance than $\mathtt{Expected}$ $\mathtt{Sarsa}$($\lambda$) if a suitable value function estimator is used. Additionally, we propose the $\mathtt{GES}$($\lambda$) algorithm under the convex-concave saddle-point formulation to extend $\mathtt{ES}$($\lambda$)-$\mathtt{CV}$ to a convergent algorithm with linear function approximation. Our proofs indicate that $\mathtt{GES}$($\lambda$) achieves a convergence rate of $\mathcal{O}(1/T)$, which matches or outperforms many state-of-the-art gradient-based algorithms, albeit with a more relaxed condition. Our numerical experiments demonstrate that the proposed algorithm outperforms several state-of-the-art gradient-based TD learning algorithms, including $\mathtt{GQ}$($\lambda$), $\mathtt{GTB}$($\lambda$), and $\mathtt{ABQ}$($\zeta$), with lower variance.",1
"This work focuses on a specific classification problem, where the information about a sample is not readily available, but has to be acquired for a cost, and there is a per-sample budget. Inspired by real-world use-cases, we analyze average and hard variations of a directly specified budget. We postulate the problem in its explicit formulation and then convert it into an equivalent MDP, that can be solved with deep reinforcement learning. Also, we evaluate a real-world inspired setting with sparse training dataset with missing features. The presented method performs robustly well in all settings across several distinct datasets, outperforming other prior-art algorithms. The method is flexible, as showcased with all mentioned modifications and can be improved with any domain independent advancement in RL.",0
"This study is centered on a particular classification problem that requires obtaining information on a sample at a cost. The budget for each sample is limited, and the information is not readily available. Real-world scenarios have inspired the analysis of both average and difficult variations of a predefined budget. The problem is explicitly formulated and then transformed into an equivalent MDP, which can be solved using deep reinforcement learning. Additionally, the method is evaluated in a real-world setting with a sparse training dataset that has missing features. The results demonstrate that the proposed method performs well across various datasets and outperforms other prior-art algorithms. The method is adaptable, as evidenced by the modifications mentioned, and can be enhanced with any domain-independent advancement in RL.",1
"Not all neural network architectures are created equal, some perform much better than others for certain tasks. But how important are the weight parameters of a neural network compared to its architecture? In this work, we question to what extent neural network architectures alone, without learning any weight parameters, can encode solutions for a given task. We propose a search method for neural network architectures that can already perform a task without any explicit weight training. To evaluate these networks, we populate the connections with a single shared weight parameter sampled from a uniform random distribution, and measure the expected performance. We demonstrate that our method can find minimal neural network architectures that can perform several reinforcement learning tasks without weight training. On a supervised learning domain, we find network architectures that achieve much higher than chance accuracy on MNIST using random weights. Interactive version of this paper at https://weightagnostic.github.io/",0
"The effectiveness of neural network architectures can vary greatly depending on the task at hand. This leads to the question of how much weight parameters contribute to a neural network's performance compared to its architecture. Our study explores whether neural network architectures, without any weight training, can still provide solutions for a given task. We have developed a search method for identifying such architectures and evaluated them by assigning a single shared weight parameter sampled from a uniform random distribution. Our findings indicate that we can identify minimal neural network architectures that can perform various reinforcement learning tasks without weight training. Additionally, we have discovered network architectures that achieve higher than chance accuracy on MNIST using random weights in a supervised learning domain. For further details, please refer to the interactive version of our paper at https://weightagnostic.github.io/.",1
"While many recent advances in deep reinforcement learning (RL) rely on model-free methods, model-based approaches remain an alluring prospect for their potential to exploit unsupervised data to learn environment model. In this work, we provide an extensive study on the design of deep generative models for RL environments and propose a sample efficient and robust method to learn the model of Atari environments. We deploy this model and propose generative adversarial tree search (GATS) a deep RL algorithm that learns the environment model and implements Monte Carlo tree search (MCTS) on the learned model for planning. While MCTS on the learned model is computationally expensive, similar to AlphaGo, GATS follows depth limited MCTS. GATS employs deep Q network (DQN) and learns a Q-function to assign values to the leaves of the tree in MCTS. We theoretical analyze GATS vis-a-vis the bias-variance trade-off and show GATS is able to mitigate the worst-case error in the Q-estimate. While we were expecting GATS to enjoy a better sample complexity and faster converges to better policies, surprisingly, GATS fails to outperform DQN. We provide a study on which we show why depth limited MCTS fails to perform desirably.",0
"Despite the prevalence of model-free methods in recent advances in deep reinforcement learning (RL), model-based approaches remain an attractive option due to their potential to utilize unsupervised data in learning environment models. This study examines the design of deep generative models for RL environments and proposes a sample-efficient and robust method for learning the model of Atari environments. The proposed algorithm, generative adversarial tree search (GATS), is a deep RL algorithm that implements Monte Carlo tree search (MCTS) on the learned model for planning. While MCTS on the learned model is computationally expensive, GATS follows depth-limited MCTS, similar to AlphaGo. GATS employs a deep Q network (DQN) to learn a Q-function that assigns values to the leaves of the tree in MCTS. We analyze GATS in terms of the bias-variance trade-off and demonstrate its ability to mitigate the worst-case error in the Q-estimate. However, despite expectations of better sample complexity and faster convergence to better policies, GATS surprisingly fails to outperform DQN. We provide a study that explains why depth-limited MCTS fails to perform as desired.",1
"Many real-world systems problems require reasoning about the long term consequences of actions taken to configure and manage the system. These problems with delayed and often sequentially aggregated reward, are often inherently reinforcement learning problems and present the opportunity to leverage the recent substantial advances in deep reinforcement learning. However, in some cases, it is not clear why deep reinforcement learning is a good fit for the problem. Sometimes, it does not perform better than the state-of-the-art solutions. And in other cases, random search or greedy algorithms could outperform deep reinforcement learning. In this paper, we review, discuss, and evaluate the recent trends of using deep reinforcement learning in system optimization. We propose a set of essential metrics to guide future works in evaluating the efficacy of using deep reinforcement learning in system optimization. Our evaluation includes challenges, the types of problems, their formulation in the deep reinforcement learning setting, embedding, the model used, efficiency, and robustness. We conclude with a discussion on open challenges and potential directions for pushing further the integration of reinforcement learning in system optimization.",0
"Many problems in real-world systems require analyzing the long-term effects of configuring and managing the system, which can involve delayed and sequentially aggregated rewards. These types of problems are often ideal candidates for reinforcement learning and can benefit from recent advancements in deep reinforcement learning. However, there are instances where it's unclear if deep reinforcement learning is appropriate, and other cases where it doesn't perform better than existing solutions or could even be outperformed by random search or greedy algorithms. This paper examines recent trends in using deep reinforcement learning for system optimization, discussing and evaluating various metrics and challenges related to its implementation. The evaluation includes factors such as problem formulation, model selection, efficiency, and robustness. The paper concludes with a discussion of open challenges and potential directions for further integration of reinforcement learning in system optimization.",1
"In this paper we present Horizon, Facebook's open source applied reinforcement learning (RL) platform. Horizon is an end-to-end platform designed to solve industry applied RL problems where datasets are large (millions to billions of observations), the feedback loop is slow (vs. a simulator), and experiments must be done with care because they don't run in a simulator. Unlike other RL platforms, which are often designed for fast prototyping and experimentation, Horizon is designed with production use cases as top of mind. The platform contains workflows to train popular deep RL algorithms and includes data preprocessing, feature transformation, distributed training, counterfactual policy evaluation, optimized serving, and a model-based data understanding tool. We also showcase and describe real examples where reinforcement learning models trained with Horizon significantly outperformed and replaced supervised learning systems at Facebook.",0
"This paper introduces Horizon, an open source applied reinforcement learning (RL) platform developed by Facebook. It is specifically designed to address industry RL problems that involve large datasets, slow feedback loops, and careful experimentation due to the absence of a simulator. Unlike other RL platforms that prioritize rapid prototyping, Horizon prioritizes production use cases. The platform includes workflows for training deep RL algorithms, as well as features for data preprocessing, feature transformation, distributed training, counterfactual policy evaluation, optimized serving, and a model-based data understanding tool. The paper also includes examples of real-world applications where Horizon outperformed supervised learning systems at Facebook.",1
"The design of neural network architectures is frequently either based on human expertise using trial/error and empirical feedback or tackled via large scale reinforcement learning strategies performed over distinct discrete architecture choices. In the latter case, the optimization is often non-differentiable and also not very amenable to derivative-free optimization methods. Most methods in use today require sizable computational resources. And if we want networks that additionally satisfy resource constraints, the above challenges are exacerbated because the search must now balance accuracy with certain budget constraints on resources. We formulate this problem as the optimization of a set function -- we find that the empirical behavior of this set function often (but not always) satisfies marginal gain and monotonicity principles -- properties central to the idea of submodularity. Based on this observation, we adapt algorithms within discrete optimization to obtain heuristic schemes for neural network architecture search, where we have resource constraints on the architecture. This simple scheme when applied on CIFAR-100 and ImageNet, identifies resource-constrained architectures with quantifiably better performance than current state-of-the-art models designed for mobile devices. Specifically, we find high-performing architectures with fewer parameters and computations by a search method that is much faster.",0
"Neural network architecture design typically relies on human expertise and trial and error, or large-scale reinforcement learning strategies that optimize over distinct discrete architecture choices. However, the optimization process can be non-differentiable and computationally intensive, making it difficult to satisfy resource constraints while balancing accuracy. To address this problem, we formulate it as a set function optimization problem and observe that it often satisfies marginal gain and monotonicity principles of submodularity. By adapting algorithms from discrete optimization, we develop heuristic schemes for neural network architecture search with resource constraints. This approach identifies high-performing architectures with fewer parameters and computations, outperforming state-of-the-art models designed for mobile devices.",1
"Deep learning algorithms often require solving a highly non-linear and nonconvex unconstrained optimization problem. Methods for solving optimization problems in large-scale machine learning, such as deep learning and deep reinforcement learning (RL), are generally restricted to the class of first-order algorithms, like stochastic gradient descent (SGD). While SGD iterates are inexpensive to compute, they have slow theoretical convergence rates. Furthermore, they require exhaustive trial-and-error to fine-tune many learning parameters. Using second-order curvature information to find search directions can help with more robust convergence for non-convex optimization problems. However, computing Hessian matrices for large-scale problems is not computationally practical. Alternatively, quasi-Newton methods construct an approximate of the Hessian matrix to build a quadratic model of the objective function. Quasi-Newton methods, like SGD, require only first-order gradient information, but they can result in superlinear convergence, which makes them attractive alternatives to SGD. The limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) approach is one of the most popular quasi-Newton methods that construct positive definite Hessian approximations. In this chapter, we propose efficient optimization methods based on L-BFGS quasi-Newton methods using line search and trust-region strategies. Our methods bridge the disparity between first- and second-order methods by using gradient information to calculate low-rank updates to Hessian approximations. We provide formal convergence analysis of these methods as well as empirical results on deep learning applications, such as image classification tasks and deep reinforcement learning on a set of ATARI 2600 video games. Our results show a robust convergence with preferred generalization characteristics as well as fast training time.",0
"The optimization problem for deep learning algorithms is often highly non-linear and nonconvex unconstrained, which presents a challenge for large-scale machine learning. Typically, first-order algorithms such as stochastic gradient descent (SGD) are used, as they are inexpensive to compute, but they have slow theoretical convergence rates and require extensive trial-and-error to fine-tune learning parameters. Using second-order curvature information can help with more robust convergence, but computing Hessian matrices for large-scale problems is not practical. Quasi-Newton methods, like SGD, construct an approximate Hessian matrix to build a quadratic model of the objective function, resulting in superlinear convergence rates. The limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) approach is one of the most popular quasi-Newton methods that construct positive definite Hessian approximations. In this chapter, we propose efficient optimization methods based on L-BFGS quasi-Newton methods using line search and trust-region strategies, which bridge the gap between first- and second-order methods by using gradient information to calculate low-rank updates to Hessian approximations. We provide formal convergence analysis of these methods and empirical results on deep learning applications, such as image classification tasks and deep reinforcement learning on a set of ATARI 2600 video games, which show robust convergence with preferred generalization characteristics and fast training time.",1
"One of the major challenges in training deep architectures for predictive tasks is the scarcity and cost of labeled training data. Active Learning (AL) is one way of addressing this challenge. In stream-based AL, observations are continuously made available to the learner that have to decide whether to request a label or to make a prediction. The goal is to reduce the request rate while at the same time maximize prediction performance. In previous research, reinforcement learning has been used for learning the AL request/prediction strategy. In our work, we propose to equip a reinforcement learning process with memory augmented neural networks, to enhance the one-shot capabilities. Moreover, we introduce Class Margin Sampling (CMS) as an extension of the standard margin sampling to the reinforcement learning setting. This strategy aims to reduce training time and improve sample efficiency in the training process. We evaluate the proposed method on a classification task using empirical accuracy of label predictions and percentage of label requests. The results indicates that the proposed method, by making use of the memory augmented networks and CMS in the training process, outperforms existing baselines.",0
"The shortage and expense of labeled training data presents a significant challenge when training deep architectures for predictive tasks. One potential solution is Active Learning (AL), which involves the continuous presentation of observations to the learner to determine whether a label request or prediction is necessary. The objective is to minimize the frequency of requests while maximizing prediction accuracy. Previous research has applied reinforcement learning to develop the AL request/prediction strategy. Our work proposes enhancing this process with memory augmented neural networks to improve one-shot capabilities. Additionally, we introduce Class Margin Sampling (CMS) as an extension of standard margin sampling to reinforcement learning, aiming to enhance training efficiency and effectiveness. We evaluate this method by assessing the empirical accuracy of label predictions and percentage of label requests in a classification task. Our results demonstrate that this approach, which incorporates memory augmented networks and CMS in the training process, outperforms current baselines.",1
"Automatic neural architecture design has shown its potential in discovering powerful neural network architectures. Existing methods, no matter based on reinforcement learning or evolutionary algorithms (EA), conduct architecture search in a discrete space, which is highly inefficient. In this paper, we propose a simple and efficient method to automatic neural architecture design based on continuous optimization. We call this new approach neural architecture optimization (NAO). There are three key components in our proposed approach: (1) An encoder embeds/maps neural network architectures into a continuous space. (2) A predictor takes the continuous representation of a network as input and predicts its accuracy. (3) A decoder maps a continuous representation of a network back to its architecture. The performance predictor and the encoder enable us to perform gradient based optimization in the continuous space to find the embedding of a new architecture with potentially better accuracy. Such a better embedding is then decoded to a network by the decoder. Experiments show that the architecture discovered by our method is very competitive for image classification task on CIFAR-10 and language modeling task on PTB, outperforming or on par with the best results of previous architecture search methods with a significantly reduction of computational resources. Specifically we obtain 1.93% test set error rate for CIFAR-10 image classification task and 56.0 test set perplexity of PTB language modeling task. Furthermore, combined with the recent proposed weight sharing mechanism, we discover powerful architecture on CIFAR-10 (with error rate 2.93%) and on PTB (with test set perplexity 56.6), with very limited computational resources (less than 10 GPU hours) for both tasks.",0
"The potential of automatic neural architecture design in discovering powerful neural network architectures has been demonstrated. However, current methods that are based on reinforcement learning or evolutionary algorithms conduct architecture search in a discrete space, which is highly inefficient. This paper introduces a new approach to automatic neural architecture design called neural architecture optimization (NAO), which is based on continuous optimization and is simple and efficient. The proposed approach has three key components: an encoder, a predictor, and a decoder. The encoder maps neural network architectures into a continuous space, while the predictor takes the continuous representation of a network as input and predicts its accuracy. The decoder maps a continuous representation of a network back to its architecture. Gradient-based optimization is performed in the continuous space to find the embedding of a new architecture with potentially better accuracy. Experiments show that our method outperforms previous architecture search methods in terms of accuracy and computational resources on both image classification tasks and language modeling tasks. Specifically, we achieved a test set error rate of 1.93% for CIFAR-10 image classification task and a test set perplexity of 56.0 for PTB language modeling task. When combined with the weight sharing mechanism, we discovered powerful architectures for both tasks using less than 10 GPU hours.",1
"While Reinforcement Learning (RL) approaches lead to significant achievements in a variety of areas in recent history, natural language tasks remained mostly unaffected, due to the compositional and combinatorial nature that makes them notoriously hard to optimize. With the emerging field of Text-Based Games (TBGs), researchers try to bridge this gap. Inspired by the success of RL algorithms on Atari games, the idea is to develop new methods in a restricted game world and then gradually move to more complex environments. Previous work in the area of TBGs has mainly focused on solving individual games. We, however, consider the task of designing an agent that not just succeeds in a single game, but performs well across a whole family of games, sharing the same theme. In this work, we present our deep RL agent--LeDeepChef--that shows generalization capabilities to never-before-seen games of the same family with different environments and task descriptions. The agent participated in Microsoft Research's ""First TextWorld Problems: A Language and Reinforcement Learning Challenge"" and outperformed all but one competitor on the final test set. The games from the challenge all share the same theme, namely cooking in a modern house environment, but differ significantly in the arrangement of the rooms, the presented objects, and the specific goal (recipe to cook). To build an agent that achieves high scores across a whole family of games, we use an actor-critic framework and prune the action-space by using ideas from hierarchical reinforcement learning and a specialized module trained on a recipe database.",0
"Despite Reinforcement Learning (RL) being successful in many areas, natural language tasks have proven difficult to optimize due to their compositional and combinatorial nature. Text-Based Games (TBGs) aim to bridge this gap by developing new methods in a restricted game world and gradually moving to more complex environments. While previous work in TBGs has focused on solving individual games, we aim to design an agent that performs well across a whole family of games with a shared theme. This work presents LeDeepChef, a deep RL agent with generalization capabilities to never-before-seen games of the same family with different environments and task descriptions. LeDeepChef participated in Microsoft Research's ""First TextWorld Problems: A Language and Reinforcement Learning Challenge"" and outperformed all but one competitor on the final test set. To achieve high scores across a family of games, we used an actor-critic framework and pruned the action-space using hierarchical reinforcement learning and a specialized module trained on a recipe database.",1
"Reinforcement learning (RL) algorithms allow artificial agents to improve their selection of actions to increase rewarding experiences in their environments. Temporal Difference (TD) Learning -- a model-free RL method -- is a leading account of the midbrain dopamine system and the basal ganglia in reinforcement learning. These algorithms typically learn a mapping from the agent's current sensed state to a selected action (known as a policy function) via learning a value function (expected future rewards). TD Learning methods have been very successful on a broad range of control tasks, but learning can become intractably slow as the state space of the environment grows. This has motivated methods that learn internal representations of the agent's state, effectively reducing the size of the state space and restructuring state representations in order to support generalization. However, TD Learning coupled with an artificial neural network, as a function approximator, has been shown to fail to learn some fairly simple control tasks, challenging this explanation of reward-based learning. We hypothesize that such failures do not arise in the brain because of the ubiquitous presence of lateral inhibition in the cortex, producing sparse distributed internal representations that support the learning of expected future reward. The sparse conjunctive representations can avoid catastrophic interference while still supporting generalization. We provide support for this conjecture through computational simulations, demonstrating the benefits of learned sparse representations for three problematic classic control tasks: Puddle-world, Mountain-car, and Acrobot.",0
"Artificial agents can use Reinforcement Learning (RL) algorithms to enhance their decision-making abilities and increase rewarding experiences in their surroundings. Temporal Difference (TD) Learning is a model-free RL method that is a prominent model of the midbrain dopamine system and the basal ganglia in reinforcement learning. These algorithms usually learn a policy function, which maps the agent's current sensed state to a chosen action, by learning a value function that estimates future rewards. Although TD Learning techniques have been successful in controlling different tasks, learning can become excessively slow as the state space of the environment expands. To tackle this problem, methods have been developed that learn the agent's internal state representations to decrease the state space size and support generalization. However, using TD Learning with an artificial neural network has shown to fail in some simple control tasks, challenging the reward-based learning explanation. We believe that such failures do not happen in the brain due to the widespread presence of lateral inhibition in the cortex, which produces sparse distributed internal representations that facilitate the learning of expected future reward. These representations can prevent catastrophic interference while still supporting generalization. We demonstrate the benefits of learned sparse representations for three challenging classic control tasks, namely Puddle-world, Mountain-car, and Acrobot, through computational simulations.",1
"Unmanned aerial vehicles (UAVs) are envisioned to complement the 5G communication infrastructure in future smart cities. Hot spots easily appear in road intersections, where effective communication among vehicles is challenging. UAVs may serve as relays with the advantages of low price, easy deployment, line-of-sight links, and flexible mobility. In this paper, we study a UAV-assisted vehicular network where the UAV jointly adjusts its transmission control (power and channel) and 3D flight to maximize the total throughput. First, we formulate a Markov decision process (MDP) problem by modeling the mobility of the UAV/vehicles and the state transitions. Secondly, we solve the target problem using a deep reinforcement learning method under unknown or unmeasurable environment variables especially in 5G, namely, the deep deterministic policy gradient (DDPG), and propose three solutions with different control objectives. Environment variables are unknown and unmeasurable, therefore, we use a deep reinforcement learning method. Moreover, considering the energy consumption of 3D flight, we extend the proposed solutions to maximize the total throughput per energy unit by encouraging or discouraging the UAV's mobility. To achieve this goal, the DDPG framework is modified. Thirdly, in a simplified model with small state space and action space, we verify the optimality of proposed algorithms. Comparing with two baseline schemes, we demonstrate the effectiveness of proposed algorithms in a realistic model.",0
"The integration of unmanned aerial vehicles (UAVs) and 5G communication infrastructure is expected to enhance future smart cities. Road intersections are notorious for being communication dead zones, especially for vehicles. UAVs offer an affordable and easily deployable solution as they have line-of-sight links and flexible mobility. Our research focuses on a UAV-assisted vehicular network that optimizes total throughput by adjusting transmission control and 3D flight. We use a Markov decision process (MDP) approach to model the mobility of UAVs/vehicles and state transitions. As environmental variables in 5G are unknown and unmeasurable, we utilize deep reinforcement learning (DDPG) to propose three solutions with different control objectives. We expand on the proposed solutions to include energy consumption by encouraging or discouraging UAV mobility, resulting in a modified DDPG framework. We verify the optimality of our algorithms in a simplified model and demonstrate their effectiveness compared to two baseline schemes in a realistic model.",1
"In E-commerce advertising, where product recommendations and product ads are presented to users simultaneously, the traditional setting is to display ads at fixed positions. However, under such a setting, the advertising system loses the flexibility to control the number and positions of ads, resulting in sub-optimal platform revenue and user experience. Consequently, major e-commerce platforms (e.g., Taobao.com) have begun to consider more flexible ways to display ads. In this paper, we investigate the problem of advertising with adaptive exposure: can we dynamically determine the number and positions of ads for each user visit under certain business constraints so that the platform revenue can be increased? More specifically, we consider two types of constraints: request-level constraint ensures user experience for each user visit, and platform-level constraint controls the overall platform monetization rate. We model this problem as a Constrained Markov Decision Process with per-state constraint (psCMDP) and propose a constrained two-level reinforcement learning approach to decompose the original problem into two relatively independent sub-problems. To accelerate policy learning, we also devise a constrained hindsight experience replay mechanism. Experimental evaluations on industry-scale real-world datasets demonstrate the merits of our approach in both obtaining higher revenue under the constraints and the effectiveness of the constrained hindsight experience replay mechanism.",0
"In the realm of E-commerce advertising, product recommendations and product ads are typically presented to users simultaneously. However, the traditional approach of displaying ads at fixed positions limits the advertising system's flexibility in controlling the number and placement of ads, resulting in an inadequate platform revenue and user experience. To address this issue, major E-commerce platforms like Taobao.com have started exploring more adaptable ways to showcase ads. This study aims to explore adaptive exposure advertising, which dynamically determines the number and positions of ads for each user visit while adhering to certain business constraints to increase platform revenue. The authors consider two types of constraints: request-level constraints to ensure user experience for each visit and platform-level constraints that control the overall platform monetization rate. The problem is modeled as a Constrained Markov Decision Process with per-state constraints (psCMDP), and a two-level reinforcement learning approach is proposed to decompose the original problem into two relatively independent sub-problems. To speed up policy learning, a constrained hindsight experience replay mechanism is also devised. Experimental evaluations on real-world datasets demonstrate the effectiveness of the proposed approach in obtaining higher revenue under constraints and the usefulness of the constrained hindsight experience replay mechanism.",1
"Reinforcement learning algorithms are gaining popularity in fields in which optimal scheduling is important, and oncology is not an exception. The complex and uncertain dynamics of cancer limit the performance of traditional model-based scheduling strategies like Optimal Control. Motivated by the recent success of model-free Deep Reinforcement Learning (DRL) in challenging control tasks and in the design of medical treatments, we use Deep Q-Network (DQN) and Deep Deterministic Policy Gradient (DDPG) to design a personalized cancer chemotherapy schedule. We show that both of them succeed in the task and outperform the Optimal Control solution in the presence of uncertainty. Furthermore, we show that DDPG can exterminate cancer more efficiently than DQN presumably due to its continuous action space. Finally, we provide some insight regarding the amount of samples required for the training.",0
"Optimal scheduling is crucial in various fields, including oncology, where reinforcement learning algorithms are becoming increasingly popular. Traditional model-based scheduling strategies, such as Optimal Control, face limitations due to the intricate and uncertain dynamics of cancer. To address this, we explore the potential of model-free Deep Reinforcement Learning (DRL) techniques, specifically the Deep Q-Network (DQN) and Deep Deterministic Policy Gradient (DDPG), in designing personalized cancer chemotherapy schedules. Our findings demonstrate that both DRL techniques perform better than Optimal Control in the presence of uncertainty. Additionally, DDPG proves to be more effective than DQN, likely due to its continuous action space. We also provide insights on the required number of training samples.",1
"We introduce the $2$-simplicial Transformer, an extension of the Transformer which includes a form of higher-dimensional attention generalising the dot-product attention, and uses this attention to update entity representations with tensor products of value vectors. We show that this architecture is a useful inductive bias for logical reasoning in the context of deep reinforcement learning.",0
"The $2$-simplicial Transformer is presented as an expansion of the Transformer, incorporating a type of attention that extends beyond the dot-product attention and applies it to modify entity representations using tensor products of value vectors. Through our analysis, we demonstrate that this structure serves as a beneficial inductive bias for deep reinforcement learning in scenarios that require logical reasoning abilities.",1
"Cancer is a complex disease, the understanding and treatment of which are being aided through increases in the volume of collected data and in the scale of deployed computing power. Consequently, there is a growing need for the development of data-driven and, in particular, deep learning methods for various tasks such as cancer diagnosis, detection, prognosis, and prediction. Despite recent successes, however, designing high-performing deep learning models for nonimage and nontext cancer data is a time-consuming, trial-and-error, manual task that requires both cancer domain and deep learning expertise. To that end, we develop a reinforcement-learning-based neural architecture search to automate deep-learning-based predictive model development for a class of representative cancer data. We develop custom building blocks that allow domain experts to incorporate the cancer-data-specific characteristics. We show that our approach discovers deep neural network architectures that have significantly fewer trainable parameters, shorter training time, and accuracy similar to or higher than those of manually designed architectures. We study and demonstrate the scalability of our approach on up to 1,024 Intel Knights Landing nodes of the Theta supercomputer at the Argonne Leadership Computing Facility.",0
"With the aid of increased data collection and larger computing power, cancer is being better understood and treated through the development of data-driven and deep learning methods. However, creating high-performing deep learning models for nonimage and nontext cancer data requires significant time, expertise, and trial-and-error. To automate this process, we have developed a reinforcement-learning-based neural architecture search that incorporates cancer-data-specific characteristics. Our approach has resulted in the discovery of deep neural network architectures that require fewer trainable parameters, shorter training time, and similar or higher accuracy compared to manually designed architectures. We have demonstrated the scalability of our approach on up to 1,024 Intel Knights Landing nodes of the Theta supercomputer at the Argonne Leadership Computing Facility.",1
"Visual paragraph generation aims to automatically describe a given image from different perspectives and organize sentences in a coherent way. In this paper, we address three critical challenges for this task in a reinforcement learning setting: the mode collapse, the delayed feedback, and the time-consuming warm-up for policy networks. Generally, we propose a novel Curiosity-driven Reinforcement Learning (CRL) framework to jointly enhance the diversity and accuracy of the generated paragraphs. First, by modeling the paragraph captioning as a long-term decision-making process and measuring the prediction uncertainty of state transitions as intrinsic rewards, the model is incentivized to memorize precise but rarely spotted descriptions to context, rather than being biased towards frequent fragments and generic patterns. Second, since the extrinsic reward from evaluation is only available until the complete paragraph is generated, we estimate its expected value at each time step with temporal-difference learning, by considering the correlations between successive actions. Then the estimated extrinsic rewards are complemented by dense intrinsic rewards produced from the derived curiosity module, in order to encourage the policy to fully explore action space and find a global optimum. Third, discounted imitation learning is integrated for learning from human demonstrations, without separately performing the time-consuming warm-up in advance. Extensive experiments conducted on the Standford image-paragraph dataset demonstrate the effectiveness and efficiency of the proposed method, improving the performance by 38.4% compared with state-of-the-art.",0
"The objective of visual paragraph generation is to automatically provide a description of an image while ensuring coherence in sentence organization. This paper aims to tackle three major challenges encountered in this task when using reinforcement learning: mode collapse, delayed feedback, and time-consuming warm-up for policy networks. The authors propose a new Curiosity-driven Reinforcement Learning (CRL) framework that simultaneously enhances the diversity and accuracy of generated paragraphs. The framework models the captioning process as a long-term decision-making process and uses intrinsic rewards to incentivize the model to memorize precise but rarely seen descriptions. The framework also uses temporal-difference learning to estimate expected extrinsic rewards at each time step and complement them with dense intrinsic rewards produced from the curiosity module. Finally, discounted imitation learning is integrated for learning from human demonstrations without requiring a separate warm-up period. Extensive experiments conducted on the Standford image-paragraph dataset demonstrate a 38.4% improvement in performance compared to state-of-the-art methods.",1
"In this paper, we settle the sampling complexity of solving discounted two-player turn-based zero-sum stochastic games up to polylogarithmic factors. Given a stochastic game with discount factor $\gamma\in(0,1)$ we provide an algorithm that computes an $\epsilon$-optimal strategy with high-probability given $\tilde{O}((1 - \gamma)^{-3} \epsilon^{-2})$ samples from the transition function for each state-action-pair. Our algorithm runs in time nearly linear in the number of samples and uses space nearly linear in the number of state-action pairs. As stochastic games generalize Markov decision processes (MDPs) our runtime and sample complexities are optimal due to Azar et al (2013). We achieve our results by showing how to generalize a near-optimal Q-learning based algorithms for MDP, in particular Sidford et al (2018), to two-player strategy computation algorithms. This overcomes limitations of standard Q-learning and strategy iteration or alternating minimization based approaches and we hope will pave the way for future reinforcement learning results by facilitating the extension of MDP results to multi-agent settings with little loss.",0
"This article establishes the sampling complexity required to solve discounted two-player turn-based zero-sum stochastic games within polylogarithmic factors. Our proposed algorithm can determine an $\epsilon$-optimal strategy with high probability for a given stochastic game with a discount factor $\gamma\in(0,1)$. The algorithm requires $\tilde{O}((1 - \gamma)^{-3} \epsilon^{-2})$ samples from the transition function for each state-action-pair. The algorithm's runtime is nearly linear with the number of samples, and the space required is almost linear with the number of state-action pairs. Our approach extends the near-optimal Q-learning based algorithms for Markov decision processes (MDPs) introduced by Sidford et al (2018) to two-player strategy computation algorithms. This overcomes the limitations of standard Q-learning and strategy iteration or alternating minimization based approaches and could facilitate future reinforcement learning results by enabling the extension of MDP results to multi-agent settings with little loss. Our runtime and sample complexities are optimal due to Azar et al (2013) since stochastic games generalize MDPs.",1
"This paper proposes a Transformer-based model to generate equations for math word problems. It achieves much better results than RNN models when copy and align mechanisms are not used, and can outperform complex copy and align RNN models. We also show that training a Transformer jointly in a generation task with two decoders, left-to-right and right-to-left, is beneficial. Such a Transformer performs better than the one with just one decoder not only because of the ensemble effect, but also because it improves the encoder training procedure. We also experiment with adding reinforcement learning to our model, showing improved performance compared to MLE training.",0
"The suggestion put forth in this article is to employ a Transformer-based approach to produce mathematical equations for word problems. The outcomes of this method surpass those of RNN models that lack copy and align mechanisms and can even outdo intricate RNN models that include these mechanisms. We also prove that it is advantageous to have two decoders for the Transformer, left-to-right and right-to-left, as it not only enhances the ensemble effect but also betters the encoder training process. Additionally, we conducted experiments using reinforcement learning in our model, which resulted in better performance than MLE training.",1
"Anomaly detection is widely applied in a variety of domains, involving for instance, smart home systems, network traffic monitoring, IoT applications and sensor networks. In this paper, we study deep reinforcement learning based active sequential testing for anomaly detection. We assume that there is an unknown number of abnormal processes at a time and the agent can only check with one sensor in each sampling step. To maximize the confidence level of the decision and minimize the stopping time concurrently, we propose a deep actor-critic reinforcement learning framework that can dynamically select the sensor based on the posterior probabilities. We provide simulation results for both the training phase and testing phase, and compare the proposed framework with the Chernoff test in terms of claim delay and loss.",0
"The detection of anomalies is widely used in various fields, such as smart home systems, network traffic monitoring, IoT applications, and sensor networks. This study explores active sequential testing for anomaly detection using deep reinforcement learning. Our assumption is that there are unknown abnormal processes, and the agent can only use one sensor at a time for each sampling step. To ensure both a high level of decision confidence and minimal stopping time, we introduce a deep actor-critic reinforcement learning framework that dynamically selects the sensor based on posterior probabilities. We conduct simulations for both the training and testing phases and compare our framework with the Chernoff test in terms of claim delay and loss.",1
"We study algorithms for average-cost reinforcement learning problems with value function approximation. Our starting point is the recently proposed POLITEX algorithm, a version of policy iteration where the policy produced in each iteration is near-optimal in hindsight for the sum of all past value function estimates. POLITEX has sublinear regret guarantees in uniformly-mixing MDPs when the value estimation error can be controlled, which can be satisfied if all policies sufficiently explore the environment. Unfortunately, this assumption is often unrealistic. Motivated by the rapid growth of interest in developing policies that learn to explore their environment in the lack of rewards (also known as no-reward learning), we replace the previous assumption that all policies explore the environment with that a single, sufficiently exploring policy is available beforehand. The main contribution of the paper is the modification of POLITEX to incorporate such an exploration policy in a way that allows us to obtain a regret guarantee similar to the previous one but without requiring that all policies explore environment. In addition to the novel theoretical guarantees, we demonstrate the benefits of our scheme on environments which are difficult to explore using simple schemes like dithering. While the solution we obtain may not achieve the best possible regret, it is the first result that shows how to control the regret in the presence of function approximation errors on problems where exploration is nontrivial. Our approach can also be seen as a way of reducing the problem of minimizing the regret to learning a good exploration policy. We believe that modular approaches like ours can be highly beneficial in tackling harder control problems.",0
"Our research focuses on algorithms for average-cost reinforcement learning problems that utilize value function approximation. We begin with the POLITEX algorithm, which is a version of policy iteration that produces policies that are near-optimal in hindsight for the sum of all past value function estimates. POLITEX has sublinear regret guarantees in uniformly-mixing MDPs when the value estimation error is controllable, assuming that all policies sufficiently explore the environment. However, this assumption is often impractical. Therefore, we modify POLITEX to incorporate a single, sufficiently exploring policy instead of assuming that all policies explore the environment. Our main contribution is the development of a regret guarantee similar to the previous one, but without the requirement of environment exploration for all policies. Furthermore, we demonstrate the advantages of our approach by applying it to environments that are challenging to explore using simple techniques such as dithering. Although our solution may not achieve the best possible regret, it is the first result that addresses the control of regret in the presence of function approximation errors in nontrivial exploration problems. Our approach is also a way of minimizing regret by learning a good exploration policy, and we believe that modular techniques like ours can be useful for addressing challenging control problems.",1
"Using touch devices to navigate in virtual 3D environments such as computer assisted design (CAD) models or geographical information systems (GIS) is inherently difficult for humans, as the 3D operations have to be performed by the user on a 2D touch surface. This ill-posed problem is classically solved with a fixed and handcrafted interaction protocol, which must be learned by the user. We propose to automatically learn a new interaction protocol allowing to map a 2D user input to 3D actions in virtual environments using reinforcement learning (RL). A fundamental problem of RL methods is the vast amount of interactions often required, which are difficult to come by when humans are involved. To overcome this limitation, we make use of two collaborative agents. The first agent models the human by learning to perform the 2D finger trajectories. The second agent acts as the interaction protocol, interpreting and translating to 3D operations the 2D finger trajectories from the first agent. We restrict the learned 2D trajectories to be similar to a training set of collected human gestures by first performing state representation learning, prior to reinforcement learning. This state representation learning is addressed by projecting the gestures into a latent space learned by a variational auto encoder (VAE).",0
"It is challenging for humans to navigate virtual 3D environments, such as CAD models or GIS, using touch devices since the 3D operations must be conducted on a 2D touch surface. Traditionally, a fixed interaction protocol, which users must learn, is used to solve this ill-posed problem. Our approach proposes to use reinforcement learning (RL) to automatically learn a new interaction protocol that maps 2D user input to 3D actions in virtual environments. However, RL methods often require extensive human interactions which are challenging to obtain. To overcome this limitation, we use two collaborative agents. The first agent models the human by learning 2D finger trajectories, while the second agent acts as the interaction protocol, interpreting and translating the 2D finger trajectories to 3D operations. We limit the learned 2D trajectories by using state representation learning, which involves projecting gestures into a latent space learned by a VAE and ensuring they are similar to a training set of collected human gestures.",1
"Imitation learning is an effective alternative approach to learn a policy when the reward function is sparse. In this paper, we consider a challenging setting where an agent and an expert use different actions from each other. We assume that the agent has access to a sparse reward function and state-only expert observations. We propose a method which gradually balances between the imitation learning cost and the reinforcement learning objective. In addition, this method adapts the agent's policy based on either mimicking expert behavior or maximizing sparse reward. We show, through navigation scenarios, that (i) an agent is able to efficiently leverage sparse rewards to outperform standard state-only imitation learning, (ii) it can learn a policy even when its actions are different from the expert, and (iii) the performance of the agent is not bounded by that of the expert, due to the optimized usage of sparse rewards.",0
"When the reward function is sparse, imitation learning can be an effective alternative for learning a policy. This study focuses on a challenging scenario where an agent and expert use different actions. The agent has access to a sparse reward function and state-only expert observations. A method is proposed that balances between the imitation learning cost and the reinforcement learning objective gradually. The agent's policy is adapted based on mimicking expert behavior or maximizing sparse reward. Results from navigation scenarios demonstrate that this method enables the agent to efficiently leverage sparse rewards, learn a policy despite differing actions from the expert, and outperform standard state-only imitation learning. Furthermore, the agent's performance is not limited by that of the expert, thanks to the optimized usage of sparse rewards.",1
"We introduce Threatened Markov Decision Processes (TMDPs) as an extension of the classical Markov Decision Process framework for Reinforcement Learning (RL). TMDPs allow suporting a decision maker against potential opponents in a RL context. We also propose a level-k thinking scheme resulting in a novel learning approach to deal with TMDPs. After introducing our framework and deriving theoretical results, relevant empirical evidence is given via extensive experiments, showing the benefits of accounting for adversaries in RL while the agent learns",0
"The extension of the classical Markov Decision Process framework for Reinforcement Learning (RL) is introduced as Threatened Markov Decision Processes (TMDPs). It enables support for a decision maker against potential opponents in an RL context. Additionally, a novel learning approach to deal with TMDPs is proposed using a level-k thinking scheme. The introduction of the framework and derivation of theoretical results are followed by extensive experiments, which offer relevant empirical evidence demonstrating the advantages of incorporating adversaries in RL during the agent's learning process.",1
"Urban spatial-temporal flows prediction is of great importance to traffic management, land use, public safety, etc. Urban flows are affected by several complex and dynamic factors, such as patterns of human activities, weather, events and holidays. Datasets evaluated the flows come from various sources in different domains, e.g. mobile phone data, taxi trajectories data, metro/bus swiping data, bike-sharing data and so on. To summarize these methodologies of urban flows prediction, in this paper, we first introduce four main factors affecting urban flows. Second, in order to further analysis urban flows, a preparation process of multi-sources spatial-temporal data related with urban flows is partitioned into three groups. Third, we choose the spatial-temporal dynamic data as a case study for the urban flows prediction task. Fourth, we analyze and compare some well-known and state-of-the-art flows prediction methods in detail, classifying them into five categories: statistics-based, traditional machine learning-based, deep learning-based, reinforcement learning-based and transfer learning-based methods. Finally, we give open challenges of urban flows prediction and an outlook in the future of this field. This paper will facilitate researchers find suitable methods and open datasets for addressing urban spatial-temporal flows forecast problems.",0
"Predicting urban spatial-temporal flows is crucial for managing traffic, land use, and public safety. These flows are influenced by various complex and dynamic factors such as human activity patterns, weather, events, and holidays. Different domains provide datasets to assess these flows, including mobile phone data, taxi trajectories data, metro/bus swiping data, bike-sharing data, and more. This paper summarizes methods for predicting urban flows by first identifying the four main factors affecting them. Secondly, it partitions the preparation process of multi-sources spatial-temporal data related to urban flows into three groups to further analyze urban flows. Thirdly, the paper selects spatial-temporal dynamic data as a case study for predicting urban flows. Fourthly, it analyzes and compares various well-known and state-of-the-art flows prediction methods by classifying them into five categories: statistics-based, traditional machine learning-based, deep learning-based, reinforcement learning-based, and transfer learning-based methods. Finally, the paper outlines open challenges for predicting urban flows and provides an outlook on the future of this field, aiding researchers in finding appropriate methods and datasets for addressing urban spatial-temporal flows forecast problems.",1
"In this paper, we propose a generic framework for devising an adaptive approximation scheme for value function approximation in reinforcement learning, which introduces multiscale approximation. The two basic ingredients are multiresolution analysis as well as tree approximation. Starting from simple refinable functions, multiresolution analysis enables us to construct a wavelet system from which the basis functions are selected adaptively, resulting in a tree structure. Furthermore, we present the convergence rate of our multiscale approximation which does not depend on the regularity of basis functions.",0
"Our paper suggests a versatile approach to creating an adaptable approximation method for value function approximation in reinforcement learning, utilizing multiscale approximation. The framework incorporates two fundamental components: multiresolution analysis and tree approximation. By utilizing basic refinable functions, the multiresolution analysis allows for the creation of a wavelet system that can select basis functions adaptively, ultimately producing a tree structure. Additionally, we demonstrate the convergence rate of our multiscale approximation, which is independent of the regularity of the basis functions.",1
"Practical application of Reinforcement Learning (RL) often involves risk considerations. We study a generalized approximation scheme for risk measures, based on Monte-Carlo simulations, where the risk measures need not necessarily be \emph{coherent}. We demonstrate that, even in simple problems, measures such as the variance of the reward-to-go do not capture the risk in a satisfactory manner. In addition, we show how a risk measure can be derived from model's realizations. We propose a neural architecture for estimating the risk and suggest the risk critic architecture that can be use to optimize a policy under general risk measures. We conclude our work with experiments that demonstrate the efficacy of our approach.",0
"The practical implementation of Reinforcement Learning (RL) often involves taking risks into account. We have researched a general method of approximating risk measures using Monte-Carlo simulations. This approach does not require the risk measures to be coherent. Our findings indicate that even simple problems cannot be satisfactorily assessed using measures such as the variance of the reward-to-go. Furthermore, we demonstrate how a risk measure can be derived from the model's realizations. We have proposed a neural architecture for estimating risk and suggest using the risk critic architecture to optimize policies according to general risk measures. Our work concludes with experiments that demonstrate the effectiveness of our approach.",1
"The performance of many algorithms in the fields of hard combinatorial problem solving, machine learning or AI in general depends on tuned hyperparameter configurations. Automated methods have been proposed to alleviate users from the tedious and error-prone task of manually searching for performance-optimized configurations across a set of problem instances. However there is still a lot of untapped potential through adjusting an algorithm's hyperparameters online since different hyperparameters are potentially optimal at different stages of the algorithm. We formulate the problem of adjusting an algorithm's hyperparameters for a given instance on the fly as a contextual MDP, making reinforcement learning (RL) the prime candidate to solve the resulting algorithm control problem in a data-driven way. Furthermore, inspired by applications of algorithm configuration, we introduce new white-box benchmarks suitable to study algorithm control. We show that on short sequences, algorithm configuration is a valid choice, but that with increasing sequence length a black-box view on the problem quickly becomes infeasible and RL performs better.",0
"The success of various algorithms in hard combinatorial problem solving, machine learning, and AI, largely depends on the adjusted hyperparameters. Automated techniques have been suggested to relieve users from the tiresome and error-prone task of manually searching for the best configurations for performance optimization across a range of problem instances. However, there is still much potential to be explored by modifying an algorithm's hyperparameters online since different hyperparameters may be optimal at different stages of the algorithm. We address the issue of adjusting an algorithm's hyperparameters for a given instance on the go by framing it as a contextual MDP, which can be tackled by reinforcement learning (RL) to solve the resulting algorithm control problem in a data-driven manner. Additionally, we introduce new white-box benchmarks, ideal for studying algorithm control, inspired by algorithm configuration applications. Our findings show that for short sequences, algorithm configuration is a suitable approach, but with increasing sequence length, a black-box view of the problem becomes impractical, and RL outperforms.",1
"Efficiently scheduling data processing jobs on distributed compute clusters requires complex algorithms. Current systems, however, use simple generalized heuristics and ignore workload characteristics, since developing and tuning a scheduling policy for each workload is infeasible. In this paper, we show that modern machine learning techniques can generate highly-efficient policies automatically. Decima uses reinforcement learning (RL) and neural networks to learn workload-specific scheduling algorithms without any human instruction beyond a high-level objective such as minimizing average job completion time. Off-the-shelf RL techniques, however, cannot handle the complexity and scale of the scheduling problem. To build Decima, we had to develop new representations for jobs' dependency graphs, design scalable RL models, and invent RL training methods for dealing with continuous stochastic job arrivals. Our prototype integration with Spark on a 25-node cluster shows that Decima improves the average job completion time over hand-tuned scheduling heuristics by at least 21%, achieving up to 2x improvement during periods of high cluster load.",0
"The process of effectively scheduling data processing tasks on distributed compute clusters requires intricate algorithms. However, current systems utilize unsophisticated generalized heuristics and do not consider the workload's characteristics, as creating and refining a scheduling policy for each workload is impractical. The paper demonstrates that contemporary machine learning methods can automatically generate highly effective policies. Decima employs reinforcement learning (RL) and neural networks to learn scheduling algorithms tailored to the workload without any human instruction other than a high-level goal, such as reducing the average job completion time. Nonetheless, off-the-shelf RL methods cannot cope with the complexity and magnitude of the scheduling problem. To create Decima, novel representations for the jobs' dependency graphs, scalable RL models, and RL training methods were developed to handle continuous stochastic job arrivals. Our research demonstrates that Decima improves the average job completion time over manually tuned scheduling heuristics by at least 21%, with up to a 2x improvement during periods of high cluster load, as shown in our prototype integration with Spark on a 25-node cluster.",1
"A Markov Decision Process (MDP) is a popular model for reinforcement learning. However, its commonly used assumption of stationary dynamics and rewards is too stringent and fails to hold in adversarial, nonstationary, or multi-agent problems. We study an episodic setting where the parameters of an MDP can differ across episodes. We learn a reliable policy of this potentially adversarial MDP by developing an Adversarial Reinforcement Learning (ARL) algorithm that reduces our MDP to a sequence of \emph{adversarial} bandit problems. ARL achieves $O(\sqrt{SATH^3})$ regret, which is optimal with respect to $S$, $A$, and $T$, and its dependence on $H$ is the best (even for the usual stationary MDP) among existing model-free methods.",0
"The Markov Decision Process (MDP) is a widely used model for reinforcement learning. However, its assumption of stationary dynamics and rewards is rigid and does not hold true in situations involving adversarial, nonstationary, or multi-agent problems. To address this, we examine an episodic scenario where the parameters of an MDP can differ in each episode. We have developed an Adversarial Reinforcement Learning (ARL) algorithm that reduces our MDP to a sequence of \emph{adversarial} bandit problems, allowing us to learn a reliable policy for this potentially adversarial MDP. ARL achieves optimal regret with respect to $S$, $A$, and $T$, with a dependence on $H$ that is the best among existing model-free methods, even for the usual stationary MDP.",1
"Reinforcement learning has become one of the best approach to train a computer game emulator capable of human level performance. In a reinforcement learning approach, an optimal value function is learned across a set of actions, or decisions, that leads to a set of states giving different rewards, with the objective to maximize the overall reward. A policy assigns to each state-action pairs an expected return. We call an optimal policy a policy for which the value function is optimal. QLBS, Q-Learner in the Black-Scholes(-Merton) Worlds, applies the reinforcement learning concepts, and noticeably, the popular Q-learning algorithm, to the financial stochastic model of Black, Scholes and Merton. It is, however, specifically optimized for the geometric Brownian motion and the vanilla options. Its range of application is, therefore, limited to vanilla option pricing within financial markets. We propose MQLV, Modified Q-Learner for the Vasicek model, a new reinforcement learning approach that determines the optimal policy of money management based on the aggregated financial transactions of the clients. It unlocks new frontiers to establish personalized credit card limits or to fulfill bank loan applications, targeting the retail banking industry. MQLV extends the simulation to mean reverting stochastic diffusion processes and it uses a digital function, a Heaviside step function expressed in its discrete form, to estimate the probability of a future event such as a payment default. In our experiments, we first show the similarities between a set of historical financial transactions and Vasicek generated transactions and, then, we underline the potential of MQLV on generated Monte Carlo simulations. Finally, MQLV is the first Q-learning Vasicek-based methodology addressing transparent decision making processes in retail banking.",0
"The use of reinforcement learning has emerged as a leading technique to train computer game emulators to perform at a human level. This approach involves learning an optimal value function across a range of actions that lead to different states and rewards, all with the ultimate goal of maximizing the overall reward. An optimal policy is one where the value function is optimal, and QLBS applies this reinforcement learning concept, specifically using the Q-learning algorithm, to the financial stochastic model of Black, Scholes and Merton. However, QLBS is limited to vanilla option pricing within financial markets. In contrast, MQLV is a new reinforcement learning approach that determines the optimal policy of money management based on the aggregated financial transactions of clients, with applications in personalized credit card limits and bank loan applications in the retail banking industry. MQLV extends the simulation to mean-reverting stochastic diffusion processes and uses a digital function to estimate the probability of future events such as payment defaults. Our experiments show the potential of MQLV on generated Monte Carlo simulations, making it the first Q-learning Vasicek-based methodology that addresses transparent decision making processes in retail banking.",1
"To make efficient use of limited spectral resources, we in this work propose a deep actor-critic reinforcement learning based framework for dynamic multichannel access. We consider both a single-user case and a scenario in which multiple users attempt to access channels simultaneously. We employ the proposed framework as a single agent in the single-user case, and extend it to a decentralized multi-agent framework in the multi-user scenario. In both cases, we develop algorithms for the actor-critic deep reinforcement learning and evaluate the proposed learning policies via experiments and numerical results. In the single-user model, in order to evaluate the performance of the proposed channel access policy and the framework's tolerance against uncertainty, we explore different channel switching patterns and different switching probabilities. In the case of multiple users, we analyze the probabilities of each user accessing channels with favorable channel conditions and the probability of collision. We also address a time-varying environment to identify the adaptive ability of the proposed framework. Additionally, we provide comparisons (in terms of both the average reward and time efficiency) between the proposed actor-critic deep reinforcement learning framework, Deep-Q network (DQN) based approach, random access, and the optimal policy when the channel dynamics are known.",0
"Our work proposes a framework for dynamic multichannel access using deep actor-critic reinforcement learning to optimize limited spectral resources. We investigate both single-user and multi-user scenarios, implementing the framework as a single agent in the former and a decentralized multi-agent framework in the latter. We develop algorithms for actor-critic deep reinforcement learning in both cases and evaluate the learning policies through experiments and numerical results. In the single-user model, we explore different channel switching patterns and probabilities to evaluate the framework's performance and tolerance to uncertainty. In the multi-user scenario, we analyze the probabilities of users accessing channels with favorable channel conditions and the probability of collision while also addressing a time-varying environment. We provide comparisons between our proposed framework, Deep-Q network (DQN) based approach, random access, and the optimal policy when channel dynamics are known, considering both average reward and time efficiency.",1
"Transfer learning methods for reinforcement learning (RL) domains facilitate the acquisition of new skills using previously acquired knowledge. The vast majority of existing approaches assume that the agents have the same design, e.g. same shape and action spaces. In this paper we address the problem of transferring previously acquired skills amongst morphologically different agents (MDAs). For instance, assuming that a bipedal agent has been trained to move forward, could this skill be transferred on to a one-leg hopper so as to make its training process for the same task more sample efficient? We frame this problem as one of subspace learning whereby we aim to infer latent factors representing the control mechanism that is common between MDAs. We propose a novel paired variational encoder-decoder model, PVED, that disentangles the control of MDAs into shared and agent-specific factors. The shared factors are then leveraged for skill transfer using RL. Theoretically, we derive a theorem indicating how the performance of PVED depends on the shared factors and agent morphologies. Experimentally, PVED has been extensively validated on four MuJoCo environments. We demonstrate its performance compared to a state-of-the-art approach and several ablation cases, visualize and interpret the hidden factors, and identify avenues for future improvements.",0
"The use of transfer learning in reinforcement learning domains enables the application of previously learned knowledge to acquire new skills. However, most current methods assume that agents have the same characteristics, such as shape and action spaces. This study aims to address the challenge of transferring skills among agents with different morphologies. For example, can the skill of a bipedal agent moving forward be transferred to a one-leg hopper to improve its training efficiency? The authors propose a subspace learning approach to infer the latent factors that control morphologically different agents. They introduce a new model called PVED, which separates the control of MDAs into shared and agent-specific factors, and use RL to leverage the shared factors for skill transfer. The authors derive a theorem to explain the performance of PVED and validate the model on four MuJoCo environments. They compare PVED to a state-of-the-art approach and analyze the hidden factors in the model. The study also identifies areas for future research.",1
"Over the recent years, there has been an explosion of studies on autonomous vehicles. Many collected large amount of data from human drivers. However, compared to the tedious data collection approach, building a virtual simulation of traffic makes the autonomous vehicle research more flexible, time-saving, and scalable. Our work features a 3D simulation that takes in real time position information parsed from street cameras. The simulation can easily switch between a global bird view of the traffic and a local perspective of a car. It can also filter out certain objects in its customized camera, creating various channels for objects of different categories. This provides alternative supervised or unsupervised ways to train deep neural networks. Another advantage of the 3D simulation is its conformation to physical laws. Its naturalness to accelerate and collide prepares the system for potential deep reinforcement learning needs.",0
"In recent years, numerous studies have been conducted on autonomous vehicles, with many of them relying on extensive data collection from human drivers. However, the process of collecting this data can be time-consuming and tedious. In contrast, the use of a virtual simulation of traffic can provide flexibility, scalability, and time-saving benefits. Our work involves a 3D simulation that uses real-time position information obtained from street cameras. This simulation offers the ability to switch between a global traffic view and a local car perspective, as well as the ability to filter out objects based on different categories. This creates various channels that can be used for supervised or unsupervised training of deep neural networks. Additionally, the 3D simulation adheres to physical laws, allowing for natural acceleration and collision, thus preparing the system for potential deep reinforcement learning needs.",1
"Graph Neural Networks (GNNs) have been popularly used for analyzing non-Euclidean data such as social network data and biological data. Despite their success, the design of graph neural networks requires a lot of manual work and domain knowledge. In this paper, we propose a Graph Neural Architecture Search method (GraphNAS for short) that enables automatic search of the best graph neural architecture based on reinforcement learning. Specifically, GraphNAS first uses a recurrent network to generate variable-length strings that describe the architectures of graph neural networks, and then trains the recurrent network with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation data set. Extensive experimental results on node classification tasks in both transductive and inductive learning settings demonstrate that GraphNAS can achieve consistently better performance on the Cora, Citeseer, Pubmed citation network, and protein-protein interaction network. On node classification tasks, GraphNAS can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy.",0
"The utilization of Graph Neural Networks (GNNs) has become widespread in the analysis of non-Euclidean data such as biological and social network data. However, creating GNN designs requires significant manual work and domain knowledge. This research presents an approach called Graph Neural Architecture Search (GraphNAS) that uses reinforcement learning to automatically search for the best GNN architecture. GraphNAS generates variable-length strings that describe the GNN architectures using a recurrent network and trains the network with reinforcement learning to maximize accuracy on a validation data set. The results of extensive experiments on node classification tasks demonstrate that GraphNAS consistently outperforms existing methods on Cora, Citeseer, Pubmed citation network, and protein-protein interaction network. Moreover, GraphNAS can produce a novel network architecture with test set accuracy comparable to the best human-invented architecture for node classification tasks.",1
