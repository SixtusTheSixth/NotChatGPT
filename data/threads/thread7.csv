"Neural networks are frequently used for image classification, but can be vulnerable to misclassification caused by adversarial images. Attempts to make neural network image classification more robust have included variations on preprocessing (cropping, applying noise, blurring), adversarial training, and dropout randomization. In this paper, we implemented a model for adversarial detection based on a combination of two of these techniques: dropout randomization with preprocessing applied to images within a given Bayesian uncertainty. We evaluated our model on the MNIST dataset, using adversarial images generated using Fast Gradient Sign Method (FGSM), Jacobian-based Saliency Map Attack (JSMA) and Basic Iterative Method (BIM) attacks. Our model achieved an average adversarial image detection accuracy of 97%, with an average image classification accuracy, after discarding images flagged as adversarial, of 99%. Our average detection accuracy exceeded that of recent papers using similar techniques.",0
"The use of neural networks is common for image classification, but they can be susceptible to misidentification due to adversarial images. Various methods have been employed to improve the robustness of neural network image classification, such as preprocessing techniques (e.g. cropping, noise application, and blurring), adversarial training, and dropout randomization. Our study introduced a model for detecting adversarial images that combined two of these techniques: dropout randomization and preprocessing based on Bayesian uncertainty. We assessed the model's performance on the MNIST dataset, using adversarial images generated through Fast Gradient Sign Method (FGSM), Jacobian-based Saliency Map Attack (JSMA), and Basic Iterative Method (BIM). Our model exhibited an average accuracy of 97% in detecting adversarial images, while achieving an average image classification accuracy of 99% after removing flagged images. Our average detection accuracy outperformed recent studies using comparable techniques.",1
"Model explanations based on pure observational data cannot compute the effects of features reliably, due to their inability to estimate how each factor alteration could affect the rest. We argue that explanations should be based on the causal model of the data and the derived intervened causal models, that represent the data distribution subject to interventions. With these models, we can compute counterfactuals, new samples that will inform us how the model reacts to feature changes on our input. We propose a novel explanation methodology based on Causal Counterfactuals and identify the limitations of current Image Generative Models in their application to counterfactual creation.",0
"Explanations that rely solely on observational data are unreliable in determining the effects of features because they are unable to accurately estimate the impact that changing one factor would have on the others. Our argument is that explanations should instead be based on the causal model of the data and the resulting intervened causal models, which represent the data distribution under specific interventions. By utilizing these models, we can generate counterfactuals, which are new samples that reveal how the model responds to modifications in the input features. Our proposed explanation methodology is centered on Causal Counterfactuals, and we highlight the limitations of current Image Generative Models in their ability to create counterfactuals.",1
"Generating a photorealistic image with intended human pose is a promising yet challenging research topic for many applications such as smart photo editing, movie making, virtual try-on, and fashion display. In this paper, we present a novel deep generative model to transfer an image of a person from a given pose to a new pose while keeping fashion item consistent. In order to formulate the framework, we employ one generator and two discriminators for image synthesis. The generator includes an image encoder, a pose encoder and a decoder. The two encoders provide good representation of visual and geometrical context which will be utilized by the decoder in order to generate a photorealistic image. Unlike existing pose-guided image generation models, we exploit two discriminators to guide the synthesis process where one discriminator differentiates between generated image and real images (training samples), and another discriminator verifies the consistency of appearance between a target pose and a generated image. We perform end-to-end training of the network to learn the parameters through back-propagation given ground-truth images. The proposed generative model is capable of synthesizing a photorealistic image of a person given a target pose. We have demonstrated our results by conducting rigorous experiments on two data sets, both quantitatively and qualitatively.",0
"The research topic of generating a photorealistic image with a desired human pose is both promising and challenging, with applications ranging from smart photo editing to fashion display. Our paper introduces a new deep generative model that can transfer a person's image from one pose to another while maintaining fashion consistency. This framework involves one generator and two discriminators for image synthesis, with an image encoder, pose encoder, and decoder in the generator. By utilizing the representation of visual and geometrical context provided by the encoders, the decoder can generate a realistic image. Unlike other pose-guided image generation models, we use two discriminators to guide the synthesis process by differentiating between real and generated images and verifying the consistency of appearance between the target pose and generated image. We train the network end-to-end using back-propagation with ground-truth images. Our generative model is capable of creating photorealistic images of people with a target pose, as demonstrated through rigorous quantitative and qualitative experiments on two different data sets.",1
"There is an urgent need for an effective video classification method by means of a small number of samples. The deficiency of samples could be effectively alleviated by generating samples through Generative Adversarial Networks (GAN), but the generation of videos on a typical category remains to be underexplored since the complex actions and the changeable viewpoints are difficult to simulate. In this paper, we propose a generative data augmentation method for temporal stream of the Temporal Segment Networks with the dynamic image. The dynamic image compresses the motion information of video into a still image, removing the interference factors such as the background. Thus it is easier to generate images with categorical motion information using GAN. We use the generated dynamic images to enhance the features, with regularization achieved as well, thereby to achieve the effect of video augmentation. In order to deal with the uneven quality of generated images, we propose a Self-Paced Selection (SPS) method, which automatically selects the high-quality generated samples to be added to the network training. Our method is verified on two benchmark datasets, HMDB51 and UCF101. The experimental results show that the method can improve the accuracy of video classification under the circumstance of sample insufficiency and sample imbalance.",0
"An urgent need exists for an effective method of video classification using a small number of samples. While Generative Adversarial Networks (GAN) can generate samples to alleviate the deficiency, the generation of videos for a typical category remains underexplored due to the difficulty of simulating complex actions and changeable viewpoints. In this study, we propose a generative data augmentation method for the Temporal Segment Networks' temporal stream with the dynamic image. The dynamic image compresses motion information into a still image, eliminating interference factors such as the background. We use the generated dynamic images to enhance features and achieve video augmentation with regularization. To address the uneven quality of generated images, we propose a Self-Paced Selection (SPS) method that automatically selects high-quality samples for network training. Our method is verified on benchmark datasets, HMDB51 and UCF101, and experimental results demonstrate its ability to improve video classification accuracy in situations of sample insufficiency and imbalance.",1
"There is a growing interest in using generative adversarial networks (GANs) to produce image content that is indistinguishable from real images as judged by a typical person. A number of GAN variants for this purpose have been proposed, however, evaluating GANs performance is inherently difficult because current methods for measuring the quality of their output are not always consistent with what a human perceives. We propose a novel approach that combines a brain-computer interface (BCI) with GANs to generate a measure we call Neuroscore, which closely mirrors the behavioral ground truth measured from participants tasked with discerning real from synthetic images. This technique we call a neuro-AI interface, as it provides an interface between a human's neural systems and an AI process. In this paper, we first compare the three most widely used metrics in the literature for evaluating GANs in terms of visual quality and compare their outputs with human judgments. Secondly we propose and demonstrate a novel approach using neural signals and rapid serial visual presentation (RSVP) that directly measures a human perceptual response to facial production quality, independent of a behavioral response measurement. The correlation between our proposed Neuroscore and human perceptual judgments has Pearson correlation statistics: $\mathrm{r}(48) = -0.767, \mathrm{p} = 2.089e-10$. We also present the bootstrap result for the correlation i.e., $\mathrm{p}\leq 0.0001$. Results show that our Neuroscore is more consistent with human judgment compared to the conventional metrics we evaluated. We conclude that neural signals have potential applications for high quality, rapid evaluation of GANs in the context of visual image synthesis.",0
"The use of generative adversarial networks (GANs) to create realistic images has gained popularity, but evaluating their performance accurately is challenging as current methods don't always align with human perception. To address this, we propose a new technique called a neuro-AI interface, which combines a brain-computer interface (BCI) with GANs to measure a human's perceptual response to synthetic images. This technique generates a measure called Neuroscore, which has a strong correlation with human judgment, as shown by our experiments. Our proposed approach is more consistent with human perception than existing metrics, and we conclude that it has potential for rapid evaluation of GANs in image synthesis.",1
"We make a minimal, but very effective alteration to the VAE model. This is about a drop-in replacement for the (sample-dependent) approximate posterior to change it from the standard white Gaussian with diagonal covariance to the first-order autoregressive Gaussian. We argue that this is a more reasonable choice to adopt for natural signals like images, as it does not force the existing correlation in the data to disappear in the posterior. Moreover, it allows more freedom for the approximate posterior to match the true posterior. This allows for the repararametrization trick, as well as the KL-divergence term to still have closed-form expressions, obviating the need for its sample-based estimation. Although providing more freedom to adapt to correlated distributions, our parametrization has even less number of parameters than the diagonal covariance, as it requires only two scalars, $\rho$ and $s$, to characterize correlation and scaling, respectively. As validated by the experiments, our proposition noticeably and consistently improves the quality of image generation in a plug-and-play manner, needing no further parameter tuning, and across all setups. The code to reproduce our experiments is available at \url{https://github.com/sssohrab/rho_VAE/}.",0
"We have made a simple yet highly effective modification to the VAE model. Specifically, we have replaced the sample-dependent approximate posterior with a first-order autoregressive Gaussian, which is a more appropriate choice for natural signals like images. Unlike the standard white Gaussian with diagonal covariance, our modification does not eliminate pre-existing correlations in the data, allowing for greater flexibility in matching the true posterior. Additionally, our approach enables the use of the repararametrization trick and eliminates the need for sample-based estimation of the KL-divergence term. Despite providing more adaptability to correlated distributions, our parametrization requires only two scalars, $\rho$ and $s$, to describe correlation and scaling, respectively, making it even more efficient than the diagonal covariance. Our experimental results demonstrate that this modification significantly and consistently enhances the quality of image generation without requiring additional parameter tuning, and across all experimental setups. To replicate our experiments, please refer to the code available at \url{https://github.com/sssohrab/rho_VAE/}.",1
"Detecting manipulated images has become a significant emerging challenge. The advent of image sharing platforms and the easy availability of advanced photo editing software have resulted in a large quantities of manipulated images being shared on the internet. While the intent behind such manipulations varies widely, concerns on the spread of fake news and misinformation is growing. Current state of the art methods for detecting these manipulated images suffers from the lack of training data due to the laborious labeling process. We address this problem in this paper, for which we introduce a manipulated image generation process that creates true positives using currently available datasets. Drawing from traditional work on image blending, we propose a novel generator for creating such examples. In addition, we also propose to further create examples that force the algorithm to focus on boundary artifacts during training. Strong experimental results validate our proposal.",0
"The emergence of manipulated images has become a significant challenge due to the widespread use of image sharing platforms and easily accessible advanced photo editing software. This has resulted in a large number of manipulated images being shared on the internet, with varying intent behind such manipulations. However, the spread of fake news and misinformation is a growing concern. Unfortunately, current state-of-the-art methods for detecting manipulated images suffer from a lack of training data, which is due to the time-consuming labeling process. In this paper, we propose a solution to this problem by introducing a manipulated image generation process that utilizes currently available datasets to create true positives. We propose a novel generator inspired by traditional work on image blending, which creates such examples. Furthermore, we suggest creating additional examples that will force the algorithm to focus on boundary artifacts during training. Our proposal is validated by strong experimental results.",1
"One of the challenges of using machine learning techniques with medical data is the frequent dearth of source image data on which to train. A representative example is automated lung cancer diagnosis, where nodule images need to be classified as suspicious or benign. In this work we propose an automatic synthetic lung nodule image generator. Our 3D shape generator is designed to augment the variety of 3D images. Our proposed system takes root in autoencoder techniques, and we provide extensive experimental characterization that demonstrates its ability to produce quality synthetic images.",0
"When working with medical data, machine learning methods often face the problem of insufficient source image data for training purposes. A specific instance of this issue is found in the automated diagnosis of lung cancer, which requires classifying nodule images as either benign or suspicious. To address this challenge, we suggest a synthetic image generator for lung nodules that operates automatically. Our generator produces a diverse range of 3D images by using an autoencoder approach, and we have conducted comprehensive experiments that confirm its ability to generate high-quality synthetic images.",1
"Given an outfit, what small changes would most improve its fashionability? This question presents an intriguing new vision challenge. We introduce Fashion++, an approach that proposes minimal adjustments to a full-body clothing outfit that will have maximal impact on its fashionability. Our model consists of a deep image generation neural network that learns to synthesize clothing conditioned on learned per-garment encodings. The latent encodings are explicitly factorized according to shape and texture, thereby allowing direct edits for both fit/presentation and color/patterns/material, respectively. We show how to bootstrap Web photos to automatically train a fashionability model, and develop an activation maximization-style approach to transform the input image into its more fashionable self. The edits suggested range from swapping in a new garment to tweaking its color, how it is worn (e.g., rolling up sleeves), or its fit (e.g., making pants baggier). Experiments demonstrate that Fashion++ provides successful edits, both according to automated metrics and human opinion. Project page is at http://vision.cs.utexas.edu/projects/FashionPlus.",0
"What minor adjustments could enhance the style of an outfit? This inquiry poses an interesting visual challenge. Our solution, Fashion++, recommends minimal changes to a complete clothing ensemble that will have the greatest impact on its fashion appeal. Our system employs a deep neural network that generates images of clothing, based on encoded garment information. The encodings are separated by shape and texture, enabling modifications to both fit and color/pattern/material. We use web photos to train a fashionability model and an activation maximization-style technique to transform the original image into a more fashionable version. Our suggested edits include adding new garments, altering color, adjusting fit, or changing how the clothing is worn. Fashion++ produces successful changes, as confirmed by automated metrics and human feedback. Visit our project page at http://vision.cs.utexas.edu/projects/FashionPlus for more information.",1
"By their very nature microscopy images of cells and tissues consist of a limited number of object types or components. In contrast to most natural scenes, the composition is known a priori. Decomposing biological images into semantically meaningful objects and layers is the aim of this paper. Building on recent approaches to image de-noising we present a framework that achieves state-of-the-art segmentation results requiring little or no manual annotations. Here, synthetic images generated by adding cell crops are sufficient to train the model. Extensive experiments on cellular images, a histology data set, and small animal videos demonstrate that our approach generalizes to a broad range of experimental settings. As the proposed methodology does not require densely labelled training images and is capable of resolving the partially overlapping objects it holds the promise of being of use in a number of different applications.",0
"The nature of microscopy images of cells and tissues is such that they are composed of a limited number of object types or components, with their composition being predetermined unlike most natural scenes. The objective of this paper is to decompose biological images into semantically meaningful objects and layers. Based on recent image de-noising approaches, we present a framework that yields state-of-the-art segmentation outcomes with minimal or no manual annotations. The model is trained on synthetic images that include cell crops. Our experimental results on cellular images, a histology data set, and small animal videos showcase the versatility of our approach across a wide range of experimental settings. Given that our methodology does not necessitate densely labelled training images and can effectively distinguish partially overlapping objects, it has the potential to be applied in various applications.",1
"Generative Adversarial Networks have been crucial in the developments made in unsupervised learning in recent times. Exemplars of image synthesis from text or other images, these networks have shown remarkable improvements over conventional methods in terms of performance. Trained on the adversarial training philosophy, these networks aim to estimate the potential distribution from the real data and then use this as input to generate the synthetic data. Based on this fundamental principle, several frameworks can be generated that are paragon implementations in several real-life applications such as art synthesis, generation of high resolution outputs and synthesis of images from human drawn sketches, to name a few. While theoretically GANs present better results and prove to be an improvement over conventional methods in many factors, the implementation of these frameworks for dedicated applications remains a challenge. This study explores and presents a taxonomy of these frameworks and their use in various image to image synthesis and text to image synthesis applications. The basic GANs, as well as a variety of different niche frameworks, are critically analyzed. The advantages of GANs for image generation over conventional methods as well their disadvantages amongst other frameworks are presented. The future applications of GANs in industries such as healthcare, art and entertainment are also discussed.",0
"Recently, Generative Adversarial Networks have played a pivotal role in the progress of unsupervised learning. These networks have exhibited exceptional advancements in performance as compared to traditional methods, serving as models for image synthesis from text or other images. Utilizing the adversarial training philosophy, GANs endeavor to approximate the potential distribution from actual data to generate synthetic data. As a result, numerous frameworks have been established based on this principle, showcasing their applicability in various domains including high-resolution output generation, art synthesis, and image synthesis from human-drawn sketches. Despite their theoretical superiority over conventional methods, the practical implementation of GANs in specific fields presents a challenge. This research delves into the taxonomy of these frameworks and their utilization in image-to-image and text-to-image synthesis applications, analyzing the basic GANs and several niche frameworks. The study also highlights the advantages and disadvantages of GANs over other frameworks for image generation and discusses their potential applications in healthcare, art, and entertainment industries.",1
"The rapid progress in synthetic image generation and manipulation has now come to a point where it raises significant concerns for the implications towards society. At best, this leads to a loss of trust in digital content, but could potentially cause further harm by spreading false information or fake news. This paper examines the realism of state-of-the-art image manipulations, and how difficult it is to detect them, either automatically or by humans. To standardize the evaluation of detection methods, we propose an automated benchmark for facial manipulation detection. In particular, the benchmark is based on DeepFakes, Face2Face, FaceSwap and NeuralTextures as prominent representatives for facial manipulations at random compression level and size. The benchmark is publicly available and contains a hidden test set as well as a database of over 1.8 million manipulated images. This dataset is over an order of magnitude larger than comparable, publicly available, forgery datasets. Based on this data, we performed a thorough analysis of data-driven forgery detectors. We show that the use of additional domainspecific knowledge improves forgery detection to unprecedented accuracy, even in the presence of strong compression, and clearly outperforms human observers.",0
"The progress made in creating and altering synthetic images has reached a point where it raises concerns for society. These concerns range from a lack of trust in digital content to the spread of false information and fake news. This research paper explores the difficulty in detecting state-of-the-art image manipulations, whether by humans or automated methods. To create a standardized way of evaluating detection methods, an automated benchmark for facial manipulation detection is proposed. This benchmark uses prominent manipulations such as DeepFakes, Face2Face, FaceSwap, and NeuralTextures at various compression levels and sizes. The benchmark is publicly available and includes a database of over 1.8 million manipulated images, which is significantly larger than comparable forgery datasets. By analyzing this data, the study shows that the use of domain-specific knowledge improves forgery detection accuracy, even in cases with strong compression, and outperforms human observers.",1
"Image generating neural networks are mostly viewed as black boxes, where any change in the input can have a number of globally effective changes on the output. In this work, we propose a method for learning disentangled representations to allow for localized image manipulations. We use face images as our example of choice. Depending on the image region, identity and other facial attributes can be modified. The proposed network can transfer parts of a face such as shape and color of eyes, hair, mouth, etc.~directly between persons while all other parts of the face remain unchanged. The network allows to generate modified images which appear like realistic images. Our model learns disentangled representations by weak supervision. We propose a localized resnet autoencoder optimized using several loss functions including a loss based on the semantic segmentation, which we interpret as masks, and a loss which enforces disentanglement by decomposition of the latent space into statistically independent subspaces. We evaluate the proposed solution w.r.t. disentanglement and generated image quality. Convincing results are demonstrated using the CelebA dataset.",0
"The majority of people consider image generating neural networks to be opaque, as any alteration to the input can produce a variety of changes in the output. This research suggests a technique for acquiring disentangled representations, enabling localized image manipulation, using face images as an example. Depending on the facial area, identity and other facial characteristics can be modified. The network can transfer specific facial features, such as eye shape and color, hair, and mouth, from one person to another while keeping all other facial features unaltered. The network produces modified images that look realistic. Our model develops disentangled representations through weak supervision. We propose a localized resnet autoencoder, which optimizes various loss functions, including a semantic segmentation-based loss, interpreted as masks, and a loss that ensures disentanglement via the decomposition of the latent space into statistically independent subspaces. We evaluate the proposed solution's disentanglement and generated image quality. The CelebA dataset exhibits convincing outcomes.",1
"We propose a self-supervised learning framework for visual odometry (VO) that incorporates correlation of consecutive frames and takes advantage of adversarial learning. Previous methods tackle self-supervised VO as a local structure from motion (SfM) problem that recovers depth from single image and relative poses from image pairs by minimizing photometric loss between warped and captured images. As single-view depth estimation is an ill-posed problem, and photometric loss is incapable of discriminating distortion artifacts of warped images, the estimated depth is vague and pose is inaccurate. In contrast to previous methods, our framework learns a compact representation of frame-to-frame correlation, which is updated by incorporating sequential information. The updated representation is used for depth estimation. Besides, we tackle VO as a self-supervised image generation task and take advantage of Generative Adversarial Networks (GAN). The generator learns to estimate depth and pose to generate a warped target image. The discriminator evaluates the quality of generated image with high-level structural perception that overcomes the problem of pixel-wise loss in previous methods. Experiments on KITTI and Cityscapes datasets show that our method obtains more accurate depth with details preserved and predicted pose outperforms state-of-the-art self-supervised methods significantly.",0
"Our proposal is a self-supervised learning framework that uses correlation between consecutive frames and incorporates adversarial learning for visual odometry (VO). Previous methods approach self-supervised VO as a local structure from motion (SfM) problem that recovers depth from a single image and relative poses from image pairs by minimizing photometric loss between warped and captured images. However, this approach is limited by the ill-posed nature of single-view depth estimation and the inability of photometric loss to distinguish distortion artifacts of warped images, leading to vague depth estimates and inaccurate poses. In contrast, our framework learns a compact representation of frame-to-frame correlation that is updated with sequential information, improving depth estimation. Additionally, we treat VO as a self-supervised image generation task using Generative Adversarial Networks (GAN), where the generator estimates depth and pose to generate a warped target image, and the discriminator evaluates the quality of the generated image with high-level structural perception, overcoming the limitations of pixel-wise loss in previous methods. Our experiments on KITTI and Cityscapes datasets demonstrate that our method achieves more accurate depth with preserved details and significantly outperforms state-of-the-art self-supervised methods in predicting poses.",1
"Lifelong learning is challenging for deep neural networks due to their susceptibility to catastrophic forgetting. Catastrophic forgetting occurs when a trained network is not able to maintain its ability to accomplish previously learned tasks when it is trained to perform new tasks. We study the problem of lifelong learning for generative models, extending a trained network to new conditional generation tasks without forgetting previous tasks, while assuming access to the training data for the current task only. In contrast to state-of-the-art memory replay based approaches which are limited to label-conditioned image generation tasks, a more generic framework for continual learning of generative models under different conditional image generation settings is proposed in this paper. Lifelong GAN employs knowledge distillation to transfer learned knowledge from previous networks to the new network. This makes it possible to perform image-conditioned generation tasks in a lifelong learning setting. We validate Lifelong GAN for both image-conditioned and label-conditioned generation tasks, and provide qualitative and quantitative results to show the generality and effectiveness of our method.",0
"Due to their tendency to experience catastrophic forgetting, deep neural networks face difficulties in achieving lifelong learning. This means that when a network is trained to perform new tasks, it may lose its ability to perform previously learned tasks. In this study, we address the challenge of lifelong learning for generative models by extending a trained network to new conditional generation tasks while preserving its ability to accomplish previous tasks. Our proposed approach is a more comprehensive framework for continual learning of generative models under different image generation settings compared to state-of-the-art memory replay based approaches, which are limited to label-conditioned image generation tasks. We introduce Lifelong GAN, a method that utilizes knowledge distillation to transfer learned knowledge from previous networks to a new network, enabling image-conditioned generation tasks in a lifelong learning setting. We demonstrate the effectiveness and generality of our method through qualitative and quantitative results for both image-conditioned and label-conditioned generation tasks.",1
"Image de-fencing is one of the important aspects of recreational photography in which the objective is to remove the fence texture present in an image and generate an aesthetically pleasing version of the same image without the fence texture. In this paper, we aim to develop an automated and effective technique for fence removal and image reconstruction using conditional Generative Adversarial Networks (cGANs). These networks have been successfully applied in several domains of Computer Vision focusing on image generation and rendering. Our initial approach is based on a two-stage architecture involving two cGANs that generate the fence mask and the inpainted image, respectively. Training of these networks is carried out independently and, during evaluation, the input image is passed through the two generators in succession to obtain the de-fenced image. The results obtained from this approach are satisfactory, but the response time is long since the image has to pass through two sets of convolution layers. To reduce the response time, we propose a second approach involving only a single cGAN architecture that is trained using the ground-truth of fenced de-fenced image pairs along with the edge map of the fenced image produced by the Canny Filter. Incorporation of the edge map helps the network to precisely detect the edges present in the input image, and also imparts it an ability to carry out high quality de-fencing in an efficient manner, even in the presence of a fewer number of layers as compared to the two-stage network. Qualitative and quantitative experimental results reported in the manuscript reveal that the de-fenced images generated by the single-stage de-fencing network have similar visual quality to those produced by the two-stage network. Comparative performance analysis also emphasizes the effectiveness of our approach over state-of-the-art image de-fencing techniques.",0
"Recreational photography involves the removal of fence textures from images to create a more aesthetically pleasing version. This paper aims to develop an automated and effective technique for fence removal and image reconstruction using conditional Generative Adversarial Networks (cGANs). These networks have been successfully used in various Computer Vision domains for image generation and rendering. Our initial approach involves two cGANs that generate the fence mask and the inpainted image, respectively, but the response time is long. To reduce this, our second approach involves a single cGAN architecture trained using the ground-truth of fenced de-fenced image pairs along with the edge map of the fenced image produced by the Canny Filter. The network can detect edges precisely and carry out high-quality de-fencing efficiently with fewer layers. Experimental results reveal that the single-stage de-fencing network produces images similar in quality to those produced by the two-stage network and outperforms state-of-the-art image de-fencing techniques.",1
"Neural architecture search (NAS) has witnessed prevailing success in image classification and (very recently) segmentation tasks. In this paper, we present the first preliminary study on introducing the NAS algorithm to generative adversarial networks (GANs), dubbed AutoGAN. The marriage of NAS and GANs faces its unique challenges. We define the search space for the generator architectural variations and use an RNN controller to guide the search, with parameter sharing and dynamic-resetting to accelerate the process. Inception score is adopted as the reward, and a multi-level search strategy is introduced to perform NAS in a progressive way. Experiments validate the effectiveness of AutoGAN on the task of unconditional image generation. Specifically, our discovered architectures achieve highly competitive performance compared to current state-of-the-art hand-crafted GANs, e.g., setting new state-of-the-art FID scores of 12.42 on CIFAR-10, and 31.01 on STL-10, respectively. We also conclude with a discussion of the current limitations and future potential of AutoGAN. The code is available at https://github.com/TAMU-VITA/AutoGAN",0
"The success of Neural architecture search (NAS) has been observed in image classification and segmentation tasks. In this study, we present an initial investigation of introducing the NAS algorithm to generative adversarial networks (GANs), which we call AutoGAN. Combining NAS and GANs has its challenges, but we define the search space for generator architectural variations and use an RNN controller to guide the search. We adopt the Inception score as the reward and introduce a multi-level search strategy for performing NAS progressively. Our experiments demonstrate the effectiveness of AutoGAN in unconditional image generation, with discovered architectures achieving highly competitive performance compared to current state-of-the-art GANs. The code for AutoGAN is available at https://github.com/TAMU-VITA/AutoGAN. Finally, we discuss the current limitations and future potential of AutoGAN.",1
"Generative Adversarial Networks (GANs) are considered the state-of-the-art in the field of image generation. They learn the joint distribution of the training data and attempt to generate new data samples in high dimensional space following the same distribution as the input. Recent improvements in GANs opened the field to many other computer vision applications based on improving and changing the characteristics of the input image to follow some given training requirements. In this paper, we propose a novel technique for the denoising and reconstruction of the micro-Doppler ($\boldsymbol{\mu}$-D) spectra of walking humans based on GANs. Two sets of experiments were collected on 22 subjects walking on a treadmill at an intermediate velocity using a \unit[25]{GHz} CW radar. In one set, a clean $\boldsymbol{\mu}$-D spectrum is collected for each subject by placing the radar at a close distance to the subject. In the other set, variations are introduced in the experiment setup to introduce different noise and clutter effects on the spectrum by changing the distance and placing reflective objects between the radar and the target. Synthetic paired noisy and noise-free spectra were used for training, while validation was carried out on the real noisy measured data. Finally, qualitative and quantitative comparison with other classical radar denoising approaches in the literature demonstrated the proposed GANs framework is better and more robust to different noise levels.",0
"The use of Generative Adversarial Networks (GANs) is currently the most advanced method for generating images. GANs learn the joint distribution of training data and use this knowledge to create new data samples in a high dimensional space that follows the same distribution as the input. Recent advancements in GANs have expanded their use in computer vision, allowing for improvements and alterations to input images to meet specific training requirements. In this study, we present a new technique for denoising and reconstructing micro-Doppler ($\boldsymbol{\mu}$-D) spectra of walking humans using GANs. The experiments involved collecting two sets of data from 22 subjects walking on a treadmill at an intermediate velocity using a \unit[25]{GHz} CW radar. One set gathered clean $\boldsymbol{\mu}$-D spectra by placing the radar close to the subject, while the other introduced variations in the experiment set up to create different noise and clutter effects on the spectrum. Synthetic paired noisy and noise-free spectra were used for training, and validation was conducted on real, noisy data. Results show that the proposed GANs framework is more effective and resilient to different noise levels than classical radar denoising methods found in the literature.",1
"Generative Adversarial Networks (GANs) have the capability of synthesizing images, which have been successfully applied to medical image synthesis tasks. However, most of existing methods merely consider the global contextual information and ignore the fine foreground structures, e.g., vessel, skeleton, which may contain diagnostic indicators for medical image analysis. Inspired by human painting procedure, which is composed of stroking and color rendering steps, we propose a Sketching-rendering Unconditional Generative Adversarial Network (SkrGAN) to introduce a sketch prior constraint to guide the medical image generation. In our SkrGAN, a sketch guidance module is utilized to generate a high quality structural sketch from random noise, then a color render mapping is used to embed the sketch-based representations and resemble the background appearances. Experimental results show that the proposed SkrGAN achieves the state-of-the-art results in synthesizing images for various image modalities, including retinal color fundus, X-Ray, Computed Tomography (CT) and Magnetic Resonance Imaging (MRI). In addition, we also show that the performances of medical image segmentation method have been improved by using our synthesized images as data augmentation.",0
"Medical image synthesis tasks can benefit greatly from Generative Adversarial Networks (GANs) as they can synthesize images. However, most existing methods do not consider the detailed foreground structures, such as vessels and skeletons, which are important diagnostic indicators for medical image analysis. To address this, we propose a Sketching-rendering Unconditional Generative Adversarial Network (SkrGAN) that uses a sketch guidance module to generate a high-quality structural sketch and a color render mapping to embed the sketch-based representations. Our approach achieves state-of-the-art results in synthesizing images for various image modalities, including retinal color fundus, X-Ray, Computed Tomography (CT), and Magnetic Resonance Imaging (MRI). We demonstrate that the synthesized images can also improve the performance of medical image segmentation methods as data augmentation. Our SkrGAN is inspired by the human painting process, which involves stroking and color rendering steps.",1
"2D path planning in static environment is a well-known problem and one of the common ways to solve it is to 1) represent the environment as a grid and 2) perform a heuristic search for a path on it. At the same time 2D grid resembles much a digital image, thus an appealing idea comes to being -- to treat the problem as an image generation task and to solve it utilizing the recent advances in deep learning. In this work we make an attempt to apply a generative neural network as a path finder and report preliminary results, convincing enough to claim that this direction of research is worth further exploration.",0
"The problem of 2D path planning in a static environment has a well-known solution involving representing the environment as a grid and performing a heuristic search for a path on it. However, as a 2D grid is similar to a digital image, a new approach has been proposed to use recent advancements in deep learning to solve the problem as an image generation task. In this study, we aim to utilize a generative neural network as a path finder and present initial findings that suggest this research direction holds promise for further exploration.",1
"In this paper, we propose a novel way to interpret text information by extracting visual feature presentation from multiple high-resolution and photo-realistic synthetic images generated by Text-to-image Generative Adversarial Network (GAN) to improve the performance of image labeling. Firstly, we design a stacked Generative Multi-Adversarial Network (GMAN), StackGMAN++, a modified version of the current state-of-the-art Text-to-image GAN, StackGAN++, to generate multiple synthetic images with various prior noises conditioned on a text. And then we extract deep visual features from the generated synthetic images to explore the underlying visual concepts for text. Finally, we combine image-level visual feature, text-level feature and visual features based on synthetic images together to predict labels for images. We conduct experiments on two benchmark datasets and the experimental results clearly demonstrate the efficacy of our proposed approach.",0
"Our paper introduces a new method for interpreting text information to enhance image labeling performance. We achieve this by utilizing multiple high-resolution, photo-realistic synthetic images generated by the Text-to-image Generative Adversarial Network (GAN) to extract visual feature presentations. Our approach involves designing a modified version of the current state-of-the-art Text-to-image GAN called StackGMAN++, which generates multiple synthetic images with different prior noises conditioned on a text. We then extract deep visual features from these images to uncover the visual concepts underlying the text. Finally, we combine image-level visual feature, text-level feature, and synthetic image-based visual features to predict image labels. Our experimental results on two benchmark datasets confirm the effectiveness of our proposed approach.",1
"Batch normalization has been widely used to improve optimization in deep neural networks. While the uncertainty in batch statistics can act as a regularizer, using these dataset statistics specific to the training set impairs generalization in certain tasks. Recently, alternative methods for normalizing feature activations in neural networks have been proposed. Among them, group normalization has been shown to yield similar, in some domains even superior performance to batch normalization. All these methods utilize a learned affine transformation after the normalization operation to increase representational power. Methods used in conditional computation define the parameters of these transformations as learnable functions of conditioning information. In this work, we study whether and where the conditional formulation of group normalization can improve generalization compared to conditional batch normalization. We evaluate performances on the tasks of visual question answering, few-shot learning, and conditional image generation.",0
"Deep neural networks often use batch normalization to optimize their performance. However, while batch statistics can act as a regularizer, using these specific statistics from the training set can hinder generalization in some cases. Recently, alternative methods for normalizing feature activations in neural networks, such as group normalization, have been proposed. Group normalization has even been shown to outperform batch normalization in certain domains. These methods all use a learned affine transformation after normalization to enhance representational power. Conditional computation methods define the parameters of these transformations as learnable functions of conditioning information. This study explores whether and where the conditional formulation of group normalization can improve generalization compared to conditional batch normalization. The study's performance evaluations focus on visual question answering, few-shot learning, and conditional image generation tasks.",1
"Probabilistic models are often trained by maximum likelihood, which corresponds to minimizing a specific f-divergence between the model and data distribution. In light of recent successes in training Generative Adversarial Networks, alternative non-likelihood training criteria have been proposed. Whilst not necessarily statistically efficient, these alternatives may better match user requirements such as sharp image generation. A general variational method for training probabilistic latent variable models using maximum likelihood is well established; however, how to train latent variable models using other f-divergences is comparatively unknown. We discuss a variational approach that, when combined with the recently introduced Spread Divergence, can be applied to train a large class of latent variable models using any f-divergence.",0
"Often, probabilistic models are trained through maximum likelihood, which involves minimizing a specific f-divergence between the model and data distribution. However, alternative non-likelihood training criteria have been suggested in light of the success of training Generative Adversarial Networks. While these alternatives may not be statistically efficient, they may better meet user requirements, such as generating sharp images. A general variational method for training probabilistic latent variable models through maximum likelihood is well-established; however, training latent variable models using other f-divergences is not widely known. We present a variational approach that, when combined with the recently introduced Spread Divergence, can be used to train a wide range of latent variable models using any f-divergence.",1
"Class-conditional extensions of generative adversarial networks (GANs), such as auxiliary classifier GAN (AC-GAN) and conditional GAN (cGAN), have garnered attention owing to their ability to decompose representations into class labels and other factors and to boost the training stability. However, a limitation is that they assume that each class is separable and ignore the relationship between classes even though class overlapping frequently occurs in a real-world scenario when data are collected on the basis of diverse or ambiguous criteria. To overcome this limitation, we address a novel problem called class-distinct and class-mutual image generation, in which the goal is to construct a generator that can capture between-class relationships and generate an image selectively conditioned on the class specificity. To solve this problem without additional supervision, we propose classifier's posterior GAN (CP-GAN), in which we redesign the generator input and the objective function of AC-GAN for class-overlapping data. Precisely, we incorporate the classifier's posterior into the generator input and optimize the generator so that the classifier's posterior of generated data corresponds with that of real data. We demonstrate the effectiveness of CP-GAN using both controlled and real-world class-overlapping data with a model configuration analysis and comparative study. Our code is available at https://github.com/takuhirok/CP-GAN/.",0
"Generative adversarial networks (GANs) have been extended to include class-dependent features, such as the auxiliary classifier GAN (AC-GAN) and conditional GAN (cGAN), which have been praised for their ability to break down representations into class labels and other factors, as well as improve training stability. However, one drawback is that they assume each class is distinct and overlook the connection between classes, despite the frequent occurrence of class overlap in real-world data collection based on diverse or unclear criteria. To overcome this limitation, we present a new problem called class-distinct and class-mutual image generation, which aims to create a generator that can capture between-class relationships and generate an image based on class specificity. We propose a solution, called classifier's posterior GAN (CP-GAN), which modifies the generator input and objective function of AC-GAN for class-overlapping data. Specifically, we incorporate the classifier's posterior into the generator input and optimize the generator to ensure that the classifier's posterior of generated data matches that of real data. We demonstrate the effectiveness of CP-GAN with both controlled and real-world class-overlapping data through model configuration analysis and comparative studies. Our code can be found at https://github.com/takuhirok/CP-GAN/.",1
"Deep learning based task systems normally rely on a large amount of manually labeled training data, which is expensive to obtain and subject to operator variations. Moreover, it does not always hold that the manually labeled data and the unlabeled data are sitting in the same distribution. In this paper, we alleviate these problems by proposing a discriminative consistent domain generation (DCDG) approach to achieve a semi-supervised learning. The discriminative consistent domain is achieved by a double-sided domain adaptation. The double-sided domain adaptation aims to make a fusion of the feature spaces of labeled data and unlabeled data. In this way, we can fit the differences of various distributions between labeled data and unlabeled data. In order to keep the discriminativeness of generated consistent domain for the task learning, we apply an indirect learning for the double-sided domain adaptation. Based on the generated discriminative consistent domain, we can use the unlabeled data to learn the task model along with the labeled data via a consistent image generation. We demonstrate the performance of our proposed DCDG on the late gadolinium enhancement cardiac MRI (LGE-CMRI) images acquired from patients with atrial fibrillation in two clinical centers for the segmentation of the left atrium anatomy (LA) and proximal pulmonary veins (PVs). The experiments show that our semi-supervised approach achieves compelling segmentation results, which can prove the robustness of DCDG for the semi-supervised learning using the unlabeled data along with labeled data acquired from a single center or multicenter studies.",0
"The usual procedure for deep learning based task systems involves using a large amount of manually labeled training data, which can be costly and subject to discrepancies. Additionally, there may not always be a match between the distributions of the labeled and unlabeled data. To address these issues, we present a discriminative consistent domain generation (DCDG) method that allows for semi-supervised learning. The DCDG approach achieves a consistent domain by using double-sided domain adaptation to combine the feature spaces of the labeled and unlabeled data, fitting the differences between their distributions. To maintain the discriminative nature of the consistent domain for task learning, we apply indirect learning for the double-sided domain adaptation. With the discriminative consistent domain generated, we can use both labeled and unlabeled data to learn the task model via consistent image generation. We demonstrate the effectiveness of DCDG on late gadolinium enhancement cardiac MRI (LGE-CMRI) images for left atrium anatomy and proximal pulmonary veins segmentation. Our semi-supervised approach produces promising segmentation results, highlighting the robustness of DCDG for combining labeled and unlabeled data from a single or multiple centers.",1
"We present a new deep learning approach to pose-guided resynthesis of human photographs. At the heart of the new approach is the estimation of the complete body surface texture based on a single photograph. Since the input photograph always observes only a part of the surface, we suggest a new inpainting method that completes the texture of the human body. Rather than working directly with colors of texture elements, the inpainting network estimates an appropriate source location in the input image for each element of the body surface. This correspondence field between the input image and the texture is then further warped into the target image coordinate frame based on the desired pose, effectively establishing the correspondence between the source and the target view even when the pose change is drastic. The final convolutional network then uses the established correspondence and all other available information to synthesize the output image. A fully-convolutional architecture with deformable skip connections guided by the estimated correspondence field is used. We show state-of-the-art result for pose-guided image synthesis. Additionally, we demonstrate the performance of our system for garment transfer and pose-guided face resynthesis.",0
"A novel deep learning method for resynthesizing human photographs based on pose guidance is introduced in this study. The estimation of the complete body surface texture from a single photograph is the foundation of this approach. Since the input image only captures a portion of the surface, a new inpainting technique is suggested to complete the texture of the human body. Rather than dealing with the colors of the texture elements directly, the inpainting network identifies an appropriate source location in the input image for each body surface element. This correspondence field is then warped into the target image coordinate frame using the desired pose, establishing the correspondence between the source and target views even when the pose change is significant. Finally, a fully-convolutional architecture with deformable skip connections guided by the estimated correspondence field is employed to synthesize the output image. Our approach produces state-of-the-art results for pose-guided image synthesis and is also effective for garment transfer and pose-guided face resynthesis.",1
"In this work, we address the task of natural image generation guided by a conditioning input. We introduce a new architecture called conditional invertible neural network (cINN). The cINN combines the purely generative INN model with an unconstrained feed-forward network, which efficiently preprocesses the conditioning input into useful features. All parameters of the cINN are jointly optimized with a stable, maximum likelihood-based training procedure. By construction, the cINN does not experience mode collapse and generates diverse samples, in contrast to e.g. cGANs. At the same time our model produces sharp images since no reconstruction loss is required, in contrast to e.g. VAEs. We demonstrate these properties for the tasks of MNIST digit generation and image colorization. Furthermore, we take advantage of our bi-directional cINN architecture to explore and manipulate emergent properties of the latent space, such as changing the image style in an intuitive way.",0
"The focus of this study is on generating natural images through a conditioning input. A new model is introduced, known as the conditional invertible neural network (cINN), which combines an unconstrained feed-forward network with the INN model to provide useful features. The cINN is optimized jointly with a stable, maximum-likelihood training procedure and is not susceptible to mode collapse, unlike cGANs. Additionally, the cINN produces sharp images without the need for reconstruction loss, unlike VAEs. The cINN is used to generate MNIST digits and colorize images, and its bi-directional architecture is utilized to manipulate the properties of the latent space, such as changing image style in a straightforward manner.",1
"Deep Neural Networks (DNNs) have begun to thrive in the field of automation systems, owing to the recent advancements in standardising various aspects such as architecture, optimization techniques, and regularization. In this paper, we take a step towards a better understanding of Spectral Normalization (SN) and its potential for standardizing regularization of a wider range of Deep Learning models, following an empirical approach. We conduct several experiments to study their training dynamics, in comparison with the ubiquitous Batch Normalization (BN) and show that SN increases the gradient sparsity and controls the gradient variance. Furthermore, we show that SN suffers from a phenomenon, we call the mean-drift effect, which mitigates its performance. We, then, propose a weight reparameterization called as the Mean Spectral Normalization (MSN) to resolve the mean drift, thereby significantly improving the network's performance. Our model performs ~16% faster as compared to BN in practice, and has fewer trainable parameters. We also show the performance of our MSN for small, medium, and large CNNs - 3-layer CNN, VGG7 and DenseNet-BC, respectively - and unsupervised image generation tasks using Generative Adversarial Networks (GANs) to evaluate its applicability for a broad range of embedded automation tasks.",0
"Recent advancements in standardizing various aspects of Deep Neural Networks (DNNs), such as architecture, optimization techniques, and regularization, have led to their increasing use in automation systems. This paper focuses on understanding the potential of Spectral Normalization (SN) to standardize regularization of a wider range of Deep Learning models. Through empirical experiments, we compare SN with Batch Normalization (BN) and demonstrate that SN improves gradient sparsity and controls gradient variance. However, SN is also susceptible to the mean-drift effect, which affects its performance. To address this, we propose Mean Spectral Normalization (MSN), which significantly improves network performance. Our model outperforms BN by ~16% and has fewer trainable parameters. We evaluate the performance of MSN on small, medium, and large CNNs, as well as unsupervised image generation tasks using Generative Adversarial Networks (GANs), demonstrating its applicability for a broad range of embedded automation tasks.",1
"Radiogenomic map linking image features and gene expression profiles is useful for noninvasively identifying molecular properties of a particular type of disease. Conventionally, such map is produced in three separate steps: 1) gene-clustering to ""metagenes"", 2) image feature extraction, and 3) statistical correlation between metagenes and image features. Each step is independently performed and relies on arbitrary measurements. In this work, we investigate the potential of an end-to-end method fusing gene data with image features to generate synthetic image and learn radiogenomic map simultaneously. To achieve this goal, we develop a generative adversarial network (GAN) conditioned on both background images and gene expression profiles, synthesizing the corresponding image. Image and gene features are fused at different scales to ensure the realism and quality of the synthesized image. We tested our method on non-small cell lung cancer (NSCLC) dataset. Results demonstrate that the proposed method produces realistic synthetic images, and provides a promising way to find gene-image relationship in a holistic end-to-end manner.",0
"A radiogenomic map is a valuable tool for identifying the molecular properties of a specific disease through image features and gene expression profiles. Conventionally, this map is created in three separate steps: gene clustering, image feature extraction, and statistical correlation. However, each step is subjective and dependent on arbitrary measurements. In this study, we explore the potential of an end-to-end method that combines gene data with image features to create synthetic images and learn the radiogenomic map at the same time. To achieve this, we use a generative adversarial network (GAN) that is conditioned on both background images and gene expression profiles, resulting in a realistic synthetic image. We fuse image and gene features at various levels to ensure the quality of the synthesized image. Our proposed method is tested on a non-small cell lung cancer (NSCLC) dataset, and the results demonstrate that it produces realistic synthetic images and offers a promising approach to discovering gene-image relationships in a comprehensive end-to-end manner.",1
"Deep generative networks such as GANs and normalizing flows flourish in the context of high-dimensional tasks such as image generation. However, so far exact modeling or extrapolation of distributional properties such as the tail asymptotics generated by a generative network is not available. In this paper, we address this issue for the first time in the deep learning literature by making two novel contributions. First, we derive upper bounds for the tails that can be expressed by a generative network and demonstrate Lp-space related properties. There we show specifically that in various situations an optimal generative network does not exist. Second, we introduce and propose copula and marginal generative flows (CM flows) which allow for an exact modeling of the tail and any prior assumption on the CDF up to an approximation of the uniform distribution. Our numerical results support the use of CM flows.",0
"Generative networks like GANs and normalizing flows excel in high-dimensional tasks like creating images. However, they currently cannot accurately model or extrapolate distributional properties such as tail asymptotics. This paper introduces two novel contributions to the deep learning literature. Firstly, upper bounds for tails expressed by a generative network are derived, demonstrating Lp-space related properties. It is shown that an optimal generative network does not exist in certain situations. Secondly, copula and marginal generative flows (CM flows) are introduced, allowing for exact modeling of the tail and any prior assumption on the CDF, up to an approximation of the uniform distribution. Numerical results support the use of CM flows.",1
"Deep image generation is becoming a tool to enhance artists and designers creativity potential. In this paper, we aim at making the generation process more structured and easier to interact with. Inspired by vector graphics systems, we propose a new deep image reconstruction paradigm where the outputs are composed from simple layers, defined by their color and a vector transparency mask. This presents a number of advantages compared to the commonly used convolutional network architectures. In particular, our layered decomposition allows simple user interaction, for example to update a given mask, or change the color of a selected layer. From a compact code, our architecture also generates vector images with a virtually infinite resolution, the color at each point in an image being a parametric function of its coordinates. We validate the efficiency of our approach by comparing reconstructions with state-of-the-art baselines given similar memory resources on CelebA and ImageNet datasets. Most importantly, we demonstrate several applications of our new image representation obtained in an unsupervised manner, including editing, vectorization and image search.",0
"The potential for artists and designers to enhance their creativity with deep image generation is on the rise. This paper aims to streamline the generation process and make it more user-friendly. Drawing inspiration from vector graphics systems, a new deep image reconstruction paradigm is proposed. This paradigm is composed of simple layers defined by color and a vector transparency mask, offering numerous advantages over commonly used convolutional network architectures. The layered decomposition allows for easy user interaction, such as updating masks or changing layer colors. Additionally, our architecture generates vector images with virtually infinite resolution, with color at each point being a parametric function of its coordinates. We compared the efficiency of our approach to state-of-the-art baselines using CelebA and ImageNet datasets, with promising results. Moreover, we demonstrate several unsupervised applications of our new image representation, including editing, vectorization, and image search.",1
"Computer vision (CV) is the process of using machines to understand and analyze imagery, which is an integral branch of artificial intelligence. Among various research areas of CV, fine-grained image analysis (FGIA) is a longstanding and fundamental problem, and has become ubiquitous in diverse real-world applications. The task of FGIA targets analyzing visual objects from subordinate categories, \eg, species of birds or models of cars. The small inter-class variations and the large intra-class variations caused by the fine-grained nature makes it a challenging problem. During the booming of deep learning, recent years have witnessed remarkable progress of FGIA using deep learning techniques. In this paper, we aim to give a survey on recent advances of deep learning based FGIA techniques in a systematic way. Specifically, we organize the existing studies of FGIA techniques into three major categories: fine-grained image recognition, fine-grained image retrieval and fine-grained image generation. In addition, we also cover some other important issues of FGIA, such as publicly available benchmark datasets and its related domain specific applications. Finally, we conclude this survey by highlighting several directions and open problems which need be further explored by the community in the future.",0
"The process of using machines to analyze imagery, known as computer vision (CV), is a crucial aspect of artificial intelligence. One area of CV research, fine-grained image analysis (FGIA), has become ubiquitous in various real-world applications and presents a longstanding and fundamental problem. FGIA involves analyzing subordinate categories of visual objects, such as bird species or car models, which can be challenging due to small inter-class variations and large intra-class variations caused by their fine-grained nature. Recently, deep learning techniques have led to remarkable progress in FGIA. This paper provides a systematic survey of deep learning-based FGIA techniques, organizing them into three categories: fine-grained image recognition, retrieval, and generation. Additionally, important issues such as benchmark datasets and domain-specific applications are covered, and directions and open problems for future exploration are highlighted.",1
"After deep generative models were successfully applied to image generation tasks, learning disentangled latent variables of data has become a crucial part of deep generative model research. Many models have been proposed to learn an interpretable and factorized representation of latent variable by modifying their objective function or model architecture. To disentangle the latent variable, some models show lower quality of reconstructed images and others increase the model complexity which is hard to train. In this paper, we propose a simple disentangling method based on a traditional whitening process. The proposed method is applied to the latent variables of variational auto-encoder (VAE), although it can be applied to any generative models with latent variables. In experiment, we apply the proposed method to simple VAE models and experiment results confirm that our method finds more interpretable factors from the latent space while keeping the reconstruction error the same as the conventional VAE's error.",0
"The importance of learning disentangled latent variables in deep generative model research has increased after the successful application of these models in image generation tasks. Several models have been suggested to achieve an interpretable and factorized representation of latent variables by altering their objective function or model architecture. However, some models compromise on image quality or increase model complexity, making them difficult to train. This paper proposes a simple disentangling method based on a conventional whitening process, which can be applied to any generative models with latent variables. The proposed method is tested on variational auto-encoder (VAE) models, and the experiment results show that it identifies more interpretable factors from the latent space without compromising on image reconstruction quality.",1
"Realistic image synthesis is to generate an image that is perceptually indistinguishable from an actual image. Generating realistic looking images with large variations (e.g., large spatial deformations and large pose change), however, is very challenging. Handing large variations as well as preserving appearance needs to be taken into account in the realistic looking image generation. In this paper, we propose a novel realistic looking image synthesis method, especially in large change demands. To do that, we devise generative guiding blocks. The proposed generative guiding block includes realistic appearance preserving discriminator and naturalistic variation transforming discriminator. By taking the proposed generative guiding blocks into generative model, the latent features at the layer of generative model are enhanced to synthesize both realistic looking- and target variation- image. With qualitative and quantitative evaluation in experiments, we demonstrated the effectiveness of the proposed generative guiding blocks, compared to the state-of-the-arts.",0
"Generating an image that looks like a real one is called realistic image synthesis. However, creating realistic images with significant variations, such as spatial deformations and changes in pose, is quite difficult. To generate realistic images with large variations, it is necessary to consider both handling these variations and maintaining appearance. In this research, we suggest a new method for generating realistic images, especially for those with significant changes. We introduce generative guiding blocks to achieve this. These blocks include a discriminator that preserves a realistic appearance and a discriminator that transforms naturalistic variations. By incorporating these blocks into the generative model, the latent features at the layer of the generative model are improved to generate both realistic-looking images and images with target variations. We conducted qualitative and quantitative evaluations in experiments, which showed that the proposed generative guiding blocks are more effective than the state-of-the-art methods.",1
"Generative Adversarial Networks (GANs) have obtained extraordinary success in the generation of realistic images, a domain where a lower pixel-level accuracy is acceptable. We study the problem, not yet tackled in the literature, of generating semantic images starting from a prior distribution. Intuitively this problem can be approached using standard methods and architectures. However, a better-suited approach is needed to avoid generating blurry, hallucinated and thus unusable images since tasks like semantic segmentation require pixel-level exactness. In this work, we present a novel architecture for learning to generate pixel-level accurate semantic images, namely Semantic Generative Adversarial Networks (SemGANs). The experimental evaluation shows that our architecture outperforms standard ones from both a quantitative and a qualitative point of view in many semantic image generation tasks.",0
"The success of Generative Adversarial Networks (GANs) in creating realistic images, where pixel-level precision is not necessary, has been remarkable. Our research explores a new challenge that has yet to be explored in literature - generating semantic images from a prior distribution. Although this task could be tackled using conventional techniques and structures, our approach must be more refined to prevent the creation of blurry or unrealistic images that are of no use for semantic segmentation that requires precise pixels. Our novel architecture, Semantic Generative Adversarial Networks (SemGANs), has been developed to generate semantic images with pixel-level accuracy. Our experimental evaluation demonstrates that our architecture surpasses standard methods in many semantic image generation tasks, both quantitatively and qualitatively.",1
"Discovering and exploiting the causality in deep neural networks (DNNs) are crucial challenges for understanding and reasoning causal effects (CE) on an explainable visual model. ""Intervention"" has been widely used for recognizing a causal relation ontologically. In this paper, we propose a causal inference framework for visual reasoning via do-calculus. To study the intervention effects on pixel-level features for causal reasoning, we introduce pixel-wise masking and adversarial perturbation. In our framework, CE is calculated using features in a latent space and perturbed prediction from a DNN-based model. We further provide the first look into the characteristics of discovered CE of adversarially perturbed images generated by gradient-based methods \footnote{~~https://github.com/jjaacckkyy63/Causal-Intervention-AE-wAdvImg}. Experimental results show that CE is a competitive and robust index for understanding DNNs when compared with conventional methods such as class-activation mappings (CAMs) on the Chest X-Ray-14 dataset for human-interpretable feature(s) (e.g., symptom) reasoning. Moreover, CE holds promises for detecting adversarial examples as it possesses distinct characteristics in the presence of adversarial perturbations.",0
"The identification and utilization of causality within deep neural networks (DNNs) pose significant challenges in comprehending and reasoning the causal effects (CE) of explainable visual models. The concept of ""intervention"" has been extensively utilized to establish a causal relationship ontologically. This paper presents a causal inference framework for visual reasoning that employs do-calculus. To facilitate causal reasoning and intervention effects on pixel-level features, the framework employs pixel-wise masking and adversarial perturbation. CE is determined using latent space features and perturbed predictions from a DNN-based model. The study further examines the characteristics of CE discovered from adversarially perturbed images generated using gradient-based methods. Experimental outcomes reveal that CE is a robust and competitive indicator for comprehending DNNs compared to conventional methods like class-activation mappings (CAMs) in human-interpretable feature reasoning for the Chest X-Ray-14 dataset (e.g., symptom). Additionally, CE offers potential in recognizing adversarial examples as it displays distinct characteristics in the presence of adversarial perturbations.",1
"Recent progress in Generative Adversarial Networks (GANs) has shown promising signs of improving GAN training via architectural change. Despite some early success, at present the design of GAN architectures requires human expertise, laborious trial-and-error testings, and often draws inspiration from its image classification counterpart. In the current paper, we present the first neural architecture search algorithm, automated neural architecture search for deep generative models, or AGAN for abbreviation, that is specifically suited for GAN training. For unsupervised image generation tasks on CIFAR-10, our algorithm finds architecture that outperforms state-of-the-art models under same regularization techniques. For supervised tasks, the automatically searched architectures also achieve highly competitive performance, outperforming best human-invented architectures at resolution $32\times32$. Moreover, we empirically demonstrate that the modules learned by AGAN are transferable to other image generation tasks such as STL-10.",0
"GANs have made significant progress in recent years, indicating that the training of GANs can be improved by modifying their architecture. However, designing GAN architectures currently requires human expertise, extensive trial-and-error testing, and often relies on image classification models. This paper introduces AGAN, the first neural architecture search algorithm specifically designed for GAN training. Our algorithm outperforms state-of-the-art models for unsupervised image generation tasks on CIFAR-10, and achieves competitive performance for supervised tasks, surpassing even the best human-designed architectures at a resolution of 32x32. We also demonstrate that the modules learned by AGAN can be applied to other image generation tasks, such as STL-10.",1
"The log-ratio (LR) operator has been widely employed to generate the difference image for synthetic aperture radar (SAR) image change detection. However, the difference image generated by this pixel-wise operator can be subject to SAR images speckle and unavoidable registration errors between bitemporal SAR images. In this letter, we proposed a spatial metric learning method to obtain a difference image more robust to the speckle by learning a metric from a set of constraint pairs. In the proposed method, spatial context is considered in constructing constraint pairs, each of which consists of patches in the same location of bitemporal SAR images. Then, a semi-definite positive metric matrix $\bf M$ can be obtained by the optimization with the max-margin criterion. Finally, we verify our proposed method on four challenging datasets of bitemporal SAR images. Experimental results demonstrate that the difference map obtained by our proposed method outperforms than other state-of-art methods.",0
"The log-ratio (LR) operator is commonly used to generate the difference image for synthetic aperture radar (SAR) image change detection. However, this method can be affected by SAR images speckle and registration errors between bitemporal SAR images. To overcome this limitation, we propose a spatial metric learning approach to obtain a more robust difference image. Our method involves learning a metric from a set of constraint pairs, which are constructed by considering the spatial context of patches in the same location of bitemporal SAR images. By optimizing with the max-margin criterion, we obtain a semi-definite positive metric matrix $\bf M$. We evaluate our method on four challenging datasets of bitemporal SAR images and demonstrate that it outperforms other state-of-the-art methods.",1
"The unsupervised training of GANs and VAEs has enabled them to generate realistic images mimicking real-world distributions and perform image-based unsupervised clustering or semi-supervised classification. Combining the power of these two generative models, we introduce Multi-Adversarial Variational autoEncoder Networks (MAVENs), a novel network architecture that incorporates an ensemble of discriminators in a VAE-GAN network, with simultaneous adversarial learning and variational inference. We apply MAVENs to the generation of synthetic images and propose a new distribution measure to quantify the quality of the generated images. Our experimental results using datasets from the computer vision and medical imaging domains---Street View House Numbers, CIFAR-10, and Chest X-Ray datasets---demonstrate competitive performance against state-of-the-art semi-supervised models both in image generation and classification tasks.",0
"GANs and VAEs have been trained without supervision, allowing them to create lifelike images that imitate real-world distributions and perform unsupervised clustering or semi-supervised classification based on images. Our new network architecture, Multi-Adversarial Variational autoEncoder Networks (MAVENs), combines these two powerful generative models by incorporating an ensemble of discriminators into a VAE-GAN network. This allows for simultaneous adversarial learning and variational inference. We utilized MAVENs to produce synthetic images and proposed a novel distribution measure to assess the quality of the generated images. Our experimental results, including datasets from computer vision and medical imaging domains such as Street View House Numbers, CIFAR-10, and Chest X-Ray, demonstrate MAVENs' competitive performance compared to state-of-the-art semi-supervised models in both image generation and classification tasks.",1
"In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore, recent work has shown that generator conditioning affects GAN performance. Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN achieves the state-of-the-art results, boosting the best published Inception score from 36.8 to 52.52 and reducing Frechet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.",0
"The Self-Attention Generative Adversarial Network (SAGAN) proposed in this paper enables attention-driven, long-range dependency modeling for image generation tasks. While traditional convolutional GANs generate high-resolution details based on spatially local points in lower-resolution feature maps, SAGAN allows for generation of details using cues from all feature locations. Additionally, the discriminator can ensure consistency between highly detailed features located in distant parts of the image. Recent research has demonstrated that generator conditioning has an impact on GAN performance, and we apply spectral normalization to the GAN generator to improve training dynamics. SAGAN achieves state-of-the-art results, increasing the best published Inception score from 36.8 to 52.52 and reducing Frechet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers reveals that the generator utilizes object-shaped neighborhoods rather than local regions of fixed shape.",1
"Our aim was to enhance visual quality and quantitative accuracy of dynamic positron emission tomography (PET)uptake images by improved image reconstruction, using sophisticated sparse penalty models that incorporate both 2D spatial+1D temporal (3DT) information. We developed two new 3DT PET reconstruction algorithms, incorporating different temporal and spatial penalties based on discrete cosine transform (DCT)w/ patches, and tensor nuclear norm (TNN) w/ patches, and compared to frame-by-frame methods; conventional 2D ordered subsets expectation maximization (OSEM) w/ post-filtering and 2D-DCT and 2D-TNN. A 3DT brain phantom with kinetic uptake (2-tissue model), and a moving 3DT cardiac/lung phantom was simulated and reconstructed. For the cardiac/lung phantom, an additional cardiac gated 2D-OSEM set was reconstructed. The structural similarity index (SSIM) and relative root mean squared error (rRMSE) relative ground truth was investigated. The image derived left ventricular (LV) volume for the cardiac/lung images was found by region growing and parametric images of the brain phantom were calculated. For the cardiac/lung phantom, 3DT-TNN yielded optimal images, and 3DT-DCT was best for the brain phantom. The optimal LV volume from the 3DT-TNN images was on average 11 and 55 percentage points closer to the true value compared to cardiac gated 2D-OSEM and 2D-OSEM respectively. Compared to 2D-OSEM, parametric images based on 3DT-DCT images generally had smaller bias and higher SSIM. Our novel methods that incorporate both 2D spatial and 1D temporal penalties produced dynamic PET images of higher quality than conventional 2D methods, w/o need for post-filtering. Breathing and cardiac motion were simultaneously captured w/o need for respiratory or cardiac gating. LV volumes were better recovered, and subsequently fitted parametric images were generally less biased and of higher quality.",0
"Our objective was to improve the visual quality and quantitative accuracy of dynamic positron emission tomography (PET) uptake images through enhanced image reconstruction. We accomplished this by utilizing advanced sparse penalty models that include both 2D spatial and 1D temporal (3DT) information. To achieve this, we created two new 3DT PET reconstruction algorithms that integrate different temporal and spatial penalties based on discrete cosine transform (DCT) with patches, and tensor nuclear norm (TNN) with patches. We compared these methods to frame-by-frame techniques such as conventional 2D ordered subsets expectation maximization (OSEM) with post-filtering, 2D-DCT, and 2D-TNN. We simulated and reconstructed a 3DT brain phantom with kinetic uptake (2-tissue model) and a moving 3DT cardiac/lung phantom. An additional cardiac gated 2D-OSEM set was reconstructed for the cardiac/lung phantom. We evaluated the structural similarity index (SSIM) and relative root mean squared error (rRMSE) relative to the ground truth. We determined the left ventricular (LV) volume for the cardiac/lung images using region growing, and calculated parametric images of the brain phantom. Our findings showed that 3DT-TNN produced optimal images for the cardiac/lung phantom, while 3DT-DCT yielded the best results for the brain phantom. The LV volume obtained from the 3DT-TNN images was closer to the true value compared to cardiac gated 2D-OSEM and 2D-OSEM, with an average difference of 11 and 55 percentage points, respectively. Parametric images based on 3DT-DCT images had less bias and higher SSIM than those based on 2D-OSEM images. Our innovative techniques that incorporate both 2D spatial and 1D temporal penalties led to dynamic PET images with higher quality than conventional 2D methods, without the need for post-filtering. Breathing and cardiac motion were simultaneously captured without requiring respiratory or cardiac gating. LV volumes were more accurately recovered, and the subsequently fitted parametric images were generally less biased and of higher quality.",1
"In this paper, we introduce the problem of jointly learning feed-forward neural networks across a set of relevant but diverse datasets. Compared to learning a separate network from each dataset in isolation, joint learning enables us to extract correlated information across multiple datasets to significantly improve the quality of learned networks. We formulate this problem as joint learning of multiple copies of the same network architecture and enforce the network weights to be shared across these networks. Instead of hand-encoding the shared network layers, we solve an optimization problem to automatically determine how layers should be shared between each pair of datasets. Experimental results show that our approach outperforms baselines without joint learning and those using pretraining-and-fine-tuning. We show the effectiveness of our approach on three tasks: image classification, learning auto-encoders, and image generation.",0
"In this paper, we present the idea of simultaneously training feed-forward neural networks across various datasets that are related but distinct. Joint learning, as opposed to learning separate networks for each dataset, allows us to extract interrelated information from multiple datasets, leading to a more advanced network. Our strategy involves training multiple identical network architectures and enforcing shared weight values across them. Rather than manually coding shared network layers, we utilize an optimization process to determine how layers should be shared between each dataset pairing. Experimental outcomes reveal that our approach surpasses baselines that do not use joint learning or pretraining-and-fine-tuning. We demonstrate the effectiveness of our method on three tasks: image classification, auto-encoder learning, and image generation.",1
"Deep generative models (DGMs) have shown promise in image generation. However, most of the existing work learn the model by simply optimizing a divergence between the marginal distributions of the model and the data, and often fail to capture the rich structures and relations in multi-object images. Human knowledge is a critical element to the success of DGMs to infer these structures. In this paper, we propose the amortized structural regularization (ASR) framework, which adopts the posterior regularization (PR) to embed human knowledge into DGMs via a set of structural constraints. We derive a lower bound of the regularized log-likelihood, which can be jointly optimized with respect to the generative model and recognition model efficiently. Empirical results show that ASR significantly outperforms the DGM baselines in terms of inference accuracy and sample quality.",0
"While Deep generative models (DGMs) have shown promise in generating images, the existing models fail to capture the complex structures and relations in multi-object images. This is because they simply optimize a divergence between the marginal distributions of the model and the data. Human knowledge is crucial to the success of DGMs in inferring these structures. In this paper, we propose the amortized structural regularization (ASR) framework that incorporates human knowledge into DGMs through a set of structural constraints using posterior regularization (PR). We derive a lower bound of the regularized log-likelihood, which can be efficiently optimized jointly with the generative model and recognition model. Our empirical results show that ASR outperforms DGM baselines in terms of both inference accuracy and sample quality.",1
"One of the main motivations for training high quality image generative models is their potential use as tools for image manipulation. Recently, generative adversarial networks (GANs) have been able to generate images of remarkable quality. Unfortunately, adversarially-trained unconditional generator networks have not been successful as image priors. One of the main requirements for a network to act as a generative image prior, is being able to generate every possible image from the target distribution. Adversarial learning often experiences mode-collapse, which manifests in generators that cannot generate some modes of the target distribution. Another requirement often not satisfied is invertibility i.e. having an efficient way of finding a valid input latent code given a required output image. In this work, we show that differently from earlier GANs, the very recently proposed style-generators are quite easy to invert. We use this important observation to propose style generators as general purpose image priors. We show that style generators outperform other GANs as well as Deep Image Prior as priors for image enhancement tasks. The latent space spanned by style-generators satisfies linear identity-pose relations. The latent space linearity, combined with invertibility, allows us to animate still facial images without supervision. Extensive experiments are performed to support the main contributions of this paper.",0
"The primary objective of training high-quality image generative models is their potential for use in image manipulation. The recent advancements in generative adversarial networks (GANs) have resulted in the generation of images of remarkable quality. However, adversarially-trained unconditional generator networks have not been successful as image priors. For a network to function as a generative image prior, it must have the ability to generate every possible image from the target distribution. Unfortunately, adversarial learning often experiences mode-collapse, resulting in generators that cannot generate some modes of the target distribution. Additionally, invertibility is another requirement that is often not met, i.e. having an efficient method of finding a valid input latent code given a required output image. In this study, we demonstrate that the recently proposed style-generators are easy to invert, unlike earlier GANs. We utilize this significant discovery to suggest style generators as general purpose image priors, which outperform other GANs and Deep Image Prior as priors for image enhancement tasks. The latent space spanned by style-generators fulfills linear identity-pose relations, which, combined with invertibility, enables us to animate still facial images without supervision. The main contributions of this paper are supported by extensive experiments.",1
"This paper studies the task of full generative modelling of realistic images of humans, guided only by coarse sketch of the pose, while providing control over the specific instance or type of outfit worn by the user. This is a difficult problem because input and output domain are very different and direct image-to-image translation becomes infeasible. We propose an end-to-end trainable network under the generative adversarial framework, that provides detailed control over the final appearance while not requiring paired training data and hence allows us to forgo the challenging problem of fitting 3D poses to 2D images. The model allows to generate novel samples conditioned on either an image taken from the target domain or a class label indicating the style of clothing (e.g., t-shirt). We thoroughly evaluate the architecture and the contributions of the individual components experimentally. Finally, we show in a large scale perceptual study that our approach can generate realistic looking images and that participants struggle in detecting fake images versus real samples, especially if faces are blurred.",0
"The aim of this research is to create realistic images of humans using a rough outline of their pose while allowing for control over the type of clothing worn. This task is challenging as the input and output domains are vastly different, making direct image-to-image translation impractical. To overcome this, we propose an end-to-end trainable network within the generative adversarial framework that yields detailed control over the final appearance without requiring paired training data. This eliminates the need to fit 3D poses to 2D images. Our model can generate new samples based on an image from the target domain or a class label indicating the clothing style. We conduct a thorough experimental evaluation of the architecture and its components. We also present a large-scale perceptual study demonstrating that our approach produces images that are difficult to distinguish from real ones, especially if faces are obscured.",1
"Style transfer is a problem of rendering image with some content in the style of another image, for example a family photo in the style of a painting of some famous artist. The drawback of classical style transfer algorithm is that it imposes style uniformly on all parts of the content image, which perturbs central objects on the content image, such as faces or text, and makes them unrecognizable. This work proposes a novel style transfer algorithm which automatically detects central objects on the content image, generates spatial importance mask and imposes style non-uniformly: central objects are stylized less to preserve their recognizability and other parts of the image are stylized as usual to preserve the style. Three methods of automatic central object detection are proposed and evaluated qualitatively and via a user evaluation study. Both comparisons demonstrate higher quality of stylization compared to the classical style transfer method.",0
"The issue of style transfer involves transforming an image's content into the style of another image, such as transforming a family photo into the style of a famous artist's painting. However, the conventional style transfer algorithm applies the style uniformly across the entire content image, causing central objects like faces or text to become unrecognizable. This study introduces a new style transfer algorithm that automatically detects central objects, creates an importance mask, and applies the style non-uniformly. The central objects are stylized less to maintain their recognizability, while the rest of the image is stylized as usual to preserve the style. The paper presents three methods of automatic central object detection and evaluates them through qualitative analysis and user evaluations, both of which demonstrate superior stylization quality compared to the classical style transfer method.",1
"Style transfer is a field with growing interest and use cases in deep learning. Recent work has shown Generative Adversarial Networks(GANs) can be used to create realistic images of virtually stained slide images in digital pathology with clinically validated interpretability. Digital pathology images are typically of extremely high resolution, making tilewise analysis necessary for deep learning applications. It has been shown that image generators with instance normalization can cause a tiling artifact when a large image is reconstructed from the tilewise analysis. We introduce a novel perceptual embedding consistency loss significantly reducing the tiling artifact created in the reconstructed whole slide image (WSI). We validate our results by comparing virtually stained slide images with consecutive real stained tissue slide images. We also demonstrate that our model is more robust to contrast, color and brightness perturbations by running comparative sensitivity analysis tests.",0
"The area of style transfer is gaining popularity and practical applications in the realm of deep learning. Recent studies have revealed the potential of Generative Adversarial Networks (GANs) to produce realistic images of stained slide images in digital pathology that are validated for clinical interpretation. Due to the extremely high resolution of digital pathology images, a tilewise analysis is necessary for deep learning purposes. However, it has been discovered that image generators with instance normalization can cause a tiling artifact when reconstructing a large image from tilewise analysis. To address this issue, we have developed a unique loss function that enhances perceptual embedding consistency and greatly reduces the tiling artifact in the reconstructed whole slide image (WSI). Our study has been validated by comparing virtually stained slide images with actual stained tissue slide images. Additionally, we have demonstrated that our model is more resilient to color, contrast, and brightness perturbations through comparative sensitivity analysis tests.",1
"We explore the use of Vector Quantized Variational AutoEncoder (VQ-VAE) models for large scale image generation. To this end, we scale and enhance the autoregressive priors used in VQ-VAE to generate synthetic samples of much higher coherence and fidelity than possible before. We use simple feed-forward encoder and decoder networks, making our model an attractive candidate for applications where the encoding and/or decoding speed is critical. Additionally, VQ-VAE requires sampling an autoregressive model only in the compressed latent space, which is an order of magnitude faster than sampling in the pixel space, especially for large images. We demonstrate that a multi-scale hierarchical organization of VQ-VAE, augmented with powerful priors over the latent codes, is able to generate samples with quality that rivals that of state of the art Generative Adversarial Networks on multifaceted datasets such as ImageNet, while not suffering from GAN's known shortcomings such as mode collapse and lack of diversity.",0
"Our study investigates the application of Vector Quantized Variational AutoEncoder (VQ-VAE) models in generating large scale images. We enhance the autoregressive priors used in VQ-VAE to generate synthetic samples with higher coherence and fidelity than previously achievable. Our model employs simple feed-forward encoder and decoder networks, making it an appealing option for applications that prioritize encoding and decoding speed. Moreover, VQ-VAE only requires sampling an autoregressive model in the compressed latent space, which is significantly faster than sampling in the pixel space, particularly for large images. By implementing a multi-scale hierarchical organization of VQ-VAE, coupled with robust priors over the latent codes, we demonstrate the model's ability to generate samples of comparable quality to state-of-the-art Generative Adversarial Networks on complex datasets such as ImageNet, while avoiding GAN's limitations like mode collapse and lack of diversity.",1
"Image translation across different domains has attracted much attention in both machine learning and computer vision communities. Taking the translation from source domain $\mathcal{D}_s$ to target domain $\mathcal{D}_t$ as an example, existing algorithms mainly rely on two kinds of loss for training: One is the discrimination loss, which is used to differentiate images generated by the models and natural images; the other is the reconstruction loss, which measures the difference between an original image and the reconstructed version through $\mathcal{D}_s\to\mathcal{D}_t\to\mathcal{D}_s$ translation. In this work, we introduce a new kind of loss, multi-path consistency loss, which evaluates the differences between direct translation $\mathcal{D}_s\to\mathcal{D}_t$ and indirect translation $\mathcal{D}_s\to\mathcal{D}_a\to\mathcal{D}_t$ with $\mathcal{D}_a$ as an auxiliary domain, to regularize training. For multi-domain translation (at least, three) which focuses on building translation models between any two domains, at each training iteration, we randomly select three domains, set them respectively as the source, auxiliary and target domains, build the multi-path consistency loss and optimize the network. For two-domain translation, we need to introduce an additional auxiliary domain and construct the multi-path consistency loss. We conduct various experiments to demonstrate the effectiveness of our proposed methods, including face-to-face translation, paint-to-photo translation, and de-raining/de-noising translation.",0
"The translation of images across different domains has garnered significant attention in the fields of machine learning and computer vision. To illustrate, current algorithms rely on two types of loss during training when translating from the source domain $\mathcal{D}_s$ to the target domain $\mathcal{D}_t. The first is the discrimination loss, which distinguishes between images generated by models and those that are natural. The second is the reconstruction loss, which measures the difference between an original image and its reconstructed version through $\mathcal{D}_s\to\mathcal{D}_t\to\mathcal{D}_s$ translation. In this study, we propose a novel loss function called multi-path consistency loss. This loss measures the discrepancies between direct translation $\mathcal{D}_s\to\mathcal{D}_t$ and indirect translation $\mathcal{D}_s\to\mathcal{D}_a\to\mathcal{D}_t$ using an auxiliary domain $\mathcal{D}_a$ to regulate training. For multi-domain translation, we randomly select three domains as the source, auxiliary, and target domains during each iteration and optimize the network using the multi-path consistency loss. For two-domain translation, we add an additional auxiliary domain and construct the multi-path consistency loss. We conduct several experiments, including face-to-face translation, paint-to-photo translation, and de-raining/de-noising translation, to demonstrate the effectiveness of our proposed methods.",1
"Visual attention mechanisms have proven to be integrally important constituent components of many modern deep neural architectures. They provide an efficient and effective way to utilize visual information selectively, which has shown to be especially valuable in multi-modal learning tasks. However, all prior attention frameworks lack the ability to explicitly model structural dependencies among attention variables, making it difficult to predict consistent attention masks. In this paper we develop a novel structured spatial attention mechanism which is end-to-end trainable and can be integrated with any feed-forward convolutional neural network. This proposed AttentionRNN layer explicitly enforces structure over the spatial attention variables by sequentially predicting attention values in the spatial mask in a bi-directional raster-scan and inverse raster-scan order. As a result, each attention value depends not only on local image or contextual information, but also on the previously predicted attention values. Our experiments show consistent quantitative and qualitative improvements on a variety of recognition tasks and datasets; including image categorization, question answering and image generation.",0
"Many modern deep neural architectures rely on visual attention mechanisms as essential components. These mechanisms allow for selective utilization of visual information, which proves particularly useful in multi-modal learning tasks. Unfortunately, previous attention frameworks have not been able to model structural dependencies among attention variables, making it difficult to predict consistent attention masks. Our paper presents a novel structured spatial attention mechanism that can be integrated with any feed-forward convolutional neural network and is end-to-end trainable. The proposed AttentionRNN layer enforces structure over the spatial attention variables by sequentially predicting attention values in the spatial mask in a bi-directional raster-scan and inverse raster-scan order. This approach ensures that each attention value considers not only local image or contextual information, but also the previously predicted attention values. Our experiments demonstrate consistent quantitative and qualitative improvements on various recognition tasks and datasets, including image categorization, question answering, and image generation.",1
"The science of solving clinical problems by analyzing images generated in clinical practice is known as medical image analysis. The aim is to extract information in an effective and efficient manner for improved clinical diagnosis. The recent advances in the field of biomedical engineering has made medical image analysis one of the top research and development area. One of the reason for this advancement is the application of machine learning techniques for the analysis of medical images. Deep learning is successfully used as a tool for machine learning, where a neural network is capable of automatically learning features. This is in contrast to those methods where traditionally hand crafted features are used. The selection and calculation of these features is a challenging task. Among deep learning techniques, deep convolutional networks are actively used for the purpose of medical image analysis. This include application areas such as segmentation, abnormality detection, disease classification, computer aided diagnosis and retrieval. In this study, a comprehensive review of the current state-of-the-art in medical image analysis using deep convolutional networks is presented. The challenges and potential of these techniques are also highlighted.",0
"Medical image analysis is the scientific practice of solving clinical issues through the analysis of images produced during clinical procedures. The primary objective is to extract information in an efficient and effective manner to improve clinical diagnosis. Biomedical engineering has made significant advancements in this field, making medical image analysis one of the top areas for research and development. This progress can be attributed to the use of machine learning techniques, particularly deep learning, which utilizes neural networks to automatically learn features. This is in contrast to the traditional method of hand-crafting features, which can be a challenging task. Deep convolutional networks are widely used in medical image analysis, specifically for segmentation, abnormality detection, disease classification, computer-aided diagnosis, and retrieval. This study presents a comprehensive review of the current state-of-the-art in medical image analysis using deep convolutional networks, along with highlighting the challenges and potential of these techniques.",1
"Recently, semi-supervised learning methods based on generative adversarial networks (GANs) have received much attention. Among them, two distinct approaches have achieved competitive results on a variety of benchmark datasets. Bad GAN learns a classifier with unrealistic samples distributed on the complement of the support of the input data. Conversely, Triple GAN consists of a three-player game that tries to leverage good generated samples to boost classification results. In this paper, we perform a comprehensive comparison of these two approaches on different benchmark datasets. We demonstrate their different properties on image generation, and sensitivity to the amount of labeled data provided. By comprehensively comparing these two methods, we hope to shed light on the future of GAN-based semi-supervised learning.",0
"The focus on semi-supervised learning methods utilizing generative adversarial networks (GANs) has recently increased. Two distinct approaches have emerged and are achieving competitive results on various benchmark datasets. The first approach, Bad GAN, generates unrealistic samples and creates a classifier based on the complement of the input data's support. In contrast, the second approach, Triple GAN, involves a three-player game that aims to enhance classification results by leveraging high-quality generated samples. This paper presents a comprehensive comparison of these two methods across multiple benchmark datasets, highlighting their unique characteristics regarding image generation and sensitivity to labeled data. By conducting this analysis, we aim to provide insights into the future of GAN-based semi-supervised learning.",1
"The advent of generative adversarial networks (GAN) has enabled new capabilities in synthesis, interpolation, and data augmentation heretofore considered very challenging. However, one of the common assumptions in most GAN architectures is the assumption of simple parametric latent-space distributions. While easy to implement, a simple latent-space distribution can be problematic for uses such as interpolation. This is due to distributional mismatches when samples are interpolated in the latent space. We present a straightforward formalization of this problem; using basic results from probability theory and off-the-shelf-optimization tools, we develop ways to arrive at appropriate non-parametric priors. The obtained prior exhibits unusual qualitative properties in terms of its shape, and quantitative benefits in terms of lower divergence with its mid-point distribution. We demonstrate that our designed prior helps improve image generation along any Euclidean straight line during interpolation, both qualitatively and quantitatively, without any additional training or architectural modifications. The proposed formulation is quite flexible, paving the way to impose newer constraints on the latent-space statistics.",0
"The introduction of generative adversarial networks (GANs) has brought about new possibilities for synthesis, interpolation, and data augmentation that were previously thought to be difficult. However, many GAN architectures assume a simplistic parametric latent-space distribution which can cause issues with interpolation due to distributional differences. We have identified this problem and used probability theory and optimization tools to develop non-parametric priors that address this issue. Our prior has unique qualitative properties and quantitative benefits, resulting in improved image generation during interpolation without requiring additional training or modifications. Our formulation is highly adaptable and can be used to enforce new constraints on latent-space statistics.",1
"The recent success of Generative Adversarial Networks (GAN) is a result of their ability to generate high quality images from a latent vector space. An important application is the generation of images from a text description, where the text description is encoded and further used in the conditioning of the generated image. Thus the generative network has to additionally learn a mapping from the text latent vector space to a highly complex and multi-modal image data distribution, which makes the training of such models challenging. To handle the complexities of fashion image and meta data, we propose Ontology Generative Adversarial Networks (O-GANs) for fashion image synthesis that is conditioned on an hierarchical fashion ontology in order to improve the image generation fidelity. We show that the incorporation of the ontology leads to better image quality as measured by Fr\'{e}chet Inception Distance and Inception Score. Additionally, we show that the O-GAN achieves better conditioning results evaluated by implicit similarity between the text and the generated image.",0
"Generative Adversarial Networks (GAN) have gained recent success due to their ability to create high-quality images from a hidden vector space. One significant application is generating images from a text description, where the text is encoded and utilized to condition the generated image. This requires the generative network to learn a mapping from the text vector space to a complicated and diverse image data distribution, which poses a challenge during training. To address the intricacies of fashion image and meta data, we propose Ontology Generative Adversarial Networks (O-GANs) for fashion image synthesis that are conditioned on a hierarchical fashion ontology to improve image generation accuracy. Our results demonstrate that incorporating the ontology improves image quality, as measured by Fr\'{e}chet Inception Distance and Inception Score, and enhances conditioning accuracy, evaluated by implicit similarity between text and generated image.",1
"Generative models have recently received renewed attention as a result of adversarial learning. Generative adversarial networks consist of samples generation model and a discrimination model able to distinguish between genuine and synthetic samples. In combination with convolutional (for the discriminator) and de-convolutional (for the generator) layers, they are particularly suitable for image generation, especially of natural scenes. However, the presence of fully connected layers adds global dependencies in the generated images. This may lead to high and global variations in the generated sample for small local variations in the input noise. In this work we propose to use architec-tures based on fully convolutional networks (including among others dilated layers), architectures specifically designed to generate globally ergodic images, that is images without global dependencies. Conducted experiments reveal that these architectures are well suited for generating natural textures such as geologic structures .",0
"Adversarial learning has sparked renewed interest in generative models. Generative adversarial networks are composed of a sample generation model and a discrimination model that can differentiate between authentic and synthetic samples. These networks are particularly effective in image creation, especially of natural landscapes, thanks to their convolutional and de-convolutional layers. However, the use of fully connected layers in the generated images can introduce global dependencies, which can cause significant variations in the output for small variations in the input noise. To overcome this issue, we propose utilizing architectures based on fully convolutional networks, which include dilated layers and are specifically designed to create globally ergodic images that lack global dependencies. Our experiments show that these architectures are well-suited for generating natural textures, such as geologic structures.",1
"We propose a novel procedure which adds ""content-addressability"" to any given unconditional implicit model e.g., a generative adversarial network (GAN). The procedure allows users to control the generative process by specifying a set (arbitrary size) of desired examples based on which similar samples are generated from the model. The proposed approach, based on kernel mean matching, is applicable to any generative models which transform latent vectors to samples, and does not require retraining of the model. Experiments on various high-dimensional image generation problems (CelebA-HQ, LSUN bedroom, bridge, tower) show that our approach is able to generate images which are consistent with the input set, while retaining the image quality of the original model. To our knowledge, this is the first work that attempts to construct, at test time, a content-addressable generative model from a trained marginal model.",0
"Our proposed method enhances any unconditional implicit model, such as a GAN, by introducing ""content-addressability"". This enables users to regulate the generative process by providing a group of desired examples, from which the model produces comparable samples. The approach, which utilizes kernel mean matching, can be applied to any generative model that changes latent vectors into samples, without necessitating model retraining. Our experiments on various high-dimensional image generation challenges (CelebA-HQ, LSUN bedroom, bridge, tower) demonstrate that our method creates images that conform to the input set while maintaining the original model's image quality. This is the first known study to develop a content-addressable generative model from a trained marginal model during testing.",1
"Deep generative models are becoming a cornerstone of modern machine learning. Recent work on conditional generative adversarial networks has shown that learning complex, high-dimensional distributions over natural images is within reach. While the latest models are able to generate high-fidelity, diverse natural images at high resolution, they rely on a vast quantity of labeled data. In this work we demonstrate how one can benefit from recent work on self- and semi-supervised learning to outperform the state of the art on both unsupervised ImageNet synthesis, as well as in the conditional setting. In particular, the proposed approach is able to match the sample quality (as measured by FID) of the current state-of-the-art conditional model BigGAN on ImageNet using only 10% of the labels and outperform it using 20% of the labels.",0
"Modern machine learning heavily relies on deep generative models, which have recently shown the capability to learn complex, high-dimensional distributions over natural images through conditional generative adversarial networks. Although these models can generate diverse, high-quality natural images at high resolutions, they require a large amount of labeled data. Our work demonstrates how self- and semi-supervised learning can improve the state-of-the-art in both unsupervised ImageNet synthesis and the conditional setting. Specifically, our proposed approach achieves sample quality on par with the current state-of-the-art conditional model BigGAN on ImageNet using only 10% of the labels, and even outperforms it using 20% of the labels.",1
"Deep generative models are rapidly becoming a common tool for researchers and developers. However, as exhaustively shown for the family of discriminative models, the test-time inference of deep neural networks cannot be fully controlled and erroneous behaviors can be induced by an attacker. In the present work, we show how a malicious user can force a pre-trained generator to reproduce arbitrary data instances by feeding it suitable adversarial inputs. Moreover, we show that these adversarial latent vectors can be shaped so as to be statistically indistinguishable from the set of genuine inputs. The proposed attack technique is evaluated with respect to various GAN images generators using different architectures, training processes and for both conditional and not-conditional setups.",0
"Deep generative models are becoming increasingly popular among researchers and developers. However, as demonstrated by discriminative models, it is not possible to fully control the test-time inference of deep neural networks, and attackers can induce erroneous behavior. In this study, we illustrate how a malicious user can use suitable adversarial inputs to compel a pre-trained generator to create any data instance. Furthermore, we demonstrate that these adversarial latent vectors can be shaped to be statistically identical to genuine inputs. We evaluate the proposed attack technique on various GAN images generators, using different architectures, training processes, and both conditional and non-conditional setups.",1
"This paper proposes a new generative adversarial network for pose transfer, i.e., transferring the pose of a given person to a target pose. The generator of the network comprises a sequence of Pose-Attentional Transfer Blocks that each transfers certain regions it attends to, generating the person image progressively. Compared with those in previous works, our generated person images possess better appearance consistency and shape consistency with the input images, thus significantly more realistic-looking. The efficacy and efficiency of the proposed network are validated both qualitatively and quantitatively on Market-1501 and DeepFashion. Furthermore, the proposed architecture can generate training images for person re-identification, alleviating data insufficiency. Codes and models are available at: https://github.com/tengteng95/Pose-Transfer.git.",0
"A new generative adversarial network is proposed in this paper for pose transfer. This involves transferring the pose of a given person to a desired target pose. The generator of the network is composed of a series of Pose-Attentional Transfer Blocks, which transfer specific regions and progressively generate the person image. Our generated person images exhibit better appearance and shape consistency with the input images, resulting in a more realistic appearance compared to previous works. The network's effectiveness and efficiency are demonstrated through qualitative and quantitative validation on Market-1501 and DeepFashion. Moreover, the proposed architecture can generate training images for person re-identification, addressing the issue of data insufficiency. The codes and models are available at https://github.com/tengteng95/Pose-Transfer.git.",1
"Histopathology slides are routinely marked by pathologists using permanent ink markers that should not be removed as they form part of the medical record. Often tumour regions are marked up for the purpose of highlighting features or other downstream processing such an gene sequencing. Once digitised there is no established method for removing this information from the whole slide images limiting its usability in research and study. Removal of marker ink from these high-resolution whole slide images is non-trivial and complex problem as they contaminate different regions and in an inconsistent manner. We propose an efficient pipeline using convolution neural networks that results in ink-free images without compromising information and image resolution. Our pipeline includes a sequential classical convolution neural network for accurate classification of contaminated image tiles, a fast region detector and a domain adaptive cycle consistent adversarial generative model for restoration of foreground pixels. Both quantitative and qualitative results on four different whole slide images show that our approach yields visually coherent ink-free whole slide images.",0
"Pathologists use permanent ink markers to mark histopathology slides, which should not be removed as they are part of the medical record. These markers are often used to highlight tumor regions or for downstream processing such as gene sequencing. However, once the slides are digitized, the marked information cannot be removed, limiting their usability in research and study. Removing marker ink from these high-resolution whole slide images is a complex problem as the ink contaminates different regions inconsistently. To address this issue, we propose an efficient pipeline that uses convolutional neural networks to produce ink-free images without compromising information and image resolution. Our pipeline includes a classical convolutional neural network for accurate classification of contaminated image tiles, a fast region detector, and a domain adaptive cycle consistent adversarial generative model for restoration of foreground pixels. Our approach yields visually coherent ink-free whole slide images, as demonstrated by quantitative and qualitative results on four different whole slide images.",1
"Adversarial images are samples that are intentionally modified to deceive machine learning systems. They are widely used in applications such as CAPTHAs to help distinguish legitimate human users from bots. However, the noise introduced during the adversarial image generation process degrades the perceptual quality and introduces artificial colours; making it also difficult for humans to classify images and recognise objects. In this letter, we propose a method to enhance the perceptual quality of these adversarial images. The proposed method is attack type agnostic and could be used in association with the existing attacks in the literature. Our experiments show that the generated adversarial images have lower Euclidean distance values while maintaining the same adversarial attack performance. Distances are reduced by 5.88% to 41.27% with an average reduction of 22% over the different attack and network types.",0
"Adversarial images are modified samples intended to deceive machine learning systems, often used in CAPTCHAs to distinguish humans from bots. However, the process of generating these images introduces noise that degrades their perceptual quality and creates artificial colors, making it difficult for humans to classify and recognize objects. In this letter, we propose a method to enhance the perceptual quality of adversarial images that is attack type agnostic and can be used in conjunction with existing attacks. Our experiments demonstrate that this method reduces Euclidean distances by 5.88% to 41.27%, with an average reduction of 22% across various attack and network types, while maintaining the same attack performance.",1
"Over the past few years, Generative Adversarial Networks (GANs) have garnered increased interest among researchers in Computer Vision, with applications including, but not limited to, image generation, translation, imputation, and super-resolution. Nevertheless, no GAN-based method has been proposed in the literature that can successfully represent, generate or translate 3D facial shapes (meshes). This can be primarily attributed to two facts, namely that (a) publicly available 3D face databases are scarce as well as limited in terms of sample size and variability (e.g., few subjects, little diversity in race and gender), and (b) mesh convolutions for deep networks present several challenges that are not entirely tackled in the literature, leading to operator approximations and model instability, often failing to preserve high-frequency components of the distribution. As a result, linear methods such as Principal Component Analysis (PCA) have been mainly utilized towards 3D shape analysis, despite being unable to capture non-linearities and high frequency details of the 3D face - such as eyelid and lip variations. In this work, we present 3DFaceGAN, the first GAN tailored towards modeling the distribution of 3D facial surfaces, while retaining the high frequency details of 3D face shapes. We conduct an extensive series of both qualitative and quantitative experiments, where the merits of 3DFaceGAN are clearly demonstrated against other, state-of-the-art methods in tasks such as 3D shape representation, generation, and translation.",0
"In recent years, Generative Adversarial Networks (GANs) have captured the attention of researchers in the field of Computer Vision. These networks have been applied in various areas, including image generation, translation, imputation, and super-resolution. However, there is a lack of GAN-based methods in the literature that can effectively generate or translate 3D facial shapes (meshes). This is primarily due to two reasons: firstly, publicly available 3D face databases are limited in terms of sample size and diversity, and secondly, mesh convolutions for deep networks pose several challenges that are not well-addressed in the literature, resulting in operator approximations and model instability. Consequently, linear methods such as Principal Component Analysis (PCA) have been mainly used for 3D shape analysis, despite their inability to capture non-linearities and high-frequency details of the 3D face. In this study, we introduce 3DFaceGAN, the first GAN designed for modeling the distribution of 3D facial surfaces while preserving high-frequency details. We conducted a series of both qualitative and quantitative experiments to demonstrate the advantages of 3DFaceGAN over other state-of-the-art methods in tasks such as 3D shape representation, generation, and translation.",1
Recent years have witnessed some exciting developments in the domain of generating images from scene-based text descriptions. These approaches have primarily focused on generating images from a static text description and are limited to generating images in a single pass. They are unable to generate an image interactively based on an incrementally additive text description (something that is more intuitive and similar to the way we describe an image). We propose a method to generate an image incrementally based on a sequence of graphs of scene descriptions (scene-graphs). We propose a recurrent network architecture that preserves the image content generated in previous steps and modifies the cumulative image as per the newly provided scene information. Our model utilizes Graph Convolutional Networks (GCN) to cater to variable-sized scene graphs along with Generative Adversarial image translation networks to generate realistic multi-object images without needing any intermediate supervision during training. We experiment with Coco-Stuff dataset which has multi-object images along with annotations describing the visual scene and show that our model significantly outperforms other approaches on the same dataset in generating visually consistent images for incrementally growing scene graphs.,0
"In recent years, there have been exciting advancements in generating images from textual descriptions of scenes. However, these methods have focused mainly on generating images from static descriptions and cannot generate images interactively based on incremental textual input. We propose a method that generates images incrementally from a sequence of scene-graphs using a recurrent network architecture that retains previously generated image content and modifies it according to new scene information. Our approach employs Graph Convolutional Networks to accommodate varying-sized scene-graphs and Generative Adversarial image translation networks to generate multi-object images realistically without intermediate supervision during training. We demonstrate the effectiveness of our model on the Coco-Stuff dataset, which includes multi-object images with annotated visual scene descriptions, and show that our approach significantly outperforms other methods in producing visually coherent images for incrementally growing scene-graphs.",1
"The potential benefit of hybrid X-ray and MR imaging in the interventional environment is large due to the combination of fast imaging with high contrast variety. However, a vast amount of existing image enhancement methods requires the image information of both modalities to be present in the same domain. To unlock this potential, we present a solution to image-to-image translation from MR projections to corresponding X-ray projection images. The approach is based on a state-of-the-art image generator network that is modified to fit the specific application. Furthermore, we propose the inclusion of a gradient map in the loss function to allow the network to emphasize high-frequency details in image generation. Our approach is capable of creating X-ray projection images with natural appearance. Additionally, our extensions show clear improvement compared to the baseline method.",0
"Combining fast imaging with high contrast variety, hybrid X-ray and MR imaging holds great potential in the interventional environment. However, existing image enhancement methods often require both modalities to be present in the same domain. To overcome this challenge, we introduce a solution for translating MR projections to corresponding X-ray projection images using a state-of-the-art image generator network adapted for this specific application. To further enhance the results, we suggest incorporating a gradient map in the loss function to emphasize high-frequency details. Our approach generates X-ray projection images that have a natural appearance and our extensions demonstrate significant improvement over the baseline method.",1
"We present a generalization of the person-image generation task, in which a human image is generated conditioned on a target pose and a set X of source appearance images. In this way, we can exploit multiple, possibly complementary images of the same person which are usually available at training and at testing time. The solution we propose is mainly based on a local attention mechanism which selects relevant information from different source image regions, avoiding the necessity to build specific generators for each specific cardinality of X. The empirical evaluation of our method shows the practical interest of addressing the person-image generation problem in a multi-source setting.",0
"Our study introduces a novel approach to the person-image generation task. Here, we generate a human image by considering a target pose and a set X of source appearance images. By doing so, we can leverage various images of the same individual available during training and testing. Our solution primarily relies on a local attention mechanism that identifies relevant information from different regions of source images. This negates the need for creating distinct generators for each X's cardinality. Empirically, our technique demonstrates the practical usefulness of tackling the person-image generation challenge in a multi-source context.",1
"Image generation has raised tremendous attention in both academic and industrial areas, especially for the conditional and target-oriented image generation, such as criminal portrait and fashion design. Although the current studies have achieved preliminary results along this direction, they always focus on class labels as the condition where spatial contents are randomly generated from latent vectors. Edge details are usually blurred since spatial information is difficult to preserve. In light of this, we propose a novel Spatially Constrained Generative Adversarial Network (SCGAN), which decouples the spatial constraints from the latent vector and makes these constraints feasible as additional controllable signals. To enhance the spatial controllability, a generator network is specially designed to take a semantic segmentation, a latent vector and an attribute-level label as inputs step by step. Besides, a segmentor network is constructed to impose spatial constraints on the generator. Experimentally, we provide both visual and quantitative results on CelebA and DeepFashion datasets, and demonstrate that the proposed SCGAN is very effective in controlling the spatial contents as well as generating high-quality images.",0
"The generation of images has garnered considerable attention in academic and industrial fields, particularly in the creation of images that are conditional and target-driven, such as fashion design and criminal portraits. While existing studies have yielded promising results in this area, they have focused mainly on class labels as conditions, leading to the random generation of spatial contents from latent vectors. Consequently, edge details in the resulting images tend to be blurry due to the difficulty of preserving spatial information. To address this issue, we propose a Spatially Constrained Generative Adversarial Network (SCGAN) that separates spatial constraints from latent vectors, making them controllable signals. Our approach involves designing a specialized generator network that takes a semantic segmentation, a latent vector, and an attribute-level label as inputs, and a segmentor network that imposes spatial constraints on the generator. Our experimental results on CelebA and DeepFashion datasets demonstrate that the SCGAN enables effective control of spatial contents and produces high-quality images.",1
"Attribute guided face image synthesis aims to manipulate attributes on a face image. Most existing methods for image-to-image translation can either perform a fixed translation between any two image domains using a single attribute or require training data with the attributes of interest for each subject. Therefore, these methods could only train one specific model for each pair of image domains, which limits their ability in dealing with more than two domains. Another disadvantage of these methods is that they often suffer from the common problem of mode collapse that degrades the quality of the generated images. To overcome these shortcomings, we propose attribute guided face image generation method using a single model, which is capable to synthesize multiple photo-realistic face images conditioned on the attributes of interest. In addition, we adopt the proposed model to increase the realism of the simulated face images while preserving the face characteristics. Compared to existing models, synthetic face images generated by our method present a good photorealistic quality on several face datasets. Finally, we demonstrate that generated facial images can be used for synthetic data augmentation, and improve the performance of the classifier used for facial expression recognition.",0
"The goal of attribute guided face image synthesis is to manipulate facial attributes. Current image-to-image translation methods either have a fixed translation between two image domains using one attribute or require training data for each subject's attribute of interest. This limits their capability to handle more than two domains, and they also suffer from mode collapse that reduces image quality. To address these issues, we propose a single model for attribute guided face image generation that can synthesize multiple realistic face images based on desired attributes. Our model enhances the realism of simulated face images while preserving facial features, resulting in photorealistic images on various face datasets. Additionally, these generated images can be used for data augmentation to improve facial expression recognition performance.",1
"Photorealistic style transfer aims to transfer the style of one image to another, but preserves the original structure and detail outline of the content image, which makes the content image still look like a real shot after the style transfer. Although some realistic image styling methods have been proposed, these methods are vulnerable to lose the details of the content image and produce some irregular distortion structures. In this paper, we use a high-resolution network as the image generation network. Compared to other methods, which reduce the resolution and then restore the high resolution, our generation network maintains high resolution throughout the process. By connecting high-resolution subnets to low-resolution subnets in parallel and repeatedly multi-scale fusion, high-resolution subnets can continuously receive information from low-resolution subnets. This allows our network to discard less information contained in the image, so the generated images may have a more elaborate structure and less distortion, which is crucial to the visual quality. We conducted extensive experiments and compared the results with existing methods. The experimental results show that our model is effective and produces better results than existing methods for photorealistic image stylization. Our source code with PyTorch framework will be publicly available at https://github.com/limingcv/Photorealistic-Style-Transfer",0
"The objective of photorealistic style transfer is to transfer the style of an image onto another while retaining the original structure and detail of the content image, resulting in a realistic image after the style transfer. However, existing realistic image styling techniques are prone to losing details and creating irregular distortions. In this study, we employed a high-resolution network as the image generation network, maintaining high resolution throughout the process. Our network connects high-resolution subnets to low-resolution subnets in parallel and repeatedly multi-scale fusion, allowing for less information loss and generating images with elaborate structures and minimal distortion, leading to improved visual quality. Our model was tested against existing methods and proved to be more effective for photorealistic image stylization. We will make our source code with PyTorch framework available publicly at https://github.com/limingcv/Photorealistic-Style-Transfer.",1
"This work showcases a new approach for causal discovery by leveraging user experiments and recent advances in photo-realistic image editing, demonstrating a potential of identifying causal factors and understanding complex systems counterfactually. We introduce the beauty learning problem as an example, which has been discussed metaphysically for centuries and been proved exists, is quantifiable, and can be learned by deep models in our recent paper, where we utilize a natural image generator coupled with user studies to infer causal effects from facial semantics to beauty outcomes, the results of which also align with existing empirical studies. We expect the proposed framework for a broader application in causal inference.",0
"The work presented here presents a novel method for discovering causality using user experiments and advancements in photo-realistic image editing. This approach has the potential to identify causal factors and provide counterfactual understanding of complex systems. An example of this is the beauty learning problem, which has been a topic of metaphysical discussion for centuries and has now been proven to exist and be quantifiable using deep models. In our recent paper, we employed a natural image generator and user studies to infer causal effects from facial semantics to beauty outcomes. The results align with existing empirical studies, and we anticipate that this framework will have broader applications in the field of causal inference.",1
"Visual compatibility is critical for fashion analysis, yet is missing in existing fashion image synthesis systems. In this paper, we propose to explicitly model visual compatibility through fashion image inpainting. To this end, we present Fashion Inpainting Networks (FiNet), a two-stage image-to-image generation framework that is able to perform compatible and diverse inpainting. Disentangling the generation of shape and appearance to ensure photorealistic results, our framework consists of a shape generation network and an appearance generation network. More importantly, for each generation network, we introduce two encoders interacting with one another to learn latent code in a shared compatibility space. The latent representations are jointly optimized with the corresponding generation network to condition the synthesis process, encouraging a diverse set of generated results that are visually compatible with existing fashion garments. In addition, our framework is readily extended to clothing reconstruction and fashion transfer, with impressive results. Extensive experiments with comparisons with state-of-the-art approaches on fashion synthesis task quantitatively and qualitatively demonstrate the effectiveness of our method.",0
"Existing fashion image synthesis systems lack visual compatibility, which is crucial for fashion analysis. In this study, we propose a fashion image inpainting approach to explicitly model visual compatibility. Our method, called Fashion Inpainting Networks (FiNet), is a two-stage image-to-image generation framework that can perform compatible and diverse inpainting. We disentangle the generation of shape and appearance to ensure photorealistic results, using a shape generation network and an appearance generation network. We introduce two encoders for each generation network to learn latent code in a shared compatibility space that is jointly optimized with the corresponding generation network. This encourages the synthesis process to generate a diverse set of visually compatible fashion garments. Our framework can also be extended to clothing reconstruction and fashion transfer, with impressive results. Extensive experiments demonstrate the effectiveness of our method, with comparisons to state-of-the-art approaches on fashion synthesis tasks.",1
"Structured representations such as scene graphs serve as an efficient and compact representation that can be used for downstream rendering or retrieval tasks. However, existing efforts to generate realistic images from scene graphs perform poorly on scene composition for cluttered or complex scenes. We propose two contributions to improve the scene composition. First, we enhance the scene graph representation with heuristic-based relations, which add minimal storage overhead. Second, we use extreme points representation to supervise the learning of the scene composition network. These methods achieve significantly higher performance over existing work (69.0% vs 51.2% in relation score metric). We additionally demonstrate how scene graphs can be used to retrieve pose-constrained image patches that are semantically similar to the source query. Improving structured scene graph representations for rendering or retrieval is an important step towards realistic image generation.",0
"Scene graphs are useful representations for downstream rendering or retrieval tasks due to their efficiency and compactness. However, when it comes to generating realistic images from scene graphs for cluttered or complex scenes, existing methods fall short. To address this issue, we propose two solutions. Firstly, we improve the scene graph representation by adding heuristic-based relations with minimal storage overhead. Secondly, we incorporate extreme points representation to supervise the learning of the scene composition network. These approaches have been shown to outperform existing methods by achieving a higher performance score of 69.0% compared to 51.2% in relation score metric. We also showcase how scene graphs can be used to retrieve pose-constrained image patches that are semantically similar to the source query. This research is a crucial step towards enhancing structured scene graph representations for rendering or retrieval, which is essential for generating realistic images.",1
"In this paper, we address unsupervised pose-guided person image generation, which is known challenging due to non-rigid deformation. Unlike previous methods learning a rock-hard direct mapping between human bodies, we propose a new pathway to decompose the hard mapping into two more accessible subtasks, namely, semantic parsing transformation and appearance generation. Firstly, a semantic generative network is proposed to transform between semantic parsing maps, in order to simplify the non-rigid deformation learning. Secondly, an appearance generative network learns to synthesize semantic-aware textures. Thirdly, we demonstrate that training our framework in an end-to-end manner further refines the semantic maps and final results accordingly. Our method is generalizable to other semantic-aware person image generation tasks, eg, clothing texture transfer and controlled image manipulation. Experimental results demonstrate the superiority of our method on DeepFashion and Market-1501 datasets, especially in keeping the clothing attributes and better body shapes.",0
"The focus of our paper is on the challenge of unsupervised pose-guided person image generation, which is difficult due to non-rigid deformation. Previous methods relied on a direct mapping between human bodies, which is inflexible. To overcome this limitation, we propose a new approach that breaks down the mapping into two manageable subtasks: semantic parsing transformation and appearance generation. Our semantic generative network simplifies the learning of non-rigid deformation by transforming between semantic parsing maps. Our appearance generative network then synthesizes semantic-aware textures. We also show that end-to-end training of our framework refines the semantic maps and improves the final results. Our method is applicable to other semantic-aware person image generation tasks, such as clothing texture transfer and controlled image manipulation. Our experimental results on the DeepFashion and Market-1501 datasets demonstrate the superiority of our method in preserving clothing attributes and achieving better body shapes.",1
"We present a probabilistic model for Sketch-Based Image Retrieval (SBIR) where, at retrieval time, we are given sketches from novel classes, that were not present at training time. Existing SBIR methods, most of which rely on learning class-wise correspondences between sketches and images, typically work well only for previously seen sketch classes, and result in poor retrieval performance on novel classes. To address this, we propose a generative model that learns to generate images, conditioned on a given novel class sketch. This enables us to reduce the SBIR problem to a standard image-to-image search problem. Our model is based on an inverse auto-regressive flow based variational autoencoder, with a feedback mechanism to ensure robust image generation. We evaluate our model on two very challenging datasets, Sketchy, and TU Berlin, with novel train-test split. The proposed approach significantly outperforms various baselines on both the datasets.",0
"Our study introduces a probabilistic model for Sketch-Based Image Retrieval (SBIR) that can handle sketches from previously unseen classes at retrieval time. Traditional SBIR methods rely on learning correspondences between sketches and images, but these methods are limited in their ability to perform well on novel classes. To overcome this limitation, we propose a generative model that can produce images based on a given novel class sketch. This approach simplifies the SBIR problem to an image-to-image search problem. Our model is built on an inverse auto-regressive flow-based variational autoencoder with a feedback mechanism that ensures robust image generation. We tested our model on two challenging datasets, Sketchy and TU Berlin, with a novel train-test split. Our proposed approach outperformed various baselines on both datasets.",1
"We study two important concepts in adversarial deep learning---adversarial training and generative adversarial network (GAN). Adversarial training is the technique used to improve the robustness of discriminator by combining adversarial attacker and discriminator in the training phase. GAN is commonly used for image generation by jointly optimizing discriminator and generator. We show these two concepts are indeed closely related and can be used to strengthen each other---adding a generator to the adversarial training procedure can improve the robustness of discriminators, and adding an adversarial attack to GAN training can improve the convergence speed and lead to better generators. Combining these two insights, we develop a framework called Rob-GAN to jointly optimize generator and discriminator in the presence of adversarial attacks---the generator generates fake images to fool discriminator; the adversarial attacker perturbs real images to fool the discriminator, and the discriminator wants to minimize loss under fake and adversarial images. Through this end-to-end training procedure, we are able to simultaneously improve the convergence speed of GAN training, the quality of synthetic images, and the robustness of discriminator under strong adversarial attacks. Experimental results demonstrate that the obtained classifier is more robust than the state-of-the-art adversarial training approach, and the generator outperforms SN-GAN on ImageNet-143.",0
"In the realm of adversarial deep learning, two key concepts are explored: adversarial training and generative adversarial network (GAN). Adversarial training enhances the strength of the discriminator by merging it with an adversarial attacker during training. GAN is a popular method for generating images by optimizing both the discriminator and the generator. These concepts are tightly connected and can be used to reinforce each other. By introducing a generator in the adversarial training process, discriminators can become more robust. Similarly, augmenting GAN training with an adversarial attack can lead to faster convergence and superior generators. A new technique called Rob-GAN is introduced, which combines these insights to jointly optimize the generator and discriminator amidst adversarial attacks. The generator produces fake images to deceive the discriminator, while the adversarial attacker alters real images to trick the discriminator, which aims to minimize loss under both fake and adversarial images. This comprehensive training approach improves the convergence speed of GAN training, the synthetic image quality, and the discriminator's robustness under severe adversarial attacks. Experimental results show that the classifier obtained from this approach is more resilient than the state-of-the-art adversarial training method, and the generator outperforms SN-GAN on ImageNet-143.",1
"As a sub-domain of text-to-image synthesis, text-to-face generation has huge potentials in public safety domain. With lack of dataset, there are almost no related research focusing on text-to-face synthesis. In this paper, we propose a fully-trained Generative Adversarial Network (FTGAN) that trains the text encoder and image decoder at the same time for fine-grained text-to-face generation. With a novel fully-trained generative network, FTGAN can synthesize higher-quality images and urge the outputs of the FTGAN are more relevant to the input sentences. In addition, we build a dataset called SCU-Text2face for text-to-face synthesis. Through extensive experiments, the FTGAN shows its superiority in boosting both generated images' quality and similarity to the input descriptions. The proposed FTGAN outperforms the previous state of the art, boosting the best reported Inception Score to 4.63 on the CUB dataset. On SCU-text2face, the face images generated by our proposed FTGAN just based on the input descriptions is of average 59% similarity to the ground-truth, which set a baseline for text-to-face synthesis.",0
"Text-to-face generation, a sub-domain of text-to-image synthesis, holds great potential in the public safety domain. However, due to the lack of relevant datasets, there is a dearth of research on this subject. To address this gap, we propose a novel fully-trained Generative Adversarial Network (FTGAN) that simultaneously trains the text encoder and image decoder for fine-grained text-to-face generation. We also introduce a new dataset called SCU-Text2face for this purpose. Our results indicate that the FTGAN generates higher-quality images that are more relevant to the input sentences, outperforming the previous state-of-the-art. Specifically, the FTGAN achieves an Inception Score of 4.63 on the CUB dataset and generates face images with an average 59% similarity to the ground-truth on SCU-Text2face, setting a baseline for text-to-face synthesis.",1
"Generating diverse yet specific data is the goal of the generative adversarial network (GAN), but it suffers from the problem of mode collapse. We introduce the concept of normalized diversity which force the model to preserve the normalized pairwise distance between the sparse samples from a latent parametric distribution and their corresponding high-dimensional outputs. The normalized diversification aims to unfold the manifold of unknown topology and non-uniform distribution, which leads to safe interpolation between valid latent variables. By alternating the maximization over the pairwise distance and updating the total distance (normalizer), we encourage the model to actively explore in the high-dimensional output space. We demonstrate that by combining the normalized diversity loss and the adversarial loss, we generate diverse data without suffering from mode collapsing. Experimental results show that our method achieves consistent improvement on unsupervised image generation, conditional image generation and hand pose estimation over strong baselines.",0
"The generative adversarial network (GAN) aims to produce a variety of specific data, but it faces mode collapse. To address this issue, we propose the concept of normalized diversity, which maintains the normalized pairwise distance between the sparse samples from a latent parametric distribution and their corresponding high-dimensional outputs. This approach aims to uncover the manifold of an unknown topology and non-uniform distribution, enabling safe interpolation between valid latent variables. By maximizing the pairwise distance and updating the total distance alternately, we encourage the model to explore actively in the high-dimensional output space. We show that combining the normalized diversity loss with the adversarial loss generates diverse data without mode collapsing. Our method consistently improves unsupervised image generation, conditional image generation, and hand pose estimation over strong baselines, as demonstrated by experimental results.",1
"Glaucoma is a major eye disease, leading to vision loss in the absence of proper medical treatment. Current diagnosis of glaucoma is performed by ophthalmologists who are often analyzing several types of medical images generated by different types of medical equipment. Capturing and analyzing these medical images is labor-intensive and expensive. In this paper, we present a novel computational approach towards glaucoma diagnosis and localization, only making use of eye fundus images that are analyzed by state-of-the-art deep learning techniques. Specifically, our approach leverages Convolutional Neural Networks (CNNs) and Gradient-weighted Class Activation Mapping (Grad-CAM) for glaucoma diagnosis and localization, respectively. Quantitative and qualitative results, as obtained for a small-sized dataset with no segmentation ground truth, demonstrate that the proposed approach is promising, for instance achieving an accuracy of 0.91$\pm0.02$ and an ROC-AUC score of 0.94 for the diagnosis task. Furthermore, we present a publicly available prototype web application that integrates our predictive model, with the goal of making effective glaucoma diagnosis available to a wide audience.",0
"Glaucoma is a significant eye disorder that can result in vision loss if not treated appropriately. The current method of diagnosing glaucoma requires ophthalmologists to analyze various medical images obtained from different types of medical equipment, which is both expensive and laborious. This study introduces a new computational approach for diagnosing and localizing glaucoma using only eye fundus images, which are analyzed using advanced deep learning techniques such as Convolutional Neural Networks (CNNs) and Gradient-weighted Class Activation Mapping (Grad-CAM). Despite the lack of segmentation ground truth in the small dataset used, the proposed approach shows promising results, including an accuracy of 0.91±0.02 and an ROC-AUC score of 0.94 for the diagnosis task. Additionally, a prototype web application incorporating the predictive model is provided for the public to access effective glaucoma diagnosis.",1
"Synthesizing photo-realistic images from text descriptions is a challenging problem. Previous studies have shown remarkable progresses on visual quality of the generated images. In this paper, we consider semantics from the input text descriptions in helping render photo-realistic images. However, diverse linguistic expressions pose challenges in extracting consistent semantics even they depict the same thing. To this end, we propose a novel photo-realistic text-to-image generation model that implicitly disentangles semantics to both fulfill the high-level semantic consistency and low-level semantic diversity. To be specific, we design (1) a Siamese mechanism in the discriminator to learn consistent high-level semantics, and (2) a visual-semantic embedding strategy by semantic-conditioned batch normalization to find diverse low-level semantics. Extensive experiments and ablation studies on CUB and MS-COCO datasets demonstrate the superiority of the proposed method in comparison to state-of-the-art methods.",0
"Generating photo-realistic images from textual descriptions is a complex task that has seen significant advancements in visual quality in previous studies. This paper focuses on utilizing the semantics of the input text to produce high-quality images. However, the challenge lies in extracting consistent semantics from varied linguistic expressions that depict the same thing. To address this issue, we propose a novel model that disentangles semantics to ensure both high-level semantic consistency and low-level semantic diversity. This is achieved through a Siamese mechanism in the discriminator to learn consistent high-level semantics, and a visual-semantic embedding strategy that utilizes semantic-conditioned batch normalization to find diverse low-level semantics. Extensive experiments on CUB and MS-COCO datasets demonstrate the superior performance of our proposed method compared to state-of-the-art methods.",1
"Since the generative neural networks have made a breakthrough in the image generation problem, lots of researches on their applications have been studied such as image restoration, style transfer and image completion. However, there has been few research generating objects in uncontrolled real-world environments. In this paper, we propose a novel approach for vehicle image generation in real-world scenes. Using a subnetwork based on a precedent work of image completion, our model makes the shape of an object. Details of objects are trained by an additional colorization and refinement subnetwork, resulting in a better quality of generated objects. Unlike many other works, our method does not require any segmentation layout but still makes a plausible vehicle in the image. We evaluate our method by using images from Berkeley Deep Drive (BDD) and Cityscape datasets, which are widely used for object detection and image segmentation problems. The adequacy of the generated images by the proposed method has also been evaluated using a widely utilized object detection algorithm and the FID score.",0
"Numerous studies have explored the applications of generative neural networks in image generation, such as image restoration, style transfer, and image completion. However, the generation of objects in uncontrolled real-world environments has received little attention. This paper introduces a new technique for generating vehicle images in real-world scenes. Our approach utilizes a subnetwork based on previous image completion research to create the shape of an object, while additional subnetworks for colorization and refinement enhance the details and overall quality of the generated object. Unlike other methods, our approach does not require segmentation layout yet still produces believable vehicle images. We evaluate our method using images from the BDD and Cityscape datasets, which are commonly used for image segmentation and object detection problems. We also assess the quality of the generated images using an object detection algorithm and the FID score.",1
"Environment perception is an important task with great practical value and bird view is an essential part for creating panoramas of surrounding environment. Due to the large gap and severe deformation between the frontal view and bird view, generating a bird view image from a single frontal view is challenging. To tackle this problem, we propose the BridgeGAN, i.e., a novel generative model for bird view synthesis. First, an intermediate view, i.e., homography view, is introduced to bridge the large gap. Next, conditioned on the three views (frontal view, homography view and bird view) in our task, a multi-GAN based model is proposed to learn the challenging cross-view translation. Extensive experiments conducted on a synthetic dataset have demonstrated that the images generated by our model are much better than those generated by existing methods, with more consistent global appearance and sharper details. Ablation studies and discussions show its reliability and robustness in some challenging cases.",0
"Creating panoramas of the surrounding environment requires environment perception, which is an important task with practical value. However, generating a bird view image from a single frontal view is challenging due to the large gap and severe deformation between the two perspectives. To address this problem, we propose the BridgeGAN, a novel generative model for bird view synthesis. The model introduces an intermediate view, the homography view, to bridge the gap and uses a multi-GAN based approach to learn cross-view translation. Our experiments on a synthetic dataset demonstrate that our model generates images with a more consistent global appearance and sharper details compared to existing methods. Ablation studies and discussions also confirm the model's reliability and robustness in challenging cases.",1
"One of the grand challenges of deep learning is the requirement to obtain large labeled training data sets. While synthesized data sets can be used to overcome this challenge, it is important that these data sets close the reality gap, i.e., a model trained on synthetic image data is able to generalize to real images. Whereas, the reality gap can be considered bridged in several application scenarios, training on synthesized images containing reflecting materials requires further research. Since the appearance of objects with reflecting materials is dominated by the surrounding environment, this interaction needs to be considered during training data generation. Therefore, within this paper we examine the effect of reflecting materials in the context of synthetic image generation for training object detectors. We investigate the influence of rendering approach used for image synthesis, the effect of domain randomization, as well as the amount of used training data. To be able to compare our results to the state-of-the-art, we focus on indoor scenes as they have been investigated extensively. Within this scenario, bathroom furniture is a natural choice for objects with reflecting materials, for which we report our findings on real and synthetic testing data.",0
"Deep learning faces a major challenge in obtaining large labeled training data sets. Although synthesized data sets can be used to overcome this issue, it is crucial to ensure that these data sets bridge the reality gap. For instance, a model trained on synthetic image data should be able to generalize to real images. However, when it comes to training on synthesized images containing reflecting materials, further research is required. This is because the appearance of objects with reflecting materials is largely influenced by the surrounding environment, which needs to be taken into account during training data generation. In this paper, we focus on examining the impact of reflecting materials on synthetic image generation for training object detectors. Specifically, we investigate the effect of the rendering approach, domain randomization, and the amount of training data used. To ensure our results are comparable to the state-of-the-art, we concentrate on indoor scenes, which have been thoroughly studied. Bathroom furniture is a natural choice for objects with reflecting materials in this scenario, and we present our findings on both real and synthetic testing data.",1
"In order to operate autonomously, a robot should explore the environment and build a model of each of the surrounding objects. A common approach is to carefully scan the whole workspace. This is time-consuming. It is also often impossible to reach all the viewpoints required to acquire full knowledge about the environment. Humans can perform shape completion of occluded objects by relying on past experience. Therefore, we propose a method that generates images of an object from various viewpoints using a single input RGB image. A deep neural network is trained to imagine the object appearance from many viewpoints. We present the whole pipeline, which takes a single RGB image as input and returns a sequence of RGB and depth images of the object. The method utilizes a CNN-based object detector to extract the object from the natural scene. Then, the proposed network generates a set of RGB and depth images. We show the results both on a synthetic dataset and on real images.",0
"To enable a robot to function independently, it must survey its surroundings and establish a model of all objects in the vicinity. Typically, a thorough scan of the entire workspace is undertaken, but this is a time-consuming process and may not always provide complete information. Humans can fill in the gaps in their knowledge of obscured objects based on prior experience. Hence, we suggest a technique that employs a deep neural network to produce images of an object from different viewpoints, all from a single input RGB image. Our method involves using a CNN-based object detector to isolate the object in a natural setting, after which the proposed network generates a series of RGB and depth images. We demonstrate our approach using both synthetic and real-world datasets.",1
"Generative Adversarial Networks (GANs) are shown to be successful at generating new and realistic samples including 3D object models. Conditional GAN, a variant of GANs, allows generating samples in given conditions. However, objects generated for each condition are different and it does not allow generation of the same object in different conditions. In this paper, we first adapt conditional GAN, which is originally designed for 2D image generation, to the problem of generating 3D models in different rotations. We then propose a new approach to guide the network to generate the same 3D sample in different and controllable rotation angles (sample pairs). Unlike previous studies, the proposed method does not require modification of the standard conditional GAN architecture and it can be integrated into the training step of any conditional GAN. Experimental results and visual comparison of 3D models show that the proposed method is successful at generating model pairs in different conditions.",0
"The effectiveness of Generative Adversarial Networks (GANs) in producing realistic and novel samples, such as 3D object models, has been demonstrated. Conditional GANs, which permit sample generation under specific conditions, have also been successful. However, the objects produced for each condition are unique, and generating the same object under different conditions is not possible. This paper introduces a modified version of conditional GANs, originally designed for 2D image creation, to generate 3D models at various rotations. Additionally, the authors propose a new technique that enables the network to generate identical 3D models at distinct and adjustable rotation angles. Unlike previous studies, this method does not necessitate alterations to the standard conditional GAN architecture and can be incorporated into any conditional GAN's training process. The experimental findings and visual inspection of 3D models indicate that this approach is effective in creating pairs of models under diverse conditions.",1
"Conditional generative adversarial networks (cGAN) have led to large improvements in the task of conditional image generation, which lies at the heart of computer vision. The major focus so far has been on performance improvement, while there has been little effort in making cGAN more robust to noise. The regression (of the generator) might lead to arbitrarily large errors in the output, which makes cGAN unreliable for real-world applications. In this work, we introduce a novel conditional GAN model, called RoCGAN, which leverages structure in the target space of the model to address the issue. Our model augments the generator with an unsupervised pathway, which promotes the outputs of the generator to span the target manifold even in the presence of intense noise. We prove that RoCGAN share similar theoretical properties as GAN and experimentally verify that our model outperforms existing state-of-the-art cGAN architectures by a large margin in a variety of domains including images from natural scenes and faces.",0
"Conditional image generation is a vital part of computer vision, and cGAN has significantly enhanced this task. However, cGAN's reliability in real-world applications has been limited due to the generator's regression causing significant errors in the output. Although prior research has mainly focused on enhancing performance, there has been little effort to make cGAN more resilient to noise. To address this issue, we propose a new model, RoCGAN, that utilizes the structure of the target space to improve the performance of the generator. Our model incorporates an unsupervised pathway that helps the generator produce outputs that span the target manifold even in the presence of significant noise. We demonstrate that RoCGAN shares similar theoretical properties as GAN and outperforms existing state-of-the-art cGAN architectures significantly, especially for images of natural scenes and faces, which we experimentally verify.",1
"This paper proposes the novel task of video generation conditioned on a SINGLE semantic label map, which provides a good balance between flexibility and quality in the generation process. Different from typical end-to-end approaches, which model both scene content and dynamics in a single step, we propose to decompose this difficult task into two sub-problems. As current image generation methods do better than video generation in terms of detail, we synthesize high quality content by only generating the first frame. Then we animate the scene based on its semantic meaning to obtain the temporally coherent video, giving us excellent results overall. We employ a cVAE for predicting optical flow as a beneficial intermediate step to generate a video sequence conditioned on the initial single frame. A semantic label map is integrated into the flow prediction module to achieve major improvements in the image-to-video generation process. Extensive experiments on the Cityscapes dataset show that our method outperforms all competing methods.",0
"In this paper, a new task is introduced that involves generating videos based on a singular semantic label map. This approach strikes a balance between flexibility and quality during the generation process. Unlike typical end-to-end approaches that tackle both scene content and dynamics simultaneously, this paper proposes dividing the task into two sub-problems. To address the issue of image detail, only the first frame is generated to create high-quality content. The scene is then animated based on its semantic meaning to produce a temporally coherent video, which yields excellent results. To achieve this, a cVAE is utilized to predict optical flow as an intermediate step in generating a video sequence based on the initial single frame. The integration of a semantic label map into the flow prediction module significantly improves the image-to-video generation process. The proposed method is tested extensively on the Cityscapes dataset and outperforms all competing methods.",1
"In this paper, we describe how to apply image-to-image translation techniques to medical blood smear data to generate new data samples and meaningfully increase small datasets. Specifically, given the segmentation mask of the microscopy image, we are able to generate photorealistic images of blood cells which are further used alongside real data during the network training for segmentation and object detection tasks. This image data generation approach is based on conditional generative adversarial networks which have proven capabilities to high-quality image synthesis. In addition to synthesizing blood images, we synthesize segmentation mask as well which leads to a diverse variety of generated samples. The effectiveness of the technique is thoroughly analyzed and quantified through a number of experiments on a manually collected and annotated dataset of blood smear taken under a microscope.",0
"The objective of this paper is to explain how image-to-image translation techniques can be utilized on medical blood smear data to create new data samples and effectively increase small datasets. Our approach involves generating realistic blood cell images based on the microscopy image's segmentation mask. These generated images are combined with actual data during network training for segmentation and object detection tasks. The method for generating image data is founded on conditional generative adversarial networks, which have been proven to produce high-quality image synthesis. Moreover, we not only generate blood images but also produce segmentation masks, resulting in a diverse range of generated samples. The effectiveness of this technique is meticulously analyzed and measured through numerous experiments on a manually collected and annotated dataset of blood smear images captured under a microscope.",1
"Deep generative models have been successfully applied to many applications. However, existing works experience limitations when generating large images (the literature usually generates small images, e.g. 32 * 32 or 128 * 128). In this paper, we propose a novel scheme, called deep tensor adversarial generative nets (TGAN), that generates large high-quality images by exploring tensor structures. Essentially, the adversarial process of TGAN takes place in a tensor space. First, we impose tensor structures for concise image representation, which is superior in capturing the pixel proximity information and the spatial patterns of elementary objects in images, over the vectorization preprocess in existing works. Secondly, we propose TGAN that integrates deep convolutional generative adversarial networks and tensor super-resolution in a cascading manner, to generate high-quality images from random distributions. More specifically, we design a tensor super-resolution process that consists of tensor dictionary learning and tensor coefficients learning. Finally, on three datasets, the proposed TGAN generates images with more realistic textures, compared with state-of-the-art adversarial autoencoders. The size of the generated images is increased by over 8.5 times, namely 374 * 374 in PASCAL2.",0
"Numerous applications have benefited from the use of deep generative models. However, these models have limitations when generating large images, as the literature typically generates small images, such as 32 * 32 or 128 * 128. This paper introduces a new approach, called deep tensor adversarial generative nets (TGAN), which generates high-quality, large images by exploring tensor structures. The adversarial process of TGAN takes place in a tensor space, where tensor structures are imposed for concise image representation. This is more effective in capturing pixel proximity information and spatial patterns of objects in images than the vectorization preprocess used in existing works. TGAN integrates deep convolutional generative adversarial networks and tensor super-resolution in a cascading manner to generate high-quality images from random distributions. The tensor super-resolution process involves tensor dictionary learning and tensor coefficients learning. TGAN generates images with more realistic textures compared to state-of-the-art adversarial autoencoders on three datasets. The size of the generated images has increased by over 8.5 times, specifically to 374 * 374 in PASCAL2.",1
"High dynamic range (HDR) imaging has recently drawn much attention in multimedia community. In this paper, we proposed a HDR image forensics method based on convolutional neural network (CNN).To our best knowledge, this is the first time to apply deep learning method on HDR image forensics. The proposed algorithm uses CNN to distinguish HDR images generated by multiple low dynamic range (LDR) images from that expanded by single LDR image using inverse tone mapping (iTM). To do this, we learn the change of statistical characteristics extracted by the proposed CNN architectures and classify two kinds of HDR images. Comparision results with some traditional statistical characteristics shows efficiency of the proposed method in HDR image source identification.",0
"The multimedia community has recently shown a great deal of interest in High Dynamic Range (HDR) imaging. This paper introduces a novel HDR image forensics technique that utilizes a convolutional neural network (CNN). To the best of our knowledge, this is the first time that a deep learning approach has been applied to HDR image forensics. Our method employs CNN to distinguish HDR images created by multiple low dynamic range (LDR) images from those expanded by a single LDR image using inverse tone mapping (iTM). We accomplish this by teaching the proposed CNN architectures to identify changes in the statistical characteristics of the images and classify the two types of HDR images. By comparing the results with those obtained using traditional statistical characteristics, we demonstrate the effectiveness of our method in identifying the source of HDR images.",1
"In this paper, we propose Object-driven Attentive Generative Adversarial Newtorks (Obj-GANs) that allow object-centered text-to-image synthesis for complex scenes. Following the two-step (layout-image) generation process, a novel object-driven attentive image generator is proposed to synthesize salient objects by paying attention to the most relevant words in the text description and the pre-generated semantic layout. In addition, a new Fast R-CNN based object-wise discriminator is proposed to provide rich object-wise discrimination signals on whether the synthesized object matches the text description and the pre-generated layout. The proposed Obj-GAN significantly outperforms the previous state of the art in various metrics on the large-scale COCO benchmark, increasing the Inception score by 27% and decreasing the FID score by 11%. A thorough comparison between the traditional grid attention and the new object-driven attention is provided through analyzing their mechanisms and visualizing their attention layers, showing insights of how the proposed model generates complex scenes in high quality.",0
"The focus of our paper is on Object-driven Attentive Generative Adversarial Networks (Obj-GANs), which enable the creation of object-centered images for intricate settings. Our approach involves a two-step process of generating a layout and then synthesizing the image, with a novel object-driven attentive image generator utilized to create prominent objects by concentrating on key words from the text description and pre-generated semantic layout. Additionally, we propose a new object-wise discriminator based on Fast R-CNN to determine if the synthesized object matches the text description and pre-generated layout. Our Obj-GAN outperforms prior benchmarks on the COCO dataset, improving the Inception score by 27% and decreasing the FID score by 11%. We compare traditional grid attention with our new object-driven attention, analyzing their mechanisms and attention layers to demonstrate how our model generates high-quality images for complex scenes.",1
"We propose a rejection sampling scheme using the discriminator of a GAN to approximately correct errors in the GAN generator distribution. We show that under quite strict assumptions, this will allow us to recover the data distribution exactly. We then examine where those strict assumptions break down and design a practical algorithm - called Discriminator Rejection Sampling (DRS) - that can be used on real data-sets. Finally, we demonstrate the efficacy of DRS on a mixture of Gaussians and on the SAGAN model, state-of-the-art in the image generation task at the time of developing this work. On ImageNet, we train an improved baseline that increases the Inception Score from 52.52 to 62.36 and reduces the Frechet Inception Distance from 18.65 to 14.79. We then use DRS to further improve on this baseline, improving the Inception Score to 76.08 and the FID to 13.75.",0
"We propose utilizing the discriminator of a GAN for a rejection sampling scheme to correct errors in the GAN generator distribution. Our goal is to recover the data distribution exactly, which can be done under strict assumptions. However, we also investigate where these assumptions break down and have developed a practical algorithm, called Discriminator Rejection Sampling (DRS), that can be applied to real data-sets. We have tested the effectiveness of DRS on a mixture of Gaussians and the SAGAN model, which was state-of-the-art in image generation at the time of this research. By using DRS on an improved baseline trained on ImageNet, we were able to increase the Inception Score from 52.52 to 76.08 and decrease the Frechet Inception Distance from 18.65 to 13.75.",1
"Recent advances in conditional image generation tasks, such as image-to-image translation and image inpainting, are largely accounted to the success of conditional GAN models, which are often optimized by the joint use of the GAN loss with the reconstruction loss. However, we reveal that this training recipe shared by almost all existing methods causes one critical side effect: lack of diversity in output samples. In order to accomplish both training stability and multimodal output generation, we propose novel training schemes with a new set of losses named moment reconstruction losses that simply replace the reconstruction loss. We show that our approach is applicable to any conditional generation tasks by performing thorough experiments on image-to-image translation, super-resolution and image inpainting using Cityscapes and CelebA dataset. Quantitative evaluations also confirm that our methods achieve a great diversity in outputs while retaining or even improving the visual fidelity of generated samples.",0
"The success of recent advancements in tasks like image-to-image translation and image inpainting is largely due to the effectiveness of conditional GAN models. These models are often optimized using a combination of the GAN loss and reconstruction loss. However, this approach leads to a lack of diversity in output samples. To address this issue and achieve both training stability and multimodal output generation, we propose a new training scheme that uses moment reconstruction losses in place of the reconstruction loss. Our approach can be applied to any conditional generation task and we demonstrate its effectiveness through experiments on image-to-image translation, super-resolution, and image inpainting using Cityscapes and CelebA dataset. Our methods result in diverse outputs while maintaining or improving visual fidelity, as confirmed by quantitative evaluations.",1
"Deep neural networks, in particular convolutional neural networks, have become highly effective tools for compressing images and solving inverse problems including denoising, inpainting, and reconstruction from few and noisy measurements. This success can be attributed in part to their ability to represent and generate natural images well. Contrary to classical tools such as wavelets, image-generating deep neural networks have a large number of parameters---typically a multiple of their output dimension---and need to be trained on large datasets. In this paper, we propose an untrained simple image model, called the deep decoder, which is a deep neural network that can generate natural images from very few weight parameters. The deep decoder has a simple architecture with no convolutions and fewer weight parameters than the output dimensionality. This underparameterization enables the deep decoder to compress images into a concise set of network weights, which we show is on par with wavelet-based thresholding. Further, underparameterization provides a barrier to overfitting, allowing the deep decoder to have state-of-the-art performance for denoising. The deep decoder is simple in the sense that each layer has an identical structure that consists of only one upsampling unit, pixel-wise linear combination of channels, ReLU activation, and channelwise normalization. This simplicity makes the network amenable to theoretical analysis, and it sheds light on the aspects of neural networks that enable them to form effective signal representations.",0
"Convolutional neural networks (CNNs) are highly effective in compressing images and solving inverse problems, such as denoising, inpainting, and reconstructing from noisy measurements. Their success can be attributed to their ability to represent and generate natural images well. However, image-generating deep neural networks typically have a large number of parameters and require training on large datasets. To address this issue, we propose the deep decoder, an untrained simple image model that generates natural images using only a few weight parameters. Unlike CNNs, the deep decoder has a simple architecture without convolutions and fewer weight parameters than output dimensionality. This underparameterization allows for compressing images into a concise set of network weights, comparable to wavelet-based thresholding. Additionally, it provides a barrier to overfitting, resulting in state-of-the-art performance for denoising. The deep decoder's simplicity makes it amenable to theoretical analysis, providing insights into the aspects of neural networks that enable effective signal representations.",1
"Semi-supervised and unsupervised Generative Adversarial Networks (GAN)-based methods have been gaining popularity in anomaly detection task recently. However, GAN training is somewhat challenging and unstable. Inspired from previous work in GAN-based image generation, we introduce a GAN-based anomaly detection framework - Adversarial Dual Autoencoders (ADAE) - consists of two autoencoders as generator and discriminator to increase training stability. We also employ discriminator reconstruction error as anomaly score for better detection performance. Experiments across different datasets of varying complexity show strong evidence of a robust model that can be used in different scenarios, one of which is brain tumor detection.",0
"In recent times, there has been an increasing interest in the use of Semi-supervised and unsupervised Generative Adversarial Networks (GAN) in the task of anomaly detection. However, GAN training is a somewhat challenging and unstable process. In light of the previous work done in GAN-based image generation, we have developed an Anomaly Detection Framework - Adversarial Dual Autoencoders (ADAE) - that utilizes two autoencoders as generator and discriminator to enhance training stability. To improve detection performance, we also utilize discriminator reconstruction error as an anomaly score. Experiments conducted on various data sets with varying complexities demonstrate a robust model that can be used in different scenarios, including brain tumor detection.",1
"Image attribute transfer aims to change an input image to a target one with expected attributes, which has received significant attention in recent years. However, most of the existing methods lack the ability to de-correlate the target attributes and irrelevant information, i.e., the other attributes and background information, thus often suffering from blurs and artifacts. To address these issues, we propose a novel Attribute Manifold Encoding GAN (AME-GAN) for fully-featured attribute transfer, which can modify and adjust every detail in the images. Specifically, our method divides the input image into image attribute part and image background part on manifolds, which are controlled by attribute latent variables and background latent variables respectively. Through enforcing attribute latent variables to Gaussian distributions and background latent variables to uniform distributions respectively, the attribute transfer procedure becomes controllable and image generation is more photo-realistic. Furthermore, we adopt a conditional multi-scale discriminator to render accurate and high-quality target attribute images. Experimental results on three popular datasets demonstrate the superiority of our proposed method in both performances of the attribute transfer and image generation quality.",0
"In recent years, image attribute transfer has gained significant attention. Its purpose is to transform an input image into a target image with desired attributes. However, most existing methods fail to separate the target attributes from irrelevant information, including background and other attributes. This often results in blurs and artifacts. To address these issues, we present a new method called Attribute Manifold Encoding GAN (AME-GAN). It enables full-featured attribute transfer by modifying every detail in images. Our approach divides input images into image attribute and background parts on manifolds controlled by attribute and background latent variables respectively. By enforcing attribute latent variables to Gaussian distributions and background latent variables to uniform distributions, we make the attribute transfer controllable and generate more photo-realistic images. We also use a conditional multi-scale discriminator to produce accurate and high-quality target attribute images. Our proposed method outperforms existing methods in terms of both attribute transfer performance and image generation quality, as demonstrated by experiments on three popular datasets.",1
"Regularized autoencoders learn the latent codes, a structure with the regularization under the distribution, which enables them the capability to infer the latent codes given observations and generate new samples given the codes. However, they are sometimes ambiguous as they tend to produce reconstructions that are not necessarily faithful reproduction of the inputs. The main reason is to enforce the learned latent code distribution to match a prior distribution while the true distribution remains unknown. To improve the reconstruction quality and learn the latent space a manifold structure, this work present a novel approach using the adversarially approximated autoencoder (AAAE) to investigate the latent codes with adversarial approximation. Instead of regularizing the latent codes by penalizing on the distance between the distributions of the model and the target, AAAE learns the autoencoder flexibly and approximates the latent space with a simpler generator. The ratio is estimated using generative adversarial network (GAN) to enforce the similarity of the distributions. Additionally, the image space is regularized with an additional adversarial regularizer. The proposed approach unifies two deep generative models for both latent space inference and diverse generation. The learning scheme is realized without regularization on the latent codes, which also encourages faithful reconstruction. Extensive validation experiments on four real-world datasets demonstrate the superior performance of AAAE. In comparison to the state-of-the-art approaches, AAAE generates samples with better quality and shares the properties of regularized autoencoder with a nice latent manifold structure.",0
"The latent codes learned by regularized autoencoders have a structure that is regulated under a distribution, giving them the ability to infer latent codes from observations and generate new samples from codes. However, they can sometimes produce ambiguous reconstructions that do not accurately reproduce the inputs due to the enforcement of a prior distribution that may not match the true distribution. To improve reconstruction quality and learn a manifold structure in the latent space, this study proposes a novel approach using the adversarially approximated autoencoder (AAAE) to investigate latent codes with adversarial approximation. Instead of penalizing the distance between the model and target distributions to regularize the latent codes, the autoencoder is learned flexibly and the latent space is approximated with a simpler generator using a generative adversarial network (GAN) to enforce distribution similarity. Additionally, the image space is regularized with an adversarial regularizer. This approach unifies two deep generative models for both latent space inference and diverse generation, and does not require regularization on the latent codes, which encourages faithful reconstruction. Extensive experiments on four real-world datasets demonstrate the superior performance of AAAE, which generates high-quality samples and shares the properties of regularized autoencoders with a nice latent manifold structure.",1
"Age-Related Macular Degeneration (AMD) is an asymptomatic retinal disease which may result in loss of vision. There is limited access to high-quality relevant retinal images and poor understanding of the features defining sub-classes of this disease. Motivated by recent advances in machine learning we specifically explore the potential of generative modeling, using Generative Adversarial Networks (GANs) and style transferring, to facilitate clinical diagnosis and disease understanding by feature extraction. We design an analytic pipeline which first generates synthetic retinal images from clinical images; a subsequent verification step is applied. In the synthesizing step we merge GANs (DCGANs and WGANs architectures) and style transferring for the image generation, whereas the verified step controls the accuracy of the generated images. We find that the generated images contain sufficient pathological details to facilitate ophthalmologists' task of disease classification and in discovery of disease relevant features. In particular, our system predicts the drusen and geographic atrophy sub-classes of AMD. Furthermore, the performance using CFP images for GANs outperforms the classification based on using only the original clinical dataset. Our results are evaluated using existing classifier of retinal diseases and class activated maps, supporting the predictive power of the synthetic images and their utility for feature extraction. Our code examples are available online.",0
"The loss of vision caused by Age-Related Macular Degeneration (AMD) is not easily detected, but high-quality retinal images and understanding of the different types of the disease are limited. With the help of machine learning, Generative Adversarial Networks (GANs) and style transferring are explored to extract disease features and aid clinical diagnosis. A pipeline is designed to generate synthetic retinal images from clinical images and verify their accuracy. The combination of DCGANs and WGANs architectures with style transferring generates the images, while verification controls their accuracy. The generated images contain enough pathological details to classify AMD sub-classes and discover relevant features. The system predicts drusen and geographic atrophy sub-classes of AMD, with better performance using CFP images for GANs than the original clinical dataset. The results are evaluated using existing classifiers and class activated maps, proving the predictive power and utility of synthetic images for feature extraction. The code examples are available online.",1
"Generative adversarial nets (GANs) are widely used to learn the data sampling process and their performance may heavily depend on the loss functions, given a limited computational budget. This study revisits MMD-GAN that uses the maximum mean discrepancy (MMD) as the loss function for GAN and makes two contributions. First, we argue that the existing MMD loss function may discourage the learning of fine details in data as it attempts to contract the discriminator outputs of real data. To address this issue, we propose a repulsive loss function to actively learn the difference among the real data by simply rearranging the terms in MMD. Second, inspired by the hinge loss, we propose a bounded Gaussian kernel to stabilize the training of MMD-GAN with the repulsive loss function. The proposed methods are applied to the unsupervised image generation tasks on CIFAR-10, STL-10, CelebA, and LSUN bedroom datasets. Results show that the repulsive loss function significantly improves over the MMD loss at no additional computational cost and outperforms other representative loss functions. The proposed methods achieve an FID score of 16.21 on the CIFAR-10 dataset using a single DCGAN network and spectral normalization.",0
"The use of Generative adversarial nets (GANs) is widespread in learning the data sampling process. However, their performance may be influenced by the loss functions with a limited computational budget. This research studies the MMD-GAN, which utilizes the maximum mean discrepancy (MMD) as its loss function for GAN, and offers two contributions. Firstly, the current MMD loss function may hinder the learning of fine data details by contracting the discriminator outputs of real data. To overcome this issue, a repulsive loss function is proposed that actively learns the difference among the real data by rearranging the terms in MMD. Secondly, a bounded Gaussian kernel is proposed, inspired by the hinge loss, to stabilize the MMD-GAN training with the repulsive loss function. These methods are applied to the unsupervised image generation tasks on numerous datasets, including CIFAR-10, STL-10, CelebA, and LSUN bedroom datasets. Results demonstrate that the repulsive loss function improves the MMD loss without any additional computational costs and outperforms other representative loss functions. The proposed methods achieve an FID score of 16.21 on the CIFAR-10 dataset using a single DCGAN network and spectral normalization.",1
"The Generative Adversarial Network (GAN) has recently been applied to generate synthetic images from text. Despite significant advances, most current state-of-the-art algorithms are regular-grid region based; when attention is used, it is mainly applied between individual regular-grid regions and a word. These approaches are sufficient to generate images that contain a single object in its foreground, such as a ""bird"" or ""flower"". However, natural languages often involve complex foreground objects and the background may also constitute a variable portion of the generated image. Therefore, the regular-grid based image attention weights may not necessarily concentrate on the intended foreground region(s), which in turn, results in an unnatural looking image. Additionally, individual words such as ""a"", ""blue"" and ""shirt"" do not necessarily provide a full visual context unless they are applied together. For this reason, in our paper, we proposed a novel method in which we introduced an additional set of attentions between true-grid regions and word phrases. The true-grid region is derived using a set of auxiliary bounding boxes. These auxiliary bounding boxes serve as superior location indicators to where the alignment and attention should be drawn with the word phrases. Word phrases are derived from analysing Part-of-Speech (POS) results. We perform experiments on this novel network architecture using the Microsoft Common Objects in Context (MSCOCO) dataset and the model generates $256 \times 256$ conditioned on a short sentence description. Our proposed approach is capable of generating more realistic images compared with the current state-of-the-art algorithms.",0
"Recently, the Generative Adversarial Network (GAN) has been utilized to create synthetic images based on text. However, current state-of-the-art methods are typically limited to regular-grid region-based processes, with attention being applied mainly between individual regions and words. While this approach is useful for generating images with a single object in the foreground, it proves insufficient for natural language processing, as it fails to account for complex foreground objects and varying background portions. Consequently, the regular-grid based attention weights may not concentrate on the intended foreground regions, leading to unnatural images. Additionally, individual words do not always provide a complete visual context unless used in conjunction with others. Hence, we introduce a new method that includes additional attention sets between true-grid regions and word phrases. The true-grid regions are determined by auxiliary bounding boxes, which serve as superior location indicators for alignment and attention with word phrases. We analyze Part-of-Speech (POS) results to derive word phrases and test our proposed approach on the Microsoft Common Objects in Context (MSCOCO) dataset. Our model generates $256 \times 256$ images based on short sentence descriptions and produces more realistic results than current state-of-the-art methods.",1
"We describe TF-Replicator, a framework for distributed machine learning designed for DeepMind researchers and implemented as an abstraction over TensorFlow. TF-Replicator simplifies writing data-parallel and model-parallel research code. The same models can be effortlessly deployed to different cluster architectures (i.e. one or many machines containing CPUs, GPUs or TPU accelerators) using synchronous or asynchronous training regimes. To demonstrate the generality and scalability of TF-Replicator, we implement and benchmark three very different models: (1) A ResNet-50 for ImageNet classification, (2) a SN-GAN for class-conditional ImageNet image generation, and (3) a D4PG reinforcement learning agent for continuous control. Our results show strong scalability performance without demanding any distributed systems expertise of the user. The TF-Replicator programming model will be open-sourced as part of TensorFlow 2.0 (see https://github.com/tensorflow/community/pull/25).",0
"TF-Replicator is a distributed machine learning framework created for DeepMind researchers and developed as a TensorFlow abstraction. It streamlines the production of data-parallel and model-parallel research code, allowing for easy deployment of the same models to various cluster architectures with synchronous or asynchronous training methods. To demonstrate its versatility and scalability, we tested TF-Replicator with three distinct models: ResNet-50 for ImageNet classification, SN-GAN for class-conditional ImageNet image creation, and D4PG reinforcement learning agent for continuous control. Our experiments indicate strong scalability performance, and the TF-Replicator programming model will be available as part of TensorFlow 2.0, which can be found at https://github.com/tensorflow/community/pull/25. Users do not require extensive knowledge of distributed systems to utilize TF-Replicator.",1
"Digitally retouching images has become a popular trend, with people posting altered images on social media and even magazines posting flawless facial images of celebrities. Further, with advancements in Generative Adversarial Networks (GANs), now changing attributes and retouching have become very easy. Such synthetic alterations have adverse effect on face recognition algorithms. While researchers have proposed to detect image tampering, detecting GANs generated images has still not been explored. This paper proposes a supervised deep learning algorithm using Convolutional Neural Networks (CNNs) to detect synthetically altered images. The algorithm yields an accuracy of 99.65% on detecting retouching on the ND-IIITD dataset. It outperforms the previous state of the art which reported an accuracy of 87% on the database. For distinguishing between real images and images generated using GANs, the proposed algorithm yields an accuracy of 99.83%.",0
"The act of digitally altering images has become a popular practice, as evidenced by the prevalence of edited images on social media and magazines showcasing celebrities with perfect facial features. With the rise of Generative Adversarial Networks (GANs), changing and retouching images has become much easier. However, these synthetic alterations can negatively impact face recognition algorithms. While some researchers have proposed ways to detect image tampering, detecting GANs-generated images remains unexplored. This paper introduces a supervised deep learning algorithm that uses Convolutional Neural Networks (CNNs) to detect synthetically altered images. The algorithm achieves an accuracy of 99.65% in detecting retouching on the ND-IIITD dataset, surpassing the previous state-of-the-art accuracy of 87%. For distinguishing between real images and GANs-generated images, the proposed algorithm achieves an accuracy of 99.83%.",1
"Learning disentangled representations from visual data, where different high-level generative factors are independently encoded, is of importance for many computer vision tasks. Solving this problem, however, typically requires to explicitly label all the factors of interest in training images. To alleviate the annotation cost, we introduce a learning setting which we refer to as ""reference-based disentangling"". Given a pool of unlabeled images, the goal is to learn a representation where a set of target factors are disentangled from others. The only supervision comes from an auxiliary ""reference set"" containing images where the factors of interest are constant. In order to address this problem, we propose reference-based variational autoencoders, a novel deep generative model designed to exploit the weak-supervision provided by the reference set. By addressing tasks such as feature learning, conditional image generation or attribute transfer, we validate the ability of the proposed model to learn disentangled representations from this minimal form of supervision.",0
"The acquisition of disentangled representations from visual data, which allows for independent encoding of various high-level generative factors, is crucial for several computer vision tasks. However, it is usually necessary to explicitly label all the relevant factors of interest in training images, making this a challenging issue. To minimize the cost of annotation, we propose a new learning approach called ""reference-based disentangling"". The aim is to acquire a representation that disentangles a set of target factors from others, using only an auxiliary ""reference set"" of images where the factors of interest remain constant. To tackle this problem, we introduce reference-based variational autoencoders, a novel deep generative model that leverages the weak supervision provided by the reference set. We demonstrate the effectiveness of our proposed model in tasks such as feature learning, attribute transfer, and conditional image generation, highlighting its ability to acquire disentangled representations using minimal supervision.",1
"In this paper, we present a generative adversarial network framework that generates compressed images instead of synthesizing raw RGB images and compressing them separately. In the real world, most images and videos are stored and transferred in a compressed format to save storage capacity and data transfer bandwidth. However, since typical generative adversarial networks generate raw RGB images, those generated images need to be compressed by a post-processing stage to reduce the data size. Among image compression methods, JPEG has been one of the most commonly used lossy compression methods for still images. Hence, we propose a novel framework that generates JPEG compressed images using generative adversarial networks. The novel generator consists of the proposed locally connected layers, chroma subsampling layers, quantization layers, residual blocks, and convolution layers. The locally connected layer is proposed to enable block-based operations. We also discuss training strategies for the proposed architecture including the loss function and the transformation between its generator and its discriminator. The proposed method is evaluated using the publicly available CIFAR-10 dataset and LSUN bedroom dataset. The results demonstrate that the proposed method is able to generate compressed data with competitive qualities. The proposed method is a promising baseline method for joint image generation and compression using generative adversarial networks.",0
"This paper introduces a new framework for generative adversarial networks that generates compressed images instead of raw RGB images that need to be compressed separately. Most real-world images and videos are already stored and transferred in compressed formats to save storage capacity and data transfer bandwidth. However, traditional generative adversarial networks generate raw RGB images, requiring a post-processing stage for compression. To address this, the proposed method generates JPEG compressed images using generative adversarial networks and includes locally connected layers, chroma subsampling layers, quantization layers, residual blocks, and convolution layers. The locally connected layer facilitates block-based operations, and training strategies are discussed, including the loss function and the transformation between the generator and discriminator. The method is evaluated using the CIFAR-10 and LSUN bedroom datasets, demonstrating competitive results. Overall, this method shows promise as a baseline approach for joint image generation and compression using generative adversarial networks.",1
"In this paper, we propose the use of a semantic image, an improved representation for video analysis, principally in combination with Inception networks. The semantic image is obtained by applying localized sparse segmentation using global clustering (LSSGC) prior to the approximate rank pooling which summarizes the motion characteristics in single or multiple images. It incorporates the background information by overlaying a static background from the window onto the subsequent segmented frames. The idea is to improve the action-motion dynamics by focusing on the region which is important for action recognition and encoding the temporal variances using the frame ranking method. We also propose the sequential combination of Inception-ResNetv2 and long-short-term memory network (LSTM) to leverage the temporal variances for improved recognition performance. Extensive analysis has been carried out on UCF101 and HMDB51 datasets which are widely used in action recognition studies. We show that (i) the semantic image generates better activations and converges faster than its original variant, (ii) using segmentation prior to approximate rank pooling yields better recognition performance, (iii) The use of LSTM leverages the temporal variance information from approximate rank pooling to model the action behavior better than the base network, (iv) the proposed representations can be adaptive as they can be used with existing methods such as temporal segment networks to improve the recognition performance, and (v) our proposed four-stream network architecture comprising of semantic images and semantic optical flows achieves state-of-the-art performance, 95.9% and 73.5% recognition accuracy on UCF101 and HMDB51, respectively.",0
"The aim of this study is to introduce a semantic image as a more effective representation for video analysis when used in conjunction with Inception networks. The semantic image is created by applying localized sparse segmentation using global clustering (LSSGC) prior to approximate rank pooling, which summarizes motion characteristics in single or multiple images. Furthermore, it includes background information by overlaying a static background onto subsequent segmented frames. The goal is to enhance action-motion dynamics by focusing on significant regions for action recognition and encoding temporal variances using the frame ranking method. Additionally, we propose combining Inception-ResNetv2 and long-short-term memory network (LSTM) to leverage temporal variances for better recognition performance. Our analysis shows that the semantic image generates better activations and converges faster than the original variant, segmentation prior to approximate rank pooling improves recognition performance, and LSTM enhances the action behavior modeling. These representations are adaptable and can be used with existing methods to improve recognition performance. Our four-stream network architecture, which includes semantic images and semantic optical flows, achieves state-of-the-art performance, with recognition accuracy of 95.9% and 73.5% on UCF101 and HMDB51, respectively.",1
"Generative Adversarial Networks (GAN) receive great attentions recently due to its excellent performance in image generation, transformation, and super-resolution. However, GAN has rarely been studied and trained for classification, leading that the generated images may not be appropriate for classification. In this paper, we propose a novel Generative Adversarial Classifier (GAC) particularly for low-resolution Handwriting Character Recognition. Specifically, involving additionally a classifier in the training process of normal GANs, GAC is calibrated for learning suitable structures and restored characters images that benefits the classification. Experimental results show that our proposed method can achieve remarkable performance in handwriting characters 8x super-resolution, approximately 10% and 20% higher than the present state-of-the-art methods respectively on benchmark data CASIA-HWDB1.1 and MNIST.",0
"Recently, there has been a lot of attention on Generative Adversarial Networks (GAN) due to their exceptional performance in generating, transforming, and super-resolving images. However, GANs have not been extensively studied and trained for classification, which results in the generated images being unsuitable for classification purposes. This paper proposes a new method, the Generative Adversarial Classifier (GAC), specifically for low-resolution Handwriting Character Recognition. By incorporating a classifier into the training process of standard GANs, GAC learns appropriate structures and restores character images that benefit classification. Experimental results demonstrate that our proposed method achieves remarkable performance in 8x super-resolution handwriting characters, approximately 10% and 20% higher than the current state-of-the-art methods on benchmark data CASIA-HWDB1.1 and MNIST, respectively.",1
"We present a novel approach to image manipulation and understanding by simultaneously learning to segment object masks, paste objects to another background image, and remove them from original images. For this purpose, we develop a novel generative model for compositional image generation, SEIGAN (Segment-Enhance-Inpaint Generative Adversarial Network), which learns these three operations together in an adversarial architecture with additional cycle consistency losses. To train, SEIGAN needs only bounding box supervision and does not require pairing or ground truth masks. SEIGAN produces better generated images (evaluated by human assessors) than other approaches and produces high-quality segmentation masks, improving over other adversarially trained approaches and getting closer to the results of fully supervised training.",0
"Our study introduces a fresh technique for comprehending and manipulating images. This involves learning to segment object masks, transfer objects onto a different backdrop, and remove them from their original context. Our innovative generative model, called SEIGAN (Segment-Enhance-Inpaint Generative Adversarial Network), achieves this by training in an adversarial architecture with added cycle consistency losses. SEIGAN only needs bounding box supervision to function and doesn't require pairing or ground truth masks. Our evaluation shows that SEIGAN outperforms other methods with superior image quality and segmentation masks, bringing it closer to fully supervised training outcomes.",1
"Generating realistic images from scene graphs asks neural networks to be able to reason about object relationships and compositionality. As a relatively new task, how to properly ensure the generated images comply with scene graphs or how to measure task performance remains an open question. In this paper, we propose to harness scene graph context to improve image generation from scene graphs. We introduce a scene graph context network that pools features generated by a graph convolutional neural network that are then provided to both the image generation network and the adversarial loss. With the context network, our model is trained to not only generate realistic looking images, but also to better preserve non-spatial object relationships. We also define two novel evaluation metrics, the relation score and the mean opinion relation score, for this task that directly evaluate scene graph compliance. We use both quantitative and qualitative studies to demonstrate that our pro-posed model outperforms the state-of-the-art on this challenging task.",0
"The task of generating lifelike images from scene graphs necessitates neural networks to possess the ability to reason about object relationships and compositionality. However, ensuring that the produced images comply with scene graphs and measuring task performance remains an open question. To address this issue, we propose utilizing scene graph context to enhance image generation from scene graphs. Our approach involves a scene graph context network that combines features generated by a graph convolutional neural network and provides them to both the image generation network and the adversarial loss. By incorporating the context network, our model learns to not only generate realistic images but also preserve non-spatial object relationships. Additionally, we introduce two novel evaluation metrics, relation score and mean opinion relation score, to directly assess scene graph compliance. Through both quantitative and qualitative studies, we demonstrate that our proposed model surpasses the current state-of-the-art on this challenging task.",1
"The primary motivation of Image-to-Image Transformation is to convert an image of one domain to another domain. Most of the research has been focused on the task of image transformation for a set of pre-defined domains. Very few works are reported that actually developed a common framework for image-to-image transformation for different domains. With the introduction of Generative Adversarial Networks (GANs) as a general framework for the image generation problem, there is a tremendous growth in the area of image-to-image transformation. Most of the research focuses over the suitable objective function for image-to-image transformation. In this paper, we propose a new Cyclic-Synthesized Generative Adversarial Networks (CSGAN) for image-to-image transformation. The proposed CSGAN uses a new objective function (loss) called Cyclic-Synthesized Loss (CS) between the synthesized image of one domain and cycled image of another domain. The performance of the proposed CSGAN is evaluated on two benchmark image-to-image transformation datasets, including CUHK Face dataset and CMP Facades dataset. The results are computed using the widely used evaluation metrics such as MSE, SSIM, PSNR, and LPIPS. The experimental results of the proposed CSGAN approach are compared with the latest state-of-the-art approaches such as GAN, Pix2Pix, DualGAN, CycleGAN and PS2GAN. The proposed CSGAN technique outperforms all the methods over CUHK dataset and exhibits the promising and comparable performance over Facades dataset in terms of both qualitative and quantitative measures. The code is available at https://github.com/KishanKancharagunta/CSGAN.",0
"The main aim of Image-to-Image Transformation is to convert an image from one domain to another. Most research has concentrated on transforming images for a particular set of domains, with few studies developing a universal framework. However, the advent of Generative Adversarial Networks (GANs) has led to a significant increase in research into image-to-image transformation, with most studies focusing on finding the most appropriate objective function. This paper proposes a new model called Cyclic-Synthesized Generative Adversarial Networks (CSGAN) for image-to-image transformation, which employs a novel objective function (loss) known as Cyclic-Synthesized Loss (CS) to compare the synthesized image of one domain with the cycled image of another domain. The proposed CSGAN was evaluated on two benchmark datasets: CUHK Face and CMP Facades, and its performance was compared with other state-of-the-art approaches such as GAN, Pix2Pix, DualGAN, CycleGAN and PS2GAN. The results show that CSGAN outperforms all other methods on the CUHK dataset and performs comparably on the Facades dataset in terms of both quantitative and qualitative measures. The code for CSGAN is available at https://github.com/KishanKancharagunta/CSGAN.",1
"Pseudo healthy synthesis, i.e. the creation of a subject-specific `healthy' image from a pathological one, could be helpful in tasks such as anomaly detection, understanding changes induced by pathology and disease or even as data augmentation. We treat this task as a factor decomposition problem: we aim to separate what appears to be healthy and where disease is (as a map). The two factors are then recombined (by a network) to reconstruct the input disease image. We train our models in an adversarial way using either paired or unpaired settings, where we pair disease images and maps (as segmentation masks) when available. We quantitatively evaluate the quality of pseudo healthy images. We show in a series of experiments, performed in ISLES and BraTS datasets, that our method is better than conditional GAN and CycleGAN, highlighting challenges in using adversarial methods in the image translation task of pseudo healthy image generation.",0
"The creation of a subject-specific ""healthy"" image from a pathological one, known as pseudo healthy synthesis, could have various applications such as detecting anomalies, understanding changes caused by pathology and disease, and enhancing data. To achieve this, we treat the task as a factor decomposition problem and aim to separate healthy and diseased areas on a map which are then recombined using a network to reconstruct the input disease image. The models are trained adversarially using paired or unpaired settings, with disease images and maps paired when available. We quantitatively assess the quality of pseudo healthy images and demonstrate through experiments on ISLES and BraTS datasets that our approach outperforms conditional GAN and CycleGAN. Our findings also highlight the challenges involved in using adversarial methods for pseudo healthy image generation.",1
"Current wisdom suggests more labeled image data is always better, and obtaining labels is the bottleneck. Yet curating a pool of sufficiently diverse and informative images is itself a challenge. In particular, training image curation is problematic for fine-grained attributes, where the subtle visual differences of interest may be rare within traditional image sources. We propose an active image generation approach to address this issue. The main idea is to jointly learn the attribute ranking task while also learning to generate novel realistic image samples that will benefit that task. We introduce an end-to-end framework that dynamically ""imagines"" image pairs that would confuse the current model, presents them to human annotators for labeling, then improves the predictive model with the new examples. With results on two datasets, we show that by thinking outside the pool of real images, our approach gains generalization accuracy for challenging fine-grained attribute comparisons.",0
"It is commonly believed that having more labeled image data is always beneficial, but the process of obtaining labels can be a bottleneck. However, creating a diverse and informative pool of images for training can also be difficult, especially for fine-grained attributes where subtle visual differences are rare in traditional image sources. To combat this issue, we propose an active image generation approach that simultaneously learns the ranking task for attributes and generates new, realistic image samples to improve the task. Our end-to-end framework generates image pairs that confuse the current model, obtains labels from human annotators, and enhances the predictive model with the new examples. Through our experiments on two datasets, we demonstrate that our approach improves the generalization accuracy for challenging fine-grained attribute comparisons by generating novel images outside the pool of real images.",1
"Generative Adversarial Networks (GAN) boast impressive capacity to generate realistic images. However, like much of the field of deep learning, they require an inordinate amount of data to produce results, thereby limiting their usefulness in generating novelty. In the same vein, recent advances in meta-learning have opened the door to many few-shot learning applications. In the present work, we propose Few-shot Image Generation using Reptile (FIGR), a GAN meta-trained with Reptile. Our model successfully generates novel images on both MNIST and Omniglot with as little as 4 images from an unseen class. We further contribute FIGR-8, a new dataset for few-shot image generation, which contains 1,548,944 icons categorized in over 18,409 classes. Trained on FIGR-8, initial results show that our model can generalize to more advanced concepts (such as ""bird"" and ""knife"") from as few as 8 samples from a previously unseen class of images and as little as 10 training steps through those 8 images. This work demonstrates the potential of training a GAN for few-shot image generation and aims to set a new benchmark for future work in the domain.",0
"Although Generative Adversarial Networks (GAN) are capable of producing lifelike images, they require a vast amount of data to generate novel results, restricting their practicality. However, recent advancements in meta-learning have enabled the development of many few-shot learning applications. Therefore, this study introduces Few-shot Image Generation using Reptile (FIGR), a GAN that is meta-trained with Reptile, which can produce unique images with just four unseen class images. Additionally, FIGR-8, a dataset consisting of 1,548,944 icons categorized in over 18,409 classes, is presented, and the model's ability to generalize to more complex concepts with as few as eight samples from a previously unseen class is demonstrated. This research showcases the potential of training a GAN for few-shot image generation and sets a new benchmark for future studies in this field.",1
"Generative modeling over natural images is one of the most fundamental machine learning problems. However, few modern generative models, including Wasserstein Generative Adversarial Nets (WGANs), are studied on manifold-valued images that are frequently encountered in real-world applications. To fill the gap, this paper first formulates the problem of generating manifold-valued images and exploits three typical instances: hue-saturation-value (HSV) color image generation, chromaticity-brightness (CB) color image generation, and diffusion-tensor (DT) image generation. For the proposed generative modeling problem, we then introduce a theorem of optimal transport to derive a new Wasserstein distance of data distributions on complete manifolds, enabling us to achieve a tractable objective under the WGAN framework. In addition, we recommend three benchmark datasets that are CIFAR-10 HSV/CB color images, ImageNet HSV/CB color images, UCL DT image datasets. On the three datasets, we experimentally demonstrate the proposed manifold-aware WGAN model can generate more plausible manifold-valued images than its competitors.",0
"Generating natural images through generative modeling is a fundamental problem in machine learning. However, current generative models, such as Wasserstein Generative Adversarial Nets (WGANs), are not often studied in relation to manifold-valued images that are commonly encountered in real-world scenarios. To address this issue, this paper presents a formulation for generating manifold-valued images and explores three specific examples: hue-saturation-value (HSV) color image generation, chromaticity-brightness (CB) color image generation, and diffusion-tensor (DT) image generation. To solve this generative modeling problem, we introduce a new Wasserstein distance using a theorem of optimal transport, which allows us to achieve a feasible objective within the WGAN framework. We also recommend three benchmark datasets, including CIFAR-10 HSV/CB color images, ImageNet HSV/CB color images, and UCL DT image datasets. Our experiments demonstrate that the proposed manifold-aware WGAN model can generate more realistic manifold-valued images compared to other models.",1
"Recent improvements to Generative Adversarial Networks (GANs) have made it possible to generate realistic images in high resolution based on natural language descriptions such as image captions. Furthermore, conditional GANs allow us to control the image generation process through labels or even natural language descriptions. However, fine-grained control of the image layout, i.e. where in the image specific objects should be located, is still difficult to achieve. This is especially true for images that should contain multiple distinct objects at different spatial locations. We introduce a new approach which allows us to control the location of arbitrarily many objects within an image by adding an object pathway to both the generator and the discriminator. Our approach does not need a detailed semantic layout but only bounding boxes and the respective labels of the desired objects are needed. The object pathway focuses solely on the individual objects and is iteratively applied at the locations specified by the bounding boxes. The global pathway focuses on the image background and the general image layout. We perform experiments on the Multi-MNIST, CLEVR, and the more complex MS-COCO data set. Our experiments show that through the use of the object pathway we can control object locations within images and can model complex scenes with multiple objects at various locations. We further show that the object pathway focuses on the individual objects and learns features relevant for these, while the global pathway focuses on global image characteristics and the image background.",0
"Advancements in Generative Adversarial Networks (GANs) have enabled the generation of high-resolution images based on natural language descriptions such as image captions. Conditional GANs have also allowed for image generation control through labels or natural language descriptions. However, fine-grained control of image layout, specifically the placement of multiple objects at different locations within an image, remains challenging. To address this, we propose a new approach that includes an object pathway in both the generator and discriminator. This pathway only requires bounding boxes and labels, not a detailed semantic layout, to control the location of arbitrary objects within the image. The object pathway focuses on individual objects and is applied iteratively at specified bounding box locations, while the global pathway focuses on the overall image layout and background. We demonstrate the effectiveness of our approach on the Multi-MNIST, CLEVR, and MS-COCO datasets, showing that our object pathway can model complex scenes with multiple objects at various locations while the global pathway learns relevant features for the image background and global image characteristics.",1
"Joint image filters leverage the guidance image as a prior and transfer the structural details from the guidance image to the target image for suppressing noise or enhancing spatial resolution. Existing methods either rely on various explicit filter constructions or hand-designed objective functions, thereby making it difficult to understand, improve, and accelerate these filters in a coherent framework. In this paper, we propose a learning-based approach for constructing joint filters based on Convolutional Neural Networks. In contrast to existing methods that consider only the guidance image, the proposed algorithm can selectively transfer salient structures that are consistent with both guidance and target images. We show that the model trained on a certain type of data, e.g., RGB and depth images, generalizes well to other modalities, e.g., flash/non-Flash and RGB/NIR images. We validate the effectiveness of the proposed joint filter through extensive experimental evaluations with state-of-the-art methods.",0
"Joint image filters use a guidance image as a reference to transfer structural details to the target image, either to reduce noise or improve spatial resolution. However, current methods for constructing these filters rely on explicit filter constructions or hand-designed objective functions, which makes them difficult to understand, improve and accelerate in a coherent manner. This paper proposes a learning-based approach for constructing joint filters that uses Convolutional Neural Networks. Unlike existing methods that only consider the guidance image, this algorithm selectively transfers salient structures that are consistent with both the guidance and target images. Furthermore, the study shows that the model trained on a specific type of data can generalize well to other modalities. The proposed joint filter is validated through extensive experimental evaluations with state-of-the-art methods.",1
"Which generative model is the most suitable for Continual Learning? This paper aims at evaluating and comparing generative models on disjoint sequential image generation tasks. We investigate how several models learn and forget, considering various strategies: rehearsal, regularization, generative replay and fine-tuning. We used two quantitative metrics to estimate the generation quality and memory ability. We experiment with sequential tasks on three commonly used benchmarks for Continual Learning (MNIST, Fashion MNIST and CIFAR10). We found that among all models, the original GAN performs best and among Continual Learning strategies, generative replay outperforms all other methods. Even if we found satisfactory combinations on MNIST and Fashion MNIST, training generative models sequentially on CIFAR10 is particularly instable, and remains a challenge. Our code is available online \footnote{\url{https://github.com/TLESORT/Generative\_Continual\_Learning}}.",0
"The aim of this paper is to evaluate and compare various generative models for their suitability in Continual Learning, specifically in disjoint sequential image generation tasks. The study explores the learning and forgetting capabilities of several models, including rehearsal, regularization, generative replay, and fine-tuning, and employs two quantitative metrics to estimate generation quality and memory ability. The study conducts experiments on three widely used Continual Learning benchmarks (MNIST, Fashion MNIST, and CIFAR10). Results indicate that the original GAN model performs best overall, and generative replay is the most effective Continual Learning strategy. However, the study notes that training generative models sequentially on CIFAR10 is unstable and presents a challenge. The research code is publicly available at the following link: \footnote{\url{https://github.com/TLESORT/Generative\_Continual\_Learning}}.",1
"Unconditional image generation has recently been dominated by generative adversarial networks (GANs). GAN methods train a generator which regresses images from random noise vectors, as well as a discriminator that attempts to differentiate between the generated images and a training set of real images. GANs have shown amazing results at generating realistic looking images. Despite their success, GANs suffer from critical drawbacks including: unstable training and mode-dropping. The weaknesses in GANs have motivated research into alternatives including: variational auto-encoders (VAEs), latent embedding learning methods (e.g. GLO) and nearest-neighbor based implicit maximum likelihood estimation (IMLE). Unfortunately at the moment, GANs still significantly outperform the alternative methods for image generation. In this work, we present a novel method - Generative Latent Nearest Neighbors (GLANN) - for training generative models without adversarial training. GLANN combines the strengths of IMLE and GLO in a way that overcomes the main drawbacks of each method. Consequently, GLANN generates images that are far better than GLO and IMLE. Our method does not suffer from mode collapse which plagues GAN training and is much more stable. Qualitative results show that GLANN outperforms a baseline consisting of 800 GANs and VAEs on commonly used datasets. Our models are also shown to be effective for training truly non-adversarial unsupervised image translation.",0
"Generative adversarial networks (GANs) have become the dominant method for unconditional image generation. They involve training a generator to create images from random noise vectors, and a discriminator to differentiate between the generated images and a set of real images. While GANs have been successful in generating realistic images, they suffer from issues such as unstable training and mode-dropping. This has led researchers to explore alternative methods such as variational auto-encoders (VAEs), GLO, and IMLE. However, GANs still outperform these alternatives. In this study, we propose a new method called Generative Latent Nearest Neighbors (GLANN), which combines the strengths of IMLE and GLO to overcome their limitations. GLANN produces high-quality images without suffering from mode collapse, making it more stable than GANs. Our qualitative results show that GLANN outperforms a baseline of 800 GANs and VAEs on commonly used datasets. Additionally, our models are effective for non-adversarial unsupervised image translation.",1
"We propose a method for learning landmark detectors for visual objects (such as the eyes and the nose in a face) without any manual supervision. We cast this as the problem of generating images that combine the appearance of the object as seen in a first example image with the geometry of the object as seen in a second example image, where the two examples differ by a viewpoint change and/or an object deformation. In order to factorize appearance and geometry, we introduce a tight bottleneck in the geometry-extraction process that selects and distils geometry-related features. Compared to standard image generation problems, which often use generative adversarial networks, our generation task is conditioned on both appearance and geometry and thus is significantly less ambiguous, to the point that adopting a simple perceptual loss formulation is sufficient. We demonstrate that our approach can learn object landmarks from synthetic image deformations or videos, all without manual supervision, while outperforming state-of-the-art unsupervised landmark detectors. We further show that our method is applicable to a large variety of datasets - faces, people, 3D objects, and digits - without any modifications.",0
"Our proposed approach for learning landmark detectors for visual objects, such as facial features like the eyes and nose, does not require any manual supervision. The technique involves generating images that combine the appearance of the object from one example image with the object's geometry from another example image, where the two examples differ due to a viewpoint change and/or an object deformation. To separate appearance and geometry, we introduce a narrow bottleneck in the geometry-extraction process that selects and distills geometry-related features. Unlike standard image generation tasks that use generative adversarial networks, our generation task relies on both appearance and geometry, making it less ambiguous. We achieve this by adopting a simple perceptual loss formulation. Our method can learn object landmarks from synthetic image deformations or videos without manual supervision, and it outperforms state-of-the-art unsupervised landmark detectors. Additionally, our approach is applicable to various datasets like faces, people, 3D objects, and digits without any modifications.",1
"We present an effective post-processing method to reduce the artifacts from sparsely reconstructed cone-beam CT (CBCT) images. The proposed method is based on the state-of-the-art, image-to-image generative models with a perceptual loss as regulation. Unlike the traditional CT artifact-reduction approaches, our method is trained in an adversarial fashion that yields more perceptually realistic outputs while preserving the anatomical structures. To address the streak artifacts that are inherently local and appear across various scales, we further propose a novel discriminator architecture based on feature pyramid networks and a differentially modulated focus map to induce the adversarial training. Our experimental results show that the proposed method can greatly correct the cone-beam artifacts from clinical CBCT images reconstructed using 1/3 projections, and outperforms strong baseline methods both quantitatively and qualitatively.",0
"We introduce a post-processing technique that effectively reduces artifacts present in sparsely reconstructed cone-beam CT (CBCT) images. The approach utilizes state-of-the-art image-to-image generative models with a perceptual loss as regulation. Our method differs from conventional CT artifact-reduction methods as it is trained adversarially, resulting in outputs with enhanced perceptual realism while preserving anatomical structures. To combat streak artifacts, which are localized and present at various scales, we propose a novel discriminator architecture based on feature pyramid networks and differentially modulated focus maps to facilitate adversarial training. Our experiments demonstrate that the proposed method significantly reduces cone-beam artifacts in clinical CBCT images reconstructed with 1/3 projections, and outperforms strong baseline methods both quantitatively and qualitatively.",1
"Recent progress in deep generative models has led to tremendous breakthroughs in image generation. However, while existing models can synthesize photorealistic images, they lack an understanding of our underlying 3D world. We present a new generative model, Visual Object Networks (VON), synthesizing natural images of objects with a disentangled 3D representation. Inspired by classic graphics rendering pipelines, we unravel our image formation process into three conditionally independent factors---shape, viewpoint, and texture---and present an end-to-end adversarial learning framework that jointly models 3D shapes and 2D images. Our model first learns to synthesize 3D shapes that are indistinguishable from real shapes. It then renders the object's 2.5D sketches (i.e., silhouette and depth map) from its shape under a sampled viewpoint. Finally, it learns to add realistic texture to these 2.5D sketches to generate natural images. The VON not only generates images that are more realistic than state-of-the-art 2D image synthesis methods, but also enables many 3D operations such as changing the viewpoint of a generated image, editing of shape and texture, linear interpolation in texture and shape space, and transferring appearance across different objects and viewpoints.",0
"The advances made in deep generative models have resulted in significant strides in image generation. However, these models lack a comprehension of the underlying 3D world, despite their ability to create realistic images. This paper introduces a new generative model, named Visual Object Networks (VON), which produces natural object images utilizing a disentangled 3D representation. We were inspired by traditional graphics rendering pipelines, and we have broken down our image formation process into three independent factors, namely shape, viewpoint, and texture. We have employed an end-to-end adversarial learning framework to jointly model 3D shapes and 2D images. Our model first learns to synthesize 3D shapes that are indistinguishable from real-life shapes. It then renders the object's 2.5D sketches, including its silhouette and depth map, from its shape under a randomly chosen viewpoint. Lastly, it learns to add realistic texture to these 2.5D sketches to create natural images. The VON not only produces images that are more realistic than state-of-the-art 2D image synthesis techniques, but it also allows for various 3D operations, such as modifying the viewpoint of a generated image, editing the shape and texture, interpolating in texture and shape space, and transferring appearance across different objects and viewpoints.",1
"The extension of image generation to video generation turns out to be a very difficult task, since the temporal dimension of videos introduces an extra challenge during the generation process. Besides, due to the limitation of memory and training stability, the generation becomes increasingly challenging with the increase of the resolution/duration of videos. In this work, we exploit the idea of progressive growing of Generative Adversarial Networks (GANs) for higher resolution video generation. In particular, we begin to produce video samples of low-resolution and short-duration, and then progressively increase both resolution and duration alone (or jointly) by adding new spatiotemporal convolutional layers to the current networks. Starting from the learning on a very raw-level spatial appearance and temporal movement of the video distribution, the proposed progressive method learns spatiotemporal information incrementally to generate higher resolution videos. Furthermore, we introduce a sliced version of Wasserstein GAN (SWGAN) loss to improve the distribution learning on the video data of high-dimension and mixed-spatiotemporal distribution. SWGAN loss replaces the distance between joint distributions by that of one-dimensional marginal distributions, making the loss easier to compute. We evaluate the proposed model on our collected face video dataset of 10,900 videos to generate photorealistic face videos of 256x256x32 resolution. In addition, our model also reaches a record inception score of 14.57 in unsupervised action recognition dataset UCF-101.",0
"Generating videos from images poses a challenge due to the temporal component, which requires additional effort during the generation process. The task becomes increasingly difficult with higher resolution and longer duration due to memory and training limitations. To address this, we implemented a progressive approach using Generative Adversarial Networks (GANs) to generate videos of increasing resolution and duration. We began with low-resolution and short-duration samples, and added spatiotemporal convolutional layers to the network to incrementally learn spatiotemporal information. We also introduced a sliced version of Wasserstein GAN (SWGAN) loss to improve distribution learning on high-dimension and mixed-spatiotemporal data. Our model generated photorealistic face videos of 256x256x32 resolution and achieved a record inception score of 14.57 on the UCF-101 dataset for unsupervised action recognition.",1
"The task of generating natural images from 3D scenes has been a long standing goal in computer graphics. On the other hand, recent developments in deep neural networks allow for trainable models that can produce natural-looking images with little or no knowledge about the scene structure. While the generated images often consist of realistic looking local patterns, the overall structure of the generated images is often inconsistent. In this work we propose a trainable, geometry-aware image generation method that leverages various types of scene information, including geometry and segmentation, to create realistic looking natural images that match the desired scene structure. Our geometrically-consistent image synthesis method is a deep neural network, called Geometry to Image Synthesis (GIS) framework, which retains the advantages of a trainable method, e.g., differentiability and adaptiveness, but, at the same time, makes a step towards the generalizability, control and quality output of modern graphics rendering engines. We utilize the GIS framework to insert vehicles in outdoor driving scenes, as well as to generate novel views of objects from the Linemod dataset. We qualitatively show that our network is able to generalize beyond the training set to novel scene geometries, object shapes and segmentations. Furthermore, we quantitatively show that the GIS framework can be used to synthesize large amounts of training data which proves beneficial for training instance segmentation models.",0
"Generating natural images from 3D scenes has been a long-standing goal in computer graphics. However, recent advances in deep neural networks have enabled trainable models to produce realistic-looking images with little or no knowledge of the scene structure. Although these generated images often feature convincing local patterns, their overall structure can be inconsistent. In this study, we present a geometry-aware image generation method that utilizes various types of scene information, such as geometry and segmentation, to create natural-looking images that match the desired scene structure. Our method, known as the Geometry to Image Synthesis (GIS) framework, is a trainable deep neural network that offers differentiability, adaptiveness, generalizability, control, and high-quality output similar to modern graphics rendering engines. We use the GIS framework to insert vehicles in outdoor driving scenes and to generate novel views of objects from the Linemod dataset. Our results show that the GIS network can generalize beyond the training set to novel scene geometries, object shapes, and segmentations. Additionally, we demonstrate that the framework can synthesize large amounts of training data to train instance segmentation models effectively.",1
"We study in this paper the problems of both image captioning and text-to-image generation, and present a novel turbo learning approach to jointly training an image-to-text generator (a.k.a. CaptionBot) and a text-to-image generator (a.k.a. DrawingBot). The key idea behind the joint training is that image-to-text generation and text-to-image generation as dual problems can form a closed loop to provide informative feedback to each other. Based on such feedback, we introduce a new loss metric by comparing the original input with the output produced by the closed loop. In addition to the old loss metrics used in CaptionBot and DrawingBot, this extra loss metric makes the jointly trained CaptionBot and DrawingBot better than the separately trained CaptionBot and DrawingBot. Furthermore, the turbo-learning approach enables semi-supervised learning since the closed loop can provide pseudo-labels for unlabeled samples. Experimental results on the COCO dataset demonstrate that the proposed turbo learning can significantly improve the performance of both CaptionBot and DrawingBot by a large margin.",0
"This paper examines the challenges associated with image captioning and text-to-image generation. We propose a new turbo learning method that allows for the joint training of an image-to-text generator (CaptionBot) and a text-to-image generator (DrawingBot). By treating image-to-text and text-to-image generation as dual problems, we create a closed loop system that provides informative feedback to both generators. We introduce a novel loss metric that compares the original input with the output produced by the closed loop, which improves the performance of the jointly trained CaptionBot and DrawingBot. This approach also enables semi-supervised learning by providing pseudo-labels for unlabeled samples. Our experimental results using the COCO dataset show that the proposed turbo learning method significantly enhances the performance of both CaptionBot and DrawingBot.",1
"Conditional image generation is effective for diverse tasks including training data synthesis for learning-based computer vision. However, despite the recent advances in generative adversarial networks (GANs), it is still a challenging task to generate images with detailed conditioning on object shapes. Existing methods for conditional image generation use category labels and/or keypoints and are only give limited control over object categories. In this work, we present SCGAN, an architecture to generate images with a desired shape specified by an input normal map. The shape-conditioned image generation task is achieved by explicitly modeling the image appearance via a latent appearance vector. The network is trained using unpaired training samples of real images and rendered normal maps. This approach enables us to generate images of arbitrary object categories with the target shape and diverse image appearances. We show the effectiveness of our method through both qualitative and quantitative evaluation on training data generation tasks.",0
"Generating images with detailed object shapes remains a challenging task despite recent advances in generative adversarial networks (GANs). Existing methods for conditional image generation are limited in their control over object categories, relying on category labels and/or keypoints. In this study, we present SCGAN, an architecture that generates images with a desired shape specified by an input normal map. The network models image appearance via a latent appearance vector and is trained using unpaired real images and rendered normal maps. Our approach allows for the generation of images of any object category with the target shape and diverse appearances. We demonstrate the effectiveness of our method through qualitative and quantitative evaluation on training data generation tasks. Conditional image generation is effective for a range of tasks, including training data synthesis for learning-based computer vision.",1
"Person re-identification is to retrieval pedestrian images from no-overlap camera views detected by pedestrian detectors. Most existing person re-identification (re-ID) models often fail to generalize well from the source domain where the models are trained to a new target domain without labels, because of the bias between the source and target domain. This issue significantly limits the scalability and usability of the models in the real world. Providing a labeled source training set and an unlabeled target training set, the aim of this paper is to improve the generalization ability of re-ID models to the target domain. To this end, we propose an image generative network named identity preserving generative adversarial network (IPGAN). The proposed method has two excellent properties: 1) only a single model is employed to translate the labeled images from the source domain to the target camera domains in an unsupervised manner; 2) The identity information of images from the source domain is preserved before and after translation. Furthermore, we propose IBN-reID model for the person re-identification task. It has better generalization ability than baseline models, especially in the cases without any domain adaptation. The IBN-reID model is trained on the translated images by supervised methods. Experimental results on Market-1501 and DukeMTMC-reID show that the images generated by IPGAN are more suitable for cross-domain person re-identification. Very competitive re-ID accuracy is achieved by our method.",0
"The process of person re-identification involves retrieving pedestrian images from non-overlapping camera views that have been detected by pedestrian detectors. However, many existing person re-identification models struggle to generalize between different domains due to biases. This limits their effectiveness in real-world scenarios. This paper aims to address this issue by proposing a new method called identity preserving generative adversarial network (IPGAN) that can translate labeled images from the source domain to the target camera domains in an unsupervised way while preserving identity information. Additionally, the IBN-reID model is introduced for the person re-identification task, which has better generalization ability than baseline models. The IBN-reID model is trained on translated images by supervised methods. Our experiments on Market-1501 and DukeMTMC-reID demonstrate that IPGAN-generated images are more suitable for cross-domain person re-identification and achieve competitive accuracy.",1
"Human image generation is a very challenging task since it is affected by many factors. Many human image generation methods focus on generating human images conditioned on a given pose, while the generated backgrounds are often blurred.In this paper,we propose a novel Partition-Controlled GAN to generate human images according to target pose and background. Firstly, human poses in the given images are extracted, and foreground/background are partitioned for further use. Secondly, we extract and fuse appearance features, pose features and background features to generate the desired images. Experiments on Market-1501 and DeepFashion datasets show that our model not only generates realistic human images but also produce the human pose and background as we want. Extensive experiments on COCO and LIP datasets indicate the potential of our method.",0
"Generating human images is a difficult task due to various factors that affect the process. While many methods focus on generating human images based on a specific pose, the backgrounds in these images often lack clarity. Our paper introduces a new approach, the Partition-Controlled GAN, for generating human images that conform to the desired pose and background. The method involves extracting human poses from given images and partitioning the foreground and background for further use. By combining appearance, pose, and background features, we achieve the desired image output. Our model produces realistic human images while also accurately depicting the desired pose and background. We conducted experiments on various datasets, including Market-1501, DeepFashion, COCO, and LIP, which show the potential of our method.",1
"In this paper, we propose Generative Adversarial Network (GAN) architectures that use Capsule Networks for image-synthesis. Based on the principal of positional-equivariance of features, Capsule Network's ability to encode spatial relationships between the features of the image helps it become a more powerful critic in comparison to Convolutional Neural Networks (CNNs) used in current architectures for image synthesis. Our proposed GAN architectures learn the data manifold much faster and therefore, synthesize visually accurate images in significantly lesser number of training samples and training epochs in comparison to GANs and its variants that use CNNs. Apart from analyzing the quantitative results corresponding the images generated by different architectures, we also explore the reasons for the lower coverage and diversity explored by the GAN architectures that use CNN critics.",0
"The aim of this paper is to introduce new Generative Adversarial Network (GAN) architectures that employ Capsule Networks for the purpose of image-synthesis. The Capsule Network's positional-equivariance of features allows it to encode spatial relationships between image features, making it a more effective critic than the Convolutional Neural Networks (CNNs) found in current image-synthesis architectures. Our GAN architectures are able to learn the data manifold much faster, resulting in visually accurate images being produced with fewer training samples and epochs than GANs and their variants that use CNNs. In addition to analyzing the quantitative results of various architectures, we also investigate why GAN architectures with CNN critics exhibit lower coverage and diversity.",1
"Deep Convolutional Neural Networks (CNNs) have been one of the most influential recent developments in computer vision, particularly for categorization. There is an increasing demand for explainable AI as these systems are deployed in the real world. However, understanding the information represented and processed in CNNs remains in most cases challenging. Within this paper, we explore the use of new information theoretic techniques developed in the field of neuroscience to enable novel understanding of how a CNN represents information. We trained a 10-layer ResNet architecture to identify 2,000 face identities from 26M images generated using a rigorously controlled 3D face rendering model that produced variations of intrinsic (i.e. face morphology, gender, age, expression and ethnicity) and extrinsic factors (i.e. 3D pose, illumination, scale and 2D translation). With our methodology, we demonstrate that unlike human's network overgeneralizes face identities even with extreme changes of face shape, but it is more sensitive to changes of texture. To understand the processing of information underlying these counterintuitive properties, we visualize the features of shape and texture that the network processes to identify faces. Then, we shed a light into the inner workings of the black box and reveal how hidden layers represent these features and whether the representations are invariant to pose. We hope that our methodology will provide an additional valuable tool for interpretability of CNNs.",0
"Computer vision has been greatly impacted by the development of Deep Convolutional Neural Networks (CNNs), especially in the area of categorization. As these systems are increasingly used in the real world, there is a growing need for explainable AI. However, comprehending the information that is processed and represented in CNNs can be quite difficult. This study explores the use of information theoretic techniques from neuroscience to gain new understanding of how CNNs represent information. A 10-layer ResNet architecture was trained to identify 2,000 face identities from 26M images generated using a 3D face rendering model. This model produced variations of intrinsic and extrinsic factors. The results of this methodology show that the network overgeneralizes face identities even with extreme changes in face shape, but is more sensitive to changes in texture. To gain insight into the processing of information, the features of shape and texture that the network uses to identify faces were visualized. This study sheds light on the inner workings of the black box and reveals how hidden layers represent these features and whether the representations are invariant to pose. It is hoped that this methodology will provide an additional valuable tool for interpreting CNNs.",1
"Thanks to their remarkable generative capabilities, GANs have gained great popularity, and are used abundantly in state-of-the-art methods and applications. In a GAN based model, a discriminator is trained to learn the real data distribution. To date, it has been used only for training purposes, where it's utilized to train the generator to provide real-looking outputs. In this paper we propose a novel method that makes an explicit use of the discriminator in test-time, in a feedback manner in order to improve the generator results. To the best of our knowledge it is the first time a discriminator is involved in test-time. We claim that the discriminator holds significant information on the real data distribution, that could be useful for test-time as well, a potential that has not been explored before.   The approach we propose does not alter the conventional training stage. At test-time, however, it transfers the output from the generator into the discriminator, and uses feedback modules (convolutional blocks) to translate the features of the discriminator layers into corrections to the features of the generator layers, which are used eventually to get a better generator result. Our method can contribute to both conditional and unconditional GANs. As demonstrated by our experiments, it can improve the results of state-of-the-art networks for super-resolution, and image generation.",0
"GANs have become increasingly popular due to their impressive generative capabilities and are widely used in advanced methods and applications. Typically, a discriminator is trained in a GAN-based model to learn the distribution of real data for training purposes, which helps the generator produce more realistic outputs. However, our paper proposes a novel approach that involves using the discriminator explicitly during test-time in a feedback loop to enhance the generator results. This is the first time a discriminator has been utilized in this way, and we believe it holds valuable information about the real data distribution that has not been explored before. Our approach does not modify the conventional training stage but instead transfers the generator's output into the discriminator during test-time, using feedback modules to translate the discriminator's features into corrections for the generator's features. We have demonstrated that our method can improve both conditional and unconditional GANs, and our experiments have shown that it can enhance the performance of state-of-the-art networks for image generation and super-resolution.",1
"The broad set of deep generative models (DGMs) has achieved remarkable advances. However, it is often difficult to incorporate rich structured domain knowledge with the end-to-end DGMs. Posterior regularization (PR) offers a principled framework to impose structured constraints on probabilistic models, but has limited applicability to the diverse DGMs that can lack a Bayesian formulation or even explicit density evaluation. PR also requires constraints to be fully specified a priori, which is impractical or suboptimal for complex knowledge with learnable uncertain parts. In this paper, we establish mathematical correspondence between PR and reinforcement learning (RL), and, based on the connection, expand PR to learn constraints as the extrinsic reward in RL. The resulting algorithm is model-agnostic to apply to any DGMs, and is flexible to adapt arbitrary constraints with the model jointly. Experiments on human image generation and templated sentence generation show models with learned knowledge constraints by our algorithm greatly improve over base generative models.",0
"The field of deep generative models (DGMs) has made incredible progress, but incorporating complex structured knowledge into end-to-end DGMs can be challenging. Posterior regularization (PR) is a useful framework for imposing structured constraints on probabilistic models, but it has limitations for DGMs that lack a Bayesian formulation or explicit density evaluation. Additionally, PR requires constraints to be fully specified beforehand, which is impractical for complex knowledge with uncertain parts that can be learned. In this study, we establish a mathematical connection between PR and reinforcement learning (RL) and use this connection to extend PR to learn constraints as the extrinsic reward in RL. Our algorithm is model-agnostic and flexible, allowing for the adaptation of arbitrary constraints with the model. Experiments on human image and sentence generation demonstrate that our algorithm significantly improves the performance of DGMs with learned knowledge constraints.",1
"The work investigates deep generative models, which allow us to use training data from one domain to build a model for another domain. We propose the Variational Bi-domain Triplet Autoencoder (VBTA) that learns a joint distribution of objects from different domains. We extend the VBTAs objective function by the relative constraints or triplets that sampled from the shared latent space across domains. In other words, we combine the deep generative models with a metric learning ideas in order to improve the final objective with the triplets information. The performance of the VBTA model is demonstrated on different tasks: image-to-image translation, bi-directional image generation and cross-lingual document classification.",0
"The research explores the use of deep generative models that enable the creation of a model for a different domain by utilizing training data from one domain. Our proposal, the Variational Bi-domain Triplet Autoencoder (VBTA), learns a joint distribution of objects across multiple domains. We enhance the VBTAs objective function by incorporating relative constraints or triplets sourced from the shared latent space between domains. This approach combines the principles of deep generative models and metric learning to enhance the objective with triplet information. We showcase the efficacy of the VBTA model in various tasks, including image-to-image translation, bi-directional image generation, and cross-lingual document classification.",1
We investigate the use of entropy-regularized optimal transport (EOT) cost in developing generative models to learn implicit distributions. Two generative models are proposed. One uses EOT cost directly in an one-shot optimization problem and the other uses EOT cost iteratively in an adversarial game. The proposed generative models show improved performance over contemporary models for image generation on MNSIT.,0
"Our research focuses on exploring the potential of entropy-regularized optimal transport (EOT) cost for developing generative models that can learn implicit distributions. We have put forward two generative models, one of which adopts EOT cost in a single-step optimization process while the other employs it iteratively in an adversarial game. Our proposed generative models have shown enhanced performance as compared to modern models for generating images on MNSIT.",1
"Disentangling factors of variation within data has become a very challenging problem for image generation tasks. Current frameworks for training a Generative Adversarial Network (GAN), learn to disentangle the representations of the data in an unsupervised fashion and capture the most significant factors of the data variations. However, these approaches ignore the principle of content and style disentanglement in image generation, which means their learned latent code may alter the content and style of the generated images at the same time. This paper describes the Style and Content Disentangled GAN (SC-GAN), a new unsupervised algorithm for training GANs that learns disentangled style and content representations of the data. We assume that the representation of an image can be decomposed into a content code that represents the geometrical information of the data, and a style code that captures textural properties. Consequently, by fixing the style portion of the latent representation, we can generate diverse images in a particular style. Reversely, we can set the content code and generate a specific scene in a variety of styles. The proposed SC-GAN has two components: a content code which is the input to the generator, and a style code which modifies the scene style through modification of the Adaptive Instance Normalization (AdaIN) layers' parameters. We evaluate the proposed SC-GAN framework on a set of baseline datasets.",0
"It has become increasingly difficult to identify the different factors contributing to variation in data, especially for image generation tasks. While current frameworks for training Generative Adversarial Networks (GANs) have been successful in learning to disentangle data representations in an unsupervised manner, they fail to consider the need for disentangling content and style in image generation. As a result, the learned latent code often alters both the content and style of the generated images simultaneously. To address this issue, this paper introduces the Style and Content Disentangled GAN (SC-GAN), an unsupervised algorithm for training GANs that can accurately distinguish between style and content representations of data. The proposed framework assumes that the representation of an image can be decomposed into a content code that represents the geometric information of the data and a style code that captures textural properties. By keeping the style portion of the latent representation fixed, the SC-GAN can generate diverse images in a particular style, and by setting the content code, it can generate a specific scene in a variety of styles. The SC-GAN consists of two components: a content code that serves as the input to the generator and a style code that modifies the scene style through the Adaptive Instance Normalization (AdaIN) layers' parameters. The proposed framework is evaluated on a set of baseline datasets to demonstrate its effectiveness.",1
"This paper deals with a method for generating realistic labeled masses. Recently, there have been many attempts to apply deep learning to various bio-image computing fields including computer-aided detection and diagnosis. In order to learn deep network model to be well-behaved in bio-image computing fields, a lot of labeled data is required. However, in many bioimaging fields, the large-size of labeled dataset is scarcely available. Although a few researches have been dedicated to solving this problem through generative model, there are some problems as follows: 1) The generated bio-image does not seem realistic; 2) the variation of generated bio-image is limited; and 3) additional label annotation task is needed. In this study, we propose a realistic labeled bio-image generation method through visual feature processing in latent space. Experimental results have shown that mass images generated by the proposed method were realistic and had wide expression range of targeted mass characteristics.",0
"The focus of this paper is on a technique for producing labeled masses that are true to life. In recent times, there have been numerous efforts to leverage deep learning in several bio-imaging areas, such as computer-aided diagnosis and detection. However, for deep network models to function correctly in bio-imaging fields, there is a need for a significant amount of labeled data. Unfortunately, in many bioimaging fields, such labeled datasets are not readily available. While some studies have attempted to resolve this issue by using generative models, they often face several challenges. These include generating unrealistic bio-images, limited variation in output, and the need for additional label annotation. Our research proposes a method for generating realistic labeled bio-images through visual feature processing in latent space. Our experiments have shown that the masses generated by our technique are realistic and have a wide range of targeted mass characteristics.",1
"An interpretable generative model for handwritten digits synthesis is proposed in this work. Modern image generative models, such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), are trained by backpropagation (BP). The training process is complex and the underlying mechanism is difficult to explain. We propose an interpretable multi-stage PCA method to achieve the same goal and use handwritten digit images synthesis as an illustrative example. First, we derive principal-component-analysis-based (PCA-based) transform kernels at each stage based on the covariance of its inputs. This results in a sequence of transforms that convert input images of correlated pixels to spectral vectors of uncorrelated components. In other words, it is a whitening process. Then, we can synthesize an image based on random vectors and multi-stage transform kernels through a coloring process. The generative model is a feedforward (FF) design since no BP is used in model parameter determination. Its design complexity is significantly lower, and the whole design process is explainable. Finally, we design an FF generative model using the MNIST dataset, compare synthesis results with those obtained by state-of-the-art GAN and VAE methods, and show that the proposed generative model achieves comparable performance.",0
"In this work, we propose a comprehensible generative model for producing handwritten digits. Currently, popular image generative models, like Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), rely on a complicated training process using backpropagation (BP), which lacks transparency. To address this issue, we introduce an interpretable multi-stage PCA approach, which also generates handwritten digit images as an example. Initially, we create principal-component-analysis-based (PCA-based) transform kernels at each stage by analyzing the covariance of the inputs, resulting in a sequence of transforms that convert correlated pixel images to uncorrelated spectral vectors. This process is known as whitening. Subsequently, we can synthesize an image through a coloring process using random vectors and multi-stage transform kernels. The generative model design is feedforward (FF), which means that no BP is involved in model parameter determination. This design approach is much simpler and more transparent. Furthermore, we implement an FF generative model using the MNIST dataset, compare the results with those obtained by state-of-the-art GAN and VAE methods, and demonstrate that the proposed generative model achieves similar performance.",1
"Polarizing filters provide a powerful way to separate diffuse and specular reflection; however, traditional methods rely on several captures and require proper alignment of the filters. Recently, camera manufacturers have proposed to embed polarizing micro-filters in front of the sensor, creating a mosaic of pixels with different polarizations. In this paper, we investigate the advantages of such camera designs. In particular, we consider different design patterns for the filter arrays and propose an algorithm to demosaic an image generated by such cameras. This essentially allows us to separate the diffuse and specular components using a single image. The performance of our algorithm is compared with a color-based method using synthetic and real data. Finally, we demonstrate how we can recover the normals of a scene using the diffuse images estimated by our method.",0
"The use of polarizing filters is a useful technique for separating diffuse and specular reflection. However, traditional methods require multiple captures and precise filter alignment. A new approach involves embedding polarizing micro-filters in front of the camera sensor, creating a mosaic of pixels with varying polarizations. This paper examines the benefits of such camera designs, exploring different filter array patterns and proposing an algorithm for demosaicing images produced by these cameras. With this algorithm, we can isolate diffuse and specular components with just one image, and we compare its performance to a color-based method using both synthetic and real data. Finally, we demonstrate how our method can recover scene normals by estimating diffuse images.",1
"Generative Adversarial Networks (GANs) have a great performance in image generation, but they need a large scale of data to train the entire framework, and often result in nonsensical results. We propose a new method referring to conditional GAN, which equipments the latent noise with mixture of Student's t-distribution with attention mechanism in addition to class information. Student's t-distribution has long tails that can provide more diversity to the latent noise. Meanwhile, the discriminator in our model implements two tasks simultaneously, judging whether the images come from the true data distribution, and identifying the class of each generated images. The parameters of the mixture model can be learned along with those of GANs. Moreover, we mathematically prove that any multivariate Student's t-distribution can be obtained by a linear transformation of a normal multivariate Student's t-distribution. Experiments comparing the proposed method with typical GAN, DeliGAN and DCGAN indicate that, our method has a great performance on generating diverse and legible objects with limited data.",0
"GANs have shown impressive results in image generation, but require a large amount of data to train and can often produce nonsensical outputs. To address these issues, we propose a novel approach that utilizes a conditional GAN equipped with a mixture of Student's t-distribution and an attention mechanism, in addition to class information, to enhance the diversity of the latent noise. By incorporating Student's t-distribution, which has long tails, we can achieve greater variability in the generated images. Furthermore, our model's discriminator performs two tasks simultaneously: distinguishing images from the true data distribution while also identifying their respective classes. We can learn the parameters of the mixture model alongside the GAN. Additionally, we prove that any multivariate Student's t-distribution can be obtained through a linear transformation of a normal multivariate Student's t-distribution. Experimental results comparing our method with conventional GAN, DeliGAN, and DCGAN show that our approach generates more diverse and recognizable objects with limited data.",1
Training data is the key component in designing algorithms for medical image analysis and in many cases it is the main bottleneck in achieving good results. Recent progress in image generation has enabled the training of neural network based solutions using synthetic data. A key factor in the generation of new samples is controlling the important appearance features and potentially being able to generate a new sample of a specific class with different variants. In this work we suggest the synthesis of new data by mixing the class specified and unspecified representation of different factors in the training data. Our experiments on liver lesion classification in CT show an average improvement of 7.4% in accuracy over the baseline training scheme.,0
"The primary element required for developing algorithms to analyze medical images is training data, and often it is the main hindrance in achieving favorable outcomes. The recent advancements in image generation have facilitated the use of synthetic data to train neural network-based solutions. The essential aspect of generating new samples is the ability to regulate significant appearance features and potentially create a new sample of a specific class with diverse variations. In this study, we propose to create fresh data by blending the class specified and unspecified representations of various factors in the training data. Our experiments on liver lesion classification in CT indicate an average increase of 7.4% in accuracy compared to the traditional training approach.",1
"In this paper we investigate the feasibility of using synthetic data to augment face datasets. In particular, we propose a novel generative adversarial network (GAN) that can disentangle identity-related attributes from non-identity-related attributes. This is done by training an embedding network that maps discrete identity labels to an identity latent space that follows a simple prior distribution, and training a GAN conditioned on samples from that distribution. Our proposed GAN allows us to augment face datasets by generating both synthetic images of subjects in the training set and synthetic images of new subjects not in the training set. By using recent advances in GAN training, we show that the synthetic images generated by our model are photo-realistic, and that training with augmented datasets can indeed increase the accuracy of face recognition models as compared with models trained with real images alone.",0
"The aim of this study is to explore the possibility of enhancing face datasets with synthetic data. The study introduces a new type of generative adversarial network (GAN) that separates identity-related attributes from non-identity-related ones. This is achieved through training an embedding network that maps discrete identity labels to an identity latent space that follows a straightforward prior distribution and a GAN that is trained on samples from that distribution. With this GAN, it is possible to augment face datasets by creating synthetic images of both subjects in the training set and those not in the training set. The study employs recent advancements in GAN training to demonstrate that the synthetic images produced by their model are of high quality and that training with augmented datasets can improve the precision of face recognition models compared to models trained solely with real images.",1
"A promising class of generative models maps points from a simple distribution to a complex distribution through an invertible neural network. Likelihood-based training of these models requires restricting their architectures to allow cheap computation of Jacobian determinants. Alternatively, the Jacobian trace can be used if the transformation is specified by an ordinary differential equation. In this paper, we use Hutchinson's trace estimator to give a scalable unbiased estimate of the log-density. The result is a continuous-time invertible generative model with unbiased density estimation and one-pass sampling, while allowing unrestricted neural network architectures. We demonstrate our approach on high-dimensional density estimation, image generation, and variational inference, achieving the state-of-the-art among exact likelihood methods with efficient sampling.",0
"An encouraging type of generative models utilizes an invertible neural network to map points from a simple distribution to a more complex one. To perform likelihood-based training on these models, their architectures must be limited to ensure the efficient computation of Jacobian determinants. Alternatively, if the transformation is specified by an ordinary differential equation, the Jacobian trace can be employed. This paper implements Hutchinson's trace estimator to produce an unrestricted neural network architecture with an unbiased estimate of the log-density. The outcome is a continuous-time invertible generative model with unbiased density estimation and efficient one-pass sampling. We showcase our approach on high-dimensional density estimation, image generation, and variational inference, achieving a state-of-the-art performance among exact likelihood methods with efficient sampling.",1
"We present a novel model architecture which leverages deep learning tools to perform exact Bayesian inference on sets of high dimensional, complex observations. Our model is provably exchangeable, meaning that the joint distribution over observations is invariant under permutation: this property lies at the heart of Bayesian inference. The model does not require variational approximations to train, and new samples can be generated conditional on previous samples, with cost linear in the size of the conditioning set. The advantages of our architecture are demonstrated on learning tasks that require generalisation from short observed sequences while modelling sequence variability, such as conditional image generation, few-shot learning, and anomaly detection.",0
"Our innovative model architecture utilizes deep learning techniques to achieve precise Bayesian inference on intricate, high-dimensional observations. The model is guaranteed to be exchangeable, ensuring that the joint distribution over observations remains unchanged under permutation, a fundamental aspect of Bayesian inference. Moreover, our model does not necessitate variational approximations to train and can generate new samples given prior samples with a cost proportional to the size of the conditioning set. We showcase the advantages of our architecture on learning tasks that call for generalization from limited observed sequences while accounting for sequence variability, including conditional image generation, few-shot learning, and anomaly detection.",1
"We propose a new recurrent generative model for generating images from text captions while attending on specific parts of text captions. Our model creates images by incrementally adding patches on a ""canvas"" while attending on words from text caption at each timestep. Finally, the canvas is passed through an upscaling network to generate images. We also introduce a new method for generating visual-semantic sentence embeddings based on self-attention over text. We compare our model's generated images with those generated Reed et. al.'s model and show that our model is a stronger baseline for text to image generation tasks.",0
"A novel recurrent generative model is suggested for producing images from text captions with a focus on particular parts of the captions. The model constructs images by gradually incorporating patches onto a ""canvas"" while taking note of the words from the text caption at each time step. Subsequently, the canvas is processed through an upscaling network to generate images. Additionally, a new technique for generating visual-semantic sentence embeddings that relies on self-attention over text is introduced. Our model's images generated are compared with those of Reed et. al.'s model, and it is demonstrated that our model serves as a stronger benchmark for the job of text to image generation.",1
"Many machine learning problems involve iteratively and alternately optimizing different task objectives with respect to different sets of parameters. Appropriately scheduling the optimization of a task objective or a set of parameters is usually crucial to the quality of convergence. In this paper, we present AutoLoss, a meta-learning framework that automatically learns and determines the optimization schedule. AutoLoss provides a generic way to represent and learn the discrete optimization schedule from metadata, allows for a dynamic and data-driven schedule in ML problems that involve alternating updates of different parameters or from different loss objectives. We apply AutoLoss on four ML tasks: d-ary quadratic regression, classification using a multi-layer perceptron (MLP), image generation using GANs, and multi-task neural machine translation (NMT). We show that the AutoLoss controller is able to capture the distribution of better optimization schedules that result in higher quality of convergence on all four tasks. The trained AutoLoss controller is generalizable -- it can guide and improve the learning of a new task model with different specifications, or on different datasets.",0
"Optimizing various task objectives with different sets of parameters is a common feature in many machine learning problems. The quality of convergence is largely determined by appropriate scheduling of the optimization of a task objective or set of parameters. This paper introduces AutoLoss, a meta-learning framework that automatically learns and determines the optimization schedule. AutoLoss uses metadata to represent and learn the discrete optimization schedule, allowing for a dynamic and data-driven schedule in machine learning problems that involve updates of different parameters or loss objectives. AutoLoss is applied to four machine learning tasks, including d-ary quadratic regression, classification using a multi-layer perceptron, image generation using GANs, and multi-task neural machine translation. The results show that the AutoLoss controller is capable of capturing the distribution of better optimization schedules, leading to higher quality convergence for all four tasks. The trained AutoLoss controller is generalizable and can guide and improve the learning of a new task model with different specifications or on different datasets.",1
"Transferring the knowledge of pretrained networks to new domains by means of finetuning is a widely used practice for applications based on discriminative models. To the best of our knowledge this practice has not been studied within the context of generative deep networks. Therefore, we study domain adaptation applied to image generation with generative adversarial networks. We evaluate several aspects of domain adaptation, including the impact of target domain size, the relative distance between source and target domain, and the initialization of conditional GANs. Our results show that using knowledge from pretrained networks can shorten the convergence time and can significantly improve the quality of the generated images, especially when the target data is limited. We show that these conclusions can also be drawn for conditional GANs even when the pretrained model was trained without conditioning. Our results also suggest that density may be more important than diversity and a dataset with one or few densely sampled classes may be a better source model than more diverse datasets such as ImageNet or Places.",0
"The practice of finetuning pretrained networks is commonly used for discriminative models to transfer knowledge to new domains. However, this approach has not been explored in the context of generative deep networks. To address this gap, we investigate domain adaptation for image generation using generative adversarial networks. Our study examines various factors, such as target domain size, distance between source and target domains, and conditional GAN initialization. Our findings demonstrate that leveraging pretrained networks can shorten convergence time and significantly enhance image quality, particularly with limited target data. We also establish that these benefits extend to conditional GANs, even if the pretrained model was not conditioned during training. Furthermore, we suggest that density may be more critical than diversity, and datasets with one or a few densely sampled classes may be better suited as source models than more diverse datasets like ImageNet or Places.",1
"Recently, generative adversarial networks have gained a lot of popularity for image generation tasks. However, such models are associated with complex learning mechanisms and demand very large relevant datasets. This work borrows concepts from image and video captioning models to form an image generative framework. The model is trained in a similar fashion as recurrent captioning model and uses the learned weights for image generation. This is done in an inverse direction, where the input is a caption and the output is an image. The vector representation of the sentence and frames are extracted from an encoder-decoder model which is initially trained on similar sentence and image pairs. Our model conditions image generation on a natural language caption. We leverage a sequence-to-sequence model to generate synthetic captions that have the same meaning for having a robust image generation. One key advantage of our method is that the traditional image captioning datasets can be used for synthetic sentence paraphrases. Results indicate that images generated through multiple captions are better at capturing the semantic meaning of the family of captions.",0
"Generative adversarial networks have become increasingly popular for generating images, but their complexity and need for large datasets pose challenges. In this study, we adopt concepts from image and video captioning models to create an image generative framework. The model is trained like a recurrent captioning model, using learned weights for image generation. However, our model works in reverse: it takes a caption as input and generates an image as output. The encoder-decoder model extracts vector representations of sentences and frames, trained initially on sentence and image pairs. Our model generates synthetic captions using a sequence-to-sequence model to ensure robust image generation. One advantage of our approach is that traditional image captioning datasets can be used for synthetic sentence paraphrases. Results show that images generated from multiple captions better capture the semantic meaning of the family of captions.",1
"Image captioning has demonstrated models that are capable of generating plausible text given input images or videos. Further, recent work in image generation has shown significant improvements in image quality when text is used as a prior. Our work ties these concepts together by creating an architecture that can enable bidirectional generation of images and text. We call this network Multi-Modal Vector Representation (MMVR). Along with MMVR, we propose two improvements to the text conditioned image generation. Firstly, a n-gram metric based cost function is introduced that generalizes the caption with respect to the image. Secondly, multiple semantically similar sentences are shown to help in generating better images. Qualitative and quantitative evaluations demonstrate that MMVR improves upon existing text conditioned image generation results by over 20%, while integrating visual and text modalities.",0
"Models in image captioning have shown the ability to produce convincing text when given images or videos as input. Recent developments in image generation have also shown improvements in image quality when text is used as a prior. Our research brings these ideas together by creating an architecture that allows for bidirectional generation of images and text, which we call Multi-Modal Vector Representation (MMVR). Along with MMVR, we propose two enhancements to text-conditioned image generation. Firstly, we introduce a cost function based on n-gram metrics that generalizes the caption in relation to the image. Secondly, we demonstrate the usefulness of displaying multiple semantically similar sentences to improve image generation. Both qualitative and quantitative assessments show that MMVR, which integrates visual and text modalities, produces better results than existing text-conditioned image generation techniques by more than 20%.",1
"In this paper, we propose a novel technique for sampling sequential images using a cylindrical transform in a cylindrical coordinate system for kidney semantic segmentation in abdominal computed tomography (CT). The images generated from a cylindrical transform augment a limited annotated set of images in three dimensions. This approach enables us to train contemporary classification deep convolutional neural networks (DCNNs) instead of fully convolutional networks (FCNs) for semantic segmentation. Typical semantic segmentation models segment a sequential set of images (e.g. CT or video) by segmenting each image independently. However, the proposed method not only considers the spatial dependency in the x-y plane, but also the spatial sequential dependency along the z-axis. The results show that classification DCNNs, trained on cylindrical transformed images, can achieve a higher segmentation performance value than FCNs using a limited number of annotated images.",0
"A new method for sampling sequential images in abdominal computed tomography (CT) for kidney semantic segmentation is proposed in this paper. The technique utilizes a cylindrical transform in a cylindrical coordinate system to generate images that can supplement the limited annotated set of images in three dimensions. With this method, contemporary classification deep convolutional neural networks (DCNNs) can be trained instead of fully convolutional networks (FCNs) for semantic segmentation. Unlike traditional semantic segmentation models that only consider spatial dependency in the x-y plane, this approach takes into account both spatial dependency in the x-y plane and sequential dependency along the z-axis. The results demonstrate that DCNNs trained on cylindrical transformed images achieve higher segmentation performance than FCNs with a limited number of annotated images.",1
"As sensing technology proliferates and becomes affordable to the general public, there is a growing trend in citizen science where scientists and volunteers form a strong partnership in conducting scientific research including problem finding, data collection, analysis, visualization, and storytelling. Providing easy-to-use computational tools to support citizen science has become an important issue. To raise the public awareness of environmental science and improve the air quality in local areas, we are currently collaborating with a local community in monitoring and documenting fugitive emissions from a coke refinery. We have helped the community members build a live camera system which captures and visualizes high resolution timelapse imagery starting from November 2014. However, searching and documenting smoke emissions manually from all video frames requires manpower and takes an impractical investment of time. This paper describes a software tool which integrates four features: (1) an algorithm based on change detection and texture segmentation for identifying smoke emissions; (2) an interactive timeline visualization providing indicators for seeking to interesting events; (3) an autonomous fast-forwarding mode for skipping uninteresting timelapse frames; and (4) a collection of animated smoke images generated automatically according to the algorithm for documentation, presentation, storytelling, and sharing. With the help of this tool, citizen scientists can now focus on the content of the story instead of time-consuming and laborious works.",0
"The increasing availability and affordability of sensing technology has led to a rise in citizen science, where scientists and volunteers work together to conduct scientific research, including problem identification, data collection, analysis, visualization, and storytelling. However, providing accessible computational tools to support citizen science has become a pressing issue. In order to promote environmental awareness and enhance local air quality, we have partnered with a community to monitor fugitive emissions from a coke refinery. The community has built a live camera system that captures high-resolution timelapse imagery from November 2014. However, manually searching and documenting smoke emissions from all video frames is impractical and requires significant manpower. To address this issue, we have developed a software tool that integrates four features: a change detection and texture segmentation algorithm to identify smoke emissions, an interactive timeline visualization with indicators for interesting events, an autonomous fast-forwarding mode to skip uninteresting frames, and a collection of animated smoke images generated automatically for documentation, presentation, storytelling, and sharing. This tool enables citizen scientists to focus on the content of the story rather than laborious tasks.",1
"Can an algorithm create original and compelling fashion designs to serve as an inspirational assistant? To help answer this question, we design and investigate different image generation models associated with different loss functions to boost creativity in fashion generation. The dimensions of our explorations include: (i) different Generative Adversarial Networks architectures that start from noise vectors to generate fashion items, (ii) novel loss functions that encourage novelty, inspired from Sharma-Mittal divergence, a generalized mutual information measure for the widely used relative entropies such as Kullback-Leibler, and (iii) a generation process following the key elements of fashion design (disentangling shape and texture components). A key challenge of this study is the evaluation of generated designs and the retrieval of best ones, hence we put together an evaluation protocol associating automatic metrics and human experimental studies that we hope will help ease future research. We show that our proposed creativity criterion yield better overall appreciation than the one employed in Creative Adversarial Networks. In the end, about 61% of our images are thought to be created by human designers rather than by a computer while also being considered original per our human subject experiments, and our proposed loss scores the highest compared to existing losses in both novelty and likability.",0
"Is it possible for an algorithm to generate unique and captivating fashion designs as a source of inspiration? To investigate this question, we explore several image generation models using various loss functions to enhance creativity in fashion design. Our exploration includes three main dimensions: (i) different Generative Adversarial Network architectures that start from noise vectors to create fashion items, (ii) innovative loss functions that promote originality, inspired by the Sharma-Mittal divergence, a mutual information measure for relative entropies such as Kullback-Leibler, and (iii) a generation process that follows the core elements of fashion design by separating shape and texture components. Evaluating our generated designs and identifying the best ones poses a significant challenge, which we address with an evaluation protocol comprising automatic metrics and human experimental studies. Our proposed criterion for creativity yields better overall appreciation compared to the Creative Adversarial Networks approach. About 61% of our images are perceived to be created by human designers rather than a computer, indicating their originality as determined by our human subject experiments. Our proposed loss function scores the highest among existing losses in both novelty and likability.",1
"One of the defining characteristics of human creativity is the ability to make conceptual leaps, creating something surprising from typical knowledge. In comparison, deep neural networks often struggle to handle cases outside of their training data, which is especially problematic for problems with limited training data. Approaches exist to transfer knowledge from problems with sufficient data to those with insufficient data, but they tend to require additional training or a domain-specific method of transfer. We present a new approach, conceptual expansion, that serves as a general representation for reusing existing trained models to derive new models without backpropagation. We evaluate our approach on few-shot variations of two tasks: image classification and image generation, and outperform standard transfer learning approaches.",0
"Human creativity is characterized by the ability to generate novel ideas by making conceptual leaps from existing knowledge. Conversely, deep neural networks often struggle with cases that fall outside of their training data, especially when the available data is limited. Although methods exist for transferring knowledge from problems with sufficient data to problems with limited data, these approaches typically require additional training or a domain-specific transfer method. We introduce a novel approach, conceptual expansion, which provides a general representation for reusing existing models without the need for backpropagation. To demonstrate the effectiveness of our approach, we evaluate it on two tasks involving few-shot variations of image classification and generation, and show that it outperforms standard transfer learning techniques.",1
"A reliable stereo algorithm is critical for many robotics applications. But textureless and specular regions can easily cause failure by making feature matching difficult. Understanding whether an algorithm is robust to these hazardous regions is important. Although many stereo benchmarks have been developed to evaluate performance, it is hard to quantify the effect of hazardous regions in real images because the location and severity of these regions are unknown. In this paper, we develop a synthetic image generation tool enabling to control hazardous factors, such as making objects more specular or transparent, to produce hazardous regions at different degrees. The densely controlled sampling strategy in virtual worlds enables to effectively stress test stereo algorithms by varying the types and degrees of the hazard. We generate a large synthetic image dataset with automatically computed hazardous regions and analyze algorithms on these regions. The observations from synthetic images are further validated by annotating hazardous regions in real-world datasets Middlebury and KITTI (which gives a sparse sampling of the hazards). Our synthetic image generation tool is based on a game engine Unreal Engine 4 and will be open-source along with the virtual scenes in our experiments. Many publicly available realistic game contents can be used by our tool to provide an enormous resource for development and evaluation of algorithms.",0
"In robotics applications, having a dependable stereo algorithm is crucial, but it can easily fail due to textureless or specular regions, making feature matching difficult. It is therefore important to determine whether an algorithm can handle these hazardous areas. While there are many stereo benchmarks available, it is challenging to gauge the impact of hazardous regions in real-world images because their location and severity are unknown. This paper presents a synthetic image generation tool that allows for the control of factors that make objects more specular or transparent, creating hazardous regions at varying degrees. This virtual world-based strategy enables the effective stress testing of stereo algorithms by altering the types and degrees of hazards. We produce a large synthetic image dataset with automatically computed hazardous regions and analyze algorithms on these regions. Our findings from synthetic images are validated by annotating hazardous areas in real-world datasets. Our synthetic image generation tool is open-source and based on Unreal Engine 4, offering a vast resource for algorithm development and evaluation using publicly available realistic game content.",1
"Understanding, reasoning, and manipulating semantic concepts of images have been a fundamental research problem for decades. Previous work mainly focused on direct manipulation on natural image manifold through color strokes, key-points, textures, and holes-to-fill. In this work, we present a novel hierarchical framework for semantic image manipulation. Key to our hierarchical framework is that we employ a structured semantic layout as our intermediate representation for manipulation. Initialized with coarse-level bounding boxes, our structure generator first creates pixel-wise semantic layout capturing the object shape, object-object interactions, and object-scene relations. Then our image generator fills in the pixel-level textures guided by the semantic layout. Such framework allows a user to manipulate images at object-level by adding, removing, and moving one bounding box at a time. Experimental evaluations demonstrate the advantages of the hierarchical manipulation framework over existing image generation and context hole-filing models, both qualitatively and quantitatively. Benefits of the hierarchical framework are further demonstrated in applications such as semantic object manipulation, interactive image editing, and data-driven image manipulation.",0
"For years, researchers have grappled with understanding and manipulating the semantic concepts of images, using methods such as color strokes, key-points, textures, and holes-to-fill. However, in this study, we introduce a new approach to semantic image manipulation that employs a hierarchical framework. Our framework utilizes a structured semantic layout as an intermediate representation for manipulation, starting with coarse-level bounding boxes. Our structure generator then creates a pixel-wise semantic layout that captures object shape, object-object interactions, and object-scene relations. Finally, our image generator fills in the pixel-level textures according to the semantic layout. This framework allows users to manipulate images at the object-level, adding, removing, and moving one bounding box at a time. Experimental evaluations show that our hierarchical manipulation framework outperforms existing image generation and context hole-filling models, both qualitatively and quantitatively. Our framework also has applications in semantic object manipulation, interactive image editing, and data-driven image manipulation.",1
"Training a good deep learning model often requires a lot of annotated data. As a large amount of labeled data is typically difficult to collect and even more difficult to annotate, data augmentation and data generation are widely used in the process of training deep neural networks. However, there is no clear common understanding on how much labeled data is needed to get satisfactory performance. In this paper, we try to address such a question using vehicle license plate character recognition as an example application. We apply computer graphic scripts and Generative Adversarial Networks to generate and augment a large number of annotated, synthesized license plate images with realistic colors, fonts, and character composition from a small number of real, manually labeled license plate images. Generated and augmented data are mixed and used as training data for the license plate recognition network modified from DenseNet. The experimental results show that the model trained from the generated mixed training data has good generalization ability, and the proposed approach achieves a new state-of-the-art accuracy on Dataset-1 and AOLP, even with a very limited number of original real license plates. In addition, the accuracy improvement caused by data generation becomes more significant when the number of labeled images is reduced. Data augmentation also plays a more significant role when the number of labeled images is increased.",0
"To train an effective deep learning model, a vast amount of annotated data is often necessary. However, collecting and annotating significant amounts of data can be challenging, which is why data augmentation and data generation are commonly utilized in the process of training deep neural networks. Despite this, there is no standard consensus on the amount of labeled data needed to achieve satisfactory performance. In this study, we aim to address this question by utilizing vehicle license plate character recognition as an example application. By implementing computer graphic scripts and Generative Adversarial Networks, we generated and augmented a considerable number of annotated, synthesized license plate images with realistic colors, fonts, and character composition from a limited number of manually labeled license plate images. The mixed data generated and augmented were used as training data for the license plate recognition network modified from DenseNet. Our experimental results indicate that the model trained with the mixed training data has excellent generalization ability, and our proposed approach achieves a new state-of-the-art accuracy on Dataset-1 and AOLP, even with a limited number of original real license plates. Furthermore, the accuracy improvement caused by data generation becomes more significant when the number of labeled images is reduced, while data augmentation plays a more significant role when the number of labeled images is increased.",1
"In this paper, we propose a method, called GridFace, to reduce facial geometric variations and improve the recognition performance. Our method rectifies the face by local homography transformations, which are estimated by a face rectification network. To encourage the image generation with canonical views, we apply a regularization based on the natural face distribution. We learn the rectification network and recognition network in an end-to-end manner. Extensive experiments show our method greatly reduces geometric variations, and gains significant improvements in unconstrained face recognition scenarios.",0
"The aim of our paper is to introduce GridFace, a technique that enhances recognition accuracy by minimizing facial geometric discrepancies. Specifically, our approach utilizes local homography transformations to rectify facial features, which are determined by a dedicated network. To induce the generation of images that conform to standard views, we incorporate a regularization method that aligns with natural facial distributions. Both the rectification and recognition networks are trained together as part of an end-to-end learning process. Our method has been extensively tested, and the results show substantial reduction in geometric variations and noteworthy progress in face recognition under uncontrolled conditions.",1
"In this paper, we introduce a new method for generating an object image from text attributes on a desired location, when the base image is given. One step further to the existing studies on text-to-image generation mainly focusing on the object's appearance, the proposed method aims to generate an object image preserving the given background information, which is the first attempt in this field. To tackle the problem, we propose a multi-conditional GAN (MC-GAN) which controls both the object and background information jointly. As a core component of MC-GAN, we propose a synthesis block which disentangles the object and background information in the training stage. This block enables MC-GAN to generate a realistic object image with the desired background by controlling the amount of the background information from the given base image using the foreground information from the text attributes. From the experiments with Caltech-200 bird and Oxford-102 flower datasets, we show that our model is able to generate photo-realistic images with a resolution of 128 x 128. The source code of MC-GAN is released.",0
"This paper presents a new approach for creating an object image based on textual attributes and a given image location. Unlike previous studies that focused solely on the object's appearance, our method aims to maintain the provided background information while generating the object image. To achieve this, we developed a multi-conditional GAN (MC-GAN) that jointly controls the object and background information. The MC-GAN includes a synthesis block that separates the foreground and background information during training. By adjusting the amount of background information from the given base image, our model can generate a realistic object image with desired background features. We tested our approach on the Caltech-200 bird and Oxford-102 flower datasets, producing high-quality images with a resolution of 128 x 128. The source code for MC-GAN is now available.",1
"We propose a novel, projection based way to incorporate the conditional information into the discriminator of GANs that respects the role of the conditional information in the underlining probabilistic model. This approach is in contrast with most frameworks of conditional GANs used in application today, which use the conditional information by concatenating the (embedded) conditional vector to the feature vectors. With this modification, we were able to significantly improve the quality of the class conditional image generation on ILSVRC2012 (ImageNet) 1000-class image dataset from the current state-of-the-art result, and we achieved this with a single pair of a discriminator and a generator. We were also able to extend the application to super-resolution and succeeded in producing highly discriminative super-resolution images. This new structure also enabled high quality category transformation based on parametric functional transformation of conditional batch normalization layers in the generator.",0
"Our proposal suggests a fresh, projection-based method of integrating conditional information into GAN discriminators, which upholds the conditional information's role in the underlying probabilistic model. This diverges from most current frameworks of conditional GANs, which append the (embedded) conditional vector to the feature vectors. With our approach, we managed to significantly enhance the quality of class conditional image generation on the ILSVRC2012 (ImageNet) 1000-class image dataset, surpassing the present state-of-the-art result with a sole pair of a generator and a discriminator. We also applied this method to super-resolution and achieved discerning super-resolution images. Additionally, this new structure allowed for high-quality category transformation based on parametric functional transformation of conditional batch normalization layers in the generator.",1
"Given a random pair of images, an arbitrary style transfer method extracts the feel from the reference image to synthesize an output based on the look of the other content image. Recent arbitrary style transfer methods transfer second order statistics from reference image onto content image via a multiplication between content image features and a transformation matrix, which is computed from features with a pre-determined algorithm. These algorithms either require computationally expensive operations, or fail to model the feature covariance and produce artifacts in synthesized images. Generalized from these methods, in this work, we derive the form of transformation matrix theoretically and present an arbitrary style transfer approach that learns the transformation matrix with a feed-forward network. Our algorithm is highly efficient yet allows a flexible combination of multi-level styles while preserving content affinity during style transfer process. We demonstrate the effectiveness of our approach on four tasks: artistic style transfer, video and photo-realistic style transfer as well as domain adaptation, including comparisons with the state-of-the-art methods.",0
"An arbitrary style transfer method can extract the feel from a reference image and synthesize an output based on the look of another content image. However, recent methods have limitations such as computationally expensive operations or failure to model feature covariance, resulting in synthesized images with artifacts. To address these issues, this work presents a new approach that learns the transformation matrix with a feed-forward network, allowing for efficient and flexible combination of multi-level styles while preserving content affinity. The effectiveness of this approach is demonstrated in four tasks, including artistic style transfer, video and photo-realistic style transfer, and domain adaptation, with comparisons to state-of-the-art methods.",1
"We propose a computational framework to jointly parse a single RGB image and reconstruct a holistic 3D configuration composed by a set of CAD models using a stochastic grammar model. Specifically, we introduce a Holistic Scene Grammar (HSG) to represent the 3D scene structure, which characterizes a joint distribution over the functional and geometric space of indoor scenes. The proposed HSG captures three essential and often latent dimensions of the indoor scenes: i) latent human context, describing the affordance and the functionality of a room arrangement, ii) geometric constraints over the scene configurations, and iii) physical constraints that guarantee physically plausible parsing and reconstruction. We solve this joint parsing and reconstruction problem in an analysis-by-synthesis fashion, seeking to minimize the differences between the input image and the rendered images generated by our 3D representation, over the space of depth, surface normal, and object segmentation map. The optimal configuration, represented by a parse graph, is inferred using Markov chain Monte Carlo (MCMC), which efficiently traverses through the non-differentiable solution space, jointly optimizing object localization, 3D layout, and hidden human context. Experimental results demonstrate that the proposed algorithm improves the generalization ability and significantly outperforms prior methods on 3D layout estimation, 3D object detection, and holistic scene understanding.",0
"Our proposed method utilizes a stochastic grammar model to jointly parse a single RGB image and construct a complete 3D configuration consisting of CAD models. To achieve this, we introduce a Holistic Scene Grammar (HSG) that characterizes the joint distribution over the functional and geometric space of indoor scenes. The HSG encompasses three critical dimensions of indoor scenes: latent human context, geometric constraints, and physical constraints. We approach the joint parsing and reconstruction problem in an analysis-by-synthesis manner, minimizing the differences between the input image and the rendered images generated by our 3D representation. The optimal configuration, represented by a parse graph, is inferred using Markov chain Monte Carlo (MCMC), which efficiently optimizes object localization, 3D layout, and hidden human context. Our experimental results demonstrate that our algorithm outperforms prior methods concerning 3D layout estimation, 3D object detection, and holistic scene understanding while improving generalization ability.",1
"Recent advances in Deep Learning and probabilistic modeling have led to strong improvements in generative models for images. On the one hand, Generative Adversarial Networks (GANs) have contributed a highly effective adversarial learning procedure, but still suffer from stability issues. On the other hand, Conditional Variational Auto-Encoders (CVAE) models provide a sound way of conditional modeling but suffer from mode-mixing issues. Therefore, recent work has turned back to simple and stable regression models that are effective at generation but give up on the sampling mechanism and the latent code representation. We propose a novel and efficient stochastic regression approach with latent drop-out codes that combines the merits of both lines of research. In addition, a new training objective increases coverage of the training distribution leading to improvements over the state of the art in terms of accuracy as well as diversity.",0
"Advancements in Deep Learning and probabilistic modeling have resulted in substantial enhancements in generative models for images. While Generative Adversarial Networks (GANs) have made a significant contribution to adversarial learning, they are still plagued by stability issues. Meanwhile, Conditional Variational Auto-Encoders (CVAE) offer a reliable way of conditional modeling but suffer from mode-mixing problems. As a result, recent research has returned to robust and straightforward regression models that are effective at generating but lack the sampling mechanism and latent code representation. We propose a new and efficient stochastic regression technique featuring latent drop-out codes that combines the best of both approaches. Additionally, a fresh training objective improves coverage of the training distribution, resulting in improved accuracy and diversity over the state of the art.",1
"The new wave of successful generative models in machine learning has increased the interest in deep learning driven de novo drug design. However, assessing the performance of such generative models is notoriously difficult. Metrics that are typically used to assess the performance of such generative models are the percentage of chemically valid molecules or the similarity to real molecules in terms of particular descriptors, such as the partition coefficient (logP) or druglikeness. However, method comparison is difficult because of the inconsistent use of evaluation metrics, the necessity for multiple metrics, and the fact that some of these measures can easily be tricked by simple rule-based systems. We propose a novel distance measure between two sets of molecules, called Fr\'echet ChemNet distance (FCD), that can be used as an evaluation metric for generative models. The FCD is similar to a recently established performance metric for comparing image generation methods, the Fr\'echet Inception Distance (FID). Whereas the FID uses one of the hidden layers of InceptionNet, the FCD utilizes the penultimate layer of a deep neural network called ChemNet, which was trained to predict drug activities. Thus, the FCD metric takes into account chemically and biologically relevant information about molecules, and also measures the diversity of the set via the distribution of generated molecules. The FCD's advantage over previous metrics is that it can detect if generated molecules are a) diverse and have similar b) chemical and c) biological properties as real molecules. We further provide an easy-to-use implementation that only requires the SMILES representation of the generated molecules as input to calculate the FCD. Implementations are available at: https://www.github.com/bioinf-jku/FCD",0
"The interest in deep learning driven de novo drug design has been heightened by the recent success of generative models in machine learning. However, evaluating the performance of these models is challenging. Commonly used metrics for assessing performance include the percentage of chemically valid molecules and similarity to real molecules based on specific descriptors such as the partition coefficient (logP) or druglikeness. Comparing methods is difficult due to inconsistent use of evaluation metrics, the need for multiple metrics, and the ease with which some measures can be misled by simple rule-based systems. To address these issues, we propose a novel distance measure called Fr\'echet ChemNet distance (FCD) that can be utilized as an evaluation metric for generative models. FCD compares two sets of molecules and is similar to the Fr\'echet Inception Distance (FID) used to compare image generation methods. However, FCD uses the penultimate layer of a deep neural network called ChemNet, which was trained to predict drug activities, to take into account chemically and biologically relevant information about molecules and measure the diversity of the set based on the distribution of generated molecules. The advantage of FCD over previous metrics is that it can detect the diversity of generated molecules and determine if they have similar chemical and biological properties to real molecules. An easy-to-use implementation that requires only the SMILES representation of the generated molecules as input to calculate the FCD is available at: https://www.github.com/bioinf-jku/FCD.",1
"We introduce a new dataset of 293,008 high definition (1360 x 1360 pixels) fashion images paired with item descriptions provided by professional stylists. Each item is photographed from a variety of angles. We provide baseline results on 1) high-resolution image generation, and 2) image generation conditioned on the given text descriptions. We invite the community to improve upon these baselines. In this paper, we also outline the details of a challenge that we are launching based upon this dataset.",0
"A fresh dataset comprising 293,008 high-quality fashion images, each measuring 1360 x 1360 pixels, is presented alongside expertly crafted item descriptions. A range of perspectives is offered for each item in the pictures. Our research includes fundamental outcomes for both 1) generating high-resolution images and 2) producing images based on the accompanying textual descriptions. We welcome the community to surpass these initial findings. Furthermore, we provide information about a competition centered on this dataset in this article.",1
"Generating large quantities of quality labeled data in medical imaging is very time consuming and expensive. The performance of supervised algorithms for various tasks on imaging has improved drastically over the years, however the availability of data to train these algorithms have become one of the main bottlenecks for implementation. To address this, we propose a semi-supervised learning method where pseudo-negative labels from unlabeled data are used to further refine the performance of a pulmonary nodule detection network in chest radiographs. After training with the proposed network, the false positive rate was reduced to 0.1266 from 0.4864 while maintaining sensitivity at 0.89.",0
"It is a time-consuming and costly process to produce a substantial amount of top-quality labeled data for medical imaging. Even though supervised algorithms have shown tremendous progress in imaging tasks, the lack of available data to train these algorithms has become a significant obstacle to their implementation. We suggest a semi-supervised learning approach that employs pseudo-negative labels from unlabeled data to enhance the performance of a chest radiograph pulmonary nodule detection network. The proposed network was trained, resulting in a decrease in the false positive rate from 0.4864 to 0.1266 while maintaining a sensitivity of 0.89.",1
"In this paper we investigate image generation guided by hand sketch. When the input sketch is badly drawn, the output of common image-to-image translation follows the input edges due to the hard condition imposed by the translation process. Instead, we propose to use sketch as weak constraint, where the output edges do not necessarily follow the input edges. We address this problem using a novel joint image completion approach, where the sketch provides the image context for completing, or generating the output image. We train a generated adversarial network, i.e, contextual GAN to learn the joint distribution of sketch and the corresponding image by using joint images. Our contextual GAN has several advantages. First, the simple joint image representation allows for simple and effective learning of joint distribution in the same image-sketch space, which avoids complicated issues in cross-domain learning. Second, while the output is related to its input overall, the generated features exhibit more freedom in appearance and do not strictly align with the input features as previous conditional GANs do. Third, from the joint image's point of view, image and sketch are of no difference, thus exactly the same deep joint image completion network can be used for image-to-sketch generation. Experiments evaluated on three different datasets show that our contextual GAN can generate more realistic images than state-of-the-art conditional GANs on challenging inputs and generalize well on common categories.",0
"The aim of this study is to explore the generation of images guided by hand-drawn sketches. One challenge encountered in the image-to-image translation process is that when the input sketch is poorly drawn, the output image tends to follow the input edges, which are hard to translate. To overcome this issue, we suggest using the sketch as a weak constraint, where the output edges need not strictly follow the input edges. We propose a novel approach to joint image completion, where the sketch provides context for generating the output image. To learn the joint distribution of sketch and image, we train a contextual GAN, which has several benefits. Firstly, the joint image representation is simple and effective for learning the joint distribution in the same image-sketch space, avoiding issues in cross-domain learning. Secondly, the output features have more freedom in appearance and do not necessarily align strictly with the input features. Thirdly, our joint image completion network can be used for image-to-sketch generation as well. Our experiments on different datasets demonstrate that our contextual GAN generates more realistic images than state-of-the-art conditional GANs and generalizes well on common categories, even with challenging inputs.",1
"We propose a novel hierarchical approach for text-to-image synthesis by inferring semantic layout. Instead of learning a direct mapping from text to image, our algorithm decomposes the generation process into multiple steps, in which it first constructs a semantic layout from the text by the layout generator and converts the layout to an image by the image generator. The proposed layout generator progressively constructs a semantic layout in a coarse-to-fine manner by generating object bounding boxes and refining each box by estimating object shapes inside the box. The image generator synthesizes an image conditioned on the inferred semantic layout, which provides a useful semantic structure of an image matching with the text description. Our model not only generates semantically more meaningful images, but also allows automatic annotation of generated images and user-controlled generation process by modifying the generated scene layout. We demonstrate the capability of the proposed model on challenging MS-COCO dataset and show that the model can substantially improve the image quality, interpretability of output and semantic alignment to input text over existing approaches.",0
"Our proposed method for text-to-image synthesis involves inferring semantic layout using a hierarchical approach. Rather than directly mapping text to image, our algorithm breaks down the generation process into multiple stages. The layout generator constructs a semantic layout from the text by generating object bounding boxes and refining them by estimating object shapes. The image generator then produces an image based on the inferred semantic layout, resulting in an image that aligns with the text description. Our model not only generates more meaningful images but also allows for automatic annotation and user-controlled modification of the generated scene layout. We demonstrate the effectiveness of our approach on the challenging MS-COCO dataset and show significant improvements in image quality, interpretability, and semantic alignment compared to existing methods.",1
"In this paper we introduce Curriculum GANs, a curriculum learning strategy for training Generative Adversarial Networks that increases the strength of the discriminator over the course of training, thereby making the learning task progressively more difficult for the generator. We demonstrate that this strategy is key to obtaining state-of-the-art results in image generation. We also show evidence that this strategy may be broadly applicable to improving GAN training in other data modalities.",0
"The purpose of this paper is to present Curriculum GANs, a technique for curriculum learning in the training of Generative Adversarial Networks. This approach enhances the discriminator's ability throughout the training process, thereby increasing the complexity of the generator's learning task. Our research indicates that this approach is crucial for achieving the best possible outcomes in image generation. Furthermore, we provide evidence that this method may have wider applications in improving GAN training for other types of data.",1
"In this paper, we investigate the use of generative adversarial networks in the task of image generation according to subjective measures of semantic attributes. Unlike the standard (CGAN) that generates images from discrete categorical labels, our architecture handles both continuous and discrete scales. Given pairwise comparisons of images, our model, called RankCGAN, performs two tasks: it learns to rank images using a subjective measure; and it learns a generative model that can be controlled by that measure. RankCGAN associates each subjective measure of interest to a distinct dimension of some latent space. We perform experiments on UT-Zap50K, PubFig and OSR datasets and demonstrate that the model is expressive and diverse enough to conduct two-attribute exploration and image editing.",0
"This study explores the use of generative adversarial networks for generating images based on subjective measures of semantic attributes. Our model, RankCGAN, differs from the standard CGAN by allowing for the generation of images from both continuous and discrete scales. By comparing pairs of images, RankCGAN performs two tasks: ranking images based on subjective measures and learning a generative model that can be controlled by those measures. Each subjective measure is associated with a distinct dimension of a latent space. We conducted experiments on three datasets and showed that RankCGAN is capable of conducting two-attribute exploration and image editing with expressive and diverse results.",1
"Deep neural networks with discrete latent variables offer the promise of better symbolic reasoning, and learning abstractions that are more useful to new tasks. There has been a surge in interest in discrete latent variable models, however, despite several recent improvements, the training of discrete latent variable models has remained challenging and their performance has mostly failed to match their continuous counterparts. Recent work on vector quantized autoencoders (VQ-VAE) has made substantial progress in this direction, with its perplexity almost matching that of a VAE on datasets such as CIFAR-10. In this work, we investigate an alternate training technique for VQ-VAE, inspired by its connection to the Expectation Maximization (EM) algorithm. Training the discrete bottleneck with EM helps us achieve better image generation results on CIFAR-10, and together with knowledge distillation, allows us to develop a non-autoregressive machine translation model whose accuracy almost matches a strong greedy autoregressive baseline Transformer, while being 3.3 times faster at inference.",0
"Although discrete latent variable models have the potential to improve symbolic reasoning and produce more useful abstractions for new tasks, their training remains challenging and their performance has generally fallen short of continuous counterparts. Nevertheless, recent progress has been made through vector quantized autoencoders (VQ-VAE), which have achieved perplexity levels close to VAE on datasets like CIFAR-10. In this study, we explore an alternative training method for VQ-VAE, inspired by its link to the Expectation Maximization (EM) algorithm. Training the discrete bottleneck with EM has allowed us to generate high-quality images on CIFAR-10 and, in conjunction with knowledge distillation, to develop a non-autoregressive machine translation model that is 3.3 times faster at inference and almost as accurate as a strong greedy autoregressive baseline Transformer.",1
"In recent years, research on image generation methods has been developing fast. The auto-encoding variational Bayes method (VAEs) was proposed in 2013, which uses variational inference to learn a latent space from the image database and then generates images using the decoder. The generative adversarial networks (GANs) came out as a promising framework, which uses adversarial training to improve the generative ability of the generator. However, the generated pictures by GANs are generally blurry. The deep convolutional generative adversarial networks (DCGANs) were then proposed to leverage the quality of generated images. Since the input noise vectors are randomly sampled from a Gaussian distribution, the generator has to map from a whole normal distribution to the images. This makes DCGANs unable to reflect the inherent structure of the training data. In this paper, we propose a novel deep model, called generative adversarial networks with decoder-encoder output noise (DE-GANs), which takes advantage of both the adversarial training and the variational Bayesain inference to improve the performance of image generation. DE-GANs use a pre-trained decoder-encoder architecture to map the random Gaussian noise vectors to informative ones and pass them to the generator of the adversarial networks. Since the decoder-encoder architecture is trained by the same images as the generators, the output vectors could carry the intrinsic distribution information of the original images. Moreover, the loss function of DE-GANs is different from GANs and DCGANs. A hidden-space loss function is added to the adversarial loss function to enhance the robustness of the model. Extensive empirical results show that DE-GANs can accelerate the convergence of the adversarial training process and improve the quality of the generated images.",0
"The field of image generation methods has been rapidly advancing in recent years. One popular method, the auto-encoding variational Bayes technique (VAEs), was introduced in 2013 and employs variational inference to learn a latent space from image data and generate images using a decoder. While generative adversarial networks (GANs) have shown promise, their generated images tend to be blurry. Deep convolutional GANs (DCGANs) were developed to improve image quality, but they fail to capture the underlying data structure. This paper introduces a novel deep model called generative adversarial networks with decoder-encoder output noise (DE-GANs). DE-GANs combine adversarial training and variational Bayesian inference, using a pre-trained decoder-encoder architecture to map random noise vectors to informative ones for the generator. DE-GANs' loss function includes a hidden-space loss function to enhance model robustness. Empirical results demonstrate that DE-GANs can accelerate the adversarial training process and improve image quality.",1
"Recent fast image style transferring methods use feed-forward neural networks to generate an output image of desired style strength from the input pair of a content and a target style image. In the existing methods, the image of intermediate style between the content and the target style is obtained by decoding a linearly interpolated feature in encoded feature space. However, there has been no work on analyzing the effectiveness of this kind of style strength interpolation so far. In this paper, we tackle the missing work on the in-depth analysis of style interpolation and propose a method that is more effective in controlling style strength. We interpret the training task of a style transfer network as a regression learning between the control parameter and output style strength. In this understanding, the existing methods are biased due to the fact that training is performed with one-sided data of full style strength (alpha = 1.0). Thus, this biased learning does not guarantee the generation of a desired intermediate style corresponding to the style control parameter between 0.0 and 1.0. To solve this problem of the biased network, we propose an unbiased learning technique which uses unbiased training data and corresponding unbiased loss for alpha = 0.0 to make the feed-forward networks to generate a zero-style image, i.e., content image when alpha = 0.0. Our experimental results verified that our unbiased learning method achieved the reconstruction of a content image with zero style strength, better regression specification between style control parameter and output style, and more stable style transfer that is insensitive to the weight of style loss without additive complexity in image generating process.",0
"Current methods for rapidly transferring image styles utilize feed-forward neural networks to produce an output image with the desired style intensity, using an input pair of a content and a target style image. These methods obtain an intermediate style image between the content and target style by decoding a linearly interpolated feature in the encoded feature space. However, there has been no analysis on the effectiveness of this style strength interpolation. In this study, we address this gap by proposing a more effective method for controlling style strength. We interpret the training task of a style transfer network as a regression learning between the control parameter and output style strength, and demonstrate that existing methods are biased due to one-sided training data. To address this bias, we propose an unbiased learning technique that utilizes unbiased training data and corresponding unbiased loss for alpha = 0.0 to generate a zero-style image. Our experiments show that our unbiased learning method achieves better regression specification between style control parameter and output style, and more stable style transfer without added complexity in the image generating process.",1
"Despite the success of generative adversarial networks (GANs) for image generation, the trade-off between visual quality and image diversity remains a significant issue. This paper achieves both aims simultaneously by improving the stability of training GANs. The key idea of the proposed approach is to implicitly regularize the discriminator using representative features. Focusing on the fact that standard GAN minimizes reverse Kullback-Leibler (KL) divergence, we transfer the representative feature, which is extracted from the data distribution using a pre-trained autoencoder (AE), to the discriminator of standard GANs. Because the AE learns to minimize forward KL divergence, our GAN training with representative features is influenced by both reverse and forward KL divergence. Consequently, the proposed approach is verified to improve visual quality and diversity of state of the art GANs using extensive evaluations.",0
"Although generative adversarial networks (GANs) have been successful in generating images, there is still a significant issue of balancing visual quality and image diversity. This problem is addressed in this paper by enhancing the stability of GAN training. The proposed approach involves implicitly regularizing the discriminator with representative features. By utilizing a pre-trained autoencoder (AE), the representative features extracted from the data distribution are transferred to the discriminator of standard GANs. Unlike standard GANs that minimize reverse Kullback-Leibler (KL) divergence, the GAN training with representative features is influenced by both reverse and forward KL divergence because the AE minimizes forward KL divergence. As a result, the proposed approach improves the visual quality and diversity of state-of-the-art GANs through thorough evaluations.",1
"In this paper, we present an object detection method that tackles the stingray detection problem based on aerial images. In this problem, the images are aerially captured on a sea-surface area by using an Unmanned Aerial Vehicle (UAV), and the stingrays swimming under (but close to) the sea surface are the target we want to detect and locate. To this end, we use a deep object detection method, faster RCNN, to train a stingray detector based on a limited training set of images. To boost the performance, we develop a new generative approach, conditional GLO, to increase the training samples of stingray, which is an extension of the Generative Latent Optimization (GLO) approach. Unlike traditional data augmentation methods that generate new data only for image classification, our proposed method that mixes foreground and background together can generate new data for an object detection task, and thus improve the training efficacy of a CNN detector. Experimental results show that satisfiable performance can be obtained by using our approach on stingray detection in aerial images.",0
"This paper introduces a method for detecting stingrays in aerial images captured by an Unmanned Aerial Vehicle (UAV). The goal is to locate stingrays swimming under the sea surface. To achieve this, the faster RCNN deep object detection method is used to train a stingray detector based on a limited training set of images. To improve performance, a new generative approach called conditional GLO is developed to increase the training samples of stingray. This method mixes foreground and background together to generate new data for object detection, which improves the training efficacy of a CNN detector. Experimental results demonstrate that our approach is effective in detecting stingrays in aerial images.",1
"In this work we combine two research threads from Vision/ Graphics and Natural Language Processing to formulate an image generation task conditioned on attributes in a multi-turn setting. By multiturn, we mean the image is generated in a series of steps of user-specified conditioning information. Our proposed approach is practically useful and offers insights into neural interpretability. We introduce a framework that includes a novel training algorithm as well as model improvements built for the multi-turn setting. We demonstrate that this framework generates a sequence of images that match the given conditioning information and that this task is useful for more detailed benchmarking and analysis of conditional image generation methods.",0
"Our work integrates two research areas, Vision/Graphics and Natural Language Processing, to develop an image generation task with conditioned attributes in a multi-turn environment. The images are generated in stages using user-defined conditioning information. Our approach is both practical and provides insights into neural interpretability. We present a framework that includes a new training algorithm and model enhancements designed for the multi-turn environment. Our results demonstrate that this framework generates a sequence of images that accurately match the given conditioning information. This task is beneficial for detailed benchmarking and analysis of conditional image generation techniques.",1
"Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual sequences. In this work, we generalize a recently proposed model architecture based on self-attention, the Transformer, to a sequence modeling formulation of image generation with a tractable likelihood. By restricting the self-attention mechanism to attend to local neighborhoods we significantly increase the size of images the model can process in practice, despite maintaining significantly larger receptive fields per layer than typical convolutional neural networks. While conceptually simple, our generative models significantly outperform the current state of the art in image generation on ImageNet, improving the best published negative log-likelihood on ImageNet from 3.83 to 3.77. We also present results on image super-resolution with a large magnification ratio, applying an encoder-decoder configuration of our architecture. In a human evaluation study, we find that images generated by our super-resolution model fool human observers three times more often than the previous state of the art.",0
"Autoregressive sequence generation or transformation has proven successful in image generation. Recent studies have demonstrated the effectiveness of self-attention in modeling textual sequences. In this research, we extend the Transformer model architecture based on self-attention to sequence modeling for image generation with a manageable likelihood. By limiting the self-attention mechanism to focus on local neighborhoods, we significantly enhance the model's ability to process larger images, while maintaining a more considerable receptive field per layer than conventional convolutional neural networks. Despite being conceptually simple, our generative models outperform the current state-of-the-art in image generation on ImageNet, reducing the best published negative log-likelihood from 3.83 to 3.77. We also present findings on image super-resolution using an encoder-decoder configuration of our architecture with a large magnification ratio. Our super-resolution model generated images that deceived human observers three times more often than the previous state-of-the-art in a human evaluation study.",1
"Generating novel, yet realistic, images of persons is a challenging task due to the complex interplay between the different image factors, such as the foreground, background and pose information. In this work, we aim at generating such images based on a novel, two-stage reconstruction pipeline that learns a disentangled representation of the aforementioned image factors and generates novel person images at the same time. First, a multi-branched reconstruction network is proposed to disentangle and encode the three factors into embedding features, which are then combined to re-compose the input image itself. Second, three corresponding mapping functions are learned in an adversarial manner in order to map Gaussian noise to the learned embedding feature space, for each factor respectively. Using the proposed framework, we can manipulate the foreground, background and pose of the input image, and also sample new embedding features to generate such targeted manipulations, that provide more control over the generation process. Experiments on Market-1501 and Deepfashion datasets show that our model does not only generate realistic person images with new foregrounds, backgrounds and poses, but also manipulates the generated factors and interpolates the in-between states. Another set of experiments on Market-1501 shows that our model can also be beneficial for the person re-identification task.",0
"The task of creating original yet realistic images of people is difficult due to the intricate relationship between various image factors, including foreground, background, and pose details. This study focuses on developing a two-stage reconstruction process that learns how to disentangle these image factors and generate new images of individuals simultaneously. The first stage involves a multi-branched reconstruction network that encodes the three factors into embedding features, which are then combined to reconstruct the original image. The second stage involves learning three corresponding mapping functions in an adversarial manner, which map Gaussian noise to the learned embedding feature space for each factor. This framework enables the manipulation of the foreground, background, and pose of the original image, as well as the generation of new embedding features to facilitate targeted manipulations and greater control over the generation process. The experiments conducted on the Market-1501 and Deepfashion datasets demonstrate that the model can generate realistic person images with new foregrounds, backgrounds, and poses, as well as interpolate between these factors. Additionally, the model can be beneficial for person re-identification tasks.",1
"This paper first presents a theory for generative adversarial methods that does not rely on the traditional minimax formulation. It shows that with a strong discriminator, a good generator can be learned so that the KL divergence between the distributions of real data and generated data improves after each functional gradient step until it converges to zero. Based on the theory, we propose a new stable generative adversarial method. A theoretical insight into the original GAN from this new viewpoint is also provided. The experiments on image generation show the effectiveness of our new method.",0
"In this paper, an alternative theory is introduced for generative adversarial approaches that does not depend on the typical minimax formulation. The theory demonstrates that by utilizing a powerful discriminator, a proficient generator can be acquired, resulting in an enhancement of the KL divergence between genuine data and generated data with every functional gradient step until it reaches zero. Using this theory, a novel, dependable generative adversarial method is suggested, and a theoretical analysis of the original GAN from this new perspective is presented. By conducting image generation experiments, the efficacy of our novel approach is demonstrated.",1
"In this paper, we propose a novel technique for generating images in the 3D domain from images with high degree of geometrical transformations. By coalescing two popular concurrent methods that have seen rapid ascension to the machine learning zeitgeist in recent years: GANs (Goodfellow et. al.) and Capsule networks (Sabour, Hinton et. al.) - we present: \textbf{CapsGAN}. We show that CapsGAN performs better than or equal to traditional CNN based GANs in generating images with high geometric transformations using rotated MNIST. In the process, we also show the efficacy of using capsules architecture in the GANs domain. Furthermore, we tackle the Gordian Knot in training GANs - the performance control and training stability by experimenting with using Wasserstein distance (gradient clipping, penalty) and Spectral Normalization. The experimental findings of this paper should propel the application of capsules and GANs in the still exciting and nascent domain of 3D image generation, and plausibly video (frame) generation.",0
"This paper proposes a new method for creating 3D images from highly transformed images. The approach combines two popular machine learning techniques, GANs and Capsule networks, resulting in a novel method called CapsGAN. The study demonstrates that CapsGAN performs equally or better than traditional CNN based GANs in generating images with high geometric transformations, using rotated MNIST as an example. The paper also explores the effectiveness of using capsules architecture in GANs and addresses the challenges in training GANs by experimenting with Wasserstein distance and Spectral Normalization. The results of this study could lead to advancements in 3D and video image generation using capsules and GANs.",1
"GANs excel at learning high dimensional distributions, but they can update generator parameters in directions that do not correspond to the steepest descent direction of the objective. Prominent examples of problematic update directions include those used in both Goodfellow's original GAN and the WGAN-GP. To formally describe an optimal update direction, we introduce a theoretical framework which allows the derivation of requirements on both the divergence and corresponding method for determining an update direction, with these requirements guaranteeing unbiased mini-batch updates in the direction of steepest descent. We propose a novel divergence which approximates the Wasserstein distance while regularizing the critic's first order information. Together with an accompanying update direction, this divergence fulfills the requirements for unbiased steepest descent updates. We verify our method, the First Order GAN, with image generation on CelebA, LSUN and CIFAR-10 and set a new state of the art on the One Billion Word language generation task. Code to reproduce experiments is available.",0
"Although GANs are effective in learning high dimensional distributions, they can update generator parameters in non-optimal directions, which is problematic. This issue was present in both Goodfellow's original GAN and the WGAN-GP. To address this, we introduce a theoretical framework that outlines requirements for an optimal update direction. These requirements ensure unbiased mini-batch updates in the direction of steepest descent. We propose a novel divergence that approximates the Wasserstein distance while regulating the critic's first order information. Along with an accompanying update direction, this divergence fulfills the requirements for unbiased steepest descent updates. We demonstrate the effectiveness of our approach, the First Order GAN, for image generation on CelebA, LSUN, and CIFAR-10. Additionally, we set a new state of the art on the One Billion Word language generation task. Our code for replicating experiments is available.",1
"Conditional domain generation is a good way to interactively control sample generation process of deep generative models. However, once a conditional generative model has been created, it is often expensive to allow it to adapt to new conditional controls, especially the network structure is relatively deep. We propose a conditioned latent domain transfer framework across latent spaces of unconditional variational autoencoders(VAE). With this framework, we can allow unconditionally trained VAEs to generate images in its domain with conditionals provided by a latent representation of another domain. This framework does not assume commonalities between two domains. We demonstrate effectiveness and robustness of our model under widely used image datasets.",0
"Conditional domain generation is an effective method for controlling the sample generation process of deep generative models through interaction. However, adjusting a conditional generative model to new conditional controls can be costly, particularly if the network structure is deep. To address this issue, we propose a framework called conditioned latent domain transfer that operates across the latent spaces of unconditional variational autoencoders (VAEs). Our approach allows unconditionally trained VAEs to generate images in their domain while utilizing conditionals provided by a latent representation from another domain. This transfer framework does not assume any commonalities between the two domains. We demonstrate the effectiveness and robustness of our model using widely used image datasets.",1
"Conversion of one font to another font is very useful in real life applications. In this paper, we propose a Convolutional Recurrent Generative model to solve the word level font transfer problem. Our network is able to convert the font style of any printed text images from its current font to the required font. The network is trained end-to-end for the complete word images. Thus it eliminates the necessary pre-processing steps, like character segmentations. We extend our model to conditional setting that helps to learn one-to-many mapping function. We employ a novel convolutional recurrent model architecture in the Generator that efficiently deals with the word images of arbitrary width. It also helps to maintain the consistency of the final images after concatenating the generated image patches of target font. Besides, the Generator and the Discriminator network, we employ a Classification network to classify the generated word images of converted font style to their subsequent font categories. Most of the earlier works related to image translation are performed on square images. Our proposed architecture is the first work which can handle images of varying widths. Word images generally have varying width depending on the number of characters present. Hence, we test our model on a synthetically generated font dataset. We compare our method with some of the state-of-the-art methods for image translation. The superior performance of our network on the same dataset proves the ability of our model to learn the font distributions.",0
"The ability to convert one font to another is extremely useful in practical applications. Our paper proposes a Convolutional Recurrent Generative model that solves the problem of transferring font styles at the word level. Our network can convert the font style of any printed text image from its current font to the desired font. The model is trained end-to-end for complete word images, which eliminates the need for pre-processing steps such as character segmentation. We also extend our model to a conditional setting, which allows for one-to-many mapping functions to be learned. Our Generator uses a novel convolutional recurrent model architecture that efficiently handles word images of arbitrary width while maintaining consistency in the final images. Additionally, we employ a Classification network to classify the generated word images into their respective font categories. Unlike previous works that focused on square images, our proposed architecture can handle images of varying widths, which is important for word images that have varying widths depending on the number of characters. We evaluate our model on a synthetically generated font dataset and compare it with state-of-the-art methods for image translation. Our network's superior performance on the same dataset proves its ability to learn font distributions.",1
"Autonomous navigation has become an increasingly popular machine learning application. Recent advances in deep learning have also resulted in great improvements to autonomous navigation. However, prior outdoor autonomous navigation depends on various expensive sensors or large amounts of real labeled data which is difficult to acquire and sometimes erroneous. The objective of this study is to train an autonomous navigation model that uses a simulator (instead of real labeled data) and an inexpensive monocular camera. In order to exploit the simulator satisfactorily, our proposed method is based on domain adaptation with adversarial learning. Specifically, we propose our model with 1) a dilated residual block in the generator, 2) cycle loss, and 3) style loss to improve the adversarial learning performance for satisfactory domain adaptation. In addition, we perform a theoretical analysis that supports the justification of our proposed method. We present empirical results of navigation in outdoor courses with various intersections using a commercial radio controlled car. We observe that our proposed method allows us to learn a favorable navigation model by generating images with realistic textures. To the best of our knowledge, this is the first work to apply domain adaptation with adversarial learning to autonomous navigation in real outdoor environments. Our proposed method can also be applied to precise image generation or other robotic tasks.",0
"The use of autonomous navigation as a machine learning application has gained popularity in recent times, with advancements in deep learning leading to significant improvements in this area. However, outdoor autonomous navigation has traditionally relied on expensive sensors or large amounts of real labeled data, which are often difficult to obtain and may contain errors. This study aims to develop an autonomous navigation model that uses a simulator and an inexpensive monocular camera, rather than real labeled data. To achieve this, the proposed method utilizes domain adaptation with adversarial learning, which includes a dilated residual block in the generator, cycle loss, and style loss to improve the adversarial learning performance for satisfactory domain adaptation. The study also includes a theoretical analysis that supports the justification of the proposed method. Empirical results demonstrate that the proposed method allows for the learning of a favorable navigation model by generating images with realistic textures. This approach is the first of its kind to apply domain adaptation with adversarial learning to autonomous navigation in real outdoor environments and can also be applied to precise image generation or other robotic tasks.",1
"Deep learning-based style transfer between images has recently become a popular area of research. A common way of encoding ""style"" is through a feature representation based on the Gram matrix of features extracted by some pre-trained neural network or some other form of feature statistics. Such a definition is based on an arbitrary human decision and may not best capture what a style really is. In trying to gain a better understanding of ""style"", we propose a metric learning-based method to explicitly encode the style of an artwork. In particular, our definition of style captures the differences between artists, as shown by classification performances, and such that the style representation can be interpreted, manipulated and visualized through style-conditioned image generation through a Generative Adversarial Network. We employ this method to explore the style space of anime portrait illustrations.",0
"Recently, there has been a surge in interest in deep learning-based style transfer between images. Typically, ""style"" is encoded using a feature representation based on the Gram matrix of features, extracted by a pre-trained neural network or some other form of feature statistics. However, this definition of style is subjective and may not accurately capture what style truly is. To gain a better understanding of style, we propose a metric learning-based approach that explicitly encodes the style of an artwork. Our approach captures the differences between artists, as demonstrated by classification performances. Additionally, our style representation can be interpreted, manipulated, and visualized through style-conditioned image generation using a Generative Adversarial Network. We apply this approach to explore the style space of anime portrait illustrations.",1
"Understanding how people represent categories is a core problem in cognitive science. Decades of research have yielded a variety of formal theories of categories, but validating them with naturalistic stimuli is difficult. The challenge is that human category representations cannot be directly observed and running informative experiments with naturalistic stimuli such as images requires a workable representation of these stimuli. Deep neural networks have recently been successful in solving a range of computer vision tasks and provide a way to compactly represent image features. Here, we introduce a method to estimate the structure of human categories that combines ideas from cognitive science and machine learning, blending human-based algorithms with state-of-the-art deep image generators. We provide qualitative and quantitative results as a proof-of-concept for the method's feasibility. Samples drawn from human distributions rival those from state-of-the-art generative models in quality and outperform alternative methods for estimating the structure of human categories.",0
"Cognitive science has long focused on comprehending how individuals perceive categories, but validating formal theories of this representation using naturalistic stimuli is problematic. The difficulty lies in the fact that direct observation of human category perception is impossible, and meaningful experimentation with naturalistic stimuli, such as images, necessitates a useful representation of these visual elements. Recently, deep neural networks have emerged as a successful tool for solving computer vision issues, offering a compact method for representing image features. This study introduces a technique for estimating the structure of human categories by combining cognitive science and machine learning concepts, using state-of-the-art deep image generators along with human-based algorithms. The results, which include both qualitative and quantitative outputs, demonstrate the feasibility of the method. The samples drawn from human distributions are of comparable quality to those generated by advanced models and perform better than alternative methods for estimating human category structure.",1
"Unsupervised image-to-image translation aims at learning the relationship between samples from two image domains without supervised pair information. The relationship between two domain images can be one-to-one, one-to-many or many-to-many. In this paper, we study the one-to-many unsupervised image translation problem in which an input sample from one domain can correspond to multiple samples in the other domain. To learn the complex relationship between the two domains, we introduce an additional variable to control the variations in our one-to-many mapping. A generative model with an XO-structure, called the XOGAN, is proposed to learn the cross domain relationship among the two domains and the ad- ditional variables. Not only can we learn to translate between the two image domains, we can also handle the translated images with additional variations. Experiments are performed on unpaired image generation tasks, including edges-to-objects translation and facial image translation. We show that the proposed XOGAN model can generate plausible images and control variations, such as color and texture, of the generated images. Moreover, while state-of-the-art unpaired image generation algorithms tend to generate images with monotonous colors, XOGAN can generate more diverse results.",0
"The objective of unsupervised image-to-image translation is to comprehend the link between samples from two image domains without the aid of supervised pair information. The relationship between the images belonging to two domains can be of three types, namely, one-to-one, one-to-many, or many-to-many. This research paper focuses on the one-to-many unsupervised image translation issue where an input sample from one domain can have multiple corresponding samples in the other domain. To comprehend the intricate relationship between the two domains, an additional variable is introduced to regulate the variations in the one-to-many mapping. To learn the cross-domain relationship between the two domains and the additional variables, a generative model with an XO-structure, known as XOGAN, is proposed. The XOGAN model not just helps in translating between the two image domains but also aids in dealing with the translated images with additional variations. This research paper conducts experiments on unpaired image generation tasks, which involve the translation of edges-to-objects and facial images. The study shows that the XOGAN model can create realistic images and regulate the variations, such as color and texture, of the generated images. Also, while the present-day unpaired image generation algorithms tend to create images with uniform colors, XOGAN can produce more diverse outcomes.",1
"Generative Adversarial Nets (GANs) and Variational Auto-Encoders (VAEs) provide impressive image generations from Gaussian white noise, but the underlying mathematics are not well understood. We compute deep convolutional network generators by inverting a fixed embedding operator. Therefore, they do not require to be optimized with a discriminator or an encoder. The embedding is Lipschitz continuous to deformations so that generators transform linear interpolations between input white noise vectors into deformations between output images. This embedding is computed with a wavelet Scattering transform. Numerical experiments demonstrate that the resulting Scattering generators have similar properties as GANs or VAEs, without learning a discriminative network or an encoder.",0
"Although Generative Adversarial Nets (GANs) and Variational Auto-Encoders (VAEs) produce impressive image generations from Gaussian white noise, their underlying mathematics remain poorly understood. Our approach, however, involves computing deep convolutional network generators by inverting a fixed embedding operator, which eliminates the need for optimization with a discriminator or encoder. The Lipschitz continuous embedding is capable of deforming and transforming linear interpolations between input white noise vectors into deformations between output images, and it is computed using a wavelet Scattering transform. Numerical experiments show that Scattering generators produced through this method exhibit similar properties to those of GANs or VAEs, all without requiring a discriminative network or encoder.",1
"Person re-identification (re-ID) models trained on one domain often fail to generalize well to another. In our attempt, we present a ""learning via translation"" framework. In the baseline, we translate the labeled images from source to target domain in an unsupervised manner. We then train re-ID models with the translated images by supervised methods. Yet, being an essential part of this framework, unsupervised image-image translation suffers from the information loss of source-domain labels during translation.   Our motivation is two-fold. First, for each image, the discriminative cues contained in its ID label should be maintained after translation. Second, given the fact that two domains have entirely different persons, a translated image should be dissimilar to any of the target IDs. To this end, we propose to preserve two types of unsupervised similarities, 1) self-similarity of an image before and after translation, and 2) domain-dissimilarity of a translated source image and a target image. Both constraints are implemented in the similarity preserving generative adversarial network (SPGAN) which consists of an Siamese network and a CycleGAN. Through domain adaptation experiment, we show that images generated by SPGAN are more suitable for domain adaptation and yield consistent and competitive re-ID accuracy on two large-scale datasets.",0
"Re-identification models for people often struggle to generalize across different domains. Our approach involves a ""learning via translation"" framework, where we unsupervisedly translate labeled images from the source to the target domain and subsequently train re-ID models using supervised methods. However, unsupervised image-image translation can lead to the loss of source-domain labels during translation, which is a crucial component of our framework. Our objective is twofold: first, we aim to preserve the discriminative cues contained in an image's ID label, and second, we aim to ensure that a translated image is dissimilar to any of the target IDs. To achieve this, we propose preserving two types of unsupervised similarities: the self-similarity of an image before and after translation, and the domain-dissimilarity of a translated source image and a target image. Our similarity preserving generative adversarial network (SPGAN), comprising a Siamese network and a CycleGAN, implements both constraints. We demonstrate through domain adaptation experiments that images generated by SPGAN are more suitable for domain adaptation, resulting in consistent and competitive re-ID accuracy on two large-scale datasets.",1
"Gaussian distributions are commonly used as a key building block in many generative models. However, their applicability has not been well explored in deep networks. In this paper, we propose a novel deep generative model named as Normal Similarity Network (NSN) where the layers are constructed with Gaussian-style filters. NSN is trained with a layer-wise non-parametric density estimation algorithm that iteratively down-samples the training images and captures the density of the down-sampled training images in the final layer. Additionally, we propose NSN-Gen for generating new samples from noise vectors by iteratively reconstructing feature maps in the hidden layers of NSN. Our experiments suggest encouraging results of the proposed model for a wide range of computer vision applications including image generation, styling and reconstruction from occluded images.",0
"Generative models often rely on Gaussian distributions as a fundamental component. However, the use of Gaussian distributions in deep networks has not been extensively explored. To address this gap, we present a new deep generative model called the Normal Similarity Network (NSN), which employs Gaussian-style filters in its layers. The NSN is trained using a layer-wise non-parametric density estimation algorithm that progressively downsamples training images and captures their density in the final layer. Furthermore, we introduce the NSN-Gen, which generates new samples from noise vectors by reconstructing feature maps in the hidden layers of the NSN. Our experiments demonstrate promising results for a variety of computer vision tasks, including image generation, styling, and reconstruction from occluded images.",1
"A recent Cell paper [Chang and Tsao, 2017] reports an interesting discovery. For the face stimuli generated by a pre-trained active appearance model (AAM), the responses of neurons in the areas of the primate brain that are responsible for face recognition exhibit strong linear relationship with the shape variables and appearance variables of the AAM that generates the face stimuli. In this paper, we show that this behavior can be replicated by a deep generative model called the generator network, which assumes that the observed signals are generated by latent random variables via a top-down convolutional neural network. Specifically, we learn the generator network from the face images generated by a pre-trained AAM model using variational auto-encoder, and we show that the inferred latent variables of the learned generator network have strong linear relationship with the shape and appearance variables of the AAM model that generates the face images. Unlike the AAM model that has an explicit shape model where the shape variables generate the control points or landmarks, the generator network has no such shape model and shape variables. Yet the generator network can learn the shape knowledge in the sense that some of the latent variables of the learned generator network capture the shape variations in the face images generated by AAM.",0
"The recent discovery reported in a Cell paper [Chang and Tsao, 2017] reveals that the neurons in the areas of the primate brain responsible for face recognition exhibit a strong linear relationship with the shape and appearance variables of a pre-trained active appearance model (AAM) that generates face stimuli. We demonstrate in this paper that this behavior can be replicated using a deep generative model called the generator network, which uses a top-down convolutional neural network to generate latent random variables that produce the observed signals. We train the generator network using the face images generated by the pre-trained AAM model with a variational auto-encoder and find that the inferred latent variables of the learned generator network also have a strong linear relationship with the AAM model's shape and appearance variables. Unlike the AAM model, the generator network does not have an explicit shape model, yet it learns shape knowledge by capturing shape variations in the face images generated by the AAM through some of its latent variables.",1
"Recurrent Neural Networks (RNNs) are powerful sequence modeling tools. However, when dealing with high dimensional inputs, the training of RNNs becomes computational expensive due to the large number of model parameters. This hinders RNNs from solving many important computer vision tasks, such as Action Recognition in Videos and Image Captioning. To overcome this problem, we propose a compact and flexible structure, namely Block-Term tensor decomposition, which greatly reduces the parameters of RNNs and improves their training efficiency. Compared with alternative low-rank approximations, such as tensor-train RNN (TT-RNN), our method, Block-Term RNN (BT-RNN), is not only more concise (when using the same rank), but also able to attain a better approximation to the original RNNs with much fewer parameters. On three challenging tasks, including Action Recognition in Videos, Image Captioning and Image Generation, BT-RNN outperforms TT-RNN and the standard RNN in terms of both prediction accuracy and convergence rate. Specifically, BT-LSTM utilizes 17,388 times fewer parameters than the standard LSTM to achieve an accuracy improvement over 15.6\% in the Action Recognition task on the UCF11 dataset.",0
"Sequence modeling can be effectively done using Recurrent Neural Networks (RNNs). However, when the inputs are high dimensional, training RNNs becomes computationally expensive due to a large number of model parameters. This makes it challenging for RNNs to solve important computer vision tasks, such as Image Captioning and Action Recognition in Videos. To address this issue, we propose a Block-Term tensor decomposition, which is a compact and flexible structure that significantly reduces the parameters of RNNs and improves their training efficiency. Our method, Block-Term RNN (BT-RNN), is more concise than alternative low-rank approximations like tensor-train RNN (TT-RNN) and can achieve a better approximation to the original RNNs with fewer parameters. In three challenging tasks, including Action Recognition in Videos, Image Captioning and Image Generation, BT-RNN outperforms TT-RNN and the standard RNN in terms of prediction accuracy and convergence rate. For instance, BT-LSTM uses 17,388 times fewer parameters than the standard LSTM, with over 15.6% accuracy improvement in the Action Recognition task on the UCF11 dataset.",1
"Generative Adversarial Networks (GANs) [Goodfellow et al. 2014] convergence in a high-resolution setting with a computational constrain of GPU memory capacity has been beset with difficulty due to the known lack of convergence rate stability. In order to boost network convergence of DCGAN (Deep Convolutional Generative Adversarial Networks) [Radford et al. 2016] and achieve good-looking high-resolution results we propose a new layered network, HDCGAN, that incorporates current state-of-the-art techniques for this effect. Glasses, a mechanism to arbitrarily improve the final GAN generated results by enlarging the input size by a telescope {\zeta} is also presented. A novel bias-free dataset, Curt\'o & Zarza, containing human faces from different ethnical groups in a wide variety of illumination conditions and image resolutions is introduced. Curt\'o is enhanced with HDCGAN synthetic images, thus being the first GAN augmented dataset of faces. We conduct extensive experiments on CelebA [Liu et al. 2015], CelebA-hq [Karras et al. 2018] and Curt\'o. HDCGAN is the current state-of-the-art in synthetic image generation on CelebA achieving a MS-SSIM of 0.1978 and a FR\'ECHET Inception Distance of 8.44.",0
"The convergence of Generative Adversarial Networks (GANs) [Goodfellow et al. 2014] in a high-resolution setting with limited GPU memory capacity has been challenging due to the lack of convergence rate stability. To overcome this issue and generate high-quality, high-resolution results, we propose a new layered network, HDCGAN, that incorporates current state-of-the-art techniques. Moreover, we introduce a mechanism called Glasses that enlarges the input size by a telescope {\zeta} to further improve the final GAN-generated results. We also present a new bias-free dataset, Curt\'o & Zarza, which includes human faces from various ethnic groups in different illumination conditions and image resolutions. HDCGAN synthetic images are added to Curt\'o, making it the first GAN-augmented dataset of faces. We carry out extensive experiments on CelebA [Liu et al. 2015], CelebA-hq [Karras et al. 2018], and Curt\'o. Our experiments show that HDCGAN is the current state-of-the-art in synthetic image generation on CelebA, achieving a MS-SSIM of 0.1978 and a FR\'ECHET Inception Distance of 8.44.",1
"Generative adversarial networks (GANs) are a class of unsupervised machine learning algorithms that can produce realistic images from randomly-sampled vectors in a multi-dimensional space. Until recently, it was not possible to generate realistic high-resolution images using GANs, which has limited their applicability to medical images that contain biomarkers only detectable at native resolution. Progressive growing of GANs is an approach wherein an image generator is trained to initially synthesize low resolution synthetic images (8x8 pixels), which are then fed to a discriminator that distinguishes these synthetic images from real downsampled images. Additional convolutional layers are then iteratively introduced to produce images at twice the previous resolution until the desired resolution is reached. In this work, we demonstrate that this approach can produce realistic medical images in two different domains; fundus photographs exhibiting vascular pathology associated with retinopathy of prematurity (ROP), and multi-modal magnetic resonance images of glioma. We also show that fine-grained details associated with pathology, such as retinal vessels or tumor heterogeneity, can be preserved and enhanced by including segmentation maps as additional channels. We envisage several applications of the approach, including image augmentation and unsupervised classification of pathology.",0
"Generative adversarial networks (GANs) are a type of machine learning algorithm that can create lifelike images from random vectors in a multi-dimensional space. Until recently, GANs were unable to generate realistic, high-quality images, limiting their use in medical imaging that requires native resolution to detect biomarkers. However, progressive growing is an alternative approach where an image generator is first trained to create low-resolution images (8x8 pixels) that are then evaluated by a discriminator to differentiate them from downsampled real images. Additional convolutional layers are added iteratively to generate images at twice the previous resolution until the desired resolution is achieved. Our research demonstrates that this method can produce realistic medical images in fundus photographs of retinopathy of prematurity (ROP) and multi-modal magnetic resonance images of glioma. Furthermore, we found that including segmentation maps as additional channels can preserve and enhance fine details of pathology such as retinal vessels or tumor heterogeneity. This approach could have several applications, including image augmentation and unsupervised pathology classification.",1
"Recently, generative adversarial networks (GANs) have shown promising performance in generating realistic images. However, they often struggle in learning complex underlying modalities in a given dataset, resulting in poor-quality generated images. To mitigate this problem, we present a novel approach called mixture of experts GAN (MEGAN), an ensemble approach of multiple generator networks. Each generator network in MEGAN specializes in generating images with a particular subset of modalities, e.g., an image class. Instead of incorporating a separate step of handcrafted clustering of multiple modalities, our proposed model is trained through an end-to-end learning of multiple generators via gating networks, which is responsible for choosing the appropriate generator network for a given condition. We adopt the categorical reparameterization trick for a categorical decision to be made in selecting a generator while maintaining the flow of the gradients. We demonstrate that individual generators learn different and salient subparts of the data and achieve a multiscale structural similarity (MS-SSIM) score of 0.2470 for CelebA and a competitive unsupervised inception score of 8.33 in CIFAR-10.",0
"GANs have been successful in creating realistic images, but they struggle to learn complex modalities in a dataset, leading to low-quality images. To address this issue, we propose an ensemble approach called MEGAN, which includes multiple generator networks, with each network specializing in generating images for a specific subset of modalities. Our model uses gating networks to select the appropriate generator network for a given condition, avoiding the need for handcrafted clustering. We use the categorical reparameterization trick to enable gradient flow during generator selection. Our results show that MEGAN achieves a MS-SSIM score of 0.2470 for CelebA and an unsupervised inception score of 8.33 in CIFAR-10, demonstrating that individual generators learn different and important subparts of the data.",1
"Generative Adversarial Networks (GANs) have seen steep ascension to the peak of ML research zeitgeist in recent years. Mostly catalyzed by its success in the domain of image generation, the technique has seen wide range of adoption in a variety of other problem domains. Although GANs have had a lot of success in producing more realistic images than other approaches, they have only seen limited use for text sequences. Generation of longer sequences compounds this problem. Most recently, SeqGAN (Yu et al., 2017) has shown improvements in adversarial evaluation and results with human evaluation compared to a MLE based trained baseline. The main contributions of this paper are three-fold: 1. We show results for sequence generation using a GAN architecture with efficient policy gradient estimators, 2. We attain improved training stability, and 3. We perform a comparative study of recent unbiased low variance gradient estimation techniques such as REBAR (Tucker et al., 2017), RELAX (Grathwohl et al., 2018) and REINFORCE (Williams, 1992). Using a simple grammar on synthetic datasets with varying length, we indicate the quality of sequences generated by the model.",0
"In recent years, Generative Adversarial Networks (GANs) have gained significant attention in the ML research community, especially due to their success in generating images. Despite this success, GANs have not been as effective in generating text sequences, particularly longer ones. However, the recent development of SeqGAN has shown promising results with improved adversarial evaluation and human evaluation compared to MLE based training. This paper presents three main contributions: 1) demonstrating results for sequence generation using an efficient GAN architecture with policy gradient estimators, 2) achieving improved training stability, and 3) conducting a comparative study of unbiased low variance gradient estimation techniques, such as REBAR, RELAX, and REINFORCE. The study uses a simple grammar on synthetic datasets with varying sequence lengths to evaluate the quality of generated sequences.",1
"In this paper, we study the problem of multi-domain image generation, the goal of which is to generate pairs of corresponding images from different domains. With the recent development in generative models, image generation has achieved great progress and has been applied to various computer vision tasks. However, multi-domain image generation may not achieve the desired performance due to the difficulty of learning the correspondence of different domain images, especially when the information of paired samples is not given. To tackle this problem, we propose Regularized Conditional GAN (RegCGAN) which is capable of learning to generate corresponding images in the absence of paired training data. RegCGAN is based on the conditional GAN, and we introduce two regularizers to guide the model to learn the corresponding semantics of different domains. We evaluate the proposed model on several tasks for which paired training data is not given, including the generation of edges and photos, the generation of faces with different attributes, etc. The experimental results show that our model can successfully generate corresponding images for all these tasks, while outperforms the baseline methods. We also introduce an approach of applying RegCGAN to unsupervised domain adaptation.",0
"The focus of this paper is on multi-domain image generation, which aims to produce pairs of corresponding images from distinct domains. Although image generation has made remarkable progress with the advancement of generative models and has been employed in various computer vision applications, multi-domain image generation can be challenging as it involves learning the correlation between images from different domains, particularly in the absence of paired data. To address this issue, we propose a Regularized Conditional GAN (RegCGAN) that utilizes two regularizers to guide the model in learning the corresponding semantics of different domains. Based on the conditional GAN, RegCGAN is capable of generating corresponding images even when paired training data is not available, as demonstrated by our experiments on tasks such as edge and photo generation, attribute-based face generation, and more. Our model outperforms other baseline methods, and we also explore the use of RegCGAN for unsupervised domain adaptation.",1
"Hyperspectral imaging holds enormous potential to improve the state-of-the-art in aerial vehicle tracking with low spatial and temporal resolutions. Recently, adaptive multi-modal hyperspectral sensors have attracted growing interest due to their ability to record extended data quickly from aerial platforms. In this study, we apply popular concepts from traditional object tracking, namely (1) Kernelized Correlation Filters (KCF) and (2) Deep Convolutional Neural Network (CNN) features to aerial tracking in hyperspectral domain. We propose the Deep Hyperspectral Kernelized Correlation Filter based tracker (DeepHKCF) to efficiently track aerial vehicles using an adaptive multi-modal hyperspectral sensor. We address low temporal resolution by designing a single KCF-in-multiple Regions-of-Interest (ROIs) approach to cover a reasonably large area. To increase the speed of deep convolutional features extraction from multiple ROIs, we design an effective ROI mapping strategy. The proposed tracker also provides flexibility to couple with the more advanced correlation filter trackers. The DeepHKCF tracker performs exceptionally well with deep features set up in a synthetic hyperspectral video generated by the Digital Imaging and Remote Sensing Image Generation (DIRSIG) software. Additionally, we generate a large, synthetic, single-channel dataset using DIRSIG to perform vehicle classification in the Wide Area Motion Imagery (WAMI) platform. This way, the high-fidelity of the DIRSIG software is proved and a large scale aerial vehicle classification dataset is released to support studies on vehicle detection and tracking in the WAMI platform.",0
"The potential of hyperspectral imaging to enhance aerial vehicle tracking is significant, especially in cases where low spatial and temporal resolutions are an issue. Adaptive multi-modal hyperspectral sensors have recently gained attention for their ability to quickly record extensive data from aerial platforms. This study explores the application of traditional object tracking concepts, such as Kernelized Correlation Filters (KCF) and Deep Convolutional Neural Network (CNN) features, to aerial tracking in the hyperspectral domain. To efficiently track aerial vehicles using an adaptive multi-modal hyperspectral sensor, we propose the Deep Hyperspectral Kernelized Correlation Filter-based tracker (DeepHKCF). Our approach addresses low temporal resolution by implementing a single KCF-in-multiple Regions-of-Interest (ROIs) method to cover a reasonably large area. Furthermore, we design an effective ROI mapping strategy to increase the speed of deep convolutional features extraction from multiple ROIs. The proposed tracker is also flexible and can be combined with more advanced correlation filter trackers. Our DeepHKCF tracker performs exceptionally well with deep features set up in a synthetic hyperspectral video generated by the Digital Imaging and Remote Sensing Image Generation (DIRSIG) software. Additionally, we generate a large, synthetic, single-channel dataset using DIRSIG to enable vehicle classification in the Wide Area Motion Imagery (WAMI) platform. This way, the high-fidelity of the DIRSIG software is validated, and a large-scale aerial vehicle classification dataset is released to support future studies on vehicle detection and tracking in the WAMI platform.",1
"Building on top of the success of generative adversarial networks (GANs), conditional GANs attempt to better direct the data generation process by conditioning with certain additional information. Inspired by the most recent AC-GAN, in this paper we propose a fast-converging conditional GAN (FC-GAN). In addition to the real/fake classifier used in vanilla GANs, our discriminator has an advanced auxiliary classifier which distinguishes each real class from an extra `fake' class. The `fake' class avoids mixing generated data with real data, which can potentially confuse the classification of real data as AC-GAN does, and makes the advanced auxiliary classifier behave as another real/fake classifier. As a result, FC-GAN can accelerate the process of differentiation of all classes, thus boost the convergence speed. Experimental results on image synthesis demonstrate our model is competitive in the quality of images generated while achieving a faster convergence rate.",0
"Conditional GANs build upon the success of generative adversarial networks (GANs) by incorporating additional information to direct the data generation process. This paper proposes a fast-converging conditional GAN (FC-GAN) inspired by the AC-GAN. The discriminator in FC-GAN has an advanced auxiliary classifier that distinguishes each real class from an extra ""fake"" class to prevent mixing generated data with real data, which can confuse classification. This makes the advanced auxiliary classifier behave as another real/fake classifier, accelerating the differentiation of all classes and boosting convergence speed. Experimental results show that FC-GAN generates high-quality images and achieves faster convergence rates.",1
"Generating images from natural language is one of the primary applications of recent conditional generative models. Besides testing our ability to model conditional, highly dimensional distributions, text to image synthesis has many exciting and practical applications such as photo editing or computer-aided content creation. Recent progress has been made using Generative Adversarial Networks (GANs). This material starts with a gentle introduction to these topics and discusses the existent state of the art models. Moreover, I propose Wasserstein GAN-CLS, a new model for conditional image generation based on the Wasserstein distance which offers guarantees of stability. Then, I show how the novel loss function of Wasserstein GAN-CLS can be used in a Conditional Progressive Growing GAN. In combination with the proposed loss, the model boosts by 7.07% the best Inception Score (on the Caltech birds dataset) of the models which use only the sentence-level visual semantics. The only model which performs better than the Conditional Wasserstein Progressive Growing GAN is the recently proposed AttnGAN which uses word-level visual semantics as well.",0
"Recent conditional generative models have found primary applications in generating images from natural language. This task not only tests our ability to model conditional, highly dimensional distributions, but also has exciting and practical applications such as photo editing or computer-aided content creation. Generative Adversarial Networks (GANs) have shown recent progress in this area. This material provides a gentle introduction to these topics and discusses the state of the art models. The proposed model is the Wasserstein GAN-CLS, which is a new conditional image generation model based on the Wasserstein distance. This model offers stability guarantees. The novel loss function of Wasserstein GAN-CLS can be used in a Conditional Progressive Growing GAN. The model, in combination with the proposed loss, boosts the best Inception Score (on the Caltech birds dataset) of the models which use only the sentence-level visual semantics by 7.07%. The only model that performs better than the Conditional Wasserstein Progressive Growing GAN is the recently proposed AttnGAN, which uses word-level visual semantics.",1
"Variational auto-encoders (VAEs) provide an attractive solution to image generation problem. However, they tend to produce blurred and over-smoothed images due to their dependence on pixel-wise reconstruction loss. This paper introduces a new approach to alleviate this problem in the VAE based generative models. Our model simultaneously learns to match the data, reconstruction loss and the latent distributions of real and fake images to improve the quality of generated samples. To compute the loss distributions, we introduce an auto-encoder based discriminator model which allows an adversarial learning procedure. The discriminator in our model also provides perceptual guidance to the VAE by matching the learned similarity metric of the real and fake samples in the latent space. To stabilize the overall training process, our model uses an error feedback approach to maintain the equilibrium between competing networks in the model. Our experiments show that the generated samples from our proposed model exhibit a diverse set of attributes and facial expressions and scale up to high-resolution images very well.",0
"The issue of blurred and over-smoothed images in Variational auto-encoders (VAEs) used for image generation has been addressed in this paper by proposing a new approach. This approach involves the use of an auto-encoder based discriminator model to compute the loss distributions and enable adversarial learning. The model also provides perceptual guidance to the VAE by matching the similarity metric of real and fake samples in the latent space. To ensure stability during training, an error feedback mechanism is implemented. Our experiments demonstrate that the proposed model generates diverse samples with various attributes and facial expressions, scaling well to high-resolution images.",1
"Person Re-identification (re-id) faces two major challenges: the lack of cross-view paired training data and learning discriminative identity-sensitive and view-invariant features in the presence of large pose variations. In this work, we address both problems by proposing a novel deep person image generation model for synthesizing realistic person images conditional on the pose. The model is based on a generative adversarial network (GAN) designed specifically for pose normalization in re-id, thus termed pose-normalization GAN (PN-GAN). With the synthesized images, we can learn a new type of deep re-id feature free of the influence of pose variations. We show that this feature is strong on its own and complementary to features learned with the original images. Importantly, under the transfer learning setting, we show that our model generalizes well to any new re-id dataset without the need for collecting any training data for model fine-tuning. The model thus has the potential to make re-id model truly scalable.",0
"Person Re-identification (re-id) is faced with two major challenges, namely, the lack of paired training data that can be used across different views, and the difficulty of learning features that are both identity-sensitive and view-invariant, particularly when there are significant pose variations. To address these challenges, we propose a novel deep learning model that generates realistic person images based on the pose. Our model, the pose-normalization GAN (PN-GAN), is specifically designed to normalize poses in re-id. By synthesizing images using this model, we can generate a new type of deep re-id feature that is not influenced by pose variations. This feature is strong on its own and complements the features learned using the original images. Importantly, our model can generalize well to new re-id datasets without requiring additional training data for fine-tuning. As such, our model has the potential to make re-id models more scalable.",1
"The task of face attribute manipulation has found increasing applications, but still remains challenging with the requirement of editing the attributes of a face image while preserving its unique details. In this paper, we choose to combine the Variational AutoEncoder (VAE) and Generative Adversarial Network (GAN) for photorealistic image generation. We propose an effective method to modify a modest amount of pixels in the feature maps of an encoder, changing the attribute strength continuously without hindering global information. Our training objectives of VAE and GAN are reinforced by the supervision of face recognition loss and cycle consistency loss for faithful preservation of face details. Moreover, we generate facial masks to enforce background consistency, which allows our training to focus on manipulating the foreground face rather than background. Experimental results demonstrate our method, called Mask-Adversarial AutoEncoder (M-AAE), can generate high-quality images with changing attributes and outperforms prior methods in detail preservation.",0
"The manipulation of face attributes is a difficult task as it requires altering specific features while maintaining the image's unique details. This paper seeks to address this challenge by utilizing a combination of the Variational AutoEncoder (VAE) and Generative Adversarial Network (GAN) to generate photorealistic images. Our proposed method modifies a small number of pixels in the encoder's feature maps to adjust attribute strength without interfering with the overall image. To preserve facial details, we incorporate face recognition loss and cycle consistency loss into our VAE and GAN training objectives. Additionally, we create facial masks to ensure background consistency and focus on the foreground. Our approach, called Mask-Adversarial AutoEncoder (M-AAE), generates high-quality images with changing attributes and surpasses previous methods in detail preservation, as demonstrated by our experimental results.",1
"It is unknown what kind of biases modern in the wild face datasets have because of their lack of annotation. A direct consequence of this is that total recognition rates alone only provide limited insight about the generalization ability of a Deep Convolutional Neural Networks (DCNNs). We propose to empirically study the effect of different types of dataset biases on the generalization ability of DCNNs. Using synthetically generated face images, we study the face recognition rate as a function of interpretable parameters such as face pose and light. The proposed method allows valuable details about the generalization performance of different DCNN architectures to be observed and compared. In our experiments, we find that: 1) Indeed, dataset bias has a significant influence on the generalization performance of DCNNs. 2) DCNNs can generalize surprisingly well to unseen illumination conditions and large sampling gaps in the pose variation. 3) Using the presented methodology we reveal that the VGG-16 architecture outperforms the AlexNet architecture at face recognition tasks because it can much better generalize to unseen face poses, although it has significantly more parameters. 4) We uncover a main limitation of current DCNN architectures, which is the difficulty to generalize when different identities to not share the same pose variation. 5) We demonstrate that our findings on synthetic data also apply when learning from real-world data. Our face image generator is publicly available to enable the community to benchmark other DCNN architectures.",0
"Due to the absence of annotation, it remains unclear what biases are present in modern in the wild face datasets. Consequently, relying solely on total recognition rates provides only limited insight into the Deep Convolutional Neural Networks' (DCNNs) generalization capability. To investigate the impact of different dataset biases on DCNNs' generalization ability, we propose an empirical study using synthetic face images. We evaluate the face recognition rate concerning interpretable parameters such as light and face pose. This method enables a detailed comparison of the generalization performance of different DCNN architectures. Our experiments demonstrate that dataset bias significantly affects DCNNs' generalization performance, but they can still generalize well to unseen pose variations and illumination conditions. Additionally, we find that VGG-16 outperforms AlexNet in face recognition tasks because it can better generalize to unseen face poses, despite having more parameters. However, we also uncover a limitation of current DCNN architectures, as they face challenges in generalizing when different identities do not share the same pose variation. Lastly, we demonstrate that our findings on synthetic data are also applicable to real-world data. We have made our face image generator publicly available for other DCNN architectures to benchmark their performance.",1
"The deep generative adversarial networks (GAN) recently have been shown to be promising for different computer vision applications, like image edit- ing, synthesizing high resolution images, generating videos, etc. These networks and the corresponding learning scheme can handle various visual space map- pings. We approach GANs with a novel training method and learning objective, to discover multiple object instances for three cases: 1) synthesizing a picture of a specific object within a cluttered scene; 2) localizing different categories in images for weakly supervised object detection; and 3) improving object discov- ery in object detection pipelines. A crucial advantage of our method is that it learns a new deep similarity metric, to distinguish multiple objects in one im- age. We demonstrate that the network can act as an encoder-decoder generating parts of an image which contain an object, or as a modified deep CNN to rep- resent images for object detection in supervised and weakly supervised scheme. Our ranking GAN offers a novel way to search through images for object specific patterns. We have conducted experiments for different scenarios and demonstrate the method performance for object synthesizing and weakly supervised object detection and classification using the MS-COCO and PASCAL VOC datasets.",0
"Recently, the use of deep generative adversarial networks (GANs) has shown promise in various computer vision applications, such as image editing, high-resolution image synthesis, and video generation. These networks, along with their corresponding learning schemes, can handle diverse visual space mappings. In this study, we introduce a novel training method and learning objective to GANs to discover multiple object instances for three scenarios: 1) synthesizing a specific object in a cluttered scene, 2) localizing different categories in images for weakly supervised object detection, and 3) improving object discovery in object detection pipelines. Our method has a significant advantage in that it learns a new deep similarity metric to distinguish multiple objects in an image. The network can act as an encoder-decoder to generate parts of an image containing an object or as a modified deep CNN to represent images for object detection in supervised and weakly supervised schemes. Our ranking GAN provides a unique way to search for object-specific patterns in images. We conducted experiments for various situations and demonstrated the method's performance for object synthesis and weakly supervised object detection and classification using the MS-COCO and PASCAL VOC datasets.",1
"In this paper, we propose the Cross-Domain Adversarial Auto-Encoder (CDAAE) to address the problem of cross-domain image inference, generation and transformation. We make the assumption that images from different domains share the same latent code space for content, while having separate latent code space for style. The proposed framework can map cross-domain data to a latent code vector consisting of a content part and a style part. The latent code vector is matched with a prior distribution so that we can generate meaningful samples from any part of the prior space. Consequently, given a sample of one domain, our framework can generate various samples of the other domain with the same content of the input. This makes the proposed framework different from the current work of cross-domain transformation. Besides, the proposed framework can be trained with both labeled and unlabeled data, which makes it also suitable for domain adaptation. Experimental results on data sets SVHN, MNIST and CASIA show the proposed framework achieved visually appealing performance for image generation task. Besides, we also demonstrate the proposed method achieved superior results for domain adaptation. Code of our experiments is available in https://github.com/luckycallor/CDAAE.",0
"The focus of this paper is on the Cross-Domain Adversarial Auto-Encoder (CDAAE) and its ability to solve the problem of cross-domain image inference, generation, and transformation. Our hypothesis is that images from various domains have a common latent code space for content, but a distinct latent code space for style. Our proposed method can convert cross-domain data into a latent code vector, which consists of content and style components. By matching the latent code vector with a prior distribution, we can generate meaningful samples from any part of the prior space. This sets our framework apart from existing cross-domain transformation techniques. In addition, the proposed framework can be trained using both labeled and unlabeled data, making it ideal for domain adaptation. We present experimental results on the SVHN, MNIST, and CASIA datasets, demonstrating that our approach achieves visually impressive results for image generation and superior performance for domain adaptation. The code used in our experiments is available for download on https://github.com/luckycallor/CDAAE.",1
"Harvesting dense pixel-level annotations to train deep neural networks for semantic segmentation is extremely expensive and unwieldy at scale. While learning from synthetic data where labels are readily available sounds promising, performance degrades significantly when testing on novel realistic data due to domain discrepancies. We present Dual Channel-wise Alignment Networks (DCAN), a simple yet effective approach to reduce domain shift at both pixel-level and feature-level. Exploring statistics in each channel of CNN feature maps, our framework performs channel-wise feature alignment, which preserves spatial structures and semantic information, in both an image generator and a segmentation network. In particular, given an image from the source domain and unlabeled samples from the target domain, the generator synthesizes new images on-the-fly to resemble samples from the target domain in appearance and the segmentation network further refines high-level features before predicting semantic maps, both of which leverage feature statistics of sampled images from the target domain. Unlike much recent and concurrent work relying on adversarial training, our framework is lightweight and easy to train. Extensive experiments on adapting models trained on synthetic segmentation benchmarks to real urban scenes demonstrate the effectiveness of the proposed framework.",0
"Collecting detailed annotations for deep neural network training in semantic segmentation is both costly and difficult to manage on a large scale. While using synthetic data with available labels seems like a viable alternative, performance suffers significantly when testing on realistic data that is new due to variations in domain. To address this issue, we introduce the Dual Channel-wise Alignment Networks (DCAN) approach, which is a straightforward yet efficient method for reducing domain shift at both pixel-level and feature-level. Our framework utilizes channel-specific feature alignment by examining statistics in each CNN feature map channel, which preserves spatial structures and semantic information in both an image generator and a segmentation network. The generator generates new images that resemble samples from the target domain in appearance, and the segmentation network further refines high-level features before predicting semantic maps, both of which leverage feature statistics of sampled images from the target domain. Unlike previous work that relies on adversarial training, our technique is lightweight and easy to train. Our extensive experiments demonstrate the effectiveness of the proposed framework for adapting models trained on synthetic segmentation benchmarks to real urban scenes.",1
"This paper studies the problem of blind face restoration from an unconstrained blurry, noisy, low-resolution, or compressed image (i.e., degraded observation). For better recovery of fine facial details, we modify the problem setting by taking both the degraded observation and a high-quality guided image of the same identity as input to our guided face restoration network (GFRNet). However, the degraded observation and guided image generally are different in pose, illumination and expression, thereby making plain CNNs (e.g., U-Net) fail to recover fine and identity-aware facial details. To tackle this issue, our GFRNet model includes both a warping subnetwork (WarpNet) and a reconstruction subnetwork (RecNet). The WarpNet is introduced to predict flow field for warping the guided image to correct pose and expression (i.e., warped guidance), while the RecNet takes the degraded observation and warped guidance as input to produce the restoration result. Due to that the ground-truth flow field is unavailable, landmark loss together with total variation regularization are incorporated to guide the learning of WarpNet. Furthermore, to make the model applicable to blind restoration, our GFRNet is trained on the synthetic data with versatile settings on blur kernel, noise level, downsampling scale factor, and JPEG quality factor. Experiments show that our GFRNet not only performs favorably against the state-of-the-art image and face restoration methods, but also generates visually photo-realistic results on real degraded facial images.",0
"The objective of this research paper is to explore the challenge of performing blind face restoration on degraded images that are blurry, noisy, low-resolution, or compressed. To enhance the recovery of intricate facial features, the authors propose a different approach where a high-quality guided image of the same individual is used along with the degraded observation as input for their guided face restoration network (GFRNet). However, since the guided image and degraded observation often differ in pose, illumination, and expression, traditional CNNs like U-Net are unable to restore fine details while preserving the individual's identity. To address this issue, the GFRNet model incorporates a WarpNet and a RecNet subnetwork. The WarpNet predicts a flow field to warp the guided image to a corrected pose and expression, which is then used for restoration by the RecNet, along with the degraded observation. Since ground-truth flow field is not available, the authors use landmark loss and total variation regularization to guide the learning of WarpNet. Additionally, to make the model applicable for blind restoration, GFRNet is trained on synthetic data with versatile settings on blur kernel, noise level, downsampling scale factor, and JPEG quality factor. The experiments demonstrate that GFRNet outperforms state-of-the-art image and face restoration methods, producing visually realistic results on real degraded facial images.",1
"Deep generative models have demonstrated great performance in image synthesis. However, results deteriorate in case of spatial deformations, since they generate images of objects directly, rather than modeling the intricate interplay of their inherent shape and appearance. We present a conditional U-Net for shape-guided image generation, conditioned on the output of a variational autoencoder for appearance. The approach is trained end-to-end on images, without requiring samples of the same object with varying pose or appearance. Experiments show that the model enables conditional image generation and transfer. Therefore, either shape or appearance can be retained from a query image, while freely altering the other. Moreover, appearance can be sampled due to its stochastic latent representation, while preserving shape. In quantitative and qualitative experiments on COCO, DeepFashion, shoes, Market-1501 and handbags, the approach demonstrates significant improvements over the state-of-the-art.",0
"Although deep generative models have shown impressive performance in image synthesis, they struggle when presented with spatial deformations. This is because they create images of objects directly, rather than modeling the complex interplay between shape and appearance. Our proposed solution is a conditional U-Net that uses the output of a variational autoencoder for appearance. This approach is trained end-to-end on images, eliminating the need for varied samples of the same object with different poses or appearances. Our experiments have shown that the model enables conditional image generation and transfer, allowing for either shape or appearance to be retained while changing the other. Additionally, the stochastic latent representation of appearance allows for sampling while preserving shape. Our approach has demonstrated significant improvements over the state-of-the-art in quantitative and qualitative experiments on various datasets, including COCO, DeepFashion, shoes, Market-1501, and handbags.",1
"It is well-known that GANs are difficult to train, and several different techniques have been proposed in order to stabilize their training. In this paper, we propose a novel training method called manifold-matching, and a new GAN model called manifold-matching GAN (MMGAN). MMGAN finds two manifolds representing the vector representations of real and fake images. If these two manifolds match, it means that real and fake images are statistically identical. To assist the manifold-matching task, we also use i) kernel tricks to find better manifold structures, ii) moving-averaged manifolds across mini-batches, and iii) a regularizer based on correlation matrix to suppress mode collapse.   We conduct in-depth experiments with three image datasets and compare with several state-of-the-art GAN models. 32.4% of images generated by the proposed MMGAN are recognized as fake images during our user study (16% enhancement compared to other state-of-the-art model). MMGAN achieved an unsupervised inception score of 7.8 for CIFAR-10.",0
"Numerous techniques have been proposed to stabilize the challenging training of GANs. In this study, we present a new training method called manifold-matching and a corresponding GAN model, MMGAN. Our approach involves finding two manifolds that represent the vector representations of real and fake images. If these manifolds match, it indicates that the real and fake images are statistically identical. We also employ kernel tricks to improve the manifold structure, use moving-averaged manifolds across mini-batches, and implement a regularizer based on correlation matrix to prevent mode collapse. We conduct extensive experiments with three image datasets and compare our results to other state-of-the-art GAN models. Our findings show that 32.4% of the images generated by MMGAN are recognized as fake images in our user study, which is a 16% increase compared to other models. Additionally, MMGAN achieved an unsupervised inception score of 7.8 for CIFAR-10.",1
"Generative Adversarial Networks (GANs) have been successfully used to synthesize realistically looking images of faces, scenery and even medical images. Unfortunately, they usually require large training datasets, which are often scarce in the medical field, and to the best of our knowledge GANs have been only applied for medical image synthesis at fairly low resolution. However, many state-of-the-art machine learning models operate on high resolution data as such data carries indispensable, valuable information. In this work, we try to generate realistically looking high resolution images of skin lesions with GANs, using only a small training dataset of 2000 samples. The nature of the data allows us to do a direct comparison between the image statistics of the generated samples and the real dataset. We both quantitatively and qualitatively compare state-of-the-art GAN architectures such as DCGAN and LAPGAN against a modification of the latter for the task of image generation at a resolution of 256x256px. Our investigation shows that we can approximate the real data distribution with all of the models, but we notice major differences when visually rating sample realism, diversity and artifacts. In a set of use-case experiments on skin lesion classification, we further show that we can successfully tackle the problem of heavy class imbalance with the help of synthesized high resolution melanoma samples.",0
"GANs have been successful in creating realistic images of faces, scenery, and medical images. However, due to the scarcity of large training datasets in the medical field, GANs have only been applied to medical image synthesis at low resolution. This is a problem as high resolution data carries valuable information. In this study, we use GANs to generate realistic high resolution images of skin lesions using a small training dataset of 2000 samples. We compare state-of-the-art GAN architectures such as DCGAN and LAPGAN against a modified version of LAPGAN for image generation at 256x256px resolution. Our investigation shows that all models can approximate the real data distribution, but there are noticeable differences in sample realism, diversity, and artifacts. Furthermore, we show that using synthesized high resolution melanoma samples, we can successfully address the problem of class imbalance in skin lesion classification.",1
"We study how to synthesize novel views of human body from a single image. Though recent deep learning based methods work well for rigid objects, they often fail on objects with large articulation, like human bodies. The core step of existing methods is to fit a map from the observable views to novel views by CNNs; however, the rich articulation modes of human body make it rather challenging for CNNs to memorize and interpolate the data well. To address the problem, we propose a novel deep learning based pipeline that explicitly estimates and leverages the geometry of the underlying human body. Our new pipeline is a composition of a shape estimation network and an image generation network, and at the interface a perspective transformation is applied to generate a forward flow for pixel value transportation. Our design is able to factor out the space of data variation and makes learning at each step much easier. Empirically, we show that the performance for pose-varying objects can be improved dramatically. Our method can also be applied on real data captured by 3D sensors, and the flow generated by our methods can be used for generating high quality results in higher resolution.",0
"Our research focuses on creating new perspectives of the human body using only one image. While deep learning methods have been successful with fixed objects, they struggle with objects like the human body that have many joint movements. Existing methods use CNNs to create a map from observable views to new views, but this is challenging for CNNs to do well due to the complexity of the human body's articulation modes. To overcome this issue, we propose a new deep learning pipeline that estimates and uses the geometry of the human body explicitly. Our method consists of a shape estimation network and an image generation network, with a perspective transformation applied at the interface for pixel value transportation. By separating the space of data variation, our method simplifies the learning process and significantly improves performance for pose-varying objects. Our approach can also be applied to real data captured by 3D sensors, and the flow generated by our methods can produce high-quality results in higher resolution.",1
"We propose a novel end-to-end semi-supervised adversarial framework to generate photorealistic face images of new identities with wide ranges of expressions, poses, and illuminations conditioned by a 3D morphable model. Previous adversarial style-transfer methods either supervise their networks with large volume of paired data or use unpaired data with a highly under-constrained two-way generative framework in an unsupervised fashion. We introduce pairwise adversarial supervision to constrain two-way domain adaptation by a small number of paired real and synthetic images for training along with the large volume of unpaired data. Extensive qualitative and quantitative experiments are performed to validate our idea. Generated face images of new identities contain pose, lighting and expression diversity and qualitative results show that they are highly constraint by the synthetic input image while adding photorealism and retaining identity information. We combine face images generated by the proposed method with the real data set to train face recognition algorithms. We evaluated the model on two challenging data sets: LFW and IJB-A. We observe that the generated images from our framework consistently improves over the performance of deep face recognition network trained with Oxford VGG Face dataset and achieves comparable results to the state-of-the-art.",0
"Our proposed approach presents a unique semi-supervised adversarial framework that can produce photorealistic face images of new individuals, complete with a broad range of expressions, poses, and illuminations, all based on a 3D morphable model. Previous methods for adversarial style-transfer either relied on a vast amount of paired data to supervise their networks or employed an unsupervised approach with unpaired data, resulting in an under-constrained two-way generative framework. Our method introduces pairwise adversarial supervision, which limits the two-way domain adaptation using a small number of paired real and synthetic images during training, in addition to a large volume of unpaired data. We conducted extensive qualitative and quantitative experiments to validate our approach, which demonstrated that our generated face images of new individuals contain diversity in terms of pose, lighting, and expression, while maintaining the identity information and being highly constrained by the synthetic input image, adding photorealism. We also combined the generated face images with the real dataset to train face recognition algorithms, and evaluated our model on two challenging datasets, LFW and IJB-A. Our results show that the generated images from our framework consistently outperform deep face recognition networks trained with the Oxford VGG Face dataset and achieve comparable results to the state-of-the-art.",1
"Existing methods for multi-domain image-to-image translation (or generation) attempt to directly map an input image (or a random vector) to an image in one of the output domains. However, most existing methods have limited scalability and robustness, since they require building independent models for each pair of domains in question. This leads to two significant shortcomings: (1) the need to train exponential number of pairwise models, and (2) the inability to leverage data from other domains when training a particular pairwise mapping. Inspired by recent work on module networks, this paper proposes ModularGAN for multi-domain image generation and image-to-image translation. ModularGAN consists of several reusable and composable modules that carry on different functions (e.g., encoding, decoding, transformations). These modules can be trained simultaneously, leveraging data from all domains, and then combined to construct specific GAN networks at test time, according to the specific image translation task. This leads to ModularGAN's superior flexibility of generating (or translating to) an image in any desired domain. Experimental results demonstrate that our model not only presents compelling perceptual results but also outperforms state-of-the-art methods on multi-domain facial attribute transfer.",0
"Most current methods for generating or translating images across multiple domains involve mapping an input image or random vector directly to an output domain image. However, these methods often have limited scalability and robustness because they require building multiple independent models for each domain pair, resulting in the need to train an exponential number of pairwise models. Additionally, these methods cannot leverage data from other domains while training a specific pairwise mapping. To address these shortcomings, this paper proposes ModularGAN, a multi-domain image generation and translation model that utilizes reusable and composable modules with different functionalities (e.g., encoding, decoding, transformations). These modules can be trained simultaneously, using data from all domains, and combined to construct specific GAN networks at test time for a desired image translation task. As a result, ModularGAN offers superior flexibility for generating or translating images in any desired domain. Experimental results demonstrate that our model not only produces compelling perceptual results but also outperforms state-of-the-art methods in multi-domain facial attribute transfer.",1
"Generative Adversarial Networks (GANs) have been promising in the field of image generation, however, they have been hard to train for language generation. GANs were originally designed to output differentiable values, so discrete language generation is challenging for them which causes high levels of instability in training GANs. Consequently, past work has resorted to pre-training with maximum-likelihood or training GANs without pre-training with a WGAN objective with a gradient penalty. In this study, we present a comparison of those approaches. Furthermore, we present the results of some experiments that indicate better training and convergence of Wasserstein GANs (WGANs) when a weaker regularization term is enforcing the Lipschitz constraint.",0
"Although Generative Adversarial Networks (GANs) have shown promise in image generation, they have been difficult to train for language generation due to their original design for differentiable outputs. This poses a challenge for discrete language generation and results in high levels of instability during training. Previous solutions have included pre-training with maximum-likelihood or training GANs without pre-training using a WGAN objective with a gradient penalty. In this study, we compare these approaches and present the outcomes of experiments that demonstrate improved training and convergence of Wasserstein GANs (WGANs) with a less restrictive regularization term enforcing the Lipschitz constraint.",1
"In this paper we address the problem of generating person images conditioned on a given pose. Specifically, given an image of a person and a target pose, we synthesize a new image of that person in the novel pose. In order to deal with pixel-to-pixel misalignments caused by the pose differences, we introduce deformable skip connections in the generator of our Generative Adversarial Network. Moreover, a nearest-neighbour loss is proposed instead of the common L1 and L2 losses in order to match the details of the generated image with the target image. We test our approach using photos of persons in different poses and we compare our method with previous work in this area showing state-of-the-art results in two benchmarks. Our method can be applied to the wider field of deformable object generation, provided that the pose of the articulated object can be extracted using a keypoint detector.",0
"The focus of this paper is on the creation of person images based on a given pose. Our objective is to generate a new image of a person in a novel pose, using a provided image and target pose. One challenge we faced was in dealing with pixel misalignments due to differences in poses. To address this, we incorporated deformable skip connections in the generator of our Generative Adversarial Network. Additionally, we proposed a nearest-neighbour loss to match the details of the generated image with the target image, instead of using the more common L1 and L2 losses. We conducted experiments on photos of people in various poses, comparing our approach with previous work in the field and achieving state-of-the-art results in two benchmarks. Our method can be applied to the generation of deformable objects, as long as the pose of the articulated object can be determined using a keypoint detector.",1
"To truly understand the visual world our models should be able not only to recognize images but also generate them. To this end, there has been exciting recent progress on generating images from natural language descriptions. These methods give stunning results on limited domains such as descriptions of birds or flowers, but struggle to faithfully reproduce complex sentences with many objects and relationships. To overcome this limitation we propose a method for generating images from scene graphs, enabling explicitly reasoning about objects and their relationships. Our model uses graph convolution to process input graphs, computes a scene layout by predicting bounding boxes and segmentation masks for objects, and converts the layout to an image with a cascaded refinement network. The network is trained adversarially against a pair of discriminators to ensure realistic outputs. We validate our approach on Visual Genome and COCO-Stuff, where qualitative results, ablations, and user studies demonstrate our method's ability to generate complex images with multiple objects.",0
"In order to gain a comprehensive understanding of the visual world, it's important for our models to not only be able to recognize images but also create them. Recently, there has been progress in generating images from natural language descriptions, but these methods have limitations when it comes to accurately reproducing complex sentences with many objects and relationships. To address this limitation, we propose a method for generating images from scene graphs, which allows for explicit reasoning about objects and their relationships. Our model uses graph convolution to process input graphs, predicts bounding boxes and segmentation masks for objects to compute a scene layout, and then converts the layout to an image using a cascaded refinement network. The network is trained using adversarial techniques against a pair of discriminators to ensure realistic outputs. We demonstrate the effectiveness of our approach on Visual Genome and COCO-Stuff through qualitative results, ablations, and user studies, showing that our method can generate complex images with multiple objects.",1
"Generative Adversarial Networks (GAN) have shown great promise in tasks like synthetic image generation, image inpainting, style transfer, and anomaly detection. However, generating discrete data is a challenge. This work presents an adversarial training based correlated discrete data (CDD) generation model. It also details an approach for conditional CDD generation. The results of our approach are presented over two datasets; job-seeking candidates skill set (private dataset) and MNIST (public dataset). From quantitative and qualitative analysis of these results, we show that our model performs better as it leverages inherent correlation in the data, than an existing model that overlooks correlation.",0
"The potential of Generative Adversarial Networks (GAN) has been demonstrated in various tasks such as image generation, image restoration, style transfer, and anomaly detection. However, generating discrete data poses a challenge. In this study, a correlated discrete data (CDD) generation model based on adversarial training is proposed, along with a method for conditional CDD generation. The effectiveness of our approach is demonstrated on two datasets: a private dataset of job applicants' skill sets and the public MNIST dataset. Through quantitative and qualitative analysis of the results, we demonstrate that our model outperforms an existing model that overlooks the inherent correlation in the data.",1
"Interaction and collaboration between humans and intelligent machines has become increasingly important as machine learning methods move into real-world applications that involve end users. While much prior work lies at the intersection of natural language and vision, such as image captioning or image generation from text descriptions, less focus has been placed on the use of language to guide or improve the performance of a learned visual processing algorithm. In this paper, we explore methods to flexibly guide a trained convolutional neural network through user input to improve its performance during inference. We do so by inserting a layer that acts as a spatio-semantic guide into the network. This guide is trained to modify the network's activations, either directly via an energy minimization scheme or indirectly through a recurrent model that translates human language queries to interaction weights. Learning the verbal interaction is fully automatic and does not require manual text annotations. We evaluate the method on two datasets, showing that guiding a pre-trained network can improve performance, and provide extensive insights into the interaction between the guide and the CNN.",0
"As machine learning techniques are implemented in real-world applications involving end users, the need for collaboration between humans and intelligent machines is increasingly important. While past research has focused on natural language and vision, such as generating images from text or image captioning, less attention has been given to using language to enhance the performance of a learned visual processing algorithm. This study aims to explore ways to guide a trained convolutional neural network through user input, in order to improve its performance during inference. To achieve this, a spatio-semantic guide is inserted into the network, which is trained to modify the network's activations either directly or indirectly through a recurrent model. The verbal interaction learning is automatic and does not require manual text annotations. The study demonstrates that guiding a pre-trained network can improve performance, and provides comprehensive insights into the interaction between the guide and the CNN. Two datasets were used for evaluation purposes.",1
"Generative models have made significant progress in the tasks of modeling complex data distributions such as natural images. The introduction of Generative Adversarial Networks (GANs) and auto-encoders lead to the possibility of training on big data sets in an unsupervised manner. However, for many generative models it is not possible to specify what kind of image should be generated and it is not possible to translate existing images into new images of similar domains. Furthermore, models that can perform image-to-image translation often need distinct models for each domain, making it hard to scale these systems to multiple domain image-to-image translation. We introduce a model that can do both, controllable image generation and image-to-image translation between multiple domains. We split our image representation into two parts encoding unstructured and structured information respectively. The latter is designed in a disentangled manner, so that different parts encode different image characteristics. We train an encoder to encode images into these representations and use a small amount of labeled data to specify what kind of information should be encoded in the disentangled part. A generator is trained to generate images from these representations using the characteristics provided by the disentangled part of the representation. Through this we can control what kind of images the generator generates, translate images between different domains, and even learn unknown data-generating factors while only using one single model.",0
"The development of Generative Adversarial Networks (GANs) and auto-encoders has greatly improved the ability of generative models to model complex data distributions, especially natural images. This has enabled unsupervised training on large datasets. However, many generative models lack the ability to specify the type of image to be generated or translate existing images into new images within similar domains. Additionally, models that can perform image-to-image translation require distinct models for each domain, which makes scaling difficult. Our proposed model addresses these limitations by splitting image representation into two parts that encode unstructured and structured information, respectively. The structured information is disentangled so that different parts encode different characteristics of the image. We train an encoder to encode images into these representations and use a small amount of labeled data to specify the information to be encoded in the disentangled part. A generator is then trained to generate images from these representations using the characteristics provided by the disentangled part of the representation. This allows for controlled image generation, image-to-image translation between multiple domains, and learning of unknown data-generating factors using a single model.",1
"Deep generative models learned through adversarial training have become increasingly popular for their ability to generate naturalistic image textures. However, aside from their texture, the visual appearance of objects is significantly influenced by their shape geometry; information which is not taken into account by existing generative models. This paper introduces the Geometry-Aware Generative Adversarial Networks (GAGAN) for incorporating geometric information into the image generation process. Specifically, in GAGAN the generator samples latent variables from the probability space of a statistical shape model. By mapping the output of the generator to a canonical coordinate frame through a differentiable geometric transformation, we enforce the geometry of the objects and add an implicit connection from the prior to the generated object. Experimental results on face generation indicate that the GAGAN can generate realistic images of faces with arbitrary facial attributes such as facial expression, pose, and morphology, that are of better quality than current GAN-based methods. Our method can be used to augment any existing GAN architecture and improve the quality of the images generated.",0
"Adversarial training has made deep generative models increasingly popular due to their ability to create naturalistic image textures. However, the appearance of objects is also heavily influenced by their shape geometry, which current generative models do not consider. To address this issue, the Geometry-Aware Generative Adversarial Networks (GAGAN) were developed to incorporate geometric information into the image generation process. GAGAN's generator samples latent variables from a statistical shape model, and a differentiable geometric transformation enforces object geometry by mapping the generator's output to a canonical coordinate frame. The GAGAN generates realistic images of faces with arbitrary facial attributes, including expression, pose, and morphology, of better quality than current GAN-based methods. Our approach can be used to enhance existing GAN architectures and improve image quality.",1
"We present a Polyhedral Scene Generator system which creates a random scene based on a few user parameters, renders the scene from random view points and creates a dataset containing the renderings and corresponding annotation files. We hope that this generator will enable research on how a program could parse a scene if it had multiple viewpoints to consider. For ambiguous scenes, typically people move their head or change their position to see the scene from different angles as well as seeing how it changes while they move; this research field is called active perception. The random scene generator presented is designed to support research in this field by generating images of scenes with known complexity characteristics and with verifiable properties with respect to the distribution of features across a population. Thus, it is well-suited for research in active perception without the requirement of a live 3D environment and mobile sensing agent, including comparative performance evaluations. The system is publicly available at https://polyhedral.eecs.yorku.ca.",0
"Our system, the Polyhedral Scene Generator, can create a randomized scene based on a few user inputs. It then renders the scene from a variety of different viewpoints and produces a dataset that includes the corresponding annotation files. Our aim is to facilitate research into how a program might parse a scene when presented with multiple viewpoints. People often change their position or move their head to view different angles of ambiguous scenes, which is called active perception. Our generator can support research in this area by creating images of scenes with known complexity and verified feature distributions that are suitable for comparative performance evaluations. This can be achieved without the need for a live 3D environment or mobile sensing agent. The system is available to the public at https://polyhedral.eecs.yorku.ca.",1
"Purpose: Probe-based Confocal Laser Endomicroscopy (pCLE) is a recent imaging modality that allows performing in vivo optical biopsies. The design of pCLE hardware, and its reliance on an optical fibre bundle, fundamentally limits the image quality with a few tens of thousands fibres, each acting as the equivalent of a single-pixel detector, assembled into a single fibre bundle. Video-registration techniques can be used to estimate high-resolution (HR) images by exploiting the temporal information contained in a sequence of low-resolution (LR) images. However, the alignment of LR frames, required for the fusion, is computationally demanding and prone to artefacts. Methods: In this work, we propose a novel synthetic data generation approach to train exemplar-based Deep Neural Networks (DNNs). HR pCLE images with enhanced quality are recovered by the models trained on pairs of estimated HR images (generated by the video-registration algorithm) and realistic synthetic LR images. Performance of three different state-of-the-art DNNs techniques were analysed on a Smart Atlas database of 8806 images from 238 pCLE video sequences. The results were validated through an extensive Image Quality Assessment (IQA) that takes into account different quality scores, including a Mean Opinion Score (MOS). Results: Results indicate that the proposed solution produces an effective improvement in the quality of the obtained reconstructed image. Conclusion: The proposed training strategy and associated DNNs allows us to perform convincing super-resolution of pCLE images.",0
"The objective of this study is to explore the potential of Probe-based Confocal Laser Endomicroscopy (pCLE) as a tool for in vivo optical biopsies. However, the image quality of pCLE is limited due to the hardware design, which relies on an optical fibre bundle consisting of a few tens of thousands of fibres that act as single-pixel detectors. Video-registration techniques can be used to improve image quality by aligning low-resolution (LR) images to estimate high-resolution (HR) images. However, this process is computationally demanding and prone to artefacts. To address this issue, the authors propose a novel synthetic data generation approach that trains Deep Neural Networks (DNNs) to enhance HR pCLE images. The performance of three different DNNs techniques was evaluated on a database of 8806 images from 238 pCLE video sequences. The results were validated through an extensive Image Quality Assessment (IQA), including a Mean Opinion Score (MOS). The study indicates that the proposed solution effectively improves the quality of reconstructed images, demonstrating the potential of this approach for super-resolution of pCLE images.",1
"We present Optimal Transport GAN (OT-GAN), a variant of generative adversarial nets minimizing a new metric measuring the distance between the generator distribution and the data distribution. This metric, which we call mini-batch energy distance, combines optimal transport in primal form with an energy distance defined in an adversarially learned feature space, resulting in a highly discriminative distance function with unbiased mini-batch gradients. Experimentally we show OT-GAN to be highly stable when trained with large mini-batches, and we present state-of-the-art results on several popular benchmark problems for image generation.",0
"Our latest development is Optimal Transport GAN (OT-GAN), a modified version of generative adversarial nets that focuses on reducing the gap between the generator distribution and the data distribution. The new metric we introduce, mini-batch energy distance, merges optimal transport in primal form with an energy distance established in an adversarially learned feature space. This combination produces a distance function that is both highly discriminative and has unbiased mini-batch gradients. We conducted experiments that demonstrate OT-GAN's remarkable stability when trained with large mini-batches. Furthermore, we achieved state-of-the-art results on various popular benchmark problems for image generation.",1
"Generating high fidelity identity-preserving faces with different facial attributes has a wide range of applications. Although a number of generative models have been developed to tackle this problem, there is still much room for further improvement.In paticular, the current solutions usually ignore the perceptual information of images, which we argue that it benefits the output of a high-quality image while preserving the identity information, especially in facial attributes learning area.To this end, we propose to train GAN iteratively via regularizing the min-max process with an integrated loss, which includes not only the per-pixel loss but also the perceptual loss. In contrast to the existing methods only deal with either image generation or transformation, our proposed iterative architecture can achieve both of them. Experiments on the multi-label facial dataset CelebA demonstrate that the proposed model has excellent performance on recognizing multiple attributes, generating a high-quality image, and transforming image with controllable attributes.",0
"The development of generative models for creating realistic faces with various facial features has numerous practical applications. However, despite existing solutions, there is still significant room for improvement. One major issue is that current models fail to consider perceptual information, which is crucial for achieving high-quality images while preserving identity information, particularly in the realm of facial attribute learning. Therefore, we suggest a new approach that involves training a GAN iteratively with an integrated loss function. This loss function incorporates both per-pixel and perceptual losses. Unlike previous models that only focus on image generation or transformation, our proposed iterative architecture can accomplish both. Our experiments using the CelebA dataset demonstrate that our model performs exceptionally well at recognizing multiple attributes, generating high-quality images, and transforming images with controllable attributes.",1
"Low-end and compact mobile cameras demonstrate limited photo quality mainly due to space, hardware and budget constraints. In this work, we propose a deep learning solution that translates photos taken by cameras with limited capabilities into DSLR-quality photos automatically. We tackle this problem by introducing a weakly supervised photo enhancer (WESPE) - a novel image-to-image Generative Adversarial Network-based architecture. The proposed model is trained by under weak supervision: unlike previous works, there is no need for strong supervision in the form of a large annotated dataset of aligned original/enhanced photo pairs. The sole requirement is two distinct datasets: one from the source camera, and one composed of arbitrary high-quality images that can be generally crawled from the Internet - the visual content they exhibit may be unrelated. Hence, our solution is repeatable for any camera: collecting the data and training can be achieved in a couple of hours. In this work, we emphasize on extensive evaluation of obtained results. Besides standard objective metrics and subjective user study, we train a virtual rater in the form of a separate CNN that mimics human raters on Flickr data and use this network to get reference scores for both original and enhanced photos. Our experiments on the DPED, KITTI and Cityscapes datasets as well as pictures from several generations of smartphones demonstrate that WESPE produces comparable or improved qualitative results with state-of-the-art strongly supervised methods.",0
"The photo quality of low-end and compact mobile cameras is limited due to constraints in space, hardware, and budget. This paper proposes a solution that utilizes deep learning to automatically enhance photos taken by such cameras to DSLR-quality. The proposed solution is a weakly supervised photo enhancer (WESPE) that utilizes an image-to-image Generative Adversarial Network-based architecture. Unlike previous works, this model is trained under weak supervision, which eliminates the need for a large annotated dataset of aligned original/enhanced photo pairs. The only requirement is two distinct datasets: one from the source camera and another composed of arbitrary high-quality images that can be generally found on the Internet. This solution is repeatable for any camera and can be achieved in just a few hours. The study also includes extensive evaluation of the results, including standard objective metrics, subjective user study, and a virtual rater in the form of a separate CNN that mimics human raters on Flickr data. The experiments conducted on the DPED, KITTI, and Cityscapes datasets, as well as pictures from several generations of smartphones, demonstrate that WESPE produces comparable or improved qualitative results with state-of-the-art strongly supervised methods.",1
"Sky/cloud images obtained from ground-based sky-cameras are usually captured using a fish-eye lens with a wide field of view. However, the sky exhibits a large dynamic range in terms of luminance, more than a conventional camera can capture. It is thus difficult to capture the details of an entire scene with a regular camera in a single shot. In most cases, the circumsolar region is over-exposed, and the regions near the horizon are under-exposed. This renders cloud segmentation for such images difficult. In this paper, we propose HDRCloudSeg -- an effective method for cloud segmentation using High-Dynamic-Range (HDR) imaging based on multi-exposure fusion. We describe the HDR image generation process and release a new database to the community for benchmarking. Our proposed approach is the first using HDR radiance maps for cloud segmentation and achieves very good results.",0
"Typically, ground-based sky-cameras use a fish-eye lens with a wide field of view to capture images of the sky and clouds. However, the sky has a broad range of luminance, making it difficult for a regular camera to capture all the details in a single shot. Often, the circumsolar region is over-exposed, and the horizon regions are under-exposed, hindering cloud segmentation. Therefore, we introduce HDRCloudSeg, a novel approach to cloud segmentation that utilizes High-Dynamic-Range (HDR) imaging through multi-exposure fusion. We outline the process of generating HDR images and present a new database for benchmarking purposes. Our technique is the first to employ HDR radiance maps for cloud segmentation and delivers excellent results.",1
"Neural text generation models are often autoregressive language models or seq2seq models. These models generate text by sampling words sequentially, with each word conditioned on the previous word, and are state-of-the-art for several machine translation and summarization benchmarks. These benchmarks are often defined by validation perplexity even though this is not a direct measure of the quality of the generated text. Additionally, these models are typically trained via maxi- mum likelihood and teacher forcing. These methods are well-suited to optimizing perplexity but can result in poor sample quality since generating text requires conditioning on sequences of words that may have never been observed at training time. We propose to improve sample quality using Generative Adversarial Networks (GANs), which explicitly train the generator to produce high quality samples and have shown a lot of success in image generation. GANs were originally designed to output differentiable values, so discrete language generation is challenging for them. We claim that validation perplexity alone is not indicative of the quality of text generated by a model. We introduce an actor-critic conditional GAN that fills in missing text conditioned on the surrounding context. We show qualitatively and quantitatively, evidence that this produces more realistic conditional and unconditional text samples compared to a maximum likelihood trained model.",0
"Typically, neural text generation models are either autoregressive language models or seq2seq models. These models generate text by selecting words sequentially, with each word being dependent on the preceding word. They are deemed state-of-the-art for various machine translation and summarization benchmarks, which are typically evaluated through validation perplexity. However, it is vital to note that validation perplexity does not directly measure the text's quality generated by these models. These models are commonly trained using maximum likelihood and teacher forcing, which are effective in optimizing perplexity but may lead to poor sample quality. To enhance the quality of generated texts, we propose using Generative Adversarial Networks (GANs), which have proved successful in image generation but face challenges in generating discrete language. We argue that validation perplexity is insufficient in determining the quality of the generated text and suggest using an actor-critic conditional GAN to fill in missing text based on the contextual information. Our approach improves the quality of both conditional and unconditional text samples compared to maximum likelihood trained models, as demonstrated through qualitative and quantitative analyses.",1
"This paper addresses a challenging problem -- how to generate multi-view cloth images from only a single view input. To generate realistic-looking images with different views from the input, we propose a new image generation model termed VariGANs that combines the strengths of the variational inference and the Generative Adversarial Networks (GANs). Our proposed VariGANs model generates the target image in a coarse-to-fine manner instead of a single pass which suffers from severe artifacts. It first performs variational inference to model global appearance of the object (e.g., shape and color) and produce a coarse image with a different view. Conditioned on the generated low resolution images, it then proceeds to perform adversarial learning to fill details and generate images of consistent details with the input. Extensive experiments conducted on two clothing datasets, MVC and DeepFashion, have demonstrated that images of a novel view generated by our model are more plausible than those generated by existing approaches, in terms of more consistent global appearance as well as richer and sharper details.",0
"The main focus of this paper is to tackle the challenging task of generating multi-view cloth images using only a single view input. To achieve this, we present a novel image generation model called VariGANs, which combines the strengths of the Generative Adversarial Networks (GANs) and the variational inference. Our proposed model generates the target image in a coarse-to-fine manner, unlike the traditional single-pass method that generates images with severe artifacts. Initially, the model performs variational inference to model the global appearance of the object, such as its shape and color, to produce a coarse image with a different view. Subsequently, the model proceeds to perform adversarial learning to fill in the details and generate images with consistent details with the input, conditioned on the generated low-resolution images. We conducted extensive experiments on two datasets, namely MVC and DeepFashion, which demonstrated that our model generates images with a more plausible novel view than existing approaches, in terms of consistent global appearance and richer and sharper details.",1
"We consider the problem of binary image generation with given properties. This problem arises in a number of practical applications, including generation of artificial porous medium for an electrode of lithium-ion batteries, for composed materials, etc. A generated image represents a porous medium and, as such, it is subject to two sets of constraints: topological constraints on the structure and process constraints on the physical process over this structure. To perform image generation we need to define a mapping from a porous medium to its physical process parameters. For a given geometry of a porous medium, this mapping can be done by solving a partial differential equation (PDE). However, embedding a PDE solver into the search procedure is computationally expensive. We use a binarized neural network to approximate a PDE solver. This allows us to encode the entire problem as a logical formula. Our main contribution is that, for the first time, we show that this problem can be tackled using decision procedures. Our experiments show that our model is able to produce random constrained images that satisfy both topological and process constraints.",0
"The issue of generating binary images with specific properties is a problem that arises in various practical applications, such as manufacturing artificial porous materials for lithium-ion battery electrodes and composite materials. The images produced represent a porous medium and must comply with both topological and process constraints. To generate such images, a mapping from porous medium to physical process parameters must be defined, typically by solving a partial differential equation (PDE). However, including a PDE solver in the search process can be computationally expensive. To address this, we use a binarized neural network to approximate the PDE solver, enabling us to encode the entire problem as a logical formula. Our main contribution is demonstrating that decision procedures can be used to solve this problem. Our experiments show that our model can generate random images that adhere to both topological and process constraints.",1
"We propose an interactive image-manipulation system with natural language instruction, which can generate a target image from a source image and an instruction that describes the difference between the source and the target image. The system makes it possible to modify a generated image interactively and make natural language conditioned image generation more controllable. We construct a neural network that handles image vectors in latent space to transform the source vector to the target vector by using the vector of instruction. The experimental results indicate that the proposed framework successfully generates the target image by using a source image and an instruction on manipulation in our dataset.",0
"Our proposal is for a system that allows for interactive image manipulation through natural language instruction. This system is capable of creating a desired target image from a source image and an instruction that specifies the changes between the two images. With this system, users can modify the generated image and have greater control over natural language-based image generation. We have developed a neural network that operates on image vectors in latent space, using the instruction vector to transform the source vector into the target vector. Our experiments demonstrate that our framework is able to successfully generate target images from source images and manipulation instructions within our dataset.",1
"Synthesizing realistic images from text descriptions on a dataset like Microsoft Common Objects in Context (MS COCO), where each image can contain several objects, is a challenging task. Prior work has used text captions to generate images. However, captions might not be informative enough to capture the entire image and insufficient for the model to be able to understand which objects in the images correspond to which words in the captions. We show that adding a dialogue that further describes the scene leads to significant improvement in the inception score and in the quality of generated images on the MS COCO dataset.",0
"Generating realistic images from textual descriptions on a dataset such as MS COCO can be a difficult task due to the presence of multiple objects in each image. Previous studies have utilized text captions for image generation, but this approach may not be comprehensive enough to convey the full image details and does not provide clarity on how the objects in the image correspond to the words in the captions. Our research demonstrates that incorporating a dialogue that elaborates on the scene significantly enhances the inception score and improves the quality of the generated images on MS COCO.",1
"This paper addresses the problem of 3D human pose estimation in the wild. A significant challenge is the lack of training data, i.e., 2D images of humans annotated with 3D poses. Such data is necessary to train state-of-the-art CNN architectures. Here, we propose a solution to generate a large set of photorealistic synthetic images of humans with 3D pose annotations. We introduce an image-based synthesis engine that artificially augments a dataset of real images with 2D human pose annotations using 3D motion capture data. Given a candidate 3D pose, our algorithm selects for each joint an image whose 2D pose locally matches the projected 3D pose. The selected images are then combined to generate a new synthetic image by stitching local image patches in a kinematically constrained manner. The resulting images are used to train an end-to-end CNN for full-body 3D pose estimation. We cluster the training data into a large number of pose classes and tackle pose estimation as a $K$-way classification problem. Such an approach is viable only with large training sets such as ours. Our method outperforms most of the published works in terms of 3D pose estimation in controlled environments (Human3.6M) and shows promising results for real-world images (LSP). This demonstrates that CNNs trained on artificial images generalize well to real images. Compared to data generated from more classical rendering engines, our synthetic images do not require any domain adaptation or fine-tuning stage.",0
"The issue of estimating 3D human poses in challenging environments is addressed in this study, which identifies the lack of 2D images with 3D pose annotations as a significant obstacle to training current CNN models. To overcome this, the authors propose a novel solution that generates a large collection of photorealistic synthetic images of humans with 3D pose annotations. An image-based synthesis engine is introduced that augments a dataset of real images with 2D human pose annotations using 3D motion capture data. The algorithm selects an image whose 2D pose locally matches the projected 3D pose for each joint of a candidate 3D pose. The selected images are then combined, and local patches are stitched together in a kinematically constrained way to generate a new synthetic image. An end-to-end CNN is trained using these resulting images for full-body 3D pose estimation. The training data is clustered into numerous pose classes, and pose estimation is treated as a K-way classification problem. This approach is only feasible with large training sets like the one used here. The proposed method outperforms most of the previously published works in terms of 3D pose estimation in controlled environments (Human3.6M) and shows promising results for real-world images (LSP). The study demonstrates that CNNs trained on artificial images generalize well to real images. Furthermore, compared to data generated from more traditional rendering engines, the synthetic images produced here do not need any domain adaptation or fine-tuning stage.",1
"How to build a good model for image generation given an abstract concept is a fundamental problem in computer vision. In this paper, we explore a generative model for the task of generating unseen images with desired features. We propose the Generative Cooperative Net (GCN) for image generation. The idea is similar to generative adversarial networks except that the generators and discriminators are trained to work accordingly. Our experiments on hand-written digit generation and facial expression generation show that GCN's two cooperative counterparts (the generator and the classifier) can work together nicely and achieve promising results. We also discovered a usage of such generative model as an data-augmentation tool. Our experiment of applying this method on a recognition task shows that it is very effective comparing to other existing methods. It is easy to set up and could help generate a very large synthesized dataset.",0
"The development of a high-quality model for generating images based on an abstract concept is a crucial issue in the field of computer vision. In this study, we investigate a generative model that can produce unseen images with specific characteristics. Our proposed method, called the Generative Cooperative Net (GCN), is similar to generative adversarial networks, but its generators and discriminators are trained to work together in harmony. Our experiments on hand-written digit and facial expression generation demonstrate that the GCN's two cooperative components can collaborate effectively and achieve promising outcomes. Additionally, we discovered that this generative model can be used as a data augmentation tool, as demonstrated by our successful application of it to a recognition task. This method is straightforward to implement and can generate a vast synthesized dataset.",1
"This paper proposes the novel Pose Guided Person Generation Network (PG$^2$) that allows to synthesize person images in arbitrary poses, based on an image of that person and a novel pose. Our generation framework PG$^2$ utilizes the pose information explicitly and consists of two key stages: pose integration and image refinement. In the first stage the condition image and the target pose are fed into a U-Net-like network to generate an initial but coarse image of the person with the target pose. The second stage then refines the initial and blurry result by training a U-Net-like generator in an adversarial way. Extensive experimental results on both 128$\times$64 re-identification images and 256$\times$256 fashion photos show that our model generates high-quality person images with convincing details.",0
"The Pose Guided Person Generation Network (PG$^2$) is a new approach proposed in this paper for creating person images in various poses using an image of the person and a novel pose. PG$^2$ utilizes pose information in two stages: pose integration and image refinement. In the first stage, a U-Net-like network is used to generate a basic image of the person in the target pose. The second stage refines the initial image using an adversarial U-Net-like generator. Our model has been tested on 128$\times$64 re-identification images and 256$\times$256 fashion photos, and the results demonstrate that it can generate high-quality person images with realistic details.",1
"Volumetric lesion segmentation via medical imaging is a powerful means to precisely assess multiple time-point lesion/tumor changes. Because manual 3D segmentation is prohibitively time consuming and requires radiological experience, current practices rely on an imprecise surrogate called response evaluation criteria in solid tumors (RECIST). Despite their coarseness, RECIST marks are commonly found in current hospital picture and archiving systems (PACS), meaning they can provide a potentially powerful, yet extraordinarily challenging, source of weak supervision for full 3D segmentation. Toward this end, we introduce a convolutional neural network based weakly supervised self-paced segmentation (WSSS) method to 1) generate the initial lesion segmentation on the axial RECIST-slice; 2) learn the data distribution on RECIST-slices; 3) adapt to segment the whole volume slice by slice to finally obtain a volumetric segmentation. In addition, we explore how super-resolution images (2~5 times beyond the physical CT imaging), generated from a proposed stacked generative adversarial network, can aid the WSSS performance. We employ the DeepLesion dataset, a comprehensive CT-image lesion dataset of 32,735 PACS-bookmarked findings, which include lesions, tumors, and lymph nodes of varying sizes, categories, body regions and surrounding contexts. These are drawn from 10,594 studies of 4,459 patients. We also validate on a lymph-node dataset, where 3D ground truth masks are available for all images. For the DeepLesion dataset, we report mean Dice coefficients of 93% on RECIST-slices and 76% in 3D lesion volumes. We further validate using a subjective user study, where an experienced radiologist accepted our WSSS-generated lesion segmentation results with a high probability of 92.4%.",0
"Medical imaging provides a powerful means for volumetric lesion segmentation, enabling precise assessment of changes in lesions/tumors over time. However, manual 3D segmentation is time-consuming and requires radiological expertise, leading to reliance on a less accurate surrogate called response evaluation criteria in solid tumors (RECIST). Despite their limitations, RECIST markings are commonly used in hospital picture and archiving systems (PACS), offering a potentially challenging source of weak supervision for full 3D segmentation. To address this, we present a weakly supervised self-paced segmentation (WSSS) method using a convolutional neural network. This approach generates an initial lesion segmentation on the axial RECIST-slice, learns the data distribution on RECIST-slices, and adapts to segment the entire volume slice by slice to achieve volumetric segmentation. Additionally, we explore the use of super-resolution images generated by a stacked generative adversarial network to aid in WSSS performance. We evaluate our method on the DeepLesion dataset, reporting mean Dice coefficients of 93% on RECIST-slices and 76% in 3D lesion volumes. We also validate our approach through a user study, with an experienced radiologist accepting our generated lesion segmentation results with a high probability of 92.4%.",1
"Incorporating encoding-decoding nets with adversarial nets has been widely adopted in image generation tasks. We observe that the state-of-the-art achievements were obtained by carefully balancing the reconstruction loss and adversarial loss, and such balance shifts with different network structures, datasets, and training strategies. Empirical studies have demonstrated that an inappropriate weight between the two losses may cause instability, and it is tricky to search for the optimal setting, especially when lacking prior knowledge on the data and network.   This paper gives the first attempt to relax the need of manual balancing by proposing the concept of \textit{decoupled learning}, where a novel network structure is designed that explicitly disentangles the backpropagation paths of the two losses.   Experimental results demonstrate the effectiveness, robustness, and generality of the proposed method. The other contribution of the paper is the design of a new evaluation metric to measure the image quality of generative models. We propose the so-called \textit{normalized relative discriminative score} (NRDS), which introduces the idea of relative comparison, rather than providing absolute estimates like existing metrics.",0
"The combination of encoding-decoding nets and adversarial nets has become a popular approach for generating images. However, achieving optimal results requires a delicate balance between the reconstruction and adversarial losses, which varies depending on the network structure, dataset, and training method. Studies have shown that an improper weighting of the losses can lead to instability and finding the best setting can be challenging without prior knowledge. This paper introduces the concept of decoupled learning, which uses a new network structure to separate the backpropagation paths of the two losses, eliminating the need for manual balancing. The proposed method is effective, robust, and applicable to various datasets. Additionally, the paper presents a new evaluation metric, the normalized relative discriminative score (NRDS), which measures image quality through relative comparison instead of absolute estimates.",1
"In this paper, we propose a novel deep neural network model that reconstructs a high dynamic range (HDR) image from a single low dynamic range (LDR) image. The proposed model is based on a convolutional neural network composed of dilated convolutional layers, and infers LDR images with various exposures and illumination from a single LDR image of the same scene. Then, the final HDR image can be formed by merging these inference results. It is relatively easy for the proposed method to find the mapping between the LDR and an HDR with a different bit depth because of the chaining structure inferring the relationship between the LDR images with brighter (or darker) exposures from a given LDR image. The method not only extends the range, but also has the advantage of restoring the light information of the actual physical world. For the HDR images obtained by the proposed method, the HDR-VDP2 Q score, which is the most popular evaluation metric for HDR images, was 56.36 for a display with a 1920$\times$1200 resolution, which is an improvement of 6 compared with the scores of conventional algorithms. In addition, when comparing the peak signal-to-noise ratio values for tone mapped HDR images generated by the proposed and conventional algorithms, the average value obtained by the proposed algorithm is 30.86 dB, which is 10 dB higher than those obtained by the conventional algorithms.",0
"This paper introduces a new deep neural network model that is capable of generating a high dynamic range (HDR) image from a single low dynamic range (LDR) image. The model is based on a convolutional neural network that uses dilated convolutional layers to infer LDR images with varying exposures and illumination from a single LDR image of the same scene. The final HDR image is then created by merging these inference results. The proposed method easily finds the mapping between the LDR and an HDR with a different bit depth due to its chaining structure, which infers the relationship between LDR images with brighter or darker exposures from a given LDR image. This method not only extends the range but also restores the light information of the actual physical world. The HDR-VDP2 Q score, which is the most popular evaluation metric for HDR images, was 56.36 for a display with a 1920x1200 resolution, indicating a 6-point improvement compared to conventional algorithms. Additionally, when comparing the peak signal-to-noise ratio values for tone-mapped HDR images generated by the proposed and conventional algorithms, the average value obtained by the proposed algorithm is 30.86 dB, which is 10 dB higher than those obtained by the conventional algorithms.",1
"Smile detection from unconstrained facial images is a specialized and challenging problem. As one of the most informative expressions, smiles convey basic underlying emotions, such as happiness and satisfaction, which lead to multiple applications, e.g., human behavior analysis and interactive controlling. Compared to the size of databases for face recognition, far less labeled data is available for training smile detection systems. To leverage the large amount of labeled data from face recognition datasets and to alleviate overfitting on smile detection, an efficient transfer learning-based smile detection approach is proposed in this paper. Unlike previous works which use either hand-engineered features or train deep convolutional networks from scratch, a well-trained deep face recognition model is explored and fine-tuned for smile detection in the wild. Three different models are built as a result of fine-tuning the face recognition model with different inputs, including aligned, unaligned and grayscale images generated from the GENKI-4K dataset. Experiments show that the proposed approach achieves improved state-of-the-art performance. Robustness of the model to noise and blur artifacts is also evaluated in this paper.",0
"Detecting smiles from facial images that are not controlled is a complex and specialized task. Smiling is an expressive expression that conveys fundamental emotions such as happiness and contentment, which have numerous applications, including human behavior analysis and interactive control. Despite the enormous amount of data available for face recognition, there is a scarcity of labeled data for training smile detection systems. This paper presents a transfer learning-based approach to smile detection, which leverages the large quantity of labeled data from face recognition datasets and reduces overfitting. Unlike previous studies that use handcrafted features or train deep convolutional networks from scratch, this approach fine-tunes a well-trained deep face recognition model for smile detection in unconstrained environments. Three models are created by fine-tuning the face recognition model with different inputs, including aligned, unaligned, and grayscale images from the GENKI-4K dataset. The proposed approach achieves state-of-the-art performance, as demonstrated by experiments. Additionally, the model's robustness to noise and blur artifacts is evaluated in this paper.",1
"We present FusedGAN, a deep network for conditional image synthesis with controllable sampling of diverse images. Fidelity, diversity and controllable sampling are the main quality measures of a good image generation model. Most existing models are insufficient in all three aspects. The FusedGAN can perform controllable sampling of diverse images with very high fidelity. We argue that controllability can be achieved by disentangling the generation process into various stages. In contrast to stacked GANs, where multiple stages of GANs are trained separately with full supervision of labeled intermediate images, the FusedGAN has a single stage pipeline with a built-in stacking of GANs. Unlike existing methods, which requires full supervision with paired conditions and images, the FusedGAN can effectively leverage more abundant images without corresponding conditions in training, to produce more diverse samples with high fidelity. We achieve this by fusing two generators: one for unconditional image generation, and the other for conditional image generation, where the two partly share a common latent space thereby disentangling the generation. We demonstrate the efficacy of the FusedGAN in fine grained image generation tasks such as text-to-image, and attribute-to-face generation.",0
"FusedGAN is a deep network that allows for the creation of diverse images with controllable sampling, while maintaining high fidelity. Three main quality measures of image generation models are fidelity, diversity, and controllable sampling, but most existing models fail to meet all three criteria. FusedGAN achieves controllability by separating the generation process into multiple stages, unlike stacked GANs which require separate training with labeled intermediate images. By fusing two generators, one for unconditional image generation and the other for conditional image generation, FusedGAN disentangles the generation process and allows for the use of abundant images without corresponding conditions in training. This results in more diverse samples with high fidelity. FusedGAN is effective in tasks such as text-to-image and attribute-to-face generation.",1
"Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the ""Fr\'echet Inception Distance"" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.",0
"GANs are known for their ability to generate realistic images using complex models that are difficult to optimize through maximum likelihood. However, the convergence of GAN training remains uncertain. To address this, we propose a two time-scale update rule (TTUR) that utilizes individual learning rates for the discriminator and generator during stochastic gradient descent for arbitrary GAN loss functions. Through the application of stochastic approximation theory, we demonstrate that TTUR achieves convergence under mild assumptions to a stationary local Nash equilibrium. Furthermore, we prove that popular optimization method Adam follows the dynamics of a heavy ball with friction, favoring flat minima in the objective landscape. We introduce the Fréchet Inception Distance (FID) for evaluating GAN image generation performance, which outperforms the Inception Score. In experiments, we show that TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP), outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.",1
"Contrast and quality of ultrasound images are adversely affected by the excessive presence of speckle. However, being an inherent imaging property, speckle helps in tissue characterization and tracking. Thus, despeckling of the ultrasound images requires the reduction of speckle extent without any oversmoothing. In this letter, we aim to address the despeckling problem using an unsupervised deep adversarial approach. A despeckling residual neural network (DRNN) is trained with an adversarial loss imposed by a discriminator. The discriminator tries to differentiate between the despeckled images generated by the DRNN and the set of high-quality images. Further to prevent the developed DRNN from oversmoothing, a structural loss term is used along with the adversarial loss. Experimental evaluations show that the proposed DRNN is able to outperform the state-of-the-art despeckling approaches.",0
"Excessive speckle can negatively impact the contrast and quality of ultrasound images. Nonetheless, speckle is an inherent property that aids in tissue characterization and tracking. To despeckle ultrasound images effectively, speckle extent must be reduced without compromising the smoothness of the image. This letter introduces an unsupervised deep adversarial approach to address the despeckling problem. A despeckling residual neural network (DRNN) is trained with an adversarial loss imposed by a discriminator that distinguishes between the despeckled images generated by the DRNN and a set of high-quality images. To prevent oversmoothing, a structural loss term is also utilized. Experimental evaluations demonstrate that the proposed DRNN outperforms other state-of-the-art despeckling methods.",1
"Semantic layouts based Image synthesizing, which has benefited from the success of Generative Adversarial Network (GAN), has drawn much attention in these days. How to enhance the synthesis image equality while keeping the stochasticity of the GAN is still a challenge. We propose a novel denoising framework to handle this problem. The overlapped objects generation is another challenging task when synthesizing images from a semantic layout to a realistic RGB photo. To overcome this deficiency, we include a one-hot semantic label map to force the generator paying more attention on the overlapped objects generation. Furthermore, we improve the loss function of the discriminator by considering perturb loss and cascade layer loss to guide the generation process. We applied our methods on the Cityscapes, Facades and NYU datasets and demonstrate the image generation ability of our model.",0
"In recent times, there has been considerable interest in Semantic layouts based Image synthesizing, which has been greatly aided by the success of Generative Adversarial Network (GAN). However, maintaining the stochasticity of the GAN while improving the quality of the synthesized image remains a challenge. To address this problem, we propose a new denoising framework. Additionally, generating overlapped objects when synthesizing images from semantic layout to realistic RGB photo is another challenge. To overcome this, we introduce a one-hot semantic label map to help the generator pay more attention to overlapped objects. We also enhance the loss function of the discriminator by incorporating perturb loss and cascade layer loss to guide the generation process. We tested our approach on Cityscapes, Facades, and NYU datasets and demonstrate the impressive image generation ability of our model.",1
"This paper introduces a deep learning enabled generative sensing framework which integrates low-end sensors with computational intelligence to attain a high recognition accuracy on par with that attained with high-end sensors. The proposed generative sensing framework aims at transforming low-end, low-quality sensor data into higher quality sensor data in terms of achieved classification accuracy. The low-end data can be transformed into higher quality data of the same modality or into data of another modality. Different from existing methods for image generation, the proposed framework is based on discriminative models and targets to maximize the recognition accuracy rather than a similarity measure. This is achieved through the introduction of selective feature regeneration in a deep neural network (DNN). The proposed generative sensing will essentially transform low-quality sensor data into high-quality information for robust perception. Results are presented to illustrate the performance of the proposed framework.",0
"A new approach to generative sensing is presented in this paper, which utilizes deep learning and computational intelligence to achieve high recognition accuracy comparable to that of high-end sensors. The framework aims to enhance low-quality sensor data by transforming it into higher quality data, either within the same modality or another. Unlike previous methods, which are based on similarity measures, this framework utilizes discriminative models and strives to maximize recognition accuracy. This is accomplished through selective feature regeneration in a deep neural network. The proposed generative sensing has the potential to convert low-quality sensor data into high-quality information for robust perception. The results demonstrate the effectiveness of the framework.",1
"Currently there is strong interest in data-driven approaches to medical image classification. However, medical imaging data is scarce, expensive, and fraught with legal concerns regarding patient privacy. Typical consent forms only allow for patient data to be used in medical journals or education, meaning the majority of medical data is inaccessible for general public research. We propose a novel, two-stage pipeline for generating synthetic medical images from a pair of generative adversarial networks, tested in practice on retinal fundi images. We develop a hierarchical generation process to divide the complex image generation task into two parts: geometry and photorealism. We hope researchers will use our pipeline to bring private medical data into the public domain, sparking growth in imaging tasks that have previously relied on the hand-tuning of models. We have begun this initiative through the development of SynthMed, an online repository for synthetic medical images.",0
"Medical image classification is currently a topic of great interest, with a focus on data-driven approaches. However, due to patient privacy concerns and the scarcity and expense of medical imaging data, accessing this data for research purposes is difficult. Consent forms typically only allow for medical journal or education use, which leaves the majority of medical data inaccessible for general research. To address this issue, we propose a two-stage pipeline that uses generative adversarial networks to create synthetic medical images. Our pipeline, which has been tested on retinal fundi images, utilizes a hierarchical generation process to divide the image creation task into geometry and photorealism. By providing researchers with access to our pipeline, we hope to encourage the use of private medical data in public research and to facilitate growth in imaging tasks that have previously relied on manually tuned models. To support this initiative, we have created SynthMed, an online repository for synthetic medical images.",1
"Emojis have become a very popular part of daily digital communication. Their appeal comes largely in part due to their ability to capture and elicit emotions in a more subtle and nuanced way than just plain text is able to. In line with recent advances in the field of deep learning, there are far reaching implications and applications that generative adversarial networks (GANs) can have for image generation. In this paper, we present a novel application of deep convolutional GANs (DC-GANs) with an optimized training procedure. We show that via incorporation of word embeddings conditioned on Google's word2vec model into the network, the generator is able to synthesize highly realistic emojis that are virtually identical to the real ones.",0
"The use of emojis has gained immense popularity in daily digital communication due to their ability to convey emotions in a more nuanced and subtle manner than plain text. With the advancements in deep learning, generative adversarial networks (GANs) have far-reaching implications for image generation. This paper introduces a new application of deep convolutional GANs (DC-GANs) with an optimized training procedure. By incorporating word embeddings conditioned on Google's word2vec model into the network, the generator can create highly realistic emojis that are virtually indistinguishable from real ones.",1
"Deep generative models are reported to be useful in broad applications including image generation. Repeated inference between data space and latent space in these models can denoise cluttered images and improve the quality of inferred results. However, previous studies only qualitatively evaluated image outputs in data space, and the mechanism behind the inference has not been investigated. The purpose of the current study is to numerically analyze changes in activity patterns of neurons in the latent space of a deep generative model called a ""variational auto-encoder"" (VAE). What kinds of inference dynamics the VAE demonstrates when noise is added to the input data are identified. The VAE embeds a dataset with clear cluster structures in the latent space and the center of each cluster of multiple correlated data points (memories) is referred as the concept. Our study demonstrated that transient dynamics of inference first approaches a concept, and then moves close to a memory. Moreover, the VAE revealed that the inference dynamics approaches a more abstract concept to the extent that the uncertainty of input data increases due to noise. It was demonstrated that by increasing the number of the latent variables, the trend of the inference dynamics to approach a concept can be enhanced, and the generalization ability of the VAE can be improved.",0
"Reports suggest that deep generative models have a wide range of applications, including image generation. These models can improve the quality of results by denoising cluttered images through repeated inference between data and latent space. However, previous studies only conducted qualitative evaluations of image outputs in data space, and the mechanism behind the inference was not explored. This study aims to analyze numerical changes in neuron activity patterns in the latent space of a deep generative model known as a ""variational auto-encoder"" (VAE). The study identifies the inference dynamics of VAE when noise is added to the input data. The VAE embeds a dataset with clear cluster structures in the latent space, and the center of each cluster of multiple correlated data points is referred to as a ""concept."" The study shows that the inference dynamics of VAE first approaches a concept and then moves closer to a memory. Moreover, the VAE reveals that the inference dynamics approach a more abstract concept as the uncertainty of input data increases due to noise. Increasing the number of latent variables enhances the trend of the inference dynamics to approach a concept and improves the generalization ability of the VAE.",1
"Stochastic image reconstruction is a key part of modern digital rock physics and materials analysis that aims to create numerous representative samples of material micro-structures for upscaling, numerical computation of effective properties and uncertainty quantification. We present a method of three-dimensional stochastic image reconstruction based on generative adversarial neural networks (GANs). GANs represent a framework of unsupervised learning methods that require no a priori inference of the probability distribution associated with the training data. Using a fully convolutional neural network allows fast sampling of large volumetric images.We apply a GAN based workflow of network training and image generation to an oolitic Ketton limestone micro-CT dataset. Minkowski functionals, effective permeability as well as velocity distributions of simulated flow within the acquired images are compared with the synthetic reconstructions generated by the deep neural network. While our results show that GANs allow a fast and accurate reconstruction of the evaluated image dataset, we address a number of open questions and challenges involved in the evaluation of generative network-based methods.",0
"In contemporary digital rock physics and materials analysis, stochastic image reconstruction plays a vital role in creating representative samples of material micro-structures for upscaling, numerical computation of effective properties, and uncertainty quantification. Our study introduces a three-dimensional stochastic image reconstruction method that utilizes generative adversarial neural networks (GANs). GANs offer an unsupervised learning approach that eliminates the need for prior inference of the probability distribution associated with the training data. By utilizing a fully convolutional neural network, we can quickly sample large volumetric images. We applied a GAN-based workflow to reconstruct the micro-CT data of oolitic Ketton limestone. We compared the Minkowski functionals, effective permeability, and velocity distributions of simulated flow with the synthetic reconstructions generated by the deep neural network. Although our results demonstrate that GANs provide fast and precise reconstruction of the evaluated image dataset, we acknowledge the challenges and open questions regarding the evaluation of generative network-based techniques.",1
"Compatibility between items, such as clothes and shoes, is a major factor among customer's purchasing decisions. However, learning ""compatibility"" is challenging due to (1) broader notions of compatibility than those of similarity, (2) the asymmetric nature of compatibility, and (3) only a small set of compatible and incompatible items are observed. We propose an end-to-end trainable system to embed each item into a latent vector and project a query item into K compatible prototypes in the same space. These prototypes reflect the broad notions of compatibility. We refer to both the embedding and prototypes as ""Compatibility Family"". In our learned space, we introduce a novel Projected Compatibility Distance (PCD) function which is differentiable and ensures diversity by aiming for at least one prototype to be close to a compatible item, whereas none of the prototypes are close to an incompatible item. We evaluate our system on a toy dataset, two Amazon product datasets, and Polyvore outfit dataset. Our method consistently achieves state-of-the-art performance. Finally, we show that we can visualize the candidate compatible prototypes using a Metric-regularized Conditional Generative Adversarial Network (MrCGAN), where the input is a projected prototype and the output is a generated image of a compatible item. We ask human evaluators to judge the relative compatibility between our generated images and images generated by CGANs conditioned directly on query items. Our generated images are significantly preferred, with roughly twice the number of votes as others.",0
"The compatibility of items, such as clothing and footwear, plays a vital role in customers' purchasing decisions. However, grasping the concept of ""compatibility"" is challenging due to various factors, including broader notions of compatibility compared to similarity, the asymmetric nature of compatibility, and limited observations of compatible and incompatible items. To address this issue, we propose an end-to-end trainable system that uses latent vectors to embed each item and project a query item into K compatible prototypes in the same space. These prototypes reflect the broader notions of compatibility and are referred to as the ""Compatibility Family."" We introduce a differentiable Projected Compatibility Distance (PCD) function in our learned space, which ensures diversity by aiming for at least one prototype to be close to a compatible item and none to an incompatible item. Our system achieves state-of-the-art performance on a toy dataset, two Amazon product datasets, and Polyvore outfit dataset. Furthermore, we visualize the compatible prototypes using a Metric-regularized Conditional Generative Adversarial Network (MrCGAN), where the input is a projected prototype, and the output is a generated image of a compatible item. Human evaluators judge our generated images to be significantly preferred, with roughly twice the number of votes compared to others.",1
"The effectiveness of generative adversarial approaches in producing images according to a specific style or visual domain has recently opened new directions to solve the unsupervised domain adaptation problem. It has been shown that source labeled images can be modified to mimic target samples making it possible to train directly a classifier in the target domain, despite the original lack of annotated data. Inverse mappings from the target to the source domain have also been evaluated but only passing through adapted feature spaces, thus without new image generation. In this paper we propose to better exploit the potential of generative adversarial networks for adaptation by introducing a novel symmetric mapping among domains. We jointly optimize bi-directional image transformations combining them with target self-labeling. Moreover we define a new class consistency loss that aligns the generators in the two directions imposing to conserve the class identity of an image passing through both domain mappings. A detailed qualitative and quantitative analysis of the reconstructed images confirm the power of our approach. By integrating the two domain specific classifiers obtained with our bi-directional network we exceed previous state-of-the-art unsupervised adaptation results on four different benchmark datasets.",0
"Recently, the use of generative adversarial techniques to produce images in a particular style or visual domain has opened up new possibilities to solve the unsupervised domain adaptation problem. By modifying source labeled images to mimic target samples, it is possible to train a classifier directly in the target domain, even in the absence of annotated data. However, inverse mappings from the target to the source domain have only been evaluated through adapted feature spaces, without new image generation. In this paper, we propose a new approach to more effectively utilize generative adversarial networks for adaptation by introducing a symmetric mapping between domains. We optimize bi-directional image transformations and combine them with target self-labeling while also defining a new class consistency loss that aligns the generators in both directions and ensures the conservation of an image's class identity through both domain mappings. Our detailed qualitative and quantitative analysis confirms the effectiveness of our approach, which surpasses previous state-of-the-art unsupervised adaptation results on four different benchmark datasets by integrating the two domain-specific classifiers obtained with our bi-directional network.",1
"Generative Adversarial Networks are proved to be efficient on various kinds of image generation tasks. However, it is still a challenge if we want to generate images precisely. Many researchers focus on how to generate images with one attribute. But image generation under multiple attributes is still a tough work. In this paper, we try to generate a variety of face images under multiple constraints using a pipeline process. The Pip-GAN (Pipeline Generative Adversarial Network) we present employs a pipeline network structure which can generate a complex facial image step by step using a neutral face image. We applied our method on two face image databases and demonstrate its ability to generate convincing novel images of unseen identities under multiple conditions previously.",0
"Various image generation tasks have proven the efficiency of Generative Adversarial Networks. However, generating precise images remains a challenge, particularly when attempting to generate images with multiple attributes. While many researchers have focused on generating images with a single attribute, dealing with multiple attributes is still difficult. This paper presents the Pip-GAN (Pipeline Generative Adversarial Network) approach, which uses a pipeline network structure to generate complex facial images step-by-step from a neutral face image. We tested our approach on two face image databases, successfully generating convincing novel images of unseen identities under multiple conditions.",1
"To realize the full potential of deep learning for medical imaging, large annotated datasets are required for training. Such datasets are difficult to acquire because labeled medical images are not usually available due to privacy issues, lack of experts available for annotation, underrepresentation of rare conditions and poor standardization. Lack of annotated data has been addressed in conventional vision applications using synthetic images refined via unsupervised adversarial training to look like real images. However, this approach is difficult to extend to general medical imaging because of the complex and diverse set of features found in real human tissues. We propose an alternative framework that uses a reverse flow, where adversarial training is used to make real medical images more like synthetic images, and hypothesize that clinically-relevant features can be preserved via self-regularization. These domain-adapted images can then be accurately interpreted by networks trained on large datasets of synthetic medical images. We test this approach for the notoriously difficult task of depth-estimation from endoscopy. We train a depth estimator on a large dataset of synthetic images generated using an accurate forward model of an endoscope and an anatomically-realistic colon. This network predicts significantly better depths when using synthetic-like domain-adapted images compared to the real images, confirming that the clinically-relevant features of depth are preserved.",0
"In order to fully utilize deep learning for medical imaging, the availability of extensive annotated datasets is crucial for training. However, obtaining such datasets can be challenging due to various factors such as privacy concerns, scarcity of expert annotators, uneven representation of rare conditions, and lack of standardization. While conventional vision applications have employed synthetic images that undergo unsupervised adversarial training to resemble real images, this approach is unsuitable for medical imaging due to the intricate and diverse features found in human tissues. To address this, we present an alternative framework that employs reverse flow, where adversarial training is utilized to make authentic medical images more akin to synthetic images, and propose that self-regularization can preserve clinically-relevant features. These domain-adapted images can then be accurately interpreted by networks that have been trained on large datasets of synthetic medical images. We demonstrate the effectiveness of this approach in the challenging task of depth-estimation from endoscopy. By training a depth estimator on a vast synthetic dataset generated through an accurate endoscope forward model and anatomically-realistic colon, we observe that the network predicts depths more precisely using domain-adapted images that resemble synthetic images, indicating preservation of clinically-relevant features.",1
"In this paper, we propose an Attentional Generative Adversarial Network (AttnGAN) that allows attention-driven, multi-stage refinement for fine-grained text-to-image generation. With a novel attentional generative network, the AttnGAN can synthesize fine-grained details at different subregions of the image by paying attentions to the relevant words in the natural language description. In addition, a deep attentional multimodal similarity model is proposed to compute a fine-grained image-text matching loss for training the generator. The proposed AttnGAN significantly outperforms the previous state of the art, boosting the best reported inception score by 14.14% on the CUB dataset and 170.25% on the more challenging COCO dataset. A detailed analysis is also performed by visualizing the attention layers of the AttnGAN. It for the first time shows that the layered attentional GAN is able to automatically select the condition at the word level for generating different parts of the image.",0
"Our paper introduces the Attentional Generative Adversarial Network (AttnGAN), a system that refines text-to-image generation through attention-driven, multi-stage processing. Using a unique attentional generative network, AttnGAN can create fine-grained details in various regions of an image by focusing on relevant words in the natural language description. To train the generator, we propose a deep attentional multimodal similarity model that calculates a fine-grained image-text matching loss. Our AttnGAN outperforms previous state-of-the-art models by 14.14% on the CUB dataset and 170.25% on the more challenging COCO dataset. We also visually analyzed the system's attention layers and found that it can automatically select word-level conditions to generate different parts of the image, a first in the field.",1
"Recent research has demonstrated the ability to estimate gaze on mobile devices by performing inference on the image from the phone's front-facing camera, and without requiring specialized hardware. While this offers wide potential applications such as in human-computer interaction, medical diagnosis and accessibility (e.g., hands free gaze as input for patients with motor disorders), current methods are limited as they rely on collecting data from real users, which is a tedious and expensive process that is hard to scale across devices. There have been some attempts to synthesize eye region data using 3D models that can simulate various head poses and camera settings, however these lack in realism.   In this paper, we improve upon a recently suggested method, and propose a generative adversarial framework to generate a large dataset of high resolution colorful images with high diversity (e.g., in subjects, head pose, camera settings) and realism, while simultaneously preserving the accuracy of gaze labels. The proposed approach operates on extended regions of the eye, and even completes missing parts of the image. Using this rich synthesized dataset, and without using any additional training data from real users, we demonstrate improvements over state-of-the-art for estimating 2D gaze position on mobile devices. We further demonstrate cross-device generalization of model performance, as well as improved robustness to diverse head pose, blur and distance.",0
"New research indicates that it is possible to estimate gaze on mobile devices by analyzing the image from the front-facing camera without specialized hardware. This method has numerous potential applications, including human-computer interaction, medical diagnosis, and hands-free gaze input for patients with motor disorders. However, current techniques rely on collecting data from real users, which is both expensive and difficult to scale across devices. Previous attempts to synthesize eye region data using 3D models have lacked realism. In this study, researchers propose a generative adversarial framework to generate a large dataset of high-resolution, colorful images with high diversity and realism. This approach operates on extended regions of the eye and can even complete missing parts of the image. By using this synthesized dataset, researchers improve the accuracy of gaze estimation on mobile devices without using any additional training data from real users. The new approach also demonstrates cross-device generalization of model performance, as well as improved robustness to diverse head poses, blur, and distance.",1
"Deconvolutional layers have been widely used in a variety of deep models for up-sampling, including encoder-decoder networks for semantic segmentation and deep generative models for unsupervised learning. One of the key limitations of deconvolutional operations is that they result in the so-called checkerboard problem. This is caused by the fact that no direct relationship exists among adjacent pixels on the output feature map. To address this problem, we propose the pixel deconvolutional layer (PixelDCL) to establish direct relationships among adjacent pixels on the up-sampled feature map. Our method is based on a fresh interpretation of the regular deconvolution operation. The resulting PixelDCL can be used to replace any deconvolutional layer in a plug-and-play manner without compromising the fully trainable capabilities of original models. The proposed PixelDCL may result in slight decrease in efficiency, but this can be overcome by an implementation trick. Experimental results on semantic segmentation demonstrate that PixelDCL can consider spatial features such as edges and shapes and yields more accurate segmentation outputs than deconvolutional layers. When used in image generation tasks, our PixelDCL can largely overcome the checkerboard problem suffered by regular deconvolution operations.",0
"Deconvolutional layers are commonly employed in deep models to increase resolution, including encoder-decoder networks for semantic segmentation and deep generative models for unsupervised learning. However, deconvolutional operations suffer from the checkerboard problem, which occurs due to a lack of direct correlation between neighboring pixels on the output feature map. To resolve this issue, we propose the PixelDCL, a pixel deconvolutional layer that creates direct relationships among adjacent pixels on the up-sampled feature map. Our approach reinterprets the regular deconvolution operation, and PixelDCL can be seamlessly integrated into existing models without compromising their trainable capabilities. Although our method may slightly reduce efficiency, we offer an implementation trick to overcome this. In semantic segmentation applications, PixelDCL can recognize spatial features such as edges and shapes, resulting in more precise segmentation outputs than deconvolutional layers. Moreover, when used in image generation tasks, our PixelDCL can effectively eliminate the checkerboard problem associated with regular deconvolution operations.",1
"Image captioning is an important but challenging task, applicable to virtual assistants, editing tools, image indexing, and support of the disabled. Its challenges are due to the variability and ambiguity of possible image descriptions. In recent years significant progress has been made in image captioning, using Recurrent Neural Networks powered by long-short-term-memory (LSTM) units. Despite mitigating the vanishing gradient problem, and despite their compelling ability to memorize dependencies, LSTM units are complex and inherently sequential across time. To address this issue, recent work has shown benefits of convolutional networks for machine translation and conditional image generation. Inspired by their success, in this paper, we develop a convolutional image captioning technique. We demonstrate its efficacy on the challenging MSCOCO dataset and demonstrate performance on par with the baseline, while having a faster training time per number of parameters. We also perform a detailed analysis, providing compelling reasons in favor of convolutional language generation approaches.",0
"The task of image captioning is essential yet difficult as it has various applications in virtual assistants, image indexing, editing tools, and support for the disabled. The main challenge arises from the diverse and uncertain image descriptions. Recently, Recurrent Neural Networks with long-short-term-memory (LSTM) units have shown significant progress in image captioning. However, LSTM units are intricate and rely on sequential processing over time, despite addressing the vanishing gradient problem and being able to memorize dependencies. To overcome this limitation, convolutional networks have been successful in machine translation and conditional image generation. Therefore, this study proposes a convolutional image captioning approach, which displays similar performance levels as the baseline with a faster training time per number of parameters. Furthermore, a detailed analysis is conducted, highlighting the advantages of convolutional language generation methods.",1
"Generative adversarial networks (GANs) are a powerful framework for generative tasks. However, they are difficult to train and tend to miss modes of the true data generation process. Although GANs can learn a rich representation of the covered modes of the data in their latent space, the framework misses an inverse mapping from data to this latent space. We propose Invariant Encoding Generative Adversarial Networks (IVE-GANs), a novel GAN framework that introduces such a mapping for individual samples from the data by utilizing features in the data which are invariant to certain transformations. Since the model maps individual samples to the latent space, it naturally encourages the generator to cover all modes. We demonstrate the effectiveness of our approach in terms of generative performance and learning rich representations on several datasets including common benchmark image generation tasks.",0
"GANs are a potent tool for generative tasks, but they have training difficulties and often fail to capture all the modes of data generation. While GANs can learn a complex representation of data modes, they lack an inverse mapping from data to the latent space. To address this, we introduce the Invariant Encoding Generative Adversarial Networks (IVE-GANs) framework, which integrates a mapping for individual data samples to the latent space using features that are invariant to specific transformations. By mapping individual samples to the latent space, our model encourages the generator to cover all modes, resulting in improved generative performance and richer representations across various datasets, including standard image generation benchmarks.",1
"Key to automatically generate natural scene images is to properly arrange among various spatial elements, especially in the depth direction. To this end, we introduce a novel depth structure preserving scene image generation network (DSP-GAN), which favors a hierarchical and heterogeneous architecture, for the purpose of depth structure preserving scene generation. The main trunk of the proposed infrastructure is built on a Hawkes point process that models the spatial dependency between different depth layers. Within each layer generative adversarial sub-networks are trained collaboratively to generate realistic scene components, conditioned on the layer information produced by the point process. We experiment our model on a sub-set of SUNdataset with annotated scene images and demonstrate that our models are capable of generating depth-realistic natural scene image.",0
"In order to automatically create natural scene images, it is crucial to effectively organize the various spatial elements, particularly in terms of depth. Therefore, we have developed a new scene image generation network called DSP-GAN, which prioritizes a hierarchical and diverse structure to preserve depth and generate realistic scenes. The main component of this network is a Hawkes point process that models the spatial relationships between different depths. In each layer, generative adversarial sub-networks generate authentic scene components, based on the layer information provided by the point process. Using a subset of SUNdataset containing annotated scene images, we conducted experiments and showed that our model can produce depth-realistic natural scene images.",1
"We present a method for reconstructing images viewed by observers based only on their eye movements. By exploring the relationships between gaze patterns and image stimuli, the ""What Are You Looking At?"" (WAYLA) system learns to synthesize photo-realistic images that are similar to the original pictures being viewed. The WAYLA approach is based on the Conditional Generative Adversarial Network (Conditional GAN) image-to-image translation technique of Isola et al. We consider two specific applications - the first, of reconstructing newspaper images from gaze heat maps, and the second, of detailed reconstruction of images containing only text. The newspaper image reconstruction process is divided into two image-to-image translation operations, the first mapping gaze heat maps into image segmentations, and the second mapping the generated segmentation into a newspaper image. We validate the performance of our approach using various evaluation metrics, along with human visual inspection. All results confirm the ability of our network to perform image generation tasks using eye tracking data.",0
"Our study introduces a novel approach to recreate images based solely on eye movements of observers. The ""What Are You Looking At?"" (WAYLA) system utilizes the Conditional Generative Adversarial Network (Conditional GAN) image-to-image translation technique of Isola et al to learn the association between gaze patterns and image stimuli, thereby generating photo-realistic images similar to the original ones. Our research focuses on two specific applications - the first being the reconstruction of newspaper images from gaze heat maps, and the second involving the detailed reconstruction of images with text. We accomplish this by performing two image-to-image translation operations, which map gaze heat maps into image segmentations followed by generating newspaper images. We evaluate the performance of our system using various metrics and human visual inspection, which confirm the successful generation of images from eye tracking data.",1
"A Triangle Generative Adversarial Network ($\Delta$-GAN) is developed for semi-supervised cross-domain joint distribution matching, where the training data consists of samples from each domain, and supervision of domain correspondence is provided by only a few paired samples. $\Delta$-GAN consists of four neural networks, two generators and two discriminators. The generators are designed to learn the two-way conditional distributions between the two domains, while the discriminators implicitly define a ternary discriminative function, which is trained to distinguish real data pairs and two kinds of fake data pairs. The generators and discriminators are trained together using adversarial learning. Under mild assumptions, in theory the joint distributions characterized by the two generators concentrate to the data distribution. In experiments, three different kinds of domain pairs are considered, image-label, image-image and image-attribute pairs. Experiments on semi-supervised image classification, image-to-image translation and attribute-based image generation demonstrate the superiority of the proposed approach.",0
"To match joint distributions across different domains with only a few paired samples, a Triangle Generative Adversarial Network ($\Delta$-GAN) was developed. $\Delta$-GAN utilizes two generators and two discriminators to learn the conditional distributions between the domains and distinguish real data pairs from two types of fake data pairs. Adversarial learning is used to train the generators and discriminators together. The proposed approach was tested on three types of domain pairs, including image-label, image-image, and image-attribute pairs. Experimental results showed that the $\Delta$-GAN approach outperformed existing methods in semi-supervised image classification, image-to-image translation, and attribute-based image generation. In theory, the joint distributions learned by the generators converge to the data distribution.",1
"In this paper, we explore automated typeface generation through image style transfer which has shown great promise in natural image generation. Existing style transfer methods for natural images generally assume that the source and target images share similar high-frequency features. However, this assumption is no longer true in typeface transformation. Inspired by the recent advancement in Generative Adversarial Networks (GANs), we propose a Hierarchical Adversarial Network (HAN) for typeface transformation. The proposed HAN consists of two sub-networks: a transfer network and a hierarchical adversarial discriminator. The transfer network maps characters from one typeface to another. A unique characteristic of typefaces is that the same radicals may have quite different appearances in different characters even under the same typeface. Hence, a stage-decoder is employed by the transfer network to leverage multiple feature layers, aiming to capture both the global and local features. The hierarchical adversarial discriminator implicitly measures data discrepancy between the generated domain and the target domain. To leverage the complementary discriminating capability of different feature layers, a hierarchical structure is proposed for the discriminator. We have experimentally demonstrated that HAN is an effective framework for typeface transfer and characters restoration.",0
"This article delves into the realm of automated typeface creation using image style transfer, a technique that has proven successful in generating natural images. Current style transfer methods for natural images assume that high-frequency features are similar between the source and target images. However, this is not the case when it comes to typeface transformation. Therefore, the authors propose a Hierarchical Adversarial Network (HAN) that comprises a transfer network and a hierarchical adversarial discriminator. The transfer network maps characters from one typeface to another, and a stage-decoder is used to capture both global and local features. The hierarchical adversarial discriminator measures the differences between the generated and target domains. The authors have demonstrated through experiments that HAN is an effective framework for typeface transfer and character restoration.",1
"Generative Adversarial Networks (GAN) have attracted much research attention recently, leading to impressive results for natural image generation. However, to date little success was observed in using GAN generated images for improving classification tasks. Here we attempt to explore, in the context of car license plate recognition, whether it is possible to generate synthetic training data using GAN to improve recognition accuracy. With a carefully-designed pipeline, we show that the answer is affirmative. First, a large-scale image set is generated using the generator of GAN, without manual annotation. Then, these images are fed to a deep convolutional neural network (DCNN) followed by a bidirectional recurrent neural network (BRNN) with long short-term memory (LSTM), which performs the feature learning and sequence labelling. Finally, the pre-trained model is fine-tuned on real images. Our experimental results on a few data sets demonstrate the effectiveness of using GAN images: an improvement of 7.5% over a strong baseline with moderate-sized real data being available. We show that the proposed framework achieves competitive recognition accuracy on challenging test datasets. We also leverage the depthwise separate convolution to construct a lightweight convolutional RNN, which is about half size and 2x faster on CPU. Combining this framework and the proposed pipeline, we make progress in performing accurate recognition on mobile and embedded devices.",0
"Recently, Generative Adversarial Networks (GAN) have garnered significant research attention due to their remarkable performance in generating natural images. However, there has been little success in using GAN-generated images for improving classification tasks. In this study, we explore whether GAN can be used to generate synthetic training data to enhance recognition accuracy in the context of car license plate recognition. We demonstrate that it is possible to generate a large-scale image set using GAN without manual annotation, and then feed these images to a deep convolutional neural network (DCNN) and a bidirectional recurrent neural network (BRNN) with long short-term memory (LSTM) to perform feature learning and sequence labelling. Finally, the pre-trained model is fine-tuned on real images. Our experiments on several datasets show that using GAN images can improve recognition accuracy by 7.5% over a strong baseline, even with moderate-sized real data available. Our proposed framework achieves competitive recognition accuracy on challenging test datasets, and we also use depthwise separate convolution to construct a lightweight convolutional RNN that is about half the size and twice as fast on CPU. By combining this framework and pipeline, we make progress in accurately recognizing license plates on mobile and embedded devices.",1
"Deep generative models based on Generative Adversarial Networks (GANs) have demonstrated impressive sample quality but in order to work they require a careful choice of architecture, parameter initialization, and selection of hyper-parameters. This fragility is in part due to a dimensional mismatch or non-overlapping support between the model distribution and the data distribution, causing their density ratio and the associated f-divergence to be undefined. We overcome this fundamental limitation and propose a new regularization approach with low computational cost that yields a stable GAN training procedure. We demonstrate the effectiveness of this regularizer across several architectures trained on common benchmark image generation tasks. Our regularization turns GAN models into reliable building blocks for deep learning.",0
"Generative Adversarial Networks (GANs) have produced remarkable results in generating high-quality samples. However, they require a cautious selection of architecture, parameter initialization, and hyper-parameters to function properly. This fragility is partly attributed to the inconsistency between the model distribution and data distribution, which renders their density ratio and f-divergence undefined. To overcome this fundamental issue, we propose a novel regularization method with minimal computational cost that ensures a stable GAN training process. We demonstrate the efficacy of this regularization approach on various architectures trained on standard image generation tasks. Our regularization transforms GAN models into dependable building blocks for deep learning.",1
"Very High Spatial Resolution (VHSR) large-scale SAR image databases are still an unresolved issue in the Remote Sensing field. In this work, we propose such a dataset and use it to explore patch-based classification in urban and periurban areas, considering 7 distinct semantic classes. In this context, we investigate the accuracy of large CNN classification models and pre-trained networks for SAR imaging systems. Furthermore, we propose a Generative Adversarial Network (GAN) for SAR image generation and test, whether the synthetic data can actually improve classification accuracy.",0
"The creation of Very High Spatial Resolution (VHSR) large-scale SAR image databases remains a challenge in the Remote Sensing domain. Our study presents a dataset of this kind and applies it to examine patch-based classification in urban and periurban regions, encompassing 7 semantic categories. We evaluate the effectiveness of extensive CNN classification models and pre-trained networks for SAR imaging systems. Additionally, we propose a Generative Adversarial Network (GAN) to generate SAR images and determine whether synthetic data can enhance classification accuracy.",1
"The recent work of Gatys et al., who characterized the style of an image by the statistics of convolutional neural network filters, ignited a renewed interest in the texture generation and image stylization problems. While their image generation technique uses a slow optimization process, recently several authors have proposed to learn generator neural networks that can produce similar outputs in one quick forward pass. While generator networks are promising, they are still inferior in visual quality and diversity compared to generation-by-optimization. In this work, we advance them in two significant ways. First, we introduce an instance normalization module to replace batch normalization with significant improvements to the quality of image stylization. Second, we improve diversity by introducing a new learning formulation that encourages generators to sample unbiasedly from the Julesz texture ensemble, which is the equivalence class of all images characterized by certain filter responses. Together, these two improvements take feed forward texture synthesis and image stylization much closer to the quality of generation-via-optimization, while retaining the speed advantage.",0
"Gatys et al.'s recent research sparked renewed interest in texture generation and image stylization by using convolutional neural network filters to characterize an image's style. Although their method involves a slow optimization process, several authors have proposed generating neural networks that can produce similar results in a single forward pass. However, these generator networks are still visually inferior and less diverse than generation-by-optimization. Our work improves generator networks in two significant ways. First, we introduce an instance normalization module that replaces batch normalization, resulting in a significant improvement in image stylization quality. Second, we enhance diversity by introducing a novel learning formulation that encourages generators to sample from the Julesz texture ensemble in an unbiased manner. This approach brings feed-forward texture synthesis and image stylization closer to the quality of generation-by-optimization while retaining the speed advantage.",1
"It this paper we revisit the fast stylization method introduced in Ulyanov et. al. (2016). We show how a small change in the stylization architecture results in a significant qualitative improvement in the generated images. The change is limited to swapping batch normalization with instance normalization, and to apply the latter both at training and testing times. The resulting method can be used to train high-performance architectures for real-time image generation. The code will is made available on github at https://github.com/DmitryUlyanov/texture_nets. Full paper can be found at arXiv:1701.02096.",0
This paper revisits the rapid stylization technique that was presented in Ulyanov et. al. (2016). Our study demonstrates that a minor modification in the stylization design can lead to a significant enhancement in the quality of the images generated. The alteration involves replacing batch normalization with instance normalization and using the latter during both training and testing phases. This improved approach can be utilized to develop high-performance architectures for real-time image generation. The code is publicly available on github at https://github.com/DmitryUlyanov/texture_nets. The complete paper can be accessed at arXiv:1701.02096.,1
"Generative Adversarial Nets (GANs) have shown promise in image generation and semi-supervised learning (SSL). However, existing GANs in SSL have two problems: (1) the generator and the discriminator (i.e. the classifier) may not be optimal at the same time; and (2) the generator cannot control the semantics of the generated samples. The problems essentially arise from the two-player formulation, where a single discriminator shares incompatible roles of identifying fake samples and predicting labels and it only estimates the data without considering the labels. To address the problems, we present triple generative adversarial net (Triple-GAN), which consists of three players---a generator, a discriminator and a classifier. The generator and the classifier characterize the conditional distributions between images and labels, and the discriminator solely focuses on identifying fake image-label pairs. We design compatible utilities to ensure that the distributions characterized by the classifier and the generator both converge to the data distribution. Our results on various datasets demonstrate that Triple-GAN as a unified model can simultaneously (1) achieve the state-of-the-art classification results among deep generative models, and (2) disentangle the classes and styles of the input and transfer smoothly in the data space via interpolation in the latent space class-conditionally.",0
"GANs have shown potential in image generation and semi-supervised learning. However, current GANs in SSL have two issues. Firstly, the generator and discriminator may not be optimal simultaneously. Secondly, the generator cannot control the meaning of the generated samples. These problems arise from the two-player formulation, where a single discriminator has incompatible roles of identifying fake samples and predicting labels. To solve these problems, we introduce Triple-GAN, consisting of three players - a generator, a discriminator, and a classifier. The generator and classifier characterize conditional distributions between images and labels, while the discriminator only focuses on identifying fake image-label pairs. We design compatible utilities to ensure that the distributions characterized by the classifier and the generator both converge to the data distribution. Our results show that Triple-GAN can achieve state-of-the-art classification results among deep generative models and disentangle the classes and styles of input while transferring smoothly in the data space via interpolation in the latent space class-conditionally.",1
"Medical image analysis and computer-assisted intervention problems are increasingly being addressed with deep-learning-based solutions. Established deep-learning platforms are flexible but do not provide specific functionality for medical image analysis and adapting them for this application requires substantial implementation effort. Thus, there has been substantial duplication of effort and incompatible infrastructure developed across many research groups. This work presents the open-source NiftyNet platform for deep learning in medical imaging. The ambition of NiftyNet is to accelerate and simplify the development of these solutions, and to provide a common mechanism for disseminating research outputs for the community to use, adapt and build upon.   NiftyNet provides a modular deep-learning pipeline for a range of medical imaging applications including segmentation, regression, image generation and representation learning applications. Components of the NiftyNet pipeline including data loading, data augmentation, network architectures, loss functions and evaluation metrics are tailored to, and take advantage of, the idiosyncracies of medical image analysis and computer-assisted intervention. NiftyNet is built on TensorFlow and supports TensorBoard visualization of 2D and 3D images and computational graphs by default.   We present 3 illustrative medical image analysis applications built using NiftyNet: (1) segmentation of multiple abdominal organs from computed tomography; (2) image regression to predict computed tomography attenuation maps from brain magnetic resonance images; and (3) generation of simulated ultrasound images for specified anatomical poses.   NiftyNet enables researchers to rapidly develop and distribute deep learning solutions for segmentation, regression, image generation and representation learning applications, or extend the platform to new applications.",0
"Deep learning is increasingly utilized to address medical image analysis and computer-assisted intervention problems. However, established platforms lack specific functionality for medical imaging, leading to substantial duplication of effort and incompatible infrastructure. To address this, NiftyNet, an open-source platform for deep learning in medical imaging, has been developed to simplify and accelerate the development of solutions and provide a common mechanism for disseminating research outputs. NiftyNet offers a modular deep-learning pipeline designed for a range of medical imaging applications, with tailored components that take advantage of the idiosyncrasies of medical image analysis. Built on TensorFlow, NiftyNet supports TensorBoard visualization of 2D and 3D images and computational graphs by default. To demonstrate its capabilities, we present three medical image analysis applications developed using NiftyNet: segmentation of abdominal organs from computed tomography, image regression to predict computed tomography attenuation maps from brain magnetic resonance images, and generation of simulated ultrasound images. NiftyNet enables researchers to easily develop and distribute deep learning solutions for various medical imaging applications or extend the platform to new ones.",1
"We present variational generative adversarial networks, a general learning framework that combines a variational auto-encoder with a generative adversarial network, for synthesizing images in fine-grained categories, such as faces of a specific person or objects in a category. Our approach models an image as a composition of label and latent attributes in a probabilistic model. By varying the fine-grained category label fed into the resulting generative model, we can generate images in a specific category with randomly drawn values on a latent attribute vector. Our approach has two novel aspects. First, we adopt a cross entropy loss for the discriminative and classifier network, but a mean discrepancy objective for the generative network. This kind of asymmetric loss function makes the GAN training more stable. Second, we adopt an encoder network to learn the relationship between the latent space and the real image space, and use pairwise feature matching to keep the structure of generated images. We experiment with natural images of faces, flowers, and birds, and demonstrate that the proposed models are capable of generating realistic and diverse samples with fine-grained category labels. We further show that our models can be applied to other tasks, such as image inpainting, super-resolution, and data augmentation for training better face recognition models.",0
"Our study introduces variational generative adversarial networks, a versatile learning framework that merges a variational auto-encoder with a generative adversarial network to create images in fine-grained categories such as a person's face or objects in a category. Our method represents an image as a blend of label and latent attributes in a probabilistic model. By changing the fine-grained category label utilized in the resulting generative model, we can create images in a specific category with randomly selected values on a latent attribute vector. Our technique has two unique features. First, we employ an asymmetric loss function that utilizes a cross-entropy loss for the discriminative and classifier network but a mean discrepancy objective for the generative network to stabilize GAN training. Second, we utilize an encoder network to learn the connection between the latent space and the actual image space and use pairwise feature matching to maintain the structure of generated images. We conducted experiments on natural images of faces, flowers, and birds and demonstrated that our models could create realistic and varied samples with fine-grained category labels. Furthermore, we demonstrated that our models could be used for other tasks such as image inpainting, super-resolution, and data augmentation to improve facial recognition models' training.",1
"GANs provide a framework for training generative models which mimic a data distribution. However, in many cases we wish to train these generative models to optimize some auxiliary objective function within the data it generates, such as making more aesthetically pleasing images. In some cases, these objective functions are difficult to evaluate, e.g. they may require human interaction. Here, we develop a system for efficiently improving a GAN to target an objective involving human interaction, specifically generating images that increase rates of positive user interactions. To improve the generative model, we build a model of human behavior in the targeted domain from a relatively small set of interactions, and then use this behavioral model as an auxiliary loss function to improve the generative model. We show that this system is successful at improving positive interaction rates, at least on simulated data, and characterize some of the factors that affect its performance.",0
"While GANs offer a way to train generative models that replicate a given data distribution, there are instances where we want to optimize the generated data to achieve certain objectives such as producing visually appealing images. However, evaluating such objectives can be challenging, especially if they require human input. To address this issue, we have developed a system that can efficiently enhance a GAN to focus on an objective that involves human interaction, specifically generating images that increase positive user engagement. This involves building a behavioral model of the targeted domain based on a limited number of interactions and using it as an auxiliary loss function to improve the performance of the generative model. We have demonstrated that this system is effective in enhancing positive interaction rates, albeit on simulated data. We have also identified some of the factors that impact its effectiveness.",1
"In this paper, we propose a novel application of Generative Adversarial Networks (GAN) to the synthesis of cells imaged by fluorescence microscopy. Compared to natural images, cells tend to have a simpler and more geometric global structure that facilitates image generation. However, the correlation between the spatial pattern of different fluorescent proteins reflects important biological functions, and synthesized images have to capture these relationships to be relevant for biological applications. We adapt GANs to the task at hand and propose new models with casual dependencies between image channels that can generate multi-channel images, which would be impossible to obtain experimentally. We evaluate our approach using two independent techniques and compare it against sensible baselines. Finally, we demonstrate that by interpolating across the latent space we can mimic the known changes in protein localization that occur through time during the cell cycle, allowing us to predict temporal evolution from static images.",0
"The aim of our paper is to suggest a fresh use of Generative Adversarial Networks (GAN) for creating synthetic cells that have been imaged through fluorescence microscopy. Cells are easier to generate than natural images because they have a simpler and more geometric global structure. However, the spatial pattern of different fluorescent proteins is important for biological functions, and synthetic images must capture these relationships to be useful for biological purposes. To overcome this, we have modified GANs to generate multi-channel images with casual dependencies between image channels, which cannot be produced experimentally. We have evaluated our approach using two independent techniques and compared it against logical baselines. Lastly, we have demonstrated that by interpolating across the latent space, we can replicate the known changes in protein localization that occur through time during the cell cycle, allowing us to forecast temporal evolution from static images.",1
"This paper introduces a novel approach to texture synthesis based on generative adversarial networks (GAN) (Goodfellow et al., 2014). We extend the structure of the input noise distribution by constructing tensors with different types of dimensions. We call this technique Periodic Spatial GAN (PSGAN). The PSGAN has several novel abilities which surpass the current state of the art in texture synthesis. First, we can learn multiple textures from datasets of one or more complex large images. Second, we show that the image generation with PSGANs has properties of a texture manifold: we can smoothly interpolate between samples in the structured noise space and generate novel samples, which lie perceptually between the textures of the original dataset. In addition, we can also accurately learn periodical textures. We make multiple experiments which show that PSGANs can flexibly handle diverse texture and image data sources. Our method is highly scalable and it can generate output images of arbitrary large size.",0
"A new method for texture synthesis using generative adversarial networks (GAN) is presented in this paper. The approach, called Periodic Spatial GAN (PSGAN), involves constructing tensors with various dimensions to expand the input noise distribution structure. PSGAN has several advantages over existing texture synthesis techniques. Firstly, it can learn multiple textures from complex, large image datasets. Secondly, image generation using PSGANs exhibits properties of a texture manifold, enabling smooth interpolation between samples in the structured noise space and generation of novel samples perceptually located between the original dataset's textures. Additionally, PSGANs can accurately learn periodic textures. The paper includes multiple experiments demonstrating the flexible handling of diverse texture and image data sources by PSGANs, which can generate output images of any size.",1
"In recent years, there has been an increasing interest in image-based plant phenotyping, applying state-of-the-art machine learning approaches to tackle challenging problems, such as leaf segmentation (a multi-instance problem) and counting. Most of these algorithms need labelled data to learn a model for the task at hand. Despite the recent release of a few plant phenotyping datasets, large annotated plant image datasets for the purpose of training deep learning algorithms are lacking. One common approach to alleviate the lack of training data is dataset augmentation. Herein, we propose an alternative solution to dataset augmentation for plant phenotyping, creating artificial images of plants using generative neural networks. We propose the Arabidopsis Rosette Image Generator (through) Adversarial Network: a deep convolutional network that is able to generate synthetic rosette-shaped plants, inspired by DCGAN (a recent adversarial network model using convolutional layers). Specifically, we trained the network using A1, A2, and A4 of the CVPPP 2017 LCC dataset, containing Arabidopsis Thaliana plants. We show that our model is able to generate realistic 128x128 colour images of plants. We train our network conditioning on leaf count, such that it is possible to generate plants with a given number of leaves suitable, among others, for training regression based models. We propose a new Ax dataset of artificial plants images, obtained by our ARIGAN. We evaluate this new dataset using a state-of-the-art leaf counting algorithm, showing that the testing error is reduced when Ax is used as part of the training data.",0
"Recently, there has been a growing interest in using advanced machine learning techniques to address complex issues in image-based plant phenotyping, such as leaf segmentation and counting. However, most of these algorithms require annotated data to learn a model for the task at hand. While some plant phenotyping datasets have been made available, there is a shortage of large, annotated image datasets for training deep learning algorithms. To address this issue, dataset augmentation is commonly used. In this study, we propose an alternative approach to dataset augmentation for plant phenotyping by generating artificial images of plants using generative neural networks. Specifically, we introduce the Arabidopsis Rosette Image Generator through Adversarial Network, which is a deep convolutional network that can produce synthetic rosette-shaped plants inspired by DCGAN. We trained the network using the CVPPP 2017 LCC dataset, containing Arabidopsis Thaliana plants. Our model can generate realistic 128x128 colour images of plants, and we conditioned the network on leaf count to enable the generation of plants with a specific number of leaves, which is useful for training regression-based models. We created a new dataset of artificial plant images, called Ax, using our ARIGAN. We evaluated this dataset using a state-of-the-art leaf counting algorithm and found that using Ax as part of the training data reduced the testing error.",1
"Recent approaches in generative adversarial networks (GANs) can automatically synthesize realistic images from descriptive text. Despite the overall fair quality, the generated images often expose visible flaws that lack structural definition for an object of interest. In this paper, we aim to extend state of the art for GAN-based text-to-image synthesis by improving perceptual quality of generated images. Differentiated from previous work, our synthetic image generator optimizes on perceptual loss functions that measure pixel, feature activation, and texture differences against a natural image. We present visually more compelling synthetic images of birds and flowers generated from text descriptions in comparison to some of the most prominent existing work.",0
"Generative adversarial networks (GANs) have been used to produce realistic images based on written descriptions. However, the quality of the generated images is not always satisfactory, as they may contain visible flaws and lack structural definition. To address this issue, we have developed a new approach that aims to enhance the perceptual quality of GAN-generated images. Unlike previous methods, our method focuses on optimizing perceptual loss functions that measure differences in pixels, feature activation, and texture compared to natural images. As a result, we have achieved more visually appealing images of birds and flowers that surpass the quality of existing methods.",1
"In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods.",0
"The aim of this paper is to introduce a new model, called Temporal Generative Adversarial Nets (TGAN), which can acquire a semantic understanding of unlabelled videos and generate new ones. Unlike current Generative Adversarial Nets (GAN) that use a single generator comprising 3D deconvolutional layers, our model employs two different types of generators: a temporal generator and an image generator. The temporal generator accepts a single latent variable as input and produces a set of latent variables, each representing an image frame in a video. The image generator then transforms these latent variables into a video. To address instability during training of GAN with such advanced networks, we utilize a recently introduced model, Wasserstein GAN, and suggest a fresh approach to training it stably in an end-to-end manner. Our experimental findings illustrate the efficacy of our techniques.",1
"We present a framework to systematically analyze convolutional neural networks (CNNs) used in classification of cars in autonomous vehicles. Our analysis procedure comprises an image generator that produces synthetic pictures by sampling in a lower dimension image modification subspace and a suite of visualization tools. The image generator produces images which can be used to test the CNN and hence expose its vulnerabilities. The presented framework can be used to extract insights of the CNN classifier, compare across classification models, or generate training and validation datasets.",0
"A framework is introduced for the systematic analysis of convolutional neural networks (CNNs) employed for classifying cars in autonomous vehicles. The analysis process entails an image generator that generates synthetic images by sampling from a lower-dimensional image modification subspace, as well as a set of visualization tools. The synthetic images created by the image generator can be utilized to test the CNN's accuracy and uncover its weaknesses. This framework can be used to gain a better understanding of the CNN classifier, compare it to other classification models, and create training and validation datasets.",1
"We present LR-GAN: an adversarial image generation model which takes scene structure and context into account. Unlike previous generative adversarial networks (GANs), the proposed GAN learns to generate image background and foregrounds separately and recursively, and stitch the foregrounds on the background in a contextually relevant manner to produce a complete natural image. For each foreground, the model learns to generate its appearance, shape and pose. The whole model is unsupervised, and is trained in an end-to-end manner with gradient descent methods. The experiments demonstrate that LR-GAN can generate more natural images with objects that are more human recognizable than DCGAN.",0
"LR-GAN is a novel adversarial image generation model that considers both scene structure and context. Unlike previous GANs, LR-GAN generates image backgrounds and foregrounds separately and recursively, and then contextualizes the foregrounds onto the background to create a full, natural image. This approach enables the model to learn to generate the appearance, shape, and pose of each foreground. The entire model is unsupervised and trained end-to-end using gradient descent methods. Our experiments show that LR-GAN produces more natural images with objects that are easier for humans to recognize than DCGAN.",1
"Semantic image inpainting is a challenging task where large missing regions have to be filled based on the available visual data. Existing methods which extract information from only a single image generally produce unsatisfactory results due to the lack of high level context. In this paper, we propose a novel method for semantic image inpainting, which generates the missing content by conditioning on the available data. Given a trained generative model, we search for the closest encoding of the corrupted image in the latent image manifold using our context and prior losses. This encoding is then passed through the generative model to infer the missing content. In our method, inference is possible irrespective of how the missing content is structured, while the state-of-the-art learning based method requires specific information about the holes in the training phase. Experiments on three datasets show that our method successfully predicts information in large missing regions and achieves pixel-level photorealism, significantly outperforming the state-of-the-art methods.",0
"The task of semantic image inpainting is difficult as it involves filling in large missing areas with reference to available visual information. Current approaches that rely on a single image for extracting information tend to produce subpar results as they lack contextual understanding. This study proposes a new method for semantic image inpainting that conditions the generation of missing content on available data. A trained generative model is used to find the nearest encoding of the damaged image in the latent image manifold, using context and prior losses. The encoding is then passed through the generative model to fill in the missing content. Unlike the leading learning-based method that needs specific training on the holes, our method can perform inference regardless of the structure of the missing content. Experiments on three datasets demonstrate that our method can effectively predict information in large missing regions and achieve a pixel-level photorealism, surpassing the current state-of-the-art methods.",1
"The inability to interpret the model prediction in semantically and visually meaningful ways is a well-known shortcoming of most existing computer-aided diagnosis methods. In this paper, we propose MDNet to establish a direct multimodal mapping between medical images and diagnostic reports that can read images, generate diagnostic reports, retrieve images by symptom descriptions, and visualize attention, to provide justifications of the network diagnosis process. MDNet includes an image model and a language model. The image model is proposed to enhance multi-scale feature ensembles and utilization efficiency. The language model, integrated with our improved attention mechanism, aims to read and explore discriminative image feature descriptions from reports to learn a direct mapping from sentence words to image pixels. The overall network is trained end-to-end by using our developed optimization strategy. Based on a pathology bladder cancer images and its diagnostic reports (BCIDR) dataset, we conduct sufficient experiments to demonstrate that MDNet outperforms comparative baselines. The proposed image model obtains state-of-the-art performance on two CIFAR datasets as well.",0
"Many computer-aided diagnosis methods have a well-known deficiency in providing semantically and visually meaningful interpretations of model predictions. In this study, we present MDNet, which establishes a direct multimodal mapping between medical images and diagnostic reports. MDNet is capable of reading images, generating diagnostic reports, retrieving images by symptom descriptions, and visualizing attention to justify the network diagnosis process. The network comprises an image model and a language model that work together to improve multi-scale feature ensembles and utilization efficiency. The language model incorporates an attention mechanism to explore discriminative image feature descriptions from reports and learn a direct mapping from sentence words to image pixels. We trained the overall network end-to-end using our optimization strategy. We conducted sufficient experiments based on a pathology bladder cancer images and its diagnostic reports (BCIDR) dataset and demonstrated that MDNet outperforms comparative baselines. Additionally, our proposed image model achieved state-of-the-art performance on two CIFAR datasets.",1
"Despite recent advances, large scale visual artifacts are still a common occurrence in images generated by GANs. Previous work has focused on improving the generator's capability to accurately imitate the data distribution $p_{data}$. In this paper, we instead explore methods that enable GANs to actively avoid errors by manipulating the input space. The core idea is to apply small changes to each noise vector in order to shift them away from areas in the input space that tend to result in errors. We derive three different architectures from that idea. The main one of these consists of a simple residual module that leads to significantly less visual artifacts, while only slightly decreasing diversity. The module is trivial to add to existing GANs and costs almost zero computation and memory.",0
"GANs still frequently produce large-scale visual defects in their generated images, despite recent advancements. Prior research has concentrated on enhancing the generator's ability to accurately mirror the data distribution $p_{data}$. This study, on the other hand, delves into techniques that allow GANs to proactively avoid errors by manipulating the input space. The fundamental concept is to implement minor modifications to every noise vector, prompting them to shift away from regions in the input space that often result in errors. We introduce three different architectures based on this idea, with the primary one being a straightforward residual module that significantly reduces visual artifacts while only slightly decreasing diversity. The module is easy to incorporate into existing GANs and requires almost no computation and memory expense.",1
"Many supervised learning tasks are emerged in dual forms, e.g., English-to-French translation vs. French-to-English translation, speech recognition vs. text to speech, and image classification vs. image generation. Two dual tasks have intrinsic connections with each other due to the probabilistic correlation between their models. This connection is, however, not effectively utilized today, since people usually train the models of two dual tasks separately and independently. In this work, we propose training the models of two dual tasks simultaneously, and explicitly exploiting the probabilistic correlation between them to regularize the training process. For ease of reference, we call the proposed approach \emph{dual supervised learning}. We demonstrate that dual supervised learning can improve the practical performances of both tasks, for various applications including machine translation, image processing, and sentiment analysis.",0
"Supervised learning tasks often come in dual forms, such as English-to-French and French-to-English translation, speech recognition and text-to-speech, and image classification and image generation. The models of two dual tasks have a probabilistic correlation and are inherently connected. However, the current practice is to train these models separately and independently, ignoring their connection. This work proposes an approach called ""dual supervised learning,"" where the models of two dual tasks are trained simultaneously, exploiting their probabilistic correlation to regulate the training process. This approach enhances the practical performance of both tasks in various applications, including sentiment analysis, image processing, and machine translation.",1
"Translating information between text and image is a fundamental problem in artificial intelligence that connects natural language processing and computer vision. In the past few years, performance in image caption generation has seen significant improvement through the adoption of recurrent neural networks (RNN). Meanwhile, text-to-image generation begun to generate plausible images using datasets of specific categories like birds and flowers. We've even seen image generation from multi-category datasets such as the Microsoft Common Objects in Context (MSCOCO) through the use of generative adversarial networks (GANs). Synthesizing objects with a complex shape, however, is still challenging. For example, animals and humans have many degrees of freedom, which means that they can take on many complex shapes. We propose a new training method called Image-Text-Image (I2T2I) which integrates text-to-image and image-to-text (image captioning) synthesis to improve the performance of text-to-image synthesis. We demonstrate that %the capability of our method to understand the sentence descriptions, so as to I2T2I can generate better multi-categories images using MSCOCO than the state-of-the-art. We also demonstrate that I2T2I can achieve transfer learning by using a pre-trained image captioning module to generate human images on the MPII Human Pose",0
"Connecting natural language processing and computer vision, translating information between text and image is a fundamental challenge in artificial intelligence. Recently, recurrent neural networks (RNN) have enhanced the performance of image caption generation. Moreover, text-to-image generation has advanced to creating plausible images of specific categories, like birds and flowers, and even multi-category datasets, such as MSCOCO, with the use of generative adversarial networks (GANs). However, generating objects with complex shapes, such as animals and humans, remains difficult due to their many degrees of freedom. To address this, we introduce a new training method, Image-Text-Image (I2T2I), which integrates image-to-text synthesis (image captioning) and text-to-image synthesis to improve performance. Our method demonstrates the capability to generate better multi-category images using MSCOCO than the state-of-the-art and achieve transfer learning by using a pre-trained image captioning module to generate human images on the MPII Human Pose.",1
"Since its appearance, Generative Adversarial Networks (GANs) have received a lot of interest in the AI community. In image generation several projects showed how GANs are able to generate photorealistic images but the results so far did not look adequate for the quality standard of visual media production industry. We present an optimized image generation process based on a Deep Convolutional Generative Adversarial Networks (DCGANs), in order to create photorealistic high-resolution images (up to 1024x1024 pixels). Furthermore, the system was fed with a limited dataset of images, less than two thousand images. All these results give more clue about future exploitation of GANs in Computer Graphics and Visual Effects.",0
"Generative Adversarial Networks (GANs) have captured significant attention in the AI community since their introduction. While several GAN projects have demonstrated the capability to generate photorealistic images, the quality has yet to meet the visual media production industry's standards. Our study presents an optimized image generation process utilizing Deep Convolutional Generative Adversarial Networks (DCGANs) to produce photorealistic high-resolution images (up to 1024x1024 pixels). Moreover, the system was trained with a limited dataset of less than two thousand images. These findings offer valuable insights into the potential use of GANs in Computer Graphics and Visual Effects in the future.",1
"We propose a new equilibrium enforcing method paired with a loss derived from the Wasserstein distance for training auto-encoder based Generative Adversarial Networks. This method balances the generator and discriminator during training. Additionally, it provides a new approximate convergence measure, fast and stable training and high visual quality. We also derive a way of controlling the trade-off between image diversity and visual quality. We focus on the image generation task, setting a new milestone in visual quality, even at higher resolutions. This is achieved while using a relatively simple model architecture and a standard training procedure.",0
"Our proposal introduces a novel approach for enforcing equilibrium and utilizes a loss function derived from the Wasserstein distance to train Generative Adversarial Networks based on auto-encoders. This technique ensures the generator and discriminator are equally balanced during the training process, while also providing a new convergence measure that is both speedy and reliable, resulting in high-quality visual output. Our research also includes a method of regulating the balance between image diversity and visual quality. Our main focus is on the image generation task, where we have achieved a new standard in visual quality, even when dealing with higher resolutions. Furthermore, this has been accomplished using a straightforward model architecture and conventional training techniques.",1
"We propose a novel technique to make neural network robust to adversarial examples using a generative adversarial network. We alternately train both classifier and generator networks. The generator network generates an adversarial perturbation that can easily fool the classifier network by using a gradient of each image. Simultaneously, the classifier network is trained to classify correctly both original and adversarial images generated by the generator. These procedures help the classifier network to become more robust to adversarial perturbations. Furthermore, our adversarial training framework efficiently reduces overfitting and outperforms other regularization methods such as Dropout. We applied our method to supervised learning for CIFAR datasets, and experimantal results show that our method significantly lowers the generalization error of the network. To the best of our knowledge, this is the first method which uses GAN to improve supervised learning.",0
"Our proposed technique employs a innovative approach to enhance the resilience of neural networks against adversarial examples by utilizing a generative adversarial network. Our approach involves the alternating training of the classifier and generator networks. The generator network generates an adversarial perturbation, which uses a gradient of each image, to mislead the classifier network. Meanwhile, the classifier network is trained to accurately classify both the original and the adversarial images generated by the generator. These procedures aid in improving the classifier network's ability to cope with adversarial perturbations. Additionally, our adversarial training framework efficiently mitigates overfitting and outperforms other regularization methods like Dropout. We applied our technique to supervised learning for CIFAR datasets, and the experimental results demonstrate that our approach significantly reduces the generalization error of the network. To the best of our knowledge, this is the first time a GAN has been used to enhance supervised learning.",1
"We propose the Margin Adaptation for Generative Adversarial Networks (MAGANs) algorithm, a novel training procedure for GANs to improve stability and performance by using an adaptive hinge loss function. We estimate the appropriate hinge loss margin with the expected energy of the target distribution, and derive principled criteria for when to update the margin. We prove that our method converges to its global optimum under certain assumptions. Evaluated on the task of unsupervised image generation, the proposed training procedure is simple yet robust on a diverse set of data, and achieves qualitative and quantitative improvements compared to the state-of-the-art.",0
"Our new training procedure for Generative Adversarial Networks (GANs), called the Margin Adaptation for Generative Adversarial Networks (MAGANs) algorithm, enhances stability and performance by employing an adaptive hinge loss function. The appropriate hinge loss margin is calculated by estimating the expected energy of the target distribution, and we establish specific standards for when to update the margin in a principled manner. We prove that our method reaches its global optimum given certain conditions. When applied to unsupervised image generation, our training approach is both straightforward and dependable across a range of data, and outperforms existing methods in both qualitative and quantitative measures.",1
"We study unsupervised generative modeling in terms of the optimal transport (OT) problem between true (but unknown) data distribution $P_X$ and the latent variable model distribution $P_G$. We show that the OT problem can be equivalently written in terms of probabilistic encoders, which are constrained to match the posterior and prior distributions over the latent space. When relaxed, this constrained optimization problem leads to a penalized optimal transport (POT) objective, which can be efficiently minimized using stochastic gradient descent by sampling from $P_X$ and $P_G$. We show that POT for the 2-Wasserstein distance coincides with the objective heuristically employed in adversarial auto-encoders (AAE) (Makhzani et al., 2016), which provides the first theoretical justification for AAEs known to the authors. We also compare POT to other popular techniques like variational auto-encoders (VAE) (Kingma and Welling, 2014). Our theoretical results include (a) a better understanding of the commonly observed blurriness of images generated by VAEs, and (b) establishing duality between Wasserstein GAN (Arjovsky and Bottou, 2017) and POT for the 1-Wasserstein distance.",0
"The focus of our research is unsupervised generative modeling and its relation to the optimal transport (OT) problem. We aim to find a solution for the OT problem between the unknown true data distribution $P_X$ and the distribution of the latent variable model, $P_G$. Our approach involves the use of probabilistic encoders to match the prior and posterior distributions of the latent space. By relaxing the optimization constraints, we obtain a penalized optimal transport (POT) objective, which can be efficiently minimized using stochastic gradient descent and sampling from $P_X$ and $P_G$. Our study reveals that the POT objective for the 2-Wasserstein distance is equivalent to the objective employed in adversarial auto-encoders (AAE). We compare our approach to other popular techniques like variational auto-encoders (VAE). Our findings shed light on the blurriness of images generated by VAEs and establish the duality between Wasserstein GAN and POT for the 1-Wasserstein distance.",1
"Variational auto-encoder (VAE) is a powerful unsupervised learning framework for image generation. One drawback of VAE is that it generates blurry images due to its Gaussianity assumption and thus L2 loss. To allow the generation of high quality images by VAE, we increase the capacity of decoder network by employing residual blocks and skip connections, which also enable efficient optimization. To overcome the limitation of L2 loss, we propose to generate images in a multi-stage manner from coarse to fine. In the simplest case, the proposed multi-stage VAE divides the decoder into two components in which the second component generates refined images based on the course images generated by the first component. Since the second component is independent of the VAE model, it can employ other loss functions beyond the L2 loss and different model architectures. The proposed framework can be easily generalized to contain more than two components. Experiment results on the MNIST and CelebA datasets demonstrate that the proposed multi-stage VAE can generate sharper images as compared to those from the original VAE.",0
"The Variational auto-encoder (VAE) is a potent unsupervised learning framework used for image generation. However, it generates blurry images due to its Gaussianity assumption and L2 loss. To produce high-quality images with VAE, we increase the decoder network's capacity by incorporating residual blocks and skip connections, enabling efficient optimization. To address the limitations of the L2 loss, we propose a multi-stage approach to generate images from coarse to fine. In the simplest scenario, the multi-stage VAE divides the decoder into two components, with the second component generating refined images based on the coarse images generated by the first component. As the second component is independent of the VAE model, it can use other loss functions and model architectures beyond L2 loss. The proposed framework can be extended to include more than two components. Experimental results on the MNIST and CelebA datasets show that the multi-stage VAE generates sharper images than the original VAE.",1
"Object Transfiguration replaces an object in an image with another object from a second image. For example it can perform tasks like ""putting exactly those eyeglasses from image A on the nose of the person in image B"". Usage of exemplar images allows more precise specification of desired modifications and improves the diversity of conditional image generation. However, previous methods that rely on feature space operations, require paired data and/or appearance models for training or disentangling objects from background. In this work, we propose a model that can learn object transfiguration from two unpaired sets of images: one set containing images that ""have"" that kind of object, and the other set being the opposite, with the mild constraint that the objects be located approximately at the same place. For example, the training data can be one set of reference face images that have eyeglasses, and another set of images that have not, both of which spatially aligned by face landmarks. Despite the weak 0/1 labels, our model can learn an ""eyeglasses"" subspace that contain multiple representatives of different types of glasses. Consequently, we can perform fine-grained control of generated images, like swapping the glasses in two images by swapping the projected components in the ""eyeglasses"" subspace, to create novel images of people wearing eyeglasses.   Overall, our deterministic generative model learns disentangled attribute subspaces from weakly labeled data by adversarial training. Experiments on CelebA and Multi-PIE datasets validate the effectiveness of the proposed model on real world data, in generating images with specified eyeglasses, smiling, hair styles, and lighting conditions etc. The code is available online.",0
"The process of Object Transfiguration involves replacing an object in an image with a different object from another image. This technique allows for precise modifications to be made, such as placing a specific pair of eyeglasses from one image onto the person in another image. While previous methods required paired data or appearance models for training, our model can learn object transfiguration from two unpaired sets of images. One set contains images with the desired object, while the other set does not, but both sets are spatially aligned. Using weak 0/1 labels, our model can learn an ""eyeglasses"" subspace that contains multiple types of glasses. This allows for fine-grained control over generated images, such as swapping the glasses between two images by swapping the projected components in the ""eyeglasses"" subspace. Our deterministic generative model learns disentangled attribute subspaces from weakly labeled data through adversarial training. Our experiments on CelebA and Multi-PIE datasets show that our model can effectively generate images with specified attributes, such as eyeglasses, smiling, hair styles, and lighting conditions. Our code is available online.",1
"Recently, realistic image generation using deep neural networks has become a hot topic in machine learning and computer vision. Images can be generated at the pixel level by learning from a large collection of images. Learning to generate colorful cartoon images from black-and-white sketches is not only an interesting research problem, but also a potential application in digital entertainment. In this paper, we investigate the sketch-to-image synthesis problem by using conditional generative adversarial networks (cGAN). We propose the auto-painter model which can automatically generate compatible colors for a sketch. The new model is not only capable of painting hand-draw sketch with proper colors, but also allowing users to indicate preferred colors. Experimental results on two sketch datasets show that the auto-painter performs better that existing image-to-image methods.",0
"Deep neural networks have recently gained attention in machine learning and computer vision for their ability to generate realistic images. By learning from a large collection of images, pixel-level images can be produced. A research problem and potential application in digital entertainment lies in the generation of colorful cartoon images from black-and-white sketches. In this study, a conditional generative adversarial network (cGAN) was used to investigate the sketch-to-image synthesis problem. The proposed auto-painter model automatically generates compatible colors for a sketch and allows users to indicate preferred colors. Experimental results on two sketch datasets indicate that the auto-painter outperforms existing image-to-image methods in both painting hand-drawn sketches with proper colors and generating preferred color choices.",1
"In this paper, we introduce robust and synergetic hand-crafted features and a simple but efficient deep feature from a convolutional neural network (CNN) architecture for defocus estimation. This paper systematically analyzes the effectiveness of different features, and shows how each feature can compensate for the weaknesses of other features when they are concatenated. For a full defocus map estimation, we extract image patches on strong edges sparsely, after which we use them for deep and hand-crafted feature extraction. In order to reduce the degree of patch-scale dependency, we also propose a multi-scale patch extraction strategy. A sparse defocus map is generated using a neural network classifier followed by a probability-joint bilateral filter. The final defocus map is obtained from the sparse defocus map with guidance from an edge-preserving filtered input image. Experimental results show that our algorithm is superior to state-of-the-art algorithms in terms of defocus estimation. Our work can be used for applications such as segmentation, blur magnification, all-in-focus image generation, and 3-D estimation.",0
"The purpose of this paper is to present our approach to estimating defocus using a combination of robust and synergistic hand-crafted features and a deep feature from a convolutional neural network (CNN) architecture. We conduct a thorough analysis of the effectiveness of each feature and demonstrate how they complement each other when concatenated. To generate a complete defocus map, we extract image patches on strong edges and use them for both deep and hand-crafted feature extraction. To reduce the impact of patch-scale dependency, we propose a multi-scale patch extraction strategy. A neural network classifier generates a sparse defocus map, which is then enhanced using a probability-joint bilateral filter. The final defocus map is obtained by combining the sparse defocus map with an edge-preserving filtered input image. Our experimental results show that our approach outperforms existing algorithms in terms of defocus estimation. Our method has potential applications in segmentation, blur magnification, all-in-focus image generation, and 3-D estimation.",1
"Distinguishing subtle differences in attributes is valuable, yet learning to make visual comparisons remains non-trivial. Not only is the number of possible comparisons quadratic in the number of training images, but also access to images adequately spanning the space of fine-grained visual differences is limited. We propose to overcome the sparsity of supervision problem via synthetically generated images. Building on a state-of-the-art image generation engine, we sample pairs of training images exhibiting slight modifications of individual attributes. Augmenting real training image pairs with these examples, we then train attribute ranking models to predict the relative strength of an attribute in novel pairs of real images. Our results on datasets of faces and fashion images show the great promise of bootstrapping imperfect image generators to counteract sample sparsity for learning to rank.",0
"Making subtle distinctions between attributes is valuable, but it is challenging to learn how to compare visually. The number of potential comparisons increases quadratically with the number of training images, and there are limited opportunities to access images that adequately represent the range of fine-grained visual differences. To address the problem of sparse supervision, we propose using synthetically generated images. By utilizing a cutting-edge image generation engine, we can produce pairs of training images that demonstrate slight adjustments to individual attributes. We supplement actual training image pairs with these samples and then utilize attribute ranking models to forecast the relative strength of an attribute in novel pairs of real images. Our findings on datasets featuring faces and fashion images demonstrate the significant potential of utilizing imperfect image generators to overcome sample scarcity and learn to rank.",1
"In the past few years, Generative Adversarial Network (GAN) became a prevalent research topic. By defining two convolutional neural networks (G-Network and D-Network) and introducing an adversarial procedure between them during the training process, GAN has ability to generate good quality images that look like natural images from a random vector. Besides image generation, GAN may have potential to deal with wide range of real world problems. In this paper, we follow the basic idea of GAN and propose a novel model for image saliency detection, which is called Supervised Adversarial Networks (SAN). Specifically, SAN also trains two models simultaneously: the G-Network takes natural images as inputs and generates corresponding saliency maps (synthetic saliency maps), and the D-Network is trained to determine whether one sample is a synthetic saliency map or ground-truth saliency map. However, different from GAN, the proposed method uses fully supervised learning to learn both G-Network and D-Network by applying class labels of the training set. Moreover, a novel kind of layer call conv-comparison layer is introduced into the D-Network to further improve the saliency performance by forcing the high-level feature of synthetic saliency maps and ground-truthes as similar as possible. Experimental results on Pascal VOC 2012 database show that the SAN model can generate high quality saliency maps for many complicate natural images.",0
"Over the past few years, Generative Adversarial Network (GAN) has emerged as a popular research area. GAN involves training two convolutional neural networks, the G-Network and D-Network, to generate high-quality images that resemble natural images from a random vector. In addition to image generation, GAN has the potential to tackle a wide range of real-world problems. This research paper proposes a new model for image saliency detection, called Supervised Adversarial Networks (SAN), based on the basic principles of GAN. Unlike GAN, SAN uses fully supervised learning to train both G-Network and D-Network by using class labels of the training set. SAN trains two models simultaneously, where the G-Network generates synthetic saliency maps from natural images, and the D-Network distinguishes between synthetic and ground-truth saliency maps. Furthermore, a novel conv-comparison layer is introduced into the D-Network to enhance the saliency performance by making the high-level feature of synthetic saliency maps and ground-truthes as similar as possible. The SAN model is tested on Pascal VOC 2012 database and demonstrates the capability to generate high-quality saliency maps for complex natural images.",1
"Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson $\chi^2$ divergence. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stable during the learning process. We evaluate LSGANs on five scene datasets and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs. We also conduct two comparison experiments between LSGANs and regular GANs to illustrate the stability of LSGANs.",0
"Generative adversarial networks (GANs) have achieved great success in unsupervised learning. However, the standard GAN model employs the sigmoid cross entropy loss function for the discriminator, which may result in vanishing gradients. To address this challenge, we propose Least Squares Generative Adversarial Networks (LSGANs) that use the least squares loss function for the discriminator. Our objective function for LSGANs minimizes the Pearson $\chi^2$ divergence and offers two advantages over standard GANs. Firstly, LSGANs generate higher quality images, and secondly, they are more stable during the learning process. We test LSGANs on five scene datasets and observe that they generate better quality images than standard GANs. Moreover, we compare LSGANs with standard GANs in two experiments, and the results demonstrate the stability of LSGANs.",1
"We introduce a new problem of generating an image based on a small number of key local patches without any geometric prior. In this work, key local patches are defined as informative regions of the target object or scene. This is a challenging problem since it requires generating realistic images and predicting locations of parts at the same time. We construct adversarial networks to tackle this problem. A generator network generates a fake image as well as a mask based on the encoder-decoder framework. On the other hand, a discriminator network aims to detect fake images. The network is trained with three losses to consider spatial, appearance, and adversarial information. The spatial loss determines whether the locations of predicted parts are correct. Input patches are restored in the output image without much modification due to the appearance loss. The adversarial loss ensures output images are realistic. The proposed network is trained without supervisory signals since no labels of key parts are required. Experimental results on six datasets demonstrate that the proposed algorithm performs favorably on challenging objects and scenes.",0
"A novel problem is presented in this study, involving the generation of an image using only a small set of key local patches without relying on any geometric prior. These patches are considered informative regions of the target object or scene, making the task particularly difficult as it requires generating realistic images and predicting part locations simultaneously. To address this challenge, adversarial networks are employed. The generator network uses the encoder-decoder framework to produce a fake image and a mask, while the discriminator network detects fake images. The network is trained using three losses to consider spatial, appearance, and adversarial information. The spatial loss assesses whether the predicted part locations are correct, while the appearance loss ensures that input patches are minimally modified in the output image. Finally, the adversarial loss guarantees that the generated images are realistic. The proposed network is trained without the need for supervisory signals, as labels of key parts are not required. Experimental results demonstrate that the proposed algorithm performs well on challenging objects and scenes across six datasets.",1
"In this work, we present the Text Conditioned Auxiliary Classifier Generative Adversarial Network, (TAC-GAN) a text to image Generative Adversarial Network (GAN) for synthesizing images from their text descriptions. Former approaches have tried to condition the generative process on the textual data; but allying it to the usage of class information, known to diversify the generated samples and improve their structural coherence, has not been explored. We trained the presented TAC-GAN model on the Oxford-102 dataset of flowers, and evaluated the discriminability of the generated images with Inception-Score, as well as their diversity using the Multi-Scale Structural Similarity Index (MS-SSIM). Our approach outperforms the state-of-the-art models, i.e., its inception score is 3.45, corresponding to a relative increase of 7.8% compared to the recently introduced StackGan. A comparison of the mean MS-SSIM scores of the training and generated samples per class shows that our approach is able to generate highly diverse images with an average MS-SSIM of 0.14 over all generated classes.",0
"This study introduces the Text Conditioned Auxiliary Classifier Generative Adversarial Network (TAC-GAN), which is a type of Generative Adversarial Network (GAN) that can produce images based on textual descriptions. While previous methods have attempted to use text data to guide the generative process, they have not incorporated class information which can increase diversity and improve structural coherence in the generated images. The TAC-GAN model was trained using the Oxford-102 dataset of flowers, and its performance was evaluated using the Inception-Score and Multi-Scale Structural Similarity Index (MS-SSIM). Our results show that TAC-GAN outperforms previous state-of-the-art models, with an inception score of 3.45, which is a 7.8% increase compared to StackGan. Additionally, the average MS-SSIM of TAC-GAN generated images across all classes was 0.14, indicating high image diversity.",1
"3D shape models are naturally parameterized using vertices and faces, \ie, composed of polygons forming a surface. However, current 3D learning paradigms for predictive and generative tasks using convolutional neural networks focus on a voxelized representation of the object. Lifting convolution operators from the traditional 2D to 3D results in high computational overhead with little additional benefit as most of the geometry information is contained on the surface boundary. Here we study the problem of directly generating the 3D shape surface of rigid and non-rigid shapes using deep convolutional neural networks. We develop a procedure to create consistent `geometry images' representing the shape surface of a category of 3D objects. We then use this consistent representation for category-specific shape surface generation from a parametric representation or an image by developing novel extensions of deep residual networks for the task of geometry image generation. Our experiments indicate that our network learns a meaningful representation of shape surfaces allowing it to interpolate between shape orientations and poses, invent new shape surfaces and reconstruct 3D shape surfaces from previously unseen images.",0
"The natural way to parameterize 3D shape models is by using vertices and faces, which consist of polygons that create a surface. However, the current 3D learning methods for predictive and generative tasks with convolutional neural networks focus on a voxelized object representation. This approach results in high computational overhead and little additional benefit, as most of the geometry information resides on the surface boundary. This study examines the problem of generating the surface of 3D shapes using deep convolutional neural networks. We propose a method for creating consistent ""geometry images"" that represent the shape surface of a category of 3D objects. Using this representation, we develop novel extensions of deep residual networks for the task of generating shape surfaces from a parametric representation or an image, specific to the category. Our experiments demonstrate that our network learns a meaningful representation of shape surfaces, enabling it to interpolate between shape orientations and poses, invent new shape surfaces, and reconstruct 3D shape surfaces from previously unseen images.",1
"PixelCNN achieves state-of-the-art results in density estimation for natural images. Although training is fast, inference is costly, requiring one network evaluation per pixel; O(N) for N pixels. This can be sped up by caching activations, but still involves generating each pixel sequentially. In this work, we propose a parallelized PixelCNN that allows more efficient inference by modeling certain pixel groups as conditionally independent. Our new PixelCNN model achieves competitive density estimation and orders of magnitude speedup - O(log N) sampling instead of O(N) - enabling the practical generation of 512x512 images. We evaluate the model on class-conditional image generation, text-to-image synthesis, and action-conditional video generation, showing that our model achieves the best results among non-pixel-autoregressive density models that allow efficient sampling.",0
"The PixelCNN algorithm has been successful in density estimation for natural images, setting the benchmark. However, while training is quick, performing inference can be time-consuming as it necessitates one network evaluation per pixel, resulting in an O(N) complexity for N pixels. Though activating caches can expedite the process, it still involves generating each pixel sequentially. Our research introduces a parallelized PixelCNN that models certain pixel groups as conditionally independent, allowing for more efficient inference. Our new PixelCNN model achieves competitive density estimation and orders of magnitude speedup, resulting in O(log N) sampling instead of O(N). This makes it possible to generate 512x512 images practically. We have evaluated the model's performance on class-conditional image generation, text-to-image synthesis, and action-conditional video generation, and our model has outperformed other non-pixel-autoregressive density models that allow efficient sampling.",1
"We present a transformation-grounded image generation network for novel 3D view synthesis from a single image. Instead of taking a 'blank slate' approach, we first explicitly infer the parts of the geometry visible both in the input and novel views and then re-cast the remaining synthesis problem as image completion. Specifically, we both predict a flow to move the pixels from the input to the novel view along with a novel visibility map that helps deal with occulsion/disocculsion. Next, conditioned on those intermediate results, we hallucinate (infer) parts of the object invisible in the input image. In addition to the new network structure, training with a combination of adversarial and perceptual loss results in a reduction in common artifacts of novel view synthesis such as distortions and holes, while successfully generating high frequency details and preserving visual aspects of the input image. We evaluate our approach on a wide range of synthetic and real examples. Both qualitative and quantitative results show our method achieves significantly better results compared to existing methods.",0
"Our study proposes a novel approach for generating 3D views from a single image using a transformation-grounded image generation network. Instead of starting from scratch, we begin by inferring the visible geometry from both the input and novel views, followed by image completion to address the synthesis problem. To achieve this, we predict a flow that transfers pixels from the input to the novel view and a visibility map that handles occlusion/disocclusion. Subsequently, we infer the invisible parts of the object based on these intermediate outcomes. We also employ a combination of adversarial and perceptual loss during training to reduce common artifacts such as distortions and holes, while preserving the visual characteristics of the input image and generating high-frequency details. Our method is evaluated on various synthetic and real examples, and the results demonstrate superior performance compared to existing techniques in both qualitative and quantitative assessments.",1
"Generative adversarial networks (GANs) are a framework for producing a generative model by way of a two-player minimax game. In this paper, we propose the \emph{Generative Multi-Adversarial Network} (GMAN), a framework that extends GANs to multiple discriminators. In previous work, the successful training of GANs requires modifying the minimax objective to accelerate training early on. In contrast, GMAN can be reliably trained with the original, untampered objective. We explore a number of design perspectives with the discriminator role ranging from formidable adversary to forgiving teacher. Image generation tasks comparing the proposed framework to standard GANs demonstrate GMAN produces higher quality samples in a fraction of the iterations when measured by a pairwise GAM-type metric.",0
"The article introduces Generative Multi-Adversarial Network (GMAN), which expands the concept of Generative Adversarial Networks (GANs) to include multiple discriminators. Unlike GANs, which require adjustments to the minimax objective to enhance early training, GMAN can be trained with the original objective. The discriminator's role in GMAN can vary from a formidable adversary to a forgiving teacher, and the framework produces higher quality samples compared to standard GANs in fewer iterations when measured by a pairwise GAM-type metric. Image generation tasks were used to demonstrate the effectiveness of the proposed framework.",1
"Generative adversarial networks (GANs) transform latent vectors into visually plausible images. It is generally thought that the original GAN formulation gives no out-of-the-box method to reverse the mapping, projecting images back into latent space. We introduce a simple, gradient-based technique called stochastic clipping. In experiments, for images generated by the GAN, we precisely recover their latent vector pre-images 100% of the time. Additional experiments demonstrate that this method is robust to noise. Finally, we show that even for unseen images, our method appears to recover unique encodings.",0
"Visually plausible images can be created by generative adversarial networks (GANs) through the transformation of latent vectors. However, it is commonly believed that the original GAN formulation does not offer a straightforward way to reverse this mapping and project images back into the latent space. To address this, we have introduced a gradient-based technique called stochastic clipping. Our experiments have shown that this method can accurately recover the latent vector pre-images for GAN-generated images with 100% success rate. Furthermore, our method has proven to be resilient to noise. Lastly, we have demonstrated that even for previously unseen images, our approach has the ability to recover unique encodings.",1
"The increasingly photorealistic sample quality of generative image models suggests their feasibility in applications beyond image generation. We present the Neural Photo Editor, an interface that leverages the power of generative neural networks to make large, semantically coherent changes to existing images. To tackle the challenge of achieving accurate reconstructions without loss of feature quality, we introduce the Introspective Adversarial Network, a novel hybridization of the VAE and GAN. Our model efficiently captures long-range dependencies through use of a computational block based on weight-shared dilated convolutions, and improves generalization performance with Orthogonal Regularization, a novel weight regularization method. We validate our contributions on CelebA, SVHN, and CIFAR-100, and produce samples and reconstructions with high visual fidelity.",0
"The realistic quality of generative image models has increased, indicating their potential for use in applications beyond just generating images. To this end, we have developed the Neural Photo Editor, which utilizes generative neural networks to make significant, coherent changes to pre-existing images. Our approach involves the Introspective Adversarial Network, a hybrid of VAE and GAN that addresses the challenge of maintaining accurate reconstructions while preserving feature quality. Our model employs weight-shared dilated convolutions to efficiently capture long-range dependencies and Orthogonal Regularization as a novel weight regularization method, improving generalization performance. We have tested our method on CelebA, SVHN, and CIFAR-100 datasets and have produced samples and reconstructions with high visual accuracy.",1
"Recently there has been an enormous interest in generative models for images in deep learning. In pursuit of this, Generative Adversarial Networks (GAN) and Variational Auto-Encoder (VAE) have surfaced as two most prominent and popular models. While VAEs tend to produce excellent reconstructions but blurry samples, GANs generate sharp but slightly distorted images. In this paper we propose a new model called Variational InfoGAN (ViGAN). Our aim is two fold: (i) To generated new images conditioned on visual descriptions, and (ii) modify the image, by fixing the latent representation of image and varying the visual description. We evaluate our model on Labeled Faces in the Wild (LFW), celebA and a modified version of MNIST datasets and demonstrate the ability of our model to generate new images as well as to modify a given image by changing attributes.",0
"Deep learning has sparked a significant interest in generative models for images, with Generative Adversarial Networks (GAN) and Variational Auto-Encoder (VAE) emerging as the most popular models. However, VAEs tend to produce blurry samples despite excellent reconstructions, while GANs generate sharp but slightly distorted images. A new model called Variational InfoGAN (ViGAN) is introduced in this paper, with the objective of generating new images based on visual descriptions and modifying images by altering the visual description while fixing the latent representation of the image. The performance of the model is evaluated on Labeled Faces in the Wild (LFW), celebA, and a modified version of MNIST datasets, demonstrating its ability to generate new images and modify attributes.",1
"We use CNNs to build a system that both classifies images of faces based on a variety of different facial attributes and generates new faces given a set of desired facial characteristics. After introducing the problem and providing context in the first section, we discuss recent work related to image generation in Section 2. In Section 3, we describe the methods used to fine-tune our CNN and generate new images using a novel approach inspired by a Gaussian mixture model. In Section 4, we discuss our working dataset and describe our preprocessing steps and handling of facial attributes. Finally, in Sections 5, 6 and 7, we explain our experiments and results and conclude in the following section. Our classification system has 82\% test accuracy. Furthermore, our generation pipeline successfully creates well-formed faces.",0
"To construct a system capable of both categorizing facial images based on various facial attributes and producing fresh faces with specific facial characteristics, we employ CNNs. The initial section outlines the problem and provides background, followed by a discussion of image generation-related research in Section 2. Section 3 covers our CNN fine-tuning techniques and introduces a new approach that draws inspiration from a Gaussian mixture model to generate fresh images. Our working dataset and facial attribute treatment are detailed in Section 4. Lastly, Sections 5, 6, and 7 delve into our experiments and findings, concluding in the final section. Our classification system boasts an 82% test accuracy, and our image generation pipeline produces well-formed faces.",1
"Generation of 3D data by deep neural network has been attracting increasing attention in the research community. The majority of extant works resort to regular representations such as volumetric grids or collection of images; however, these representations obscure the natural invariance of 3D shapes under geometric transformations and also suffer from a number of other issues. In this paper we address the problem of 3D reconstruction from a single image, generating a straight-forward form of output -- point cloud coordinates. Along with this problem arises a unique and interesting issue, that the groundtruth shape for an input image may be ambiguous. Driven by this unorthodox output form and the inherent ambiguity in groundtruth, we design architecture, loss function and learning paradigm that are novel and effective. Our final solution is a conditional shape sampler, capable of predicting multiple plausible 3D point clouds from an input image. In experiments not only can our system outperform state-of-the-art methods on single image based 3d reconstruction benchmarks; but it also shows a strong performance for 3d shape completion and promising ability in making multiple plausible predictions.",0
"The research community has recently shown a growing interest in using deep neural networks to generate 3D data. However, many current approaches rely on regular representations like volumetric grids or collections of images, which do not fully capture the natural invariance of 3D shapes and have several limitations. This study aims to address the challenge of reconstructing 3D shapes from a single image by producing point cloud coordinates as output. An important issue arises as the true shape of an input image may be uncertain. To overcome this, the researchers developed a unique architecture, loss function, and learning paradigm that can effectively predict multiple plausible 3D point clouds. The proposed solution, a conditional shape sampler, not only outperforms existing methods on single image-based 3D reconstruction benchmarks but also shows strong performance in 3D shape completion and the ability to generate multiple plausible predictions.",1
"Communicating and sharing intelligence among agents is an important facet of achieving Artificial General Intelligence. As a first step towards this challenge, we introduce a novel framework for image generation: Message Passing Multi-Agent Generative Adversarial Networks (MPM GANs). While GANs have recently been shown to be very effective for image generation and other tasks, these networks have been limited to mostly single generator-discriminator networks. We show that we can obtain multi-agent GANs that communicate through message passing to achieve better image generation. The objectives of the individual agents in this framework are two fold: a co-operation objective and a competing objective. The co-operation objective ensures that the message sharing mechanism guides the other generator to generate better than itself while the competing objective encourages each generator to generate better than its counterpart. We analyze and visualize the messages that these GANs share among themselves in various scenarios. We quantitatively show that the message sharing formulation serves as a regularizer for the adversarial training. Qualitatively, we show that the different generators capture different traits of the underlying data distribution.",0
"To attain Artificial General Intelligence, it is essential to have agents communicate and exchange information. Our new framework for generating images, known as Message Passing Multi-Agent Generative Adversarial Networks (MPM GANs), is a crucial initial step towards this goal. While GANs have proven to be highly effective for image creation and other tasks, they have mostly been single generator-discriminator networks. Our approach uses multi-agent GANs that communicate through message passing to improve image creation. The agents have two objectives: cooperation and competition. The cooperation objective prompts the sharing of messages between generators, guiding them to improve image quality. The competition objective motivates each generator to outperform its counterpart. We examine the messages shared by these GANs in various situations and demonstrate that the message sharing mechanism serves as a regularizer for adversarial training. Moreover, we show that the different generators capture distinct characteristics of the underlying data distribution.",1
"Automatic image synthesis research has been rapidly growing with deep networks getting more and more expressive. In the last couple of years, we have observed images of digits, indoor scenes, birds, chairs, etc. being automatically generated. The expressive power of image generators have also been enhanced by introducing several forms of conditioning variables such as object names, sentences, bounding box and key-point locations. In this work, we propose a novel deep conditional generative adversarial network architecture that takes its strength from the semantic layout and scene attributes integrated as conditioning variables. We show that our architecture is able to generate realistic outdoor scene images under different conditions, e.g. day-night, sunny-foggy, with clear object boundaries.",0
"Due to the increasing expressiveness of deep networks, research on automatic image synthesis has been expanding rapidly. Over the past few years, various types of images, including digits, indoor scenes, birds and chairs, have been generated automatically. Conditioning variables, such as object names, sentences, bounding box and key-point locations, have also been introduced to enhance the expressive power of image generators. In this study, we introduce a new deep conditional generative adversarial network architecture that utilizes semantic layout and scene attributes as conditioning variables. Our architecture is capable of producing realistic outdoor scene images under different conditions, such as day-night, sunny-foggy, and with distinct object boundaries.",1
"We present an image-conditional image generation model. The model transfers an input domain to a target domain in semantic level, and generates the target image in pixel level. To generate realistic target images, we employ the real/fake-discriminator as in Generative Adversarial Nets, but also introduce a novel domain-discriminator to make the generated image relevant to the input image. We verify our model through a challenging task of generating a piece of clothing from an input image of a dressed person. We present a high quality clothing dataset containing the two domains, and succeed in demonstrating decent results.",0
"Our model is an image-conditional image generation system that transfers input domains to a semantic level target domain and produces pixel level target images. To create realistic target images, we use a real/fake-discriminator similar to Generative Adversarial Nets. We also introduce a domain-discriminator to ensure the generated image is related to the input image. We put our model to the test by generating clothing from an input image of a clothed person, using a high-quality clothing dataset we created. Our results demonstrate decent output.",1
"We propose a new framework for the analysis of low-rank tensors which lies at the intersection of spectral graph theory and signal processing. As a first step, we present a new graph based low-rank decomposition which approximates the classical low-rank SVD for matrices and multi-linear SVD for tensors. Then, building on this novel decomposition we construct a general class of convex optimization problems for approximately solving low-rank tensor inverse problems, such as tensor Robust PCA. The whole framework is named as 'Multilinear Low-rank tensors on Graphs (MLRTG)'. Our theoretical analysis shows: 1) MLRTG stands on the notion of approximate stationarity of multi-dimensional signals on graphs and 2) the approximation error depends on the eigen gaps of the graphs. We demonstrate applications for a wide variety of 4 artificial and 12 real tensor datasets, such as EEG, FMRI, BCI, surveillance videos and hyperspectral images. Generalization of the tensor concepts to non-euclidean domain, orders of magnitude speed-up, low-memory requirement and significantly enhanced performance at low SNR are the key aspects of our framework.",0
"Our proposed framework bridges spectral graph theory and signal processing for the analysis of low-rank tensors. Initially, we introduce a graph-based low-rank decomposition that approximates the traditional low-rank SVD for matrices and multi-linear SVD for tensors. We then utilize this innovative decomposition to create a group of general convex optimization problems that approximate low-rank tensor inverse problems, like tensor Robust PCA. Our complete framework is called 'Multilinear Low-rank tensors on Graphs (MLRTG)', and its theoretical analysis indicates that it relies on the idea of approximate stationarity of multi-dimensional signals on graphs, and the approximation error relies on the eigen gaps of the graphs. We showcase applications for a broad range of 4 artificial and 12 real tensor datasets, including EEG, FMRI, BCI, surveillance videos, and hyperspectral images. Key features of our framework include generalization of tensor concepts to the non-euclidean domain, orders of magnitude speed-up, low-memory requirement, and significantly improved performance at low SNR.",1
"We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given function f, which accepts inputs in either domains, would remain unchanged. Other than the function f, the training data is unsupervised and consist of a set of samples from each domain. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f-constancy component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.",0
"In this study, we aim to address the issue of transferring a sample from one domain to another domain in a similar manner. Our focus is on two interconnected domains, S and T, where we intend to acquire knowledge about a generative function G that can map an input sample from S to the T domain. The primary objective is to ensure that the output of a given function f, which can accept inputs from both domains, remains constant. The training data is unsupervised and comprises a collection of samples from each domain. To achieve this objective, we propose the Domain Transfer Network (DTN), which utilizes a multi-faceted loss function that includes a multiclass GAN loss, an f-consistency component, and a regulatory component that promotes G to map samples from T to themselves. We have applied this method to visual domains such as digits and facial images and demonstrated its ability to generate convincing novel images of unseen entities while preserving their identity.",1
"Gatys et al. (2015) showed that pair-wise products of features in a convolutional network are a very effective representation of image textures. We propose a simple modification to that representation which makes it possible to incorporate long-range structure into image generation, and to render images that satisfy various symmetry constraints. We show how this can greatly improve rendering of regular textures and of images that contain other kinds of symmetric structure. We also present applications to inpainting and season transfer.",0
"In 2015, Gatys et al. demonstrated the effectiveness of using pair-wise products of features in a convolutional network to represent image textures. Our proposal involves a minor alteration to this representation, allowing for the inclusion of long-range structure in image generation and the ability to produce images that meet a variety of symmetry requirements. This modification vastly enhances the rendering of both regular textures and images with other types of symmetric structure. Additionally, we showcase its use in applications such as inpainting and season transfer.",1
"This paper addresses the problem of 3D human pose estimation in the wild. A significant challenge is the lack of training data, i.e., 2D images of humans annotated with 3D poses. Such data is necessary to train state-of-the-art CNN architectures. Here, we propose a solution to generate a large set of photorealistic synthetic images of humans with 3D pose annotations. We introduce an image-based synthesis engine that artificially augments a dataset of real images with 2D human pose annotations using 3D Motion Capture (MoCap) data. Given a candidate 3D pose our algorithm selects for each joint an image whose 2D pose locally matches the projected 3D pose. The selected images are then combined to generate a new synthetic image by stitching local image patches in a kinematically constrained manner. The resulting images are used to train an end-to-end CNN for full-body 3D pose estimation. We cluster the training data into a large number of pose classes and tackle pose estimation as a K-way classification problem. Such an approach is viable only with large training sets such as ours. Our method outperforms the state of the art in terms of 3D pose estimation in controlled environments (Human3.6M) and shows promising results for in-the-wild images (LSP). This demonstrates that CNNs trained on artificial images generalize well to real images.",0
"The main focus of this study is the challenge of estimating 3D human poses in natural settings. One of the significant obstacles is the lack of training data, which is necessary for training modern CNN architectures. The proposed solution involves generating a vast collection of artificially created images of humans with 3D pose annotations using an image-based synthesis engine. The engine augments a dataset of real images with 2D human pose annotations using 3D Motion Capture (MoCap) data. The algorithm selects images for each joint whose 2D pose matches the projected 3D pose, and these images are combined to create a new synthetic image. The resulting images are then used to train an end-to-end CNN for full-body 3D pose estimation. The training data is clustered into numerous pose classes, and the pose estimation is addressed as a K-way classification problem. This approach is only viable with a large training dataset, such as the one used in this study. The results indicate that the proposed method outperforms the state of the art in terms of 3D pose estimation in controlled environments (Human3.6M) and shows promising results for in-the-wild images (LSP). This demonstrates that CNNs trained on artificial images generalize well to real images.",1
"The Teacher Forcing algorithm trains recurrent networks by supplying observed sequence values as inputs during training and using the network's own one-step-ahead predictions to do multi-step sampling. We introduce the Professor Forcing algorithm, which uses adversarial domain adaptation to encourage the dynamics of the recurrent network to be the same when training the network and when sampling from the network over multiple time steps. We apply Professor Forcing to language modeling, vocal synthesis on raw waveforms, handwriting generation, and image generation. Empirically we find that Professor Forcing acts as a regularizer, improving test likelihood on character level Penn Treebank and sequential MNIST. We also find that the model qualitatively improves samples, especially when sampling for a large number of time steps. This is supported by human evaluation of sample quality. Trade-offs between Professor Forcing and Scheduled Sampling are discussed. We produce T-SNEs showing that Professor Forcing successfully makes the dynamics of the network during training and sampling more similar.",0
"During the training of recurrent networks, the Teacher Forcing algorithm uses observed sequence values as inputs and the network's one-step-ahead predictions for multi-step sampling. However, the newly introduced Professor Forcing algorithm implements adversarial domain adaptation to ensure that the network's dynamics remain consistent during both training and sampling. We conduct experiments on language modeling, vocal synthesis, handwriting generation, and image generation, and observe that Professor Forcing acts as a regularizer and improves the likelihood of test results on character level Penn Treebank and sequential MNIST. Moreover, it enhances sample quality, particularly for extended time steps, as confirmed by human evaluation. We discuss the trade-offs between Professor Forcing and Scheduled Sampling and provide T-SNEs to demonstrate that Professor Forcing successfully aligns the network's dynamics during training and sampling.",1
"This paper investigates a novel problem of generating images from visual attributes. We model the image as a composite of foreground and background and develop a layered generative model with disentangled latent variables that can be learned end-to-end using a variational auto-encoder. We experiment with natural images of faces and birds and demonstrate that the proposed models are capable of generating realistic and diverse samples with disentangled latent representations. We use a general energy minimization algorithm for posterior inference of latent variables given novel images. Therefore, the learned generative models show excellent quantitative and visual results in the tasks of attribute-conditioned image reconstruction and completion.",0
"The focus of this research paper is on the production of images based on visual attributes. Our approach involves breaking down the image into foreground and background components, and utilizing a layered generative model that incorporates disentangled latent variables. This model can be trained end-to-end using a variational auto-encoder. We conducted experiments using images of faces and birds, and found that our model was capable of generating a range of realistic and diverse samples with disentangled latent representations. We employed a general energy minimization algorithm for posterior inference of latent variables in order to produce attribute-conditioned image reconstruction and completion, and our results were both visually and quantitatively impressive.",1
"This letter proposes a simple method of transferring rain structures of a given exemplar rain image into a target image. Given the exemplar rain image and its corresponding masked rain image, rain patches including rain structures are extracted randomly, and then residual rain patches are obtained by subtracting those rain patches from their mean patches. Next, residual rain patches are selected randomly, and then added to the given target image along a raster scanning direction. To decrease boundary artifacts around the added patches on the target image, minimum error boundary cuts are found using dynamic programming, and then blending is conducted between overlapping patches. Our experiment shows that the proposed method can generate realistic rain images that have similar rain structures in the exemplar images. Moreover, it is expected that the proposed method can be used for rain removal. More specifically, natural images and synthetic rain images generated via the proposed method can be used to learn classifiers, for example, deep neural networks, in a supervised manner.",0
"In this letter, a straightforward approach is introduced for transferring rain structures from one image to another. First, rain patches with structures are randomly extracted from an exemplar rain image and its corresponding masked rain image. Then, residual rain patches are obtained by subtracting these patches from their mean patches. Afterward, the residual patches are randomly chosen and added to the target image in a raster scanning direction. To avoid boundary artifacts, minimum error boundary cuts are found using dynamic programming, and blending is performed between overlapping patches. The results of our experiment indicate that this method can produce realistic rain images that closely resemble the exemplar images. Additionally, this technique could be used for rain removal, and natural and synthetic rain images generated using this method may be used to train classifiers like deep neural networks in a supervised manner.",1
"Generative Adversarial Networks (GAN) are able to learn excellent representations for unlabelled data which can be applied to image generation and scene classification. Representations learned by GANs have not yet been applied to retrieval. In this paper, we show that the representations learned by GANs can indeed be used for retrieval. We consider heritage documents that contain unlabelled Merchant Marks, sketch-like symbols that are similar to hieroglyphs. We introduce a novel GAN architecture with design features that make it suitable for sketch retrieval. The performance of this sketch-GAN is compared to a modified version of the original GAN architecture with respect to simple invariance properties. Experiments suggest that sketch-GANs learn representations that are suitable for retrieval and which also have increased stability to rotation, scale and translation compared to the standard GAN architecture.",0
"GANs have the ability to learn exceptional representations for unlabeled data, which can be utilized for image generation and scene classification. However, the learned representations have not yet been implemented for retrieval. The aim of this study is to demonstrate that the representations learned by GANs can be used for retrieval by focusing on heritage documents containing unlabelled Merchant Marks, which are sketch-like symbols similar to hieroglyphs. To achieve this, a novel GAN architecture is introduced, specifically designed for sketch retrieval. The performance of this sketch-GAN is compared to a modified version of the original GAN architecture in terms of simple invariance properties. The results of the experiments suggest that the sketch-GANs are capable of learning representations suitable for retrieval, which also exhibit increased stability to rotation, scale, and translation when compared to the standard GAN architecture.",1
"Generating synthetic images is an art which emulates the natural process of image generation in a closest possible manner. In this work, we exploit such a framework for data generation in handwritten domain. We render synthetic data using open source fonts and incorporate data augmentation schemes. As part of this work, we release 9M synthetic handwritten word image corpus which could be useful for training deep network architectures and advancing the performance in handwritten word spotting and recognition tasks.",0
"The creation of artificial images involves replicating the process of natural image production as closely as possible. In this study, we utilize this technique to generate data in the field of handwritten text. We use freely available fonts to produce synthetic data and apply methods for data augmentation. As a result of our efforts, we provide a corpus of 9 million synthetic handwritten word images that can aid in the development of deep neural networks and the improvement of performance in tasks related to the identification and retrieval of handwritten words.",1
"In this work, we consider the task of generating highly-realistic images of a given face with a redirected gaze. We treat this problem as a specific instance of conditional image generation and suggest a new deep architecture that can handle this task very well as revealed by numerical comparison with prior art and a user study. Our deep architecture performs coarse-to-fine warping with an additional intensity correction of individual pixels. All these operations are performed in a feed-forward manner, and the parameters associated with different operations are learned jointly in the end-to-end fashion. After learning, the resulting neural network can synthesize images with manipulated gaze, while the redirection angle can be selected arbitrarily from a certain range and provided as an input to the network.",0
"In this study, our focus is on creating realistic images of a given face with a redirected gaze. Our approach involves treating this task as a specific example of conditional image generation. To address this, we propose a new deep architecture that outperforms previous methods, as demonstrated through numerical comparison and a user study. Our deep architecture utilizes a coarse-to-fine warping technique and intensity correction of individual pixels, all of which are executed in a feed-forward manner. By learning the parameters associated with these operations jointly and in an end-to-end fashion, our resulting neural network can successfully synthesize images with manipulated gaze. The redirection angle can be selected within a certain range and provided as input to the network.",1
"Current generative frameworks use end-to-end learning and generate images by sampling from uniform noise distribution. However, these approaches ignore the most basic principle of image formation: images are product of: (a) Structure: the underlying 3D model; (b) Style: the texture mapped onto structure. In this paper, we factorize the image generation process and propose Style and Structure Generative Adversarial Network (S^2-GAN). Our S^2-GAN has two components: the Structure-GAN generates a surface normal map; the Style-GAN takes the surface normal map as input and generates the 2D image. Apart from a real vs. generated loss function, we use an additional loss with computed surface normals from generated images. The two GANs are first trained independently, and then merged together via joint learning. We show our S^2-GAN model is interpretable, generates more realistic images and can be used to learn unsupervised RGBD representations.",0
"Existing generative frameworks utilize end-to-end learning and generate images through uniform noise distribution sampling, but they neglect the fundamental principle of image formation that involves structure and style. The former pertains to the underlying 3D model, while the latter relates to the texture mapped onto the structure. In this study, we introduce the Style and Structure Generative Adversarial Network (S^2-GAN) that separates the image generation process. Our S^2-GAN consists of two components: the Structure-GAN that produces a surface normal map and the Style-GAN that generates the 2D image using the surface normal map as input. We employ real versus generated loss function and an additional loss with computed surface normals from generated images. The two GANs are initially trained independently, then merged through joint learning. Our S^2-GAN model is interpretable, produces more realistic images, and can be utilized for unsupervised RGBD representations learning.",1
"Illumination effects cause problems for many computer vision algorithms. We present a user-friendly interactive system for robust illumination-invariant image generation. Compared with the previous automated illumination-invariant image derivation approaches, our system enables users to specify a particular kind of illumination variation for removal. The derivation of illumination-invariant image is guided by the user input. The input is a stroke that defines an area covering a set of pixels whose intensities are influenced predominately by the illumination variation. This additional flexibility enhances the robustness for processing non-linearly rendered images and the images of the scenes where their illumination variations are difficult to estimate automatically. Finally, we present some evaluation results of our method.",0
"Many computer vision algorithms face issues due to illumination effects. Our system offers an interactive solution for generating illumination-invariant images that is easy to use. Unlike previous automated approaches, our system allows users to specify the type of illumination variation they want to remove. The user input is a stroke that identifies an area covering a set of pixels predominantly affected by illumination variation. This feature improves the system's ability to process non-linearly rendered images and scenes where automatic estimation of illumination variation is challenging. Lastly, we provide evaluation results of our approach.",1
"This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.",0
"In this study, a novel image density model based on PixelCNN architecture is examined for conditional image generation. The model is flexible in that it can be conditioned on various types of vectors, such as descriptive labels, tags, or latent embeddings from other networks. When conditioned on class labels from the ImageNet database, the model can produce diverse and realistic scenes of animals, objects, landscapes, and structures. In addition, when conditioned on an embedding created by a convolutional network from a single image of an unknown person's face, the model can generate various portraits of the same person with different facial expressions, poses, and lighting conditions. Furthermore, the proposed model's gated convolutional layers enhance the log-likelihood of PixelCNN, matching the performance of PixelRNN on ImageNet while reducing computational costs. The study also demonstrates the capacity of conditional PixelCNN as a potent decoder in an image autoencoder.",1
"Memory units have been widely used to enrich the capabilities of deep networks on capturing long-term dependencies in reasoning and prediction tasks, but little investigation exists on deep generative models (DGMs) which are good at inferring high-level invariant representations from unlabeled data. This paper presents a deep generative model with a possibly large external memory and an attention mechanism to capture the local detail information that is often lost in the bottom-up abstraction process in representation learning. By adopting a smooth attention model, the whole network is trained end-to-end by optimizing a variational bound of data likelihood via auto-encoding variational Bayesian methods, where an asymmetric recognition network is learnt jointly to infer high-level invariant representations. The asymmetric architecture can reduce the competition between bottom-up invariant feature extraction and top-down generation of instance details. Our experiments on several datasets demonstrate that memory can significantly boost the performance of DGMs and even achieve state-of-the-art results on various tasks, including density estimation, image generation, and missing value imputation.",0
"Although memory units have been frequently utilized to enhance deep networks' ability to capture long-term dependencies in prediction and reasoning tasks, little research has been conducted on deep generative models (DGMs), which are skilled at deducing high-level invariant representations from unlabelled data. This article introduces a deep generative model with a potentially large external memory and an attention mechanism to capture local detail information typically lost in the bottom-up abstraction process in representation learning. Using a smooth attention model, the entire network is trained end-to-end by optimizing a variational bound of data likelihood through auto-encoding variational Bayesian methods. The asymmetric recognition network is learned jointly to deduce high-level invariant representations, reducing the competition between bottom-up invariant feature extraction and top-down generation of instance details. Our experiments on various datasets demonstrate that memory can significantly boost DGMs' performance, even achieving state-of-the-art results on density estimation, image generation, and missing value imputation tasks.",1
"Humans have an impressive ability to reason about new concepts and experiences from just a single example. In particular, humans have an ability for one-shot generalization: an ability to encounter a new concept, understand its structure, and then be able to generate compelling alternative variations of the concept. We develop machine learning systems with this important capacity by developing new deep generative models, models that combine the representational power of deep learning with the inferential power of Bayesian reasoning. We develop a class of sequential generative models that are built on the principles of feedback and attention. These two characteristics lead to generative models that are among the state-of-the art in density estimation and image generation. We demonstrate the one-shot generalization ability of our models using three tasks: unconditional sampling, generating new exemplars of a given concept, and generating new exemplars of a family of concepts. In all cases our models are able to generate compelling and diverse samples---having seen new examples just once---providing an important class of general-purpose models for one-shot machine learning.",0
"Just from a single example, humans can reason about new concepts and experiences, showcasing their impressive ability for one-shot generalization. This skill allows them to understand the structure of a new concept and generate alternative variations of it. To imbue machine learning systems with this ability, deep generative models that combine the power of deep learning with Bayesian reasoning are developed. A new class of sequential generative models that follow feedback and attention principles are also created, resulting in state-of-the-art models for density estimation and image generation. The models are tested in three tasks, namely unconditional sampling, generating new exemplars of a given concept, and generating new exemplars of a family of concepts, where they can generate diverse and compelling samples even after seeing new examples just once. These models are crucial for one-shot machine learning and are considered a general-purpose solution.",1
"Deep learning algorithm display powerful ability in Computer Vision area, in recent year, the CNN has been applied to solve problems in the subarea of Image-generating, which has been widely applied in areas such as photo editing, image design, computer animation, real-time rendering for large scale of scenes and for visual effects in movies. However in the texture synthesize procedure. The state-of-art CNN can not capture the spatial location of texture in image, lead to significant distortion after texture synthesize, we propose a new way to generating-image by adding the semantic segment step with deep learning algorithm as Pre-Processing and analyze the outcome.",0
"In recent years, the Computer Vision area has witnessed the powerful capabilities of deep learning algorithms, particularly in the subarea of Image-generating. The CNN has been widely applied in photo editing, image design, computer animation, real-time rendering for large-scale scenes, and visual effects in movies. However, current state-of-the-art CNNs are unable to capture the spatial location of texture in the image during the texture synthesis procedure, resulting in significant distortions. To overcome this limitation, we propose a new approach to image generation by integrating a semantic segmentation step as a pre-processing step with deep learning algorithms. We also analyze the resulting outcomes.",1
"For applications such as airport border control, biometric technologies that can process many capture subjects quickly, efficiently, with weak supervision, and with minimal discomfort are desirable. Facial recognition is particularly appealing because it is minimally invasive yet offers relatively good recognition performance. Unfortunately, the combination of weak supervision and minimal invasiveness makes even highly accurate facial recognition systems susceptible to spoofing via presentation attacks. Thus, there is great demand for an effective and low cost system capable of rejecting such attacks.To this end we introduce PARAPH -- a novel hardware extension that exploits different measurements of light polarization to yield an image space in which presentation media are readily discernible from Bona Fide facial characteristics. The PARAPH system is inexpensive with an added cost of less than 10 US dollars. The system makes two polarization measurements in rapid succession, allowing them to be approximately pixel-aligned, with a frame rate limited by the camera, not the system. There are no moving parts above the molecular level, due to the efficient use of twisted nematic liquid crystals. We present evaluation images using three presentation attack media next to an actual face -- high quality photos on glossy and matte paper and a video of the face on an LCD. In each case, the actual face in the image generated by PARAPH is structurally discernible from the presentations, which appear either as noise (print attacks) or saturated images (replay attacks).",0
"Efficient and non-invasive biometric technologies are necessary for various applications such as airport border control. Facial recognition is an attractive option due to its minimally invasive nature and good recognition performance. However, weak supervision and minimal invasiveness make facial recognition systems vulnerable to presentation attacks, which has increased the demand for an affordable and effective system that can reject such attacks. We introduce PARAPH, a low-cost hardware extension that utilizes light polarization measurements to distinguish between presentation media and genuine facial features. The system is cost-effective, with a price tag of less than 10 US dollars, and has no moving parts above the molecular level. By making two polarization measurements in rapid succession, PARAPH creates an image space where presentation attacks appear as noise or saturated images, while actual faces remain structurally distinguishable. Evaluation images show that PARAPH effectively distinguishes between high-quality photos on glossy and matte paper and a video of a face on an LCD.",1
"While deep convolutional neural networks (CNNs) have shown a great success in single-label image classification, it is important to note that real world images generally contain multiple labels, which could correspond to different objects, scenes, actions and attributes in an image. Traditional approaches to multi-label image classification learn independent classifiers for each category and employ ranking or thresholding on the classification results. These techniques, although working well, fail to explicitly exploit the label dependencies in an image. In this paper, we utilize recurrent neural networks (RNNs) to address this problem. Combined with CNNs, the proposed CNN-RNN framework learns a joint image-label embedding to characterize the semantic label dependency as well as the image-label relevance, and it can be trained end-to-end from scratch to integrate both information in a unified framework. Experimental results on public benchmark datasets demonstrate that the proposed architecture achieves better performance than the state-of-the-art multi-label classification model",0
"Although deep convolutional neural networks (CNNs) have demonstrated impressive success in single-label image classification, it's important to acknowledge that real-world images typically contain multiple labels that correspond to various objects, scenes, actions, and attributes. Conventional approaches to multi-label image classification involve training independent classifiers for each category and utilizing ranking or thresholding on the classification outcomes. While these techniques work effectively, they do not explicitly leverage the label dependencies present in an image. This paper addresses this issue by using recurrent neural networks (RNNs) in combination with CNNs. The proposed CNN-RNN framework learns a joint image-label embedding that characterizes both the semantic label dependency and the image-label relevance. It can be trained end-to-end from scratch to integrate both pieces of information in a unified framework. Experimental results on public benchmark datasets demonstrate that the proposed architecture outperforms the current state-of-the-art multi-label classification model.",1
"Convolutional neural networks (CNNs) have proven highly effective at image synthesis and style transfer. For most users, however, using them as tools can be a challenging task due to their unpredictable behavior that goes against common intuitions. This paper introduces a novel concept to augment such generative architectures with semantic annotations, either by manually authoring pixel labels or using existing solutions for semantic segmentation. The result is a content-aware generative algorithm that offers meaningful control over the outcome. Thus, we increase the quality of images generated by avoiding common glitches, make the results look significantly more plausible, and extend the functional range of these algorithms---whether for portraits or landscapes, etc. Applications include semantic style transfer and turning doodles with few colors into masterful paintings!",0
"Image synthesis and style transfer have been greatly enhanced by the effectiveness of convolutional neural networks (CNNs). Nonetheless, most users find it challenging to utilize them due to their unpredictable behavior, which defies common intuitions. To address this issue, this study proposes a new approach to augment generative architectures with semantic annotations, either through manual pixel labeling or by adopting existing semantic segmentation solutions. This method results in a content-aware generative algorithm that provides more meaningful control over the output. As a result, this technique avoids common glitches, significantly improves the plausibility of results, and expands the functional capabilities of these algorithms, irrespective of whether they are used for landscapes, portraits, or other applications. The potential applications of this approach include semantic style transfer and converting simple doodles into intricate paintings.",1
"Motivated by the recent progress in generative models, we introduce a model that generates images from natural language descriptions. The proposed model iteratively draws patches on a canvas, while attending to the relevant words in the description. After training on Microsoft COCO, we compare our model with several baseline generative models on image generation and retrieval tasks. We demonstrate that our model produces higher quality samples than other approaches and generates images with novel scene compositions corresponding to previously unseen captions in the dataset.",0
"We present a model that creates images based on natural language descriptions, inspired by the advancements in generative models. Our model follows an iterative process of drawing patches on a canvas and paying attention to the relevant words in the description. Through training on Microsoft COCO, we compare our model to various other generative models on tasks related to image generation and retrieval. Our results show that our model outperforms other approaches by generating high-quality samples and producing images with unique scene compositions that correspond to previously unseen captions in the dataset.",1
"Image-generating machine learning models are typically trained with loss functions based on distance in the image space. This often leads to over-smoothed results. We propose a class of loss functions, which we call deep perceptual similarity metrics (DeePSiM), that mitigate this problem. Instead of computing distances in the image space, we compute distances between image features extracted by deep neural networks. This metric better reflects perceptually similarity of images and thus leads to better results. We show three applications: autoencoder training, a modification of a variational autoencoder, and inversion of deep convolutional networks. In all cases, the generated images look sharp and resemble natural images.",0
"Machine learning models that produce images are usually trained with loss functions that are based on measuring distances in the image space. This approach often results in images that appear overly smoothed. To address this issue, we introduce a new type of loss function called deep perceptual similarity metrics (DeePSiM) that can remedy this problem. Instead of calculating distances in the image space, DeePSiM measures distances between image features that are obtained from deep neural networks. This method better captures the perceptual similarity of images, resulting in improved image quality. We demonstrate the effectiveness of this approach in three applications: autoencoder training, modifying a variational autoencoder, and inverting deep convolutional networks. In all cases, the generated images are sharp and have a natural appearance.",1
"A deep generative model is developed for representation and analysis of images, based on a hierarchical convolutional dictionary-learning framework. Stochastic {\em unpooling} is employed to link consecutive layers in the model, yielding top-down image generation. A Bayesian support vector machine is linked to the top-layer features, yielding max-margin discrimination. Deep deconvolutional inference is employed when testing, to infer the latent features, and the top-layer features are connected with the max-margin classifier for discrimination tasks. The model is efficiently trained using a Monte Carlo expectation-maximization (MCEM) algorithm, with implementation on graphical processor units (GPUs) for efficient large-scale learning, and fast testing. Excellent results are obtained on several benchmark datasets, including ImageNet, demonstrating that the proposed model achieves results that are highly competitive with similarly sized convolutional neural networks.",0
"A hierarchical convolutional dictionary-learning framework is utilized to create a deep generative model for the representation and analysis of images. The model incorporates stochastic ""unpooling"" to connect consecutive layers and facilitate top-down image generation. A Bayesian support vector machine is integrated with the top-layer features to enable max-margin discrimination. During testing, deep deconvolutional inference is employed to infer latent features, and the top-layer features are connected with the max-margin classifier for discrimination tasks. The Monte Carlo expectation-maximization (MCEM) algorithm efficiently trains the model, and graphical processor units (GPUs) are used for large-scale learning and fast testing. The proposed model achieves excellent results on various benchmark datasets, including ImageNet, and is highly competitive with similarly sized convolutional neural networks.",1
"Learning the distribution of images in order to generate new samples is a challenging task due to the high dimensionality of the data and the highly non-linear relations that are involved. Nevertheless, some promising results have been reported in the literature recently,building on deep network architectures. In this work, we zoom in on a specific type of image generation: given an image and knowing the category of objects it belongs to (e.g. faces), our goal is to generate a similar and plausible image, but with some altered attributes. This is particularly challenging, as the model needs to learn to disentangle the effect of each attribute and to apply a desired attribute change to a given input image, while keeping the other attributes and overall object appearance intact. To this end, we learn a convolutional network, where the desired attribute information is encoded then merged with the encoded image at feature map level. We show promising results, both qualitatively as well as quantitatively, in the context of a retrieval experiment, on two face datasets (MultiPie and CAS-PEAL-R1).",0
"Generating new samples by learning the distribution of images is a difficult task due to the complexity of the data and its non-linear relationships. However, recent literature has shown encouraging results through the use of deep network architectures. The focus of this study is on a specific type of image generation, which aims to produce a similar and realistic image with some modified attributes, given an image and its object category (e.g. faces). This is challenging because the model must learn to separate each attribute and apply the desired change while maintaining the original object appearance. To achieve this, a convolutional network was trained, where attribute information was encoded and merged with the image at the feature map level. The study demonstrated promising results in both qualitative and quantitative analyses, using two face datasets (MultiPie and CAS-PEAL-R1) in a retrieval experiment.",1
"We introduce language-driven image generation, the task of generating an image visualizing the semantic contents of a word embedding, e.g., given the word embedding of grasshopper, we generate a natural image of a grasshopper. We implement a simple method based on two mapping functions. The first takes as input a word embedding (as produced, e.g., by the word2vec toolkit) and maps it onto a high-level visual space (e.g., the space defined by one of the top layers of a Convolutional Neural Network). The second function maps this abstract visual representation to pixel space, in order to generate the target image. Several user studies suggest that the current system produces images that capture general visual properties of the concepts encoded in the word embedding, such as color or typical environment, and are sufficient to discriminate between general categories of objects.",0
"Our study introduces a language-driven approach to generating images that represent the semantic contents of a word embedding. For instance, if we have the word embedding for ""grasshopper,"" we can generate a natural image of a grasshopper. Our method uses two mapping functions. The first takes the word embedding produced by tools like word2vec and maps it to a high-level visual space, such as the space defined by a Convolutional Neural Network layer. The second function then maps this abstract visual representation to pixel space to generate the desired image. Our system has undergone several user studies, which suggest that the generated images capture general visual properties of the concepts represented by the word embedding, such as color and typical environment. These images are also sufficient to distinguish between general categories of objects.",1
"Selecting the most suitable local invariant feature detector for a particular application has rendered the task of evaluating feature detectors a critical issue in vision research. No state-of-the-art image feature detector works satisfactorily under all types of image transformations. Although the literature offers a variety of comparison works focusing on performance evaluation of image feature detectors under several types of image transformation, the influence of the scene content on the performance of local feature detectors has received little attention so far. This paper aims to bridge this gap with a new framework for determining the type of scenes, which maximize and minimize the performance of detectors in terms of repeatability rate. Several state-of-the-art feature detectors have been assessed utilizing a large database of 12936 images generated by applying uniform light and blur changes to 539 scenes captured from the real world. The results obtained provide new insights into the behaviour of feature detectors.",0
"Evaluating feature detectors is a crucial issue in vision research as no image feature detector can work effectively under all image transformations. While there are many comparison works evaluating detector performance under various image transformations, little attention has been given to how scene content affects local feature detector performance. This study aims to address this gap by presenting a new framework that determines which scenes maximize and minimize detector performance in terms of repeatability rate. The study evaluated several state-of-the-art feature detectors using a large database of 12936 images that underwent uniform light and blur changes applied to 539 real-world scenes. The results provide new insights into the behavior of feature detectors.",1
"Tracking moving objects from a video sequence requires segmentation of these objects from the background image. However, getting the actual background image automatically without object detection and using only the video is difficult. In this paper, we describe a novel algorithm that generates background from real world images without foreground detection. The algorithm assumes that the background image is shown in the majority of the video. Given this simple assumption, the method described in this paper is able to accurately generate, with high probability, the background image from a video using only a small number of binary operations.",0
"In order to track objects that are in motion within a video sequence, it is essential to differentiate these objects from their background imagery. This is a challenging task as obtaining the genuine background image solely from the video, without object detection, can prove to be difficult. This research paper presents a unique algorithm that generates a background image from actual images of the real world, without the need for foreground detection. The algorithm operates on the assumption that the background image dominates the majority of the video. Thus, using a few binary operations, the algorithm is able to accurately and with high probability generate the background image from the video.",1
"In this paper we are proposing the use of GIMP Retinex, a filter of the GNU Image Manipulation Program, for enhancing foggy images. This filter involves adjusting four different parameters to find the output image which has to be preferred according to some specific purposes. Aiming to obtain a processing, which is able of choosing automatically the best image from a given set, we are proposing a method for the generation a bulk set of GIMP Retinex filtered images and a preliminary approach for selecting and ranking them.",0
"The proposed method in this paper suggests utilizing the GIMP Retinex filter, a component of the GNU Image Manipulation Program, to enhance hazy images. To achieve the desired output image, four parameters need to be adjusted based on the specific goals. To automate the selection process and obtain the optimal image from a set of images, a technique is proposed for generating a set of GIMP Retinex filtered images and a preliminary method for their selection and ranking.",1
"Recent advances in depth imaging sensors provide easy access to the synchronized depth with color, called RGB-D image. In this paper, we propose an unsupervised method for indoor RGB-D image segmentation and analysis. We consider a statistical image generation model based on the color and geometry of the scene. Our method consists of a joint color-spatial-directional clustering method followed by a statistical planar region merging method. We evaluate our method on the NYU depth database and compare it with existing unsupervised RGB-D segmentation methods. Results show that, it is comparable with the state of the art methods and it needs less computation time. Moreover, it opens interesting perspectives to fuse color and geometry in an unsupervised manner.",0
"RGB-D images, which synchronize depth with color, are now easily accessible due to recent advancements in depth imaging sensors. The focus of this paper is to present an unsupervised approach for the segmentation and analysis of indoor RGB-D images. Our method utilizes a statistical image generation model that is based on the scene's color and geometry. The process involves a joint color-spatial-directional clustering method, followed by a statistical planar region merging method. We have tested our approach on the NYU depth database and compared it to other unsupervised RGB-D segmentation methods. Our results show that our method is comparable to state-of-the-art techniques while requiring less computation time. Additionally, our approach presents exciting opportunities for unsupervised fusion of color and geometry.",1
"This work investigates how the traditional image classification pipelines can be extended into a deep architecture, inspired by recent successes of deep neural networks. We propose a deep boosting framework based on layer-by-layer joint feature boosting and dictionary learning. In each layer, we construct a dictionary of filters by combining the filters from the lower layer, and iteratively optimize the image representation with a joint discriminative-generative formulation, i.e. minimization of empirical classification error plus regularization of analysis image generation over training images. For optimization, we perform two iterating steps: i) to minimize the classification error, select the most discriminative features using the gentle adaboost algorithm; ii) according to the feature selection, update the filters to minimize the regularization on analysis image representation using the gradient descent method. Once the optimization is converged, we learn the higher layer representation in the same way. Our model delivers several distinct advantages. First, our layer-wise optimization provides the potential to build very deep architectures. Second, the generated image representation is compact and meaningful. In several visual recognition tasks, our framework outperforms existing state-of-the-art approaches.",0
"This study explores the extension of traditional image classification pipelines to a deep architecture, inspired by the success of deep neural networks. We propose a deep boosting framework that utilizes layer-by-layer joint feature boosting and dictionary learning. For each layer, we build a dictionary of filters by combining lower layer filters and optimize image representation with a joint discriminative-generative formulation, minimizing empirical classification error and regularization of analysis image generation over training images. Optimization involves two iterative steps: first, minimizing classification error by selecting the most discriminative features using the gentle adaboost algorithm, and second, updating filters based on feature selection to minimize regularization on analysis image representation with the gradient descent method. Our model has advantages, including the potential to build deep architectures using layer-wise optimization and generating compact and meaningful image representations. Our framework outperforms existing state-of-the-art approaches in several visual recognition tasks.",1
"In this paper, we introduce a new dataset consisting of 360,001 focused natural language descriptions for 10,738 images. This dataset, the Visual Madlibs dataset, is collected using automatically produced fill-in-the-blank templates designed to gather targeted descriptions about: people and objects, their appearances, activities, and interactions, as well as inferences about the general scene or its broader context. We provide several analyses of the Visual Madlibs dataset and demonstrate its applicability to two new description generation tasks: focused description generation, and multiple-choice question-answering for images. Experiments using joint-embedding and deep learning methods show promising results on these tasks.",0
"A new dataset called the Visual Madlibs dataset is presented in this paper, which contains 360,001 natural language descriptions about 10,738 images. The dataset is collected using templates that fill in the blanks and target descriptions about people, objects, their appearances, activities, interactions, and inferences about the scene or context. The paper also includes analyses of the dataset and its usefulness for generating focused descriptions and multiple-choice question-answering for images. Joint-embedding and deep learning methods are employed in experiments, which yield promising results.",1
"This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye.",0
"The Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation is presented in this article. DRAW networks incorporate a unique spatial attention mechanism that imitates the human eye's foveation and a sequential variational auto-encoding framework that enables the step-by-step creation of intricate images. The technology significantly advances the current state of generative models on MNIST and, when trained on the Street View House Numbers dataset, produces images that are indistinguishable from genuine data with the unaided eye.",1
"The profusion of spectral bands generated by the acquisition process of hyperspectral images generally leads to high computational costs. Such difficulties arise in particular with nonlinear unmixing methods, which are naturally more complex than linear ones. This complexity, associated with the high redundancy of information within the complete set of bands, make the search of band selection algorithms relevant. With this work, we propose a band selection strategy in reproducing kernel Hilbert spaces that allows to drastically reduce the processing time required by nonlinear unmixing techniques. Simulation results show a complexity reduction of two orders of magnitude without compromising unmixing performance.",0
"Hyperspectral image acquisition produces numerous spectral bands, which often results in significant computational expenses. Nonlinear unmixing methods are particularly challenging due to their inherent complexity compared to linear methods, coupled with the excessive redundancy of information across all bands. As a result, it is essential to explore band selection algorithms that can address these challenges. In this study, we introduce a band selection approach in reproducing kernel Hilbert spaces that considerably reduces the processing time required by nonlinear unmixing techniques. Our simulation results demonstrate a two-order-of-magnitude reduction in complexity without compromising unmixing performance.",1
"The computation of the geometric transformation between a reference and a target image, known as registration or alignment, corresponds to the projection of the target image onto the transformation manifold of the reference image (the set of images generated by its geometric transformations). It, however, often takes a nontrivial form such that the exact computation of projections on the manifold is difficult. The tangent distance method is an effective algorithm to solve this problem by exploiting a linear approximation of the manifold. As theoretical studies about the tangent distance algorithm have been largely overlooked, we present in this work a detailed performance analysis of this useful algorithm, which can eventually help its implementation. We consider a popular image registration setting using a multiscale pyramid of lowpass filtered versions of the (possibly noisy) reference and target images, which is particularly useful for recovering large transformations. We first show that the alignment error has a nonmonotonic variation with the filter size, due to the opposing effects of filtering on both manifold nonlinearity and image noise. We then study the convergence of the multiscale tangent distance method to the optimal solution. We finally examine the performance of the tangent distance method in image classification applications. Our theoretical findings are confirmed by experiments on image transformation models involving translations, rotations and scalings. Our study is the first detailed study of the tangent distance algorithm that leads to a better understanding of its efficacy and to the proper selection of its design parameters.",0
"Registration or alignment, the process of computing the geometric transformation between a reference and target image, involves projecting the target image onto the transformation manifold of the reference image, which is the set of images generated by its geometric transformations. However, this projection often presents a challenging problem, as it takes a complex form that makes the exact computation of projections on the manifold difficult. The tangent distance method is an effective algorithm that solves this issue by utilizing a linear approximation of the manifold. Despite its usefulness, theoretical studies of the tangent distance algorithm have been overlooked. This work provides a comprehensive performance analysis of the algorithm, which can aid its implementation. We focus on a common image registration approach that uses a multiscale pyramid of lowpass filtered versions of the reference and target images, particularly useful for recovering large transformations. Our analysis reveals that the alignment error has a nonmonotonic variation with the filter size, due to the opposing effects of filtering on both manifold nonlinearity and image noise. We also examine the convergence of the multiscale tangent distance method to the optimal solution and evaluate its performance in image classification applications. Our theoretical findings align with experiments on image transformation models involving translations, rotations, and scalings. This study is the first of its kind to provide a detailed analysis of the tangent distance algorithm, offering insights into its effectiveness and the appropriate selection of design parameters.",1
"This paper focuses on two main issues; first one is the impact of Similarity Search to learning the training sample in metric space, and searching based on supervised learning classi-fication. In particular, four metrics space searching are based on spatial information that are introduced as the following; Cheby-shev Distance (CD); Bray Curtis Distance (BCD); Manhattan Distance (MD) and Euclidean Distance(ED) classifiers. The second issue investigates the performance of combination of mul-ti-sensor images on the supervised learning classification accura-cy. QuickBird multispectral data (MS) and panchromatic data (PAN) have been used in this study to demonstrate the enhance-ment and accuracy assessment of fused image over the original images. The supervised classification results of fusion image generated better than the MS did. QuickBird and the best results with ED classifier than the other did.",0
"This paper addresses two main topics. The first topic explores how Similarity Search impacts learning the training sample in metric space and searching based on supervised learning classification. Four metric space searching methods based on spatial information are examined: Cheby-shev Distance (CD), Bray Curtis Distance (BCD), Manhattan Distance (MD), and Euclidean Distance (ED) classifiers. The second topic investigates the effect of combining multi-sensor images on the accuracy of supervised learning classification. QuickBird multispectral data (MS) and panchromatic data (PAN) are used to illustrate the enhancement and accuracy assessment of a fused image compared to the original images. The results of the supervised classification show that the fusion image outperformed the MS image, with QuickBird and the ED classifier yielding the best results.",1
"Latent feature models are attractive for image modeling, since images generally contain multiple objects. However, many latent feature models ignore that objects can appear at different locations or require pre-segmentation of images. While the transformed Indian buffet process (tIBP) provides a method for modeling transformation-invariant features in unsegmented binary images, its current form is inappropriate for real images because of its computational cost and modeling assumptions. We combine the tIBP with likelihoods appropriate for real images and develop an efficient inference, using the cross-correlation between images and features, that is theoretically and empirically faster than existing inference techniques. Our method discovers reasonable components and achieve effective image reconstruction in natural images.",0
"Models that incorporate latent features are appealing for image modeling due to the presence of multiple objects in most images. However, many such models neglect the fact that objects can appear at various locations or necessitate prior segmentation of images. The transformed Indian buffet process (tIBP) offers a way to model transformation-invariant features in unsegmented binary images, but its present form is unsuitable for actual images due to computational costs and modeling assumptions. We have merged the tIBP with likelihoods tailored to real images, and developed an efficient inference that utilizes cross-correlation between images and features, which is both theoretically and empirically faster than current inference techniques. Our approach identifies practical components and produces effective image reconstruction in natural images.",1
"In this paper, we have proposed an extended version of Absolute Moment Block Truncation Coding (AMBTC) to compress images. Generally the elements of a bitplane used in the variants of Block Truncation Coding (BTC) are of size 1 bit. But it has been extended to two bits in the proposed method. Number of statistical moments preserved to reconstruct the compressed has also been raised from 2 to 4. Hence, the quality of the reconstructed images has been improved significantly from 33.62 to 38.12 with the increase in bpp by 1. The increased bpp (3) is further reduced to 1.75in multiple levels: in one level, by dropping 4 elements of the bitplane in such a away that the pixel values of the dropped elements can easily be interpolated with out much of loss in the quality, in level two, eight elements are dropped and reconstructed later and in level three, the size of the statistical moments is reduced. The experiments were carried over standard images of varying intensities. In all the cases, the proposed method outperforms the existing AMBTC technique in terms of both PSNR and bpp.",0
"The aim of this paper is to present an improved version of Absolute Moment Block Truncation Coding (AMBTC) that effectively compresses images. Typically, the bitplane elements used in Block Truncation Coding (BTC) variants consist of 1 bit; however, our proposed method extends this to 2 bits. Additionally, we have increased the number of statistical moments preserved for image reconstruction from 2 to 4. As a result, the quality of the reconstructed images has significantly improved from 33.62 to 38.12 with an increase in bpp by 1. We have further reduced the increased bpp (3) to 1.75 by dropping several elements of the bitplane in multiple levels. In the first level, we drop 4 elements of the bitplane in a way that the pixel values of the dropped elements can be interpolated without significant loss in quality. In the second level, we drop 8 elements and reconstruct them later. In the third level, we reduce the size of the statistical moments. We conducted experiments on standard images with varying intensities and found that our proposed method outperforms the existing AMBTC technique in terms of both PSNR and bpp.",1
"The COVID-19 pandemic has caused many shutdowns in different industries around the world. Sectors such as infrastructure construction and maintenance projects have not been suspended due to their significant effect on people's routine life. In such projects, workers work close together that makes a high risk of infection. The World Health Organization recommends wearing a face mask and practicing physical distancing to mitigate the virus's spread. This paper developed a computer vision system to automatically detect the violation of face mask wearing and physical distancing among construction workers to assure their safety on infrastructure projects during the pandemic. For the face mask detection, the paper collected and annotated 1,000 images, including different types of face mask wearing, and added them to a pre-existing face mask dataset to develop a dataset of 1,853 images. Then trained and tested multiple Tensorflow state-of-the-art object detection models on the face mask dataset and chose the Faster R-CNN Inception ResNet V2 network that yielded the accuracy of 99.8%. For physical distance detection, the paper employed the Faster R-CNN Inception V2 to detect people. A transformation matrix was used to eliminate the camera angle's effect on the object distances on the image. The Euclidian distance used the pixels of the transformed image to compute the actual distance between people. A threshold of six feet was considered to capture physical distance violation. The paper also used transfer learning for training the model. The final model was applied on four videos of road maintenance projects in Houston, TX, that effectively detected the face mask and physical distance. We recommend that construction owners use the proposed system to enhance construction workers' safety in the pandemic situation.",0
"Due to the COVID-19 pandemic, numerous industries worldwide have experienced shutdowns. However, infrastructure construction and maintenance projects have continued as they significantly impact people's daily lives. Yet, workers in these projects often work in close proximity to each other, increasing the risk of infection. The World Health Organization advises the use of face masks and physical distancing to minimize the virus's spread. To ensure workers' safety in infrastructure projects during the pandemic, this study developed a computer vision system capable of automatically detecting violations of face mask wearing and physical distancing among construction workers. The study collected and annotated 1,000 images of different types of face masks to add to an existing dataset, resulting in a dataset of 1,853 images. The study trained multiple Tensorflow object detection models on the face mask dataset, ultimately selecting the Faster R-CNN Inception ResNet V2 network, which demonstrated 99.8% accuracy. For physical distance detection, the study used the Faster R-CNN Inception V2 to locate people and employed a transformation matrix to eliminate camera angle effects on object distances. A threshold of six feet was used to identify physical distance violations. The study utilized transfer learning to train the model, which was then tested on four videos of road maintenance projects in Houston, TX. The system effectively detected violations of face mask wearing and physical distancing, and we recommend that construction owners use this system to improve workers' safety during the pandemic.",1
"While modern deep neural networks are performant perception modules, performance (accuracy) alone is insufficient, particularly for safety-critical robotic applications such as self-driving vehicles. Robot autonomy stacks also require these otherwise blackbox models to produce reliable and calibrated measures of confidence on their predictions. Existing approaches estimate uncertainty from these neural network perception stacks by modifying network architectures, inference procedure, or loss functions. However, in general, these methods lack calibration, meaning that the predictive uncertainties do not faithfully represent the true underlying uncertainties (process noise). Our key insight is that calibration is only achieved by imposing constraints across multiple examples, such as those in a mini-batch; as opposed to existing approaches which only impose constraints per-sample, often leading to overconfident (thus miscalibrated) uncertainty estimates. By enforcing the distribution of outputs of a neural network to resemble a target distribution by minimizing an $f$-divergence, we obtain significantly better-calibrated models compared to prior approaches. Our approach, $f$-Cal, outperforms existing uncertainty calibration approaches on robot perception tasks such as object detection and monocular depth estimation over multiple real-world benchmarks.",0
"Although modern deep neural networks are effective perception modules, their accuracy alone is insufficient for safety-critical robotic applications like self-driving vehicles. Reliable and calibrated measures of confidence on their predictions are necessary for robot autonomy stacks. Various methods have been used to estimate uncertainty from neural network perception stacks, such as modifying network architectures, inference procedures, or loss functions. However, these methods often lack calibration and do not accurately represent the true underlying uncertainties. Our insight is that calibration is achieved by imposing constraints across multiple examples, such as those in a mini-batch, rather than per-sample. By minimizing an $f$-divergence to enforce the distribution of outputs of a neural network to resemble a target distribution, our approach, called $f$-Cal, provides significantly better-calibrated models compared to prior approaches. It outperforms existing uncertainty calibration approaches on robot perception tasks like object detection and monocular depth estimation across multiple real-world benchmarks.",1
"Rotation augmentations generally improve a model's invariance/equivariance to rotation - except in object detection. In object detection the shape is not known, therefore rotation creates a label ambiguity. We show that the de-facto method for bounding box label rotation, the Largest Box Method, creates very large labels, leading to poor performance and in many cases worse performance than using no rotation at all. We propose a new method of rotation augmentation that can be implemented in a few lines of code. First, we create a differentiable approximation of label accuracy and show that axis-aligning the bounding box around an ellipse is optimal. We then introduce Rotation Uncertainty (RU) Loss, allowing the model to adapt to the uncertainty of the labels. On five different datasets (including COCO, PascalVOC, and Transparent Object Bin Picking), this approach improves the rotational invariance of both one-stage and two-stage architectures when measured with AP, AP50, and AP75. The code is available at \url{https://github.com/akasha-imaging/ICCV2021}.",0
"Rotation augmentations are generally effective in enhancing a model's ability to maintain invariance or equivariance with respect to rotation, except in the case of object detection. This is due to the fact that the shape of objects is often unknown, which leads to confusion in labeling when rotation is applied. It has been observed that the commonly used Largest Box Method to label bounding boxes after rotation results in a significant increase in label size, thereby reducing the model's performance. In fact, using no rotation at all may lead to better performance in some cases. To address this issue, we propose a novel approach to rotation augmentation that is easy to implement. Our method involves creating a differentiable approximation of the label accuracy and aligning the bounding box around an ellipse to optimize performance. We also introduce the Rotation Uncertainty (RU) Loss, which enables the model to adapt to the uncertainty of the labels. Our approach has been tested on five different datasets, including COCO, PascalVOC, and Transparent Object Bin Picking, and has been found to improve the rotational invariance of both one-stage and two-stage architectures based on AP, AP50, and AP75 metrics. The code for our method is available at \url{https://github.com/akasha-imaging/ICCV2021}.",1
"There is a rapid growth of applications of Unmanned Aerial Vehicles (UAVs) in traffic management, such as traffic surveillance, monitoring, and incident detection. However, the existing literature lacks solutions to real-time incident detection while addressing privacy issues in practice. This study explored real-time vehicle detection algorithms on both visual and infrared cameras and conducted experiments comparing their performance. Red Green Blue (RGB) videos and thermal images were collected from a UAS platform along highways in the Tampa, Florida, area. Experiments were designed to quantify the performance of a real-time background subtraction-based method in vehicle detection from a stationary camera on hovering UAVs under free-flow conditions. Several parameters were set in the experiments based on the geometry of the drone and sensor relative to the roadway. The results show that a background subtraction-based method can achieve good detection performance on RGB images (F1 scores around 0.9 for most cases), and a more varied performance is seen on thermal images with different azimuth angles. The results of these experiments will help inform the development of protocols, standards, and guidance for the use of drones to detect highway congestion and provide input for the development of incident detection algorithms.",0
"The utilization of Unmanned Aerial Vehicles (UAVs) for traffic management, including traffic surveillance, monitoring, and incident detection, is rapidly increasing. However, current literature lacks solutions for real-time incident detection that also address privacy concerns in practical scenarios. To address this gap, this study examined real-time vehicle detection algorithms with both visual and infrared cameras and conducted experiments to compare their performance. The experiments involved collecting Red Green Blue (RGB) videos and thermal images from a UAS platform flying over highways in Tampa, Florida. The study assessed the efficacy of a real-time background subtraction-based method in vehicle detection from stationary cameras on hovering UAVs under free-flow conditions. Various parameters were established based on the drone and sensor's geometry relative to the road. Results indicate that the background subtraction-based method achieves excellent detection performance on RGB images (F1 scores approximately 0.9 in most cases). However, thermal images' performance varied significantly, depending on the azimuth angle. These findings will aid in developing protocols, standards, and guidance for utilizing drones to detect highway congestion and input in developing incident detection algorithms.",1
"This paper proposes a few-shot method based on Faster R-CNN and representation learning for object detection in aerial images. The two classification branches of Faster R-CNN are replaced by prototypical networks for online adaptation to new classes. These networks produce embeddings vectors for each generated box, which are then compared with class prototypes. The distance between an embedding and a prototype determines the corresponding classification score. The resulting networks are trained in an episodic manner. A new detection task is randomly sampled at each epoch, consisting in detecting only a subset of the classes annotated in the dataset. This training strategy encourages the network to adapt to new classes as it would at test time. In addition, several ideas are explored to improve the proposed method such as a hard negative examples mining strategy and self-supervised clustering for background objects. The performance of our method is assessed on DOTA, a large-scale remote sensing images dataset. The experiments conducted provide a broader understanding of the capabilities of representation learning. It highlights in particular some intrinsic weaknesses for the few-shot object detection task. Finally, some suggestions and perspectives are formulated according to these insights.",0
"A new approach for object detection in aerial images using Faster R-CNN and representation learning is proposed in this paper. The prototypical networks are utilized to replace the two classification branches of Faster R-CNN to allow online adaptation to new classes. The generated box produces embedding vectors which are compared with class prototypes to determine the corresponding classification score. The episodic training of the resulting networks includes a random sampling of a new detection task at each epoch to detect only a subset of the classes annotated in the dataset. The proposed method is enhanced by various techniques, such as hard negative examples mining and self-supervised clustering for background objects. The performance of the method is tested on DOTA, a vast remote sensing images dataset, to provide a comprehensive understanding of the representation learning capabilities and to reveal intrinsic weaknesses in the few-shot object detection task. The findings lead to suggestions and perspectives for future research.",1
"Model compression techniques allow to significantly reduce the computational cost associated with data processing by deep neural networks with only a minor decrease in average accuracy. Simultaneously, reducing the model size may have a large effect on noisy cases or objects belonging to less frequent classes. It is a crucial problem from the perspective of the models' safety, especially for object detection in the autonomous driving setting, which is considered in this work. It was shown in the paper that the sensitivity of compressed models to different distortion types is nuanced, and some of the corruptions are heavily impacted by the compression methods (i.e., additive noise), while others (blur effect) are only slightly affected. A common way to improve the robustness of models is to use data augmentation, which was confirmed to positively affect models' robustness, also for highly compressed models. It was further shown that while data imbalance methods brought only a slight increase in accuracy for the baseline model (without compression), the impact was more striking at higher compression rates for the structured pruning. Finally, methods for handling data imbalance brought a significant improvement of the pruned models' worst-detected class accuracy.",0
"By using model compression techniques, the computational cost of deep neural networks can be greatly reduced with only a slight decrease in average accuracy. However, shrinking the model size can have a significant impact on objects that belong to less frequent classes or in noisy cases, which is a crucial issue for ensuring model safety, especially in object detection for autonomous driving. The study found that the sensitivity of compressed models to different distortion types is nuanced, with some corruptions heavily affected by compression methods, such as additive noise, while others, like blur effect, are only slightly impacted. To improve model robustness, data augmentation is a common technique that positively affects compressed models. The research also revealed that data imbalance methods had a greater impact on highly compressed models for structured pruning, with a significant improvement in worst-detected class accuracy for pruned models using methods for handling data imbalance.",1
"Effectively structuring deep knowledge plays a pivotal role in transfer from teacher to student, especially in semantic vision tasks. In this paper, we present a simple knowledge structure to exploit and encode information inside the detection system to facilitate detector knowledge distillation. Specifically, aiming at solving the feature imbalance problem while further excavating the missing relation inside semantic instances, we design a graph whose nodes correspond to instance proposal-level features and edges represent the relation between nodes. To further refine this graph, we design an adaptive background loss weight to reduce node noise and background samples mining to prune trivial edges. We transfer the entire graph as encoded knowledge representation from teacher to student, capturing local and global information simultaneously. We achieve new state-of-the-art results on the challenging COCO object detection task with diverse student-teacher pairs on both one- and two-stage detectors. We also experiment with instance segmentation to demonstrate robustness of our method. It is notable that distilled Faster R-CNN with ResNet18-FPN and ResNet50-FPN yields 38.68 and 41.82 Box AP respectively on the COCO benchmark, Faster R-CNN with ResNet101-FPN significantly achieves 43.38 AP, which outperforms ResNet152-FPN teacher about 0.7 AP. Code: https://github.com/dvlab-research/Dsig.",0
"The successful transfer of knowledge from teacher to student, particularly in semantic vision tasks, relies heavily on having a well-structured and comprehensive understanding of the subject matter. In this research, we introduce a straightforward knowledge structure that utilizes and encodes information within the detection system to facilitate detector knowledge distillation. To address the feature imbalance issue and identify missing relations within semantic instances, we create a graph that represents the relationship between instance proposal-level features and edges. We also design an adaptive background loss weight and background sample mining to refine the graph and reduce node noise and trivial edges. The entire graph is then transferred as an encoded knowledge representation from teacher to student, capturing both local and global information simultaneously. Our approach achieves state-of-the-art results on the challenging COCO object detection task, using diverse student-teacher pairs on both one- and two-stage detectors. We also demonstrate the robustness of our method with instance segmentation. Specifically, our distilled Faster R-CNN models using ResNet18-FPN and ResNet50-FPN achieve 38.68 and 41.82 Box AP, respectively, on the COCO benchmark. Our Faster R-CNN model using ResNet101-FPN achieves 43.38 AP, which outperforms the ResNet152-FPN teacher by approximately 0.7 AP. The code for our method is available at https://github.com/dvlab-research/Dsig.",1
"Recently, many arbitrary-oriented object detection (AOOD) methods have been proposed and attracted widespread attention in many fields. However, most of them are based on anchor-boxes or standard Gaussian heatmaps. Such label assignment strategy may not only fail to reflect the shape and direction characteristics of arbitrary-oriented objects, but also have high parameter-tuning efforts. In this paper, a novel AOOD method called General Gaussian Heatmap Labeling (GGHL) is proposed. Specifically, an anchor-free object-adaptation label assignment (OLA) strategy is presented to define the positive candidates based on two-dimensional (2-D) oriented Gaussian heatmaps, which reflect the shape and direction features of arbitrary-oriented objects. Based on OLA, an oriented-bounding-box (OBB) representation component (ORC) is developed to indicate OBBs and adjust the Gaussian center prior weights to fit the characteristics of different objects adaptively through neural network learning. Moreover, a joint-optimization loss (JOL) with area normalization and dynamic confidence weighting is designed to refine the misalign optimal results of different subtasks. Extensive experiments on public datasets demonstrate that the proposed GGHL improves the AOOD performance with low parameter-tuning and time costs. Furthermore, it is generally applicable to most AOOD methods to improve their performance including lightweight models on embedded platforms.",0
"Numerous arbitrary-oriented object detection (AOOD) techniques have emerged recently and garnered considerable attention across various fields. However, many of them rely on anchor-boxes or standard Gaussian heatmaps for label assignment, which can fail to reflect the shape and direction of arbitrary-oriented objects and require significant parameter-tuning efforts. To address these limitations, this paper introduces a new AOOD approach called General Gaussian Heatmap Labeling (GGHL). GGHL uses an anchor-free object-adaptation label assignment (OLA) strategy that defines positive candidates using two-dimensional (2-D) oriented Gaussian heatmaps, which capture the shape and direction features of arbitrary-oriented objects. Additionally, GGHL employs an oriented-bounding-box (OBB) representation component (ORC) that adjusts the Gaussian center prior weights and indicates OBBs to fit the characteristics of different objects through neural network learning. Finally, GGHL employs a joint-optimization loss (JOL) with area normalization and dynamic confidence weighting to refine the optimal results of different subtasks. Experimental results on public datasets demonstrate that GGHL improves AOOD performance with low parameter-tuning and time costs. Moreover, it can be applied to most AOOD methods, including lightweight models on embedded platforms, to enhance their performance.",1
"Multi-task learning (MTL) is an efficient way to improve the performance of related tasks by sharing knowledge. However, most existing MTL networks run on a single end and are not suitable for collaborative intelligence (CI) scenarios. In this work, we propose an MTL network with a deep joint source-channel coding (JSCC) framework, which allows operating under CI scenarios. We first propose a feature fusion based MTL network (FFMNet) for joint object detection and semantic segmentation. Compared with other MTL networks, FFMNet gets higher performance with fewer parameters. Then FFMNet is split into two parts, which run on a mobile device and an edge server respectively. The feature generated by the mobile device is transmitted through the wireless channel to the edge server. To reduce the transmission overhead of the intermediate feature, a deep JSCC network is designed. By combining two networks together, the whole model achieves 512x compression for the intermediate feature and a performance loss within 2% on both tasks. At last, by training with noise, the FFMNet with JSCC is robust to various channel conditions and outperforms the separate source and channel coding scheme.",0
"The use of multi-task learning (MTL) can enhance the performance of related tasks through knowledge sharing. However, existing MTL networks are typically limited to single end use and are not suitable for collaborative intelligence (CI) scenarios. To address this issue, our study proposes a novel MTL network that utilizes a deep joint source-channel coding (JSCC) framework to enable CI scenarios. Initially, we introduce a feature fusion based MTL network (FFMNet) that combines object detection and semantic segmentation, resulting in higher performance with fewer parameters compared to other MTL networks. FFMNet is then divided into two parts, one running on a mobile device and the other on an edge server. The mobile device generates a feature that is transmitted wirelessly to the edge server, and to reduce the overhead of intermediate feature transmission, a deep JSCC network is designed. By combining the two networks, we achieve 512x compression for the intermediate feature and a performance loss within 2% on both tasks. Lastly, we train FFMNet with JSCC with noise to ensure robustness to various channel conditions, outperforming separate source and channel coding schemes.",1
"Circulating Tumor Cells (CTCs) bear great promise as biomarkers in tumor prognosis. However, the process of identification and later enumeration of CTCs require manual labor, which is error-prone and time-consuming. The recent developments in object detection via Deep Learning using Mask-RCNNs and wider availability of pre-trained models have enabled sensitive tasks with limited data of such to be tackled with unprecedented accuracy. In this report, we present a novel 3-stage detection model for automated identification of Circulating Tumor Cells in multi-channel darkfield microscopic images comprised of: RetinaNet based identification of Cytokeratin (CK) stains, Mask-RCNN based cell detection of DAPI cell nuclei and Otsu thresholding to detect CD-45s. The training dataset is composed of 46 high variance data points, with 10 Negative and 36 Positive data points. The test set is composed of 420 negative data points. The final accuracy of the pipeline is 98.81%.",0
"Biomarkers for tumor prognosis show potential in the form of Circulating Tumor Cells (CTCs). However, identifying and counting CTCs manually is prone to errors and time-consuming. Fortunately, advances in object detection through Deep Learning using Mask-RCNNs and pre-trained models have led to the ability to undertake sensitive tasks with limited data with unprecedented accuracy. This report introduces a new 3-stage detection model for automated identification of CTCs in multi-channel darkfield microscopic images. The model includes RetinaNet based identification of Cytokeratin (CK) stains, Mask-RCNN based cell detection of DAPI cell nuclei and Otsu thresholding to detect CD-45s. The training dataset comprises 46 high variance data points, with 10 Negative and 36 Positive data points, while the test set includes 420 negative data points. The pipeline's final accuracy is 98.81%.",1
"Self-attention networks have revolutionized natural language processing and are making impressive strides in image analysis tasks such as image classification and object detection. Inspired by this success, we investigate the application of self-attention networks to 3D point cloud processing. We design self-attention layers for point clouds and use these to construct self-attention networks for tasks such as semantic scene segmentation, object part segmentation, and object classification. Our Point Transformer design improves upon prior work across domains and tasks. For example, on the challenging S3DIS dataset for large-scale semantic scene segmentation, the Point Transformer attains an mIoU of 70.4% on Area 5, outperforming the strongest prior model by 3.3 absolute percentage points and crossing the 70% mIoU threshold for the first time.",0
"The use of self-attention networks has brought about a significant change in natural language processing and has also shown great progress in image analysis, including image classification and object detection. Inspired by this success, we have explored the possibility of applying self-attention networks to 3D point cloud processing. We have designed self-attention layers specifically for point clouds and utilized them to create self-attention networks for various tasks like semantic scene segmentation, object part segmentation, and object classification. Our Point Transformer design has surpassed previous work in different domains and tasks. For instance, on the challenging S3DIS dataset for large-scale semantic scene segmentation, the Point Transformer has achieved an mIoU of 70.4% on Area 5, surpassing the strongest previous model by 3.3 percentage points and crossing the 70% mIoU threshold for the first time.",1
"Neural network quantization enables the deployment of models on edge devices. An essential requirement for their hardware efficiency is that the quantizers are hardware-friendly: uniform, symmetric, and with power-of-two thresholds. To the best of our knowledge, current post-training quantization methods do not support all of these constraints simultaneously. In this work, we introduce a hardware-friendly post training quantization (HPTQ) framework, which addresses this problem by synergistically combining several known quantization methods. We perform a large-scale study on four tasks: classification, object detection, semantic segmentation and pose estimation over a wide variety of network architectures. Our extensive experiments show that competitive results can be obtained under hardware-friendly constraints.",0
"The use of neural network quantization allows models to be deployed on edge devices. To ensure hardware efficiency, quantizers must be uniform, symmetric, and have power-of-two thresholds. However, current post-training quantization methods do not fulfill all of these requirements simultaneously. To solve this issue, we have developed a hardware-friendly post training quantization (HPTQ) framework, which combines several quantization methods. Our study examines four tasks across various network architectures: classification, object detection, semantic segmentation, and pose estimation. Our results demonstrate that it is possible to achieve competitive results while adhering to hardware-friendly constraints.",1
"The de-facto approach to many vision tasks is to start from pretrained visual representations, typically learned via supervised training on ImageNet. Recent methods have explored unsupervised pretraining to scale to vast quantities of unlabeled images. In contrast, we aim to learn high-quality visual representations from fewer images. To this end, we revisit supervised pretraining, and seek data-efficient alternatives to classification-based pretraining. We propose VirTex -- a pretraining approach using semantically dense captions to learn visual representations. We train convolutional networks from scratch on COCO Captions, and transfer them to downstream recognition tasks including image classification, object detection, and instance segmentation. On all tasks, VirTex yields features that match or exceed those learned on ImageNet -- supervised or unsupervised -- despite using up to ten times fewer images.",0
"The prevalent method for many vision tasks involves utilizing pretrained visual representations, which are usually acquired through supervised training on ImageNet. Recent techniques have experimented with unsupervised pretraining to scale up to vast amounts of unlabeled images. However, our objective is to develop high-quality visual representations using fewer images. Therefore, we revisit supervised pretraining and explore data-efficient alternatives to classification-based pretraining. Our solution, VirTex, employs semantically dense captions to learn visual representations through pretraining. We train convolutional networks from the start on COCO Captions and then transfer them to downstream recognition tasks, including object detection, instance segmentation, and image classification. Despite using up to ten times fewer images, VirTex produces features that are equivalent to or better than those learned through supervised or unsupervised training on ImageNet in all tasks.",1
"Machine learning models commonly exhibit unexpected failures post-deployment due to either data shifts or uncommon situations in the training environment. Domain experts typically go through the tedious process of inspecting the failure cases manually, identifying failure modes and then attempting to fix the model. In this work, we aim to standardise and bring principles to this process through answering two critical questions: (i) how do we know that we have identified meaningful and distinct failure types?; (ii) how can we validate that a model has, indeed, been repaired? We suggest that the quality of the identified failure types can be validated through measuring the intra- and inter-type generalisation after fine-tuning and introduce metrics to compare different subtyping methods. Furthermore, we argue that a model can be considered repaired if it achieves high accuracy on the failure types while retaining performance on the previously correct data. We combine these two ideas into a principled framework for evaluating the quality of both the identified failure subtypes and model repairment. We evaluate its utility on a classification and an object detection tasks. Our code is available at https://github.com/Rokken-lab6/Failure-Analysis-and-Model-Repairment",0
"After deployment, machine learning models can fail unexpectedly due to data shifts or rare circumstances in the training environment. Typically, domain experts must manually inspect failure cases, identify failure modes, and attempt to repair the model. Our work aims to standardize and establish principles for this process by addressing two critical questions: (i) how can we determine if we have identified meaningful and distinct failure types?; (ii) how can we validate that a model has been successfully repaired? We propose that the quality of identified failure types can be assessed by measuring intra- and inter-type generalization after fine-tuning, and we introduce metrics to compare subtyping methods. Additionally, we argue that a model is considered repaired if it achieves high accuracy on failure types while maintaining performance on previously correct data. We combine these concepts into a framework for evaluating the quality of both failure subtypes and model repairment, which we test on classification and object detection tasks. Our code is available at https://github.com/Rokken-lab6/Failure-Analysis-and-Model-Repairment.",1
"Within the context of autonomous driving, safety-related metrics for deep neural networks have been widely studied for image classification and object detection. In this paper, we further consider safety-aware correctness and robustness metrics specialized for semantic segmentation. The novelty of our proposal is to move beyond pixel-level metrics: Given two images with each having N pixels being class-flipped, the designed metrics should, depending on the clustering of pixels being class-flipped or the location of occurrence, reflect a different level of safety criticality. The result evaluated on an autonomous driving dataset demonstrates the validity and practicality of our proposed methodology.",0
"The topic of safety-related measurements for deep neural networks in autonomous driving has been extensively explored for image classification and object detection. This article takes it a step further by examining safety-conscious correctness and resilience measurements that are tailored to semantic segmentation. The originality of our concept lies in going beyond pixel-level metrics. The proposed metrics should demonstrate varying degrees of safety criticality based on the clustering of pixels flipped or their occurrence location when two images each with N pixels are subject to class-flipping. An autonomous driving dataset was used to evaluate the effectiveness and feasibility of our methodology, with promising outcomes.",1
"The architecture of deep convolutional networks (CNNs) has evolved for years, becoming more accurate and faster. However, it is still challenging to design reasonable network structures that aim at obtaining the best accuracy under a limited computational budget. In this paper, we propose a Tree block, named after its appearance, which extends the One-Shot Aggregation (OSA) module while being more lightweight and flexible. Specifically, the Tree block replaces each of the $3\times3$ Conv layers in OSA into a stack of shallow residual block (SRB) and $1\times1$ Conv layer. The $1\times1$ Conv layer is responsible for dimension increasing and the SRB is fed into the next step. By doing this, when aggregating the same number of subsequent feature maps, the Tree block has a deeper network structure while having less model complexity. In addition, residual connection and efficient channel attention(ECA) is added to the Tree block to further improve the performance of the network. Based on the Tree block, we build efficient backbone models calling TreeNets. TreeNet has a similar network architecture to ResNet, making it flexible to replace ResNet in various computer vision frameworks. We comprehensively evaluate TreeNet on common-used benchmarks, including ImageNet-1k for classification, MS COCO for object detection, and instance segmentation. Experimental results demonstrate that TreeNet is more efficient and performs favorably against the current state-of-the-art backbone methods.",0
"For years, the architecture of deep convolutional networks (CNNs) has been improving in terms of accuracy and speed. However, creating network structures that achieve the best accuracy while operating within limited computational resources remains challenging. This paper introduces the Tree block, which is a lightweight and flexible extension of the One-Shot Aggregation (OSA) module. The Tree block replaces each $3\times3$ Conv layer in OSA with a stack of shallow residual blocks (SRBs) and a $1\times1$ Conv layer. The $1\times1$ Conv layer increases dimensionality while the SRB is used in the next step. This helps the Tree block achieve a deeper network structure while being less complex. Additionally, residual connections and efficient channel attention (ECA) are included in the Tree block to further enhance network performance. Using the Tree block, efficient backbone models called TreeNets are created. TreeNets have similar network architecture as ResNet and can replace ResNet in various computer vision frameworks. Comprehensive evaluations of TreeNet on commonly used benchmarks, including ImageNet-1k for classification, MS COCO for object detection, and instance segmentation, show that TreeNet is more efficient and performs better than current state-of-the-art backbone methods.",1
"In recent years, photogrammetry has been widely used in many areas to create photorealistic 3D virtual data representing the physical environment. The innovation of small unmanned aerial vehicles (sUAVs) has provided additional high-resolution imaging capabilities with low cost for mapping a relatively large area of interest. These cutting-edge technologies have caught the US Army and Navy's attention for the purpose of rapid 3D battlefield reconstruction, virtual training, and simulations. Our previous works have demonstrated the importance of information extraction from the derived photogrammetric data to create semantic-rich virtual environments (Chen et al., 2019). For example, an increase of simulation realism and fidelity was achieved by segmenting and replacing photogrammetric trees with game-ready tree models. In this work, we further investigated the semantic information extraction problem and focused on the ground material segmentation and object detection tasks. The main innovation of this work was that we leveraged both the original 2D images and the derived 3D photogrammetric data to overcome the challenges faced when using each individual data source. For ground material segmentation, we utilized an existing convolutional neural network architecture (i.e., 3DMV) which was originally designed for segmenting RGB-D sensed indoor data. We improved its performance for outdoor photogrammetric data by introducing a depth pooling layer in the architecture to take into consideration the distance between the source images and the reconstructed terrain model. To test the performance of our improved 3DMV, a ground truth ground material database was created using data from the One World Terrain (OWT) data repository. Finally, a workflow for importing the segmented ground materials into a virtual simulation scene was introduced, and visual results are reported in this paper.",0
"In recent times, the use of photogrammetry has become widespread in various fields to produce realistic 3D virtual data that represents physical environments. The advent of small unmanned aerial vehicles (sUAVs) has added to the imaging capabilities with low cost, enabling the mapping of vast areas of interest. These advancements have captured the attention of the US Army and Navy for creating 3D battlefield reconstructions, virtual training, and simulations. Previous research has highlighted the importance of extracting information from photogrammetric data to create environments with semantic-rich details. This study aims to investigate the problem of extracting semantic information and focuses on ground material segmentation and object detection tasks. The study leverages both the original 2D images and derived 3D photogrammetric data to overcome the challenges faced when using individual data sources. For ground material segmentation, an existing convolutional neural network architecture (3DMV) was used, which originally segmented RGB-D sensed indoor data. The performance of the architecture was improved for outdoor photogrammetric data by adding a depth pooling layer that considers the distance between source images and reconstructed terrain models. To test the improved 3DMV's performance, a ground truth ground material database was created using data from the One World Terrain (OWT) data repository. Finally, a workflow for importing segmented ground materials into virtual simulation scenes was introduced, and the study reports visual results.",1
"Autonomous driving requires 3D maps that provide accurate and up-to-date information about semantic landmarks. Due to the wider availability and lower cost of cameras compared with laser scanners, vision-based mapping has attracted much attention from academia and industry. Among the existing solutions, Structure-from-Motion (SfM) technology has proved to be feasible for building 3D maps from crowdsourced data, since it allows unordered images as input. Previous works on SfM have mainly focused on issues related to building 3D point clouds and calculating camera poses, leaving the issues of automatic change detection and localization open.   We propose in this paper an SfM-based solution for automatic map update, with a focus on real-time change detection and localization. Our solution builds on comparison of semantic map data (e.g. types and locations of traffic signs). Through a novel design of the pixel-wise 3D localization algorithm, our system can locate the objects detected from 2D images in a 3D space, utilizing sparse SfM point clouds. Experiments with dashcam videos collected from two urban areas prove that the system is able to locate visible traffic signs in front along the driving direction with a median distance error of 1.52 meters. Moreover, it can detect up to 80\% of the changes with a median distance error of 2.21 meters. The result analysis also shows the potential of significantly improving the system performance in the future by increasing the accuracy of the background technology in use, including in particularly the object detection and point cloud geo-registration algorithms.",0
"The development of autonomous driving technology necessitates the use of 3D maps that are precise and current in their data concerning semantic landmarks. Vision-based mapping using cameras has become increasingly popular as they are more accessible and economical than laser scanners. Structure-from-Motion (SfM) technology has been successful in constructing 3D maps from unordered images and has been the focus of previous research. However, the automatic change detection and localization aspects of SfM technology still require attention. Our research proposes an SfM-based solution for updating maps in real-time that focuses on change detection and localization. Our system utilizes sparse SfM point clouds to locate objects detected from 2D images in a 3D space through a pixel-wise 3D localization algorithm. Our experiments, which involved analyzing dashcam videos from two urban areas, showed that our system could locate visible traffic signs in front of the vehicle with a median distance error of 1.52 meters. Additionally, our system could identify up to 80% of the changes with a median distance error of 2.21 meters. Further improvements to the accuracy of the technology used, including object detection and point cloud geo-registration algorithms, could significantly enhance the system's performance.",1
"We classify the discontinuity of loss in both five-param and eight-param rotated object detection methods as rotation sensitivity error (RSE) which will result in performance degeneration. We introduce a novel modulated rotation loss to alleviate the problem and propose a rotation sensitivity detection network (RSDet) which is consists of an eight-param single-stage rotated object detector and the modulated rotation loss. Our proposed RSDet has several advantages: 1) it reformulates the rotated object detection problem as predicting the corners of objects while most previous methods employ a five-para-based regression method with different measurement units. 2) modulated rotation loss achieves consistent improvement on both five-param and eight-param rotated object detection methods by solving the discontinuity of loss. To further improve the accuracy of our method on objects smaller than 10 pixels, we introduce a novel RSDet++ which is consists of a point-based anchor-free rotated object detector and a modulated rotation loss. Extensive experiments demonstrate the effectiveness of both RSDet and RSDet++, which achieve competitive results on rotated object detection in the challenging benchmarks DOTA1.0, DOTA1.5, and DOTA2.0. We hope the proposed method can provide a new perspective for designing algorithms to solve rotated object detection and pay more attention to tiny objects. The codes and models are available at: https://github.com/yangxue0827/RotationDetection.",0
"The loss discontinuity in both the five-param and eight-param rotated object detection methods is identified as the rotation sensitivity error (RSE), which leads to a decline in performance. To address this issue, we have developed a novel modulated rotation loss and introduced a rotation sensitivity detection network (RSDet). This comprises an eight-param single-stage rotated object detector and the modulated rotation loss. Our proposed RSDet offers several advantages. Firstly, it reformulates the rotated object detection problem by predicting the corners of objects, whereas most previous methods use a five-para-based regression method with different measurement units. Secondly, the modulated rotation loss consistently enhances the performance of both five-param and eight-param rotated object detection methods by addressing the loss discontinuity. To further enhance the accuracy of our approach for objects smaller than 10 pixels, we have introduced RSDet++, which consists of a point-based anchor-free rotated object detector and a modulated rotation loss. Our extensive experiments demonstrate the effectiveness of both RSDet and RSDet++, which deliver competitive results for rotated object detection in the challenging benchmarks DOTA1.0, DOTA1.5, and DOTA2.0. We expect that our proposed method will offer a new perspective for designing algorithms to solve rotated object detection and draw attention to tiny objects. The codes and models are available at: https://github.com/yangxue0827/RotationDetection.",1
"This work investigates the problem of sketch-guided object localization (SGOL), where human sketches are used as queries to conduct the object localization in natural images. In this cross-modal setting, we first contribute with a tough-to-beat baseline that without any specific SGOL training is able to outperform the previous works on a fixed set of classes. The baseline is useful to analyze the performance of SGOL approaches based on available simple yet powerful methods. We advance prior arts by proposing a sketch-conditioned DETR (DEtection TRansformer) architecture which avoids a hard classification and alleviates the domain gap between sketches and images to localize object instances. Although the main goal of SGOL is focused on object detection, we explored its natural extension to sketch-guided instance segmentation. This novel task allows to move towards identifying the objects at pixel level, which is of key importance in several applications. We experimentally demonstrate that our model and its variants significantly advance over previous state-of-the-art results. All training and testing code of our model will be released to facilitate future research{{https://github.com/priba/sgol_wild}}.",0
"The focus of this study is on the issue of sketch-guided object localization (SGOL), which involves using human sketches as queries to locate objects in natural images. The researchers developed a baseline method that surpasses previous works on a fixed set of classes, even without any specific SGOL training. This baseline method is useful for analyzing the performance of SGOL approaches that rely on simple yet powerful methods. The authors then proposed a sketch-conditioned DETR architecture that avoids a hard classification and addresses the domain gap between sketches and images for localizing object instances. The study also explores the extension of SGOL to sketch-guided instance segmentation, which involves identifying objects at the pixel level. The authors demonstrate that their model and its variants significantly outperform previous state-of-the-art results. The researchers plan to release the training and testing code of their model to facilitate future research.",1
"Machine learning has endless applications in the health care industry. White blood cell classification is one of the interesting and promising area of research. The classification of the white blood cells plays an important part in the medical diagnosis. In practise white blood cell classification is performed by the haematologist by taking a small smear of blood and careful examination under the microscope. The current procedures to identify the white blood cell subtype is more time taking and error-prone. The computer aided detection and diagnosis of the white blood cells tend to avoid the human error and reduce the time taken to classify the white blood cells. In the recent years several deep learning approaches have been developed in the context of classification of the white blood cells that are able to identify but are unable to localize the positions of white blood cells in the blood cell image. Following this, the present research proposes to utilize YOLOv3 object detection technique to localize and classify the white blood cells with bounding boxes. With exhaustive experimental analysis, the proposed work is found to detect the white blood cell with 99.2% accuracy and classify with 90% accuracy.",0
"The healthcare industry has numerous potential applications for machine learning, with white blood cell classification being a particularly promising area of research. Accurately classifying white blood cells is crucial for medical diagnoses, but current methods rely on haematologists examining blood smears under a microscope, which can be time-consuming and prone to error. Computer-aided detection and diagnosis using deep learning approaches can help reduce these issues, but most methods are limited to identifying white blood cell subtypes without localizing their positions in the image. To address this limitation, the present research proposes using the YOLOv3 object detection technique to both classify and localize white blood cells using bounding boxes. The proposed method achieved a 99.2% accuracy in detecting white blood cells and 90% accuracy in classification, as demonstrated through exhaustive experimental analysis.",1
"Monocular 3D object detection is an important task for autonomous driving considering its advantage of low cost. It is much more challenging than conventional 2D cases due to its inherent ill-posed property, which is mainly reflected in the lack of depth information. Recent progress on 2D detection offers opportunities to better solving this problem. However, it is non-trivial to make a general adapted 2D detector work in this 3D task. In this paper, we study this problem with a practice built on a fully convolutional single-stage detector and propose a general framework FCOS3D. Specifically, we first transform the commonly defined 7-DoF 3D targets to the image domain and decouple them as 2D and 3D attributes. Then the objects are distributed to different feature levels with consideration of their 2D scales and assigned only according to the projected 3D-center for the training procedure. Furthermore, the center-ness is redefined with a 2D Gaussian distribution based on the 3D-center to fit the 3D target formulation. All of these make this framework simple yet effective, getting rid of any 2D detection or 2D-3D correspondence priors. Our solution achieves 1st place out of all the vision-only methods in the nuScenes 3D detection challenge of NeurIPS 2020. Code and models are released at https://github.com/open-mmlab/mmdetection3d.",0
"Due to its cost-effectiveness, monocular 3D object detection is a crucial aspect of autonomous driving. However, it poses more challenges than conventional 2D detection due to its inherently ill-posed property, which stems from the lack of depth information. Recent advancements in 2D detection have provided opportunities to tackle this problem, but it remains difficult to apply a general 2D detector to this 3D task. This paper proposes a framework called FCOS3D, which utilizes a fully convolutional single-stage detector. The framework transforms 7-DoF 3D targets into the image domain, decoupling them as 2D and 3D attributes. Objects are then distributed to various feature levels based on their 2D scales and are only assigned according to the projected 3D-center during training. The center-ness is redefined with a 2D Gaussian distribution based on the 3D-center to fit the 3D target formulation. This framework simplifies the process and eliminates the need for any 2D detection or 2D-3D correspondence priors. The proposed solution achieved 1st place in the nuScenes 3D detection challenge of NeurIPS 2020, and the code and models are publicly available at https://github.com/open-mmlab/mmdetection3d.",1
"Salient human detection (SHD) in dynamic 360{\deg} immersive videos is of great importance for various applications such as robotics, inter-human and human-object interaction in augmented reality. However, 360{\deg} video SHD has been seldom discussed in the computer vision community due to a lack of datasets with large-scale omnidirectional videos and rich annotations. To this end, we propose SHD360, the first 360{\deg} video SHD dataset which contains various real-life daily scenes. Our SHD360 provides six-level hierarchical annotations for 6,268 key frames uniformly sampled from 37,403 omnidirectional video frames at 4K resolution. Specifically, each collected frame is labeled with a super-class, a sub-class, associated attributes (e.g., geometrical distortion), bounding boxes and per-pixel object-/instance-level masks. As a result, our SHD360 contains totally 16,238 salient human instances with manually annotated pixel-wise ground truth. Since so far there is no method proposed for 360{\deg} image/video SHD, we systematically benchmark 11 representative state-of-the-art salient object detection (SOD) approaches on our SHD360, and explore key issues derived from extensive experimenting results. We hope our proposed dataset and benchmark could serve as a good starting point for advancing human-centric researches towards 360{\deg} panoramic data. Our dataset and benchmark is publicly available at https://github.com/PanoAsh/SHD360.",0
"The detection of humans in dynamic 360{\deg} immersive videos, known as Salient Human Detection (SHD), is essential for a variety of applications, including robotics, augmented reality, and human-object interaction. However, due to a shortage of datasets with large-scale omnidirectional videos and comprehensive annotations, 360{\deg} video SHD has not been extensively studied in the computer vision community. To address this, we introduce SHD360, the first 360{\deg} video SHD dataset that features real-life daily scenes. Our dataset includes 6,268 key frames with six-level hierarchical annotations, such as super-class, sub-class, bounding boxes, and per-pixel object-/instance-level masks. It comprises a total of 16,238 manually annotated salient human instances with pixel-wise ground truth. We systematically evaluate 11 state-of-the-art salient object detection (SOD) approaches on our SHD360, and analyze the main issues arising from extensive experimentation. We hope that our proposed dataset and benchmark will facilitate the advancement of human-centric research in 360{\deg} panoramic data. Our dataset and benchmark are publicly accessible at https://github.com/PanoAsh/SHD360.",1
"In this paper, we propose the first self-distillation framework for general object detection, termed LGD (Label-Guided self-Distillation). Previous studies rely on a strong pretrained teacher to provide instructive knowledge for distillation. However, this could be unavailable in real-world scenarios. Instead, we generate an instructive knowledge by inter-and-intra relation modeling among objects, requiring only student representations and regular labels. In detail, our framework involves sparse label-appearance encoding, inter-object relation adaptation and intra-object knowledge mapping to obtain the instructive knowledge. Modules in LGD are trained end-to-end with student detector and are discarded in inference. Empirically, LGD obtains decent results on various detectors, datasets, and extensive task like instance segmentation. For example in MS-COCO dataset, LGD improves RetinaNet with ResNet-50 under 2x single-scale training from 36.2% to 39.0% mAP (+ 2.8%). For much stronger detectors like FCOS with ResNeXt-101 DCN v2 under 2x multi-scale training (46.1%), LGD achieves 47.9% (+ 1.8%). For pedestrian detection in CrowdHuman dataset, LGD boosts mMR by 2.3% for Faster R-CNN with ResNet-50. Compared with a classical teacher-based method FGFI, LGD not only performs better without requiring pretrained teacher but also with 51% lower training cost beyond inherent student learning.",0
"The LGD (Label-Guided self-Distillation) framework proposed in this paper is the first self-distillation model for general object detection. Unlike previous studies that rely on a strong pretrained teacher for instructive knowledge, LGD generates instructive knowledge through inter-and-intra relation modeling among objects, using only student representations and regular labels. The framework involves sparse label-appearance encoding, inter-object relation adaptation, and intra-object knowledge mapping to obtain the instructive knowledge. LGD modules are trained end-to-end with the student detector and are not used in inference. Empirically, LGD achieves decent results on various detectors, datasets, and extensive tasks like instance segmentation. For instance, on the MS-COCO dataset, LGD improves RetinaNet with ResNet-50 under 2x single-scale training from 36.2% to 39.0% mAP (+2.8%). For stronger detectors like FCOS with ResNeXt-101 DCN v2 under 2x multi-scale training (46.1%), LGD achieves 47.9% (+1.8%). In pedestrian detection in the CrowdHuman dataset, LGD boosts mMR by 2.3% for Faster R-CNN with ResNet-50. Compared to the classical teacher-based method FGFI, LGD performs better without requiring a pretrained teacher and with 51% lower training cost beyond inherent student learning.",1
"Embedding Artificial Intelligence onto low-power devices is a challenging task that has been partly overcome with recent advances in machine learning and hardware design. Presently, deep neural networks can be deployed on embedded targets to perform different tasks such as speech recognition,object detection or Human Activity Recognition. However, there is still room for optimization of deep neural networks onto embedded devices. These optimizations mainly address power consumption,memory and real-time constraints, but also an easier deployment at the edge. Moreover, there is still a need for a better understanding of what can be achieved for different use cases. This work focuses on quantization and deployment of deep neural networks onto low-power 32-bit microcontrollers. The quantization methods, relevant in the context of an embedded execution onto a microcontroller, are first outlined. Then, a new framework for end-to-end deep neural networks training, quantization and deployment is presented. This framework, called MicroAI, is designed as an alternative to existing inference engines (TensorFlow Lite for Microcontrollers and STM32CubeAI). Our framework can indeed be easily adjusted and/or extended for specific use cases. Execution using single precision 32-bit floating-point as well as fixed-point on 8- and 16-bit integers are supported. The proposed quantization method is evaluated with three different datasets (UCI-HAR, Spoken MNIST and GTSRB). Finally, a comparison study between MicroAI and both existing embedded inference engines is provided in terms of memory and power efficiency. On-device evaluation is done using ARM Cortex-M4F-based microcontrollers (Ambiq Apollo3 and STM32L452RE).",0
"The challenge of integrating Artificial Intelligence into low-power devices has been partially resolved thanks to recent advancements in machine learning and hardware design. Deep neural networks are now able to perform various tasks on embedded targets, such as Human Activity Recognition, object detection, and speech recognition. However, there is still a need for optimization of deep neural networks on embedded devices, particularly with regards to power consumption, memory constraints, and real-time limitations, as well as easier deployment at the edge. Additionally, further research is necessary to fully comprehend the potential for different use cases. This study concentrates on the quantization and deployment of deep neural networks on low-power 32-bit microcontrollers. It outlines the pertinent quantization methods required for embedded execution on a microcontroller, followed by the presentation of a new framework called MicroAI for deep neural network training, quantization, and deployment. Our framework serves as an alternative to existing inference engines, such as TensorFlow Lite for Microcontrollers and STM32CubeAI, and can be customized to suit specific use cases. It supports execution using single precision 32-bit floating-point as well as fixed-point on 8- and 16-bit integers. The proposed quantization method is evaluated utilizing three distinct datasets (UCI-HAR, Spoken MNIST, and GTSRB). Finally, a comparative analysis of MicroAI and both existing embedded inference engines is conducted in terms of memory and power efficiency. On-device evaluation is performed using ARM Cortex-M4F-based microcontrollers (Ambiq Apollo3 and STM32L452RE).",1
"Real-world object detection is highly desired to be equipped with the learning expandability that can enlarge its detection classes incrementally. Moreover, such learning from only few annotated training samples further adds the flexibility for the object detector, which is highly expected in many applications such as autonomous driving, robotics, etc. However, such sequential learning scenario with few-shot training samples generally causes catastrophic forgetting and dramatic overfitting. In this paper, to address the above incremental few-shot learning issues, a novel Incremental Few-Shot Object Detection (iFSOD) method is proposed to enable the effective continual learning from few-shot samples. Specifically, a Double-Branch Framework (DBF) is proposed to decouple the feature representation of base and novel (few-shot) class, which facilitates both the old-knowledge retention and new-class adaption simultaneously. Furthermore, a progressive model updating rule is carried out to preserve the long-term memory on old classes effectively when adapt to sequential new classes. Moreover, an inter-task class separation loss is proposed to extend the decision region of new-coming classes for better feature discrimination. We conduct experiments on both Pascal VOC and MS-COCO, which demonstrate that our method can effectively solve the problem of incremental few-shot detection and significantly improve the detection accuracy on both base and novel classes.",0
"The ability to expand the detection classes of real-world object detection is highly desired, along with the capability to learn from only a few annotated training samples. This flexibility is essential in applications such as robotics and autonomous driving. However, the sequential learning scenario with few-shot training samples often results in overfitting and catastrophic forgetting. To address these issues, the proposed Incremental Few-Shot Object Detection (iFSOD) method enables effective continual learning from few-shot samples. The Double-Branch Framework (DBF) decouples the feature representation of base and novel classes to facilitate both the retention of old knowledge and adaptation to new classes. A progressive model updating rule preserves long-term memory on old classes while adapting to sequential new classes. Additionally, an inter-task class separation loss extends the decision region of new classes for better feature discrimination. Experiments on Pascal VOC and MS-COCO demonstrate that the proposed method effectively solves the problem of incremental few-shot detection and significantly improves detection accuracy on both base and novel classes.",1
"Recently, three-dimensional (3D) detection based on stereo images has progressed remarkably; however, most advanced methods adopt anchor-based two-dimensional (2D) detection or depth estimation to address this problem. Nevertheless, high computational cost inhibits these methods from achieving real-time performance. In this study, we propose a 3D object detection method, Stereo CenterNet (SC), using geometric information in stereo imagery. SC predicts the four semantic key points of the 3D bounding box of the object in space and utilizes 2D left and right boxes, 3D dimension, orientation, and key points to restore the bounding box of the object in the 3D space. Subsequently, we adopt an improved photometric alignment module to further optimize the position of the 3D bounding box. Experiments conducted on the KITTI dataset indicate that the proposed SC exhibits the best speed-accuracy trade-off among advanced methods without using extra data.",0
"The detection of three-dimensional (3D) objects using stereo images has shown significant progress lately. However, most advanced methods rely on anchor-based two-dimensional (2D) detection or depth estimation to tackle this issue. Unfortunately, the high computational cost associated with these methods hinders their real-time performance. This research presents a new 3D object detection approach called Stereo CenterNet (SC), which utilizes geometric information from stereo imagery. SC predicts four semantic key points of the 3D bounding box to restore the object's position in 3D space. Additionally, an improved photometric alignment module is adopted to enhance the accuracy of the 3D bounding box position. The experiments carried out on the KITTI dataset demonstrate that SC achieves the optimal balance between speed and accuracy among advanced methods, without requiring additional data.",1
"The evolution of smaller, faster processors and cheaper digital storage mechanisms across the last 4-5 decades has vastly increased the opportunity to integrate intelligent technologies in a wide range of practical environments to address a broad spectrum of tasks. One exciting application domain for such technologies is precision agriculture, where the ability to integrate on-board machine vision with data-driven actuation means that farmers can make decisions about crop care and harvesting at the level of the individual plant rather than the whole field. This makes sense both economically and environmentally. However, the key driver for this capability is fast and robust machine vision -- typically driven by machine learning (ML) solutions and dependent on accurate modelling. One critical challenge is that the bulk of ML-based vision research considers only metrics that evaluate the accuracy of object detection and do not assess practical factors. This paper introduces three metrics that highlight different aspects relevant for real-world deployment of precision weeding and demonstrates their utility through experimental results.",0
"Over the past 4-5 decades, the advancement of smaller, faster processors and cost-effective digital storage mechanisms has created numerous opportunities to incorporate intelligent technologies in practical settings to tackle a variety of tasks. Precision agriculture is one such domain where the integration of on-board machine vision and data-driven actuation enables farmers to make crop care and harvesting decisions at a plant level, which is both economically and environmentally sound. However, the primary driving force behind this capability is fast and reliable machine vision, which is largely dependent on accurate modelling through machine learning (ML) solutions. Despite this, most ML-based vision research only focuses on metrics that evaluate object detection accuracy, neglecting practical considerations. This study introduces three metrics that highlight different critical aspects concerning real-world deployment of precision weeding and showcases their effectiveness through experimental results.",1
"6D pose estimation is the task of predicting the translation and orientation of objects in a given input image, which is a crucial prerequisite for many robotics and augmented reality applications. Lately, the Transformer Network architecture, equipped with a multi-head self-attention mechanism, is emerging to achieve state-of-the-art results in many computer vision tasks. DETR, a Transformer-based model, formulated object detection as a set prediction problem and achieved impressive results without standard components like region of interest pooling, non-maximal suppression, and bounding box proposals. In this work, we propose T6D-Direct, a real-time single-stage direct method with a transformer-based architecture built on DETR to perform 6D multi-object pose direct estimation. We evaluate the performance of our method on the YCB-Video dataset. Our method achieves the fastest inference time, and the pose estimation accuracy is comparable to state-of-the-art methods.",0
"The crucial task of 6D pose estimation involves predicting the orientation and translation of objects in an input image, which is necessary for various applications, including robotics and augmented reality. Recently, the Transformer Network architecture, featuring a multi-head self-attention mechanism, has been gaining popularity for achieving exceptional results in computer vision tasks. DETR is a Transformer-based model that has accomplished impressive object detection results without using standard components such as region of interest pooling, non-maximal suppression, and bounding box proposals. Our work introduces T6D-Direct, a real-time single-stage direct method with a transformer-based architecture based on DETR, designed to perform 6D multi-object pose direct estimation. We tested our method on the YCB-Video dataset and observed that our method has the fastest inference time while maintaining pose estimation accuracy comparable to state-of-the-art methods.",1
"This paper presents Pix2Seq, a simple and generic framework for object detection. Unlike existing approaches that explicitly integrate prior knowledge about the task, we simply cast object detection as a language modeling task conditioned on the observed pixel inputs. Object descriptions (e.g., bounding boxes and class labels) are expressed as sequences of discrete tokens, and we train a neural net to perceive the image and generate the desired sequence. Our approach is based mainly on the intuition that if a neural net knows about where and what the objects are, we just need to teach it how to read them out. Beyond the use of task-specific data augmentations, our approach makes minimal assumptions about the task, yet it achieves competitive results on the challenging COCO dataset, compared to highly specialized and well optimized detection algorithms.",0
"The article introduces Pix2Seq, a straightforward and versatile framework for object detection. Instead of incorporating prior knowledge of the task, we treat object detection as a language modeling task based on the pixel inputs. Object descriptions, such as bounding boxes and class labels, are represented as sequences of discrete tokens, and we utilize a neural network to interpret the image and produce the desired sequence. Our method is based on the belief that if a neural network understands the location and identity of objects, it only needs to be taught how to extract this information. Our approach requires minimal assumptions about the task, and with the aid of task-specific data augmentations, it competes with highly specialized and well-optimized detection algorithms on the challenging COCO dataset.",1
"Every year, the Aedes aegypti mosquito infects millions of people with diseases such as dengue, zika, chikungunya, and urban yellow fever. The main form to combat these diseases is to avoid mosquito reproduction by searching for and eliminating the potential mosquito breeding grounds. In this work, we introduce a comprehensive dataset of aerial videos, acquired with an unmanned aerial vehicle, containing possible mosquito breeding sites. All frames of the video dataset were manually annotated with bounding boxes identifying all objects of interest. This dataset was employed to develop an automatic detection system of such objects based on deep convolutional networks. We propose the exploitation of the temporal information contained in the videos by the incorporation, in the object detection pipeline, of a spatio-temporal consistency module that can register the detected objects, minimizing most false-positive and false-negative occurrences. Using the ResNet-50-FPN as a backbone, we achieve F$_1$-scores of 0.65 and 0.77 on the object-level detection of `tires' and `water tanks', respectively, illustrating the system capabilities to properly locate potential mosquito breeding objects.",0
"Millions of individuals are infected with illnesses like dengue, zika, chikungunya, and urban yellow fever by the Aedes aegypti mosquito each year. The most effective method of preventing these diseases is to locate and eliminate potential breeding sites for mosquitoes. In this study, we present a comprehensive dataset of aerial videos obtained using an unmanned aerial vehicle that contains possible breeding grounds for mosquitoes. All frames in the dataset were manually marked with bounding boxes that identify all objects of interest. We developed an automated detection system for these objects using deep convolutional networks and incorporated a spatio-temporal consistency module in the object detection pipeline to take advantage of the temporal information in the videos and minimize the occurrence of false-positives and false-negatives. Our use of ResNet-50-FPN as a backbone enabled us to obtain F$_1$-scores of 0.65 and 0.77 for object-level detection of ""tires"" and ""water tanks,"" respectively, demonstrating the system's ability to accurately locate potential mosquito breeding sites.",1
"Object detection is a fundamental task in computer vision. While approaches for axis-aligned bounding box detection have made substantial progress in recent years, they perform poorly on oriented objects which are common in several real-world scenarios such as aerial view imagery and security camera footage. In these cases, a large part of a predicted bounding box will, undesirably, cover non-object related areas. Therefore, oriented object detection has emerged with the aim of generalizing object detection to arbitrary orientations. This enables a tighter fit to oriented objects, leading to a better separation of bounding boxes especially in case of dense object distributions. The vast majority of the work in this area has focused on complex two-stage anchor-based approaches. Anchors act as priors on the bounding box shape and require attentive hyper-parameter fine-tuning on a per-dataset basis, increased model size, and come with computational overhead. In this work, we present DAFNe: A Dense one-stage Anchor-Free deep Network for oriented object detection. As a one-stage model, DAFNe performs predictions on a dense grid over the input image, being architecturally simpler and faster, as well as easier to optimize than its two-stage counterparts. Furthermore, as an anchor-free model, DAFNe reduces the prediction complexity by refraining from employing bounding box anchors. Moreover, we introduce an orientation-aware generalization of the center-ness function for arbitrarily oriented bounding boxes to down-weight low-quality predictions and a center-to-corner bounding box prediction strategy that improves object localization performance. DAFNe improves the prediction accuracy over the previous best one-stage anchor-free model results on DOTA 1.0 by 4.65% mAP, setting the new state-of-the-art results by achieving 76.95% mAP.",0
"Computer vision relies heavily on object detection, which has driven the development of axis-aligned bounding box detection techniques. Despite significant progress in this area, these methods tend to underperform when it comes to oriented objects that are more common in real-world scenarios, such as aerial views and security camera footage. Predicted bounding boxes often extend beyond the object and cover non-object related areas, which is undesirable. To address this issue, oriented object detection has been introduced to generalize object detection to arbitrary orientations, resulting in more precise fits to oriented objects and better separation of bounding boxes, especially in dense object distributions. However, most of the existing approaches rely on complex two-stage anchor-based techniques that involve priors on the bounding box shape and require attentive hyper-parameter fine-tuning, increased model size, and computational overhead. In contrast, DAFNe is a dense one-stage anchor-free deep network that performs predictions on a dense grid over the input image, making it simpler, faster, easier to optimize, and more accurate than previous approaches. It also introduces an orientation-aware generalization of the center-ness function for bounding boxes and a center-to-corner bounding box prediction strategy that further improves object localization performance. DAFNe outperforms previous state-of-the-art results by achieving 76.95% mAP on DOTA 1.0, a 4.65% improvement over the previous best one-stage anchor-free model.",1
"In this paper, we present an efficient and high-performance neural architecture, termed Point-Voxel Transformer (PVT)for 3D deep learning, which deeply integrates both 3D voxel-based and point-based self-attention computation to learn more discriminative features from 3D data. Specifically, we conduct multi-head self-attention (MSA) computation in voxels to obtain the efficient learning pattern and the coarse-grained local features while performing self-attention in points to provide finer-grained information about the global context. In addition, to reduce the cost of MSA computation with high efficiency, we design a cyclic shifted boxing scheme by limiting the MSA computation to non-overlapping local box and also preserving cross-box connection. Evaluated on classification benchmark, our method not only achieves state-of-the-art accuracy of 94.0% (no voting) but outperforms previous Transformer-based models with 7x measured speedup on average. On part and semantic segmentation, our model also obtains strong performance(86.5% and 68.2% mIoU, respectively). For 3D object detection task, we replace the primitives in Frustrum PointNet with PVT block and achieve an improvement of 8.6% AP.",0
"This paper presents the Point-Voxel Transformer (PVT), a neural architecture for 3D deep learning that combines 3D voxel-based and point-based self-attention computation for more effective feature learning. The approach utilizes multi-head self-attention (MSA) computation in voxels to learn efficient patterns and coarse-grained local features, while performing self-attention in points to capture finer-grained information about the global context. To optimize MSA computation, the authors designed a cyclic shifted boxing scheme that limits computation to non-overlapping local boxes while preserving cross-box connections. Results show that the PVT model achieves state-of-the-art accuracy of 94.0% (no voting) on classification benchmark, outperforming Transformer-based models with 7x measured speedup on average. The model also demonstrates strong performance on part and semantic segmentation, with 86.5% and 68.2% mIoU, respectively. In 3D object detection, replacing Frustrum PointNet primitives with PVT block yields an 8.6% AP improvement.",1
"Monocular 3D object detection encounters occlusion problems in many application scenarios, such as traffic monitoring, pedestrian monitoring, etc., which leads to serious false negative. Multi-view object detection effectively solves this problem by combining data from different perspectives. However, due to label confusion and feature confusion, the orientation estimation of multi-view 3D object detection is intractable, which is important for object tracking and intention prediction. In this paper, we propose a novel multi-view 3D object detection method named MVM3Det which simultaneously estimates the 3D position and orientation of the object according to the multi-view monocular information. The method consists of two parts: 1) Position proposal network, which integrates the features from different perspectives into consistent global features through feature orthogonal transformation to estimate the position. 2) Multi-branch orientation estimation network, which introduces feature perspective pooling to overcome the two confusion problems during the orientation estimation. In addition, we present a first dataset for multi-view 3D object detection named MVM3D. Comparing with State-Of-The-Art (SOTA) methods on our dataset and public dataset WildTrack, our method achieves very competitive results.",0
"In various situations like traffic monitoring, pedestrian monitoring, and others, monocular 3D object detection is challenged by occlusion, leading to significant false negatives. Multi-view object detection addresses this issue by integrating data from different perspectives. However, it remains difficult to estimate orientation accurately due to confusion in labeling and features, which is crucial for object tracking and intention prediction. To overcome this, we propose a new method called MVM3Det that simultaneously estimates the 3D position and orientation of the object based on multi-view monocular data. The approach has two components: 1) A position proposal network that utilizes feature orthogonal transformation to generate consistent global features from different views and estimate position, and 2) A multi-branch orientation estimation network that introduces feature perspective pooling to overcome confusion problems during orientation estimation. Furthermore, we introduce MVM3D, the first dataset for multi-view 3D object detection. Our method achieves highly competitive results compared to State-Of-The-Art (SOTA) approaches on both our dataset and the public dataset WildTrack.",1
"Facial analysis is an active research area in computer vision, with many practical applications. Most of the existing studies focus on addressing one specific task and maximizing its performance. For a complete facial analysis system, one needs to solve these tasks efficiently to ensure a smooth experience. In this work, we present a system-level design of a real-time facial analysis system. With a collection of deep neural networks for object detection, classification, and regression, the system recognizes age, gender, facial expression, and facial similarity for each person that appears in the camera view. We investigate the parallelization and interplay of individual tasks. Results on common off-the-shelf architecture show that the system's accuracy is comparable to the state-of-the-art methods, and the recognition speed satisfies real-time requirements. Moreover, we propose a multitask network for jointly predicting the first three attributes, i.e., age, gender, and facial expression. Source code and trained models are available at https://github.com/mahehu/TUT-live-age-estimator.",0
"Computer vision research is actively exploring facial analysis, which has numerous practical applications. Existing studies concentrate on enhancing a single task's performance, but a comprehensive facial analysis system must efficiently address all tasks. This paper presents a system-level design for a real-time facial analysis system that recognizes age, gender, facial expression, and similarity using deep neural networks for object detection, classification, and regression. The study examines the interaction and parallelization of individual tasks, and the system achieves accuracy comparable to state-of-the-art techniques and meets real-time requirements. Additionally, the researchers propose a multitask network for predicting age, gender, and facial expression simultaneously. The source code and trained models are available at https://github.com/mahehu/TUT-live-age-estimator.",1
"3D object detection is an important capability needed in various practical applications such as driver assistance systems. Monocular 3D detection, as a representative general setting among image-based approaches, provides a more economical solution than conventional settings relying on LiDARs. It has drawn increasing attention recently but still yields unsatisfactory results. This paper first presents a systematic study on this problem. We observe that the current monocular 3D detection can be simplified as an instance depth estimation problem: The inaccurate instance depth blocks all the other 3D attribute predictions from improving the overall detection performance. However, recent methods directly estimate the depth based on isolated instances or pixels while ignoring the geometric relations across different objects. These geometric relations can be valuable constraints as the key information about depth is not directly manifest in the monocular image. Therefore, we construct geometric relation graphs across predicted objects and use the graph to facilitate depth estimation. As the preliminary depth estimation of each instance is usually inaccurate in this ill-posed setting, we incorporate a probabilistic representation to capture the uncertainty. It provides an important indicator to identify confident predictions and further guide the depth propagation. Despite the simplicity of the basic idea, our method obtains significant improvements on KITTI and nuScenes benchmarks, achieving 1st place out of all monocular vision-only methods while still maintaining real-time efficiency. Code and models will be released at https://github.com/open-mmlab/mmdetection3d.",0
"In various practical applications such as driver assistance systems, the capability of 3D object detection is crucial. Monocular 3D detection is a more cost-effective solution than traditional LiDAR-based approaches and has received increasing attention. However, it still produces unsatisfactory results. This paper presents a systematic study on the problem and finds that inaccurate instance depth is the main obstacle to improving overall detection performance. Recent methods ignore geometric relations among objects when estimating depth, which can be a valuable constraint. To address this, we construct geometric relation graphs and use them to facilitate depth estimation. As the preliminary depth estimation for each instance is often inaccurate, we incorporate a probabilistic representation to capture uncertainty. Our method achieves significant improvements on KITTI and nuScenes benchmarks and ranks 1st among all monocular vision-only methods while maintaining real-time efficiency. Code and models are available at https://github.com/open-mmlab/mmdetection3d.",1
"Self-supervised monocular depth prediction provides a cost-effective solution to obtain the 3D location of each pixel. However, the existing approaches usually lead to unsatisfactory accuracy, which is critical for autonomous robots. In this paper, we propose a novel two-stage network to advance the self-supervised monocular dense depth learning by leveraging low-cost sparse (e.g. 4-beam) LiDAR. Unlike the existing methods that use sparse LiDAR mainly in a manner of time-consuming iterative post-processing, our model fuses monocular image features and sparse LiDAR features to predict initial depth maps. Then, an efficient feed-forward refine network is further designed to correct the errors in these initial depth maps in pseudo-3D space with real-time performance. Extensive experiments show that our proposed model significantly outperforms all the state-of-the-art self-supervised methods, as well as the sparse-LiDAR-based methods on both self-supervised monocular depth prediction and completion tasks. With the accurate dense depth prediction, our model outperforms the state-of-the-art sparse-LiDAR-based method (Pseudo-LiDAR++) by more than 68% for the downstream task monocular 3D object detection on the KITTI Leaderboard.",0
"The use of self-supervised monocular depth prediction is a cost-effective way to determine the 3D location of pixels. However, current approaches often result in unsatisfactory accuracy, which is crucial for autonomous robots. This study proposes a two-stage network that leverages low-cost sparse LiDAR (e.g., 4-beam) to improve self-supervised monocular dense depth learning. Unlike existing methods that use sparse LiDAR iteratively for post-processing, the proposed model combines monocular image features and sparse LiDAR features to predict initial depth maps. An efficient feed-forward refine network then corrects errors in the initial depth maps in pseudo-3D space with real-time performance. Extensive experiments demonstrate that this model outperforms state-of-the-art self-supervised and sparse-LiDAR-based methods for both depth prediction and completion tasks. The proposed model also outperforms Pseudo-LiDAR++ by more than 68% for monocular 3D object detection on the KITTI Leaderboard.",1
"Rotated object detection is a challenging task in aerial images as the object in aerial images are displayed in arbitrary directions and usually densely packed. Although considerable progress has been made, there are still challenges that existing regression-based rotation detectors suffer the problem of discontinuous boundaries, which is directly caused by angular periodicity or corner ordering. In this paper, we propose a simple effective framework to address the above challenges. Instead of directly regressing the five parameters (coordinates of the central point, width, height, and rotation angle) or the four vertices, we use the area ratio of parallelogram (ARP) to accurately describe a multi-oriented object. Specifically, we regress coordinates of center point, height and width of minimum circumscribed rectangle of oriented object and three area ratios {\lambda}_1, {\lambda}_2 and {\lambda}_3. This may facilitate the offset learning and avoid the issue of angular periodicity or label points sequence for oriented objects. To further remedy the confusion issue nearly horizontal objects, we employ the area ratio between the object and its horizontal bounding box (minimum circumscribed rectangle) to guide the selection of horizontal or oriented detection for each object. We also propose a rotation efficient IoU loss (R-EIoU) to connect the horizontal bounding box with the three area ratios and improve the accurate for the rotating bounding box. Experimental results on three remote sensing datasets including HRSC2016, DOTA and UCAS-AOD and scene text including ICDAR2015 show that our method achieves superior detection performance compared with many state-of-the-art approaches. The code and model will be coming with paper published.",0
"Detecting rotated objects in aerial images is a difficult task due to the arbitrary directions and dense packing of objects. While regression-based rotation detectors have made progress, they still struggle with discontinuous boundaries caused by angular periodicity or corner ordering. This paper introduces a simple and effective framework to overcome these challenges. Instead of regressing five parameters or four vertices, we use the area ratio of parallelogram (ARP) to accurately describe multi-oriented objects. We regress the center point, height, and width of the minimum circumscribed rectangle of the oriented object, as well as three area ratios. This approach facilitates offset learning and avoids issues with angular periodicity or label points sequence. Additionally, we use the area ratio between an object and its horizontal bounding box to guide the selection of horizontal or oriented detection. We propose a rotation efficient IoU loss (R-EIoU) to improve the accuracy of the rotating bounding box. Our method outperforms many state-of-the-art approaches on three remote sensing datasets and scene text. The code and model will be available upon paper publication.",1
"Modern neural networks have found to be miscalibrated in terms of confidence calibration, i.e., their predicted confidence scores do not reflect the observed accuracy or precision. Recent work has introduced methods for post-hoc confidence calibration for classification as well as for object detection to address this issue. Especially in safety critical applications, it is crucial to obtain a reliable self-assessment of a model. But what if the calibration method itself is uncertain, e.g., due to an insufficient knowledge base?   We introduce Bayesian confidence calibration - a framework to obtain calibrated confidence estimates in conjunction with an uncertainty of the calibration method. Commonly, Bayesian neural networks (BNN) are used to indicate a network's uncertainty about a certain prediction. BNNs are interpreted as neural networks that use distributions instead of weights for inference. We transfer this idea of using distributions to confidence calibration. For this purpose, we use stochastic variational inference to build a calibration mapping that outputs a probability distribution rather than a single calibrated estimate. Using this approach, we achieve state-of-the-art calibration performance for object detection calibration. Finally, we show that this additional type of uncertainty can be used as a sufficient criterion for covariate shift detection. All code is open source and available at https://github.com/EFS-OpenSource/calibration-framework.",0
"The confidence calibration of modern neural networks has been discovered to be inaccurate, meaning that the confidence scores they predict are not a true reflection of their accuracy or precision. To address this issue, recent work has introduced methods for post-hoc confidence calibration for classification and object detection. Obtaining a reliable self-assessment of a model is especially important in safety critical applications. However, if the calibration method itself is uncertain due to insufficient knowledge, this can be problematic. To tackle this, we have developed a framework called Bayesian confidence calibration, which obtains calibrated confidence estimates along with an uncertainty of the calibration method. Bayesian neural networks are commonly used to indicate a network's uncertainty about a specific prediction, and we have employed this concept to confidence calibration by using stochastic variational inference to build a calibration mapping that outputs a probability distribution instead of a single calibrated estimate. This approach has resulted in state-of-the-art calibration performance for object detection calibration. Additionally, we have demonstrated that this type of uncertainty can be used as a sufficient criterion for covariate shift detection. Our code is open source and available on https://github.com/EFS-OpenSource/calibration-framework.",1
"Detecting the reflection symmetry plane of an object represented by a 3D point cloud is a fundamental problem in 3D computer vision and geometry processing due to its various applications such as compression, object detection, robotic grasping, 3D surface reconstruction, etc. There exist several efficient approaches for solving this problem for clean 3D point clouds. However, this problem becomes difficult to solve in the presence of outliers and missing parts due to occlusions while scanning the objects through 3D scanners. The existing methods try to overcome these challenges mostly by voting-based techniques but fail in challenging settings. In this work, we propose a statistical estimator for the plane of reflection symmetry that is robust to outliers and missing parts. We pose the problem of finding the optimal estimator as an optimization problem on a 2-sphere that quickly converges to the global solution. We further propose a 3D point descriptor that is invariant to 3D reflection symmetry using the spectral properties of the geodesic distance matrix constructed from the neighbors of a point. This helps us in decoupling the chicken-and-egg problem of finding optimal symmetry plane and correspondences between the reflective symmetric points. We show that the proposed approach achieves the state-of-the-art performance on the benchmarks dataset.",0
"The discovery of the reflection symmetry plane of a 3D point cloud object is an essential task in the fields of 3D computer vision and geometry processing, as it has numerous applications, such as compression, object detection, robotic grasping, and 3D surface reconstruction. Several effective methods exist for solving this problem for clean 3D point clouds. However, the presence of outliers and missing parts due to occlusions during the scanning process makes this problem harder to solve. Most of the existing methods use voting-based techniques to overcome these challenges, but they are not successful in challenging environments. In this study, we suggest a statistical estimator for the plane of reflection symmetry that is resilient to outliers and missing parts. We pose the problem of discovering the optimal estimator as an optimization problem on a 2-sphere, which leads to a fast convergence to the global solution. Additionally, we offer a 3D point descriptor that is invariant to 3D reflection symmetry using the spectral properties of the geodesic distance matrix built from the neighbors of a point. This helps us to separate the problem of discovering the optimal symmetry plane and the correspondences between reflective symmetric points. Our proposed approach achieves state-of-the-art performance on the benchmarks dataset.",1
"Object detection in thermal infrared spectrum provides more reliable data source in low-lighting conditions and different weather conditions, as it is useful both in-cabin and outside for pedestrian, animal, and vehicular detection as well as for detecting street-signs & lighting poles. This paper is about exploring and adapting state-of-the-art object detection and classifier framework on thermal vision with seven distinct classes for advanced driver-assistance systems (ADAS). The trained network variants on public datasets are validated on test data with three different test approaches which include test-time with no augmentation, test-time augmentation, and test-time with model ensembling. Additionally, the efficacy of trained networks is tested on locally gathered novel test-data captured with an uncooled LWIR prototype thermal camera in challenging weather and environmental scenarios. The performance analysis of trained models is investigated by computing precision, recall, and mean average precision scores (mAP). Furthermore, the trained model architecture is optimized using TensorRT inference accelerator and deployed on resource-constrained edge hardware Nvidia Jetson Nano to explicitly reduce the inference time on GPU as well as edge devices for further real-time onboard installations.",0
"The use of thermal infrared spectrum for object detection is more dependable in low-light and diverse weather conditions, making it valuable for detecting pedestrians, animals, vehicles, street signs, and lighting poles both inside and outside a vehicle. This research study focuses on applying cutting-edge object detection and classifier frameworks to thermal vision to enhance advanced driver-assistance systems (ADAS) by training network variants on public datasets and validating them with three different test approaches. The effectiveness of the trained networks is also tested on newly collected test-data captured with an uncooled LWIR prototype thermal camera in challenging weather and environmental situations, and their performance is analyzed using precision, recall, and mean average precision scores (mAP). Finally, the trained model architecture is optimized using TensorRT inference accelerator and deployed on Nvidia Jetson Nano, a resource-limited edge hardware, to reduce the inference time on GPU and edge devices for onboard real-time installations.",1
"For autonomous driving, radar is an important sensor type. On the one hand, radar offers a direct measurement of the radial velocity of targets in the environment. On the other hand, in literature, radar sensors are known for their robustness against several kinds of adverse weather conditions. However, on the downside, radar is susceptible to ghost targets or clutter which can be caused by several different causes, e.g., reflective surfaces in the environment. Ghost targets, for instance, can result in erroneous object detections. To this end, it is desirable to identify anomalous targets as early as possible in radar data. In this work, we present an approach based on PointNets to detect anomalous radar targets. Modifying the PointNet-architecture driven by our task, we developed a novel grouping variant which contributes to a multi-form grouping module. Our method is evaluated on a real-world dataset in urban scenarios and shows promising results for the detection of anomalous radar targets.",0
"Radar is a crucial sensor type for self-driving vehicles because it provides direct measurement of the radial velocity of objects in the environment and is known for its resilience to various weather conditions. However, radar is vulnerable to ghost targets or clutter caused by reflective surfaces in the surroundings, which can lead to incorrect object detections. Therefore, it is essential to detect anomalous targets in radar data as soon as possible. This study employs PointNets to identify anomalous radar targets, using a modified architecture that includes a novel grouping variant for a multi-form grouping module. The efficacy of the method is evaluated in urban scenarios using a real-world dataset, and it shows promising results for detecting anomalous radar targets.",1
"Objects in aerial images usually have arbitrary orientations and are densely located over the ground, making them extremely challenge to be detected. Many recently developed methods attempt to solve these issues by estimating an extra orientation parameter and placing dense anchors, which will result in high model complexity and computational costs. In this paper, we propose an arbitrary-oriented region proposal network (AO-RPN) to generate oriented proposals transformed from horizontal anchors. The AO-RPN is very efficient with only a few amounts of parameters increase than the original RPN. Furthermore, to obtain accurate bounding boxes, we decouple the detection task into multiple subtasks and propose a multi-head network to accomplish them. Each head is specially designed to learn the features optimal for the corresponding task, which allows our network to detect objects accurately. We name it MRDet short for Multi-head Rotated object Detector for convenience. We test the proposed MRDet on two challenging benchmarks, i.e., DOTA and HRSC2016, and compare it with several state-of-the-art methods. Our method achieves very promising results which clearly demonstrate its effectiveness.",0
"Detecting objects in aerial images can be difficult due to their arbitrary orientations and dense location on the ground. The complexity of the problem has led researchers to develop methods that estimate an extra orientation parameter and use dense anchors, resulting in high model complexity and computational costs. To address these issues, we propose the use of an arbitrary-oriented region proposal network (AO-RPN) that generates oriented proposals from horizontal anchors. Our AO-RPN is efficient, with only a small increase in parameter count compared to the original RPN. Additionally, we use a multi-head network to decouple the detection task into multiple subtasks, allowing for accurate bounding box predictions. Our network, named MRDet, achieves promising results on two challenging benchmarks, DOTA and HRSC2016, and outperforms several state-of-the-art methods.",1
"Few-shot object detection has been extensively investigated by incorporating meta-learning into region-based detection frameworks. Despite its success, the said paradigm is constrained by several factors, such as (i) low-quality region proposals for novel classes and (ii) negligence of the inter-class correlation among different classes. Such limitations hinder the generalization of base-class knowledge for the detection of novel-class objects. In this work, we design Meta-DETR, a novel few-shot detection framework that incorporates correlational aggregation for meta-learning into DETR detection frameworks. Meta-DETR works entirely at image level without any region proposals, which circumvents the constraint of inaccurate proposals in prevalent few-shot detection frameworks. Besides, Meta-DETR can simultaneously attend to multiple support classes within a single feed-forward. This unique design allows capturing the inter-class correlation among different classes, which significantly reduces the misclassification of similar classes and enhances knowledge generalization to novel classes. Experiments over multiple few-shot object detection benchmarks show that the proposed Meta-DETR outperforms state-of-the-art methods by large margins. The implementation codes will be released at https://github.com/ZhangGongjie/Meta-DETR.",0
"The integration of meta-learning into region-based detection frameworks has extensively explored the few-shot object detection. However, this paradigm has some limitations, including low-quality region proposals for novel classes and the neglect of inter-class correlation among different classes. These limitations restrict the generalization of base-class knowledge for detecting novel-class objects. In this study, a new few-shot detection framework named Meta-DETR is presented. It incorporates correlational aggregation for meta-learning into DETR detection frameworks and works entirely at the image level without any region proposals. By simultaneously attending to multiple support classes within a single feed-forward, Meta-DETR captures the inter-class correlation among different classes, which reduces the misclassification of similar classes and enhances knowledge generalization to novel classes. Experimental results on multiple few-shot object detection benchmarks demonstrate that Meta-DETR significantly outperforms state-of-the-art methods. The implementation codes for Meta-DETR will be available at https://github.com/ZhangGongjie/Meta-DETR.",1
"Computer Vision has played a major role in Intelligent Transportation Systems (ITS) and traffic surveillance. Along with the rapidly growing automated vehicles and crowded cities, the automated and advanced traffic management systems (ATMS) using video surveillance infrastructures have been evolved by the implementation of Deep Neural Networks. In this research, we provide a practical platform for real-time traffic monitoring, including 3D vehicle/pedestrian detection, speed detection, trajectory estimation, congestion detection, as well as monitoring the interaction of vehicles and pedestrians, all using a single CCTV traffic camera. We adapt a custom YOLOv5 deep neural network model for vehicle/pedestrian detection and an enhanced SORT tracking algorithm. For the first time, a hybrid satellite-ground based inverse perspective mapping (SG-IPM) method for camera auto-calibration is also developed which leads to an accurate 3D object detection and visualisation. We also develop a hierarchical traffic modelling solution based on short- and long-term temporal video data stream to understand the traffic flow, bottlenecks, and risky spots for vulnerable road users. Several experiments on real-world scenarios and comparisons with state-of-the-art are conducted using various traffic monitoring datasets, including MIO-TCD, UA-DETRAC and GRAM-RTM collected from highways, intersections, and urban areas under different lighting and weather conditions.",0
"Intelligent Transportation Systems and traffic surveillance have been significantly impacted by Computer Vision. The rise of automated vehicles and urban congestion has led to the development of advanced traffic management systems, utilizing video surveillance infrastructure and Deep Neural Networks. Our study offers a practical solution for real-time traffic monitoring, utilizing a single CCTV traffic camera to detect vehicles, pedestrians, speed, trajectory, and congestion. We employ a custom YOLOv5 deep neural network model and an enhanced SORT tracking algorithm for vehicle/pedestrian detection. Additionally, we introduce a hybrid satellite-ground based inverse perspective mapping (SG-IPM) method for camera auto-calibration, resulting in accurate 3D object detection and visualization. We also develop a hierarchical traffic modeling approach based on short- and long-term temporal video data stream to analyze traffic flow, bottlenecks, and risky areas for vulnerable road users. We conduct several experiments on real-world scenarios and compare our results with state-of-the-art using various traffic monitoring datasets, including MIO-TCD, UA-DETRAC, and GRAM-RTM collected from highways, intersections, and urban areas with varying lighting and weather conditions.",1
"Unsupervised domain adaptive object detection aims to adapt a well-trained detector from its original source domain with rich labeled data to a new target domain with unlabeled data. Recently, mainstream approaches perform this task through adversarial learning, yet still suffer from two limitations. First, they mainly align marginal distribution by unsupervised cross-domain feature matching, and ignore each feature's categorical and positional information that can be exploited for conditional alignment; Second, they treat all classes as equally important for transferring cross-domain knowledge and ignore that different classes usually have different transferability. In this paper, we propose a joint adaptive detection framework (JADF) to address the above challenges. First, an end-to-end joint adversarial adaptation framework for object detection is proposed, which aligns both marginal and conditional distributions between domains without introducing any extra hyperparameter. Next, to consider the transferability of each object class, a metric for class-wise transferability assessment is proposed, which is incorporated into the JADF objective for domain adaptation. Further, an extended study from unsupervised domain adaptation (UDA) to unsupervised few-shot domain adaptation (UFDA) is conducted, where only a few unlabeled training images are available in unlabeled target domain. Extensive experiments validate that JADF is effective in both the UDA and UFDA settings, achieving significant performance gains over existing state-of-the-art cross-domain detection methods.",0
"The goal of unsupervised domain adaptive object detection is to adapt a well-trained detector to a new target domain with unlabeled data. Mainstream approaches use adversarial learning, but they have two limitations. Firstly, they mainly align marginal distribution and ignore categorical and positional information. Secondly, they treat all classes equally, overlooking that different classes have different transferability. This paper proposes a joint adaptive detection framework (JADF) to address these challenges. JADF aligns both marginal and conditional distributions without extra hyperparameters and considers the transferability of each object class. The study extends from unsupervised domain adaptation (UDA) to unsupervised few-shot domain adaptation (UFDA). JADF is effective in both settings and outperforms existing cross-domain detection methods.",1
"MLP-based architectures, which consist of a sequence of consecutive multi-layer perceptron blocks, have recently been found to reach comparable results to convolutional and transformer-based methods. However, most adopt spatial MLPs which take fixed dimension inputs, therefore making it difficult to apply them to downstream tasks, such as object detection and semantic segmentation. Moreover, single-stage designs further limit performance in other computer vision tasks and fully connected layers bear heavy computation. To tackle these problems, we propose ConvMLP: a hierarchical Convolutional MLP for visual recognition, which is a light-weight, stage-wise, co-design of convolution layers, and MLPs. In particular, ConvMLP-S achieves 76.8% top-1 accuracy on ImageNet-1k with 9M parameters and 2.4G MACs (15% and 19% of MLP-Mixer-B/16, respectively). Experiments on object detection and semantic segmentation further show that visual representation learned by ConvMLP can be seamlessly transferred and achieve competitive results with fewer parameters. Our code and pre-trained models are publicly available at https://github.com/SHI-Labs/Convolutional-MLPs.",0
"Recently, MLP-based architectures, comprising of consecutive multi-layer perceptron blocks, have been shown to produce comparable results to convolutional and transformer-based methods. However, most of these architectures use spatial MLPs that take inputs with fixed dimensions, thereby making it challenging to apply them to downstream tasks like object detection and semantic segmentation. Additionally, single-stage designs restrict performance in other computer vision tasks, and fully connected layers require heavy computation. To address these issues, we propose a hierarchical Convolutional MLP called ConvMLP, which is a lightweight, stage-wise design that combines convolution layers and MLPs. Specifically, our ConvMLP-S model achieves a top-1 accuracy of 76.8% on ImageNet-1k with only 9M parameters and 2.4G MACs, representing 15% and 19% of MLP-Mixer-B/16, respectively. Our experiments on object detection and semantic segmentation demonstrate that ConvMLP's visual representation can be transferred seamlessly and achieve competitive results with fewer parameters. Our code and pre-trained models are available at https://github.com/SHI-Labs/Convolutional-MLPs.",1
"Semantic segmentation of fine-resolution urban scene images plays a vital role in extensive practical applications, such as land cover mapping, urban change detection, environmental protection and economic assessment. Driven by rapid developments in deep learning technologies, convolutional neural networks (CNNs) have dominated the semantic segmentation task for many years. Convolutional neural networks adopt hierarchical feature representation and have strong local context extraction. However, the local property of the convolution layer limits the network from capturing global information that is crucial for improving fine-resolution image segmentation. Recently, Transformer comprise a hot topic in the computer vision domain. Vision Transformer demonstrates the great capability of global information modelling, boosting many vision tasks, such as image classification, object detection and especially semantic segmentation. In this paper, we propose an efficient hybrid Transformer (EHT) for semantic segmentation of urban scene images. EHT takes advantage of CNNs and Transformer, learning global-local context to strengthen the feature representation. Extensive experiments demonstrate that EHT has higher efficiency with competitive accuracy compared with state-of-the-art benchmark methods. Specifically, the proposed EHT achieves a 67.0% mIoU on the UAVid test set and outperforms other lightweight models significantly. The code will be available soon.",0
"The segmentation of fine-resolution urban scene images is crucial for practical applications such as land cover mapping, urban change detection, environmental protection, and economic assessment. Convolutional neural networks (CNNs) have been the dominant technology for semantic segmentation due to their hierarchical feature representation and local context extraction. However, CNNs are limited in capturing global information, which is essential for improving fine-resolution image segmentation. Recently, Transformer has gained attention in computer vision, demonstrating the ability to model global information and improve image classification, object detection, and semantic segmentation. This paper proposes an efficient hybrid Transformer (EHT) for urban scene image segmentation that combines CNNs and Transformer to learn global-local context and enhance feature representation. Experimental results show that EHT achieves higher efficiency with competitive accuracy compared to state-of-the-art benchmark methods, achieving a 67.0% mIoU on the UAVid test set and outperforming other lightweight models significantly. The code will be available soon.",1
"Knowledge distillation methods are proved to be promising in improving the performance of neural networks and no additional computational expenses are required during the inference time. For the sake of boosting the accuracy of object detection, a great number of knowledge distillation methods have been proposed particularly designed for object detection. However, most of these methods only focus on feature-level distillation and label-level distillation, leaving the label assignment step, a unique and paramount procedure for object detection, by the wayside. In this work, we come up with a simple but effective knowledge distillation approach focusing on label assignment in object detection, in which the positive and negative samples of student network are selected in accordance with the predictions of teacher network. Our method shows encouraging results on the MSCOCO2017 benchmark, and can not only be applied to both one-stage detectors and two-stage detectors but also be utilized orthogonally with other knowledge distillation methods.",0
"The utilization of knowledge distillation techniques has been proven to enhance the performance of neural networks without incurring additional computational costs during inference. In order to enhance object detection accuracy, numerous knowledge distillation methods have been developed specifically for this purpose. However, most of these methods focus solely on feature-level and label-level distillation, disregarding the vital label assignment step involved in object detection. This study introduces a straightforward yet effective knowledge distillation approach that concentrates on the label assignment process in object detection. Our approach involves the selection of positive and negative samples based on the teacher network's predictions for the student network. Our method delivers promising results on the MSCOCO2017 benchmark and can be applied to both one-stage and two-stage detectors while being used in conjunction with other knowledge distillation techniques.",1
"We propose the Parallel Residual Bi-Fusion Feature Pyramid Network (PRB-FPN) for fast and accurate single-shot object detection. Feature Pyramid (FP) is widely used in recent visual detection, however the top-down pathway of FP cannot preserve accurate localization due to pooling shifting. The advantage of FP is weaken as deeper backbones with more layers are used. To address this issue, we propose a new parallel FP structure with bi-directional (top-down and bottom-up) fusion and associated improvements to retain high-quality features for accurate localization. Our method is particularly suitable for detecting small objects. We provide the following design improvements: (1) A parallel bifusion FP structure with a Bottom-up Fusion Module (BFM) to detect both small and large objects at once with high accuracy. (2) A COncatenation and RE-organization (CORE) module provides a bottom-up pathway for feature fusion, which leads to the bi-directional fusion FP that can recover lost information from lower-layer feature maps. (3) The CORE feature is further purified to retain richer contextual information. Such purification is performed with CORE in a few iterations in both top-down and bottom-up pathways. (4) The adding of a residual design to CORE leads to a new Re-CORE module that enables easy training and integration with a wide range of (deeper or lighter) backbones. The proposed network achieves state-of-the-art performance on UAVDT17 and MS COCO datasets.",0
"We suggest the use of the Parallel Residual Bi-Fusion Feature Pyramid Network (PRB-FPN) to quickly and accurately detect single-shot objects. Although the Feature Pyramid (FP) is commonly utilized for visual detection, the top-down pathway of FP cannot preserve precise localization due to pooling shifting, causing the FP's advantage to diminish with deeper backbones. To overcome this issue, we propose a new parallel FP structure with bi-directional fusion, including the Bottom-up Fusion Module (BFM) and the COncatenation and RE-organization (CORE) module, to enhance the quality of features for precise localization. Our approach is especially effective in detecting small objects, and we introduce design improvements such as CORE feature purification and the addition of a residual design to the Re-CORE module for easy training and integration with a variety of backbones. Our proposed network achieves the best results on the UAVDT17 and MS COCO datasets.",1
"This report summarizes the results of Learning to Understand Aerial Images (LUAI) 2021 challenge held on ICCV 2021, which focuses on object detection and semantic segmentation in aerial images. Using DOTA-v2.0 and GID-15 datasets, this challenge proposes three tasks for oriented object detection, horizontal object detection, and semantic segmentation of common categories in aerial images. This challenge received a total of 146 registrations on the three tasks. Through the challenge, we hope to draw attention from a wide range of communities and call for more efforts on the problems of learning to understand aerial images.",0
"The report provides an overview of the outcomes of the ICCV 2021 Learning to Understand Aerial Images (LUAI) challenge. The challenge aimed to tackle object detection and semantic segmentation in aerial images, utilizing the DOTA-v2.0 and GID-15 datasets and featuring three tasks: oriented object detection, horizontal object detection, and semantic segmentation of common categories in aerial images. A total of 146 registrations were received for the three tasks. The challenge aims to raise awareness among various communities and encourage further research into understanding aerial images.",1
"Self-Attention has become prevalent in computer vision models. Inspired by fully connected Conditional Random Fields (CRFs), we decompose it into local and context terms. They correspond to the unary and binary terms in CRF and are implemented by attention mechanisms with projection matrices. We observe that the unary terms only make small contributions to the outputs, and meanwhile standard CNNs that rely solely on the unary terms achieve great performances on a variety of tasks. Therefore, we propose Locally Enhanced Self-Attention (LESA), which enhances the unary term by incorporating it with convolutions, and utilizes a fusion module to dynamically couple the unary and binary operations. In our experiments, we replace the self-attention modules with LESA. The results on ImageNet and COCO show the superiority of LESA over convolution and self-attention baselines for the tasks of image recognition, object detection, and instance segmentation. The code is made publicly available.",0
"The use of Self-Attention in computer vision models has become widespread. Taking inspiration from fully connected Conditional Random Fields (CRFs), we break it down into local and context terms, which correspond to the unary and binary terms in CRF. These are implemented using attention mechanisms with projection matrices. Our observation is that the unary terms have minimal impact on the outputs, and standard CNNs that rely solely on the unary terms achieve excellent results across various tasks. To address this, we propose Locally Enhanced Self-Attention (LESA), which enhances the unary term by incorporating it with convolutions. We also utilize a fusion module to dynamically couple the unary and binary operations. In our experiments, we replace the self-attention modules with LESA and achieve superior results for image recognition, object detection, and instance segmentation on ImageNet and COCO. We have made the code publicly available.",1
"Data is a crucial component of machine learning. The field is reliant on data to train, validate, and test models. With increased technical capabilities, machine learning research has boomed in both academic and industry settings, and one major focus has been on computer vision. Computer vision is a popular domain of machine learning increasingly pertinent to real-world applications, from facial recognition in policing to object detection for autonomous vehicles. Given computer vision's propensity to shape machine learning research and impact human life, we seek to understand disciplinary practices around dataset documentation - how data is collected, curated, annotated, and packaged into datasets for computer vision researchers and practitioners to use for model tuning and development. Specifically, we examine what dataset documentation communicates about the underlying values of vision data and the larger practices and goals of computer vision as a field. To conduct this study, we collected a corpus of about 500 computer vision datasets, from which we sampled 114 dataset publications across different vision tasks. Through both a structured and thematic content analysis, we document a number of values around accepted data practices, what makes desirable data, and the treatment of humans in the dataset construction process. We discuss how computer vision datasets authors value efficiency at the expense of care; universality at the expense of contextuality; impartiality at the expense of positionality; and model work at the expense of data work. Many of the silenced values we identify sit in opposition with social computing practices. We conclude with suggestions on how to better incorporate silenced values into the dataset creation and curation process.",0
"Machine learning heavily relies on data for training, validating, and testing models, particularly in the domain of computer vision, which has become increasingly relevant in real-world applications such as facial recognition and object detection. Due to the significant impact of computer vision on society, it is crucial to understand the practices surrounding dataset documentation, including data collection, curation, annotation, and packaging. By analyzing a corpus of 500 computer vision datasets and sampling 114 dataset publications, we identify the values that underlie dataset practices and their implications for the larger goals of the computer vision field. Our findings suggest that computer vision dataset authors prioritize efficiency, universality, and impartiality over care, contextuality, and positionality, respectively, which contradict social computing practices. We propose incorporating these silenced values into the dataset creation and curation process to enhance the ethical and social responsibility of computer vision research.",1
"We propose 3DETR, an end-to-end Transformer based object detection model for 3D point clouds. Compared to existing detection methods that employ a number of 3D-specific inductive biases, 3DETR requires minimal modifications to the vanilla Transformer block. Specifically, we find that a standard Transformer with non-parametric queries and Fourier positional embeddings is competitive with specialized architectures that employ libraries of 3D-specific operators with hand-tuned hyperparameters. Nevertheless, 3DETR is conceptually simple and easy to implement, enabling further improvements by incorporating 3D domain knowledge. Through extensive experiments, we show 3DETR outperforms the well-established and highly optimized VoteNet baselines on the challenging ScanNetV2 dataset by 9.5%. Furthermore, we show 3DETR is applicable to 3D tasks beyond detection, and can serve as a building block for future research.",0
"Our proposed model, 3DETR, is an object detection model for 3D point clouds based on the Transformer architecture. Unlike other detection methods that rely heavily on 3D-specific inductive biases, 3DETR requires only minor adjustments to the vanilla Transformer block. By using non-parametric queries and Fourier positional embeddings, a standard Transformer is able to compete with specialized architectures that use 3D-specific operators with customized hyperparameters. 3DETR is straightforward to understand and implement, and can be further enhanced by integrating 3D domain knowledge. Our experiments demonstrate that 3DETR outperforms the established VoteNet baseline by 9.5% on the challenging ScanNetV2 dataset. In addition, 3DETR is versatile and can be used for other 3D tasks, making it a valuable research tool.",1
"Salient object detection is a fundamental topic in computer vision. Previous methods based on RGB-D often suffer from the incompatibility of multi-modal feature fusion and the insufficiency of multi-scale feature aggregation. To tackle these two dilemmas, we propose a novel multi-modal and multi-scale refined network (M2RNet). Three essential components are presented in this network. The nested dual attention module (NDAM) explicitly exploits the combined features of RGB and depth flows. The adjacent interactive aggregation module (AIAM) gradually integrates the neighbor features of high, middle and low levels. The joint hybrid optimization loss (JHOL) makes the predictions have a prominent outline. Extensive experiments demonstrate that our method outperforms other state-of-the-art approaches.",0
"The detection of significant objects is a crucial aspect of computer vision. However, prior techniques that rely on RGB-D frequently encounter issues with the fusion of multi-modal features and the aggregation of multi-scale features. To overcome these challenges, we introduce a new network called the Multi-Modal and Multi-Scale Refined Network (M2RNet), which comprises three key components. The Nested Dual Attention Module (NDAM) harnesses the combined features of RGB and depth flows. The Adjacent Interactive Aggregation Module (AIAM) progressively combines the features of neighboring levels. The Joint Hybrid Optimization Loss (JHOL) enhances the accuracy of object predictions. Our extensive experiments prove that our approach outperforms other state-of-the-art methods.",1
"Self-supervised representation learning for visual pre-training has achieved remarkable success with sample (instance or pixel) discrimination and semantics discovery of instance, whereas there still exists a non-negligible gap between pre-trained model and downstream dense prediction tasks. Concretely, these downstream tasks require more accurate representation, in other words, the pixels from the same object must belong to a shared semantic category, which is lacking in the previous methods. In this work, we present Dense Semantic Contrast (DSC) for modeling semantic category decision boundaries at a dense level to meet the requirement of these tasks. Furthermore, we propose a dense cross-image semantic contrastive learning framework for multi-granularity representation learning. Specially, we explicitly explore the semantic structure of the dataset by mining relations among pixels from different perspectives. For intra-image relation modeling, we discover pixel neighbors from multiple views. And for inter-image relations, we enforce pixel representation from the same semantic class to be more similar than the representation from different classes in one mini-batch. Experimental results show that our DSC model outperforms state-of-the-art methods when transferring to downstream dense prediction tasks, including object detection, semantic segmentation, and instance segmentation. Code will be made available.",0
"Visual pre-training through self-supervised representation learning has achieved notable success in semantics discovery of instance and sample discrimination of pixel or instance. However, there still exists a significant gap between downstream dense prediction tasks and pre-trained models. These tasks require a more precise representation, specifically, pixels of an object must belong to a shared semantic category, which has been lacking in prior methods. Our work introduces Dense Semantic Contrast (DSC) to model semantic category decision boundaries at a dense level to meet these task requirements. Additionally, we propose a dense cross-image semantic contrastive learning framework for multi-granularity representation learning, where we explicitly examine the semantic structure of the dataset by mining relations among pixels from different perspectives. Our model, DSC, outperforms state-of-the-art methods when transferring to downstream dense prediction tasks, including object detection, semantic segmentation, and instance segmentation. Code will be available.",1
"Few-shot object detection aims to detect instances of specific categories in a query image with only a handful of support samples. Although this takes less effort than obtaining enough annotated images for supervised object detection, it results in a far inferior performance compared to the conventional object detection methods. In this paper, we propose a meta-learning-based approach that considers the unique characteristics of each support sample. Rather than simply averaging the information of the support samples to generate a single prototype per category, our method can better utilize the information of each support sample by treating each support sample as an individual prototype. Specifically, we introduce two types of attention mechanisms for aggregating the query and support feature maps. The first is to refine the information of few-shot samples by extracting shared information between the support samples through attention. Second, each support sample is used as a class code to leverage the information by comparing similarities between each support feature and query features. Our proposed method is complementary to the previous methods, making it easy to plug and play for further improvement. We have evaluated our method on PASCAL VOC and COCO benchmarks, and the results verify the effectiveness of our method. In particular, the advantages of our method are maximized when there is more diversity among support data.",0
"The goal of few-shot object detection is to detect specific categories in a query image with only a few support samples. While this method requires less effort than supervised object detection, it typically performs worse. To address this issue, we propose a meta-learning-based approach that considers the unique characteristics of each support sample. Rather than averaging the support samples to create a single prototype per category, our method treats each support sample as an individual prototype, resulting in better utilization of the information. Our approach includes two attention mechanisms for aggregating the query and support feature maps. The first attention mechanism refines the information of few-shot samples by extracting shared information between the support samples. The second mechanism uses each support sample as a class code to compare similarities between support and query features. Our proposed method is complementary to previous methods and can be easily integrated for further improvement. We evaluated our approach on PASCAL VOC and COCO benchmarks, and the results demonstrate its effectiveness, particularly when the support data is diverse.",1
"Machine/deep-learning (ML/DL) based techniques are emerging as a driving force behind many cutting-edge technologies, achieving high accuracy on computer vision workloads such as image classification and object detection. However, training these models involving large parameters is both time-consuming and energy-hogging. In this regard, several prior works have advocated for sparsity to speed up the of DL training and more so, the inference phase. This work begins with the observation that during training, sparsity in the forward and backward passes are correlated. In that context, we investigate two types of sparsity (input and output type) inherent in gradient descent-based optimization algorithms and propose a hardware micro-architecture to leverage the same. Our experimental results use five state-of-the-art CNN models on the Imagenet dataset, and show back propagation speedups in the range of 1.69$\times$ to 5.43$\times$, compared to the dense baseline execution. By exploiting sparsity in both the forward and backward passes, speedup improvements range from 1.68$\times$ to 3.30$\times$ over the sparsity-agnostic baseline execution. Our work also achieves significant reduction in training iteration time over several previously proposed dense as well as sparse accelerator based platforms, in addition to achieving order of magnitude energy efficiency improvements over GPU based execution.",0
"The use of ML/DL techniques is becoming increasingly popular in advancing technologies, particularly in computer vision tasks such as image classification and object detection. However, training these models with a large number of parameters is time-consuming and energy-intensive. To address this issue, previous studies have suggested utilizing sparsity to speed up DL training and inference. This study examines two types of sparsity (input and output) in gradient descent-based optimization algorithms and proposes a hardware micro-architecture to take advantage of them. Experimentation with five CNN models on the Imagenet dataset shows back propagation speedups ranging from 1.69$\times$ to 5.43$\times$ compared to the dense baseline execution. By utilizing sparsity in both the forward and backward passes, speedup improvements range from 1.68$\times$ to 3.30$\times$ over the sparsity-agnostic baseline execution. Additionally, this work achieves faster training iteration times and significantly improved energy efficiency compared to GPU-based execution and previously proposed dense and sparse accelerator platforms.",1
"While recent progress has significantly boosted few-shot classification (FSC) performance, few-shot object detection (FSOD) remains challenging for modern learning systems. Existing FSOD systems follow FSC approaches, ignoring critical issues such as spatial variability and uncertain representations, and consequently result in low performance. Observing this, we propose a novel \textbf{Dual-Awareness Attention (DAnA)} mechanism that enables networks to adaptively interpret the given support images. DAnA transforms support images into \textbf{query-position-aware} (QPA) features, guiding detection networks precisely by assigning customized support information to each local region of the query. In addition, the proposed DAnA component is flexible and adaptable to multiple existing object detection frameworks. By adopting DAnA, conventional object detection networks, Faster R-CNN and RetinaNet, which are not designed explicitly for few-shot learning, reach state-of-the-art performance in FSOD tasks. In comparison with previous methods, our model significantly increases the performance by 47\% (+6.9 AP), showing remarkable ability under various evaluation settings.",0
"Although few-shot classification (FSC) performance has seen recent progress, few-shot object detection (FSOD) remains a challenge for modern learning systems. Existing FSOD systems follow FSC approaches, but this ignores crucial issues such as spatial variability and uncertain representations, resulting in low performance. To address this, we propose a new mechanism called Dual-Awareness Attention (DAnA), which allows networks to interpret support images adaptively. DAnA converts support images into query-position-aware (QPA) features, which guide detection networks more accurately by assigning customized support information to each local region of the query. Furthermore, the DAnA component is versatile and can be adapted to multiple existing object detection frameworks. By utilizing DAnA, Faster R-CNN and RetinaNet, traditional object detection networks not explicitly designed for few-shot learning, achieve state-of-the-art performance in FSOD tasks. Compared to previous methods, our model improves performance by 47% (+6.9 AP) and demonstrates impressive capability in various evaluation settings.",1
"Recently, DETR pioneered the solution of vision tasks with transformers, it directly translates the image feature map into the object detection result. Though effective, translating the full feature map can be costly due to redundant computation on some area like the background. In this work, we encapsulate the idea of reducing spatial redundancy into a novel poll and pool (PnP) sampling module, with which we build an end-to-end PnP-DETR architecture that adaptively allocates its computation spatially to be more efficient. Concretely, the PnP module abstracts the image feature map into fine foreground object feature vectors and a small number of coarse background contextual feature vectors. The transformer models information interaction within the fine-coarse feature space and translates the features into the detection result. Moreover, the PnP-augmented model can instantly achieve various desired trade-offs between performance and computation with a single model by varying the sampled feature length, without requiring to train multiple models as existing methods. Thus it offers greater flexibility for deployment in diverse scenarios with varying computation constraint. We further validate the generalizability of the PnP module on panoptic segmentation and the recent transformer-based image recognition model ViT and show consistent efficiency gain. We believe our method makes a step for efficient visual analysis with transformers, wherein spatial redundancy is commonly observed. Code will be available at \url{https://github.com/twangnh/pnp-detr}.",0
"DETR has recently led the way in utilizing transformers to handle vision tasks by directly translating image feature maps into object detection results. However, this approach can be costly due to redundant computation in areas such as the background. To address this issue, we introduce a new module called poll and pool (PnP) sampling, which reduces spatial redundancy and allows for more efficient computation allocation. Our PnP-DETR architecture utilizes this module to abstract the image feature map into fine foreground object feature vectors and a few coarse background contextual feature vectors, which are then processed by the transformer for detection. This approach offers greater flexibility for deployment in different scenarios with varying computation constraints, as it allows for instant trade-offs between performance and computation with a single model. We also validate the generalizability of the PnP module on panoptic segmentation and ViT-based image recognition models, showing consistent efficiency gains. Our method represents a step towards efficient visual analysis with transformers, an area where spatial redundancy is often observed. Code for our approach is available at \url{https://github.com/twangnh/pnp-detr}.",1
"We present a new and complex traffic dataset, METEOR, which captures traffic patterns in unstructured scenarios in India. METEOR consists of more than 1000 one-minute video clips, over 2 million annotated frames with ego-vehicle trajectories, and more than 13 million bounding boxes for surrounding vehicles or traffic agents. METEOR is a unique dataset in terms of capturing the heterogeneity of microscopic and macroscopic traffic characteristics. Furthermore, we provide annotations for rare and interesting driving behaviors such as cut-ins, yielding, overtaking, overspeeding, zigzagging, sudden lane changing, running traffic signals, driving in the wrong lanes, taking wrong turns, lack of right-of-way rules at intersections, etc. We also present diverse traffic scenarios corresponding to rainy weather, nighttime driving, driving in rural areas with unmarked roads, and high-density traffic scenarios. We use our novel dataset to evaluate the performance of object detection and behavior prediction algorithms. We show that state-of-the-art object detectors fail in these challenging conditions and also propose a new benchmark test: action-behavior prediction with a baseline mAP score of 70.74.",0
"The METEOR traffic dataset we introduce is intricate and captures traffic patterns in disorganized settings in India. With over 1000 one-minute video clips, more than 2 million annotated frames featuring ego-vehicle trajectories, and over 13 million bounding boxes for surrounding vehicles or traffic agents, METEOR is distinctive in its ability to capture microscopic and macroscopic traffic features. Additionally, we provide annotations for infrequent and captivating driving behaviors such as cut-ins, yielding, overtaking, overspeeding, zigzagging, sudden lane changing, running traffic signals, driving in the wrong lanes, taking wrong turns, lack of right-of-way rules at intersections, and others. We also present a range of traffic scenarios, including rainy weather, nighttime driving, driving in rural areas with unmarked roads, and high-density traffic scenarios. We leverage our new dataset to assess the accuracy of object detection and behavior prediction algorithms and demonstrate that state-of-the-art object detectors struggle in these challenging conditions. Furthermore, we propose a new benchmark test, action-behavior prediction, with a baseline mAP score of 70.74.",1
"Deep Bregman divergence measures divergence of data points using neural networks which is beyond Euclidean distance and capable of capturing divergence over distributions. In this paper, we propose deep Bregman divergences for contrastive learning of visual representation and we aim to enhance contrastive loss used in self-supervised learning by training additional networks based on functional Bregman divergence. In contrast to the conventional contrastive learning methods which are solely based on divergences between single points, our framework can capture the divergence between distributions which improves the quality of learned representation. By combining conventional contrastive loss with the proposed divergence loss, our method outperforms baseline and most of previous methods for self-supervised and semi-supervised learning on multiple classifications and object detection tasks and datasets. The source code of the method and of all the experiments are available at supplementary.",0
"The deep Bregman divergence is a measure of data point divergence that goes beyond the Euclidean distance and can capture divergence over distributions using neural networks. Our paper introduces deep Bregman divergences for contrastive learning of visual representation. We propose to enhance the contrastive loss used in self-supervised learning by training additional networks based on functional Bregman divergence. Unlike conventional contrastive learning methods that rely solely on divergences between single points, our framework can capture the divergence between distributions and improve the quality of learned representation. By combining the proposed divergence loss with conventional contrastive loss, our method outperforms baseline and many previous methods for self-supervised and semi-supervised learning on various classifications and object detection tasks and datasets. The source code for the method and experiments is available in the supplementary materials.",1
"With the increasing popularity of deep learning, Convolutional Neural Networks (CNNs) have been widely applied in various domains, such as image classification and object detection, and achieve stunning success in terms of their high accuracy over the traditional statistical methods. To exploit the potential of CNN models, a huge amount of research and industry efforts have been devoted to optimizing CNNs. Among these endeavors, CNN architecture design has attracted tremendous attention because of its great potential of improving model accuracy or reducing model complexity. However, existing work either introduces repeated training overhead in the search process or lacks an interpretable metric to guide the design. To clear these hurdles, we propose 3D-Receptive Field (3DRF), an explainable and easy-to-compute metric, to estimate the quality of a CNN architecture and guide the search process of designs. To validate the effectiveness of 3DRF, we build a static optimizer to improve the CNN architectures at both the stage level and the kernel level. Our optimizer not only provides a clear and reproducible procedure but also mitigates unnecessary training efforts in the architecture search process. Extensive experiments and studies show that the models generated by our optimizer can achieve up to 5.47% accuracy improvement and up to 65.38% parameters deduction, compared with state-of-the-art CNN structures like MobileNet and ResNet.",0
"Convolutional Neural Networks (CNNs) have become increasingly popular in various domains, such as image classification and object detection, due to their high accuracy over traditional statistical methods. To optimize CNN models, significant research and industry efforts have focused on CNN architecture design, which has the potential to improve model accuracy or reduce complexity. However, existing approaches often introduce training overhead or lack an interpretable metric to guide design. To address these issues, we propose a new metric, 3D-Receptive Field (3DRF), which is both explainable and easy to compute. This metric estimates the quality of a CNN architecture and guides the design search process. To validate our approach, we build a static optimizer that improves CNN architectures at both the stage and kernel levels. Our optimizer provides a clear and reproducible procedure while reducing unnecessary training efforts. Extensive experiments show that our optimizer can improve accuracy by up to 5.47% and reduce parameters by up to 65.38% compared to state-of-the-art CNN structures like MobileNet and ResNet.",1
"Recent advances in unsupervised domain adaptation have significantly improved the recognition accuracy of CNNs by alleviating the domain shift between (labeled) source and (unlabeled) target data distributions. While the problem of single-target domain adaptation (STDA) for object detection has recently received much attention, multi-target domain adaptation (MTDA) remains largely unexplored, despite its practical relevance in several real-world applications, such as multi-camera video surveillance. Compared to the STDA problem that may involve large domain shifts between complex source and target distributions, MTDA faces additional challenges, most notably the computational requirements and catastrophic forgetting of previously-learned targets, which can depend on the order of target adaptations. STDA for detection can be applied to MTDA by adapting one model per target, or one common model with a mixture of data from target domains. However, these approaches are either costly or inaccurate. The only state-of-art MTDA method specialized for detection learns targets incrementally, one target at a time, and mitigates the loss of knowledge by using a duplicated detection model for knowledge distillation, which is computationally expensive and does not scale well to many domains. In this paper, we introduce an efficient approach for incremental learning that generalizes well to multiple target domains. Our MTDA approach is more suitable for real-world applications since it allows updating the detection model incrementally, without storing data from previous-learned target domains, nor retraining when a new target domain becomes available. Our proposed method, MTDA-DTM, achieved the highest level of detection accuracy compared against state-of-the-art approaches on several MTDA detection benchmarks and Wildtrack, a benchmark for multi-camera pedestrian detection.",0
"CNN recognition accuracy has been enhanced by recent unsupervised domain adaptation advances that reduce domain shift between labeled source and unlabeled target data distributions. While single-target domain adaptation (STDA) for object detection has received much attention, multi-target domain adaptation (MTDA) has not, despite its practical relevance in multi-camera video surveillance and other real-world applications. Compared to STDA, MTDA involves additional challenges such as computational requirements and catastrophic forgetting of previously-learned targets. STDA can be applied to MTDA by adapting one model per target or one common model, but these approaches are costly or inaccurate. The only state-of-art MTDA method for detection learns targets incrementally but is computationally expensive and not scalable. In this paper, we present an efficient approach for incremental learning that generalizes well to multiple target domains. Our MTDA-DTM method allows for incremental updating of the detection model without storing data from previous-learned target domains or retraining. It achieved the highest level of detection accuracy compared to state-of-the-art approaches on several MTDA detection benchmarks and Wildtrack.",1
"A significant amount of redundancy exists between consecutive frames of a video. Object detectors typically produce detections for one image at a time, without any capabilities for taking advantage of this redundancy. Meanwhile, many applications for object detection work with videos, including intelligent transportation systems, advanced driver assistance systems and video surveillance. Our work aims at taking advantage of the similarity between video frames to produce better detections. We propose FFAVOD, standing for feature fusion architecture for video object detection. We first introduce a novel video object detection architecture that allows a network to share feature maps between nearby frames. Second, we propose a feature fusion module that learns to merge feature maps to enhance them. We show that using the proposed architecture and the fusion module can improve the performance of three base object detectors on two object detection benchmarks containing sequences of moving road users. Additionally, to further increase performance, we propose an improvement to the SpotNet attention module. Using our architecture on the improved SpotNet detector, we obtain the state-of-the-art performance on the UA-DETRAC public benchmark as well as on the UAVDT dataset. Code is available at https://github.com/hu64/FFAVOD.",0
"Consecutive frames of a video contain a lot of redundancy, but object detectors typically only produce detections for one image at a time without utilizing this redundancy. However, many applications for object detection, such as intelligent transportation systems, advanced driver assistance systems, and video surveillance, require analyzing videos. Our work focuses on exploiting the similarity between video frames to improve object detection. We introduce FFAVOD, a feature fusion architecture for video object detection, which includes a novel video object detection architecture that allows for sharing feature maps between nearby frames and a feature fusion module that merges feature maps to enhance them. Our proposed architecture and fusion module improve the performance of three base object detectors on two object detection benchmarks with sequences of moving road users. We also propose an enhancement to the SpotNet attention module to further increase performance. Using our architecture with the improved SpotNet detector achieves state-of-the-art performance on the UA-DETRAC public benchmark and the UAVDT dataset. Our code is available at https://github.com/hu64/FFAVOD.",1
"In object detection, multi-level prediction (e.g., FPN, YOLO) and resampling skills (e.g., focal loss, ATSS) have drastically improved one-stage detector performance. However, how to improve the performance by optimizing the feature pyramid level-by-level remains unexplored. We find that, during training, the ratio of positive over negative samples varies across pyramid levels (\emph{level imbalance}), which is not addressed by current one-stage detectors. To mediate the influence of level imbalance, we propose a Unified Multi-level Optimization Paradigm (UMOP) consisting of two components: 1) an independent classification loss supervising each pyramid level with individual resampling considerations; 2) a progressive hard-case mining loss defining all losses across the pyramid levels without extra level-wise settings. With UMOP as a plug-and-play scheme, modern one-stage detectors can attain a ~1.5 AP improvement with fewer training iterations and no additional computation overhead. Our best model achieves 55.1 AP on COCO test-dev. Code is available at https://github.com/zimoqingfeng/UMOP.",0
"The performance of one-stage object detectors has significantly improved through the use of multi-level prediction and resampling techniques. However, there has been no exploration into optimizing the feature pyramid level-by-level to further improve performance. It has been found that the ratio of positive to negative samples varies across pyramid levels during training, which is not currently addressed by one-stage detectors. To address this issue, a Unified Multi-level Optimization Paradigm (UMOP) has been proposed. UMOP includes two components: 1) an independent classification loss for each pyramid level with individual resampling considerations; and 2) a progressive hard-case mining loss that defines all losses across the pyramid levels without requiring extra level-wise settings. UMOP can be used as a plug-and-play scheme to achieve a ~1.5 AP improvement with fewer training iterations and no additional computation overhead. The best model using UMOP achieves 55.1 AP on COCO test-dev, and the code is available at https://github.com/zimoqingfeng/UMOP.",1
"Physical adversarial attacks in object detection have attracted increasing attention. However, most previous works focus on hiding the objects from the detector by generating an individual adversarial patch, which only covers the planar part of the vehicle's surface and fails to attack the detector in physical scenarios for multi-view, long-distance and partially occluded objects. To bridge the gap between digital attacks and physical attacks, we exploit the full 3D vehicle surface to propose a robust Full-coverage Camouflage Attack (FCA) to fool detectors. Specifically, we first try rendering the non-planar camouflage texture over the full vehicle surface. To mimic the real-world environment conditions, we then introduce a transformation function to transfer the rendered camouflaged vehicle into a photo-realistic scenario. Finally, we design an efficient loss function to optimize the camouflage texture. Experiments show that the full-coverage camouflage attack can not only outperform state-of-the-art methods under various test cases but also generalize to different environments, vehicles, and object detectors.",0
"The focus on physical adversarial attacks in object detection has increased, but previous studies have concentrated on creating an individual adversarial patch to hide objects from the detector, which only conceals the flat portion of a vehicle's surface. This method is ineffective in physical scenarios where objects are partially occluded, distant, or viewed from multiple angles. To address this gap between digital and physical attacks, we propose a Full-coverage Camouflage Attack (FCA) that leverages the entire 3D surface of a vehicle for a robust attack. Our approach involves rendering a non-planar camouflage texture over the entire vehicle surface, followed by a transformation to a photo-realistic environment. We also design an efficient loss function to optimize the camouflage texture. Our experiments demonstrate that the FCA method outperforms state-of-the-art techniques in various test cases and is effective in different environments, with various vehicles and object detectors.",1
"For autonomous vehicles to viably replace human drivers they must contend with inclement weather. Falling rain and snow introduce noise in LiDAR returns resulting in both false positive and false negative object detections. In this article we introduce the Winter Adverse Driving dataSet (WADS) collected in the snow belt region of Michigan's Upper Peninsula. WADS is the first multi-modal dataset featuring dense point-wise labeled sequential LiDAR scans collected in severe winter weather; weather that would cause an experienced driver to alter their driving behavior. We have labelled and will make available over 7 GB or 3.6 billion labelled LiDAR points out of over 26 TB of total LiDAR and camera data collected. We also present the Dynamic Statistical Outlier Removal (DSOR) filter, a statistical PCL-based filter capable or removing snow with a higher recall than the state of the art snow de-noising filter while being 28\% faster. Further, the DSOR filter is shown to have a lower time complexity compared to the state of the art resulting in an improved scalability.   Our labeled dataset and DSOR filter will be made available at https://bitbucket.org/autonomymtu/dsor_filter",0
"To replace human drivers, autonomous vehicles must face challenges posed by inclement weather. Rain and snow can cause LiDAR returns to become noisy, leading to inaccurate object detection. This article introduces the Winter Adverse Driving dataSet (WADS), which was collected in Michigan's Upper Peninsula and is the first multi-modal dataset to feature dense point-wise labeled sequential LiDAR scans in severe winter weather. We have labeled and will provide access to over 7 GB or 3.6 billion labeled LiDAR points out of a total of 26 TB of LiDAR and camera data collected. Additionally, we introduce the Dynamic Statistical Outlier Removal (DSOR) filter, a statistical PCL-based filter that can remove snow with higher recall than the current state of the art snow de-noising filter and is also 28% faster. The DSOR filter has lower time complexity than the state of the art, resulting in improved scalability. Our labeled dataset and DSOR filter will be available at https://bitbucket.org/autonomymtu/dsor_filter.",1
"Albeit current salient object detection (SOD) works have achieved fantastic progress, they are cast into the shade when it comes to the integrity of the predicted salient regions. We define the concept of integrity at both the micro and macro level. Specifically, at the micro level, the model should highlight all parts that belong to a certain salient object, while at the macro level, the model needs to discover all salient objects from the given image scene. To facilitate integrity learning for salient object detection, we design a novel Integrity Cognition Network (ICON), which explores three important components to learn strong integrity features. 1) Unlike the existing models that focus more on feature discriminability, we introduce a diverse feature aggregation (DFA) component to aggregate features with various receptive fields (i.e.,, kernel shape and context) and increase the feature diversity. Such diversity is the foundation for mining the integral salient objects. 2) Based on the DFA features, we introduce the integrity channel enhancement (ICE) component with the goal of enhancing feature channels that highlight the integral salient objects at the macro level, while suppressing the other distracting ones. 3) After extracting the enhanced features, the part-whole verification (PWV) method is employed to determine whether the part and whole object features have strong agreement. Such part-whole agreements can further improve the micro-level integrity for each salient object. To demonstrate the effectiveness of ICON, comprehensive experiments are conducted on seven challenging benchmarks, where promising results are achieved.",0
"Although significant progress has been made in salient object detection (SOD), the accuracy of the predicted salient regions is still lacking. The concept of integrity is defined at both micro and macro levels. At the micro level, the model should highlight all parts of a salient object, while at the macro level, it should discover all salient objects in an image scene. To address these issues, we introduce a novel Integrity Cognition Network (ICON) that consists of three key components. Firstly, we introduce a diverse feature aggregation (DFA) component to increase the feature diversity and enable mining of integral salient objects. Secondly, we use the integrity channel enhancement (ICE) component to enhance feature channels that highlight integral salient objects while suppressing distracting ones. Lastly, we employ the part-whole verification (PWV) method to improve the micro-level integrity for each salient object. Comprehensive experiments on seven challenging benchmarks demonstrate the effectiveness of ICON.",1
"Though 3D object detection from point clouds has achieved rapid progress in recent years, the lack of flexible and high-performance proposal refinement remains a great hurdle for existing state-of-the-art two-stage detectors. Previous works on refining 3D proposals have relied on human-designed components such as keypoints sampling, set abstraction and multi-scale feature fusion to produce powerful 3D object representations. Such methods, however, have limited ability to capture rich contextual dependencies among points. In this paper, we leverage the high-quality region proposal network and a Channel-wise Transformer architecture to constitute our two-stage 3D object detection framework (CT3D) with minimal hand-crafted design. The proposed CT3D simultaneously performs proposal-aware embedding and channel-wise context aggregation for the point features within each proposal. Specifically, CT3D uses proposal's keypoints for spatial contextual modelling and learns attention propagation in the encoding module, mapping the proposal to point embeddings. Next, a new channel-wise decoding module enriches the query-key interaction via channel-wise re-weighting to effectively merge multi-level contexts, which contributes to more accurate object predictions. Extensive experiments demonstrate that our CT3D method has superior performance and excellent scalability. Remarkably, CT3D achieves the AP of 81.77% in the moderate car category on the KITTI test 3D detection benchmark, outperforms state-of-the-art 3D detectors.",0
"While advances in 3D object detection from point clouds have been significant over the last few years, current state-of-the-art two-stage detectors face challenges due to inflexible and low-performance proposal refinement. Previous methods for refining 3D proposals have relied on human-designed components, such as keypoints sampling and set abstraction, which have limited ability to capture contextual dependencies among points. In this paper, we propose a two-stage 3D object detection framework (CT3D) that utilizes a high-quality region proposal network and a Channel-wise Transformer architecture with minimal hand-crafted design. CT3D performs proposal-aware embedding and channel-wise context aggregation for point features within each proposal, using the proposal's keypoints for spatial contextual modeling and attention propagation in the encoding module. The new channel-wise decoding module enriches the query-key interaction via channel-wise re-weighting to effectively merge multi-level contexts, resulting in more accurate object predictions. Our extensive experiments show that CT3D outperforms state-of-the-art 3D detectors, achieving an AP of 81.77% in the moderate car category on the KITTI test 3D detection benchmark, indicating superior performance and scalability.",1
"3D object detection and dense depth estimation are one of the most vital tasks in autonomous driving. Multiple sensor modalities can jointly attribute towards better robot perception, and to that end, we introduce a method for jointly training 3D object detection and monocular dense depth reconstruction neural networks. It takes as inputs, a LiDAR point-cloud, and a single RGB image during inference and produces object pose predictions as well as a densely reconstructed depth map. LiDAR point-cloud is converted into a set of voxels, and its features are extracted using 3D convolution layers, from which we regress object pose parameters. Corresponding RGB image features are extracted using another 2D convolutional neural network. We further use these combined features to predict a dense depth map. While our object detection is trained in a supervised manner, the depth prediction network is trained with both self-supervised and supervised loss functions. We also introduce a loss function, edge-preserving smooth loss, and show that this results in better depth estimation compared to the edge-aware smooth loss function, frequently used in depth prediction works.",0
"Autonomous driving heavily relies on 3D object detection and dense depth estimation, which are crucial tasks. A combination of sensor modalities can enhance robot perception, and our proposed method aims to jointly train 3D object detection and monocular dense depth reconstruction neural networks. During inference, the model takes a LiDAR point-cloud and an RGB image as inputs, producing object pose predictions and a densely reconstructed depth map. The LiDAR point-cloud is converted into voxels, and features are extracted using 3D convolution layers for object pose parameter regression. The corresponding RGB image features are extracted using a 2D convolutional neural network, and the combined features are used to predict a dense depth map. Our supervised object detection is paired with a self-supervised and supervised loss function for the depth prediction network. We also introduce the edge-preserving smooth loss function, which outperforms the edge-aware smooth loss function commonly used in depth prediction works, resulting in better depth estimation.",1
"Ultrasound (US) imaging is highly effective with regards to both cost and versatility in real-time diagnosis; however, determination of fetal gender by US scan in the early stages of pregnancy is also a cause of sex-selective abortion. This work proposes a deep learning object detection approach to accurately mask fetal gender in US images in order to increase the accessibility of the technology. We demonstrate how the YOLOv5L architecture exhibits superior performance relative to other object detection models on this task. Our model achieves 45.8% AP[0.5:0.95], 92% F1-score and 0.006 False Positive Per Image rate on our test set. Furthermore, we introduce a bounding box delay rule based on frame-to-frame structural similarity to reduce the false negative rate by 85%, further improving masking reliability.",0
"Although ultrasound imaging is cost-effective and versatile for real-time diagnosis, it can contribute to sex-selective abortion when used to determine fetal gender early on in pregnancy. To increase the accessibility of this technology, this study proposes a deep learning object detection method to accurately mask fetal gender in ultrasound images. The YOLOv5L architecture outperforms other models, achieving a 45.8% AP[0.5:0.95], 92% F1-score, and a 0.006 False Positive Per Image rate on the test set. Additionally, a bounding box delay rule based on frame-to-frame structural similarity reduces the false negative rate by 85%, improving masking reliability.",1
"In instance-level detection tasks (e.g., object detection), reducing input resolution is an easy option to improve runtime efficiency. However, this option traditionally hurts the detection performance much. This paper focuses on boosting the performance of low-resolution models by distilling knowledge from a high- or multi-resolution model. We first identify the challenge of applying knowledge distillation (KD) to teacher and student networks that act on different input resolutions. To tackle it, we explore the idea of spatially aligning feature maps between models of varying input resolutions by shifting feature pyramid positions and introduce aligned multi-scale training to train a multi-scale teacher that can distill its knowledge to a low-resolution student. Further, we propose crossing feature-level fusion to dynamically fuse teacher's multi-resolution features to guide the student better. On several instance-level detection tasks and datasets, the low-resolution models trained via our approach perform competitively with high-resolution models trained via conventional multi-scale training, while outperforming the latter's low-resolution models by 2.1% to 3.6% in terms of mAP. Our code is made publicly available at https://github.com/dvlab-research/MSAD.",0
"Reducing input resolution is a common approach to improve runtime efficiency in instance-level detection tasks, but it typically has a negative impact on detection performance. This paper aims to enhance the performance of low-resolution models by distilling knowledge from high- or multi-resolution models. However, spatially aligning feature maps between models of different input resolutions presents a challenge. To address this issue, we propose aligned multi-scale training, which involves shifting feature pyramid positions and training a multi-scale teacher to distill its knowledge to a low-resolution student. We also suggest crossing feature-level fusion to better guide the student. Our approach trains low-resolution models that perform competitively with high-resolution models trained through conventional multi-scale training, while outperforming the latter's low-resolution models by 2.1% to 3.6% in terms of mAP. The code is publicly available at https://github.com/dvlab-research/MSAD.",1
"A critical aspect of autonomous vehicles (AVs) is the object detection stage, which is increasingly being performed with sensor fusion models: multimodal 3D object detection models which utilize both 2D RGB image data and 3D data from a LIDAR sensor as inputs. In this work, we perform the first study to analyze the robustness of a high-performance, open source sensor fusion model architecture towards adversarial attacks and challenge the popular belief that the use of additional sensors automatically mitigate the risk of adversarial attacks. We find that despite the use of a LIDAR sensor, the model is vulnerable to our purposefully crafted image-based adversarial attacks including disappearance, universal patch, and spoofing. After identifying the underlying reason, we explore some potential defenses and provide some recommendations for improved sensor fusion models.",0
"The detection of objects is a crucial component of autonomous vehicles (AVs) and is increasingly being carried out using sensor fusion models. These models combine 2D RGB image data with 3D data from a LIDAR sensor to create multimodal 3D object detection models. In this study, we investigate the robustness of a high-performance, open-source sensor fusion model architecture to adversarial attacks. We challenge the widely held belief that the use of multiple sensors automatically reduces the risk of adversarial attacks. Our findings show that our purposefully crafted image-based adversarial attacks, such as disappearance, universal patch, and spoofing, can still compromise the model despite the use of a LIDAR sensor. We identify the underlying cause and suggest some potential defenses and recommendations for improving sensor fusion models.",1
"Prior work on 6-DoF object pose estimation has largely focused on instance-level processing, in which a textured CAD model is available for each object being detected. Category-level 6-DoF pose estimation represents an important step toward developing robotic vision systems that operate in unstructured, real-world scenarios. In this work, we propose a single-stage, keypoint-based approach for category-level object pose estimation that operates on unknown object instances within a known category using a single RGB image as input. The proposed network performs 2D object detection, detects 2D keypoints, estimates 6-DoF pose, and regresses relative bounding cuboid dimensions. These quantities are estimated in a sequential fashion, leveraging the recent idea of convGRU for propagating information from easier tasks to those that are more difficult. We favor simplicity in our design choices: generic cuboid vertex coordinates, single-stage network, and monocular RGB input. We conduct extensive experiments on the challenging Objectron benchmark, outperforming state-of-the-art methods on the 3D IoU metric (27.6% higher than the MobilePose single-stage approach and 7.1% higher than the related two-stage approach).",0
"The majority of previous research on 6-DoF object pose estimation has focused on instance-level processing, which requires a textured CAD model for each object being detected. However, category-level 6-DoF pose estimation is crucial for the advancement of robotic vision systems that can function in unstructured, real-world scenarios. In this study, we propose a simple keypoint-based method for category-level object pose estimation that utilizes a single RGB image as input, and can detect unknown object instances within a known category. Our proposed network performs 2D object detection, keypoint detection, 6-DoF pose estimation, and relative bounding cuboid dimension regression in a sequential manner. We employ generic cuboid vertex coordinates, a single-stage network, and monocular RGB input to keep our design choices uncomplicated. We conduct extensive experiments on the Objectron benchmark and surpass state-of-the-art approaches on the 3D IoU metric, outperforming the MobilePose single-stage approach by 27.6% and the related two-stage approach by 7.1%.",1
"Many objects do not appear frequently enough in complex scenes (e.g., certain handbags in living rooms) for training an accurate object detector, but are often found frequently by themselves (e.g., in product images). Yet, these object-centric images are not effectively leveraged for improving object detection in scene-centric images. In this paper, we propose Mosaic of Object-centric images as Scene-centric images (MosaicOS), a simple and novel framework that is surprisingly effective at tackling the challenges of long-tailed object detection. Keys to our approach are three-fold: (i) pseudo scene-centric image construction from object-centric images for mitigating domain differences, (ii) high-quality bounding box imputation using the object-centric images' class labels, and (iii) a multi-stage training procedure. On LVIS object detection (and instance segmentation), MosaicOS leads to a massive 60% (and 23%) relative improvement in average precision for rare object categories. We also show that our framework can be compatibly used with other existing approaches to achieve even further gains. Our pre-trained models are publicly available at https://github.com/czhang0528/MosaicOS/.",0
"Detecting certain objects accurately in complex scenes can be challenging due to their infrequent appearance, but they are often found frequently in isolation, such as in product images. Unfortunately, object-centric images are not effectively utilized to improve object detection in scene-centric images. To address this issue, we present Mosaic of Object-centric images as Scene-centric images (MosaicOS), which is a simple yet effective framework for long-tailed object detection. Our approach includes constructing pseudo scene-centric images from object-centric images to mitigate domain differences, high-quality bounding box imputation using the object-centric images' class labels, and a multi-stage training procedure. MosaicOS achieves a remarkable 60% (and 23%) relative improvement in average precision for rare object categories on LVIS object detection (and instance segmentation). Additionally, our framework can be used compatibly with other existing approaches for further gains. Our pre-trained models are publicly available at https://github.com/czhang0528/MosaicOS/.",1
"The classification and regression head are both indispensable components to build up a dense object detector, which are usually supervised by the same training samples and thus expected to have consistency with each other for detecting objects accurately in the detection pipeline. In this paper, we break the convention of the same training samples for these two heads in dense detectors and explore a novel supervisory paradigm, termed as Mutual Supervision (MuSu), to respectively and mutually assign training samples for the classification and regression head to ensure this consistency. MuSu defines training samples for the regression head mainly based on classification predicting scores and in turn, defines samples for the classification head based on localization scores from the regression head. Experimental results show that the convergence of detectors trained by this mutual supervision is guaranteed and the effectiveness of the proposed method is verified on the challenging MS COCO benchmark. We also find that tiling more anchors at the same location benefits detectors and leads to further improvements under this training scheme. We hope this work can inspire further researches on the interaction of the classification and regression task in detection and the supervision paradigm for detectors, especially separately for these two heads.",0
"To create an accurate dense object detector, both the classification and regression head are crucial components that are typically trained using the same samples. In this paper, we introduce a new supervisory approach called Mutual Supervision (MuSu), which assigns training samples for each head separately while ensuring consistency between them. MuSu uses classification predicting scores to define training samples for the regression head and localization scores from the regression head to define samples for the classification head. Our experimental results demonstrate that using MuSu guarantees convergence of detectors and improves performance on the challenging MS COCO benchmark. Additionally, we discovered that tiling more anchors at the same location further benefits detectors under this training scheme. This research inspires further exploration of the interaction between classification and regression tasks in detection and the supervision paradigm for detectors, particularly focusing on these two heads separately.",1
"We present Voxel Transformer (VoTr), a novel and effective voxel-based Transformer backbone for 3D object detection from point clouds. Conventional 3D convolutional backbones in voxel-based 3D detectors cannot efficiently capture large context information, which is crucial for object recognition and localization, owing to the limited receptive fields. In this paper, we resolve the problem by introducing a Transformer-based architecture that enables long-range relationships between voxels by self-attention. Given the fact that non-empty voxels are naturally sparse but numerous, directly applying standard Transformer on voxels is non-trivial. To this end, we propose the sparse voxel module and the submanifold voxel module, which can operate on the empty and non-empty voxel positions effectively. To further enlarge the attention range while maintaining comparable computational overhead to the convolutional counterparts, we propose two attention mechanisms for multi-head attention in those two modules: Local Attention and Dilated Attention, and we further propose Fast Voxel Query to accelerate the querying process in multi-head attention. VoTr contains a series of sparse and submanifold voxel modules and can be applied in most voxel-based detectors. Our proposed VoTr shows consistent improvement over the convolutional baselines while maintaining computational efficiency on the KITTI dataset and the Waymo Open dataset.",0
"VoTr is a new and efficient backbone for 3D object detection from point clouds that uses a Transformer-based architecture. The limited receptive fields of conventional 3D convolutional backbones in voxel-based 3D detectors make it difficult to capture large context information, which is crucial for object recognition and localization. In order to overcome this problem, we have introduced a Transformer-based architecture that allows for long-range relationships between voxels by self-attention. However, the fact that non-empty voxels are naturally sparse but numerous makes it challenging to apply the standard Transformer on voxels. To address this issue, we have proposed the sparse voxel module and the submanifold voxel module, which can effectively operate on empty and non-empty voxel positions. To further increase the attention range while maintaining comparable computational overhead to convolutional counterparts, we have suggested two attention mechanisms for multi-head attention in those two modules: Local Attention and Dilated Attention. Additionally, we have proposed Fast Voxel Query to accelerate the querying process in multi-head attention. VoTr consists of a series of sparse and submanifold voxel modules and can be used in most voxel-based detectors. We have demonstrated that VoTr outperforms convolutional baselines while maintaining computational efficiency on the KITTI dataset and the Waymo Open dataset.",1
"Unsupervised domain adaptation, which involves transferring knowledge from a label-rich source domain to an unlabeled target domain, can be used to substantially reduce annotation costs in the field of object detection. In this study, we demonstrate that adversarial training in the source domain can be employed as a new approach for unsupervised domain adaptation. Specifically, we establish that adversarially trained detectors achieve improved detection performance in target domains that are significantly shifted from source domains. This phenomenon is attributed to the fact that adversarially trained detectors can be used to extract robust features that are in alignment with human perception and worth transferring across domains while discarding domain-specific non-robust features. In addition, we propose a method that combines adversarial training and feature alignment to ensure the improved alignment of robust features with the target domain. We conduct experiments on four benchmark datasets and confirm the effectiveness of our proposed approach on large domain shifts from real to artistic images. Compared to the baseline models, the adversarially trained detectors improve the mean average precision by up to 7.7\%, and further by up to 11.8\% when feature alignments are incorporated.",0
"The use of unsupervised domain adaptation, which transfers knowledge from a label-rich source domain to an unlabeled target domain, can lead to significant savings in annotation costs for object detection. Adversarial training in the source domain is explored as a new method for unsupervised domain adaptation. The study finds that adversarially trained detectors perform better in target domains that differ significantly from source domains by extracting robust features that align with human perception and discarding domain-specific non-robust features. To improve alignment, a method combining adversarial training and feature alignment is proposed and tested on four benchmark datasets. The results show that the proposed approach is effective, particularly when large domain shifts from real to artistic images are involved, with mean average precision improving by up to 7.7\% with adversarial training alone and up to 11.8\% when feature alignment is included, compared to baseline models.",1
"3D scene understanding from point clouds plays a vital role for various robotic applications. Unfortunately, current state-of-the-art methods use separate neural networks for different tasks like object detection or room layout estimation. Such a scheme has two limitations: 1) Storing and running several networks for different tasks are expensive for typical robotic platforms. 2) The intrinsic structure of separate outputs are ignored and potentially violated. To this end, we propose the first transformer architecture that predicts 3D objects and layouts simultaneously, using point cloud inputs. Unlike existing methods that either estimate layout keypoints or edges, we directly parameterize room layout as a set of quads. As such, the proposed architecture is termed as P(oint)Q(uad)-Transformer. Along with the novel quad representation, we propose a tailored physical constraint loss function that discourages object-layout interference. The quantitative and qualitative evaluations on the public benchmark ScanNet show that the proposed PQ-Transformer succeeds to jointly parse 3D objects and layouts, running at a quasi-real-time (8.91 FPS) rate without efficiency-oriented optimization. Moreover, the new physical constraint loss can improve strong baselines, and the F1-score of the room layout is significantly promoted from 37.9% to 57.9%.",0
"The significance of 3D scene understanding from point clouds for various robotic applications cannot be overstated. However, current state-of-the-art techniques employ distinct neural networks for different tasks such as object detection and room layout estimation, which has two drawbacks. Firstly, it is costly to store and execute multiple networks for different tasks on typical robotic platforms. Secondly, the inherent structure of separate outputs is disregarded and could be compromised. In response to this, we propose the P(oint)Q(uad)-Transformer, the first transformer architecture that utilizes point cloud inputs to predict 3D objects and layouts simultaneously. Unlike existing methods that estimate layout keypoints or edges, we represent room layout as a group of quads. To prevent object-layout interference, we introduce a customized physical constraint loss function. The PQ-Transformer has been successfully evaluated on the ScanNet benchmark, demonstrating the ability to jointly parse 3D objects and layouts at a quasi-real-time rate of 8.91 FPS without efficiency-oriented optimization. Additionally, the new physical constraint loss can enhance strong baselines, and the F1-score of the room layout is significantly improved from 37.9% to 57.9%.",1
"Video object detection is challenging in the presence of appearance deterioration in certain video frames. Therefore, it is a natural choice to aggregate temporal information from other frames of the same video into the current frame. However, RoI Align, as one of the most core procedures of video detectors, still remains extracting features from a single-frame feature map for proposals, making the extracted RoI features lack temporal information from videos. In this work, considering the features of the same object instance are highly similar among frames in a video, a novel Temporal RoI Align operator is proposed to extract features from other frames feature maps for current frame proposals by utilizing feature similarity. The proposed Temporal RoI Align operator can extract temporal information from the entire video for proposals. We integrate it into single-frame video detectors and other state-of-the-art video detectors, and conduct quantitative experiments to demonstrate that the proposed Temporal RoI Align operator can consistently and significantly boost the performance. Besides, the proposed Temporal RoI Align can also be applied into video instance segmentation. Codes are available at https://github.com/open-mmlab/mmtracking",0
"Detecting objects in videos is difficult when certain frames have deteriorated appearances. To overcome this challenge, aggregating temporal information from other frames of the same video into the current frame is a natural approach. However, the RoI Align procedure, which is a core component of video detectors, still only extracts features from a single-frame feature map for proposals, resulting in a lack of temporal information from videos. To address this issue, we propose a novel Temporal RoI Align operator that extracts features from other frames' feature maps for current frame proposals by utilizing feature similarity, given that features of the same object instance are highly similar across frames in a video. Our proposed Temporal RoI Align operator extracts temporal information from the entire video for proposals and can be integrated into single-frame video detectors and other state-of-the-art video detectors. We conducted quantitative experiments to show that the proposed Temporal RoI Align operator consistently and significantly boosts performance. Additionally, the proposed Temporal RoI Align can also be applied to video instance segmentation. Our code is available at https://github.com/open-mmlab/mmtracking.",1
"In this paper, we propose an approach to improve image captioning solutions for images with novel objects that do not have caption labels in the training dataset. Our approach is agnostic to model architecture, and primarily focuses on training technique that uses existing fully paired image-caption data and the images with only the novel object detection labels (partially paired data). We create synthetic paired captioning data for these novel objects by leveraging context from existing image-caption pairs. We further re-use these partially paired images with novel objects to create pseudo-label captions that are used to fine-tune the captioning model. Using a popular captioning model (Up-Down) as baseline, our approach achieves state-of-the-art results on held-out MS COCO out-of-domain test split, and improves F1 metric and CIDEr for novel object images by 75.8 and 26.6 points respectively, compared to baseline model that does not use partially paired images during training.",0
"The objective of this paper is to present a method to enhance image captioning solutions for images that have novel objects without any caption labels in the training dataset. Our approach is not model-dependent and concentrates on a training technique that employs fully paired image-caption data along with partially paired data consisting of images with only novel object detection labels. To generate synthetic paired captioning data for the novel objects, we make use of context from existing image-caption pairs. Additionally, we reuse the partially paired images with novel objects to create pseudo-label captions which are utilized to fine-tune the captioning model. Our proposed approach, which uses the popular captioning model (Up-Down) as a baseline, achieves state-of-the-art outcomes on the held-out MS COCO out-of-domain test split. Furthermore, it enhances the F1 metric and CIDEr for novel object images by 75.8 and 26.6 points respectively, when compared to the baseline model that does not utilize partially paired images during training.",1
"Multitask learning is a common approach in machine learning, which allows to train multiple objectives with a shared architecture. It has been shown that by training multiple tasks together inference time and compute resources can be saved, while the objectives performance remains on a similar or even higher level. However, in perception related multitask networks only closely related tasks can be found, such as object detection, instance and semantic segmentation or depth estimation. Multitask networks with diverse tasks and their effects with respect to efficiency on one another are not well studied. In this paper we augment the CenterNet anchor-free approach for training multiple diverse perception related tasks together, including the task of object detection and semantic segmentation as well as human pose estimation. We refer to this DNN as Multitask-CenterNet (MCN). Additionally, we study different MCN settings for efficiency. The MCN can perform several tasks at once while maintaining, and in some cases even exceeding, the performance values of its corresponding single task networks. More importantly, the MCN architecture decreases inference time and reduces network size when compared to a composition of single task networks.",0
"Machine learning commonly employs multitask learning, which trains multiple objectives using a shared architecture. This technique has proven to save inference time and computational resources, while maintaining or improving performance. However, perception-related multitask networks typically only involve closely related tasks, such as object detection, instance and semantic segmentation, or depth estimation. Multitask networks involving diverse tasks and their impact on efficiency are poorly researched. In this study, we enhance the CenterNet anchor-free approach to train multiple perception-related tasks simultaneously, including object detection, semantic segmentation, and human pose estimation, creating the Multitask-CenterNet (MCN) deep neural network. We also investigate various MCN settings for efficiency. The MCN achieves several tasks while maintaining, and sometimes even surpassing, the performance of its corresponding single-task networks. Moreover, the MCN architecture reduces inference time and network size compared to a composition of single-task networks.",1
"Recently, face recognition in the wild has achieved remarkable success and one key engine is the increasing size of training data. For example, the largest face dataset, WebFace42M contains about 2 million identities and 42 million faces. However, a massive number of faces raise the constraints in training time, computing resources, and memory cost. The current research on this problem mainly focuses on designing an efficient Fully-connected layer (FC) to reduce GPU memory consumption caused by a large number of identities. In this work, we relax these constraints by resolving the redundancy problem of the up-to-date face datasets caused by the greedily collecting operation (i.e. the core-set selection perspective). As the first attempt in this perspective on the face recognition problem, we find that existing methods are limited in both performance and efficiency. For superior cost-efficiency, we contribute a novel filtering strategy dubbed Face-NMS. Face-NMS works on feature space and simultaneously considers the local and global sparsity in generating core sets. In practice, Face-NMS is analogous to Non-Maximum Suppression (NMS) in the object detection community. It ranks the faces by their potential contribution to the overall sparsity and filters out the superfluous face in the pairs with high similarity for local sparsity. With respect to the efficiency aspect, Face-NMS accelerates the whole pipeline by applying a smaller but sufficient proxy dataset in training the proxy model. As a result, with Face-NMS, we successfully scale down the WebFace42M dataset to 60% while retaining its performance on the main benchmarks, offering a 40% resource-saving and 1.64 times acceleration. The code is publicly available for reference at https://github.com/HuangJunJie2017/Face-NMS.",0
"The success of face recognition in the wild can be attributed to the increased size of training data, such as the WebFace42M dataset, which boasts 2 million identities and 42 million faces. However, the sheer volume of faces presents challenges in terms of training time, computing resources, and memory cost. To address this issue, current research primarily focuses on creating efficient Fully-connected layers (FC) to reduce GPU memory consumption. In this study, we take a different approach by tackling the redundancy problem of face datasets caused by a greedy collection process. Our novel filtering strategy, Face-NMS, operates on the feature space and considers both local and global sparsity to generate core sets. This method ranks faces based on their potential contribution to overall sparsity and filters out superfluous faces with high similarity for local sparsity. By using a smaller proxy dataset in training the proxy model and implementing Face-NMS, we successfully scaled down the WebFace42M dataset to 60% while maintaining performance on main benchmarks, resulting in a 40% resource-saving and 1.64 times acceleration. The code is available for reference at https://github.com/HuangJunJie2017/Face-NMS.",1
"Recent studies indicate that hierarchical Vision Transformer with a macro architecture of interleaved non-overlapped window-based self-attention \& shifted-window operation is able to achieve state-of-the-art performance in various visual recognition tasks, and challenges the ubiquitous convolutional neural networks (CNNs) using densely slid kernels. Most follow-up works attempt to replace the shifted-window operation with other kinds of cross-window communication paradigms, while treating self-attention as the de-facto standard for window-based information aggregation. In this manuscript, we question whether self-attention is the only choice for hierarchical Vision Transformer to attain strong performance, and the effects of different kinds of cross-window communication. To this end, we replace self-attention layers with embarrassingly simple linear mapping layers, and the resulting proof-of-concept architecture termed as LinMapper can achieve very strong performance in ImageNet-1k image recognition. Moreover, we find that LinMapper is able to better leverage the pre-trained representations from image recognition and demonstrates excellent transfer learning properties on downstream dense prediction tasks such as object detection and instance segmentation. We also experiment with other alternatives to self-attention for content aggregation inside each non-overlapped window under different cross-window communication approaches, which all give similar competitive results. Our study reveals that the \textbf{macro architecture} of Swin model families, other than specific aggregation layers or specific means of cross-window communication, may be more responsible for its strong performance and is the real challenger to the ubiquitous CNN's dense sliding window paradigm. Code and models will be publicly available to facilitate future research.",0
"Recent research suggests that hierarchical Vision Transformer, which uses interleaved non-overlapped window-based self-attention and shifted-window operation, can outperform convolutional neural networks in various visual recognition tasks. While many subsequent studies have attempted to replace shifted-window operation with other cross-window communication methods, self-attention has remained the standard for window-based information aggregation. In this paper, we challenge the assumption that self-attention is the only way for hierarchical Vision Transformer to achieve strong performance and examine the impact of different cross-window communication approaches. We introduce a proof-of-concept architecture called LinMapper, which replaces self-attention layers with simple linear mapping layers and achieves impressive results in ImageNet-1k image recognition. LinMapper also demonstrates strong transfer learning capabilities for downstream tasks such as object detection and instance segmentation. Our study shows that the macro architecture of the Swin model families, rather than specific aggregation layers or cross-window communication methods, is responsible for their excellent performance and is a viable alternative to CNN's dense sliding window paradigm. We will make our code and models publicly available to facilitate further research.",1
"The reasonable employment of RGB and depth data show great significance in promoting the development of computer vision tasks and robot-environment interaction. However, there are different advantages and disadvantages in the early and late fusion of the two types of data. Besides, due to the diversity of object information, using a single type of data in a specific scenario tends to result in semantic misleading. Based on the above considerations, we propose an adaptively-cooperative fusion network (ACFNet) with ResinRes structure for salient object detection. This structure is designed to flexibly utilize the advantages of feature fusion in early and late stages. Secondly, an adaptively-cooperative semantic guidance (ACG) scheme is designed to suppress inaccurate features in the guidance phase. Further, we proposed a type-based attention module (TAM) to optimize the network and enhance the multi-scale perception of different objects. For different objects, the features generated by different types of convolution are enhanced or suppressed by the gated mechanism for segmentation optimization. ACG and TAM optimize the transfer of feature streams according to their data attributes and convolution attributes, respectively. Sufficient experiments conducted on RGB-D SOD datasets illustrate that the proposed network performs favorably against 18 state-of-the-art algorithms.",0
"The utilization of RGB and depth data is crucial for the advancement of computer vision tasks and interaction between robots and their environment. However, early and late fusion of the two data types have their own benefits and drawbacks. Additionally, using only one type of data in specific scenarios can lead to misleading interpretations. To address these issues, we propose the Adaptively-Cooperative Fusion Network (ACFNet) with ResinRes architecture for salient object detection. This design allows for flexible utilization of feature fusion at both early and late stages. We also introduce an Adaptively-Cooperative Semantic Guidance (ACG) scheme to suppress inaccurate features during the guidance phase. Furthermore, we present a Type-Based Attention Module (TAM) to optimize the network and enhance multi-scale perception of different objects. The features generated by different types of convolution are enhanced or suppressed through the gated mechanism for segmentation optimization. ACG and TAM optimize the transfer of feature streams based on their data and convolution attributes, respectively. Our experiments on RGB-D SOD datasets demonstrate that the proposed network outperforms 18 state-of-the-art algorithms.",1
"In this study, we introduce a measure for machine perception, inspired by the concept of Just Noticeable Difference (JND) of human perception. Based on this measure, we suggest an adversarial image generation algorithm, which iteratively distorts an image by an additive noise until the model detects the change in the image by outputting a false label. The noise added to the original image is defined as the gradient of the cost function of the model. A novel cost function is defined to explicitly minimize the amount of perturbation applied to the input image while enforcing the perceptual similarity between the adversarial and input images. For this purpose, the cost function is regularized by the well-known total variation and bounded range terms to meet the natural appearance of the adversarial image. We evaluate the adversarial images generated by our algorithm both qualitatively and quantitatively on CIFAR10, ImageNet, and MS COCO datasets. Our experiments on image classification and object detection tasks show that adversarial images generated by our JND method are both more successful in deceiving the recognition/detection models and less perturbed compared to the images generated by the state-of-the-art methods, namely, FGV, FSGM, and DeepFool methods.",0
"This study presents a new way to measure machine perception, which is inspired by the concept of human perception's Just Noticeable Difference (JND). The study proposes an adversarial image generation algorithm that distorts images using additive noise until the model outputs a false label. The noise is defined as the gradient of the model's cost function, and the cost function is designed to minimize perturbation while enforcing perceptual similarity between the adversarial and input images. The cost function is regularized using total variation and bounded range terms to maintain the natural appearance of the adversarial image. The algorithm is evaluated on CIFAR10, ImageNet, and MS COCO datasets, and the results show that our JND method generates adversarial images that are more successful in deceiving recognition/detection models and less perturbed compared to state-of-the-art methods (FGV, FSGM, and DeepFool).",1
"Since many safety-critical systems, such as surgical robots and autonomous driving cars operate in unstable environments with sensor noise and incomplete data, it is desirable for object detectors to take the localization uncertainty into account. However, there are several limitations of the existing uncertainty estimation methods for anchor-based object detection. 1) They model the uncertainty of the heterogeneous object properties with different characteristics and scales, such as location (center point) and scale (width, height), which could be difficult to estimate. 2) They model box offsets as Gaussian distributions, which is not compatible with the ground truth bounding boxes that follow the Dirac delta distribution. 3) Since anchor-based methods are sensitive to anchor hyperparameters, the localization uncertainty for them could be also highly sensitive to the choice of hyperparameters as well. To tackle these limitations, we propose a new localization uncertainty estimation method called UAD for anchor-free object detection. Our method captures the uncertainty in four directions of box offsets~(left, right, top, bottom) that are homogeneous, so that it can tell which direction is uncertain, and provides a quantitative value of uncertainty in $[0, 1]$. To enable such uncertainty estimation, we design a new uncertainty loss, negative power log-likelihood loss, to measure the localization uncertainty by weighting the likelihood loss by its IoU, which alleviates the model misspecification problem. Furthermore, we propose an uncertainty-aware focal loss for reflecting the estimated uncertainty to the classification score. Experimental results on COCO datasets demonstrate that our method significantly improves FCOS, by up to 1.8 points, without sacrificing computational efficiency.",0
"Object detectors for safety-critical systems, like surgical robots and autonomous cars, need to consider localization uncertainty due to unstable environments with sensor noise and incomplete data. However, existing uncertainty estimation methods for anchor-based object detection have limitations. These include difficulty estimating the uncertainty of heterogeneous object properties, modeling box offsets as Gaussian distributions incompatible with ground truth bounding boxes, and sensitivity to anchor hyperparameters. To address these limitations, we propose a new localization uncertainty estimation method called UAD for anchor-free object detection. Our method captures uncertainty in four homogeneous directions of box offsets and provides a quantitative value of uncertainty in the range of 0 to 1. We also design a new uncertainty loss, negative power log-likelihood loss, to measure localization uncertainty and propose an uncertainty-aware focal loss for reflecting estimated uncertainty to the classification score. Experimental results on COCO datasets show that our method significantly improves FCOS by up to 1.8 points without sacrificing computational efficiency.",1
"Machine learning has been utilized to perform tasks in many different domains such as classification, object detection, image segmentation and natural language analysis. Data labeling has always been one of the most important tasks in machine learning. However, labeling large amounts of data increases the monetary cost in machine learning. As a result, researchers started to focus on reducing data annotation and labeling costs. Transfer learning was designed and widely used as an efficient approach that can reasonably reduce the negative impact of limited data, which in turn, reduces the data preparation cost. Even transferring previous knowledge from a source domain reduces the amount of data needed in a target domain. However, large amounts of annotated data are still demanded to build robust models and improve the prediction accuracy of the model. Therefore, researchers started to pay more attention on auto annotation and labeling. In this survey paper, we provide a review of previous techniques that focuses on optimized data annotation and labeling for video, audio, and text data.",0
"The utilization of machine learning has extended to a variety of domains, including classification, object detection, image segmentation, and natural language analysis. Data labeling is a crucial task in machine learning, but it often results in high costs. To address this issue, researchers have turned their attention to reducing data annotation and labeling costs, with transfer learning being a widely used and efficient approach. This approach involves transferring previous knowledge from a source domain to reduce the amount of data needed in a target domain. However, to build robust models and improve prediction accuracy, large amounts of annotated data are still required. As a result, researchers have begun to focus on auto annotation and labeling. This survey paper provides a comprehensive review of previous techniques that optimize data annotation and labeling for video, audio, and text data.",1
"Monocular 3D detection currently struggles with extremely lower detection rates compared to LiDAR-based methods. The poor accuracy is mainly caused by the absence of accurate location cues due to the ill-posed nature of monocular imagery. LiDAR point clouds, which provide precise spatial measurement, can offer beneficial information for the training of monocular methods. To make use of LiDAR point clouds, prior works project them to form depth map labels, subsequently training a dense depth estimator to extract explicit location features. This indirect and complicated way introduces intermediate products, i.e., depth map predictions, taking much computation costs as well as leading to suboptimal performances. In this paper, we propose LPCG (LiDAR point cloud guided monocular 3D object detection), which is a general framework for guiding the training of monocular 3D detectors with LiDAR point clouds. Specifically, we use LiDAR point clouds to generate pseudo labels, allowing monocular 3D detectors to benefit from easy-collected massive unlabeled data. LPCG works well under both supervised and unsupervised setups. Thanks to a general design, LPCG can be plugged into any monocular 3D detector, significantly boosting the performance. As a result, we take the first place on KITTI monocular 3D/BEV (bird's-eye-view) detection benchmark with a considerable margin. The code will be made publicly available soon.",0
"Compared to LiDAR-based methods, monocular 3D detection presently has significantly lower detection rates, largely due to the lack of accurate location cues in monocular imagery. To address this issue, previous works have projected LiDAR point clouds into depth maps to train a dense depth estimator, which extracts explicit location features. However, this indirect and complicated approach introduces intermediate products and results in high computation costs and suboptimal performances. In this paper, we propose LPCG, a general framework that guides the training of monocular 3D detectors with LiDAR point clouds. LPCG generates pseudo labels from LiDAR point clouds, allowing monocular 3D detectors to benefit from easily collected, massive unlabeled data. LPCG works well under both supervised and unsupervised setups and can be integrated into any monocular 3D detector, significantly improving performance. As a result, we achieved first place on the KITTI monocular 3D/BEV detection benchmark with a significant margin. The code for LPCG will be publicly available soon.",1
"3D object detector based on Hough voting achieves great success and derives many follow-up works. Despite constantly refreshing the detection accuracy, these works suffer from handcrafted components used to eliminate redundant boxes, and thus are non-end-to-end and time-consuming. In this work, we propose a suppress-and-refine framework to remove these handcrafted components. To fully utilize full-resolution information and achieve real-time speed, it directly consumes feature points and redundant 3D proposals. Specifically, it first suppresses noisy 3D feature points and then feeds them to 3D proposals for the following RoI-aware refinement. With the gating mechanism to build fine proposal features and the self-attention mechanism to model relationships, our method can produce high-quality predictions with a small computation budget in an end-to-end manner. To this end, we present the first fully end-to-end 3D detector, SRDet, on the basis of VoteNet. It achieves state-of-the-art performance on the challenging ScanNetV2 and SUN RGB-D datasets with the fastest speed ever. Our code will be available at https://github.com/ZJULearning/SRDet.",0
"The Hough voting-based 3D object detector has been successful and has inspired numerous subsequent works. However, these works have been hindered by the use of handcrafted components to eliminate redundant boxes, resulting in non-end-to-end and time-consuming processes, despite constant improvements in detection accuracy. In this study, we propose a suppress-and-refine framework that eliminates the need for handcrafted components. Our method utilizes full-resolution information and directly consumes feature points and redundant 3D proposals to achieve real-time speed. The framework first suppresses noisy 3D feature points and then feeds them to 3D proposals for RoI-aware refinement. Our method employs a gating mechanism to build fine proposal features and a self-attention mechanism to model relationships, resulting in high-quality predictions with a small computation budget in an end-to-end manner. As a result, we introduce the first fully end-to-end 3D detector, SRDet, based on VoteNet, which achieves state-of-the-art performance on the challenging ScanNetV2 and SUN RGB-D datasets with the fastest speed ever. Our code can be found at https://github.com/ZJULearning/SRDet.",1
"One-stage object detectors rely on a point feature to predict the detection results. However, the point feature often lacks the information of the whole object, thereby leading to a misalignment between the object and the point feature. Meanwhile, the classification and regression tasks are sensitive to different object regions, but their features are spatially aligned. Both of these two problems hinder the detection performance. In order to solve these two problems, we propose a simple and plug-in operator that can generate aligned and disentangled features for each task, respectively, without breaking the fully convolutional manner. By predicting two task-aware point sets that are located in each sensitive region, the proposed operator can align the point feature with the object and disentangle the two tasks from the spatial dimension. We also reveal an interesting finding of the opposite effect of the long-range skip connection for classification and regression. On the basis of the Object-Aligned and Task-disentangled operator (OAT), we propose OAT-Net, which explicitly exploits point-set features for accurate detection results. Extensive experiments on the MS-COCO dataset show that OAT can consistently boost different state-of-the-art one-stage detectors by $\sim$2 AP. Notably, OAT-Net with Res2Net-101-DCN backbone achieves 53.7 AP on the COCO test-dev.",0
"Point features utilized by one-stage object detectors often lack information about the entire object, which can result in misalignment between the object and the point feature. Additionally, the classification and regression tasks depend on different object regions, but their features are aligned spatially. Both of these issues can negatively impact detection performance. To address these problems, we propose a straightforward and plug-in operator that can produce aligned and disentangled features for each task without disrupting the fully convolutional manner. This operator predicts two task-aware point sets located in the sensitive regions, allowing for alignment of the point feature with the object and disentanglement of the two tasks from the spatial dimension. We also discovered that long-range skip connections have opposite effects on classification and regression. Based on the Object-Aligned and Task-disentangled operator (OAT), we introduce OAT-Net, which utilizes point-set features for improved detection accuracy. Extensive experiments on the MS-COCO dataset demonstrate that OAT consistently enhances various state-of-the-art one-stage detectors by approximately 2 AP. Notably, OAT-Net with a Res2Net-101-DCN backbone achieves 53.7 AP on the COCO test-dev.",1
"Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks e.g., Long short-term memory (LSTM). Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers i.e., self-attention, large-scale pre-training, and bidirectional encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization) and 3D analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works.",0
"The remarkable performance of Transformer models on language tasks has piqued the interest of the computer vision community, who are now exploring their potential for solving vision-related problems. Transformers possess several advantages, including the ability to model long dependencies between input sequence elements and support parallel processing. Unlike recurrent networks such as Long short-term memory (LSTM), Transformers require minimal inductive biases for their design and are ideal as set-functions. Moreover, they can process multiple modalities with ease and are scalable to large capacity networks and massive datasets. As a result, Transformer networks have made exciting progress in various vision tasks. This survey provides a comprehensive overview of Transformer models in computer vision, starting with an introduction to the fundamental concepts behind their success, such as self-attention, large-scale pre-training, and bidirectional encoding. The survey then covers the extensive applications of Transformers in vision, including image classification, object detection, action recognition, and segmentation, as well as generative modeling, multi-modal tasks, video processing, low-level vision, and 3D analysis. The strengths and limitations of popular techniques are compared in terms of architectural design and their experimental value. Finally, the survey presents an analysis of open research directions and possible future works.",1
"Due to the large success in object detection and instance segmentation, Mask R-CNN attracts great attention and is widely adopted as a strong baseline for arbitrary-shaped scene text detection and spotting. However, two issues remain to be settled. The first is dense text case, which is easy to be neglected but quite practical. There may exist multiple instances in one proposal, which makes it difficult for the mask head to distinguish different instances and degrades the performance. In this work, we argue that the performance degradation results from the learning confusion issue in the mask head. We propose to use an MLP decoder instead of the ""deconv-conv"" decoder in the mask head, which alleviates the issue and promotes robustness significantly. And we propose instance-aware mask learning in which the mask head learns to predict the shape of the whole instance rather than classify each pixel to text or non-text. With instance-aware mask learning, the mask branch can learn separated and compact masks. The second is that due to large variations in scale and aspect ratio, RPN needs complicated anchor settings, making it hard to maintain and transfer across different datasets. To settle this issue, we propose an adaptive label assignment in which all instances especially those with extreme aspect ratios are guaranteed to be associated with enough anchors. Equipped with these components, the proposed method named MAYOR achieves state-of-the-art performance on five benchmarks including DAST1500, MSRA-TD500, ICDAR2015, CTW1500, and Total-Text.",0
"Mask R-CNN has gained significant attention and is widely accepted as a strong baseline for arbitrary-shaped scene text detection and spotting due to its success in object detection and instance segmentation. However, two issues remain unresolved. The first issue is the dense text case, which can be easily overlooked but is highly practical. Multiple instances may exist in one proposal, making it challenging for the mask head to distinguish different instances and leading to performance degradation. Our research shows that this issue is caused by the learning confusion problem in the mask head. We propose using an MLP decoder instead of the ""deconv-conv"" decoder in the mask head, which significantly improves robustness. We also propose instance-aware mask learning, which enables the mask head to learn to predict the shape of the entire instance instead of classifying each pixel as text or non-text. This approach allows the mask branch to learn compact masks. The second issue is that the RPN requires complex anchor settings due to variations in scale and aspect ratio, making it difficult to maintain and transfer across different datasets. To address this, we suggest an adaptive label assignment that ensures all instances, particularly those with extreme aspect ratios, are associated with enough anchors. By incorporating these elements, our proposed method, called MAYOR, achieves state-of-the-art performance on five benchmarks, including DAST1500, MSRA-TD500, ICDAR2015, CTW1500, and Total-Text.",1
"Conventional RGB-D salient object detection methods aim to leverage depth as complementary information to find the salient regions in both modalities. However, the salient object detection results heavily rely on the quality of captured depth data which sometimes are unavailable. In this work, we make the first attempt to solve the RGB-D salient object detection problem with a novel depth-awareness framework. This framework only relies on RGB data in the testing phase, utilizing captured depth data as supervision for representation learning. To construct our framework as well as achieving accurate salient detection results, we propose a Ubiquitous Target Awareness (UTA) network to solve three important challenges in RGB-D SOD task: 1) a depth awareness module to excavate depth information and to mine ambiguous regions via adaptive depth-error weights, 2) a spatial-aware cross-modal interaction and a channel-aware cross-level interaction, exploiting the low-level boundary cues and amplifying high-level salient channels, and 3) a gated multi-scale predictor module to perceive the object saliency in different contextual scales. Besides its high performance, our proposed UTA network is depth-free for inference and runs in real-time with 43 FPS. Experimental evidence demonstrates that our proposed network not only surpasses the state-of-the-art methods on five public RGB-D SOD benchmarks by a large margin, but also verifies its extensibility on five public RGB SOD benchmarks.",0
"The traditional approach to RGB-D salient object detection utilizes depth information as supplementary data to identify salient regions in both modalities. However, the accuracy of salient object detection outcomes heavily depends on the quality of the captured depth data, which may not always be available. This study proposes a novel depth-awareness framework that solely depends on RGB data during the testing phase, while using captured depth data for representation learning. To address the challenges of RGB-D SOD, a Ubiquitous Target Awareness (UTA) network is proposed, which comprises a depth awareness module, a cross-modal interaction module, and a multi-scale predictor module. The UTA network achieves high accuracy, runs in real-time with 43 FPS, and outperforms existing methods on five public RGB-D SOD benchmarks. Furthermore, the network's extensibility is verified on five public RGB SOD benchmarks.",1
"Online continual learning from data streams in dynamic environments is a critical direction in the computer vision field. However, realistic benchmarks and fundamental studies in this line are still missing. To bridge the gap, we present a new online continual object detection benchmark with an egocentric video dataset, Objects Around Krishna (OAK). OAK adopts the KrishnaCAM videos, an ego-centric video stream collected over nine months by a graduate student. OAK provides exhaustive bounding box annotations of 80 video snippets (~17.5 hours) for 105 object categories in outdoor scenes. The emergence of new object categories in our benchmark follows a pattern similar to what a single person might see in their day-to-day life. The dataset also captures the natural distribution shifts as the person travels to different places. These egocentric long-running videos provide a realistic playground for continual learning algorithms, especially in online embodied settings. We also introduce new evaluation metrics to evaluate the model performance and catastrophic forgetting and provide baseline studies for online continual object detection. We believe this benchmark will pose new exciting challenges for learning from non-stationary data in continual learning. The OAK dataset and the associated benchmark are released at https://oakdata.github.io/.",0
"Continual learning from data streams in dynamic environments is a crucial aspect of the computer vision field. However, there is still a lack of realistic benchmarks and fundamental studies in this area. To address this gap, we have developed a novel online continual object detection benchmark using an egocentric video dataset called Objects Around Krishna (OAK). This dataset comprises 80 video snippets (~17.5 hours) of 105 object categories in outdoor scenes, with exhaustive bounding box annotations. It is based on the KrishnaCAM videos, an ego-centric video stream collected over nine months by a graduate student. The emergence of new object categories in our benchmark follows a pattern similar to daily life, and the dataset captures natural distribution shifts as the person travels to different places. These long-running videos provide a realistic environment for continual learning algorithms, especially in online embodied settings. We have also introduced new evaluation metrics to assess model performance and catastrophic forgetting, and provided baseline studies for online continual object detection. We believe that this benchmark will present exciting challenges for learning from non-stationary data in continual learning. The OAK dataset and benchmark are now available at https://oakdata.github.io/.",1
"According to recent studies, commonly used computer vision datasets contain about 4% of label errors. For example, the COCO dataset is known for its high level of noise in data labels, which limits its use for training robust neural deep architectures in a real-world scenario. To model such a noise, in this paper we have proposed the homoscedastic aleatoric uncertainty estimation, and present a series of novel loss functions to address the problem of image object detection at scale. Specifically, the proposed functions are based on Bayesian inference and we have incorporated them into the common community-adopted object detection deep learning architecture RetinaNet. We have also shown that modeling of homoscedastic aleatoric uncertainty using our novel functions allows to increase the model interpretability and to improve the object detection performance being evaluated on the COCO dataset.",0
"Recent studies have revealed that computer vision datasets frequently contain label errors, with an average of 4%. The COCO dataset is particularly susceptible to noise in data labels, which hinders its ability to train robust neural deep architectures for real-world applications. To address this issue, the authors of this paper have introduced homoscedastic aleatoric uncertainty estimation to model such noise. They have also developed a set of innovative loss functions based on Bayesian inference, which have been integrated into the RetinaNet object detection deep learning architecture commonly used by the community. The authors demonstrate that by utilizing their novel functions to model homoscedastic aleatoric uncertainty, they can improve model interpretability and enhance object detection performance on the COCO dataset.",1
"Knowledge distillation (KD) is an effective framework that aims to transfer meaningful information from a large teacher to a smaller student. Generally, KD often involves how to define and transfer knowledge. Previous KD methods often focus on mining various forms of knowledge, for example, feature maps and refined information. However, the knowledge is derived from the primary supervised task and thus is highly task-specific. Motivated by the recent success of self-supervised representation learning, we propose an auxiliary self-supervision augmented task to guide networks to learn more meaningful features. Therefore, we can derive soft self-supervision augmented distributions as richer dark knowledge from this task for KD. Unlike previous knowledge, this distribution encodes joint knowledge from supervised and self-supervised feature learning. Beyond knowledge exploration, another crucial aspect is how to learn and distill our proposed knowledge effectively. To fully take advantage of hierarchical feature maps, we propose to append several auxiliary branches at various hidden layers. Each auxiliary branch is guided to learn self-supervision augmented task and distill this distribution from teacher to student. Thus we call our KD method as Hierarchical Self-Supervision Augmented Knowledge Distillation (HSSAKD). Experiments on standard image classification show that both offline and online HSSAKD achieves state-of-the-art performance in the field of KD. Further transfer experiments on object detection further verify that HSSAKD can guide the network to learn better features, which can be attributed to learn and distill an auxiliary self-supervision augmented task effectively.",0
"The framework of Knowledge Distillation (KD) is effective in transferring important information from a larger teacher to a smaller student. The focus of previous KD methods has been on mining various types of knowledge, such as feature maps and refined information. However, this knowledge is highly specific to the supervised task. To address this issue, we propose an auxiliary self-supervision augmented task that guides networks to learn more valuable features. This enables us to derive soft self-supervision augmented distributions as richer dark knowledge for KD. Unlike previous knowledge, this distribution encodes joint knowledge from supervised and self-supervised feature learning. To ensure effective learning and distillation of this knowledge, we propose appending auxiliary branches at various hidden layers. Each auxiliary branch is guided to learn the self-supervision augmented task and distill the distribution from teacher to student. Our method, Hierarchical Self-Supervision Augmented Knowledge Distillation (HSSAKD), achieves state-of-the-art performance in both offline and online settings for image classification. Further transfer experiments on object detection confirm that HSSAKD can guide networks to learn better features by effectively learning and distilling the auxiliary self-supervision augmented task.",1
"The use of mobiles phones when driving have been a major factor when it comes to road traffic incidents and the process of capturing such violations can be a laborious task. Advancements in both modern object detection frameworks and high-performance hardware has paved the way for a more automated approach when it comes to video surveillance. In this work, we propose a custom-trained state-of-the-art object detector to work with roadside cameras to capture driver phone usage without the need for human intervention. The proposed approach also addresses the issues caused by windscreen glare and introduces the steps required to remedy this. Twelve pre-trained models are fine-tuned with our custom dataset using four popular object detection methods: YOLO, SSD, Faster R-CNN, and CenterNet. Out of all the object detectors tested, the YOLO yields the highest accuracy levels of up to 96% (AP10) and frame rates of up to ~30 FPS. DeepSort object tracking algorithm is also integrated into the best-performing model to collect records of only the unique violations, and enable the proposed approach to count the number of vehicles. The proposed automated system will collect the output images of the identified violations, timestamps of each violation, and total vehicle count. Data can be accessed via a purpose-built user interface.",0
"Mobile phone use while driving has been a significant contributor to road traffic incidents, and detecting these violations can be a tedious process. However, advances in object detection frameworks and hardware have enabled a more automated approach to video surveillance. This study proposes a state-of-the-art object detector to work with roadside cameras and capture driver phone usage without human intervention. The proposed approach addresses issues caused by windscreen glare and fine-tunes twelve pre-trained models using four popular object detection methods. The YOLO detector yields the highest accuracy levels of up to 96% and frame rates of up to ~30 FPS. The DeepSort object tracking algorithm is integrated into the best-performing model to collect records of unique violations and count the number of vehicles. The automated system outputs identified violations, timestamps, and total vehicle count, which can be accessed via a user interface.",1
"As moving objects always draw more attention of human eyes, the temporal motive information is always exploited complementarily with spatial information to detect salient objects in videos. Although efficient tools such as optical flow have been proposed to extract temporal motive information, it often encounters difficulties when used for saliency detection due to the movement of camera or the partial movement of salient objects. In this paper, we investigate the complimentary roles of spatial and temporal information and propose a novel dynamic spatiotemporal network (DS-Net) for more effective fusion of spatiotemporal information. We construct a symmetric two-bypass network to explicitly extract spatial and temporal features. A dynamic weight generator (DWG) is designed to automatically learn the reliability of corresponding saliency branch. And a top-down cross attentive aggregation (CAA) procedure is designed so as to facilitate dynamic complementary aggregation of spatiotemporal features. Finally, the features are modified by spatial attention with the guidance of coarse saliency map and then go through decoder part for final saliency map. Experimental results on five benchmarks VOS, DAVIS, FBMS, SegTrack-v2, and ViSal demonstrate that the proposed method achieves superior performance than state-of-the-art algorithms. The source code is available at https://github.com/TJUMMG/DS-Net.",0
"Human eyes are naturally drawn to moving objects, so it is common to use both spatial and temporal information to detect salient objects in videos. However, using tools like optical flow to extract temporal information for saliency detection can be difficult due to camera movement or partial movement of the salient object. In this paper, we propose a dynamic spatiotemporal network (DS-Net) that effectively fuses spatiotemporal information. We use a symmetric two-bypass network to extract spatial and temporal features, a dynamic weight generator (DWG) to learn the reliability of the saliency branch, and a top-down cross attentive aggregation (CAA) procedure to dynamically complement spatiotemporal features. Finally, we modify the features with spatial attention and a coarse saliency map before passing them through a decoder for the final saliency map. Our method outperforms state-of-the-art algorithms on five benchmarks (VOS, DAVIS, FBMS, SegTrack-v2, and ViSal), and the source code is available at https://github.com/TJUMMG/DS-Net.",1
"We present a simple yet effective progressive self-guided loss function to facilitate deep learning-based salient object detection (SOD) in images. The saliency maps produced by the most relevant works still suffer from incomplete predictions due to the internal complexity of salient objects. Our proposed progressive self-guided loss simulates a morphological closing operation on the model predictions for progressively creating auxiliary training supervisions to step-wisely guide the training process. We demonstrate that this new loss function can guide the SOD model to highlight more complete salient objects step-by-step and meanwhile help to uncover the spatial dependencies of the salient object pixels in a region growing manner. Moreover, a new feature aggregation module is proposed to capture multi-scale features and aggregate them adaptively by a branch-wise attention mechanism. Benefiting from this module, our SOD framework takes advantage of adaptively aggregated multi-scale features to locate and detect salient objects effectively. Experimental results on several benchmark datasets show that our loss function not only advances the performance of existing SOD models without architecture modification but also helps our proposed framework to achieve state-of-the-art performance.",0
"Our study introduces a straightforward yet efficient progressive self-guided loss function for facilitating deep learning-based salient object detection (SOD) in images. Despite the works' relevance, the saliency maps produced still result in incomplete predictions due to the intricate nature of salient objects. To address this issue, our proposed progressive self-guided loss imitates a morphological closing operation on the model predictions, progressively generating auxiliary training supervisions to guide the training process step-by-step. Our results demonstrate that this loss function can guide the SOD model to highlight more complete salient objects and reveal the spatial dependencies of the salient object pixels in a region growing manner. Additionally, we propose a new feature aggregation module that captures multi-scale features and adapts them by a branch-wise attention mechanism. Our SOD framework utilizes this module to locate and detect salient objects effectively. Our experimental results on several benchmark datasets show that our loss function enhances the performance of existing SOD models and helps our proposed framework achieve state-of-the-art performance without modifying the architecture.",1
"We train an object detector built from convolutional neural networks to count interference fringes in elliptical antinode regions in frames of high-speed video recordings of transient oscillations in Caribbean steelpan drums illuminated by electronic speckle pattern interferometry (ESPI). The annotations provided by our model aim to contribute to the understanding of time-dependent behavior in such drums by tracking the development of sympathetic vibration modes. The system is trained on a dataset of crowdsourced human-annotated images obtained from the Zooniverse Steelpan Vibrations Project. Due to the small number of human-annotated images and the ambiguity of the annotation task, we also evaluate the model on a large corpus of synthetic images whose properties have been matched to the real images by style transfer using a Generative Adversarial Network. Applying the model to thousands of unlabeled video frames, we measure oscillations consistent with audio recordings of these drum strikes. One unanticipated result is that sympathetic oscillations of higher-octave notes significantly precede the rise in sound intensity of the corresponding second harmonic tones; the mechanism responsible for this remains unidentified. This paper primarily concerns the development of the predictive model; further exploration of the steelpan images and deeper physical insights await its further application.",0
"Our study involves training an object detector using convolutional neural networks to count interference fringes in elliptical antinode areas of high-speed video recordings of transient oscillations in Caribbean steelpan drums illuminated by electronic speckle pattern interferometry (ESPI). The annotations provided by our model seek to enhance the comprehension of time-dependent behavior in such drums by tracking the growth of sympathetic vibration modes. To achieve this, we used a dataset of human-annotated images obtained from the Zooniverse Steelpan Vibrations Project. However, due to the limited number of annotated images and the ambiguity of the annotation task, we also evaluated the model on a vast collection of synthetic images produced using a Generative Adversarial Network that matched the real images' properties through style transfer. Using the model on thousands of unlabeled video frames, we observed oscillations that corresponded to audio recordings of the drum strikes. A surprising outcome was that the sympathetic oscillations of higher-octave notes occurred significantly before the rise in sound intensity of the corresponding second harmonic tones, and the cause of this phenomenon remains unknown. This paper focuses on developing the predictive model, and further exploration of the steelpan images and deeper physical insights await its future application.",1
"While deep neural networks have achieved remarkable success in various computer vision tasks, they often fail to generalize to new domains and subtle variations of input images. Several defenses have been proposed to improve the robustness against these variations. However, current defenses can only withstand the specific attack used in training, and the models often remain vulnerable to other input variations. Moreover, these methods often degrade performance of the model on clean images and do not generalize to out-of-domain samples. In this paper we present Generative Adversarial Training, an approach to simultaneously improve the model's generalization to the test set and out-of-domain samples as well as its robustness to unseen adversarial attacks. Instead of altering a low-level pre-defined aspect of images, we generate a spectrum of low-level, mid-level and high-level changes using generative models with a disentangled latent space. Adversarial training with these examples enable the model to withstand a wide range of attacks by observing a variety of input alterations during training. We show that our approach not only improves performance of the model on clean images and out-of-domain samples but also makes it robust against unforeseen attacks and outperforms prior work. We validate effectiveness of our method by demonstrating results on various tasks such as classification, segmentation and object detection.",0
"Although deep neural networks have displayed impressive success in different computer vision tasks, they often struggle to generalize to new domains and subtle variations of input images. Many defenses have been proposed to enhance the robustness against these variations. However, current defenses only handle the specific attack used in training, leaving the models vulnerable to other input variations. Also, these approaches usually decrease the model's performance on clean images and do not apply to out-of-domain samples. Our paper introduces Generative Adversarial Training, a technique that simultaneously boosts the model's generalization to the test set and out-of-domain samples, and its resilience to unseen adversarial attacks. Instead of modifying a low-level pre-defined aspect of images, we use generative models with a disentangled latent space to generate a range of low-level, mid-level, and high-level changes. Adversarial training with these examples lets the model withstand a wide range of attacks by observing multiple input alterations during training. Our approach not only enhances the model's performance on clean images and out-of-domain samples, but also makes it robust against unforeseen attacks and outperforms previous work. We validate the effectiveness of our method by demonstrating results on various tasks like classification, segmentation, and object detection.",1
"Event cameras are novel sensors with outstanding properties such as high temporal resolution and high dynamic range. Despite these characteristics, event-based vision has been held back by the shortage of labeled datasets due to the novelty of event cameras. To overcome this drawback, we propose a task transfer method that allows models to be trained directly with labeled images and unlabeled event data. Compared to previous approaches, (i) our method transfers from single images to events instead of high frame rate videos, and (ii) does not rely on paired sensor data. To achieve this, we leverage the generative event model to split event features into content and motion features. This feature split enables to efficiently match the latent space for events and images, which is crucial for a successful task transfer. Thus, our approach unlocks the vast amount of existing image datasets for the training of event-based neural networks. Our task transfer method consistently outperforms methods applicable in the Unsupervised Domain Adaptation setting for object detection by 0.26 mAP (increase by 93%) and classification by 2.7% accuracy.",0
"Event cameras possess exceptional qualities like high temporal resolution and dynamic range, yet the scarcity of labeled datasets has hindered the progress of event-based vision. To resolve this issue, we introduce a task transfer technique that enables direct training of models with labeled images and unlabeled event data. Our approach is distinct from previous methods as it transfers from single images to events, instead of high frame rate videos, and does not require paired sensor data. We utilize the generative event model to divide event features into content and motion features, which facilitates the efficient matching of the latent space for events and images. Our method unlocks the potential of existing image datasets for the training of event-based neural networks. The results demonstrate that our task transfer method consistently outperforms unsupervised domain adaptation methods for object detection and classification by 0.26 mAP (93% increase) and 2.7% accuracy, respectively.",1
"We present a flexible and high-performance framework, named Pyramid R-CNN, for two-stage 3D object detection from point clouds. Current approaches generally rely on the points or voxels of interest for RoI feature extraction on the second stage, but cannot effectively handle the sparsity and non-uniform distribution of those points, and this may result in failures in detecting objects that are far away. To resolve the problems, we propose a novel second-stage module, named pyramid RoI head, to adaptively learn the features from the sparse points of interest. The pyramid RoI head consists of three key components. Firstly, we propose the RoI-grid Pyramid, which mitigates the sparsity problem by extensively collecting points of interest for each RoI in a pyramid manner. Secondly, we propose RoI-grid Attention, a new operation that can encode richer information from sparse points by incorporating conventional attention-based and graph-based point operators into a unified formulation. Thirdly, we propose the Density-Aware Radius Prediction (DARP) module, which can adapt to different point density levels by dynamically adjusting the focusing range of RoIs. Combining the three components, our pyramid RoI head is robust to the sparse and imbalanced circumstances, and can be applied upon various 3D backbones to consistently boost the detection performance. Extensive experiments show that Pyramid R-CNN outperforms the state-of-the-art 3D detection models by a large margin on both the KITTI dataset and the Waymo Open dataset.",0
"We introduce a versatile and high-performing framework called Pyramid R-CNN for detecting 3D objects from point clouds in two stages. Current methods rely on points or voxels of interest for RoI feature extraction during the second stage, but this approach fails to effectively manage the sparsity and non-uniform distribution of these points, leading to unsuccessful object detection at greater distances. To address these challenges, we have developed a novel second-stage module named pyramid RoI head, which can learn features adaptively from sparse points of interest. The pyramid RoI head has three critical components. First, RoI-grid Pyramid offers a solution that collects points of interest for each RoI extensively in a pyramid manner, mitigating the sparsity issue. Second, RoI-grid Attention offers a new operation that encodes richer information from sparse points by incorporating conventional attention-based and graph-based point operators into a unified formulation. Third, the Density-Aware Radius Prediction (DARP) module can adapt to different point density levels by dynamically adjusting the focusing range of RoIs. By combining these three components, our pyramid RoI head is robust in sparse and imbalanced circumstances and can consistently enhance detection performance on various 3D backbones. Extensive experiments have demonstrated that Pyramid R-CNN performs significantly better than the current state-of-the-art 3D detection models on both the KITTI dataset and the Waymo Open dataset.",1
"Object detection on Unmanned Aerial Vehicles (UAVs) is still a challenging task. The recordings are mostly sparse and contain only small objects. In this work, we propose a simple tiling method that improves the detection capability in the remote sensing case without modifying the model itself. By reducing the background bias and enabling the usage of higher image resolutions during training, our method can improve the performance of models substantially. The procedure was validated on three different data sets and outperformed similar approaches in performance and speed.",0
"Detecting objects on UAVs remains a difficult undertaking due to the sparsity of recordings and the presence of small objects. In this study, we present a straightforward tiling approach that enhances remote sensing detection capabilities without altering the model. Our method significantly enhances model performance by mitigating background bias and allowing for higher image resolutions during training. We verified the efficacy of our approach on three distinct datasets, and it surpassed comparable methods in both speed and accuracy.",1
"Detection of objects is extremely important in various aerial vision-based applications. Over the last few years, the methods based on convolution neural networks have made substantial progress. However, because of the large variety of object scales, densities, and arbitrary orientations, the current detectors struggle with the extraction of semantically strong features for small-scale objects by a predefined convolution kernel. To address this problem, we propose the rotation equivariant feature image pyramid network (REFIPN), an image pyramid network based on rotation equivariance convolution. The proposed model adopts single-shot detector in parallel with a lightweight image pyramid module to extract representative features and generate regions of interest in an optimization approach. The proposed network extracts feature in a wide range of scales and orientations by using novel convolution filters. These features are used to generate vector fields and determine the weight and angle of the highest-scoring orientation for all spatial locations on an image. By this approach, the performance for small-sized object detection is enhanced without sacrificing the performance for large-sized object detection. The performance of the proposed model is validated on two commonly used aerial benchmarks and the results show our proposed model can achieve state-of-the-art performance with satisfactory efficiency.",0
"The identification of objects is crucial in a variety of aerial vision-based applications. Convolution neural networks have made significant advancements in recent years, but they struggle to extract semantically robust features for small-scale objects due to their diverse scales, densities, and arbitrary orientations. To tackle this issue, we present the rotation equivariant feature image pyramid network (REFIPN), which is an image pyramid network that employs rotation equivariance convolution. The proposed model integrates a single-shot detector with a lightweight image pyramid module to extract representative features and produce regions of interest through an optimization approach. By utilizing novel convolution filters, the proposed network extracts features across a broad range of sizes and angles. These features are then utilized to calculate vector fields and identify the weight and angle of the highest-scoring orientation for all spatial locations in an image. This approach enhances the performance of small-object detection without jeopardizing the performance of large-object detection. Our proposed model's effectiveness is confirmed by its performance on two widely used aerial benchmarks, attaining state-of-the-art results while maintaining acceptable efficiency.",1
"In multi-object detection using neural networks, the fundamental problem is, ""How should the network learn a variable number of bounding boxes in different input images?"". Previous methods train a multi-object detection network through a procedure that directly assigns the ground truth bounding boxes to the specific locations of the network's output. However, this procedure makes the training of a multi-object detection network too heuristic and complicated. In this paper, we reformulate the multi-object detection task as a problem of density estimation of bounding boxes. Instead of assigning each ground truth to specific locations of network's output, we train a network by estimating the probability density of bounding boxes in an input image using a mixture model. For this purpose, we propose a novel network for object detection called Mixture Density Object Detector (MDOD), and the corresponding objective function for the density-estimation-based training. We applied MDOD to MS COCO dataset. Our proposed method not only deals with multi-object detection problems in a new approach, but also improves detection performances through MDOD. The code is available: https://github.com/yoojy31/MDOD.",0
"When it comes to multi-object detection using neural networks, the main issue is how to teach the network to identify varying numbers of bounding boxes in different input images. Previous approaches attempted to train the network by directly assigning ground truth bounding boxes to specific output locations, but this method proved overly complex and heuristic. Instead, our paper presents a new approach to the multi-object detection task, treating it as a density estimation problem for bounding boxes. Rather than assigning exact locations to each ground truth object, we train the network to estimate the probability density of bounding boxes in an input image using a mixture model. Our proposed network, the Mixture Density Object Detector (MDOD), and corresponding objective function provide an innovative solution to this problem. We tested our approach on the MS COCO dataset, achieving improved detection performance. The code for MDOD is available on GitHub at https://github.com/yoojy31/MDOD.",1
"Moving object detection is a crucial task in computer vision. Event-based cameras are bio-inspired cameras that work by mimicking the working of the human eye. These cameras have multiple advantages over conventional frame-based cameras, like reduced latency, HDR, reduced motion blur during high motion, low power consumption, etc. However, these advantages come at a high cost, as event-based cameras are noise sensitive and have low resolution. Moreover, the task of moving object detection in these cameras is difficult, as event-based sensors capture only the binary changes in brightness of a scene, lacking useful visual features like texture and color. In this paper, we investigate the application of the k-means clustering technique in detecting moving objects in event-based data. Experimental results in publicly available datasets using k-means show significant improvement in performance over the state-of-the-art methods.",0
"Detecting moving objects is a critical activity in computer vision. Event-based cameras are cameras that imitate the human eye and offer numerous benefits over traditional frame-based cameras, including reduced latency, high dynamic range (HDR), reduced motion blur, and low power consumption. However, the advantages of event-based cameras come at a high cost due to their sensitivity to noise and low resolution. Additionally, detecting moving objects using event-based sensors is challenging since the sensors only capture binary changes in brightness, lacking important visual features like color and texture. This study explores the use of k-means clustering in identifying moving objects in event-based data. The results of experiments conducted using k-means on publicly available datasets indicate a significant improvement in performance compared to state-of-the-art techniques.",1
"Precisely detection of Unmanned Aerial Vehicles(UAVs) plays a critical role in UAV defense systems. Deep learning is widely adopted for UAV object detection whereas researches on this topic are limited by the amount of dataset and small scale of UAV. To tackle these problems, a novel comprehensive approach that combines transfer learning based on simulation data and adaptive fusion is proposed. Firstly, the open-source plugin AirSim proposed by Microsoft is used to generate mass realistic simulation data. Secondly, transfer learning is applied to obtain a pre-trained YOLOv5 model on the simulated dataset and fine-tuned model on the real-world dataset. Finally, an adaptive fusion mechanism is proposed to further improve small object detection performance. Experiment results demonstrate the effectiveness of simulation-based transfer learning which leads to a 2.7% performance increase on UAV object detection. Furthermore, with transfer learning and adaptive fusion mechanism, 7.1% improvement is achieved compared to the original YOLO v5 model.",0
"In the realm of UAV defense systems, the precise detection of Unmanned Aerial Vehicles (UAVs) is of utmost importance. While Deep Learning is a popular method for UAV object detection, research is limited due to inadequate datasets and the small scale of UAVs. To address this, a novel approach combining transfer learning based on simulation data and adaptive fusion is proposed. Firstly, the open-source plugin AirSim by Microsoft is used to generate realistic simulation data. Secondly, transfer learning is utilized to obtain a pre-trained YOLOv5 model on the simulated dataset and fine-tuned model on the real-world dataset. Finally, an adaptive fusion mechanism is suggested to enhance small object detection performance. The experiment results show that simulation-based transfer learning leads to a 2.7% improvement in UAV object detection performance. Moreover, by using transfer learning and the adaptive fusion mechanism, a 7.1% improvement is seen in comparison to the original YOLOv5 model.",1
"Zero-shot object detection (ZSD), the task that extends conventional detection models to detecting objects from unseen categories, has emerged as a new challenge in computer vision. Most existing approaches tackle the ZSD task with a strict mapping-transfer strategy, which may lead to suboptimal ZSD results: 1) the learning process of those models ignores the available unseen class information, and thus can be easily biased towards the seen categories; 2) the original visual feature space is not well-structured and lack of discriminative information. To address these issues, we develop a novel Semantics-Guided Contrastive Network for ZSD, named ContrastZSD, a detection framework that first brings contrastive learning mechanism into the realm of zero-shot detection. Particularly, ContrastZSD incorporates two semantics-guided contrastive learning subnets that contrast between region-category and region-region pairs respectively. The pairwise contrastive tasks take advantage of additional supervision signals derived from both ground truth label and pre-defined class similarity distribution. Under the guidance of those explicit semantic supervision, the model can learn more knowledge about unseen categories to avoid the bias problem to seen concepts, while optimizing the data structure of visual features to be more discriminative for better visual-semantic alignment. Extensive experiments are conducted on two popular benchmarks for ZSD, i.e., PASCAL VOC and MS COCO. Results show that our method outperforms the previous state-of-the-art on both ZSD and generalized ZSD tasks.",0
"Zero-shot object detection (ZSD) is a recent challenge in computer vision that involves detecting objects from unknown categories. Most current methods for ZSD rely on a rigid mapping-transfer approach, which may result in suboptimal performance due to two main issues: 1) these models do not consider information about unseen categories during the learning process, leading to a bias towards known categories; and 2) the original visual feature space lacks structure and discriminative information. To overcome these limitations, we propose a new framework for ZSD called ContrastZSD, which incorporates two semantics-guided contrastive learning subnets. These subnets contrast between region-category and region-region pairs, respectively, using supervision signals derived from both ground truth labels and pre-defined class similarity distributions. With explicit semantic supervision, the model can learn more about unseen categories, avoiding bias towards known categories. Our approach optimizes the data structure of visual features to be more discriminative, leading to better visual-semantic alignment. We evaluated ContrastZSD on two popular ZSD benchmarks (PASCAL VOC and MS COCO) and achieved state-of-the-art performance on both ZSD and generalized ZSD tasks.",1
"Weakly-supervised salient object detection (WSOD) aims to develop saliency models using image-level annotations. Despite of the success of previous works, explorations on an effective training strategy for the saliency network and accurate matches between image-level annotations and salient objects are still inadequate. In this work, 1) we propose a self-calibrated training strategy by explicitly establishing a mutual calibration loop between pseudo labels and network predictions, liberating the saliency network from error-prone propagation caused by pseudo labels. 2) we prove that even a much smaller dataset (merely 1.8% of ImageNet) with well-matched annotations can facilitate models to achieve better performance as well as generalizability. This sheds new light on the development of WSOD and encourages more contributions to the community. Comprehensive experiments demonstrate that our method outperforms all the existing WSOD methods by adopting the self-calibrated strategy only. Steady improvements are further achieved by training on the proposed dataset. Additionally, our method achieves 94.7% of the performance of fully-supervised methods on average. And what is more, the fully supervised models adopting our predicted results as ""ground truths"" achieve successful results (95.6% for BASNet and 97.3% for ITSD on F-measure), while costing only 0.32% of labeling time for pixel-level annotation.",0
"The objective of Weakly-supervised salient object detection (WSOD) is to create saliency models utilizing image-level annotations. While previous attempts have been successful, the training strategy for the saliency network and accurate matching of image-level annotations and salient objects require further exploration. This study proposes a self-calibrated training strategy to establish a mutual calibration loop between pseudo labels and network predictions, freeing the saliency network from error-prone propagation. Moreover, the study proves that a smaller dataset with well-matched annotations can enhance model performance and generalizability. The results show that the proposed method outperforms existing WSOD methods, and the fully supervised models utilizing the predicted results achieve successful results while only costing 0.32% of labeling time for pixel-level annotation.",1
"Previous video object segmentation approaches mainly focus on using simplex solutions between appearance and motion, limiting feature collaboration efficiency among and across these two cues. In this work, we study a novel and efficient full-duplex strategy network (FSNet) to address this issue, by considering a better mutual restraint scheme between motion and appearance in exploiting the cross-modal features from the fusion and decoding stage. Specifically, we introduce the relational cross-attention module (RCAM) to achieve bidirectional message propagation across embedding sub-spaces. To improve the model's robustness and update the inconsistent features from the spatial-temporal embeddings, we adopt the bidirectional purification module (BPM) after the RCAM. Extensive experiments on five popular benchmarks show that our FSNet is robust to various challenging scenarios (e.g., motion blur, occlusion) and achieves favourable performance against existing cutting-edges both in the video object segmentation and video salient object detection tasks. The project is publicly available at: https://dpfan.net/FSNet.",0
"The previous methods for video object segmentation focused on using simple solutions that only considered appearance and motion separately. This limited the effectiveness of combining features from these two cues. To address this issue, we propose a novel and efficient full-duplex strategy network called FSNet. Our approach improves collaboration between appearance and motion by using a better mutual restraint scheme and cross-modal feature fusion. We introduce the relational cross-attention module (RCAM) to achieve bidirectional message propagation and the bidirectional purification module (BPM) to improve model robustness. Our FSNet achieves favorable performance on five popular benchmarks, even in challenging scenarios such as motion blur and occlusion. Our project is publicly available at https://dpfan.net/FSNet.",1
"Many power line companies are using UAVs to perform their inspection processes instead of putting their workers at risk by making them climb high voltage power line towers, for instance. A crucial task for the inspection is to detect and classify assets in the power transmission lines. However, public data related to power line assets are scarce, preventing a faster evolution of this area. This work proposes the Power Line Assets Dataset, containing high-resolution and real-world images of multiple high-voltage power line components. It has 2,409 annotated objects divided into five classes: transmission tower, insulator, spacer, tower plate, and Stockbridge damper, which vary in size (resolution), orientation, illumination, angulation, and background. This work also presents an evaluation with popular deep object detection methods, showing considerable room for improvement. The STN PLAD dataset is publicly available at https://github.com/andreluizbvs/PLAD.",0
"To avoid endangering workers by having them climb high voltage power line towers, many power line companies are using UAVs for inspections. One important aspect of these inspections is identifying and categorizing power line assets. Unfortunately, there is a lack of public data related to power line assets, which slows down progress in this field. To address this issue, the Power Line Assets Dataset has been created. This dataset includes real-world, high-resolution images of various high-voltage power line components, with 2,409 annotated objects divided into five classes. These objects vary in size, orientation, illumination, angulation, and background. The dataset has been evaluated using popular deep object detection methods, which indicate that there is considerable room for improvement. The STN PLAD dataset is now publicly available on https://github.com/andreluizbvs/PLAD.",1
"We present 4D-Net, a 3D object detection approach, which utilizes 3D Point Cloud and RGB sensing information, both in time. We are able to incorporate the 4D information by performing a novel dynamic connection learning across various feature representations and levels of abstraction, as well as by observing geometric constraints. Our approach outperforms the state-of-the-art and strong baselines on the Waymo Open Dataset. 4D-Net is better able to use motion cues and dense image information to detect distant objects more successfully.",0
"Our approach, called 4D-Net, is a 3D object detection method that makes use of both 3D Point Cloud and RGB sensing information over time. By incorporating 4D information through dynamic connection learning across multiple feature representations and levels of abstraction, as well as taking into account geometric constraints, we have achieved superior performance compared to state-of-the-art and strong baselines on the Waymo Open Dataset. 4D-Net is particularly adept at utilizing motion cues and dense image information to more accurately detect distant objects.",1
"Aiming at discovering and locating most distinctive objects from visual scenes, salient object detection (SOD) plays an essential role in various computer vision systems. Coming to the era of high resolution, SOD methods are facing new challenges. The major limitation of previous methods is that they try to identify the salient regions and estimate the accurate objects boundaries simultaneously with a single regression task at low-resolution. This practice ignores the inherent difference between the two difficult problems, resulting in poor detection quality. In this paper, we propose a novel deep learning framework for high-resolution SOD task, which disentangles the task into a low-resolution saliency classification network (LRSCN) and a high-resolution refinement network (HRRN). As a pixel-wise classification task, LRSCN is designed to capture sufficient semantics at low-resolution to identify the definite salient, background and uncertain image regions. HRRN is a regression task, which aims at accurately refining the saliency value of pixels in the uncertain region to preserve a clear object boundary at high-resolution with limited GPU memory. It is worth noting that by introducing uncertainty into the training process, our HRRN can well address the high-resolution refinement task without using any high-resolution training data. Extensive experiments on high-resolution saliency datasets as well as some widely used saliency benchmarks show that the proposed method achieves superior performance compared to the state-of-the-art methods.",0
"Salient object detection (SOD) is crucial in various computer vision systems as it aims to discover and locate the most distinct objects in visual scenes. However, with the emergence of high resolution, SOD methods are struggling with new challenges. Previous methods attempted to identify salient regions and estimate accurate object boundaries simultaneously with a single regression task at low-resolution, resulting in poor detection quality. To address this issue, we present a new deep learning framework for high-resolution SOD that disentangles the task into a low-resolution saliency classification network (LRSCN) and a high-resolution refinement network (HRRN). LRSCN is a pixel-wise classification task designed to capture sufficient semantics at low-resolution to identify definite salient, background, and uncertain image regions. HRRN, on the other hand, is a regression task that accurately refines the saliency value of pixels in the uncertain region to preserve a clear object boundary at high-resolution with limited GPU memory. Notably, our HRRN can handle high-resolution refinement tasks without using any high-resolution training data by introducing uncertainty into the training process. Our experiments on high-resolution saliency datasets and widely used saliency benchmarks demonstrate that our proposed method outperforms state-of-the-art methods.",1
"Advances in deep learning recognition have led to accurate object detection with 2D images. However, these 2D perception methods are insufficient for complete 3D world information. Concurrently, advanced 3D shape estimation approaches focus on the shape itself, without considering metric scale. These methods cannot determine the accurate location and orientation of objects. To tackle this problem, we propose a framework that jointly estimates a metric scale shape and pose from a single RGB image. Our framework has two branches: the Metric Scale Object Shape branch (MSOS) and the Normalized Object Coordinate Space branch (NOCS). The MSOS branch estimates the metric scale shape observed in the camera coordinates. The NOCS branch predicts the normalized object coordinate space (NOCS) map and performs similarity transformation with the rendered depth map from a predicted metric scale mesh to obtain 6d pose and size. Additionally, we introduce the Normalized Object Center Estimation (NOCE) to estimate the geometrically aligned distance from the camera to the object center. We validated our method on both synthetic and real-world datasets to evaluate category-level object pose and shape.",0
"Accurate object detection with 2D images has been made possible through the advancements in deep learning recognition. However, these methods fall short in providing complete 3D world information. Meanwhile, advanced 3D shape estimation techniques primarily focus on the shape alone, disregarding metric scale, which prevents them from accurately determining the location and orientation of objects. To address this issue, we present a framework that simultaneously estimates a metric scale shape and pose based on a single RGB image. Our framework has two branches: the Metric Scale Object Shape branch (MSOS) and the Normalized Object Coordinate Space branch (NOCS). The MSOS branch calculates the metric scale shape observed in camera coordinates, while the NOCS branch predicts the Normalized Object Coordinate Space (NOCS) map and uses the rendered depth map from a predicted metric scale mesh to derive 6d pose and size. We also introduce the Normalized Object Center Estimation (NOCE) to determine the geometrically aligned distance between the camera and the object center. We evaluated our approach on both synthetic and real-world datasets to assess object pose and shape at the category level.",1
"Autonomous driving is the key technology of intelligent logistics in Industrial Internet of Things (IIoT). In autonomous driving, the appearance of incomplete point clouds losing geometric and semantic information is inevitable owing to limitations of occlusion, sensor resolution, and viewing angle when the Light Detection And Ranging (LiDAR) is applied. The emergence of incomplete point clouds, especially incomplete vehicle point clouds, would lead to the reduction of the accuracy of autonomous driving vehicles in object detection, traffic alert, and collision avoidance. Existing point cloud completion networks, such as Point Fractal Network (PF-Net), focus on the accuracy of point cloud completion, without considering the efficiency of inference process, which makes it difficult for them to be deployed for vehicle point cloud repair in autonomous driving. To address the above problem, in this paper, we propose an efficient deep learning approach to repair incomplete vehicle point cloud accurately and efficiently in autonomous driving. In the proposed method, an efficient downsampling algorithm combining incremental sampling and one-time sampling is presented to improves the inference speed of the PF-Net based on Generative Adversarial Network (GAN). To evaluate the performance of the proposed method, a real dataset is used, and an autonomous driving scene is created, where three incomplete vehicle point clouds with 5 different sizes are set for three autonomous driving situations. The improved PF-Net can achieve the speedups of over 19x with almost the same accuracy when compared to the original PF-Net. Experimental results demonstrate that the improved PF-Net can be applied to efficiently complete vehicle point clouds in autonomous driving.",0
"Intelligent logistics in the Industrial Internet of Things (IIoT) rely heavily on autonomous driving technology. However, due to the Light Detection And Ranging (LiDAR) sensors limitations, incomplete point clouds often occur in autonomous driving, particularly in vehicle point clouds, leading to reduced accuracy in collision avoidance, traffic alert, and object detection. Existing point cloud completion networks, such as PF-Net, prioritize accuracy over efficiency, making them unsuitable for autonomous driving applications. Therefore, this paper proposes an efficient deep learning approach to repair incomplete vehicle point clouds with high accuracy and speed. The proposed approach employs an efficient downsampling algorithm that combines incremental and one-time sampling to improve the inference speed of PF-Net based on GAN. To evaluate the method's performance, a real dataset is used, and an autonomous driving scene is created with three incomplete vehicle point clouds of varying sizes. The improved PF-Net achieves speedups of over 19x while maintaining accuracy levels similar to the original PF-Net. The experimental results demonstrate that the improved PF-Net is efficient and effective in completing vehicle point clouds in autonomous driving.",1
"To date, various 3D scene understanding tasks still lack practical and generalizable pre-trained models, primarily due to the intricate nature of 3D scene understanding tasks and their immense variations introduced by camera views, lighting, occlusions, etc. In this paper, we tackle this challenge by introducing a spatio-temporal representation learning (STRL) framework, capable of learning from unlabeled 3D point clouds in a self-supervised fashion. Inspired by how infants learn from visual data in the wild, we explore the rich spatio-temporal cues derived from the 3D data. Specifically, STRL takes two temporally-correlated frames from a 3D point cloud sequence as the input, transforms it with the spatial data augmentation, and learns the invariant representation self-supervisedly. To corroborate the efficacy of STRL, we conduct extensive experiments on three types (synthetic, indoor, and outdoor) of datasets. Experimental results demonstrate that, compared with supervised learning methods, the learned self-supervised representation facilitates various models to attain comparable or even better performances while capable of generalizing pre-trained models to downstream tasks, including 3D shape classification, 3D object detection, and 3D semantic segmentation. Moreover, the spatio-temporal contextual cues embedded in 3D point clouds significantly improve the learned representations.",0
"Up to now, the lack of practical and widely applicable pre-trained models for various 3D scene understanding tasks has been primarily attributed to the complex nature of these tasks and the wide range of variations that are introduced by factors such as camera views, lighting, and occlusions. In this study, we address this challenge by introducing a framework for spatio-temporal representation learning (STRL), which is capable of self-supervised learning from unlabeled 3D point clouds. We draw inspiration from how infants learn from visual data in natural environments, and explore the rich spatio-temporal cues that can be derived from 3D data. Specifically, STRL processes two temporally-correlated frames from a sequence of 3D point clouds as input, applies spatial data augmentation, and learns an invariant representation through self-supervised learning. We conducted extensive experiments on three types of datasets (synthetic, indoor, and outdoor) to validate the efficacy of STRL. The results demonstrate that the self-supervised representation learned by STRL facilitates various models to achieve comparable or even better performances compared to supervised learning methods, while being able to generalize pre-trained models to downstream tasks such as 3D shape classification, 3D object detection, and 3D semantic segmentation. Additionally, the spatio-temporal contextual cues embedded in 3D point clouds significantly enhance the learned representations.",1
"Adversarial attacks are feasible in the real world for object detection. However, most of the previous works have tried to learn ""patches"" applied to an object to fool detectors, which become less effective or even ineffective in squint view angles. To address this issue, we propose the Dense Proposals Attack (DPA) to learn robust, physical and targeted adversarial camouflages for detectors. The camouflages are robust because they remain adversarial when filmed under arbitrary viewpoint and different illumination conditions, physical because they function well both in the 3D virtual scene and the real world, and targeted because they can cause detectors to misidentify an object as a specific target class. In order to make the generated camouflages robust in the physical world, we introduce a combination of viewpoint shifts, lighting and other natural transformations to model the physical phenomena. In addition, to improve the attacks, DPA substantially attacks all the classifications in the fixed region proposals. Moreover, we build a virtual 3D scene using the Unity simulation engine to fairly and reproducibly evaluate different physical attacks. Extensive experiments demonstrate that DPA outperforms the state-of-the-art methods significantly, and generalizes well to the real world, posing a potential threat to the security-critical computer vision systems.",0
"Object detection is vulnerable to adversarial attacks in the real world, but previous attempts have focused on ""patches"" that are ineffective in squint view angles. To address this issue, we propose the Dense Proposals Attack (DPA) which generates robust, physical, and targeted adversarial camouflages for detectors. These camouflages remain adversarial under different viewpoints and illumination conditions, and can cause detectors to misidentify an object as a specific target class. To ensure the camouflages are effective in the physical world, we use a combination of viewpoint shifts, lighting, and other natural transformations. DPA attacks all classifications in fixed region proposals and is evaluated using a virtual 3D scene built with the Unity simulation engine. Our experiments show that DPA outperforms state-of-the-art methods and poses a potential threat to security-critical computer vision systems.",1
"Recently, logo detection has received more and more attention for its wide applications in the multimedia field, such as intellectual property protection, product brand management, and logo duration monitoring. Unlike general object detection, logo detection is a challenging task, especially for small logo objects and large aspect ratio logo objects in the real-world scenario. In this paper, we propose a novel approach, named Discriminative Semantic Feature Pyramid Network with Guided Anchoring (DSFP-GA), which can address these challenges via aggregating the semantic information and generating different aspect ratio anchor boxes. More specifically, our approach mainly consists of Discriminative Semantic Feature Pyramid (DSFP) and Guided Anchoring (GA). Considering that low-level feature maps that are used to detect small logo objects lack semantic information, we propose the DSFP, which can enrich more discriminative semantic features of low-level feature maps and can achieve better performance on small logo objects. Furthermore, preset anchor boxes are less efficient for detecting large aspect ratio logo objects. We therefore integrate the GA into our method to generate large aspect ratio anchor boxes to mitigate this issue. Extensive experimental results on four benchmarks demonstrate the effectiveness of our proposed DSFP-GA. Moreover, we further conduct visual analysis and ablation studies to illustrate the advantage of our method in detecting small and large aspect logo objects. The code and models can be found at https://github.com/Zhangbaisong/DSFP-GA.",0
"In the multimedia field, logo detection is gaining popularity due to its diverse applications, including logo duration monitoring, intellectual property protection, and product brand management. However, detecting logos is a challenging task, particularly for small and large aspect ratio logo objects in real-world scenarios. In this paper, we introduce a new method called Discriminative Semantic Feature Pyramid Network with Guided Anchoring (DSFP-GA) that effectively tackles these challenges by consolidating semantic information and generating different aspect ratio anchor boxes. Our approach comprises of two components: Discriminative Semantic Feature Pyramid (DSFP) and Guided Anchoring (GA). To address the lack of semantic information in low-level feature maps for detecting small logo objects, we propose DSFP, which enriches more discriminative semantic features of low-level feature maps and provides better performance. In addition, we integrate GA to generate large aspect ratio anchor boxes, which are less efficient for detecting large aspect ratio logo objects. Our extensive experiments on four benchmarks demonstrate the effectiveness of our proposed DSFP-GA. Furthermore, we perform visual analysis and ablation studies to illustrate the advantages of our approach in detecting small and large aspect logo objects. Code and models for our method can be found at https://github.com/Zhangbaisong/DSFP-GA.",1
"A panoptic driving perception system is an essential part of autonomous driving. A high-precision and real-time perception system can assist the vehicle in making the reasonable decision while driving. We present a panoptic driving perception network (YOLOP) to perform traffic object detection, drivable area segmentation and lane detection simultaneously. It is composed of one encoder for feature extraction and three decoders to handle the specific tasks. Our model performs extremely well on the challenging BDD100K dataset, achieving state-of-the-art on all three tasks in terms of accuracy and speed. Besides, we verify the effectiveness of our multi-task learning model for joint training via ablative studies. To our best knowledge, this is the first work that can process these three visual perception tasks simultaneously in real-time on an embedded device Jetson TX2(23 FPS) and maintain excellent accuracy. To facilitate further research, the source codes and pre-trained models will be released at https://github.com/hustvl/YOLOP.",0
"Autonomous driving relies heavily on a panoptic driving perception system, which enables the vehicle to make reasonable decisions while on the road. A real-time and high-precision perception system is necessary to achieve this. To address this need, we have developed a panoptic driving perception network (YOLOP) that performs traffic object detection, drivable area segmentation, and lane detection all at once. The model consists of an encoder for feature extraction and three decoders for specific tasks. We have tested our model on the challenging BDD100K dataset and achieved state-of-the-art results in terms of accuracy and speed. Additionally, we have conducted ablative studies to confirm the effectiveness of multitask learning for joint training. Our work is the first to process these three tasks simultaneously in real-time on an embedded device, Jetson TX2 (23 FPS), while maintaining excellent accuracy. To support further research, we will release the source codes and pre-trained models on https://github.com/hustvl/YOLOP.",1
"With the increase of structure complexity, convolutional neural networks (CNNs) take a fair amount of computation cost. Meanwhile, existing research reveals the salient parameter redundancy in CNNs. The current pruning methods can compress CNNs with little performance drop, but when the pruning ratio increases, the accuracy loss is more serious. Moreover, some iterative pruning methods are difficult to accurately identify and delete unimportant parameters due to the accuracy drop during pruning. We propose a novel adversarial iterative pruning method (AIP) for CNNs based on knowledge transfer. The original network is regarded as the teacher while the compressed network is the student. We apply attention maps and output features to transfer information from the teacher to the student. Then, a shallow fully-connected network is designed as the discriminator to allow the output of two networks to play an adversarial game, thereby it can quickly recover the pruned accuracy among pruning intervals. Finally, an iterative pruning scheme based on the importance of channels is proposed. We conduct extensive experiments on the image classification tasks CIFAR-10, CIFAR-100, and ILSVRC-2012 to verify our pruning method can achieve efficient compression for CNNs even without accuracy loss. On the ILSVRC-2012, when removing 36.78% parameters and 45.55% floating-point operations (FLOPs) of ResNet-18, the Top-1 accuracy drop are only 0.66%. Our method is superior to some state-of-the-art pruning schemes in terms of compressing rate and accuracy. Moreover, we further demonstrate that AIP has good generalization on the object detection task PASCAL VOC.",0
"Convolutional neural networks (CNNs) become computationally expensive as the structure complexity increases. While current pruning methods are effective in compressing CNNs with minimal performance loss, accuracy loss becomes more severe as the pruning ratio increases. Additionally, identifying and removing unimportant parameters accurately using iterative pruning methods is challenging due to accuracy drop during pruning. To address this, we propose an adversarial iterative pruning method (AIP) for CNNs based on knowledge transfer. The original network acts as the teacher while the compressed network is the student. Attention maps and output features are used to transfer information from the teacher to the student, and a shallow fully-connected network serves as the discriminator to enable the output of the two networks to engage in an adversarial game, which leads to the quick recovery of pruned accuracy among pruning intervals. Finally, an iterative pruning scheme based on the importance of channels is introduced. Experiments conducted on CIFAR-10, CIFAR-100, and ILSVRC-2012 for image classification tasks demonstrate the effectiveness of our pruning method in achieving efficient compression for CNNs without compromising accuracy. For example, when removing 36.78% parameters and 45.55% floating-point operations (FLOPs) of ResNet-18 on ILSVRC-2012, the Top-1 accuracy drop is only 0.66%. Our method outperforms some state-of-the-art pruning schemes in terms of compressing rate and accuracy. Furthermore, we demonstrate the generalization of AIP on the PASCAL VOC object detection task.",1
"We propose Rank & Sort (RS) Loss, a ranking-based loss function to train deep object detection and instance segmentation methods (i.e. visual detectors). RS Loss supervises the classifier, a sub-network of these methods, to rank each positive above all negatives as well as to sort positives among themselves with respect to (wrt.) their localisation qualities (e.g. Intersection-over-Union - IoU). To tackle the non-differentiable nature of ranking and sorting, we reformulate the incorporation of error-driven update with backpropagation as Identity Update, which enables us to model our novel sorting error among positives. With RS Loss, we significantly simplify training: (i) Thanks to our sorting objective, the positives are prioritized by the classifier without an additional auxiliary head (e.g. for centerness, IoU, mask-IoU), (ii) due to its ranking-based nature, RS Loss is robust to class imbalance, and thus, no sampling heuristic is required, and (iii) we address the multi-task nature of visual detectors using tuning-free task-balancing coefficients. Using RS Loss, we train seven diverse visual detectors only by tuning the learning rate, and show that it consistently outperforms baselines: e.g. our RS Loss improves (i) Faster R-CNN by ~ 3 box AP and aLRP Loss (ranking-based baseline) by ~ 2 box AP on COCO dataset, (ii) Mask R-CNN with repeat factor sampling (RFS) by 3.5 mask AP (~ 7 AP for rare classes) on LVIS dataset; and also outperforms all counterparts. Code is available at: https://github.com/kemaloksuz/RankSortLoss",0
"The Rank & Sort (RS) Loss is proposed as a ranking-based loss function to train deep object detection and instance segmentation methods, also known as visual detectors. This loss function supervises the sub-network classifier to rank each positive above all negatives and sort positives based on their localisation qualities, such as Intersection-over-Union (IoU). To address the non-differentiable nature of ranking and sorting, the authors introduce the Identity Update technique to incorporate error-driven update with backpropagation. With RS Loss, training is simplified by prioritizing positives without an additional auxiliary head, being robust to class imbalance, and using tuning-free task-balancing coefficients. By using RS Loss, the authors train seven different visual detectors and show consistent improvement over baselines, including Faster R-CNN and Mask R-CNN. The code is available at https://github.com/kemaloksuz/RankSortLoss.",1
"Over recent years, deep learning-based computer vision systems have been applied to images at an ever-increasing pace, oftentimes representing the only type of consumption for those images. Given the dramatic explosion in the number of images generated per day, a question arises: how much better would an image codec targeting machine-consumption perform against state-of-the-art codecs targeting human-consumption? In this paper, we propose an image codec for machines which is neural network (NN) based and end-to-end learned. In particular, we propose a set of training strategies that address the delicate problem of balancing competing loss functions, such as computer vision task losses, image distortion losses, and rate loss. Our experimental results show that our NN-based codec outperforms the state-of-the-art Versa-tile Video Coding (VVC) standard on the object detection and instance segmentation tasks, achieving -37.87% and -32.90% of BD-rate gain, respectively, while being fast thanks to its compact size. To the best of our knowledge, this is the first end-to-end learned machine-targeted image codec.",0
"In recent years, computer vision systems that use deep learning have been increasingly applied to images, often being the sole means of their consumption. With the vast number of images being generated daily, it begs the question of how much better an image codec tailored for machines would perform compared to state-of-the-art codecs designed for human consumption. This paper proposes a neural network-based image codec designed for machines, which has been end-to-end learned. The authors suggest several training strategies to tackle the complex issue of balancing competing loss functions, such as image distortion losses, computer vision task losses, and rate loss. Results from experiments show that their codec outperforms the Versatile Video Coding (VVC) standard on object detection and instance segmentation tasks, achieving -37.87% and -32.90% of BD-rate gain, respectively. Additionally, their codec is compact and fast. This is the first machine-targeted image codec learned end-to-end, to the best of our knowledge.",1
"Existing salient object detection (SOD) methods mainly rely on CNN-based U-shaped structures with skip connections to combine the global contexts and local spatial details that are crucial for locating salient objects and refining object details, respectively. Despite great successes, the ability of CNN in learning global contexts is limited. Recently, the vision transformer has achieved revolutionary progress in computer vision owing to its powerful modeling of global dependencies. However, directly applying the transformer to SOD is suboptimal because the transformer lacks the ability to learn local spatial representations. To this end, this paper explores the combination of transformer and CNN to learn both global and local representations for SOD. We propose a transformer-based Asymmetric Bilateral U-Net (ABiU-Net). The asymmetric bilateral encoder has a transformer path and a lightweight CNN path, where the two paths communicate at each encoder stage to learn complementary global contexts and local spatial details, respectively. The asymmetric bilateral decoder also consists of two paths to process features from the transformer and CNN encoder paths, with communication at each decoder stage for decoding coarse salient object locations and find-grained object details, respectively. Such communication between the two encoder/decoder paths enables AbiU-Net to learn complementary global and local representations, taking advantage of the natural properties of transformer and CNN, respectively. Hence, ABiU-Net provides a new perspective for transformer-based SOD. Extensive experiments demonstrate that ABiU-Net performs favorably against previous state-of-the-art SOD methods. The code will be released.",0
"Most salient object detection (SOD) methods currently rely on U-shaped structures with CNN-based skip connections to combine global contexts and local spatial details for locating and refining salient objects. Despite their successes, CNNs have limited ability in learning global contexts. Recently, vision transformers have shown remarkable progress in computer vision by modeling global dependencies effectively. However, directly applying transformers to SOD is not optimal as they cannot learn local spatial representations. Therefore, this study proposes a transformer-based Asymmetric Bilateral U-Net (ABiU-Net) that combines CNN and transformer to learn both global and local representations for SOD. The ABiU-Net model features an asymmetric bilateral encoder and decoder with a transformer path and a lightweight CNN path that communicate at each encoder/decoder stage to learn complementary global contexts and local spatial details. The communication between the two paths enables ABiU-Net to take advantage of the natural properties of transformers and CNNs, respectively. The study's experiments show that ABiU-Net outperforms previous state-of-the-art SOD methods, and the code will be released.",1
"Unsupervised domain adaptive object detection aims to adapt a well-trained detector from its original source domain with rich labeled data to a new target domain with unlabeled data. Previous works focus on improving the domain adaptability of region-based detectors, e.g., Faster-RCNN, through matching cross-domain instance-level features that are explicitly extracted from a region proposal network (RPN). However, this is unsuitable for region-free detectors such as single shot detector (SSD), which perform a dense prediction from all possible locations in an image and do not have the RPN to encode such instance-level features. As a result, they fail to align important image regions and crucial instance-level features between the domains of region-free detectors. In this work, we propose an adversarial module to strengthen the cross-domain matching of instance-level features for region-free detectors. Firstly, to emphasize the important regions of image, the DSEM learns to predict a transferable foreground enhancement mask that can be utilized to suppress the background disturbance in an image. Secondly, considering that region-free detectors recognize objects of different scales using multi-scale feature maps, the DSEM encodes both multi-level semantic representations and multi-instance spatial-contextual relationships across different domains. Finally, the DSEM is pluggable into different region-free detectors, ultimately achieving the densely semantic feature matching via adversarial learning. Extensive experiments have been conducted on PASCAL VOC, Clipart, Comic, Watercolor, and FoggyCityscape benchmarks, and their results well demonstrate that the proposed approach not only improves the domain adaptability of region-free detectors but also outperforms existing domain adaptive region-based detectors under various domain shift settings.",0
"The goal of unsupervised domain adaptive object detection is to adapt a detector trained with labeled data from one domain to a new domain with unlabeled data. Region-based detectors, such as Faster-RCNN, have been the focus of previous works to improve domain adaptability by matching cross-domain instance-level features extracted from a region proposal network. However, this approach is not suitable for region-free detectors like SSD, which predict from all possible locations in an image without an RPN for feature encoding. Thus, important image regions and instance-level features cannot be aligned. This paper proposes an adversarial module that learns to predict a transferable foreground enhancement mask to suppress background disturbance and encodes multi-level semantic representations and multi-instance spatial-contextual relationships for different scales and domains. The DSEM module can be plugged into different region-free detectors and achieves densely semantic feature matching via adversarial learning. Experimental results on various benchmarks show that the proposed approach not only improves domain adaptability for region-free detectors but also outperforms existing domain adaptive region-based detectors.",1
"Domain Randomization (DR) is known to require a significant amount of training data for good performance. We argue that this is due to DR's strategy of random data generation using a uniform distribution over simulation parameters, as a result, DR often generates samples which are uninformative for the learner. In this work, we theoretically analyze DR using ideas from multi-source domain adaptation. Based on our findings, we propose Adversarial Domain Randomization (ADR) as an efficient variant of DR which generates adversarial samples with respect to the learner during training. We implement ADR as a policy whose action space is the quantized simulation parameter space. At each iteration, the policy's action generates labeled data and the reward is set as negative of learner's loss on this data. As a result, we observe ADR frequently generates novel samples for the learner like truncated and occluded objects for object detection and confusing classes for image classification. We perform evaluations on datasets like CLEVR, Syn2Real, and VIRAT for various tasks where we demonstrate that ADR outperforms DR by generating fewer data samples.",0
"To achieve good performance, Domain Randomization (DR) requires a significant amount of training data. This is attributed to DR's use of a uniform distribution over simulation parameters to generate random data, resulting in uninformative samples for the learner. In this study, we apply multi-source domain adaptation concepts to analyze DR theoretically. Our findings lead us to propose Adversarial Domain Randomization (ADR) as a more efficient variant of DR. ADR generates adversarial samples with respect to the learner during training by implementing it as a policy in the quantized simulation parameter space. The policy's action generates labeled data and the reward is set as the negative of the learner's loss on this data. ADR frequently generates novel samples for the learner, such as truncated and occluded objects for object detection and confusing classes for image classification. We evaluated ADR on various datasets, including CLEVR, Syn2Real, and VIRAT, for different tasks and demonstrated that it outperforms DR by generating fewer data samples.",1
"Knowledge distillation has become one of the most important model compression techniques by distilling knowledge from larger teacher networks to smaller student ones. Although great success has been achieved by prior distillation methods via delicately designing various types of knowledge, they overlook the functional properties of neural networks, which makes the process of applying those techniques to new tasks unreliable and non-trivial. To alleviate such problem, in this paper, we initially leverage Lipschitz continuity to better represent the functional characteristic of neural networks and guide the knowledge distillation process. In particular, we propose a novel Lipschitz Continuity Guided Knowledge Distillation framework to faithfully distill knowledge by minimizing the distance between two neural networks' Lipschitz constants, which enables teacher networks to better regularize student networks and improve the corresponding performance. We derive an explainable approximation algorithm with an explicit theoretical derivation to address the NP-hard problem of calculating the Lipschitz constant. Experimental results have shown that our method outperforms other benchmarks over several knowledge distillation tasks (e.g., classification, segmentation and object detection) on CIFAR-100, ImageNet, and PASCAL VOC datasets.",0
"The technique of knowledge distillation has grown in importance as a means of compressing models by transferring knowledge from larger teacher networks to smaller student ones. While prior methods have enjoyed success by designing various types of knowledge, they have neglected to consider the functional properties of neural networks, making it difficult to apply these techniques to new tasks. To address this issue, our paper proposes leveraging Lipschitz continuity to better represent neural network functional characteristics and guide the knowledge distillation process. Our novel Lipschitz Continuity Guided Knowledge Distillation framework distills knowledge by minimizing the distance between two neural networks' Lipschitz constants, which improves performance by enabling teacher networks to better regularize student networks. We provide an explainable approximation algorithm with explicit theoretical derivation to address the NP-hard problem of calculating the Lipschitz constant. Experimental results demonstrate that our approach outperforms other benchmarks in various knowledge distillation tasks (e.g., classification, segmentation, and object detection) on datasets such as CIFAR-100, ImageNet, and PASCAL VOC.",1
"Learning requires both study and curiosity. A good learner is not only good at extracting information from the data given to it, but also skilled at finding the right new information to learn from. This is especially true when a human operator is required to provide the ground truth - such a source should only be queried sparingly. In this work, we address the problem of curiosity as it relates to online, real-time, human-in-the-loop training of an object detection algorithm onboard a robotic platform, one where motion produces new views of the subject. We propose a deep reinforcement learning approach that decides when to ask the human user for ground truth, and when to move. Through a series of experiments, we demonstrate that our agent learns a movement and request policy that is at least 3x more effective at using human user interactions to train an object detector than untrained approaches, and is generalizable to a variety of subjects and environments.",0
"Both inquisitiveness and study are necessary for learning. A proficient learner is skilled in not only extracting information from the provided data, but also adept at discovering new information to learn from. This is particularly significant when a human operator is required to provide the accurate information, as such a source should be utilized conservatively. In this study, we aim to tackle the issue of curiosity in connection to real-time, online, human-in-the-loop training of an object detection algorithm on a robotic platform, where motion leads to new perspectives of the subject. We suggest a deep reinforcement learning approach that determines when to seek input from the human user for ground truth and when to relocate. By conducting a series of experiments, we demonstrate that our agent learns a policy for movement and request that is at least three times more effective in utilizing human user interactions to train an object detector than untrained methods, and is transferable to various subjects and environments.",1
"Point clouds and images could provide complementary information when representing 3D objects. Fusing the two kinds of data usually helps to improve the detection results. However, it is challenging to fuse the two data modalities, due to their different characteristics and the interference from the non-interest areas. To solve this problem, we propose a Multi-Branch Deep Fusion Network (MBDF-Net) for 3D object detection. The proposed detector has two stages. In the first stage, our multi-branch feature extraction network utilizes Adaptive Attention Fusion (AAF) modules to produce cross-modal fusion features from single-modal semantic features. In the second stage, we use a region of interest (RoI) -pooled fusion module to generate enhanced local features for refinement. A novel attention-based hybrid sampling strategy is also proposed for selecting key points in the downsampling process. We evaluate our approach on two widely used benchmark datasets including KITTI and SUN-RGBD. The experimental results demonstrate the advantages of our method over state-of-the-art approaches.",0
"When representing 3D objects, point clouds and images offer complementary information that, when fused together, can improve detection results. However, fusing these two data modalities is challenging due to their different characteristics and interference from non-interest areas. To address this issue, we present the Multi-Branch Deep Fusion Network (MBDF-Net) for 3D object detection. Our detector has two stages: the first stage uses Adaptive Attention Fusion (AAF) modules to produce cross-modal fusion features from single-modal semantic features, while the second stage employs a region of interest (RoI)-pooled fusion module to generate enhanced local features for refinement. We also propose a novel attention-based hybrid sampling strategy for selecting key points during the downsampling process. Our approach is evaluated on two benchmark datasets (KITTI and SUN-RGBD) and outperforms state-of-the-art approaches.",1
"As one of the basic tasks of computer vision, object detection has been widely used in many intelligent applications. However, object detection algorithms are usually heavyweight in computation, hindering their implementations on resource-constrained edge devices. Current edge-cloud collaboration methods, such as CNN partition over Edge-cloud devices, are not suitable for object detection since the huge data size of the intermediate results will introduce extravagant communication costs. To address this challenge, we propose a small-big model framework that deploys a big model in the cloud and a small model on the edge devices. Upon receiving data, the edge device operates a difficult-case discriminator to classify the images into easy cases and difficult cases according to the specific semantics of the images. The easy cases will be processed locally at the edge, and the difficult cases will be uploaded to the cloud. Experimental results on the VOC, COCO, HELMET datasets using two different object detection algorithms demonstrate that the small-big model system can detect 94.01%-97.84% of objects with only about 50% images uploaded to the cloud when using SSD. In addition, the small-big model averagely reaches 91.22%- 92.52% end-to-end mAP of the scheme that uploading all images to the cloud.",0
"Object detection is a fundamental task in computer vision with widespread applications. However, the computational complexity of object detection algorithms makes their implementation on resource-limited edge devices problematic. Existing edge-cloud collaboration models, such as CNN partitioning, are not suitable for object detection due to the large size of intermediate results, leading to high communication costs. To overcome this challenge, we propose a small-big model framework, which employs a big model in the cloud and a small model on the edge devices. The edge device uses a difficult-case discriminator to classify images based on specific semantics into easy and difficult cases. Easy cases are processed locally, while difficult cases are uploaded to the cloud. Experimental results on VOC, COCO, and HELMET datasets using two different object detection algorithms show that the small-big model can detect 94.01%-97.84% of objects with only 50% of images uploaded to the cloud using SSD. On average, the small-big model achieves 91.22%-92.52% end-to-end mAP, outperforming the scheme of uploading all images to the cloud.",1
"State-of-the-art object detection models are frequently trained offline using available datasets, such as ImageNet: large and overly diverse data that are unbalanced and hard to cluster semantically. This kind of training drops the object detection performance should the change in illumination, in the environmental conditions (e.g., rain), or in the lens positioning (out-of-focus blur) occur. We propose a decentralized hierarchical multi-agent deep reinforcement learning approach for intelligently controlling the camera and the lens focusing settings, leading to significant improvement to the capacity of the popular detection models (YOLO, Fast R-CNN, and Retina are considered). The algorithm relies on the latent representation of the camera's stream and, thus, it is the first method to allow a completely no-reference tuning of the camera, where the system trains itself to auto-focus itself.",0
"Commonly, advanced models for detecting objects are trained offline using accessible datasets like ImageNet. However, these datasets are extensive and have an uneven distribution of data, making it challenging to semantically cluster them. Consequently, this type of training can cause the object detection performance to drop when there is a variation in illumination, environmental conditions or lens positioning. To address this issue, we suggest using a decentralized hierarchical multi-agent deep reinforcement learning method to control the camera and lens focusing settings. This method enhances the ability of popular detection models like YOLO, Fast R-CNN and Retina. Our approach is based on the latent representation of the camera's stream and is the first method to allow the system to train itself to self-focus without any reference to the camera.",1
"Feature pyramids have been proven powerful in image understanding tasks that require multi-scale features. State-of-the-art methods for multi-scale feature learning focus on performing feature interactions across space and scales using neural networks with a fixed topology. In this paper, we propose graph feature pyramid networks that are capable of adapting their topological structures to varying intrinsic image structures and supporting simultaneous feature interactions across all scales. We first define an image-specific superpixel hierarchy for each input image to represent its intrinsic image structures. The graph feature pyramid network inherits its structure from this superpixel hierarchy. Contextual and hierarchical layers are designed to achieve feature interactions within the same scale and across different scales. To make these layers more powerful, we introduce two types of local channel attention for graph neural networks by generalizing global channel attention for convolutional neural networks. The proposed graph feature pyramid network can enhance the multiscale features from a convolutional feature pyramid network. We evaluate our graph feature pyramid network in the object detection task by integrating it into the Faster R-CNN algorithm. The modified algorithm outperforms not only previous state-of-the-art feature pyramid-based methods with a clear margin but also other popular detection methods on both MS-COCO 2017 validation and test datasets.",0
The effectiveness of feature pyramids in image understanding tasks requiring multi-scale features has been established. Current methods for multi-scale feature learning utilize neural networks with a fixed topology to perform feature interactions across space and scales. This paper proposes a graph feature pyramid network that can adapt its topological structure to suit different intrinsic image structures and enable simultaneous feature interactions across all scales. The network derives its structure from an image-specific superpixel hierarchy representing intrinsic image structures. Contextual and hierarchical layers facilitate feature interactions within and across scales. Local channel attention for graph neural networks is introduced to enhance the power of these layers by generalizing global channel attention for convolutional neural networks. The graph feature pyramid network therefore enhances the multiscale features of a convolutional feature pyramid network. The proposed network is evaluated in object detection tasks by integrating it into the Faster R-CNN algorithm. The modified algorithm surpasses previous state-of-the-art feature pyramid-based methods and other popular detection methods on both MS-COCO 2017 validation and test datasets.,1
"For dealing with traffic bottlenecks at airports, aircraft object detection is insufficient. Every airport generally has a variety of planes with various physical and technological requirements as well as diverse service requirements. Detecting the presence of new planes will not address all traffic congestion issues. Identifying the type of airplane, on the other hand, will entirely fix the problem because it will offer important information about the plane's technical specifications (i.e., the time it needs to be served and its appropriate place in the airport). Several studies have provided various contributions to address airport traffic jams; however, their ultimate goal was to determine the existence of airplane objects. This paper provides a practical approach to identify the type of airplane in airports depending on the results provided by the airplane detection process using mask region convolution neural network. The key feature employed to identify the type of airplane is the surface area calculated based on the results of airplane detection. The surface area is used to assess the estimated cabin length which is considered as an additional key feature for identifying the airplane type. The length of any detected plane may be calculated by measuring the distance between the detected plane's two furthest points. The suggested approach's performance is assessed using average accuracies and a confusion matrix. The findings show that this method is dependable. This method will greatly aid in the management of airport traffic congestion.",0
"Aircraft object detection alone is not sufficient to handle traffic bottlenecks at airports, as every airport has planes with varying physical and technological needs and service requirements. Detecting the presence of new planes does not address all congestion issues, but identifying the plane type can solve the problem by providing important technical information such as the time required for service and its appropriate location in the airport. Previous studies focused on determining the existence of airplane objects but did not fully address the problem. This paper presents a practical approach that uses a mask region convolution neural network to identify the airplane type by calculating its surface area and estimated cabin length. The approach's performance is evaluated using average accuracies and a confusion matrix, demonstrating its reliability. This method will greatly assist in managing airport traffic congestion.",1
"Depth Completion can produce a dense depth map from a sparse input and provide a more complete 3D description of the environment. Despite great progress made in depth completion, the sparsity of the input and low density of the ground truth still make this problem challenging. In this work, we propose DenseLiDAR, a novel real-time pseudo-depth guided depth completion neural network. We exploit dense pseudo-depth map obtained from simple morphological operations to guide the network in three aspects: (1) Constructing a residual structure for the output; (2) Rectifying the sparse input data; (3) Providing dense structural loss for training the network. Thanks to these novel designs, higher performance of the output could be achieved. In addition, two new metrics for better evaluating the quality of the predicted depth map are also presented. Extensive experiments on KITTI depth completion benchmark suggest that our model is able to achieve the state-of-the-art performance at the highest frame rate of 50Hz. The predicted dense depth is further evaluated by several downstream robotic perception or positioning tasks. For the task of 3D object detection, 3~5 percent performance gains on small objects categories are achieved on KITTI 3D object detection dataset. For RGB-D SLAM, higher accuracy on vehicle's trajectory is also obtained in KITTI Odometry dataset. These promising results not only verify the high quality of our depth prediction, but also demonstrate the potential of improving the related downstream tasks by using depth completion results.",0
"The process of Depth Completion involves generating a dense depth map from a sparse input, which leads to a more comprehensive 3D depiction of the surroundings. Although the field of depth completion has advanced significantly, the scarcity of input and the inadequacy of the ground truth continue to pose challenges. In this study, we introduce DenseLiDAR, an innovative neural network for real-time pseudo-depth guided depth completion. The network employs a dense pseudo-depth map, obtained from basic morphological operations, to guide it in three ways: (1) constructing a residual structure for the output, (2) rectifying the sparse input data, and (3) providing dense structural loss for training the network. These novel designs result in higher output performance, and two new metrics have been proposed to evaluate the quality of the predicted depth map. Our experiments on the KITTI depth completion benchmark demonstrate that our model achieves state-of-the-art performance at a frame rate of 50Hz. Furthermore, the accuracy of the predicted dense depth is evaluated using various robotic perception and positioning tasks. Our depth prediction results not only demonstrate high quality but also have the potential to improve downstream tasks in the field of depth completion.",1
"This work tackles the unsupervised cross-domain object detection problem which aims to generalize a pre-trained object detector to a new target domain without labels. We propose an uncertainty-aware model adaptation method, which is based on two motivations: 1) the estimation and exploitation of model uncertainty in a new domain is critical for reliable domain adaptation; and 2) the joint alignment of distributions for inputs (feature alignment) and outputs (self-training) is needed. To this end, we compose a Bayesian CNN-based framework for uncertainty estimation in object detection, and propose an algorithm for generation of uncertainty-aware pseudo-labels. We also devise a scheme for joint feature alignment and self-training of the object detection model with uncertainty-aware pseudo-labels. Experiments on multiple cross-domain object detection benchmarks show that our proposed method achieves state-of-the-art performance.",0
"The objective of this study is to address the challenge of unsupervised cross-domain object detection, which involves generalizing a pre-existing object detector to a new target domain without any labels. Our approach involves an uncertainty-aware model adaptation method that is based on two key factors: 1) the estimation and utilization of model uncertainty in a new domain is essential for effective domain adaptation; and 2) the joint alignment of distributions for inputs (feature alignment) and outputs (self-training) is required. To achieve this, we have developed a Bayesian CNN-based framework for uncertainty estimation in object detection, and have created an algorithm for generating uncertainty-aware pseudo-labels. In addition, we have designed a strategy for simultaneous feature alignment and self-training of the object detection model using uncertainty-aware pseudo-labels. Our proposed method has been tested on several cross-domain object detection benchmarks and has demonstrated superior performance compared to existing methods.",1
"A big part of achieving Artificial General Intelligence(AGI) is to build a machine that can see and listen like humans. Much work has focused on designing models for image classification, video classification, object detection, pose estimation, speech recognition, etc., and has achieved significant progress in recent years thanks to deep learning. However, understanding the world is not enough. An AI agent also needs to know how to talk, especially how to communicate with a human. While perception (vision, for example) is more common across animal species, the use of complicated language is unique to humans and is one of the most important aspects of intelligence.   In this thesis, we focus on generating textual output given visual input. In Chapter 3, we focus on generating the referring expression, a text description for an object in the image so that a receiver can infer which object is being described. We use a comprehension machine to directly guide the generated referring expressions to be more discriminative. In Chapter 4, we introduce a method that encourages discriminability in image caption generation. We show that more discriminative captioning models generate more descriptive captions. In Chapter 5, we study how training objectives and sampling methods affect the models' ability to generate diverse captions. We find that a popular captioning training strategy will be detrimental to the diversity of generated captions. In Chapter 6, we propose a model that can control the length of generated captions. By changing the desired length, one can influence the style and descriptiveness of the captions. Finally, in Chapter 7, we rank/generate informative image tags according to their information utility. The proposed method better matches what humans think are the most important tags for the images.",0
"To achieve AGI, it is crucial to develop a machine that can see and hear like humans. While significant progress has been made in designing models for image and video classification, object detection, pose estimation, and speech recognition using deep learning, understanding the world alone is insufficient. An AI agent must also know how to communicate with humans effectively. Our thesis focuses on generating text output from visual input. In Chapter 3, we concentrate on creating a referring expression, a text description of an object in an image, to help the receiver identify the object. We use a comprehension machine to produce more precise referral expressions. In Chapter 4, we introduce a method that encourages image caption generation models to produce more descriptive and distinctive captions. In Chapter 5, we explore how training objectives and sampling methods affect the diversity of generated captions. We find that a prevalent captioning training strategy harms the diversity of generated captions. In Chapter 6, our model can control the length of generated captions, allowing one to influence the style and descriptiveness of the captions. Finally, in Chapter 7, we develop a method to rank/generate informative image tags based on their information utility to match human preferences.",1
"One-stage object detection is commonly implemented by optimizing two sub-tasks: object classification and localization, using heads with two parallel branches, which might lead to a certain level of spatial misalignment in predictions between the two tasks. In this work, we propose a Task-aligned One-stage Object Detection (TOOD) that explicitly aligns the two tasks in a learning-based manner. First, we design a novel Task-aligned Head (T-Head) which offers a better balance between learning task-interactive and task-specific features, as well as a greater flexibility to learn the alignment via a task-aligned predictor. Second, we propose Task Alignment Learning (TAL) to explicitly pull closer (or even unify) the optimal anchors for the two tasks during training via a designed sample assignment scheme and a task-aligned loss. Extensive experiments are conducted on MS-COCO, where TOOD achieves a 51.1 AP at single-model single-scale testing. This surpasses the recent one-stage detectors by a large margin, such as ATSS (47.7 AP), GFL (48.2 AP), and PAA (49.0 AP), with fewer parameters and FLOPs. Qualitative results also demonstrate the effectiveness of TOOD for better aligning the tasks of object classification and localization. Code is available at https://github.com/fcjian/TOOD.",0
"Commonly, one-stage object detection involves optimizing two sub-tasks, namely object classification and localization, through the use of heads with parallel branches. This approach may result in spatial misalignment in predictions between the two tasks. To address this issue, we propose a Task-aligned One-stage Object Detection (TOOD) that uses a learning-based method to align the two tasks. Our approach involves designing a Task-aligned Head (T-Head) that balances learning task-specific and task-interactive features, and allows for greater flexibility in learning the alignment via a task-aligned predictor. Additionally, we propose Task Alignment Learning (TAL) to bring the optimal anchors for the two tasks closer together during training, using a designed sample assignment scheme and a task-aligned loss. Our experiments on MS-COCO show that TOOD outperforms recent one-stage detectors, such as ATSS, GFL, and PAA, with fewer parameters and FLOPs. The effectiveness of TOOD in aligning object classification and localization is also demonstrated through qualitative results. Code for TOOD is available at https://github.com/fcjian/TOOD.",1
"Automatic detection of prohibited items within complex and cluttered X-ray security imagery is essential to maintaining transport security, where prior work on automatic prohibited item detection focus primarily on pseudo-colour (rgb}) X-ray imagery. In this work we study the impact of variant X-ray imagery, i.e., X-ray energy response (high, low}) and effective-z compared to rgb, via the use of deep Convolutional Neural Networks (CNN) for the joint object detection and segmentation task posed within X-ray baggage security screening. We evaluate state-of-the-art CNN architectures (Mask R-CNN, YOLACT, CARAFE and Cascade Mask R-CNN) to explore the transferability of models trained with such 'raw' variant imagery between the varying X-ray security scanners that exhibits differing imaging geometries, image resolutions and material colour profiles. Overall, we observe maximal detection performance using CARAFE, attributable to training using combination of rgb, high, low, and effective-z X-ray imagery, obtaining 0.7 mean Average Precision (mAP) for a six class object detection problem. Our results also exhibit a remarkable degree of generalisation capability in terms of cross-scanner transferability (AP: 0.835/0.611) for a one class object detection problem by combining rgb, high, low, and effective-z imagery.",0
"To maintain transport security, it is crucial to automatically detect prohibited items in complex and cluttered X-ray security images. Previous research on automatic prohibited item detection has primarily focused on pseudo-colour X-ray imagery. In this study, we examine the use of deep Convolutional Neural Networks (CNN) for joint object detection and segmentation in X-ray baggage security screening. We evaluate various CNN architectures to investigate the transferability of models trained on variant X-ray imagery (i.e., X-ray energy response and effective-z) between different X-ray security scanners with varying imaging geometries, resolutions, and material colour profiles. Our results show that using a combination of rgb, high, low, and effective-z X-ray imagery in training leads to the best performance, achieving a mean Average Precision (mAP) of 0.7 for a six-class object detection problem using the CARAFE architecture. Additionally, we demonstrate strong generalisation capability in terms of cross-scanner transferability for a one-class object detection problem.",1
"Automotive radar sensors output a lot of unwanted clutter or ghost detections, whose position and velocity do not correspond to any real object in the sensor's field of view. This poses a substantial challenge for environment perception methods like object detection or tracking. Especially problematic are clutter detections that occur in groups or at similar locations in multiple consecutive measurements. In this paper, a new algorithm for identifying such erroneous detections is presented. It is mainly based on the modeling of specific commonly occurring wave propagation paths that lead to clutter. In particular, the three effects explicitly covered are reflections at the underbody of a car or truck, signals traveling back and forth between the vehicle on which the sensor is mounted and another object, and multipath propagation via specular reflection. The latter often occurs near guardrails, concrete walls or similar reflective surfaces. Each of these effects is described both theoretically and regarding a method for identifying the corresponding clutter detections. Identification is done by analyzing detections generated from a single sensor measurement only. The final algorithm is evaluated on recordings of real extra-urban traffic. For labeling, a semi-automatic process is employed. The results are promising, both in terms of performance and regarding the very low execution time. Typically, a large part of clutter is found, while only a small ratio of detections corresponding to real objects are falsely classified by the algorithm.",0
"The output of automotive radar sensors often includes false detections known as clutter or ghost detections. These detections do not correspond to any actual objects in the sensor's field of view and can create difficulties for object detection and tracking. Clutter detections that occur in groups or at similar locations in consecutive measurements are particularly challenging. This paper presents a new algorithm that identifies such erroneous detections. The algorithm is based on the modeling of common wave propagation paths that lead to clutter, including reflections at the underbody of a vehicle, signals traveling back and forth between the sensor-mounted vehicle and other objects, and multipath propagation via specular reflection. The algorithm analyzes detections from a single sensor measurement and is evaluated on recordings of real extra-urban traffic using a semi-automatic labeling process. The results are promising, with a large portion of clutter identified and a low false classification rate for detections corresponding to real objects. Additionally, the execution time is very low.",1
"Vehicular object detection is the heart of any intelligent traffic system. It is essential for urban traffic management. R-CNN, Fast R-CNN, Faster R-CNN and YOLO were some of the earlier state-of-the-art models. Region based CNN methods have the problem of higher inference time which makes it unrealistic to use the model in real-time. YOLO on the other hand struggles to detect small objects that appear in groups. In this paper, we propose a method that can locate and classify vehicular objects from a given densely crowded image using YOLOv5. The shortcoming of YOLO was solved my ensembling 4 different models. Our proposed model performs well on images taken from both top view and side view of the street in both day and night. The performance of our proposed model was measured on Dhaka AI dataset which contains densely crowded vehicular images. Our experiment shows that our model achieved mAP@0.5 of 0.458 with inference time of 0.75 sec which outperforms other state-of-the-art models on performance. Hence, the model can be implemented in the street for real-time traffic detection which can be used for traffic control and data collection.",0
"The detection of vehicles is crucial for the functioning of intelligent traffic systems in urban areas. Previous models such as R-CNN, Fast R-CNN, Faster R-CNN and YOLO have been considered state-of-the-art, but they have drawbacks such as high inference time or difficulty detecting small objects in groups. Our paper presents a solution by using YOLOv5 and combining four models to locate and classify vehicles in densely crowded images. Our proposed model performs well in both top and side views of streets, in both day and night, and has been tested on the Dhaka AI dataset. Our model outperforms other state-of-the-art models with an mAP@0.5 of 0.458 and an inference time of 0.75 sec. This makes it ideal for real-time traffic detection, which can be used for traffic control and data collection.",1
"Classification and regression are two pillars of object detectors. In most CNN-based detectors, these two pillars are optimized independently. Without direct interactions between them, the classification loss and the regression loss can not be optimized synchronously toward the optimal direction in the training phase. This clearly leads to lots of inconsistent predictions with high classification score but low localization accuracy or low classification score but high localization accuracy in the inference phase, especially for the objects of irregular shape and occlusion, which severely hurts the detection performance of existing detectors after NMS. To reconcile prediction consistency for balanced object detection, we propose a Harmonic loss to harmonize the optimization of classification branch and localization branch. The Harmonic loss enables these two branches to supervise and promote each other during training, thereby producing consistent predictions with high co-occurrence of top classification and localization in the inference phase. Furthermore, in order to prevent the localization loss from being dominated by outliers during training phase, a Harmonic IoU loss is proposed to harmonize the weight of the localization loss of different IoU-level samples. Comprehensive experiments on benchmarks PASCAL VOC and MS COCO demonstrate the generality and effectiveness of our model for facilitating existing object detectors to state-of-the-art accuracy.",0
"Object detectors rely on two main components: classification and regression. In CNN-based detectors, these components are usually optimized separately, resulting in inconsistent predictions during training. This is particularly problematic for objects with irregular shapes and occlusion, leading to poor detection performance after NMS. To address this issue, we propose the Harmonic loss, which harmonizes the optimization of the classification and localization branches. By supervising and promoting each other during training, the Harmonic loss produces consistent predictions with high classification and localization accuracy. Additionally, we introduce the Harmonic IoU loss to prevent the localization loss from being dominated by outliers. Our approach is effective across benchmarks and facilitates existing object detectors to achieve state-of-the-art accuracy.",1
"A growing number of commercially available mobile phones come with integrated high-resolution digital cameras. That enables a new class of dedicated applications to image analysis such as mobile visual search, image cropping, object detection, content-based image retrieval, image classification. In this paper, a new mobile application for image content retrieval and classification for mobile device display is proposed to enrich the visual experience of users. The mobile application can extract a certain number of images based on the content of an image with visual saliency methods aiming at detecting the most critical regions in a given image from a perceptual viewpoint. First, the most critical areas from a perceptual perspective are extracted using the local maxima of a 2D saliency function. Next, a salient region is cropped using the bounding box centred on the local maxima of the thresholded Saliency Map of the image. Then, each image crop feds into an Image Classification system based on SVM and SIFT descriptors to detect the class of object present in the image. ImageNet repository was used as the reference for semantic category classification. Android platform was used to implement the mobile application on a client-server architecture. A mobile client sends the photo taken by the camera to the server, which processes the image and returns the results (image contents such as image crops and related target classes) to the mobile client. The application was run on thousands of pictures and showed encouraging results towards a better user visual experience with mobile displays.",0
"The utilization of high-resolution digital cameras integrated into mobile phones has led to the development of dedicated applications in image analysis, such as mobile visual search, content-based image retrieval, and image classification. This paper proposes a new mobile application that enhances users' visual experience by retrieving and classifying image content on mobile device displays. The application utilizes visual saliency methods to extract critical regions in an image from a perceptual perspective. The local maxima of a 2D saliency function are used to extract the most critical areas, followed by cropping a salient region using the bounding box centered on the thresholded Saliency Map's local maxima. The cropped images are then fed into an Image Classification system based on SVM and SIFT descriptors to detect the object's class present in the image, using the ImageNet repository as the reference for semantic category classification. The mobile application was implemented on an Android platform using a client-server architecture, where the mobile client sends the photo taken by the camera to the server, which processes the image and returns the results, such as image crops and related target classes, to the mobile client. The application was tested on thousands of pictures and displayed encouraging results towards enhancing users' visual experience with mobile displays.",1
"Computer vision tasks such as object detection and semantic/instance segmentation rely on the painstaking annotation of large training datasets. In this paper, we propose LocTex that takes advantage of the low-cost localized textual annotations (i.e., captions and synchronized mouse-over gestures) to reduce the annotation effort. We introduce a contrastive pre-training framework between images and captions and propose to supervise the cross-modal attention map with rendered mouse traces to provide coarse localization signals. Our learned visual features capture rich semantics (from free-form captions) and accurate localization (from mouse traces), which are very effective when transferred to various downstream vision tasks. Compared with ImageNet supervised pre-training, LocTex can reduce the size of the pre-training dataset by 10x or the target dataset by 2x while achieving comparable or even improved performance on COCO instance segmentation. When provided with the same amount of annotations, LocTex achieves around 4% higher accuracy than the previous state-of-the-art ""vision+language"" pre-training approach on the task of PASCAL VOC image classification.",0
"Object detection and semantic/instance segmentation in computer vision rely heavily on the laborious annotation of large training datasets. Our paper proposes a solution called LocTex that utilizes localized textual annotations (captions and synchronized mouse-over gestures) to decrease the annotation effort. We present a contrastive pre-training framework for images and captions and suggest supervising the cross-modal attention map with rendered mouse traces to provide rough localization signals. Our visual features learn rich semantics and accurate localization, which effectively transfer to different vision tasks. Compared to ImageNet supervised pre-training, LocTex can reduce the pre-training dataset's size by 10 times or the target dataset's size by 2 times while achieving comparable or improved performance on COCO instance segmentation. When given the same amount of annotations, LocTex attains approximately 4% higher accuracy than the previous state-of-the-art ""vision+language"" pre-training approach on the PASCAL VOC image classification task.",1
"In this paper, we present Co-scale conv-attentional image Transformers (CoaT), a Transformer-based image classifier equipped with co-scale and conv-attentional mechanisms. First, the co-scale mechanism maintains the integrity of Transformers' encoder branches at individual scales, while allowing representations learned at different scales to effectively communicate with each other; we design a series of serial and parallel blocks to realize the co-scale mechanism. Second, we devise a conv-attentional mechanism by realizing a relative position embedding formulation in the factorized attention module with an efficient convolution-like implementation. CoaT empowers image Transformers with enriched multi-scale and contextual modeling capabilities. On ImageNet, relatively small CoaT models attain superior classification results compared with similar-sized convolutional neural networks and image/vision Transformers. The effectiveness of CoaT's backbone is also illustrated on object detection and instance segmentation, demonstrating its applicability to downstream computer vision tasks.",0
"The purpose of this research is to introduce Co-scale conv-attentional image Transformers (CoaT), a Transformer-based image classifier that features co-scale and conv-attentional mechanisms. The co-scale mechanism maintains the integrity of Transformers' encoder branches at different scales, enabling effective communication between learned representations. The co-scale mechanism is achieved through the use of serial and parallel blocks. Additionally, a conv-attentional mechanism is developed by implementing a relative position embedding formulation in the factorized attention module with an efficient convolution-like implementation. CoaT enhances image Transformers by providing multi-scale and contextual modeling capabilities. The results of experiments conducted using ImageNet show that relatively small CoaT models perform better in classification than similar-sized convolutional neural networks and image/vision Transformers. The effectiveness of CoaT's backbone is also demonstrated in object detection and instance segmentation, highlighting its usefulness in downstream computer vision tasks.",1
"Deep neural networks, especially convolutional deep neural networks, are state-of-the-art methods to classify, segment or even generate images, movies, or sounds. However, these methods lack of a good semantic understanding of what happens internally. The question, why a COVID-19 detector has classified a stack of lung-ct images as positive, is sometimes more interesting than the overall specificity and sensitivity. Especially when human domain expert knowledge disagrees with the given output. This way, human domain experts could also be advised to reconsider their choice, regarding the information pointed out by the system. In addition, the deep learning model can be controlled, and a present dataset bias can be found. Currently, most explainable AI methods in the computer vision domain are purely used on image classification, where the images are ordinary images in the visible spectrum. As a result, there is no comparison on how the methods behave with multimodal image data, as well as most methods have not been investigated on how they behave when used for object detection. This work tries to close the gaps. Firstly, investigating three saliency map generator methods on how their maps differ across the different spectra. This is achieved via accurate and systematic training. Secondly, we examine how they behave when used for object detection. As a practical problem, we chose object detection in the infrared and visual spectrum for autonomous driving. The dataset used in this work is the Multispectral Object Detection Dataset, where each scene is available in the FIR, MIR and NIR as well as visual spectrum. The results show that there are differences between the infrared and visual activation maps. Further, an advanced training with both, the infrared and visual data not only improves the network's output, it also leads to more focused spots in the saliency maps.",0
"Convolutional deep neural networks are cutting-edge techniques for classifying, segmenting, and generating images, movies, or sounds. However, these methods lack a thorough understanding of the internal semantics. Sometimes, it is more intriguing to know why a COVID-19 detector has classified a stack of lung-ct images as positive, instead of just the overall sensitivity and specificity. This is especially true when human experts disagree with the system's output. This approach can help domain experts reconsider their decisions based on the system's recommendations. Moreover, this study aims to develop explainable AI methods for object detection using multimodal image data. Specifically, we investigate three saliency map generator methods and compare their performance across different spectra. We also examine how they work in object detection, using the Multispectral Object Detection Dataset for autonomous driving. The results indicate significant differences in the infrared and visual activation maps, and advanced training with both types of data leads to more focused spots in the saliency maps.",1
"With the development of underwater object grabbing technology, underwater object recognition and segmentation of high accuracy has become a challenge. The existing underwater object detection technology can only give the general position of an object, unable to give more detailed information such as the outline of the object, which seriously affects the grabbing efficiency. To address this problem, we label and establish the first underwater semantic segmentation dataset of real scene(DUT-USEG:DUT Underwater Segmentation Dataset). The DUT- USEG dataset includes 6617 images, 1487 of which have semantic segmentation and instance segmentation annotations, and the remaining 5130 images have object detection box annotations. Based on this dataset, we propose a semi-supervised underwater semantic segmentation network focusing on the boundaries(US-Net: Underwater Segmentation Network). By designing a pseudo label generator and a boundary detection subnetwork, this network realizes the fine learning of boundaries between underwater objects and background, and improves the segmentation effect of boundary areas. Experiments show that the proposed method improves by 6.7% in three categories of holothurian, echinus, starfish in DUT-USEG dataset, and achieves state-of-the-art results. The DUT- USEG dataset will be released at https://github.com/baxiyi/DUT-USEG.",0
"The accuracy of underwater object recognition and segmentation has become a challenge due to the advancement of underwater object grabbing technology. The existing underwater object detection technology provides only a general position of an object, which is inadequate for precise grabbing. Therefore, to solve this issue, we created the DUT-USEG dataset, which includes 6617 images with annotations of semantic segmentation, instance segmentation, and object detection box. Based on this dataset, we developed the US-Net, a semi-supervised underwater semantic segmentation network that focuses on boundaries. This network employs a pseudo label generator and a boundary detection subnetwork to improve the segmentation effect of boundary areas. The experiments showed that our proposed method achieved state-of-the-art results and improved the segmentation accuracy by 6.7% for three categories of underwater objects, namely holothurian, echinus, and starfish in the DUT-USEG dataset. The DUT-USEG dataset is available at https://github.com/baxiyi/DUT-USEG.",1
"Object detection on drone-captured scenarios is a recent popular task. As drones always navigate in different altitudes, the object scale varies violently, which burdens the optimization of networks. Moreover, high-speed and low-altitude flight bring in the motion blur on the densely packed objects, which leads to great challenge of object distinction. To solve the two issues mentioned above, we propose TPH-YOLOv5. Based on YOLOv5, we add one more prediction head to detect different-scale objects. Then we replace the original prediction heads with Transformer Prediction Heads (TPH) to explore the prediction potential with self-attention mechanism. We also integrate convolutional block attention model (CBAM) to find attention region on scenarios with dense objects. To achieve more improvement of our proposed TPH-YOLOv5, we provide bags of useful strategies such as data augmentation, multiscale testing, multi-model integration and utilizing extra classifier. Extensive experiments on dataset VisDrone2021 show that TPH-YOLOv5 have good performance with impressive interpretability on drone-captured scenarios. On DET-test-challenge dataset, the AP result of TPH-YOLOv5 are 39.18%, which is better than previous SOTA method (DPNetV3) by 1.81%. On VisDrone Challenge 2021, TPHYOLOv5 wins 5th place and achieves well-matched results with 1st place model (AP 39.43%). Compared to baseline model (YOLOv5), TPH-YOLOv5 improves about 7%, which is encouraging and competitive.",0
"The task of detecting objects in drone-captured scenarios is currently popular, but it presents challenges due to the varying object scales caused by drones navigating at different altitudes. Additionally, densely packed objects may experience motion blur during high-speed, low-altitude flights, making it difficult to distinguish them. Our proposed solution, TPH-YOLOv5, builds on YOLOv5 by adding a prediction head to detect different-scale objects and replacing the original prediction heads with Transformer Prediction Heads (TPH) that utilize self-attention mechanisms. We also integrate the convolutional block attention model (CBAM) to identify attention regions in scenarios with dense objects. To further improve TPH-YOLOv5, we implement several strategies such as data augmentation, multiscale testing, multi-model integration, and using an extra classifier. Our experiments on the VisDrone2021 dataset demonstrate that TPH-YOLOv5 achieves good performance and interpretability on drone-captured scenarios. TPH-YOLOv5 outperforms the previous state-of-the-art method (DPNetV3) by 1.81% on the DET-test-challenge dataset, achieving an AP result of 39.18%. On the VisDrone Challenge 2021, TPH-YOLOv5 places 5th and achieves results similar to the 1st place model (AP 39.43%). Compared to the baseline model (YOLOv5), TPH-YOLOv5 delivers a 7% improvement, making it a competitive and encouraging solution.",1
"Labeling semantic segmentation datasets is a costly and laborious process if compared with tasks like image classification and object detection. This is especially true for remote sensing applications that not only work with extremely high spatial resolution data but also commonly require the knowledge of experts of the area to perform the manual labeling. Data augmentation techniques help to improve deep learning models under the circumstance of few and imbalanced labeled samples. In this work, we propose a novel data augmentation method focused on exploring the spatial context of remote sensing semantic segmentation. This method, ChessMix, creates new synthetic images from the existing training set by mixing transformed mini-patches across the dataset in a chessboard-like grid. ChessMix prioritizes patches with more examples of the rarest classes to alleviate the imbalance problems. The results in three diverse well-known remote sensing datasets show that this is a promising approach that helps to improve the networks' performance, working especially well in datasets with few available data. The results also show that ChessMix is capable of improving the segmentation of objects with few labeled pixels when compared to the most common data augmentation methods widely used.",0
"Semantic segmentation datasets require extensive and expensive labeling efforts, particularly in remote sensing applications that involve high-resolution data and expert knowledge. To address the challenge of few and imbalanced labeled samples, data augmentation techniques are employed to enhance deep learning models. In this study, we introduce a novel data augmentation method called ChessMix, which leverages the spatial context of remote sensing semantic segmentation. ChessMix generates synthetic images by blending mini-patches in a chessboard-like pattern, with a focus on rare classes to address imbalance issues. Our analysis on three different remote sensing datasets demonstrates that ChessMix is a promising approach that enhances the performance of deep learning networks, particularly in the context of datasets with limited training samples. Furthermore, ChessMix outperforms other commonly used data augmentation methods in segmenting objects with limited labeled pixels.",1
"Label assignment in object detection aims to assign targets, foreground or background, to sampled regions in an image. Unlike labeling for image classification, this problem is not well defined due to the object's bounding box. In this paper, we investigate the problem from a perspective of distillation, hence we call Label Assignment Distillation (LAD). Our initial motivation is very simple, we use a teacher network to generate labels for the student. This can be achieved in two ways: either using the teacher's prediction as the direct targets (soft label), or through the hard labels dynamically assigned by the teacher (LAD). Our experiments reveal that: (i) LAD is more effective than soft-label, but they are complementary. (ii) Using LAD, a smaller teacher can also improve a larger student significantly, while soft-label can't. We then introduce Co-learning LAD, in which two networks simultaneously learn from scratch and the role of teacher and student are dynamically interchanged. Using PAA-ResNet50 as a teacher, our LAD techniques can improve detectors PAA-ResNet101 and PAA-ResNeXt101 to $46 \rm AP$ and $47.5\rm AP$ on the COCO test-dev set. With a strong teacher PAA-SwinB, we improve the PAA-ResNet50 to $43.9\rm AP$ with only \1x schedule training, and PAA-ResNet101 to $47.9\rm AP$, significantly surpassing the current methods. Our source code and checkpoints will be released at https://github.com/cybercore-co-ltd/CoLAD_paper.",0
"Object detection involves assigning targets, foreground or background, to regions sampled from an image. However, this task is not well-defined due to the object's bounding box, unlike image classification. To address this issue, we propose Label Assignment Distillation (LAD) as a method of distilling knowledge from a teacher network to a student. The teacher network generates labels for the student either through soft labeling or through the teacher's hard labels, dynamically assigned (LAD). Our experiments demonstrate that LAD is more effective than soft labeling and can improve a larger student significantly, while soft labeling cannot. We introduce Co-learning LAD, where two networks simultaneously learn from scratch, and the teacher-student roles are dynamically interchanged. Our LAD techniques, using PAA-ResNet50 as a teacher, improve detectors PAA-ResNet101 and PAA-ResNeXt101 to $46 \rm AP$ and $47.5\rm AP$ on the COCO test-dev set. With a strong teacher PAA-SwinB, we improve PAA-ResNet50 and PAA-ResNet101 to $43.9\rm AP$ and $47.9\rm AP$, respectively, surpassing current methods. Our source code and checkpoints will be available at https://github.com/cybercore-co-ltd/CoLAD_paper.",1
"Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehending the theories of deep learning, reinforcement learning, and deep reinforcement learning. We then propose a categorization of deep reinforcement learning methodologies and discuss their advantages and limitations. In particular, we divide deep reinforcement learning into seven main categories according to their applications in computer vision, i.e. (i)landmark localization (ii) object detection; (iii) object tracking; (iv) registration on both 2D image and 3D image volumetric data (v) image segmentation; (vi) videos analysis; and (vii) other applications. Each of these categories is further analyzed with reinforcement learning techniques, network design, and performance. Moreover, we provide a comprehensive analysis of the existing publicly available datasets and examine source code availability. Finally, we present some open issues and discuss future research directions on deep reinforcement learning in computer vision",0
"The use of deep reinforcement learning enhances the reinforcement learning framework by incorporating the advanced capabilities of deep neural networks. Recent studies have showcased the impressive achievements of deep reinforcement learning in different fields, such as finance, medicine, healthcare, video games, robotics, and computer vision. The present work offers an in-depth review of the latest research developments and state-of-the-art advances in deep reinforcement learning in the domain of computer vision. We begin by exploring the fundamental concepts of deep learning, reinforcement learning, and deep reinforcement learning. We then propose a classification system for deep reinforcement learning approaches and discuss their respective benefits and limitations. Specifically, we divide deep reinforcement learning into seven main categories based on their applications in computer vision, including landmark localization, object detection, object tracking, registration for both 2D and 3D image volumetric data, image segmentation, videos analysis, and other applications. Each of these categories is further scrutinized with reinforcement learning techniques, network design, and performance. Additionally, we conduct a thorough analysis of publicly available datasets and examine the availability of source code. Finally, we present outstanding issues and deliberations on future research directions related to deep reinforcement learning in computer vision.",1
"We explore object detection with two attributes: color and material. The task aims to simultaneously detect objects and infer their color and material. A straight-forward approach is to add attribute heads at the very end of a usual object detection pipeline. However, we observe that the two goals are in conflict: Object detection should be attribute-independent and attributes be largely object-independent. Features computed by a standard detection network entangle the category and attribute features; we disentangle them by the use of a two-stream model where the category and attribute features are computed independently but the classification heads share Regions of Interest (RoIs). Compared with a traditional single-stream model, our model shows significant improvements over VG-20, a subset of Visual Genome, on both supervised and attribute transfer tasks.",0
"Our focus is on object detection through the identification of color and material. The objective is to detect objects while also determining their respective color and material. A common approach is to integrate attribute heads to the end of the object detection pipeline. However, we have observed a conflict between the two goals. Object detection should not depend on attributes, and attributes should not be influenced by objects. Standard detection networks create a connection between category and attribute features. To resolve this, we use a two-stream model that independently computes category and attribute features, but shares Regions of Interest (RoIs) for classification. Our model demonstrates significant improvement over VG-20, a subset of Visual Genome, in both supervised and attribute transfer tasks, when compared to a traditional single-stream model.",1
"Weakly Supervised Object Detection (WSOD), aiming to train detectors with only image-level annotations, has arisen increasing attention. Current state-of-the-art approaches mainly follow a two-stage training strategy whichintegrates a fully supervised detector (FSD) with a pure WSOD model. There are two main problems hindering the performance of the two-phase WSOD approaches, i.e., insufficient learning problem and strict reliance between the FSD and the pseudo ground truth (PGT) generated by theWSOD model. This paper proposes pseudo ground truth refinement network (PGTRNet), a simple yet effective method without introducing any extra learnable parameters, to cope with these problems. PGTRNet utilizes multiple bounding boxes to establish the PGT, mitigating the insufficient learning problem. Besides, we propose a novel online PGT refinement approach to steadily improve the quality of PGTby fully taking advantage of the power of FSD during the second-phase training, decoupling the first and second-phase models. Elaborate experiments are conducted on the PASCAL VOC 2007 benchmark to verify the effectiveness of our methods. Experimental results demonstrate that PGTRNet boosts the backbone model by 2.074% mAP and achieves the state-of-the-art performance, showing the significant potentials of the second-phase training.",0
"The attention towards Weakly Supervised Object Detection (WSOD), which aims to train detectors using only image-level annotations, has increased. The current approach involves a two-stage training strategy that combines a fully supervised detector (FSD) with a pure WSOD model. However, this approach faces two significant issues: insufficient learning and strict reliance on the pseudo ground truth (PGT) generated by the WSOD model. To address these problems, this paper proposes a simple yet effective method called the pseudo ground truth refinement network (PGTRNet), which uses multiple bounding boxes to create the PGT and mitigates the insufficient learning problem. Additionally, an online PGT refinement approach is proposed to improve the quality of PGT by fully utilizing the FSD's power during the second-phase training, thereby decoupling the first and second-phase models. Experiments conducted on the PASCAL VOC 2007 benchmark demonstrate that PGTRNet enhances the backbone model by 2.074% mAP and achieves state-of-the-art performance, highlighting the potential of the second-phase training.",1
"Existing deep learning-based approaches for monocular 3D object detection in autonomous driving often model the object as a rotated 3D cuboid while the object's geometric shape has been ignored. In this work, we propose an approach for incorporating the shape-aware 2D/3D constraints into the 3D detection framework. Specifically, we employ the deep neural network to learn distinguished 2D keypoints in the 2D image domain and regress their corresponding 3D coordinates in the local 3D object coordinate first. Then the 2D/3D geometric constraints are built by these correspondences for each object to boost the detection performance. For generating the ground truth of 2D/3D keypoints, an automatic model-fitting approach has been proposed by fitting the deformed 3D object model and the object mask in the 2D image. The proposed framework has been verified on the public KITTI dataset and the experimental results demonstrate that by using additional geometrical constraints the detection performance has been significantly improved as compared to the baseline method. More importantly, the proposed framework achieves state-of-the-art performance with real time. Data and code will be available at https://github.com/zongdai/AutoShape",0
"The current deep learning-based techniques for autonomous driving's monocular 3D object detection generally represent the object as a rotated 3D cuboid and disregard the object's geometric shape. This paper suggests a new method that incorporates shape-aware 2D/3D constraints into the 3D detection framework. The approach employs a deep neural network to learn unique 2D keypoints in the 2D image domain and then regress their corresponding 3D coordinates in the local 3D object coordinate. These correspondences are then used to create 2D/3D geometric constraints for each object to improve detection performance. To generate the ground truth of 2D/3D keypoints, a model-fitting technique has been proposed by fitting the deformed 3D object model and the object mask in the 2D image. The proposed framework has been tested on the KITTI dataset, and the results show that the detection performance has significantly improved compared to the baseline method by using additional geometrical constraints. Furthermore, the proposed framework achieves state-of-the-art performance with real-time processing. The data and code for the proposed framework are available at https://github.com/zongdai/AutoShape.",1
"Most of the existing object detection methods generate poor glass detection results, due to the fact that the transparent glass shares the same appearance with arbitrary objects behind it in an image. Different from traditional deep learning-based wisdoms that simply use the object boundary as auxiliary supervision, we exploit label decoupling to decompose the original labeled ground-truth (GT) map into an interior-diffusion map and a boundary-diffusion map. The GT map in collaboration with the two newly generated maps breaks the imbalanced distribution of the object boundary, leading to improved glass detection quality. We have three key contributions to solve the transparent glass detection problem: (1) We propose a three-stream neural network (call GlassNet for short) to fully absorb beneficial features in the three maps. (2) We design a multi-scale interactive dilation module to explore a wider range of contextual information. (3) We develop an attention-based boundary-aware feature Mosaic module to integrate multi-modal information. Extensive experiments on the benchmark dataset exhibit clear improvements of our method over SOTAs, in terms of both the overall glass detection accuracy and boundary clearness.",0
"The detection of transparent glass objects in images remains a challenge for most existing object detection methods as they often produce poor results due to the similarity of the glass with other objects behind it. Our approach differs from traditional deep learning-based methods that rely on object boundaries as auxiliary supervision. Instead, we use label decoupling to break down the original labeled ground-truth (GT) map into an interior-diffusion map and a boundary-diffusion map, which, when used in collaboration with the GT map, improve the detection quality of transparent glass. Our approach involves three key contributions: (1) the proposal of a three-stream neural network, GlassNet, to extract useful features from the three maps, (2) the design of a multi-scale interactive dilation module to explore contextual information more broadly, and (3) the development of an attention-based boundary-aware feature Mosaic module to integrate multi-modal information. Our method outperforms state-of-the-art methods in terms of overall glass detection accuracy and boundary clearness, as demonstrated by extensive experiments on benchmark datasets.",1
"SSD (Single Shot Multibox Detector) is one of the most successful object detectors for its high accuracy and fast speed. However, the features from shallow layer (mainly Conv4_3) of SSD lack semantic information, resulting in poor performance in small objects. In this paper, we proposed DDSSD (Dilation and Deconvolution Single Shot Multibox Detector), an enhanced SSD with a novel feature fusion module which can improve the performance over SSD for small object detection. In the feature fusion module, dilation convolution module is utilized to enlarge the receptive field of features from shallow layer and deconvolution module is adopted to increase the size of feature maps from high layer. Our network achieves 79.7% mAP on PASCAL VOC2007 test and 28.3% mmAP on MS COCO test-dev at 41 FPS with only 300x300 input using a single Nvidia 1080 GPU. Especially, for small objects, DDSSD achieves 10.5% on MS COCO and 22.8% on FLIR thermal dataset, outperforming a lot of state-of-the-art object detection algorithms in both aspects of accuracy and speed.",0
"SSD is renowned for its high accuracy and fast speed as an object detector. Nevertheless, its shallow layer features, specifically Conv4_3, lack semantic information, resulting in poor small object detection performance. Thus, we present DDSSD, or Dilation and Deconvolution Single Shot Multibox Detector, which improves on SSD by incorporating a feature fusion module. Our module employs dilation convolution to enlarge the receptive field of shallow layer features and deconvolution to increase the size of high layer feature maps. With only a single Nvidia 1080 GPU and a 300x300 input, our network achieves 79.7% mAP on PASCAL VOC2007 test and 28.3% mmAP on MS COCO test-dev at 41 FPS. DDSSD excels in small object detection, achieving 10.5% on MS COCO and 22.8% on FLIR thermal dataset, surpassing many state-of-the-art object detection algorithms in both accuracy and speed.",1
"Object detection has been applied in a wide variety of real world scenarios, so detection algorithms must provide confidence in the results to ensure that appropriate decisions can be made based on their results. Accordingly, several studies have investigated the probabilistic confidence of bounding box regression. However, such approaches have been restricted to anchor-based detectors, which use box confidence values as additional screening scores during non-maximum suppression (NMS) procedures. In this paper, we propose a more efficient uncertainty-aware dense detector (UADET) that predicts four-directional localization uncertainties via Gaussian modeling. Furthermore, a simple uncertainty attention module (UAM) that exploits box confidence maps is proposed to improve performance through feature refinement. Experiments using the MS COCO benchmark show that our UADET consistently surpasses baseline FCOS, and that our best model, ResNext-64x4d-101-DCN, obtains a single model, single-scale AP of 48.3% on COCO test-dev, thus achieving the state-of-the-art among various object detectors.",0
"The confidence of object detection results is crucial in making appropriate decisions in real world applications. Bounding box regression has been studied to determine the probabilistic confidence of anchor-based detectors, which use box confidence values during non-maximum suppression procedures. However, this approach has limitations. To address this, we propose a more efficient detector, UADET, which predicts four-directional localization uncertainties using Gaussian modeling, and a UAM module that uses box confidence maps for feature refinement. Experiments using the MS COCO benchmark show that our UADET outperforms FCOS and achieves state-of-the-art results with a single-scale AP of 48.3% on COCO test-dev using the ResNext-64x4d-101-DCN model.",1
"Activation functions play a pivotal role in determining the training dynamics and neural network performance. The widely adopted activation function ReLU despite being simple and effective has few disadvantages including the Dying ReLU problem. In order to tackle such problems, we propose a novel activation function called Serf which is self-regularized and nonmonotonic in nature. Like Mish, Serf also belongs to the Swish family of functions. Based on several experiments on computer vision (image classification and object detection) and natural language processing (machine translation, sentiment classification and multimodal entailment) tasks with different state-of-the-art architectures, it is observed that Serf vastly outperforms ReLU (baseline) and other activation functions including both Swish and Mish, with a markedly bigger margin on deeper architectures. Ablation studies further demonstrate that Serf based architectures perform better than those of Swish and Mish in varying scenarios, validating the effectiveness and compatibility of Serf with varying depth, complexity, optimizers, learning rates, batch sizes, initializers and dropout rates. Finally, we investigate the mathematical relation between Swish and Serf, thereby showing the impact of preconditioner function ingrained in the first derivative of Serf which provides a regularization effect making gradients smoother and optimization faster.",0
"The role of activation functions is crucial in determining the performance and training dynamics of neural networks. While the widely used ReLU activation function is effective, it has some drawbacks, such as the Dying ReLU problem. To address these issues, we propose a new activation function called Serf, which is self-regularized and nonmonotonic, and belongs to the Swish family of functions like Mish. Our experiments show that Serf outperforms ReLU and other activation functions in various computer vision and natural language processing tasks, especially with deeper architectures. Ablation studies confirm that Serf is effective and compatible with different scenarios, including varying depth, complexity, optimizers, learning rates, batch sizes, initializers, and dropout rates. We also explore the mathematical relationship between Swish and Serf, highlighting the impact of the preconditioner function in the first derivative of Serf, which smoothes gradients and speeds up optimization.",1
"The anchor-based detectors handle the problem of scale variation by building the feature pyramid and directly setting different scales of anchors on each cell in different layers. However, it is difficult for box-wise anchors to guide the adaptive learning of scale-specific features in each layer because there is no one-to-one correspondence between box-wise anchors and pixel-level features. In order to alleviate the problem, in this paper, we propose a scale-customized weak segmentation (SCWS) block at the pixel level for scale customized object feature learning in each layer. By integrating the SCWS blocks into the single-shot detector, a scale-aware object detector (SCOD) is constructed to detect objects of different sizes naturally and accurately. Furthermore, the standard location loss neglects the fact that the hard and easy samples may be seriously imbalanced. A forthcoming problem is that it is unable to get more accurate bounding boxes due to the imbalance. To address this problem, an adaptive IoU (AIoU) loss via a simple yet effective squeeze operation is specified in our SCOD. Extensive experiments on PASCAL VOC and MS COCO demonstrate the superiority of our SCOD.",0
"The feature pyramid created by anchor-based detectors is used to handle scale variation, with different scales of anchors being set directly on each cell in different layers. However, the lack of a one-to-one correspondence between box-wise anchors and pixel-level features makes it difficult for them to guide the adaptive learning of scale-specific features in each layer. To address this issue, we propose the use of a scale-customized weak segmentation (SCWS) block at the pixel level for scale customized object feature learning in each layer. By integrating these blocks into the single-shot detector, we construct a scale-aware object detector (SCOD) that can detect objects of different sizes accurately. Moreover, the imbalance between hard and easy samples in the standard location loss can lead to inaccurate bounding boxes. To tackle this, we specify an adaptive IoU (AIoU) loss through a simple squeeze operation in our SCOD. Our experiments on PASCAL VOC and MS COCO demonstrate the superiority of our SCOD.",1
"Recent advances in 3D perception have shown impressive progress in understanding geometric structures of 3Dshapes and even scenes. Inspired by these advances in geometric understanding, we aim to imbue image-based perception with representations learned under geometric constraints. We introduce an approach to learn view-invariant,geometry-aware representations for network pre-training, based on multi-view RGB-D data, that can then be effectively transferred to downstream 2D tasks. We propose to employ contrastive learning under both multi-view im-age constraints and image-geometry constraints to encode3D priors into learned 2D representations. This results not only in improvement over 2D-only representation learning on the image-based tasks of semantic segmentation, instance segmentation, and object detection on real-world in-door datasets, but moreover, provides significant improvement in the low data regime. We show a significant improvement of 6.0% on semantic segmentation on full data as well as 11.9% on 20% data against baselines on ScanNet.",0
"In recent times, there has been remarkable progress in comprehending the geometric structures of 3D shapes and scenes through 3D perception advancements. This development has inspired us to equip image-based perception with geometrically constrained learned representations. Our approach involves learning geometry-aware and view-invariant representations for network pre-training using multi-view RGB-D data. These learned representations can be easily transferred to downstream 2D tasks. We suggest using contrastive learning under both multi-view image constraints and image-geometry constraints to encode 3D priors into learned 2D representations. This method not only enhances 2D-only representation learning for image-based tasks like semantic segmentation, instance segmentation, and object detection on indoor datasets but also significantly improves performance in low data scenarios. Our approach showed a substantial improvement of 6.0% on semantic segmentation on full data and 11.9% on 20% data compared to baselines on ScanNet.",1
"Surround View fisheye cameras are commonly deployed in automated driving for 360\deg{} near-field sensing around the vehicle. This work presents a multi-task visual perception network on unrectified fisheye images to enable the vehicle to sense its surrounding environment. It consists of six primary tasks necessary for an autonomous driving system: depth estimation, visual odometry, semantic segmentation, motion segmentation, object detection, and lens soiling detection. We demonstrate that the jointly trained model performs better than the respective single task versions. Our multi-task model has a shared encoder providing a significant computational advantage and has synergized decoders where tasks support each other. We propose a novel camera geometry based adaptation mechanism to encode the fisheye distortion model both at training and inference. This was crucial to enable training on the WoodScape dataset, comprised of data from different parts of the world collected by 12 different cameras mounted on three different cars with different intrinsics and viewpoints. Given that bounding boxes is not a good representation for distorted fisheye images, we also extend object detection to use a polygon with non-uniformly sampled vertices. We additionally evaluate our model on standard automotive datasets, namely KITTI and Cityscapes. We obtain the state-of-the-art results on KITTI for depth estimation and pose estimation tasks and competitive performance on the other tasks. We perform extensive ablation studies on various architecture choices and task weighting methodologies. A short video at https://youtu.be/xbSjZ5OfPes provides qualitative results.",0
"Automated driving employs Surround View fisheye cameras to detect the surrounding environment of the vehicle. This study introduces a multi-task visual perception network that utilizes unrectified fisheye images to perform six essential tasks for an autonomous driving system: depth estimation, visual odometry, semantic segmentation, motion segmentation, object detection, and lens soiling detection. The jointly trained model outperforms the separate single task versions, featuring a shared encoder that provides a significant computational advantage and synergized decoders where tasks complement each other. Moreover, the study proposes a novel camera geometry-based adaptation mechanism that encodes the fisheye distortion model during training and inference to enable training on the WoodScape dataset, which comprises data from various locations worldwide and collected by different cameras mounted on three different cars with different intrinsics and viewpoints. Since bounding boxes are not suitable for distorted fisheye images, the study extends object detection to utilize a polygon with non-uniformly sampled vertices. The model is evaluated on KITTI and Cityscapes, two standard automotive datasets, and achieves state-of-the-art results on KITTI for depth estimation and pose estimation tasks and competitive performance on other tasks. The study performs extensive ablation studies on various architecture choices and task weighting methodologies, and a short video showcasing qualitative results is available at https://youtu.be/xbSjZ5OfPes.",1
"Counting microbial colonies is a fundamental task in microbiology and has many applications in numerous industry branches. Despite this, current studies towards automatic microbial counting using artificial intelligence are hardly comparable due to the lack of unified methodology and the availability of large datasets. The recently introduced AGAR dataset is the answer to the second need, but the research carried out is still not exhaustive. To tackle this problem, we compared the performance of three well-known deep learning approaches for object detection on the AGAR dataset, namely two-stage, one-stage and transformer based neural networks. The achieved results may serve as a benchmark for future experiments.",0
"Microbial colony counting is a crucial aspect of microbiology with extensive applications across various industries. However, automated microbial counting using artificial intelligence poses a challenge due to the absence of standardized methodology and inadequate large datasets. The AGAR dataset has recently been introduced to address the latter issue, but further research is needed. To overcome this hurdle, we evaluated the efficacy of three popular deep learning techniques for object detection - two-stage, one-stage, and transformer-based neural networks - on the AGAR dataset. Our findings can serve as a reference point for future investigations.",1
"Understanding the scene around the ego-vehicle is key to assisted and autonomous driving. Nowadays, this is mostly conducted using cameras and laser scanners, despite their reduced performances in adverse weather conditions. Automotive radars are low-cost active sensors that measure properties of surrounding objects, including their relative speed, and have the key advantage of not being impacted by rain, snow or fog. However, they are seldom used for scene understanding due to the size and complexity of radar raw data and the lack of annotated datasets. Fortunately, recent open-sourced datasets have opened up research on classification, object detection and semantic segmentation with raw radar signals using end-to-end trainable models. In this work, we propose several novel architectures, and their associated losses, which analyse multiple ""views"" of the range-angle-Doppler radar tensor to segment it semantically. Experiments conducted on the recent CARRADA dataset demonstrate that our best model outperforms alternative models, derived either from the semantic segmentation of natural images or from radar scene understanding, while requiring significantly fewer parameters. Both our code and trained models are available at https://github.com/valeoai/MVRSS.",0
"The key to assisted and autonomous driving is to comprehend the environment surrounding the ego-vehicle. Typically, cameras and laser scanners are used for this purpose, but their performance is compromised in inclement weather. Automotive radars are a cost-effective option that measure the characteristics of nearby objects, such as their relative speed. Unlike cameras and laser scanners, they are not affected by precipitation or fog. Nonetheless, they are not widely employed for scene comprehension due to the size and complexity of the raw data, as well as the absence of annotated datasets. However, recent publicly available datasets have enabled research on classification, object detection, and semantic segmentation using end-to-end trainable models with raw radar signals. In this study, we propose several unique architectures and their corresponding losses that semantically segment the range-angle-Doppler radar tensor by analyzing multiple ""views."" We conducted experiments on the current CARRADA dataset, and our best model surpasses other models derived from natural image semantic segmentation or radar scene comprehension, while using significantly fewer parameters. Our code and trained models are both accessible at https://github.com/valeoai/MVRSS.",1
"This paper presents a new self-supervised system for learning to detect novel and previously unseen categories of objects in images. The proposed system receives as input several unlabeled videos of scenes containing various objects. The frames of the videos are segmented into objects using depth information, and the segments are tracked along each video. The system then constructs a weighted graph that connects sequences based on the similarities between the objects that they contain. The similarity between two sequences of objects is measured by using generic visual features, after automatically re-arranging the frames in the two sequences to align the viewpoints of the objects. The graph is used to sample triplets of similar and dissimilar examples by performing random walks. The triplet examples are finally used to train a siamese neural network that projects the generic visual features into a low-dimensional manifold. Experiments on three public datasets, YCB-Video, CORe50 and RGBD-Object, show that the projected low-dimensional features improve the accuracy of clustering unknown objects into novel categories, and outperform several recent unsupervised clustering techniques.",0
"A new system for self-supervised learning is presented in this paper, which aims to detect previously unseen categories of objects in images. The system takes in multiple unlabeled videos that contain various objects, segments the frames using depth information, and tracks the segments along each video. By using generic visual features, the system constructs a weighted graph that connects sequences based on the similarities between the objects they contain. The graph is then used to sample triplets of similar and dissimilar examples by performing random walks. Finally, a siamese neural network is trained using the triplet examples to project the generic visual features into a low-dimensional manifold. The experiments conducted on three public datasets, YCB-Video, CORe50, and RGBD-Object, demonstrate that the projected low-dimensional features enhance the accuracy of clustering unknown objects into novel categories and surpass several recent unsupervised clustering techniques.",1
"Small objects have relatively low resolution, the unobvious visual features which are difficult to be extracted, so the existing object detection methods cannot effectively detect small objects, and the detection speed and stability are poor. Thus, this paper proposes a small object detection algorithm based on FSSD, meanwhile, in order to reduce the computational cost and storage space, pruning is carried out to achieve model compression. Firstly, the semantic information contained in the features of different layers can be used to detect different scale objects, and the feature fusion method is improved to obtain more information beneficial to small objects; secondly, batch normalization layer is introduced to accelerate the training of neural network and make the model sparse; finally, the model is pruned by scaling factor to get the corresponding compressed model. The experimental results show that the average accuracy (mAP) of the algorithm can reach 80.4% on PASCAL VOC and the speed is 59.5 FPS on GTX1080ti. After pruning, the compressed model can reach 79.9% mAP, and 79.5 FPS in detection speed. On MS COCO, the best detection accuracy (APs) is 12.1%, and the overall detection accuracy is 49.8% AP when IoU is 0.5. The algorithm can not only improve the detection accuracy of small objects, but also greatly improves the detection speed, which reaches a balance between speed and accuracy.",0
"Due to their small size, small objects are difficult to detect using existing object detection methods as they have low resolution and unobvious visual features. This results in poor detection speed and stability. To address this issue, a small object detection algorithm based on FSSD is proposed in this paper. To reduce computational cost and storage space, pruning is carried out for model compression. The algorithm uses semantic information from different layers to detect objects of different scales, improves feature fusion to obtain more information beneficial to small objects, introduces batch normalization to accelerate neural network training, and prunes the model by scaling factor to get the compressed model. Experimental results show that the algorithm achieves an average accuracy (mAP) of 80.4% on PASCAL VOC and a speed of 59.5 FPS on GTX1080ti. After pruning, the compressed model achieves 79.9% mAP and 79.5 FPS in detection speed. On MS COCO, the algorithm achieves a detection accuracy of 12.1% for APs and an overall detection accuracy of 49.8% for IoU 0.5. The algorithm improves detection accuracy of small objects and detection speed, achieving a balance between speed and accuracy.",1
"3D multi-object tracking in LiDAR point clouds is a key ingredient for self-driving vehicles. Existing methods are predominantly based on the tracking-by-detection pipeline and inevitably require a heuristic matching step for the detection association. In this paper, we present SimTrack to simplify the hand-crafted tracking paradigm by proposing an end-to-end trainable model for joint detection and tracking from raw point clouds. Our key design is to predict the first-appear location of each object in a given snippet to get the tracking identity and then update the location based on motion estimation. In the inference, the heuristic matching step can be completely waived by a simple read-off operation. SimTrack integrates the tracked object association, newborn object detection, and dead track killing in a single unified model. We conduct extensive evaluations on two large-scale datasets: nuScenes and Waymo Open Dataset. Experimental results reveal that our simple approach compares favorably with the state-of-the-art methods while ruling out the heuristic matching rules.",0
"The ability to perform 3D multi-object tracking in LiDAR point clouds is crucial for autonomous vehicles. The majority of existing methods rely on tracking-by-detection and necessitate heuristic matching for detection association. Our study introduces SimTrack, which addresses this issue by proposing an end-to-end trainable model that combines detection and tracking from raw point clouds. Our method predicts an object's initial location in a given snippet to establish tracking identity and updates it based on motion estimation. The heuristic matching step is eliminated during inference with a simple read-off operation. SimTrack integrates tracked object association, newborn object detection, and dead track killing into a single unified model. We evaluated SimTrack on two large-scale datasets, nuScenes and Waymo Open Dataset, and found that our approach outperforms the state-of-the-art methods while eliminating the need for heuristic matching rules.",1
"Localizing objects and estimating their extent in 3D is an important step towards high-level 3D scene understanding, which has many applications in Augmented Reality and Robotics. We present ODAM, a system for 3D Object Detection, Association, and Mapping using posed RGB videos. The proposed system relies on a deep learning front-end to detect 3D objects from a given RGB frame and associate them to a global object-based map using a graph neural network (GNN). Based on these frame-to-model associations, our back-end optimizes object bounding volumes, represented as super-quadrics, under multi-view geometry constraints and the object scale prior. We validate the proposed system on ScanNet where we show a significant improvement over existing RGB-only methods.",0
"Detecting and estimating the size of objects in a 3D space is a crucial step towards comprehending high-level 3D scenes, with various applications in Robotics and Augmented Reality. Our system, ODAM, utilizes posed RGB videos and presents a method for 3D Object Detection, Association, and Mapping. The system incorporates a deep learning front-end to detect 3D objects in an RGB frame and uses a graph neural network (GNN) to associate them with a global object-based map. Our back-end optimizes object bounding volumes, represented as super-quadrics, based on frame-to-model associations, taking into account multi-view geometry constraints and the object scale prior. The proposed system has been validated on ScanNet, and our results demonstrate a significant improvement over existing RGB-only methods.",1
"Existing state-of-the-art saliency detection methods heavily rely on CNN-based architectures. Alternatively, we rethink this task from a convolution-free sequence-to-sequence perspective and predict saliency by modeling long-range dependencies, which can not be achieved by convolution. Specifically, we develop a novel unified model based on a pure transformer, namely, Visual Saliency Transformer (VST), for both RGB and RGB-D salient object detection (SOD). It takes image patches as inputs and leverages the transformer to propagate global contexts among image patches. Unlike conventional architectures used in Vision Transformer (ViT), we leverage multi-level token fusion and propose a new token upsampling method under the transformer framework to get high-resolution detection results. We also develop a token-based multi-task decoder to simultaneously perform saliency and boundary detection by introducing task-related tokens and a novel patch-task-attention mechanism. Experimental results show that our model outperforms existing methods on both RGB and RGB-D SOD benchmark datasets. Most importantly, our whole framework not only provides a new perspective for the SOD field but also shows a new paradigm for transformer-based dense prediction models. Code is available at https://github.com/nnizhang/VST.",0
"Currently, the latest saliency detection techniques rely heavily on CNN-based architectures. However, we propose an alternative approach by approaching this task from a convolution-free sequence-to-sequence perspective. Our method models long-range dependencies to predict saliency, which cannot be achieved with convolution. To achieve this, we introduce a new unified model called Visual Saliency Transformer (VST), which is based on a pure transformer and can be used for both RGB and RGB-D salient object detection (SOD). The VST takes image patches as inputs and uses the transformer to propagate global contexts among them. Unlike conventional architectures used in Vision Transformer (ViT), our approach leverages multi-level token fusion and introduces a new token upsampling method under the transformer framework to achieve high-resolution detection results. Additionally, we develop a token-based multi-task decoder that uses task-related tokens and a novel patch-task-attention mechanism to perform saliency and boundary detection simultaneously. Our experiments show that our model achieves better results than existing methods on both RGB and RGB-D SOD benchmark datasets. This approach not only provides a new perspective for the SOD field but also presents a new paradigm for transformer-based dense prediction models. If interested, the code is available at https://github.com/nnizhang/VST.",1
"Building reliable object detectors that are robust to domain shifts, such as various changes in context, viewpoint, and object appearances, is critical for real-world applications. In this work, we study the effectiveness of auxiliary self-supervised tasks to improve the out-of-distribution generalization of object detectors. Inspired by the principle of maximum entropy, we introduce a novel self-supervised task, instance-level temporal cycle confusion (CycConf), which operates on the region features of the object detectors. For each object, the task is to find the most different object proposals in the adjacent frame in a video and then cycle back to itself for self-supervision. CycConf encourages the object detector to explore invariant structures across instances under various motions, which leads to improved model robustness in unseen domains at test time. We observe consistent out-of-domain performance improvements when training object detectors in tandem with self-supervised tasks on large-scale video datasets (BDD100K and Waymo open data). The joint training framework also establishes a new state-of-the-art on standard unsupervised domain adaptative detection benchmarks (Cityscapes, Foggy Cityscapes, and Sim10K). The code and models are available at https://github.com/xinw1012/cycle-confusion.",0
"Developing dependable object detectors that can withstand domain shifts, such as changes in context, viewpoint, and object appearance, is crucial for practical applications. Our study explores the efficacy of auxiliary self-supervised tasks in enhancing the out-of-distribution generalization of object detectors. Drawing inspiration from the maximum entropy principle, we introduce a new self-supervised task called instance-level temporal cycle confusion (CycConf), which operates on object detector region features. This task aims to identify the most distinct object proposals in the adjacent frame of a video for each object and then return to itself for self-supervision. CycConf fosters object detector exploration of invariant structures across instances during various movements, resulting in improved model robustness in unknown domains during testing. We consistently observed out-of-domain performance enhancements when training object detectors in combination with self-supervised tasks on large-scale video datasets (BDD100K and Waymo open data). The joint training framework also establishes a new standard for unsupervised domain adaptive detection benchmarks (Cityscapes, Foggy Cityscapes, and Sim10K). Our code and models are available at https://github.com/xinw1012/cycle-confusion.",1
"We present a new pipeline for holistic 3D scene understanding from a single image, which could predict object shapes, object poses, and scene layout. As it is a highly ill-posed problem, existing methods usually suffer from inaccurate estimation of both shapes and layout especially for the cluttered scene due to the heavy occlusion between objects. We propose to utilize the latest deep implicit representation to solve this challenge. We not only propose an image-based local structured implicit network to improve the object shape estimation, but also refine the 3D object pose and scene layout via a novel implicit scene graph neural network that exploits the implicit local object features. A novel physical violation loss is also proposed to avoid incorrect context between objects. Extensive experiments demonstrate that our method outperforms the state-of-the-art methods in terms of object shape, scene layout estimation, and 3D object detection.",0
"A new pipeline has been introduced for comprehensive 3D scene comprehension from a single image. This pipeline can estimate object shapes, object poses, and scene layout, all of which are challenging tasks due to the highly ambiguous nature of the problem. The current methods often fall short in accurately estimating shapes and layouts, particularly in cluttered scenes where objects are heavily occluded. To tackle this issue, the latest deep implicit representation is utilized. The proposed approach includes an image-based local structured implicit network to enhance object shape estimation, as well as a novel implicit scene graph neural network that utilizes the implicit local object features to refine 3D object pose and scene layout. Furthermore, a new physical violation loss is proposed to prevent incorrect context between objects. The extensive experiments conducted show that this method outperforms the state-of-the-art techniques in terms of object shape, scene layout estimation, and 3D object detection.",1
"Monocular 3D object detection is of great significance for autonomous driving but remains challenging. The core challenge is to predict the distance of objects in the absence of explicit depth information. Unlike regressing the distance as a single variable in most existing methods, we propose a novel geometry-based distance decomposition to recover the distance by its factors. The decomposition factors the distance of objects into the most representative and stable variables, i.e. the physical height and the projected visual height in the image plane. Moreover, the decomposition maintains the self-consistency between the two heights, leading to robust distance prediction when both predicted heights are inaccurate. The decomposition also enables us to trace the causes of the distance uncertainty for different scenarios. Such decomposition makes the distance prediction interpretable, accurate, and robust. Our method directly predicts 3D bounding boxes from RGB images with a compact architecture, making the training and inference simple and efficient. The experimental results show that our method achieves the state-of-the-art performance on the monocular 3D Object Detection and Birds Eye View tasks of the KITTI dataset, and can generalize to images with different camera intrinsics.",0
"Detecting 3D objects using only one camera is crucial for self-driving cars, but it is still a difficult task. The main difficulty lies in predicting the distance of objects without having explicit depth information. Although existing methods usually estimate distance as a single variable, we propose a new approach that decomposes distance into its factors based on geometry. This enables us to identify the most representative and stable variables, namely the physical and projected visual heights. By maintaining consistency between the two heights, our method can still predict distance accurately even when both heights are imprecise. Additionally, the decomposition allows us to understand the reasons for distance uncertainty in different scenarios, making our predictions more interpretable, accurate, and reliable. Our approach uses a compact architecture to directly predict 3D bounding boxes from RGB images, making both training and inference straightforward and efficient. Our experimental results demonstrate that our method outperforms existing approaches on the KITTI dataset's monocular 3D Object Detection and Birds Eye View tasks, and can be used with images captured under different camera intrinsics.",1
"Contour information plays a vital role in salient object detection. However, excessive false positives remain in predictions from existing contour-based models due to insufficient contour-saliency fusion. In this work, we designed a network for better edge quality in salient object detection. We proposed a contour-saliency blending module to exchange information between contour and saliency. We adopted recursive CNN to increase contour-saliency fusion while keeping the total trainable parameters the same. Furthermore, we designed a stage-wise feature extraction module to help the model pick up the most helpful features from previous intermediate saliency predictions. Besides, we proposed two new loss functions, namely Dual Confinement Loss and Confidence Loss, for our model to generate better boundary predictions. Evaluation results on five common benchmark datasets reveal that our model achieves competitive state-of-the-art performance.",0
"The detection of prominent objects heavily relies on contour information. However, the current contour-based models produce an excess of false positives due to insufficient fusion of contour-saliency. To address this issue, we developed a network that enhances the quality of edge detection in salient object detection. Our approach involves a contour-saliency blending module that enables the exchange of information between contour and saliency. We used recursive CNN to strengthen the contour-saliency fusion while maintaining the same total trainable parameters. Additionally, we designed a stage-wise feature extraction module to assist the model in selecting the most useful features from previous intermediate saliency predictions. Our model employs two new loss functions, Dual Confinement Loss and Confidence Loss, to produce better boundary predictions. Evaluation on five benchmark datasets shows that our model is competitive and achieves state-of-the-art performance.",1
"Many recent studies have shown that deep neural models are vulnerable to adversarial samples: images with imperceptible perturbations, for example, can fool image classifiers. In this paper, we present the first type-specific approach to generating adversarial examples for object detection, which entails detecting bounding boxes around multiple objects present in the image and classifying them at the same time, making it a harder task than against image classification. We specifically aim to attack the widely used Faster R-CNN by changing the predicted label for a particular object in an image: where prior work has targeted one specific object (a stop sign), we generalise to arbitrary objects, with the key challenge being the need to change the labels of all bounding boxes for all instances of that object type. To do so, we propose a novel method, named Pick-Object-Attack. Pick-Object-Attack successfully adds perturbations only to bounding boxes for the targeted object, preserving the labels of other detected objects in the image. In terms of perceptibility, the perturbations induced by the method are very small. Furthermore, for the first time, we examine the effect of adversarial attacks on object detection in terms of a downstream task, image captioning; we show that where a method that can modify all object types leads to very obvious changes in captions, the changes from our constrained attack are much less apparent.",0
"Recent studies have revealed that deep neural models are susceptible to adversarial samples, as even minor alterations to images can deceive image classifiers. This paper introduces a novel approach to generating adversarial examples for object detection, which involves identifying bounding boxes around multiple objects in an image and classifying them simultaneously. This is a more challenging task than image classification, and the target is the widely used Faster R-CNN. Unlike previous research that has focused on a single specific object (such as a stop sign), this study aims to generalize the attack to any object type. The key challenge is changing the labels of all bounding boxes for all instances of that object type. To achieve this, the authors propose a new method called Pick-Object-Attack. This approach effectively adds perturbations only to the bounding boxes for the targeted object while preserving the labels of other detected objects in the image. The perturbations induced by this method are hardly noticeable. In addition, the paper examines the impact of adversarial attacks on object detection in terms of a downstream task, image captioning. The results show that the changes from the Pick-Object-Attack are much less apparent than those from a method that can modify all object types.",1
"Transformer architectures have become the model of choice in natural language processing and are now being introduced into computer vision tasks such as image classification, object detection, and semantic segmentation. However, in the field of human pose estimation, convolutional architectures still remain dominant. In this work, we present PoseFormer, a purely transformer-based approach for 3D human pose estimation in videos without convolutional architectures involved. Inspired by recent developments in vision transformers, we design a spatial-temporal transformer structure to comprehensively model the human joint relations within each frame as well as the temporal correlations across frames, then output an accurate 3D human pose of the center frame. We quantitatively and qualitatively evaluate our method on two popular and standard benchmark datasets: Human3.6M and MPI-INF-3DHP. Extensive experiments show that PoseFormer achieves state-of-the-art performance on both datasets. Code is available at \url{https://github.com/zczcwh/PoseFormer}",0
"Although transformers have become the preferred model in natural language processing and have been introduced to computer vision tasks such as image classification, object detection, and semantic segmentation, convolutional architectures still dominate in human pose estimation. In this study, we introduce PoseFormer, a transformer-based approach for 3D human pose estimation in videos that does not rely on convolutional architectures. Our spatial-temporal transformer structure allows us to comprehensively model human joint relations within each frame and temporal correlations across frames, resulting in an accurate 3D human pose of the center frame. We evaluate our method on two standard benchmark datasets, Human3.6M and MPI-INF-3DHP, and demonstrate state-of-the-art performance. Code is available at \url{https://github.com/zczcwh/PoseFormer}.",1
"Deep learning-based methods for video pedestrian detection and tracking require large volumes of training data to achieve good performance. However, data acquisition in crowded public environments raises data privacy concerns -- we are not allowed to simply record and store data without the explicit consent of all participants. Furthermore, the annotation of such data for computer vision applications usually requires a substantial amount of manual effort, especially in the video domain. Labeling instances of pedestrians in highly crowded scenarios can be challenging even for human annotators and may introduce errors in the training data. In this paper, we study how we can advance different aspects of multi-person tracking using solely synthetic data. To this end, we generate MOTSynth, a large, highly diverse synthetic dataset for object detection and tracking using a rendering game engine. Our experiments show that MOTSynth can be used as a replacement for real data on tasks such as pedestrian detection, re-identification, segmentation, and tracking.",0
"To achieve satisfactory performance in video pedestrian detection and tracking using deep learning-based methods, a significant amount of training data is required. However, data acquisition in public areas with high foot traffic raises concerns related to data privacy as recording and storing data without explicit consent is not permitted. Additionally, annotating such data for computer vision applications can be a laborious task, particularly in the video domain, and labeling instances of pedestrians in crowded scenarios can be challenging, even for human annotators, leading to errors in the training data. In this study, we examine how we can enhance various aspects of multi-person tracking by employing solely synthetic data. Thus, we have developed MOTSynth, an extensive and highly diverse synthetic dataset for object detection and tracking using a rendering game engine. Our findings indicate that MOTSynth can be used instead of real data for tasks such as pedestrian detection, segmentation, re-identification, and tracking.",1
"Deep-learning based salient object detection methods achieve great improvements. However, there are still problems existing in the predictions, such as blurry boundary and inaccurate location, which is mainly caused by inadequate feature extraction and integration. In this paper, we propose a Multi-scale Edge-based U-shape Network (MEUN) to integrate various features at different scales to achieve better performance. To extract more useful information for boundary prediction, U-shape Edge Network modules are embedded in each decoder units. Besides, the additional down-sampling module alleviates the location inaccuracy. Experimental results on four benchmark datasets demonstrate the validity and reliability of the proposed method. Multi-scale Edge based U-shape Network also shows its superiority when compared with 15 state-of-the-art salient object detection methods.",0
"Significant advancements have been made in deep-learning based salient object detection techniques. Despite these advancements, issues such as imprecise location and blurred boundary in the predictions still persist. These issues are mainly attributed to insufficient feature extraction and integration. To address this problem, this study introduces a Multi-scale Edge-based U-shape Network (MEUN) that integrates diverse features at various scales to enhance performance. Additionally, U-shape Edge Network modules are integrated into each decoder unit to obtain more valuable data for boundary prediction. Furthermore, the inclusion of an extra down-sampling module helps reduce location inaccuracy. The efficacy and dependability of the proposed method are supported by experimental results on four benchmark datasets. Furthermore, when compared to 15 other prominent salient object detection methods, the Multi-scale Edge-based U-shape Network proves its superiority.",1
"In order to withstand the ever-increasing invasion of privacy by CCTV cameras and technologies, on par CCTV-aware solutions must exist that provide privacy, safety, and cybersecurity features. We argue that a first important step towards such CCTV-aware solutions must be a mapping system (e.g., Google Maps, OpenStreetMap) that provides both privacy and safety routing and navigation options. However, this in turn requires that the mapping system contains updated information on CCTV cameras' exact geo-location, coverage area, and possibly other meta-data (e.g., resolution, facial recognition features, operator). Such information is however missing from current mapping systems, and there are several ways to fix this. One solution is to perform CCTV camera detection on geo-location tagged images, e.g., street view imagery on various platforms, user images publicly posted in image sharing platforms such as Flickr. Unfortunately, to the best of our knowledge, there are no computer vision models for CCTV camera object detection as well as no mapping system that supports privacy and safety routing options.   To close these gaps, with this paper we introduce CCTVCV -- the first and only computer vision MS COCO-compatible models that are able to accurately detect CCTV and video surveillance cameras in images and video frames. To this end, our best detectors were built using 8387 images that were manually reviewed and annotated to contain 10419 CCTV camera instances, and achieve an accuracy of up to 98.7%. Moreover, we build and evaluate multiple models, present a comprehensive comparison of their performance, and outline core challenges associated with such research.",0
"To combat the growing invasion of privacy by CCTV cameras, there must be solutions that prioritize privacy, safety, and cybersecurity. A crucial first step in achieving this is the development of a mapping system that offers routing and navigation options with these features in mind. However, this requires updated information on the exact location, coverage area, and other metadata of CCTV cameras, which is currently lacking in mapping systems. One possible solution is to detect CCTV cameras through geo-tagged images on platforms such as Flickr or street view imagery. Unfortunately, there are currently no computer vision models or mapping systems that support privacy and safety routing options. In this paper, we introduce CCTVCV - the first computer vision models that can accurately detect CCTV and video surveillance cameras with up to 98.7% accuracy. We present multiple models, evaluate their performance, and highlight the challenges associated with this research.",1
"Active learning aims to reduce labeling costs by selecting only the most informative samples on a dataset. Few existing works have addressed active learning for object detection. Most of these methods are based on multiple models or are straightforward extensions of classification methods, hence estimate an image's informativeness using only the classification head. In this paper, we propose a novel deep active learning approach for object detection. Our approach relies on mixture density networks that estimate a probabilistic distribution for each localization and classification head's output. We explicitly estimate the aleatoric and epistemic uncertainty in a single forward pass of a single model. Our method uses a scoring function that aggregates these two types of uncertainties for both heads to obtain every image's informativeness score. We demonstrate the efficacy of our approach in PASCAL VOC and MS-COCO datasets. Our approach outperforms single-model based methods and performs on par with multi-model based methods at a fraction of the computing cost.",0
"The objective of active learning is to decrease the expenses associated with labeling by selecting only the most informative samples from a dataset. While a few studies have examined active learning for object detection, most of them depend on multiple models or are simple extensions of classification methods, which estimate an image's informativeness based solely on the classification head. This paper introduces a new deep active learning technique for object detection that uses mixture density networks to estimate a probabilistic distribution for each output of the localization and classification head. In a single forward pass of a single model, we explicitly estimate the aleatoric and epistemic uncertainty. Our method employs a scoring function that combines these two types of uncertainties for both heads to determine the informativeness score for each image. We demonstrate the effectiveness of our method on PASCAL VOC and MS-COCO datasets, revealing that it outperforms single-model based approaches and performs similarly to multi-model based approaches at a fraction of the computational cost.",1
"In this paper, we investigate the knowledge distillation (KD) strategy for object detection and propose an effective framework applicable to both homogeneous and heterogeneous student-teacher pairs. The conventional feature imitation paradigm introduces imitation masks to focus on informative foreground areas while excluding the background noises. However, we find that those methods fail to fully utilize the semantic information in all feature pyramid levels, which leads to inefficiency for knowledge distillation between FPN-based detectors. To this end, we propose a novel semantic-guided feature imitation technique, which automatically performs soft matching between feature pairs across all pyramid levels to provide the optimal guidance to the student. To push the envelop even further, we introduce contrastive distillation to effectively capture the information encoded in the relationship between different feature regions. Finally, we propose a generalized detection KD pipeline, which is capable of distilling both homogeneous and heterogeneous detector pairs. Our method consistently outperforms the existing detection KD techniques, and works when (1) components in the framework are used separately and in conjunction; (2) for both homogeneous and heterogenous student-teacher pairs and (3) on multiple detection benchmarks. With a powerful X101-FasterRCNN-Instaboost detector as the teacher, R50-FasterRCNN reaches 44.0% AP, R50-RetinaNet reaches 43.3% AP and R50-FCOS reaches 43.1% AP on COCO dataset.",0
"This paper explores the effectiveness of the knowledge distillation (KD) strategy in object detection and presents a framework that can be applied to both homogeneous and heterogeneous student-teacher pairs. The traditional feature imitation approach uses imitation masks to concentrate on the foreground areas while disregarding the background noise. However, this method is inadequate in fully utilizing the semantic information across all feature pyramid levels, leading to inefficiencies in knowledge distillation between FPN-based detectors. To address this, we introduce a novel semantic-guided feature imitation technique that performs soft matching between feature pairs across all pyramid levels to provide optimal guidance to the student. We also introduce contrastive distillation to effectively capture the information encoded in the relationship between different feature regions. Finally, we propose a generalized detection KD pipeline that works for both homogeneous and heterogeneous detector pairs. Our method outperforms existing detection KD techniques and is effective when used separately and in conjunction, on multiple detection benchmarks. With a powerful X101-FasterRCNN-Instaboost detector as the teacher, R50-FasterRCNN reaches 44.0% AP, R50-RetinaNet reaches 43.3% AP, and R50-FCOS reaches 43.1% AP on the COCO dataset.",1
"Computer vision has flourished in recent years thanks to Deep Learning advancements, fast and scalable hardware solutions and large availability of structured image data. Convolutional Neural Networks trained on supervised tasks with backpropagation learn to extract meaningful representations from raw pixels automatically, and surpass shallow methods in image understanding. Though convenient, data-driven feature learning is prone to dataset bias: a network learns its parameters from training signals alone, and will usually perform poorly if train and test distribution differ. To alleviate this problem, research on Domain Generalization (DG), Domain Adaptation (DA) and their variations is increasing. This thesis contributes to these research topics by presenting novel and effective ways to solve the dataset bias problem in its various settings. We propose new frameworks for Domain Generalization and Domain Adaptation which make use of feature aggregation strategies and visual transformations via data-augmentation and multi-task integration of self-supervision. We also design an algorithm that adapts an object detection model to any out of distribution sample at test time. With through experimentation, we show how our proposed solutions outperform competitive state-of-the-art approaches in established DG and DA benchmarks.",0
"Recent advancements in Deep Learning, coupled with the availability of structured image data and scalable hardware solutions, have led to significant progress in Computer Vision. Convolutional Neural Networks, trained on supervised tasks using backpropagation, have shown superior performance in image understanding compared to shallow methods. However, data-driven feature learning has a potential drawback - dataset bias. This happens when a network learns its parameters solely from training signals, leading to poor performance when the train and test distributions differ. To address this issue, researchers are increasingly exploring Domain Generalization (DG) and Domain Adaptation (DA) techniques. This thesis contributes to these efforts by proposing novel frameworks for DG and DA, using feature aggregation, visual transformations, data augmentation, and multi-task integration of self-supervision. We also introduce an algorithm that adapts object detection models to out-of-distribution samples during test time. Our experiments demonstrate that our proposed solutions outperform existing state-of-the-art approaches in established DG and DA benchmarks.",1
"Active learning for object detection is conventionally achieved by applying techniques developed for classification in a way that aggregates individual detections into image-level selection criteria. This is typically coupled with the costly assumption that every image selected for labelling must be exhaustively annotated. This yields incremental improvements on well-curated vision datasets and struggles in the presence of data imbalance and visual clutter that occurs in real-world imagery. Alternatives to the image-level approach are surprisingly under-explored in the literature. In this work, we introduce a new strategy that subsumes previous Image-level and Object-level approaches into a generalized, Region-level approach that promotes spatial-diversity by avoiding nearby redundant queries from the same image and minimizes context-switching for the labeler. We show that this approach significantly decreases labeling effort and improves rare object search on realistic data with inherent class-imbalance and cluttered scenes.",0
"Traditionally, active learning for object detection involves using classification techniques to combine individual detections and create image-level selection criteria. However, this method is expensive as it assumes that each selected image must be fully annotated. While this approach yields some improvements on high-quality vision datasets, it struggles with data imbalance and visual clutter present in real-world images. Despite this, other options besides the image-level approach have not been extensively explored in literature. This study introduces a new approach that combines both Image-level and Object-level methods into a generalized Region-level approach that prioritizes spatial diversity, reducing redundant queries from nearby images and minimizing context-switching for the labeler. The study shows that this approach significantly reduces labeling effort and improves rare object detection in realistic data with class-imbalance and cluttered scenes.",1
"In this paper, we introduce the task of multi-view RGB-based 3D object detection as an end-to-end optimization problem. To address this problem, we propose ImVoxelNet, a novel fully convolutional method of 3D object detection based on monocular or multi-view RGB images. The number of monocular images in each multi-view input can variate during training and inference; actually, this number might be unique for each multi-view input. ImVoxelNet successfully handles both indoor and outdoor scenes, which makes it general-purpose. Specifically, it achieves state-of-the-art results in car detection on KITTI (monocular) and nuScenes (multi-view) benchmarks among all methods that accept RGB images. Moreover, it surpasses existing RGB-based 3D object detection methods on the SUN RGB-D dataset. On ScanNet, ImVoxelNet sets a new benchmark for multi-view 3D object detection. The source code and the trained models are available at https://github.com/saic-vul/imvoxelnet.",0
"The aim of this article is to present a new approach to multi-view RGB-based 3D object detection through an end-to-end optimization process. Our method, ImVoxelNet, utilizes a fully convolutional approach to detect 3D objects based on mono or multi-view RGB images, with the number of mono images varying in each multi-view input during training and inference. ImVoxelNet is versatile enough to handle both indoor and outdoor scenes and has achieved impressive results in car detection on KITTI (monocular) and nuScenes (multi-view) benchmarks. Our system also outperforms existing RGB-based 3D object detection methods on the SUN RGB-D dataset and sets a new benchmark for multi-view 3D object detection on ScanNet. The source code and trained models for ImVoxelNet are available at https://github.com/saic-vul/imvoxelnet.",1
"Few-shot object detection, which aims at detecting novel objects rapidly from extremely few annotated examples of previously unseen classes, has attracted significant research interest in the community. Most existing approaches employ the Faster R-CNN as basic detection framework, yet, due to the lack of tailored considerations for data-scarce scenario, their performance is often not satisfactory. In this paper, we look closely into the conventional Faster R-CNN and analyze its contradictions from two orthogonal perspectives, namely multi-stage (RPN vs. RCNN) and multi-task (classification vs. localization). To resolve these issues, we propose a simple yet effective architecture, named Decoupled Faster R-CNN (DeFRCN). To be concrete, we extend Faster R-CNN by introducing Gradient Decoupled Layer for multi-stage decoupling and Prototypical Calibration Block for multi-task decoupling. The former is a novel deep layer with redefining the feature-forward operation and gradient-backward operation for decoupling its subsequent layer and preceding layer, and the latter is an offline prototype-based classification model with taking the proposals from detector as input and boosting the original classification scores with additional pairwise scores for calibration. Extensive experiments on multiple benchmarks show our framework is remarkably superior to other existing approaches and establishes a new state-of-the-art in few-shot literature.",0
"The community has shown a significant interest in few-shot object detection, which involves detecting new objects with minimal annotated examples of previously unseen classes. However, current approaches using Faster R-CNN as the basic detection framework do not perform well in data-scarce scenarios. In this paper, we examine the conventional Faster R-CNN from two perspectives and propose a new architecture called Decoupled Faster R-CNN (DeFRCN) to address these issues. We introduce a Gradient Decoupled Layer for multi-stage decoupling and a Prototypical Calibration Block for multi-task decoupling. The former is a deep layer that redefines feature-forward and gradient-backward operations to decouple subsequent and preceding layers. The latter is an offline prototype-based classification model that boosts original classification scores with additional pairwise scores for calibration. Our experiments show that our framework outperforms existing approaches and sets a new state-of-the-art in few-shot literature.",1
"Existing point-cloud based 3D object detectors use convolution-like operators to process information in a local neighbourhood with fixed-weight kernels and aggregate global context hierarchically. However, non-local neural networks and self-attention for 2D vision have shown that explicitly modeling long-range interactions can lead to more robust and competitive models. In this paper, we propose two variants of self-attention for contextual modeling in 3D object detection by augmenting convolutional features with self-attention features. We first incorporate the pairwise self-attention mechanism into the current state-of-the-art BEV, voxel and point-based detectors and show consistent improvement over strong baseline models of up to 1.5 3D AP while simultaneously reducing their parameter footprint and computational cost by 15-80% and 30-50%, respectively, on the KITTI validation set. We next propose a self-attention variant that samples a subset of the most representative features by learning deformations over randomly sampled locations. This not only allows us to scale explicit global contextual modeling to larger point-clouds, but also leads to more discriminative and informative feature descriptors. Our method can be flexibly applied to most state-of-the-art detectors with increased accuracy and parameter and compute efficiency. We show our proposed method improves 3D object detection performance on KITTI, nuScenes and Waymo Open datasets. Code is available at https://github.com/AutoVision-cloud/SA-Det3D.",0
"Current 3D object detectors based on point clouds use convolution-like operators to process information locally with fixed-weight kernels and hierarchically aggregate global context. However, recent research suggests that explicitly modeling long-range interactions through non-local neural networks and self-attention can result in more robust and competitive models for 2D vision. This paper proposes two variations of self-attention for contextual modeling in 3D object detection by enhancing convolutional features with self-attention features. The first approach incorporates pairwise self-attention into BEV, voxel, and point-based detectors, resulting in consistent improvement over strong baseline models of up to 1.5 3D AP, while reducing parameter footprint and computational cost by 15-80% and 30-50%, respectively, on the KITTI validation set. The second approach proposes a self-attention variant that samples a subset of the most representative features by learning deformations over randomly sampled locations, enabling explicit global contextual modeling for larger point clouds and leading to more informative and discriminative feature descriptors. The proposed method can be applied flexibly to most state-of-the-art detectors, improving accuracy and parameter and compute efficiency. The paper demonstrates improved 3D object detection performance on KITTI, nuScenes, and Waymo Open datasets. The code is available at https://github.com/AutoVision-cloud/SA-Det3D.",1
"We propose a semi-automatic bounding box annotation method for visual object tracking by utilizing temporal information with a tracking-by-detection approach. For detection, we use an off-the-shelf object detector which is trained iteratively with the annotations generated by the proposed method, and we perform object detection on each frame independently. We employ Multiple Hypothesis Tracking (MHT) to exploit temporal information and to reduce the number of false-positives which makes it possible to use lower objectness thresholds for detection to increase recall. The tracklets formed by MHT are evaluated by human operators to enlarge the training set. This novel incremental learning approach helps to perform annotation iteratively. The experiments performed on AUTH Multidrone Dataset reveal that the annotation workload can be reduced up to 96% by the proposed approach.",0
"By incorporating temporal information into a tracking-by-detection approach, we suggest a semi-automatic bounding box annotation method for visual object tracking. Our proposed method involves using an off-the-shelf object detector for detection, which is iteratively trained with annotations generated by our method. Object detection is carried out independently on each frame. To minimize false-positives and increase recall, we utilize Multiple Hypothesis Tracking (MHT). MHT forms tracklets that are evaluated by human operators to expand the training set. This incremental learning approach enables iterative annotation. Our experiments on AUTH Multidrone Dataset indicate that our proposed approach can reduce annotation workload by up to 96%.",1
"Videos captured using Transmission Electron Microscopy (TEM) can encode details regarding the morphological and temporal evolution of a material by taking snapshots of the microstructure sequentially. However, manual analysis of such video is tedious, error-prone, unreliable, and prohibitively time-consuming if one wishes to analyze a significant fraction of frames for even videos of modest length. In this work, we developed an automated TEM video analysis system for microstructural features based on the advanced object detection model called YOLO and tested the system on an in-situ ion irradiation TEM video of dislocation loops formed in a FeCrAl alloy. The system provides analysis of features observed in TEM including both static and dynamic properties using the YOLO-based defect detection module coupled to a geometry analysis module and a dynamic tracking module. Results show that the system can achieve human comparable performance with an F1 score of 0.89 for fast, consistent, and scalable frame-level defect analysis. This result is obtained on a real but exceptionally clean and stable data set and more challenging data sets may not achieve this performance. The dynamic tracking also enabled evaluation of individual defect evolution like per defect growth rate at a fidelity never before achieved using common human analysis methods. Our work shows that automatically detecting and tracking interesting microstructures and properties contained in TEM videos is viable and opens new doors for evaluating materials dynamics.",0
"Transmission Electron Microscopy (TEM) videos can capture the morphological and temporal evolution of materials by taking sequential snapshots of their microstructure. However, manual analysis of such videos is time-consuming, unreliable, and prone to errors, especially when analyzing a significant fraction of frames. To address this issue, we developed an automated TEM video analysis system that utilizes the advanced object detection model YOLO. We tested the system on an in-situ ion irradiation TEM video of dislocation loops formed in a FeCrAl alloy. The system provides analysis of both static and dynamic properties of TEM features using a defect detection module, a geometry analysis module, and a dynamic tracking module. Our results show that the system achieves human-comparable performance with an F1 score of 0.89 for fast and scalable frame-level defect analysis. This automated approach can detect and track interesting microstructures and properties contained in TEM videos, enabling new opportunities for evaluating materials dynamics. However, the system's performance may be affected when analyzing more challenging data sets.",1
"The recently-developed DETR approach applies the transformer encoder and decoder architecture to object detection and achieves promising performance. In this paper, we handle the critical issue, slow training convergence, and present a conditional cross-attention mechanism for fast DETR training. Our approach is motivated by that the cross-attention in DETR relies highly on the content embeddings for localizing the four extremities and predicting the box, which increases the need for high-quality content embeddings and thus the training difficulty. Our approach, named conditional DETR, learns a conditional spatial query from the decoder embedding for decoder multi-head cross-attention. The benefit is that through the conditional spatial query, each cross-attention head is able to attend to a band containing a distinct region, e.g., one object extremity or a region inside the object box. This narrows down the spatial range for localizing the distinct regions for object classification and box regression, thus relaxing the dependence on the content embeddings and easing the training. Empirical results show that conditional DETR converges 6.7x faster for the backbones R50 and R101 and 10x faster for stronger backbones DC5-R50 and DC5-R101. Code is available at https://github.com/Atten4Vis/ConditionalDETR.",0
"The transformer encoder and decoder architecture has been successfully applied to object detection with the recently-developed DETR approach. However, slow training convergence remains a critical issue. To address this, we propose a new approach called conditional DETR, which introduces a conditional cross-attention mechanism for faster training. The current cross-attention in DETR relies heavily on high-quality content embeddings for localizing object extremities and predicting the box, increasing the difficulty of training. With conditional DETR, we introduce a conditional spatial query from the decoder embedding for decoder multi-head cross-attention. This allows each cross-attention head to focus on a specific region, such as an object extremity or a region inside the object box, thereby narrowing down the spatial range for object classification and box regression. This reduces the dependence on content embeddings, making training easier. We demonstrate that conditional DETR converges 6.7x faster for backbones R50 and R101 and 10x faster for stronger backbones DC5-R50 and DC5-R101 through empirical results. The code is available at https://github.com/Atten4Vis/ConditionalDETR.",1
"Convolutional neural networks (CNNs) have developed to become powerful models for various computer vision tasks ranging from object detection to semantic segmentation. However, most of the state-of-the-art CNNs cannot be deployed directly on edge devices such as smartphones and drones, which need low latency under limited power and memory bandwidth. One popular, straightforward approach to compressing CNNs is network slimming, which imposes $\ell_1$ regularization on the channel-associated scaling factors via the batch normalization layers during training. Network slimming thereby identifies insignificant channels that can be pruned for inference. In this paper, we propose replacing the $\ell_1$ penalty with an alternative nonconvex, sparsity-inducing penalty in order to yield a more compressed and/or accurate CNN architecture. We investigate $\ell_p (0 < p < 1)$, transformed $\ell_1$ (T$\ell_1$), minimax concave penalty (MCP), and smoothly clipped absolute deviation (SCAD) due to their recent successes and popularity in solving sparse optimization problems, such as compressed sensing and variable selection. We demonstrate the effectiveness of network slimming with nonconvex penalties on three neural network architectures -- VGG-19, DenseNet-40, and ResNet-164 -- on standard image classification datasets. Based on the numerical experiments, T$\ell_1$ preserves model accuracy against channel pruning, $\ell_{1/2, 3/4}$ yield better compressed models with similar accuracies after retraining as $\ell_1$, and MCP and SCAD provide more accurate models after retraining with similar compression as $\ell_1$. Network slimming with T$\ell_1$ regularization also outperforms the latest Bayesian modification of network slimming in compressing a CNN architecture in terms of memory storage while preserving its model accuracy after channel pruning.",0
"From object detection to semantic segmentation, convolutional neural networks (CNNs) have become powerful models for various computer vision tasks. However, most of the state-of-the-art CNNs are not suitable for deployment on edge devices such as drones and smartphones that require low latency and limited power and memory bandwidth. One popular approach to compressing CNNs is network slimming, which utilizes $\ell_1$ regularization on channel-associated scaling factors via batch normalization layers during training to identify insignificant channels that can be pruned for inference. In this paper, we propose replacing the $\ell_1$ penalty with an alternative nonconvex, sparsity-inducing penalty to achieve a more accurate and/or compressed CNN architecture. We explore $\ell_p (0 < p < 1)$, transformed $\ell_1$ (T$\ell_1$), minimax concave penalty (MCP), and smoothly clipped absolute deviation (SCAD) as they have shown recent success in solving sparse optimization problems such as variable selection and compressed sensing. We demonstrate the effectiveness of network slimming with nonconvex penalties on three neural network architectures, namely VGG-19, DenseNet-40, and ResNet-164, using standard image classification datasets. Our numerical experiments show that T$\ell_1$ preserves model accuracy after channel pruning, while $\ell_{1/2, 3/4}$ yield better compressed models with similar accuracies after retraining as $\ell_1$. MCP and SCAD provide more accurate models after retraining with similar compression as $\ell_1$. Furthermore, network slimming with T$\ell_1$ regularization outperforms the latest Bayesian modification of network slimming in terms of memory storage while preserving model accuracy after channel pruning.",1
"We present Region Similarity Representation Learning (ReSim), a new approach to self-supervised representation learning for localization-based tasks such as object detection and segmentation. While existing work has largely focused on solely learning global representations for an entire image, ReSim learns both regional representations for localization as well as semantic image-level representations. ReSim operates by sliding a fixed-sized window across the overlapping area between two views (e.g., image crops), aligning these areas with their corresponding convolutional feature map regions, and then maximizing the feature similarity across views. As a result, ReSim learns spatially and semantically consistent feature representation throughout the convolutional feature maps of a neural network. A shift or scale of an image region, e.g., a shift or scale of an object, has a corresponding change in the feature maps; this allows downstream tasks to leverage these representations for localization. Through object detection, instance segmentation, and dense pose estimation experiments, we illustrate how ReSim learns representations which significantly improve the localization and classification performance compared to a competitive MoCo-v2 baseline: $+2.7$ AP$^{\text{bb}}_{75}$ VOC, $+1.1$ AP$^{\text{bb}}_{75}$ COCO, and $+1.9$ AP$^{\text{mk}}$ Cityscapes. Code and pre-trained models are released at: \url{https://github.com/Tete-Xiao/ReSim}",0
"Introducing Region Similarity Representation Learning (ReSim), a novel technique for self-supervised representation learning that is specifically designed for localization-based tasks, such as object detection and segmentation. Unlike previous approaches, which primarily focused on learning global representations for an entire image, ReSim learns both regional and semantic image-level representations. ReSim works by sliding a fixed-sized window across the overlapping region between two views, aligning these areas with their corresponding convolutional feature map regions, and maximizing the feature similarity across views. This allows ReSim to learn spatially and semantically consistent feature representations throughout the convolutional feature maps of a neural network. As a result, downstream tasks can leverage these representations for localization, as a shift or scale of an object corresponds to a change in the feature maps. Through experiments in object detection, instance segmentation, and dense pose estimation, we demonstrate that ReSim significantly improves localization and classification performance compared to a competitive MoCo-v2 baseline, with increases in AP75bb VOC, AP75bb COCO, and APmk Cityscapes of 2.7, 1.1, and 1.9, respectively. The code and pre-trained models are available at: \url{https://github.com/Tete-Xiao/ReSim}.",1
"Attention mechanism has been regarded as an advanced technique to capture long-range feature interactions and to boost the representation capability for convolutional neural networks. However, we found two ignored problems in current attentional activations-based models: the approximation problem and the insufficient capacity problem of the attention maps. To solve the two problems together, we initially propose an attention module for convolutional neural networks by developing an AW-convolution, where the shape of attention maps matches that of the weights rather than the activations. Our proposed attention module is a complementary method to previous attention-based schemes, such as those that apply the attention mechanism to explore the relationship between channel-wise and spatial features. Experiments on several datasets for image classification and object detection tasks show the effectiveness of our proposed attention module. In particular, our proposed attention module achieves 1.00% Top-1 accuracy improvement on ImageNet classification over a ResNet101 baseline and 0.63 COCO-style Average Precision improvement on the COCO object detection on top of a Faster R-CNN baseline with the backbone of ResNet101-FPN. When integrating with the previous attentional activations-based models, our proposed attention module can further increase their Top-1 accuracy on ImageNet classification by up to 0.57% and COCO-style Average Precision on the COCO object detection by up to 0.45. Code and pre-trained models will be publicly available.",0
"The attention mechanism is a technique that has been considered as an advanced means of capturing long-range feature interactions and enhancing the representation capability of convolutional neural networks. However, we have identified two problems that are currently overlooked in attentional activations-based models: the approximation problem and the insufficient capacity problem of the attention maps. To address these issues, we have introduced an attention module for convolutional neural networks that utilizes an AW-convolution to match the shape of attention maps with the weights instead of the activations. Our proposed attention module is a complementary approach to previous attention-based schemes, such as those that use the attention mechanism to explore the connection between channel-wise and spatial features. Experiments conducted on various datasets for image classification and object detection tasks have demonstrated the effectiveness of our proposed attention module. Particularly, our attention module has resulted in a 1.00% Top-1 accuracy improvement on ImageNet classification and a 0.63 COCO-style Average Precision improvement on the COCO object detection task over a Faster R-CNN baseline with the ResNet101-FPN backbone. When combined with previous attentional activations-based models, our attention module has further improved their Top-1 accuracy on ImageNet classification by up to 0.57% and COCO-style Average Precision on the COCO object detection by up to 0.45. We will make our code and pre-trained models available to the public.",1
"Unmanned Aerial Vehicles (UAV) can pose a major risk for aviation safety, due to both negligent and malicious use. For this reason, the automated detection and tracking of UAV is a fundamental task in aerial security systems. Common technologies for UAV detection include visible-band and thermal infrared imaging, radio frequency and radar. Recent advances in deep neural networks (DNNs) for image-based object detection open the possibility to use visual information for this detection and tracking task. Furthermore, these detection architectures can be implemented as backbones for visual tracking systems, thereby enabling persistent tracking of UAV incursions. To date, no comprehensive performance benchmark exists that applies DNNs to visible-band imagery for UAV detection and tracking. To this end, three datasets with varied environmental conditions for UAV detection and tracking, comprising a total of 241 videos (331,486 images), are assessed using four detection architectures and three tracking frameworks. The best performing detector architecture obtains an mAP of 98.6% and the best performing tracking framework obtains a MOTA of 96.3%. Cross-modality evaluation is carried out between visible and infrared spectrums, achieving a maximal 82.8% mAP on visible images when training in the infrared modality. These results provide the first public multi-approach benchmark for state-of-the-art deep learning-based methods and give insight into which detection and tracking architectures are effective in the UAV domain.",0
"The use of Unmanned Aerial Vehicles (UAVs) can be unsafe for aviation due to both careless and malicious intentions. Therefore, it is crucial to have automated systems in place to detect and track UAVs for aerial security. Common technologies utilized for UAV detection include visible-band and thermal infrared imaging, radio frequency, and radar. Recent advancements in deep neural networks (DNNs) have opened up the possibility of using visual information for detection and tracking. This can be implemented as a backbone for visual tracking systems, enabling persistent tracking of UAVs. However, there is currently no comprehensive benchmark for DNNs in visible-band imagery for UAV detection and tracking. In this study, three datasets with various environmental conditions were assessed using four detection architectures and three tracking frameworks. The best performing detection architecture achieved an mAP of 98.6%, and the best tracking framework achieved a MOTA of 96.3%. Cross-modality evaluation between visible and infrared spectrums resulted in maximal 82.8% mAP on visible images when trained in the infrared modality. These findings provide valuable insight into effective detection and tracking architectures for UAVs and establish the first public benchmark for state-of-the-art deep learning-based methods.",1
"Deep neural networks have proven increasingly important for automotive scene understanding with new algorithms offering constant improvements of the detection performance. However, there is little emphasis on experiences and needs for deployment in embedded environments. We therefore perform a case study of the deployment of two representative object detection networks on an edge AI platform. In particular, we consider RetinaNet for image-based 2D object detection and PointPillars for LiDAR-based 3D object detection. We describe the modifications necessary to convert the algorithms from a PyTorch training environment to the deployment environment taking into account the available tools. We evaluate the runtime of the deployed DNN using two different libraries, TensorRT and TorchScript. In our experiments, we observe slight advantages of TensorRT for convolutional layers and TorchScript for fully connected layers. We also study the trade-off between runtime and performance, when selecting an optimized setup for deployment, and observe that quantization significantly reduces the runtime while having only little impact on the detection performance.",0
"Automotive scene understanding heavily relies on deep neural networks, which are constantly improving in detection performance thanks to new algorithms. However, little attention has been paid to the deployment of these networks in embedded environments. To address this issue, we present a case study in which we deploy two representative object detection networks on an edge AI platform: RetinaNet for 2D object detection in images and PointPillars for 3D object detection using LiDAR technology. We detail the necessary modifications required to convert the PyTorch training algorithms to a deployment environment, considering the available tools. We evaluate the runtime of the deployed deep neural networks using two libraries, TensorRT and TorchScript. Our results show that TensorRT performs better for convolutional layers and TorchScript for fully connected layers. We also explore the trade-off between runtime and performance when selecting an optimized setup for deployment. Our findings suggest that quantization significantly reduces the runtime with minimal impact on detection performance.",1
"Object detection is a challenging task in remote sensing because objects only occupy a few pixels in the images, and the models are required to simultaneously learn object locations and detection. Even though the established approaches well perform for the objects of regular sizes, they achieve weak performance when analyzing small ones or getting stuck in the local minima (e.g. false object parts). Two possible issues stand in their way. First, the existing methods struggle to perform stably on the detection of small objects because of the complicated background. Second, most of the standard methods used hand-crafted features, and do not work well on the detection of objects parts of which are missing. We here address the above issues and propose a new architecture with a multiple patch feature pyramid network (MPFP-Net). Different from the current models that during training only pursue the most discriminative patches, in MPFPNet the patches are divided into class-affiliated subsets, in which the patches are related and based on the primary loss function, a sequence of smooth loss functions are determined for the subsets to improve the model for collecting small object parts. To enhance the feature representation for patch selection, we introduce an effective method to regularize the residual values and make the fusion transition layers strictly norm-preserving. The network contains bottom-up and crosswise connections to fuse the features of different scales to achieve better accuracy, compared to several state-of-the-art object detection models. Also, the developed architecture is more efficient than the baselines.",0
"Remote sensing makes object detection a difficult task because the objects are only present in a few pixels in the images, and models must learn object location and detection simultaneously. Although established approaches perform well for objects of regular sizes, they struggle with small objects and false object parts, which can cause them to get stuck in local minima. These difficulties arise because existing methods fail to perform stably on small objects due to the complex background, and most standard methods use hand-crafted features that do not work well for objects with missing parts. The proposed multiple patch feature pyramid network (MPFP-Net) addresses these issues by dividing patches into class-affiliated subsets and using a sequence of smooth loss functions to improve the model's ability to collect small object parts. The network also includes bottom-up and crosswise connections to fuse features of different scales and enhance accuracy, while being more efficient than baseline models.",1
"Incremental few-shot learning has emerged as a new and challenging area in deep learning, whose objective is to train deep learning models using very few samples of new class data, and none of the old class data. In this work we tackle the problem of batch incremental few-shot road object detection using data from the India Driving Dataset (IDD). Our approach, DualFusion, combines object detectors in a manner that allows us to learn to detect rare objects with very limited data, all without severely degrading the performance of the detector on the abundant classes. In the IDD OpenSet incremental few-shot detection task, we achieve a mAP50 score of 40.0 on the base classes and an overall mAP50 score of 38.8, both of which are the highest to date. In the COCO batch incremental few-shot detection task, we achieve a novel AP score of 9.9, surpassing the state-of-the-art novel class performance on the same by over 6.6 times.",0
"The area of incremental few-shot learning is a new and challenging field within deep learning. Its objective is to train deep learning models with very few samples of new class data, while disregarding old class data. Our study focuses on batch incremental few-shot road object detection utilizing data from the India Driving Dataset (IDD). Our approach, known as DualFusion, combines object detectors in a way that enables us to identify rare objects despite having limited data, without hindering the detector's performance on the abundant classes. We achieved a mAP50 score of 40.0 and an overall mAP50 score of 38.8, both of which are the highest to date, in the IDD OpenSet incremental few-shot detection task. In the COCO batch incremental few-shot detection task, we achieved a novel AP score of 9.9, which is more than 6.6 times better than the state-of-the-art novel class performance.",1
"As one of the most fundamental and challenging problems in computer vision, object detection tries to locate object instances and find their categories in natural images. The most important step in the evaluation of object detection algorithm is calculating the intersection-over-union (IoU) between the predicted bounding box and the ground truth one. Although this procedure is well-defined and solved for planar images, it is not easy for spherical image object detection. Existing methods either compute the IoUs based on biased bounding box representations or make excessive approximations, thus would give incorrect results. In this paper, we first identify that spherical rectangles are unbiased bounding boxes for objects in spherical images, and then propose an analytical method for IoU calculation without any approximations. Based on the unbiased representation and calculation, we also present an anchor free object detection algorithm for spherical images. The experiments on two spherical object detection datasets show that the proposed method can achieve better performance than existing methods.",0
"Object detection is a difficult and fundamental problem in computer vision, aiming to locate objects and categorize them in natural images. The intersection-over-union (IoU) between the predicted and actual bounding boxes is crucial in evaluating detection algorithms, but it is challenging for spherical image object detection. Current methods either use biased bounding box representations or approximations, leading to inaccurate results. This paper proposes using unbiased spherical rectangles to represent objects and presents an analytical method for IoU calculation without approximations. An anchor-free object detection algorithm for spherical images is also introduced, achieving superior results compared to existing methods in experiments on two spherical object detection datasets.",1
"We propose UniT, a Unified Transformer model to simultaneously learn the most prominent tasks across different domains, ranging from object detection to natural language understanding and multimodal reasoning. Based on the transformer encoder-decoder architecture, our UniT model encodes each input modality with an encoder and makes predictions on each task with a shared decoder over the encoded input representations, followed by task-specific output heads. The entire model is jointly trained end-to-end with losses from each task. Compared to previous efforts on multi-task learning with transformers, we share the same model parameters across all tasks instead of separately fine-tuning task-specific models and handle a much higher variety of tasks across different domains. In our experiments, we learn 7 tasks jointly over 8 datasets, achieving strong performance on each task with significantly fewer parameters. Our code is available in MMF at https://mmf.sh.",0
"We introduce UniT, a model that uses the transformer architecture to learn multiple tasks simultaneously across various domains, including object detection, natural language understanding, and multimodal reasoning. UniT encodes each input modality using an encoder and predicts each task using a shared decoder over the encoded input representations, followed by task-specific output heads. The model is trained end-to-end with losses from each task. Unlike previous multi-task learning efforts with transformers, we use the same model parameters for all tasks and handle a wider range of tasks across different domains. In our experiments, we jointly learn seven tasks over eight datasets and achieve strong performance on each with significantly fewer parameters. Our code is accessible through MMF at https://mmf.sh.",1
"Vision Transformer (ViT) extends the application range of transformers from language processing to computer vision tasks as being an alternative architecture against the existing convolutional neural networks (CNN). Since the transformer-based architecture has been innovative for computer vision modeling, the design convention towards an effective architecture has been less studied yet. From the successful design principles of CNN, we investigate the role of spatial dimension conversion and its effectiveness on transformer-based architecture. We particularly attend to the dimension reduction principle of CNNs; as the depth increases, a conventional CNN increases channel dimension and decreases spatial dimensions. We empirically show that such a spatial dimension reduction is beneficial to a transformer architecture as well, and propose a novel Pooling-based Vision Transformer (PiT) upon the original ViT model. We show that PiT achieves the improved model capability and generalization performance against ViT. Throughout the extensive experiments, we further show PiT outperforms the baseline on several tasks such as image classification, object detection, and robustness evaluation. Source codes and ImageNet models are available at https://github.com/naver-ai/pit",0
"The Vision Transformer (ViT) is a new architecture that expands the use of transformers beyond language processing to include computer vision tasks, offering an alternative to the established convolutional neural networks (CNN). However, there has been little research into designing an effective architecture for ViT. By examining the successful design principles of CNN, we study the role of spatial dimension conversion and its effectiveness for ViT. We focus on the spatial dimension reduction principle of CNNs, which decreases spatial dimensions while increasing channel dimension as depth increases. We demonstrate that this principle is beneficial for ViT and propose a new model called Pooling-based Vision Transformer (PiT), which outperforms the original ViT in image classification, object detection, and robustness evaluation tasks. The source codes and ImageNet models are available at https://github.com/naver-ai/pit.",1
"Deep learning-based 3D object detection has achieved unprecedented success with the advent of large-scale autonomous driving datasets. However, drastic performance degradation remains a critical challenge for cross-domain deployment. In addition, existing 3D domain adaptive detection methods often assume prior access to the target domain annotations, which is rarely feasible in the real world. To address this challenge, we study a more realistic setting, unsupervised 3D domain adaptive detection, which only utilizes source domain annotations. 1) We first comprehensively investigate the major underlying factors of the domain gap in 3D detection. Our key insight is that geometric mismatch is the key factor of domain shift. 2) Then, we propose a novel and unified framework, Multi-Level Consistency Network (MLC-Net), which employs a teacher-student paradigm to generate adaptive and reliable pseudo-targets. MLC-Net exploits point-, instance- and neural statistics-level consistency to facilitate cross-domain transfer. Extensive experiments demonstrate that MLC-Net outperforms existing state-of-the-art methods (including those using additional target domain information) on standard benchmarks. Notably, our approach is detector-agnostic, which achieves consistent gains on both single- and two-stage 3D detectors.",0
"The success of deep learning-based 3D object detection in autonomous driving datasets has been remarkable. However, the issue of performance degradation when deploying across different domains remains a challenge. Current methods for 3D domain adaptive detection assume access to target domain annotations, which is not always practical. To address this, we propose an unsupervised 3D domain adaptive detection approach that only uses source domain annotations. We first analyze the main factors contributing to the domain gap in 3D detection and find that geometric mismatch is the key issue. We then introduce a new framework called Multi-Level Consistency Network (MLC-Net) that uses a teacher-student paradigm to generate adaptive and reliable pseudo-targets. MLC-Net leverages point-, instance-, and neural statistics-level consistency to facilitate cross-domain transfer. Our experiments show that MLC-Net outperforms existing state-of-the-art methods (even those with additional target domain information) on standard benchmarks. Notably, our approach is detector-agnostic and achieves consistent gains on both single- and two-stage 3D detectors.",1
"The conventional detectors tend to make imbalanced classification and suffer performance drop, when the distribution of the training data is severely skewed. In this paper, we propose to use the mean classification score to indicate the classification accuracy for each category during training. Based on this indicator, we balance the classification via an Equilibrium Loss (EBL) and a Memory-augmented Feature Sampling (MFS) method. Specifically, EBL increases the intensity of the adjustment of the decision boundary for the weak classes by a designed score-guided loss margin between any two classes. On the other hand, MFS improves the frequency and accuracy of the adjustment of the decision boundary for the weak classes through over-sampling the instance features of those classes. Therefore, EBL and MFS work collaboratively for finding the classification equilibrium in long-tailed detection, and dramatically improve the performance of tail classes while maintaining or even improving the performance of head classes. We conduct experiments on LVIS using Mask R-CNN with various backbones including ResNet-50-FPN and ResNet-101-FPN to show the superiority of the proposed method. It improves the detection performance of tail classes by 15.6 AP, and outperforms the most recent long-tailed object detectors by more than 1 AP. Code is available at https://github.com/fcjian/LOCE.",0
"When the distribution of training data is heavily skewed, conventional detectors often produce imbalanced classification and experience a decrease in performance. To address this issue, our paper suggests using the mean classification score to indicate the accuracy of each category during training. We then use an Equilibrium Loss (EBL) and a Memory-augmented Feature Sampling (MFS) method to balance the classification. EBL intensifies the adjustment of the decision boundary for weak classes through a score-guided loss margin between any two classes, while MFS over-samples the instance features of those classes to improve the frequency and accuracy of the decision boundary adjustment. EBL and MFS work together to achieve classification equilibrium in long-tailed detection, significantly improving the performance of tail classes while maintaining or even improving the performance of head classes. Our experiments on LVIS using Mask R-CNN with ResNet-50-FPN and ResNet-101-FPN backbones demonstrate the effectiveness of our proposed method, which improves the detection performance of tail classes by 15.6 AP and outperforms the most recent long-tailed object detectors by more than 1 AP. The code is available at https://github.com/fcjian/LOCE.",1
"Multi-level feature fusion is a fundamental topic in computer vision. It has been exploited to detect, segment and classify objects at various scales. When multi-level features meet multi-modal cues, the optimal feature aggregation and multi-modal learning strategy become a hot potato. In this paper, we leverage the inherent multi-modal and multi-level nature of RGB-D salient object detection to devise a novel cascaded refinement network. In particular, first, we propose to regroup the multi-level features into teacher and student features using a bifurcated backbone strategy (BBS). Second, we introduce a depth-enhanced module (DEM) to excavate informative depth cues from the channel and spatial views. Then, RGB and depth modalities are fused in a complementary way. Our architecture, named Bifurcated Backbone Strategy Network (BBS-Net), is simple, efficient, and backbone-independent. Extensive experiments show that BBS-Net significantly outperforms eighteen SOTA models on eight challenging datasets under five evaluation measures, demonstrating the superiority of our approach ($\sim 4 \%$ improvement in S-measure $vs.$ the top-ranked model: DMRA-iccv2019). In addition, we provide a comprehensive analysis on the generalization ability of different RGB-D datasets and provide a powerful training set for future research.",0
"Computer vision relies heavily on multi-level feature fusion, which is used to identify, segment and classify objects of different sizes. However, when multi-level features are combined with multi-modal cues, determining the optimal feature aggregation and multi-modal learning strategy becomes a difficult task. To tackle this challenge, we propose a new approach for RGB-D salient object detection called the Bifurcated Backbone Strategy Network (BBS-Net), which leverages the inherent multi-modal and multi-level nature of the task. Our method involves regrouping the multi-level features into teacher and student features using a bifurcated backbone strategy (BBS), and introducing a depth-enhanced module (DEM) to extract informative depth cues from both channel and spatial views. The RGB and depth modalities are then fused in a complementary way. Our approach is simple, efficient, and backbone-independent, and outperforms eighteen state-of-the-art models on eight challenging datasets across five evaluation measures. We provide a comprehensive analysis of the generalization ability of different RGB-D datasets and offer a powerful training set for future research. Furthermore, our method achieves a $\sim 4 \%$ improvement in S-measure compared to the top-ranked model, DMRA-iccv2019.",1
"3D point cloud understanding has made great progress in recent years. However, one major bottleneck is the scarcity of annotated real datasets, especially compared to 2D object detection tasks, since a large amount of labor is involved in annotating the real scans of a scene. A promising solution to this problem is to make better use of the synthetic dataset, which consists of CAD object models, to boost the learning on real datasets. This can be achieved by the pre-training and fine-tuning procedure. However, recent work on 3D pre-training exhibits failure when transfer features learned on synthetic objects to other real-world applications. In this work, we put forward a new method called RandomRooms to accomplish this objective. In particular, we propose to generate random layouts of a scene by making use of the objects in the synthetic CAD dataset and learn the 3D scene representation by applying object-level contrastive learning on two random scenes generated from the same set of synthetic objects. The model pre-trained in this way can serve as a better initialization when later fine-tuning on the 3D object detection task. Empirically, we show consistent improvement in downstream 3D detection tasks on several base models, especially when less training data are used, which strongly demonstrates the effectiveness and generalization of our method. Benefiting from the rich semantic knowledge and diverse objects from synthetic data, our method establishes the new state-of-the-art on widely-used 3D detection benchmarks ScanNetV2 and SUN RGB-D. We expect our attempt to provide a new perspective for bridging object and scene-level 3D understanding.",0
"The field of 3D point cloud understanding has advanced considerably in recent years, although the scarcity of annotated real datasets remains a significant bottleneck. Annotating real scans of a scene requires significant labor, making it challenging to obtain as many annotated datasets as 2D object detection tasks. One solution to this problem is to utilize synthetic datasets composed of CAD object models to enhance learning on real datasets. However, previous attempts at using pre-training on synthetic objects have been unsuccessful when transferring features to real-world applications. To address this issue, we introduce a new method called RandomRooms. Our approach generates random scene layouts using synthetic CAD objects, and we learn 3D scene representations through object-level contrastive learning on two random scenes generated from the same set of synthetic objects. This pre-training method results in a better initialization when fine-tuning on the 3D object detection task, leading to consistent improvement on several base models, particularly when using less training data. Our method establishes a new state-of-the-art on widely-used 3D detection benchmarks, ScanNetV2 and SUN RGB-D. By leveraging the rich semantic knowledge and diverse objects from synthetic data, we hope to provide a new perspective for bridging object and scene-level 3D understanding.",1
"Vision plays a crucial role to comprehend the world around us as more than 85% of the external information is obtained through the vision system. It largely influences our mobility, cognition, information access, and interaction with the environment as well as with other people. Blindness prevents a person from gaining knowledge of the surrounding environment and makes unassisted navigation, object recognition, obstacle avoidance, and reading tasks major challenges. Many existing systems are often limited by cost and complexity. To help the visually challenged overcome these difficulties faced in everyday life, we propose the idea of VisBuddy, a smart assistant which will help the visually challenged with their day-to-day activities. VisBuddy is a voice-based assistant, where the user can give voice commands to perform specific tasks. VisBuddy uses the techniques of image captioning for describing the user's surroundings, optical character recognition (OCR) for reading the text in the user's view, object detection to search and find the objects in a room and web scraping to give the user the latest news. VisBuddy has been built by combining the concepts from Deep Learning and the Internet of Things. Thus, VisBuddy serves as a cost-efficient, powerful and all-in-one assistant for the visually challenged by helping them with their day-to-day activities.",0
"The ability to see is essential in comprehending the world around us, as over 85% of external information is obtained through our vision system. Our vision greatly impacts our mobility, cognitive abilities, information access, and interactions with both our environment and other people. For those who are blind, gaining knowledge of their surroundings, navigating unassisted, recognizing objects, avoiding obstacles, and reading become significant challenges. Unfortunately, many existing systems are often too costly and complex. In order to help those with visual impairments overcome these obstacles in their daily lives, we present the idea of VisBuddy, a smart assistant that can perform a variety of tasks for the visually challenged. VisBuddy is a voice-activated assistant that utilizes image captioning to describe surroundings, optical character recognition (OCR) to read text in the user's view, object detection to locate objects in a room, and web scraping to provide the latest news. Combining concepts from Deep Learning and the Internet of Things, VisBuddy is a cost-effective, powerful, and all-in-one assistant that provides valuable support to the visually challenged in their daily activities.",1
"This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \textbf{S}hifted \textbf{win}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at~\url{https://github.com/microsoft/Swin-Transformer}.",0
"Introducing the Swin Transformer, a novel vision Transformer, that offers a versatile backbone for computer vision. Unlike language, the visual domain presents unique challenges such as diverse scale variations and higher pixel resolution. Our answer to these challenges is a hierarchical Transformer that uses a Shifted windowing approach to enhance efficiency in the self-attention computation. This architecture is capable of scaling to different levels and has a linear computational complexity that adapts to image size. The Swin Transformer is an ideal choice for various vision tasks such as image classification, object detection, and semantic segmentation, delivering top-notch performance exceeding previous state-of-the-art models. Additionally, the hierarchical design and the shifted window approach prove beneficial for all-MLP architectures. Access the code and models for the Swin Transformer at~\url{https://github.com/microsoft/Swin-Transformer}.",1
"We propose the first learning-based approach for fast moving objects detection. Such objects are highly blurred and move over large distances within one video frame. Fast moving objects are associated with a deblurring and matting problem, also called deblatting. We show that the separation of deblatting into consecutive matting and deblurring allows achieving real-time performance, i.e. an order of magnitude speed-up, and thus enabling new classes of application. The proposed method detects fast moving objects as a truncated distance function to the trajectory by learning from synthetic data. For the sharp appearance estimation and accurate trajectory estimation, we propose a matting and fitting network that estimates the blurred appearance without background, followed by an energy minimization based deblurring. The state-of-the-art methods are outperformed in terms of recall, precision, trajectory estimation, and sharp appearance reconstruction. Compared to other methods, such as deblatting, the inference is of several orders of magnitude faster and allows applications such as real-time fast moving object detection and retrieval in large video collections.",0
"Our novel approach introduces a learning-based method for detecting fast moving objects, which are often characterized by significant blurring and the ability to traverse considerable distances within the confines of a single video frame. The deblurring and matting task, also known as deblatting, is a common issue associated with fast moving objects. Our research demonstrates that by breaking down deblatting into sequential matting and deblurring stages, we can achieve real-time functionality and enable new applications. Our proposed methodology involves utilizing synthetic data to train a truncated distance function to the trajectory for effectively detecting fast moving objects. We present a matting and fitting network for precise trajectory estimation and sharp appearance reconstruction, which is followed by energy minimization-based deblurring. Our approach outperforms state-of-the-art methods in terms of recall, precision, trajectory estimation, and sharp appearance reconstruction. In comparison to other techniques, such as deblatting, our approach is several orders of magnitude faster, making it ideal for real-time fast moving object detection and retrieval in large video collections.",1
"A large gap exists between fully-supervised object detection and weakly-supervised object detection. To narrow this gap, some methods consider knowledge transfer from additional fully-supervised dataset. But these methods do not fully exploit discriminative category information in the fully-supervised dataset, thus causing low mAP. To solve this issue, we propose a novel category transfer framework for weakly supervised object detection. The intuition is to fully leverage both visually-discriminative and semantically-correlated category information in the fully-supervised dataset to enhance the object-classification ability of a weakly-supervised detector. To handle overlapping category transfer, we propose a double-supervision mean teacher to gather common category information and bridge the domain gap between two datasets. To handle non-overlapping category transfer, we propose a semantic graph convolutional network to promote the aggregation of semantic features between correlated categories. Experiments are conducted with Pascal VOC 2007 as the target weakly-supervised dataset and COCO as the source fully-supervised dataset. Our category transfer framework achieves 63.5% mAP and 80.3% CorLoc with 5 overlapping categories between two datasets, which outperforms the state-of-the-art methods. Codes are avaliable at https://github.com/MediaBrain-SJTU/CaT.",0
"The gap between fully-supervised and weakly-supervised object detection is significant, but some methods attempt to narrow it by transferring knowledge from additional fully-supervised datasets. However, these methods do not adequately exploit the discriminative category information in the fully-supervised dataset, leading to low mAP. To address this issue, we introduce a novel category transfer framework for weakly-supervised object detection that leverages both visually-discriminative and semantically-correlated category information in the fully-supervised dataset to improve the object-classification ability of a weakly-supervised detector. To handle overlapping and non-overlapping category transfer, we propose a double-supervision mean teacher and a semantic graph convolutional network, respectively. Our experiments on Pascal VOC 2007 and COCO datasets demonstrate that our framework outperforms state-of-the-art methods, achieving 63.5% mAP and 80.3% CorLoc with 5 overlapping categories. The codes for our framework are available at https://github.com/MediaBrain-SJTU/CaT.",1
"Vision Transformer (ViT) attains state-of-the-art performance in visual recognition, and the variant, Local Vision Transformer, makes further improvements. The major component in Local Vision Transformer, local attention, performs the attention separately over small local windows. We rephrase local attention as a channel-wise locally-connected layer and analyze it from two network regularization manners, sparse connectivity and weight sharing, as well as weight computation. Sparse connectivity: there is no connection across channels, and each position is connected to the positions within a small local window. Weight sharing: the connection weights for one position are shared across channels or within each group of channels. Dynamic weight: the connection weights are dynamically predicted according to each image instance. We point out that local attention resembles depth-wise convolution and its dynamic version in sparse connectivity. The main difference lies in weight sharing - depth-wise convolution shares connection weights (kernel weights) across spatial positions. We empirically observe that the models based on depth-wise convolution and the dynamic variant with lower computation complexity perform on-par with or sometimes slightly better than Swin Transformer, an instance of Local Vision Transformer, for ImageNet classification, COCO object detection and ADE semantic segmentation. These observations suggest that Local Vision Transformer takes advantage of two regularization forms and dynamic weight to increase the network capacity.",0
"The state-of-the-art performance in visual recognition is achieved by Vision Transformer (ViT), with further improvements made by the Local Vision Transformer. The Local Vision Transformer includes a major component called local attention, which performs attention separately over small local windows. We rephrase local attention as a channel-wise locally-connected layer and examine it from two network regularization methods: sparse connectivity and weight sharing, as well as weight computation. Sparse connectivity applies no connection across channels, and each position is linked to positions within a small local window. Weight sharing shares connection weights for one position across channels or within each group of channels. Dynamic weight predicts connection weights dynamically according to each image instance. Local attention resembles depth-wise convolution and its dynamic version in sparse connectivity. The primary distinction is in weight sharing - depth-wise convolution shares connection weights (kernel weights) across spatial positions. Empirical observations show that models based on depth-wise convolution and the dynamic variant perform on-par with or slightly better than Swin Transformer, a Local Vision Transformer instance, for ImageNet classification, COCO object detection, and ADE semantic segmentation. These observations indicate that Local Vision Transformer utilizes two regularization forms and dynamic weight to increase network capacity.",1
"We propose contextual convolution (CoConv) for visual recognition. CoConv is a direct replacement of the standard convolution, which is the core component of convolutional neural networks. CoConv is implicitly equipped with the capability of incorporating contextual information while maintaining a similar number of parameters and computational cost compared to the standard convolution. CoConv is inspired by neuroscience studies indicating that (i) neurons, even from the primary visual cortex (V1 area), are involved in detection of contextual cues and that (ii) the activity of a visual neuron can be influenced by the stimuli placed entirely outside of its theoretical receptive field. On the one hand, we integrate CoConv in the widely-used residual networks and show improved recognition performance over baselines on the core tasks and benchmarks for visual recognition, namely image classification on the ImageNet data set and object detection on the MS COCO data set. On the other hand, we introduce CoConv in the generator of a state-of-the-art Generative Adversarial Network, showing improved generative results on CIFAR-10 and CelebA. Our code is available at https://github.com/iduta/coconv.",0
"Our proposal is for the use of contextual convolution (CoConv) in visual recognition. CoConv is a direct replacement for the standard convolution, a key element of convolutional neural networks. It has the ability to incorporate contextual information without increasing parameters or computational cost. This idea is influenced by neuroscience research, which shows that neurons, even in the primary visual cortex, detect contextual cues, and that the activity of a visual neuron can be influenced by stimuli outside of its receptive field. We have integrated CoConv into widely-used residual networks, resulting in improved recognition performance on core visual recognition tasks, including image classification on the ImageNet dataset and object detection on the MS COCO dataset. Additionally, we have introduced CoConv into the generator of a state-of-the-art Generative Adversarial Network, producing better generative results on CIFAR-10 and CelebA. Our code is available at https://github.com/iduta/coconv.",1
"With the help of the deep learning paradigm, many point cloud networks have been invented for visual analysis. However, there is great potential for development of these networks since the given information of point cloud data has not been fully exploited. To improve the effectiveness of existing networks in analyzing point cloud data, we propose a plug-and-play module, PnP-3D, aiming to refine the fundamental point cloud feature representations by involving more local context and global bilinear response from explicit 3D space and implicit feature space. To thoroughly evaluate our approach, we conduct experiments on three standard point cloud analysis tasks, including classification, semantic segmentation, and object detection, where we select three state-of-the-art networks from each task for evaluation. Serving as a plug-and-play module, PnP-3D can significantly boost the performances of established networks. In addition to achieving state-of-the-art results on four widely used point cloud benchmarks, we present comprehensive ablation studies and visualizations to demonstrate our approach's advantages. The code will be available at https://github.com/ShiQiu0419/pnp-3d.",0
"Numerous point cloud networks have been developed for visual analysis with the aid of the deep learning paradigm. However, there is untapped potential for further development of these networks, as the provided point cloud data is not fully utilized. To enhance the effectiveness of existing networks in analyzing point cloud data, we introduce a plug-and-play module, PnP-3D. Our aim is to refine the fundamental point cloud feature representations by incorporating additional local context and global bilinear response from both explicit 3D space and implicit feature space. To thoroughly assess our methodology, we conduct experiments on three standard point cloud analysis tasks, namely classification, semantic segmentation, and object detection, and evaluate three state-of-the-art networks from each task. PnP-3D serves as a plug-and-play module that significantly enhances the performances of established networks. Additionally, we present extensive ablation studies and visualizations to showcase the advantages of our approach. The code for PnP-3D will be accessible at https://github.com/ShiQiu0419/pnp-3d.",1
"Cross-domain object detection and semantic segmentation have witnessed impressive progress recently. Existing approaches mainly consider the domain shift resulting from external environments including the changes of background, illumination or weather, while distinct camera intrinsic parameters appear commonly in different domains, and their influence for domain adaptation has been very rarely explored. In this paper, we observe that the Field of View (FoV) gap induces noticeable instance appearance differences between the source and target domains. We further discover that the FoV gap between two domains impairs domain adaptation performance under both the FoV-increasing (source FoV < target FoV) and FoV-decreasing cases. Motivated by the observations, we propose the \textbf{Position-Invariant Transform} (PIT) to better align images in different domains. We also introduce a reverse PIT for mapping the transformed/aligned images back to the original image space and design a loss re-weighting strategy to accelerate the training process. Our method can be easily plugged into existing cross-domain detection/segmentation frameworks while bringing about negligible computational overhead. Extensive experiments demonstrate that our method can soundly boost the performance on both cross-domain object detection and segmentation for state-of-the-art techniques. Our code is available at https://github.com/sheepooo/PIT-Position-Invariant-Transform.",0
"Recent advancements have been made in cross-domain object detection and semantic segmentation. However, most existing approaches only consider domain shift caused by external factors such as changes in background, illumination, or weather. Little attention has been paid to the influence of camera intrinsic parameters that commonly appear in different domains. In this study, we observed that the Field of View (FoV) gap causes significant appearance differences between source and target domains, which hinders domain adaptation performance in both FoV-increasing and FoV-decreasing cases. To address this issue, we propose the Position-Invariant Transform (PIT) to better align images in different domains. Our method can be easily integrated into existing frameworks and has negligible computational overhead. Through extensive experiments, we demonstrate that our approach significantly improves the performance of state-of-the-art techniques for cross-domain object detection and segmentation. Our code is available at https://github.com/sheepooo/PIT-Position-Invariant-Transform.",1
"This work addresses the challenging task of LiDAR-based 3D object detection in foggy weather. Collecting and annotating data in such a scenario is very time, labor and cost intensive. In this paper, we tackle this problem by simulating physically accurate fog into clear-weather scenes, so that the abundant existing real datasets captured in clear weather can be repurposed for our task. Our contributions are twofold: 1) We develop a physically valid fog simulation method that is applicable to any LiDAR dataset. This unleashes the acquisition of large-scale foggy training data at no extra cost. These partially synthetic data can be used to improve the robustness of several perception methods, such as 3D object detection and tracking or simultaneous localization and mapping, on real foggy data. 2) Through extensive experiments with several state-of-the-art detection approaches, we show that our fog simulation can be leveraged to significantly improve the performance for 3D object detection in the presence of fog. Thus, we are the first to provide strong 3D object detection baselines on the Seeing Through Fog dataset. Our code is available at www.trace.ethz.ch/lidar_fog_simulation.",0
"The focus of this study is the difficult task of detecting 3D objects using LiDAR in foggy weather conditions. The process of collecting and annotating data for this task is very costly in terms of time and labor. This paper proposes a solution by introducing physically accurate fog simulation into clear-weather scenes. This enables the use of existing clear-weather datasets for the detection of objects in foggy weather. The study offers two contributions: firstly, the development of a fog simulation method that is applicable to any LiDAR dataset, enabling the acquisition of large-scale foggy training data at no extra cost. Secondly, through experiments with several detection approaches, the study shows that the fog simulation significantly improves 3D object detection performance in foggy weather. The study provides strong detection baselines on the Seeing Through Fog dataset and the code is available at www.trace.ethz.ch/lidar_fog_simulation.",1
"Our work aims to obtain 3D reconstruction of hands and manipulated objects from monocular videos. Reconstructing hand-object manipulations holds a great potential for robotics and learning from human demonstrations. The supervised learning approach to this problem, however, requires 3D supervision and remains limited to constrained laboratory settings and simulators for which 3D ground truth is available. In this paper we first propose a learning-free fitting approach for hand-object reconstruction which can seamlessly handle two-hand object interactions. Our method relies on cues obtained with common methods for object detection, hand pose estimation and instance segmentation. We quantitatively evaluate our approach and show that it can be applied to datasets with varying levels of difficulty for which training data is unavailable.",0
"The objective of our work is to achieve 3D reconstruction of hands and objects being manipulated using monocular videos. This process has great potential for robotics and learning from human demonstrations. However, the supervised learning method requires 3D supervision and is only feasible in restricted laboratory settings and simulators where 3D ground truth is accessible. In this article, we present a learning-free fitting method for hand-object reconstruction that can handle two-hand object interactions effortlessly. Our technique relies on cues obtained from common object detection, hand pose estimation, and instance segmentation methods. We have conducted a quantitative evaluation of our approach and demonstrated its applicability to datasets with varying levels of difficulty for which training data is unavailable.",1
"Object detection in optical remote sensing images is an important and challenging task. In recent years, the methods based on convolutional neural networks have made good progress. However, due to the large variation in object scale, aspect ratio, and arbitrary orientation, the detection performance is difficult to be further improved. In this paper, we discuss the role of discriminative features in object detection, and then propose a Critical Feature Capturing Network (CFC-Net) to improve detection accuracy from three aspects: building powerful feature representation, refining preset anchors, and optimizing label assignment. Specifically, we first decouple the classification and regression features, and then construct robust critical features adapted to the respective tasks through the Polarization Attention Module (PAM). With the extracted discriminative regression features, the Rotation Anchor Refinement Module (R-ARM) performs localization refinement on preset horizontal anchors to obtain superior rotation anchors. Next, the Dynamic Anchor Learning (DAL) strategy is given to adaptively select high-quality anchors based on their ability to capture critical features. The proposed framework creates more powerful semantic representations for objects in remote sensing images and achieves high-performance real-time object detection. Experimental results on three remote sensing datasets including HRSC2016, DOTA, and UCAS-AOD show that our method achieves superior detection performance compared with many state-of-the-art approaches. Code and models are available at https://github.com/ming71/CFC-Net.",0
"Detecting objects in optical remote sensing images is a challenging and significant task, with recent advancements in convolutional neural networks showing promise. However, detection performance is difficult to improve due to variations in object scale, aspect ratio, and orientation. This paper discusses the importance of discriminative features in object detection and proposes a Critical Feature Capturing Network (CFC-Net) to enhance accuracy. CFC-Net achieves this by building robust feature representation, refining anchors and optimizing label assignment. The Polarization Attention Module (PAM) is used to construct critical features that are adapted to the respective tasks. The Rotation Anchor Refinement Module (R-ARM) refines localization for horizontal anchors to obtain superior rotation anchors. Dynamic Anchor Learning (DAL) is used to select high-quality anchors that capture critical features. The proposed framework creates powerful semantic representations for objects, enabling high-performance real-time object detection in remote sensing images. Experimental results show that our method outperforms many state-of-the-art approaches on three remote sensing datasets. Our code and models are available at https://github.com/ming71/CFC-Net.",1
"Convolutional Neural Networks achieve state-of-the-art accuracy in object detection tasks. However, they have large computational and energy requirements that challenge their deployment on resource-constrained edge devices. Object detection takes an image as an input, and identifies the existing object classes as well as their locations in the image. In this paper, we leverage the prior knowledge about the probabilities that different object categories can occur jointly to increase the efficiency of object detection models. In particular, our technique clusters the object categories based on their spatial co-occurrence probability. We use those clusters to design an adaptive network. During runtime, a branch controller decides which part(s) of the network to execute based on the spatial context of the input frame. Our experiments using COCO dataset show that our adaptive object detection model achieves up to 45% reduction in the energy consumption, and up to 27% reduction in the latency, with a small loss in the average precision (AP) of object detection.",0
"Object detection tasks can be accurately accomplished by Convolutional Neural Networks. However, these networks require significant computational power and energy, making them unsuitable for use on resource-constrained edge devices. Object detection involves identifying the objects present in an image and their respective categories. This paper proposes a method that makes use of prior knowledge about the likelihood of different object categories co-occurring to enhance the efficiency of object detection models. The technique clusters object categories based on their spatial co-occurrence probability, resulting in an adaptive network design. During runtime, a branch controller determines which part(s) of the network to execute based on the spatial context of the input frame. The experiments conducted using the COCO dataset reveal that the proposed adaptive object detection model reduces energy consumption by up to 45% and latency by up to 27%, with only a minor decrease in the average precision of object detection.",1
"Tremendous progress has been made in visual representation learning, notably with the recent success of self-supervised contrastive learning methods. Supervised contrastive learning has also been shown to outperform its cross-entropy counterparts by leveraging labels for choosing where to contrast. However, there has been little work to explore the transfer capability of contrastive learning to a different domain. In this paper, we conduct a comprehensive study on the transferability of learned representations of different contrastive approaches for linear evaluation, full-network transfer, and few-shot recognition on 12 downstream datasets from different domains, and object detection tasks on MSCOCO and VOC0712. The results show that the contrastive approaches learn representations that are easily transferable to a different downstream task. We further observe that the joint objective of self-supervised contrastive loss with cross-entropy/supervised-contrastive loss leads to better transferability of these models over their supervised counterparts. Our analysis reveals that the representations learned from the contrastive approaches contain more low/mid-level semantics than cross-entropy models, which enables them to quickly adapt to a new task. Our codes and models will be publicly available to facilitate future research on transferability of visual representations.",0
"Significant advancements have been made in visual representation learning, particularly with the recent triumphs of self-supervised contrastive learning methods. While supervised contrastive learning outperforms its cross-entropy equivalents by utilizing labels to determine where to contrast, little research has been conducted to explore the transferability of contrastive learning to another domain. This study comprehensively examines the transfer capability of various contrastive approaches for linear evaluation, full-network transfer, and few-shot recognition on 12 downstream datasets from different domains, as well as object detection tasks on MSCOCO and VOC0712. The results indicate that contrastive approaches learn easily transferable representations for a different downstream task. Additionally, our analysis uncovers that the joint objective of self-supervised contrastive loss with cross-entropy/supervised-contrastive loss leads to better transferability of these models than their supervised counterparts. The representations learned from contrastive approaches contain more low/mid-level semantics than cross-entropy models, which enables them to quickly adapt to a new task. Our codes and models will be available to facilitate future research on transferability of visual representations.",1
"Object proposals have become an integral preprocessing steps of many vision pipelines including object detection, weakly supervised detection, object discovery, tracking, etc. Compared to the learning-free methods, learning-based proposals have become popular recently due to the growing interest in object detection. The common paradigm is to learn object proposals from data labeled with a set of object regions and their corresponding categories. However, this approach often struggles with novel objects in the open world that are absent in the training set. In this paper, we identify that the problem is that the binary classifiers in existing proposal methods tend to overfit to the training categories. Therefore, we propose a classification-free Object Localization Network (OLN) which estimates the objectness of each region purely by how well the location and shape of a region overlap with any ground-truth object (e.g., centerness and IoU). This simple strategy learns generalizable objectness and outperforms existing proposals on cross-category generalization on COCO, as well as cross-dataset evaluation on RoboNet, Object365, and EpicKitchens. Finally, we demonstrate the merit of OLN for long-tail object detection on large vocabulary dataset, LVIS, where we notice clear improvement in rare and common categories.",0
"Many vision pipelines, including object detection, weakly supervised detection, object discovery, and tracking, rely on object proposals as an essential preprocessing step. Recently, learning-based proposals have gained popularity due to the interest in object detection, although they struggle with novel objects not present in the training set. This issue arises because binary classifiers in existing proposal methods tend to overfit to the training categories. To address this problem, we propose a classification-free Object Localization Network (OLN) that estimates the objectness of each region solely based on how well its location and shape overlap with any ground-truth object. This approach learns generalizable objectness and outperforms existing proposals on cross-category generalization on COCO, as well as cross-dataset evaluation on RoboNet, Object365, and EpicKitchens. Furthermore, we demonstrate the effectiveness of OLN for long-tail object detection on the large vocabulary dataset, LVIS, where we observe significant improvements in both rare and common categories.",1
"In autonomous driving, a LiDAR-based object detector should perform reliably at different geographic locations and under various weather conditions. While recent 3D detection research focuses on improving performance within a single domain, our study reveals that the performance of modern detectors can drop drastically cross-domain. In this paper, we investigate unsupervised domain adaptation (UDA) for LiDAR-based 3D object detection. On the Waymo Domain Adaptation dataset, we identify the deteriorating point cloud quality as the root cause of the performance drop. To address this issue, we present Semantic Point Generation (SPG), a general approach to enhance the reliability of LiDAR detectors against domain shifts. Specifically, SPG generates semantic points at the predicted foreground regions and faithfully recovers missing parts of the foreground objects, which are caused by phenomena such as occlusions, low reflectance or weather interference. By merging the semantic points with the original points, we obtain an augmented point cloud, which can be directly consumed by modern LiDAR-based detectors. To validate the wide applicability of SPG, we experiment with two representative detectors, PointPillars and PV-RCNN. On the UDA task, SPG significantly improves both detectors across all object categories of interest and at all difficulty levels. SPG can also benefit object detection in the original domain. On the Waymo Open Dataset and KITTI, SPG improves 3D detection results of these two methods across all categories. Combined with PV-RCNN, SPG achieves state-of-the-art 3D detection results on KITTI.",0
"The performance of LiDAR-based object detectors in autonomous driving must be consistent across different geographic locations and weather conditions. Although current 3D detection research is focused on enhancing performance within a single domain, our investigation demonstrates that modern detectors can experience a significant drop in performance when applied to different domains. Thus, we explore the possibility of unsupervised domain adaptation (UDA) in LiDAR-based 3D object detection. Using the Waymo Domain Adaptation dataset, we identify that the quality of point clouds is the main cause of performance deteriorations. To address this issue, we present Semantic Point Generation (SPG), a technique that enhances the reliability of LiDAR detectors against domain shifts. SPG generates semantic points in predicted foreground regions and recovers missing parts of foreground objects caused by occlusions, low reflectance, or weather interference. By merging the semantic points with the original points, we create an augmented point cloud that modern LiDAR-based detectors can directly use. Our experiments demonstrate that SPG significantly improves the performance of two representative detectors, PointPillars and PV-RCNN, across all object categories and difficulty levels in the UDA task. Additionally, SPG enhances 3D detection results for both detectors in the original domain, as shown in our experiments on the Waymo Open Dataset and KITTI. Finally, when combined with PV-RCNN, SPG achieves state-of-the-art 3D detection results on KITTI.",1
"To improve the generalization of detectors, for domain adaptive object detection (DAOD), recent advances mainly explore aligning feature-level distributions between the source and single-target domain, which may neglect the impact of domain-specific information existing in the aligned features. Towards DAOD, it is important to extract domain-invariant object representations. To this end, in this paper, we try to disentangle domain-invariant representations from domain-specific representations. And we propose a novel disentangled method based on vector decomposition. Firstly, an extractor is devised to separate domain-invariant representations from the input, which are used for extracting object proposals. Secondly, domain-specific representations are introduced as the differences between the input and domain-invariant representations. Through the difference operation, the gap between the domain-specific and domain-invariant representations is enlarged, which promotes domain-invariant representations to contain more domain-irrelevant information. In the experiment, we separately evaluate our method on the single- and compound-target case. For the single-target case, experimental results of four domain-shift scenes show our method obtains a significant performance gain over baseline methods. Moreover, for the compound-target case (i.e., the target is a compound of two different domains without domain labels), our method outperforms baseline methods by around 4%, which demonstrates the effectiveness of our method.",0
"Recent advancements in domain adaptive object detection (DAOD) have primarily focused on aligning feature-level distributions between the source and single-target domains to enhance the generalization of detectors. However, this approach may disregard the impact of domain-specific information present in aligned features. Thus, it is crucial to extract domain-invariant object representations for successful DAOD. In this study, we propose a novel disentangled method utilizing vector decomposition to separate domain-invariant and domain-specific representations. We introduce an extractor to isolate domain-invariant representations from the input for object proposal extraction. The differences between the input and domain-invariant representations are utilized as domain-specific representations. We increase the gap between domain-specific and domain-invariant representations through the difference operation, promoting domain-invariant representations to contain more domain-irrelevant information. Our method outperforms baseline methods in both single- and compound-target cases, with a significant performance gain of around 4% in the latter. These findings demonstrate the effectiveness of our approach for DAOD.",1
"In this paper, we present a self-training method, named ST3D++, with a holistic pseudo label denoising pipeline for unsupervised domain adaptation on 3D object detection. ST3D++ aims at reducing noise in pseudo label generation as well as alleviating the negative impacts of noisy pseudo labels on model training. First, ST3D++ pre-trains the 3D object detector on the labeled source domain with random object scaling (ROS) which is designed to reduce target domain pseudo label noise arising from object scale bias of the source domain. Then, the detector is progressively improved through alternating between generating pseudo labels and training the object detector with pseudo-labeled target domain data. Here, we equip the pseudo label generation process with a hybrid quality-aware triplet memory to improve the quality and stability of generated pseudo labels. Meanwhile, in the model training stage, we propose a source data assisted training strategy and a curriculum data augmentation policy to effectively rectify noisy gradient directions and avoid model over-fitting to noisy pseudo labeled data. These specific designs enable the detector to be trained on meticulously refined pseudo labeled target data with denoised training signals, and thus effectively facilitate adapting an object detector to a target domain without requiring annotations. Finally, our method is assessed on four 3D benchmark datasets (i.e., Waymo, KITTI, Lyft, and nuScenes) for three common categories (i.e., car, pedestrian and bicycle). ST3D++ achieves state-of-the-art performance on all evaluated settings, outperforming the corresponding baseline by a large margin (e.g., 9.6% $\sim$ 38.16% on Waymo $\rightarrow$ KITTI in terms of AP$_{\text{3D}}$), and even surpasses the fully supervised oracle results on the KITTI 3D object detection benchmark with target prior. Code will be available.",0
"The paper introduces a technique for unsupervised domain adaptation on 3D object detection, called ST3D++. The method includes a pseudo label denoising pipeline to reduce noise in the generation of pseudo labels and minimize negative effects on model training. ST3D++ starts with pre-training the object detector on the labeled source domain with random object scaling (ROS) to reduce target domain pseudo label noise due to object scale bias. The detector is then improved through alternating between generating pseudo labels and training the object detector with pseudo-labeled target domain data. A hybrid quality-aware triplet memory is used to improve the quality and stability of generated pseudo labels. In the model training stage, a source data assisted training strategy and a curriculum data augmentation policy are proposed to rectify noisy gradient directions and avoid model over-fitting to noisy pseudo labeled data. ST3D++ enables the detector to be trained on meticulously refined pseudo labeled target data with denoised training signals, facilitating domain adaptation without requiring annotations. The method is evaluated on four 3D benchmark datasets (Waymo, KITTI, Lyft, and nuScenes) for three categories (car, pedestrian, and bicycle) and achieves state-of-the-art performance, outperforming the corresponding baseline by a large margin. Code will be available.",1
"Few-shot object detection (FSOD) aims to strengthen the performance of novel object detection with few labeled samples. To alleviate the constraint of few samples, enhancing the generalization ability of learned features for novel objects plays a key role. Thus, the feature learning process of FSOD should focus more on intrinsical object characteristics, which are invariant under different visual changes and therefore are helpful for feature generalization. Unlike previous attempts of the meta-learning paradigm, in this paper, we explore how to enhance object features with intrinsical characteristics that are universal across different object categories. We propose a new prototype, namely universal prototype, that is learned from all object categories. Besides the advantage of characterizing invariant characteristics, the universal prototypes alleviate the impact of unbalanced object categories. After enhancing object features with the universal prototypes, we impose a consistency loss to maximize the agreement between the enhanced features and the original ones, which is beneficial for learning invariant object characteristics. Thus, we develop a new framework of few-shot object detection with universal prototypes ({FSOD}^{up}) that owns the merit of feature generalization towards novel objects. Experimental results on PASCAL VOC and MS COCO show the effectiveness of {FSOD}^{up}. Particularly, for the 1-shot case of VOC Split2, {FSOD}^{up} outperforms the baseline by 6.8% in terms of mAP.",0
"The goal of few-shot object detection (FSOD) is to improve the accuracy of detecting new objects with limited labeled data. To overcome the challenge of having few samples, it is essential to enhance the generalization ability of the learned features for new objects. Therefore, the feature learning process should focus on intrinsic characteristics of the object that remain consistent despite visual changes, aiding in feature generalization. In contrast to previous meta-learning approaches, this study proposes a novel prototype called the universal prototype, which is learned from all object categories and has intrinsic characteristics universal across different object categories. Using universal prototypes enhances object features and reduces the impact of unbalanced object categories. To maximize the agreement between the original and enhanced features, a consistency loss is imposed. The result is a new framework for FSOD with universal prototypes ({FSOD}^{up}) that has the advantage of feature generalization for new objects. The effectiveness of {FSOD}^{up} is demonstrated through experiments on PASCAL VOC and MS COCO, where it outperforms the baseline by 6.8% in terms of mAP for the 1-shot case of VOC Split2.",1
"3D object detection is an important task in computer vision. Most existing methods require a large number of high-quality 3D annotations, which are expensive to collect. Especially for outdoor scenes, the problem becomes more severe due to the sparseness of the point cloud and the complexity of urban scenes. Semi-supervised learning is a promising technique to mitigate the data annotation issue. Inspired by this, we propose a novel semi-supervised framework based on pseudo-labeling for outdoor 3D object detection tasks. We design the Adaptive Class Confidence Selection module (ACCS) to generate high-quality pseudo-labels. Besides, we propose Holistic Point Cloud Augmentation (HPCA) for unlabeled data to improve robustness. Experiments on the KITTI benchmark demonstrate the effectiveness of our method.",0
"Computer vision relies heavily on the task of detecting 3D objects, which is crucial. However, most current approaches require a significant number of high-quality 3D annotations, which are often difficult and costly to obtain. This issue is particularly severe in outdoor settings, where the point cloud is often sparse and the urban scene is complex. To circumvent this issue, semi-supervised learning is an effective technique. In line with this, we present a fresh semi-supervised framework that relies on pseudo-labeling for outdoor 3D object detection tasks. Our approach uses the Adaptive Class Confidence Selection (ACCS) module to create dependable pseudo-labels while also introducing Holistic Point Cloud Augmentation (HPCA) to improve the robustness of unlabeled data. Our method's effectiveness is demonstrated through experiments conducted on the KITTI benchmark.",1
"Unmanned aerial vehicles (UAVs) equipped with multiple complementary sensors have tremendous potential for fast autonomous or remote-controlled semantic scene analysis, e.g., for disaster examination. In this work, we propose a UAV system for real-time semantic inference and fusion of multiple sensor modalities. Semantic segmentation of LiDAR scans and RGB images, as well as object detection on RGB and thermal images, run online onboard the UAV computer using lightweight CNN architectures and embedded inference accelerators. We follow a late fusion approach where semantic information from multiple modalities augments 3D point clouds and image segmentation masks while also generating an allocentric semantic map. Our system provides augmented semantic images and point clouds with $\approx\,$9$\,$Hz. We evaluate the integrated system in real-world experiments in an urban environment.",0
"The potential of unmanned aerial vehicles (UAVs) that have several complementary sensors is immense since they can quickly analyze scenes autonomously or remotely, especially in disaster scenarios. This study proposes a UAV system that can perform real-time semantic inference and combine several sensor modalities. The system can conduct semantic segmentation of LiDAR scans and RGB images and object detection on RGB and thermal images in real-time on the UAV computer with the help of lightweight CNN architectures and embedded inference accelerators. Our approach is a late fusion method where semantic data from various modalities enhances 3D point clouds and image segmentation masks, leading to an allocentric semantic map. The system provides augmented semantic images and point clouds at a frequency of approximately 9Hz. Real-world experiments were conducted in an urban environment to evaluate the integrated system.",1
"Video captioning targets interpreting the complex visual contents as text descriptions, which requires the model to fully understand video scenes including objects and their interactions. Prevailing methods adopt off-the-shelf object detection networks to give object proposals and use the attention mechanism to model the relations between objects. They often miss some undefined semantic concepts of the pretrained model and fail to identify exact predicate relationships between objects. In this paper, we investigate an open research task of generating text descriptions for the given videos, and propose Cross-Modal Graph (CMG) with meta concepts for video captioning. Specifically, to cover the useful semantic concepts in video captions, we weakly learn the corresponding visual regions for text descriptions, where the associated visual regions and textual words are named cross-modal meta concepts. We further build meta concept graphs dynamically with the learned cross-modal meta concepts. We also construct holistic video-level and local frame-level video graphs with the predicted predicates to model video sequence structures. We validate the efficacy of our proposed techniques with extensive experiments and achieve state-of-the-art results on two public datasets.",0
"The objective of video captioning is to interpret intricate visual content as text descriptions, which necessitates the model to understand video scenes including objects and their interactions. Existing methods use off-the-shelf object detection networks to provide object proposals and employ the attention mechanism to model relations between objects. However, these methods often overlook some undefined semantic concepts of the pretrained model and are unable to identify precise predicate relationships between objects. This paper explores an open research task of generating text descriptions for given videos and introduces Cross-Modal Graph (CMG) with meta concepts for video captioning. To capture useful semantic concepts in video captions, we weakly learn corresponding visual regions for text descriptions, which we refer to as cross-modal meta concepts comprising associated visual regions and textual words. We dynamically build meta concept graphs with learned cross-modal meta concepts and construct holistic video-level and local frame-level video graphs with predicted predicates to model video sequence structures. We demonstrate the effectiveness of our proposed techniques through extensive experiments and achieve state-of-the-art results on two public datasets.",1
"Knowledge distillation (KD) is a popular method to train efficient networks (""student"") with the help of high-capacity networks (""teacher""). Traditional methods use the teacher's soft logits as extra supervision to train the student network. In this paper, we argue that it is more advantageous to make the student mimic the teacher's features in the penultimate layer. Not only the student can directly learn more effective information from the teacher feature, feature mimicking can also be applied for teachers trained without a softmax layer. Experiments show that it can achieve higher accuracy than traditional KD. To further facilitate feature mimicking, we decompose a feature vector into the magnitude and the direction. We argue that the teacher should give more freedom to the student feature's magnitude, and let the student pay more attention on mimicking the feature direction. To meet this requirement, we propose a loss term based on locality-sensitive hashing (LSH). With the help of this new loss, our method indeed mimics feature directions more accurately, relaxes constraints on feature magnitudes, and achieves state-of-the-art distillation accuracy. We provide theoretical analyses of how LSH facilitates feature direction mimicking, and further extend feature mimicking to multi-label recognition and object detection.",0
"The method of Knowledge Distillation (KD) is widely used to train efficient networks called ""student"" by utilizing high-capacity networks known as ""teacher"". The traditional approach involves using the teacher's soft logits to provide extra supervision for training the student network. However, in this article, we propose that it is more beneficial to have the student mimic the teacher's features in the penultimate layer. This approach enables the student to learn more effective information directly from the teacher's features, and it can also be applied to teachers trained without a softmax layer. Our experiments have proven that this approach can achieve higher accuracy than traditional KD. To enhance feature mimicking, we break down a feature vector into magnitude and direction. We suggest that the teacher should allow more freedom to the student feature's magnitude, while the student should focus more on mimicking the feature direction. To achieve this, we propose a loss term based on locality-sensitive hashing (LSH). With this new loss, our method accurately mimics feature directions, relaxes constraints on feature magnitudes, and achieves state-of-the-art distillation accuracy. We provide theoretical analyses of how LSH facilitates feature direction mimicking, and we also extend feature mimicking to multi-label recognition and object detection.",1
"Recent progress in 3D object detection from single images leverages monocular depth estimation as a way to produce 3D pointclouds, turning cameras into pseudo-lidar sensors. These two-stage detectors improve with the accuracy of the intermediate depth estimation network, which can itself be improved without manual labels via large-scale self-supervised learning. However, they tend to suffer from overfitting more than end-to-end methods, are more complex, and the gap with similar lidar-based detectors remains significant. In this work, we propose an end-to-end, single stage, monocular 3D object detector, DD3D, that can benefit from depth pre-training like pseudo-lidar methods, but without their limitations. Our architecture is designed for effective information transfer between depth estimation and 3D detection, allowing us to scale with the amount of unlabeled pre-training data. Our method achieves state-of-the-art results on two challenging benchmarks, with 16.34% and 9.28% AP for Cars and Pedestrians (respectively) on the KITTI-3D benchmark, and 41.5% mAP on NuScenes.",0
"Monocular depth estimation has been used to produce 3D pointclouds that turn cameras into pseudo-lidar sensors, resulting in recent progress in 3D object detection from single images. However, two-stage detectors that rely on intermediate depth estimation networks tend to suffer from overfitting, are more complex, and have a significant gap compared to similar lidar-based detectors. To address these limitations, we propose an end-to-end, single stage, monocular 3D object detector, DD3D, that can benefit from depth pre-training like pseudo-lidar methods. Our architecture facilitates effective information transfer between depth estimation and 3D detection, enabling us to scale with unlabeled pre-training data. With our method, we achieve state-of-the-art results on two challenging benchmarks, with 16.34% and 9.28% AP for Cars and Pedestrians (respectively) on the KITTI-3D benchmark, and 41.5% mAP on NuScenes.",1
"The multi-modal salient object detection model based on RGB-D information has better robustness in the real world. However, it remains nontrivial to better adaptively balance effective multi-modal information in the feature fusion phase. In this letter, we propose a novel gated recoding network (GRNet) to evaluate the information validity of the two modes, and balance their influence. Our framework is divided into three phases: perception phase, recoding mixing phase and feature integration phase. First, A perception encoder is adopted to extract multi-level single-modal features, which lays the foundation for multi-modal semantic comparative analysis. Then, a modal-adaptive gate unit (MGU) is proposed to suppress the invalid information and transfer the effective modal features to the recoding mixer and the hybrid branch decoder. The recoding mixer is responsible for recoding and mixing the balanced multi-modal information. Finally, the hybrid branch decoder completes the multi-level feature integration under the guidance of an optional edge guidance stream (OEGS). Experiments and analysis on eight popular benchmarks verify that our framework performs favorably against 9 state-of-art methods.",0
"The real-world robustness of the RGB-D based multi-modal salient object detection model is superior, but effectively balancing multi-modal information during the feature fusion phase remains challenging. To address this, we introduce a novel gated recoding network (GRNet) that evaluates the validity of the two modes and balances their influence. Our framework comprises three phases: perception, recoding mixing, and feature integration. Initially, the perception encoder extracts multi-level single-modal features, enabling multi-modal semantic comparative analysis. The modal-adaptive gate unit (MGU) suppresses invalid information and transfers effective modal features to the recoding mixer and hybrid branch decoder. The recoding mixer recodes and blends the balanced multi-modal information, and the hybrid branch decoder integrates multi-level features, guided by an optional edge guidance stream (OEGS). We demonstrate through experiments and analysis on eight popular benchmarks that our framework outperforms nine state-of-art methods.",1
"Geometry Projection is a powerful depth estimation method in monocular 3D object detection. It estimates depth dependent on heights, which introduces mathematical priors into the deep model. But projection process also introduces the error amplification problem, in which the error of the estimated height will be amplified and reflected greatly at the output depth. This property leads to uncontrollable depth inferences and also damages the training efficiency. In this paper, we propose a Geometry Uncertainty Projection Network (GUP Net) to tackle the error amplification problem at both inference and training stages. Specifically, a GUP module is proposed to obtains the geometry-guided uncertainty of the inferred depth, which not only provides high reliable confidence for each depth but also benefits depth learning. Furthermore, at the training stage, we propose a Hierarchical Task Learning strategy to reduce the instability caused by error amplification. This learning algorithm monitors the learning situation of each task by a proposed indicator and adaptively assigns the proper loss weights for different tasks according to their pre-tasks situation. Based on that, each task starts learning only when its pre-tasks are learned well, which can significantly improve the stability and efficiency of the training process. Extensive experiments demonstrate the effectiveness of the proposed method. The overall model can infer more reliable object depth than existing methods and outperforms the state-of-the-art image-based monocular 3D detectors by 3.74% and 4.7% AP40 of the car and pedestrian categories on the KITTI benchmark.",0
"Monocular 3D object detection relies on Geometry Projection, a method that estimates depth based on heights and incorporates mathematical priors into the deep model. However, this method also introduces the error amplification problem, where errors in estimated height are greatly reflected in the output depth, leading to uncontrollable depth inferences and damaging training efficiency. Our proposed solution is the Geometry Uncertainty Projection Network (GUP Net), which addresses this problem at both inference and training stages. The GUP module obtains the geometry-guided uncertainty of inferred depth, providing reliable confidence and benefiting depth learning. The Hierarchical Task Learning strategy is used to reduce instability by monitoring each task's learning situation and assigning proper loss weights accordingly. This approach significantly improves training efficiency and stability. Experiments demonstrate the effectiveness of our method, which outperforms existing techniques by 3.74% and 4.7% AP40 of the car and pedestrian categories on the KITTI benchmark, respectively.",1
"Autonomous parking systems start with the detection of available parking slots. Parking slot detection performance has been dramatically improved by deep learning techniques. Deep learning-based object detection methods can be categorized into one-stage and two-stage approaches. Although it is well-known that the two-stage approach outperforms the one-stage approach in general object detection, they have performed similarly in parking slot detection so far. We consider this is because the two-stage approach has not yet been adequately specialized for parking slot detection. Thus, this paper proposes a highly specialized two-stage parking slot detector that uses region-specific multi-scale feature extraction. In the first stage, the proposed method finds the entrance of the parking slot as a region proposal by estimating its center, length, and orientation. The second stage of this method designates specific regions that most contain the desired information and extracts features from them. That is, features for the location and orientation are separately extracted from only the specific regions that most contain the locational and orientational information. In addition, multi-resolution feature maps are utilized to increase both positioning and classification accuracies. A high-resolution feature map is used to extract detailed information (location and orientation), while another low-resolution feature map is used to extract semantic information (type and occupancy). In experiments, the proposed method was quantitatively evaluated with two large-scale public parking slot detection datasets and outperformed previous methods, including both one-stage and two-stage approaches.",0
"The detection of available parking slots is the first step in autonomous parking systems, and it has been greatly enhanced by deep learning techniques. These techniques involve one-stage and two-stage approaches for object detection, with the latter typically performing better. However, current two-stage methods have not been specialized enough for parking slot detection. This study proposes a specialized two-stage parking slot detector that employs region-specific multi-scale feature extraction. The first stage identifies the entrance of the parking slot by estimating its center, length, and orientation. The second stage extracts features from specific regions that contain the desired information, with location and orientation features extracted separately. Multi-resolution feature maps are used to increase positioning and classification accuracies. The proposed method outperforms previous methods, including both one-stage and two-stage approaches, in experiments with two large-scale public parking slot detection datasets.",1
"Deep learning and convolutional neural networks allow achieving impressive performance in computer vision tasks, such as object detection and semantic segmentation (SS). However, recent studies have shown evident weaknesses of such models against adversarial perturbations. In a real-world scenario instead, like autonomous driving, more attention should be devoted to real-world adversarial examples (RWAEs), which are physical objects (e.g., billboards and printable patches) optimized to be adversarial to the entire perception pipeline. This paper presents an in-depth evaluation of the robustness of popular SS models by testing the effects of both digital and real-world adversarial patches. These patches are crafted with powerful attacks enriched with a novel loss function. Firstly, an investigation on the Cityscapes dataset is conducted by extending the Expectation Over Transformation (EOT) paradigm to cope with SS. Then, a novel attack optimization, called scene-specific attack, is proposed. Such an attack leverages the CARLA driving simulator to improve the transferability of the proposed EOT-based attack to a real 3D environment. Finally, a printed physical billboard containing an adversarial patch was tested in an outdoor driving scenario to assess the feasibility of the studied attacks in the real world. Exhaustive experiments revealed that the proposed attack formulations outperform previous work to craft both digital and real-world adversarial patches for SS. At the same time, the experimental results showed how these attacks are notably less effective in the real world, hence questioning the practical relevance of adversarial attacks to SS models for autonomous/assisted driving.",0
"Computer vision tasks, such as object detection and semantic segmentation, can achieve impressive performance through the use of deep learning and convolutional neural networks. However, recent studies have exposed the weaknesses of these models against adversarial perturbations. In real-world scenarios, such as autonomous driving, it is important to focus on real-world adversarial examples (RWAEs), which are physical objects designed to be adversarial to the entire perception pipeline. This study evaluates the robustness of popular semantic segmentation models against both digital and real-world adversarial patches. The patches are created using powerful attacks with a novel loss function. The study includes an investigation on the Cityscapes dataset using the Expectation Over Transformation paradigm, as well as a proposed scene-specific attack optimization that utilizes the CARLA driving simulator to improve the transferability of the EOT-based attack to a real 3D environment. Finally, a printed physical billboard containing an adversarial patch is tested in an outdoor driving scenario to determine the feasibility of the studied attacks in the real world. The results show that the proposed attack formulations are more effective than previous work in creating both digital and real-world adversarial patches for semantic segmentation. However, the study also reveals that these attacks are notably less effective in the real world, which raises questions about their practical relevance for autonomous/assisted driving.",1
"The goals of this research were to search for Convolutional Neural Network (CNN) architectures, suitable for an on-device processor with limited computing resources, performing at substantially lower Network Architecture Search (NAS) costs. A new algorithm entitled an Early Exit Population Initialisation (EE-PI) for Evolutionary Algorithm (EA) was developed to achieve both goals. The EE-PI reduces the total number of parameters in the search process by filtering the models with fewer parameters than the maximum threshold. It will look for a new model to replace those models with parameters more than the threshold. Thereby, reducing the number of parameters, memory usage for model storage and processing time while maintaining the same performance or accuracy. The search time was reduced to 0.52 GPU day. This is a huge and significant achievement compared to the NAS of 4 GPU days achieved using NSGA-Net, 3,150 GPU days by the AmoebaNet model, and the 2,000 GPU days by the NASNet model. As well, Early Exit Evolutionary Algorithm networks (EEEA-Nets) yield network architectures with minimal error and computational cost suitable for a given dataset as a class of network algorithms. Using EEEA-Net on CIFAR-10, CIFAR-100, and ImageNet datasets, our experiments showed that EEEA-Net achieved the lowest error rate among state-of-the-art NAS models, with 2.46% for CIFAR-10, 15.02% for CIFAR-100, and 23.8% for ImageNet dataset. Further, we implemented this image recognition architecture for other tasks, such as object detection, semantic segmentation, and keypoint detection tasks, and, in our experiments, EEEA-Net-C2 outperformed MobileNet-V3 on all of these various tasks. (The algorithm code is available at https://github.com/chakkritte/EEEA-Net).",0
"The aim of this study was to locate Convolutional Neural Network (CNN) designs that are suitable for an on-device processor with limited computing resources, while reducing Network Architecture Search (NAS) costs significantly. To achieve these objectives, a new algorithm was developed called Early Exit Population Initialisation (EE-PI) for Evolutionary Algorithm (EA). The EE-PI decreases the number of parameters in the search process by filtering out models with fewer parameters than the maximum threshold and replacing them with new models. This method reduces the number of parameters, memory usage for model storage and processing time, while maintaining the same accuracy. The search time was reduced to 0.52 GPU day, which is a significant accomplishment compared to other NAS models. The Early Exit Evolutionary Algorithm networks (EEEA-Nets) provide network architectures with minimal error and computational cost, making them suitable for a given dataset. The EEEA-Net was tested on CIFAR-10, CIFAR-100, and ImageNet datasets and achieved the lowest error rate among state-of-the-art NAS models. Moreover, this image recognition architecture was implemented for various tasks, including object detection, semantic segmentation, and keypoint detection tasks, and outperformed MobileNet-V3 in all experiments. The algorithm code is available at https://github.com/chakkritte/EEEA-Net.",1
"Recognizing and localizing objects in the 3D space is a crucial ability for an AI agent to perceive its surrounding environment. While significant progress has been achieved with expensive LiDAR point clouds, it poses a great challenge for 3D object detection given only a monocular image. While there exist different alternatives for tackling this problem, it is found that they are either equipped with heavy networks to fuse RGB and depth information or empirically ineffective to process millions of pseudo-LiDAR points. With in-depth examination, we realize that these limitations are rooted in inaccurate object localization. In this paper, we propose a novel and lightweight approach, dubbed {\em Progressive Coordinate Transforms} (PCT) to facilitate learning coordinate representations. Specifically, a localization boosting mechanism with confidence-aware loss is introduced to progressively refine the localization prediction. In addition, semantic image representation is also exploited to compensate for the usage of patch proposals. Despite being lightweight and simple, our strategy leads to superior improvements on the KITTI and Waymo Open Dataset monocular 3D detection benchmarks. At the same time, our proposed PCT shows great generalization to most coordinate-based 3D detection frameworks. The code is available at: https://github.com/amazon-research/progressive-coordinate-transforms .",0
"The ability to recognize and locate objects in a 3D space is crucial for an AI agent to understand its surroundings. While expensive LiDAR point clouds have made significant progress, detecting 3D objects with only a monocular image is a challenge. Alternative approaches either use heavy networks to fuse RGB and depth information or struggle to process millions of pseudo-LiDAR points. These limitations are due to inaccurate object localization. In this paper, we propose a lightweight approach called ""Progressive Coordinate Transforms"" (PCT) that facilitates learning coordinate representations. We introduce a localization boosting mechanism with confidence-aware loss to refine the localization prediction progressively. Additionally, we use semantic image representation to compensate for patch proposals. Despite being lightweight and straightforward, our strategy leads to superior improvements on the KITTI and Waymo Open Dataset monocular 3D detection benchmarks. Our proposed PCT also shows great generalization to most coordinate-based 3D detection frameworks. The code is available at https://github.com/amazon-research/progressive-coordinate-transforms.",1
"We present Mobile-Former, a parallel design of MobileNet and Transformer with a two-way bridge in between. This structure leverages the advantage of MobileNet at local processing and transformer at global interaction. And the bridge enables bidirectional fusion of local and global features. Different with recent works on vision transformer, the transformer in Mobile-Former contains very few tokens (e.g. less than 6 tokens) that are randomly initialized, resulting in low computational cost. Combining with the proposed light-weight cross attention to model the bridge, Mobile-Former is not only computationally efficient, but also has more representation power, outperforming MobileNetV3 at low FLOP regime from 25M to 500M FLOPs on ImageNet classification. For instance, it achieves 77.9\% top-1 accuracy at 294M FLOPs, gaining 1.3\% over MobileNetV3 but saving 17\% of computations. When transferring to object detection, Mobile-Former outperforms MobileNetV3 by 8.6 AP.",0
"The Mobile-Former is a novel design that combines MobileNet and Transformer, using a two-way bridge to take advantage of the benefits of each. By leveraging MobileNet's local processing and Transformer's global interaction, the bridge allows for bidirectional fusion of local and global features. Unlike recent works on vision transformers, Mobile-Former's Transformer only has a few randomly initialized tokens (less than 6), resulting in lower computational costs. Additionally, Mobile-Former's proposed light-weight cross attention further improves computational efficiency while increasing representation power. Results show that Mobile-Former outperforms MobileNetV3 in low FLOP regimes on ImageNet classification, achieving 77.9% top-1 accuracy at 294M FLOPs and gaining 1.3% while saving 17% of computations. Furthermore, Mobile-Former surpasses MobileNetV3 by 8.6 AP in object detection.",1
"Video objection detection is a challenging task because isolated video frames may encounter appearance deterioration, which introduces great confusion for detection. One of the popular solutions is to exploit the temporal information and enhance per-frame representation through aggregating features from neighboring frames. Despite achieving improvements in detection, existing methods focus on the selection of higher-level video frames for aggregation rather than modeling lower-level temporal relations to increase the feature representation. To address this limitation, we propose a novel solution named TF-Blender,which includes three modules: 1) Temporal relation mod-els the relations between the current frame and its neighboring frames to preserve spatial information. 2). Feature adjustment enriches the representation of every neigh-boring feature map; 3) Feature blender combines outputs from the first two modules and produces stronger features for the later detection tasks. For its simplicity, TF-Blender can be effortlessly plugged into any detection network to improve detection behavior. Extensive evaluations on ImageNet VID and YouTube-VIS benchmarks indicate the performance guarantees of using TF-Blender on recent state-of-the-art methods.",0
"Detecting objections in video is a difficult task due to the potential for appearance deterioration in isolated video frames, which can cause confusion during detection. Many current methods utilize temporal information to enhance per-frame representation by aggregating features from neighboring frames, but they primarily focus on selecting higher-level video frames for aggregation rather than modeling lower-level temporal relations for improved feature representation. To overcome this limitation, we propose a new solution called TF-Blender, which includes three modules: 1) Temporal relation models to preserve spatial information by capturing relations between the current frame and its neighboring frames, 2) Feature adjustment to enrich the representation of every neighboring feature map, and 3) Feature blender to combine outputs from the first two modules and produce stronger features for detection tasks. TF-Blender is simple and can be easily integrated into any detection network for improved detection performance. Extensive evaluations on ImageNet VID and YouTube-VIS benchmarks demonstrate the effectiveness of TF-Blender with recent state-of-the-art methods.",1
"Current state-of-the-art two-stage detectors generate oriented proposals through time-consuming schemes. This diminishes the detectors' speed, thereby becoming the computational bottleneck in advanced oriented object detection systems. This work proposes an effective and simple oriented object detection framework, termed Oriented R-CNN, which is a general two-stage oriented detector with promising accuracy and efficiency. To be specific, in the first stage, we propose an oriented Region Proposal Network (oriented RPN) that directly generates high-quality oriented proposals in a nearly cost-free manner. The second stage is oriented R-CNN head for refining oriented Regions of Interest (oriented RoIs) and recognizing them. Without tricks, oriented R-CNN with ResNet50 achieves state-of-the-art detection accuracy on two commonly-used datasets for oriented object detection including DOTA (75.87% mAP) and HRSC2016 (96.50% mAP), while having a speed of 15.1 FPS with the image size of 1024$\times$1024 on a single RTX 2080Ti. We hope our work could inspire rethinking the design of oriented detectors and serve as a baseline for oriented object detection. Code is available at https://github.com/jbwang1997/OBBDetection.",0
"Advanced oriented object detection systems face computational bottlenecks due to the time-consuming schemes used by current state-of-the-art two-stage detectors to generate oriented proposals. To address this issue, this study proposes Oriented R-CNN, a general two-stage oriented detector that offers promising accuracy and efficiency. The framework includes an oriented Region Proposal Network (oriented RPN) that generates high-quality oriented proposals at a low cost in the first stage and an oriented R-CNN head for refining oriented Regions of Interest (oriented RoIs) and recognizing them in the second stage. Oriented R-CNN achieves state-of-the-art detection accuracy on two commonly-used datasets for oriented object detection without tricks, using ResNet50. With an image size of 1024$\times$1024 on a single RTX 2080Ti, it achieves a speed of 15.1 FPS. The authors hope that their work will inspire the design of oriented detectors and serve as a baseline for oriented object detection. The code is available at https://github.com/jbwang1997/OBBDetection.",1
"To accommodate rapid changes in the real world, the cognition system of humans is capable of continually learning concepts. On the contrary, conventional deep learning models lack this capability of preserving previously learned knowledge. When a neural network is fine-tuned to learn new tasks, its performance on previously trained tasks will significantly deteriorate. Many recent works on incremental object detection tackle this problem by introducing advanced regularization. Although these methods have shown promising results, the benefits are often short-lived after the first incremental step. Under multi-step incremental learning, the trade-off between old knowledge preserving and new task learning becomes progressively more severe. Thus, the performance of regularization-based incremental object detectors gradually decays for subsequent learning steps. In this paper, we aim to alleviate this performance decay on multi-step incremental detection tasks by proposing a dilatable incremental object detector (DIODE). For the task-shared parameters, our method adaptively penalizes the changes of important weights for previous tasks. At the same time, the structure of the model is dilated or expanded by a limited number of task-specific parameters to promote new task learning. Extensive experiments on PASCAL VOC and COCO datasets demonstrate substantial improvements over the state-of-the-art methods. Notably, compared with the state-of-the-art methods, our method achieves up to 6.0% performance improvement by increasing the number of parameters by just 1.2% for each newly learned task.",0
"Humans have the ability to continually learn concepts in order to adapt to the fast-changing world around us. However, conventional deep learning models lack this capability, causing a significant deterioration in performance when fine-tuned for new tasks. While some recent approaches have introduced advanced regularization to address this issue, the benefits are often short-lived and the trade-off between preserving old knowledge and learning new tasks becomes progressively more severe with multi-step incremental learning. To address this, we propose a dilatable incremental object detector (DIODE) that adaptively penalizes changes to important parameters for previous tasks while promoting new task learning with a limited number of task-specific parameters. Our experiments on PASCAL VOC and COCO datasets demonstrate significant improvements over current methods, achieving up to 6.0% performance improvement with only a 1.2% increase in parameters for each newly learned task.",1
"With autonomous driving developing in a booming stage, accurate object detection in complex scenarios attract wide attention to ensure the safety of autonomous driving. Millimeter wave (mmWave) radar and vision fusion is a mainstream solution for accurate obstacle detection. This article presents a detailed survey on mmWave radar and vision fusion based obstacle detection methods. Firstly, we introduce the tasks, evaluation criteria and datasets of object detection for autonomous driving. Then, the process of mmWave radar and vision fusion is divided into three parts: sensor deployment, sensor calibration and sensor fusion, which are reviewed comprehensively. Especially, we classify the fusion methods into data level, decision level and feature level fusion methods. Besides, we introduce the fusion of lidar and vision in autonomous driving in the aspects of obstacle detection, object classification and road segmentation, which is promising in the future. Finally, we summarize this article.",0
"As autonomous driving continues to rapidly develop, there is a growing focus on achieving accurate object detection in complex scenarios to ensure safety. One mainstream solution for achieving this is through the fusion of millimeter wave (mmWave) radar and vision. In this article, we provide a detailed survey of obstacle detection methods based on mmWave radar and vision fusion. Firstly, we discuss the tasks, evaluation criteria, and datasets involved in object detection for autonomous driving. We then comprehensively review the three main parts of the mmWave radar and vision fusion process: sensor deployment, sensor calibration, and sensor fusion. We also categorize fusion methods into data level, decision level, and feature level fusion methods. Additionally, we explore the potential of lidar and vision fusion for obstacle detection, object classification, and road segmentation in autonomous driving. Finally, we summarize the key points of this article.",1
"In recent years, computer vision algorithms have become more and more powerful, which enabled technologies such as autonomous driving to evolve with rapid pace. However, current algorithms mainly share one limitation: They rely on directly visible objects. This is a major drawback compared to human behavior, where indirect visual cues caused by the actual object (e.g., shadows) are already used intuitively to retrieve information or anticipate occurring objects. While driving at night, this performance deficit becomes even more obvious: Humans already process the light artifacts caused by oncoming vehicles to assume their future appearance, whereas current object detection systems rely on the oncoming vehicle's direct visibility. Based on previous work in this subject, we present with this paper a complete system capable of solving the task to providently detect oncoming vehicles at nighttime based on their caused light artifacts. For that, we outline the full algorithm architecture ranging from the detection of light artifacts in the image space, localizing the objects in the three-dimensional space, and verifying the objects over time. To demonstrate the applicability, we deploy the system in a test vehicle and use the information of providently detected vehicles to control the glare-free high beam system proactively. Using this experimental setting, we quantify the time benefit that the provident vehicle detection system provides compared to an in-production computer vision system. Additionally, the glare-free high beam use case provides a real-time and real-world visualization interface of the detection results. With this contribution, we want to put awareness on the unconventional sensing task of provident object detection and further close the performance gap between human behavior and computer vision algorithms in order to bring autonomous and automated driving a step forward.",0
"Computer vision algorithms have greatly advanced in recent years, allowing for the rapid evolution of technologies such as autonomous driving. However, current algorithms are limited in that they rely solely on directly visible objects, which is unlike the human ability to use indirect visual cues to anticipate objects. This is particularly evident when driving at night, where humans use light artifacts caused by oncoming vehicles to predict their future appearance, while object detection systems rely only on direct visibility. To address this issue, we present a complete system capable of detecting oncoming vehicles at night based on their caused light artifacts. Our algorithm architecture includes detecting light artifacts, localizing objects in 3D space, and verifying objects over time. We demonstrate the system's applicability by using it to control a glare-free high beam system in a test vehicle, providing real-time detection results. Our contribution aims to bridge the performance gap between human and computer vision and advance automated driving technology.",1
"Although using convolutional neural networks (CNNs) as backbones achieves great successes in computer vision, this work investigates a simple backbone network useful for many dense prediction tasks without convolutions. Unlike the recently-proposed Transformer model (e.g., ViT) that is specially designed for image classification, we propose Pyramid Vision Transformer~(PVT), which overcomes the difficulties of porting Transformer to various dense prediction tasks. PVT has several merits compared to prior arts. (1) Different from ViT that typically has low-resolution outputs and high computational and memory cost, PVT can be not only trained on dense partitions of the image to achieve high output resolution, which is important for dense predictions but also using a progressive shrinking pyramid to reduce computations of large feature maps. (2) PVT inherits the advantages from both CNN and Transformer, making it a unified backbone in various vision tasks without convolutions by simply replacing CNN backbones. (3) We validate PVT by conducting extensive experiments, showing that it boosts the performance of many downstream tasks, e.g., object detection, semantic, and instance segmentation. For example, with a comparable number of parameters, RetinaNet+PVT achieves 40.4 AP on the COCO dataset, surpassing RetinNet+ResNet50 (36.3 AP) by 4.1 absolute AP. We hope PVT could serve as an alternative and useful backbone for pixel-level predictions and facilitate future researches. Code is available at https://github.com/whai362/PVT.",0
"While convolutional neural networks (CNNs) have been successful in computer vision, this study explores a simpler backbone network that is effective for many dense prediction tasks without using convolutions. Unlike the Transformer model designed for image classification, the authors propose the Pyramid Vision Transformer (PVT) to address the challenges of adapting Transformer to various dense prediction tasks. PVT has several advantages over previous methods: it can be trained on dense partitions of the image to achieve high output resolution, it reduces computations of large feature maps using a progressive shrinking pyramid, and it serves as a unified backbone in various vision tasks without convolutions by replacing CNN backbones. The authors demonstrate the effectiveness of PVT through extensive experiments, showing that it improves the performance of multiple downstream tasks such as object detection, semantic, and instance segmentation. RetinaNet+PVT achieves 40.4 AP on the COCO dataset, surpassing RetinNet+ResNet50 (36.3 AP) by 4.1 absolute AP. The authors hope that PVT will be a useful backbone for pixel-level predictions and facilitate future research. The code is available at https://github.com/whai362/PVT.",1
"Domain adaptation aims to bridge the domain shifts between the source and target domains. These shifts may span different dimensions such as fog, rainfall, etc. However, recent methods typically do not consider explicit prior knowledge on a specific dimension, thus leading to less desired adaptation performance. In this paper, we study a practical setting called Specific Domain Adaptation (SDA) that aligns the source and target domains in a demanded-specific dimension. Within this setting, we observe the intra-domain gap induced by different domainness (i.e., numerical magnitudes of this dimension) is crucial when adapting to a specific domain. To address the problem, we propose a novel Self-Adversarial Disentangling (SAD) framework. In particular, given a specific dimension, we first enrich the source domain by introducing a domainness creator with providing additional supervisory signals. Guided by the created domainness, we design a self-adversarial regularizer and two loss functions to jointly disentangle the latent representations into domainness-specific and domainness-invariant features, thus mitigating the intra-domain gap. Our method can be easily taken as a plug-and-play framework and does not introduce any extra costs in the inference time. We achieve consistent improvements over state-of-the-art methods in both object detection and semantic segmentation tasks.",0
"The objective of domain adaptation is to minimize the differences between the source and target domains, which can vary across various dimensions such as fog or rainfall. However, current techniques typically do not incorporate prior knowledge on a particular dimension, which results in suboptimal adaptation outcomes. This article examines a practical scenario known as Specific Domain Adaptation (SDA), which aims to align the source and target domains in a specific dimension. Within this context, we discovered that the intra-domain gap caused by domainness differences is critical when adapting to a specific domain. To address this issue, we propose a novel approach called Self-Adversarial Disentangling (SAD) framework. We first enrich the source domain by introducing a domainness creator, which provides additional supervisory signals, and then utilize a self-adversarial regularizer and two loss functions to disentangle the latent representations into domainness-specific and domainness-invariant features, thereby reducing the intra-domain gap. Our approach is plug-and-play and does not incur any additional expenses during inference. We achieve consistent improvements over state-of-the-art techniques in both object detection and semantic segmentation tasks.",1
"In current object detection, algorithms require the object to be directly visible in order to be detected. As humans, however, we intuitively use visual cues caused by the respective object to already make assumptions about its appearance. In the context of driving, such cues can be shadows during the day and often light reflections at night. In this paper, we study the problem of how to map this intuitive human behavior to computer vision algorithms to detect oncoming vehicles at night just from the light reflections they cause by their headlights. For that, we present an extensive open-source dataset containing 59746 annotated grayscale images out of 346 different scenes in a rural environment at night. In these images, all oncoming vehicles, their corresponding light objects (e.g., headlamps), and their respective light reflections (e.g., light reflections on guardrails) are labeled. In this context, we discuss the characteristics of the dataset and the challenges in objectively describing visual cues such as light reflections. We provide different metrics for different ways to approach the task and report the results we achieved using state-of-the-art and custom object detection models as a first benchmark. With that, we want to bring attention to a new and so far neglected field in computer vision research, encourage more researchers to tackle the problem, and thereby further close the gap between human performance and computer vision systems.",0
"Algorithms in current object detection require objects to be visible directly to detect them. However, humans use visual cues caused by the respective object to make assumptions about its appearance. These cues include shadows during the day and light reflections at night. This paper explores how to map human intuition to computer vision algorithms to detect oncoming vehicles at night using only the light reflections from their headlights. The paper presents an extensive open-source dataset containing 59746 annotated grayscale images of 346 different scenes in a rural environment at night, where all oncoming vehicles, their corresponding light objects, and their respective light reflections are labeled. The paper discusses the dataset's characteristics and the challenges in describing visual cues objectively, providing different metrics for various approaches. The paper then reports the results achieved using state-of-the-art and custom object detection models as a first benchmark. The aim is to draw attention to a new and neglected field in computer vision research, encourage more researchers to tackle the problem, and narrow the gap between human and computer vision system performance.",1
"Two-stage methods have dominated Human-Object Interaction (HOI) detection for several years. Recently, one-stage HOI detection methods have become popular. In this paper, we aim to explore the essential pros and cons of two-stage and one-stage methods. With this as the goal, we find that conventional two-stage methods mainly suffer from positioning positive interactive human-object pairs, while one-stage methods are challenging to make an appropriate trade-off on multi-task learning, i.e., object detection, and interaction classification. Therefore, a core problem is how to take the essence and discard the dregs from the conventional two types of methods. To this end, we propose a novel one-stage framework with disentangling human-object detection and interaction classification in a cascade manner. In detail, we first design a human-object pair generator based on a state-of-the-art one-stage HOI detector by removing the interaction classification module or head and then design a relatively isolated interaction classifier to classify each human-object pair. Two cascade decoders in our proposed framework can focus on one specific task, detection or interaction classification. In terms of the specific implementation, we adopt a transformer-based HOI detector as our base model. The newly introduced disentangling paradigm outperforms existing methods by a large margin, with a significant relative mAP gain of 9.32% on HICO-Det.",0
"For years, two-stage methods have been dominant in Human-Object Interaction (HOI) detection, but recently, one-stage HOI detection methods have gained popularity. This paper aims to examine the advantages and disadvantages of both methods and how to improve them. The issue with two-stage methods is identifying positive interactive human-object pairs, while one-stage methods struggle with multi-task learning, specifically object detection and interaction classification. The challenge is to combine the best aspects of each method. The proposed solution is a novel one-stage framework that separates human-object detection and interaction classification in a cascade manner. The framework uses a human-object pair generator and a relatively isolated interaction classifier. The framework's cascade decoders focus on detection or interaction classification tasks. The base model is a transformer-based HOI detector. This disentangling approach outperforms existing methods by 9.32% on HICO-Det.",1
"In this work, we address the problem of 3D object detection from point cloud data in real time. For autonomous vehicles to work, it is very important for the perception component to detect the real world objects with both high accuracy and fast inference. We propose a novel neural network architecture along with the training and optimization details for detecting 3D objects using point cloud data. We present anchor design along with custom loss functions used in this work. A combination of spatial and channel wise attention module is used in this work. We use the Kitti 3D Birds Eye View dataset for benchmarking and validating our results. Our method surpasses previous state of the art in this domain both in terms of average precision and speed running at > 30 FPS. Finally, we present the ablation study to demonstrate that the performance of our network is generalizable. This makes it a feasible option to be deployed in real time applications like self driving cars.",0
"The focus of our work is to tackle the challenge of detecting 3D objects from point cloud data in real time. It is crucial for self-driving vehicles to have a perception component that can accurately and quickly identify objects in the real world. To achieve this, we introduce a new neural network architecture and provide details on the training and optimization process to detect 3D objects using point cloud data. Our approach involves designing anchors and custom loss functions, as well as incorporating a combination of spatial and channel wise attention modules. We evaluate our method on the Kitti 3D Birds Eye View dataset and demonstrate that it outperforms previous state-of-the-art approaches in terms of both average precision and speed, running at > 30 FPS. Additionally, we conduct an ablation study to show that our network's performance is generalizable, making it a viable option for real-time applications such as self-driving cars.",1
"Attention mechanism of late has been quite popular in the computer vision community. A lot of work has been done to improve the performance of the network, although almost always it results in increased computational complexity. In this paper, we propose a new attention module that not only achieves the best performance but also has lesser parameters compared to most existing models. Our attention module can easily be integrated with other convolutional neural networks because of its lightweight nature. The proposed network named Dual Multi Scale Attention Network (DMSANet) is comprised of two parts: the first part is used to extract features at various scales and aggregate them, the second part uses spatial and channel attention modules in parallel to adaptively integrate local features with their global dependencies. We benchmark our network performance for Image Classification on ImageNet dataset, Object Detection and Instance Segmentation both on MS COCO dataset.",0
"The use of attention mechanism has gained popularity in the computer vision community. Researchers have made significant efforts to enhance network performance, however, this often results in increased computational complexity. This paper introduces a new attention module that not only achieves optimal performance but also has fewer parameters compared to existing models. Due to its lightweight nature, our attention module can be easily integrated with other convolutional neural networks. Our proposed network, Dual Multi Scale Attention Network (DMSANet), consists of two parts: the first part extracts features at different scales and aggregates them, while the second part uses spatial and channel attention modules to adaptively integrate local features with their global dependencies. We evaluate the performance of our network for Image Classification on ImageNet dataset, Object Detection, and Instance Segmentation on MS COCO dataset.",1
"Multi-object tracking (MOT) with camera-LiDAR fusion demands accurate results of object detection, affinity computation and data association in real time. This paper presents an efficient multi-modal MOT framework with online joint detection and tracking schemes and robust data association for autonomous driving applications. The novelty of this work includes: (1) development of an end-to-end deep neural network for joint object detection and correlation using 2D and 3D measurements; (2) development of a robust affinity computation module to compute occlusion-aware appearance and motion affinities in 3D space; (3) development of a comprehensive data association module for joint optimization among detection confidences, affinities and start-end probabilities. The experiment results on the KITTI tracking benchmark demonstrate the superior performance of the proposed method in terms of both tracking accuracy and processing speed.",0
"Accurate results of object detection, affinity computation, and data association in real-time are essential for Multi-Object Tracking (MOT) with camera-LiDAR fusion. This paper introduces a multi-modal MOT framework with efficient joint detection and tracking schemes and robust data association for autonomous driving applications. The work's novelty includes the development of an end-to-end deep neural network for joint object detection and correlation using 2D and 3D measurements, a robust affinity computation module to calculate occlusion-aware appearance and motion affinities in 3D space, and a comprehensive data association module for joint optimization among detection confidences, affinities, and start-end probabilities. The experiment results on the KITTI tracking benchmark demonstrate the proposed method's superior performance in terms of tracking accuracy and processing speed.",1
"Scene understanding is crucial for autonomous systems which intend to operate in the real world. Single task vision networks extract information only based on some aspects of the scene. In multi-task learning (MTL), on the other hand, these single tasks are jointly learned, thereby providing an opportunity for tasks to share information and obtain a more comprehensive understanding. To this end, we develop UniNet, a unified scene understanding network that accurately and efficiently infers vital vision tasks including object detection, semantic segmentation, instance segmentation, monocular depth estimation, and monocular instance depth prediction. As these tasks look at different semantic and geometric information, they can either complement or conflict with each other. Therefore, understanding inter-task relationships can provide useful cues to enable complementary information sharing. We evaluate the task relationships in UniNet through the lens of adversarial attacks based on the notion that they can exploit learned biases and task interactions in the neural network. Extensive experiments on the Cityscapes dataset, using untargeted and targeted attacks reveal that semantic tasks strongly interact amongst themselves, and the same holds for geometric tasks. Additionally, we show that the relationship between semantic and geometric tasks is asymmetric and their interaction becomes weaker as we move towards higher-level representations.",0
"Having a comprehensive understanding of a scene is critical for autonomous systems that operate in the real world. Traditional single task vision networks only extract information based on some aspects of the scene. In contrast, multi-task learning (MTL) involves learning several tasks together, enabling them to share information and gain a better understanding of the scene. To address this, we introduce UniNet, a unified scene understanding network that accurately and efficiently performs essential vision tasks such as object detection, semantic segmentation, instance segmentation, monocular depth estimation, and monocular instance depth prediction. Since these tasks look at different semantic and geometric information, understanding their inter-task relationships is critical to enable information sharing. We evaluate the task relationships in UniNet by exploring the impact of adversarial attacks that exploit learned biases and task interactions in the neural network. Our experiments on the Cityscapes dataset reveal that semantic tasks strongly interact with each other, as do geometric tasks. Furthermore, we show that the relationship between semantic and geometric tasks is asymmetric, and their interaction becomes weaker as we move towards higher-level representations.",1
"The defect detection task can be regarded as a realistic scenario of object detection in the computer vision field and it is widely used in the industrial field. Directly applying vanilla object detector to defect detection task can achieve promising results, while there still exists challenging issues that have not been solved. The first issue is the texture shift which means a trained defect detector model will be easily affected by unseen texture, and the second issue is partial visual confusion which indicates that a partial defect box is visually similar with a complete box. To tackle these two problems, we propose a Reference-based Defect Detection Network (RDDN). Specifically, we introduce template reference and context reference to against those two problems, respectively. Template reference can reduce the texture shift from image, feature or region levels, and encourage the detectors to focus more on the defective area as a result. We can use either well-aligned template images or the outputs of a pseudo template generator as template references in this work, and they are jointly trained with detectors by the supervision of normal samples. To solve the partial visual confusion issue, we propose to leverage the carried context information of context reference, which is the concentric bigger box of each region proposal, to perform more accurate region classification and regression. Experiments on two defect detection datasets demonstrate the effectiveness of our proposed approach.",0
"The task of detecting defects is a practical application of object detection in computer vision and is commonly used in industry. Although conventional object detectors can produce satisfactory results, there are still two significant issues that need to be addressed. Firstly, trained defect detectors are prone to texture shift, where unseen textures can affect their performance. Secondly, partial visual confusion can occur when a partial defect box appears similar to a complete box. To overcome these obstacles, we propose the Reference-based Defect Detection Network (RDDN), which utilizes template reference and context reference to address texture shift and partial visual confusion, respectively. Template reference reduces texture shift and encourages the detector to focus on defective areas using well-aligned template images or pseudo template generators. Meanwhile, context reference leverages the carried context information of the concentric bigger box of each region proposal to improve region classification and regression. Our experiments on two defect detection datasets demonstrate the effectiveness of our proposed approach.",1
"Accurate detection of obstacles in 3D is an essential task for autonomous driving and intelligent transportation. In this work, we propose a general multimodal fusion framework FusionPainting to fuse the 2D RGB image and 3D point clouds at a semantic level for boosting the 3D object detection task. Especially, the FusionPainting framework consists of three main modules: a multi-modal semantic segmentation module, an adaptive attention-based semantic fusion module, and a 3D object detector. First, semantic information is obtained for 2D images and 3D Lidar point clouds based on 2D and 3D segmentation approaches. Then the segmentation results from different sensors are adaptively fused based on the proposed attention-based semantic fusion module. Finally, the point clouds painted with the fused semantic label are sent to the 3D detector for obtaining the 3D objection results. The effectiveness of the proposed framework has been verified on the large-scale nuScenes detection benchmark by comparing it with three different baselines. The experimental results show that the fusion strategy can significantly improve the detection performance compared to the methods using only point clouds, and the methods using point clouds only painted with 2D segmentation information. Furthermore, the proposed approach outperforms other state-of-the-art methods on the nuScenes testing benchmark.",0
"The ability to accurately detect obstacles in 3D is crucial for the success of autonomous driving and intelligent transportation. We introduce a novel multimodal fusion framework called FusionPainting, which merges 2D RGB images and 3D point clouds at a semantic level to enhance the 3D object detection process. Our FusionPainting framework comprises three key modules: a multi-modal semantic segmentation module, an adaptive attention-based semantic fusion module, and a 3D object detector. Initially, we obtain semantic information from 2D images and 3D Lidar point clouds using respective segmentation techniques. Subsequently, we merge the segmentation outcomes from different sensors using the attention-based semantic fusion module. Finally, we send the point clouds, painted with the fused semantic label, to the 3D detector to obtain the 3D object detection results. We evaluated the effectiveness of our proposed framework on the large-scale nuScenes detection benchmark by comparing it with three different baselines. Our experimental results reveal that the fusion strategy significantly improves the detection performance compared to methods that solely rely on point clouds or point clouds painted with 2D segmentation information. Moreover, our approach outperforms other state-of-the-art methods on the nuScenes testing benchmark.",1
"In the past few years, mobile deep-learning deployment progressed by leaps and bounds, but solutions still struggle to accommodate its severe and fluctuating operational restrictions, which include bandwidth, latency, computation, and energy. In this work, we help to bridge that gap, introducing the first configurable solution for object detection that manages the triple communication-computation-accuracy trade-off with a single set of weights. Our solution shows state-of-the-art results on COCO-2017, adding only a minor penalty on the base EfficientDet-D2 architecture. Our design is robust to the choice of base architecture and compressor and should adapt well for future architectures.",0
"Mobile deep-learning deployment has made significant progress in recent years. However, there are still challenges in accommodating its operational restrictions, which include bandwidth, latency, computation, and energy. To address this gap, we have introduced a configurable solution for object detection that manages the trade-off between communication, computation, and accuracy with a single set of weights. Our solution has achieved state-of-the-art results on COCO-2017, with only a minor penalty on the base EfficientDet-D2 architecture. Our design is also adaptable to future architectures and compressors, making it a robust choice for object detection.",1
"With the continuous improvement of the performance of object detectors via advanced model architectures, imbalance problems in the training process have received more attention. It is a common paradigm in object detection frameworks to perform multi-scale detection. However, each scale is treated equally during training. In this paper, we carefully study the objective imbalance of multi-scale detector training. We argue that the loss in each scale level is neither equally important nor independent. Different from the existing solutions of setting multi-task weights, we dynamically optimize the loss weight of each scale level in the training process. Specifically, we propose an Adaptive Variance Weighting (AVW) to balance multi-scale loss according to the statistical variance. Then we develop a novel Reinforcement Learning Optimization (RLO) to decide the weighting scheme probabilistically during training. The proposed dynamic methods make better utilization of multi-scale training loss without extra computational complexity and learnable parameters for backpropagation. Experiments show that our approaches can consistently boost the performance over various baseline detectors on Pascal VOC and MS COCO benchmark.",0
"The issue of imbalance in the training process for object detectors has gained more attention due to the continuous improvement of advanced model architectures. Object detection frameworks typically use multi-scale detection, but each scale is equally treated during training. This paper examines the objective imbalance of multi-scale detector training and argues that the loss in each scale level is not equally important or independent. Instead of using existing solutions, this paper proposes the Adaptive Variance Weighting (AVW) method to balance multi-scale loss based on statistical variance. Additionally, a Reinforcement Learning Optimization (RLO) method is developed to probabilistically decide the weighting scheme during training. These dynamic methods improve the utilization of multi-scale training loss without additional computational complexity or learnable parameters for backpropagation. Experimental results demonstrate consistent performance improvements over various baseline detectors on Pascal VOC and MS COCO benchmark.",1
"Salient object detection is the pixel-level dense prediction task which can highlight the prominent object in the scene. Recently U-Net framework is widely used, and continuous convolution and pooling operations generate multi-level features which are complementary with each other. In view of the more contribution of high-level features for the performance, we propose a triplet transformer embedding module to enhance them by learning long-range dependencies across layers. It is the first to use three transformer encoders with shared weights to enhance multi-level features. By further designing scale adjustment module to process the input, devising three-stream decoder to process the output and attaching depth features to color features for the multi-modal fusion, the proposed triplet transformer embedding network (TriTransNet) achieves the state-of-the-art performance in RGB-D salient object detection, and pushes the performance to a new level. Experimental results demonstrate the effectiveness of the proposed modules and the competition of TriTransNet.",0
"Salient object detection is a task that identifies the most prominent object in a scene at the pixel level. The U-Net framework, which uses continuous convolution and pooling operations to generate multi-level features that complement each other, has become popular. However, because high-level features contribute more to performance, we have developed a triplet transformer embedding module that learns long-range dependencies across layers to enhance these features. The module is unique in its use of three transformer encoders with shared weights to improve multi-level features. To further enhance performance, we designed a scale adjustment module to process input, a three-stream decoder to process output, and attached depth features to color features for multi-modal fusion. Our proposed network, TriTransNet, achieves state-of-the-art performance in RGB-D salient object detection, taking performance to new heights. Experimental results demonstrate the effectiveness of our proposed modules and TriTransNet's competitiveness.",1
"We study the problem of object detection from a novel perspective in which annotation budget constraints are taken into consideration, appropriately coined Budget Aware Object Detection (BAOD). When provided with a fixed budget, we propose a strategy for building a diverse and informative dataset that can be used to optimally train a robust detector. We investigate both optimization and learning-based methods to sample which images to annotate and what type of annotation (strongly or weakly supervised) to annotate them with. We adopt a hybrid supervised learning framework to train the object detector from both these types of annotation. We conduct a comprehensive empirical study showing that a handcrafted optimization method outperforms other selection techniques including random sampling, uncertainty sampling and active learning. By combining an optimal image/annotation selection scheme with hybrid supervised learning to solve the BAOD problem, we show that one can achieve the performance of a strongly supervised detector on PASCAL-VOC 2007 while saving 12.8% of its original annotation budget. Furthermore, when $100\%$ of the budget is used, it surpasses this performance by 2.0 mAP percentage points.",0
"Our research explores a unique approach to object detection called Budget Aware Object Detection (BAOD), which considers limitations on annotation resources. To effectively train a robust detector with a fixed budget, we propose a strategy for selecting a diverse and informative dataset using optimization and learning-based techniques. By combining both strongly and weakly supervised annotations in a hybrid framework, we demonstrate that our handcrafted optimization method outperforms other selection techniques, such as random sampling, uncertainty sampling, and active learning. Our results show that the hybrid supervised learning approach to solving the BAOD problem achieves the same level of performance as a strongly supervised detector on PASCAL-VOC 2007 with a 12.8% reduction in annotation budget. Additionally, when using the full budget, our approach outperforms the strongly supervised detector by 2.0 mAP percentage points.",1
"Salient object detection (SOD) is viewed as a pixel-wise saliency modeling task by traditional deep learning-based methods. A limitation of current SOD models is insufficient utilization of inter-pixel information, which usually results in imperfect segmentation near edge regions and low spatial coherence. As we demonstrate, using a saliency mask as the only label is suboptimal. To address this limitation, we propose a connectivity-based approach called bilateral connectivity network (BiconNet), which uses connectivity masks together with saliency masks as labels for effective modeling of inter-pixel relationships and object saliency. Moreover, we propose a bilateral voting module to enhance the output connectivity map, and a novel edge feature enhancement method that efficiently utilizes edge-specific features. Through comprehensive experiments on five benchmark datasets, we demonstrate that our proposed method can be plugged into any existing state-of-the-art saliency-based SOD framework to improve its performance with negligible parameter increase.",0
"Traditional deep learning-based methods consider Salient object detection (SOD) as a pixel-wise saliency modeling task. However, current SOD models have a limitation in not utilizing inter-pixel information adequately, resulting in imperfect segmentation near edge regions and low spatial coherence. We have found that using a saliency mask as the only label is not optimal. To overcome this limitation, we introduce a bilateral connectivity network (BiconNet) that uses connectivity masks along with saliency masks as labels to effectively model inter-pixel relationships and object saliency. In addition, we propose a bilateral voting module to enhance the output connectivity map and a novel edge feature enhancement technique that efficiently utilizes edge-specific features. We have conducted comprehensive experiments on five benchmark datasets, and our proposed method can improve the performance of any existing state-of-the-art saliency-based SOD framework with negligible parameter increase.",1
"In this paper, we present an Intersection-over-Union (IoU) guided two-stage 3D object detector with a voxel-to-point decoder. To preserve the necessary information from all raw points and maintain the high box recall in voxel based Region Proposal Network (RPN), we propose a residual voxel-to-point decoder to extract the point features in addition to the map-view features from the voxel based RPN. We use a 3D Region of Interest (RoI) alignment to crop and align the features with the proposal boxes for accurately perceiving the object position. The RoI-Aligned features are finally aggregated with the corner geometry embeddings that can provide the potentially missing corner information in the box refinement stage. We propose a simple and efficient method to align the estimated IoUs to the refined proposal boxes as a more relevant localization confidence. The comprehensive experiments on KITTI and Waymo Open Dataset demonstrate that our method achieves significant improvements with novel architectures against the existing methods. The code is available on Github URL\footnote{\url{https://github.com/jialeli1/From-Voxel-to-Point}}.",0
"This paper introduces a two-stage 3D object detector with an Intersection-over-Union (IoU) guidance and voxel-to-point decoder. To maintain high box recall in the voxel based Region Proposal Network (RPN) while preserving information from all raw points, we propose a residual voxel-to-point decoder that extracts both point and map-view features. We also use a 3D Region of Interest (RoI) alignment to align features with proposal boxes for accurate object positioning. The RoI-Aligned features are combined with corner geometry embeddings to provide potentially missing corner information. We also propose a method to align estimated IoUs with refined proposal boxes for improved localization confidence. Comprehensive experiments on KITTI and Waymo Open Dataset demonstrate the superiority of our method over existing methods, and the code is available on Github.",1
"Most of the existing single-stage and two-stage 3D object detectors are anchor-based methods, while the efficient but challenging anchor-free single-stage 3D object detection is not well investigated. Recent studies on 2D object detection show that the anchor-free methods also are of great potential. However, the unordered and sparse properties of point clouds prevent us from directly leveraging the advanced 2D methods on 3D point clouds. We overcome this by converting the voxel-based sparse 3D feature volumes into the sparse 2D feature maps. We propose an attentive module to fit the sparse feature maps to dense mostly on the object regions through the deformable convolution tower and the supervised mask-guided attention. By directly regressing the 3D bounding box from the enhanced and dense feature maps, we construct a novel single-stage 3D detector for point clouds in an anchor-free manner. We propose an IoU-based detection confidence re-calibration scheme to improve the correlation between the detection confidence score and the accuracy of the bounding box regression. Our code is publicly available at \url{https://github.com/jialeli1/MGAF-3DSSD}.",0
"The majority of current 3D object detectors are anchor-based, with few investigating the anchor-free single-stage approach which is efficient but challenging. While anchor-free methods have proven promising in 2D object detection, the unordered and sparse nature of point clouds has made applying these methods to 3D point clouds difficult. To overcome this, we have developed an attentive module that converts voxel-based sparse 3D feature volumes into sparse 2D feature maps, which are then fit to dense feature maps using a deformable convolution tower and supervised mask-guided attention. Our novel single-stage 3D detector regresses the 3D bounding box directly from the enhanced and dense feature maps, without the use of anchors. We have also introduced an IoU-based detection confidence re-calibration scheme to improve the correlation between the detection confidence score and the accuracy of the bounding box regression. Our code is available publicly on GitHub at \url{https://github.com/jialeli1/MGAF-3DSSD}.",1
"The semantic representation of deep features is essential for image context understanding, and effective fusion of features with different semantic representations can significantly improve the model's performance on salient object detection. In this paper, a novel method called MPI is proposed for salient object detection. Firstly, a multi-receptive enhancement module (MRE) is designed to effectively expand the receptive fields of features from different layers and generate features with different receptive fields. MRE can enhance the semantic representation and improve the model's perception of the image context, which enables the model to locate the salient object accurately. Secondly, in order to reduce the reuse of redundant information in the complex top-down fusion method and weaken the differences between semantic features, a relatively simple but effective parallel fusion strategy (PFS) is proposed. It allows multi-scale features to better interact with each other, thus improving the overall performance of the model. Experimental results on multiple datasets demonstrate that the proposed method outperforms state-of-the-art methods under different evaluation metrics.",0
"Understanding the context of images requires a semantic representation of deep features, and combining features with different semantic representations can significantly enhance the accuracy of salient object detection models. This paper presents a new approach called MPI for detecting salient objects. Firstly, a multi-receptive enhancement module (MRE) is created to expand the receptive fields of features from various layers and generate features with different receptive fields. This module improves the semantic representation of the features and enhances the model's perception of the image context, leading to more accurate detection of the salient object. Secondly, to reduce redundant information and minimize the differences between semantic features, a simple but effective parallel fusion strategy (PFS) is proposed. This strategy allows multi-scale features to interact better with each other, resulting in improved model performance. Experimental results on multiple datasets show that the proposed method outperforms existing methods in various evaluation metrics.",1
"With the advancements made in deep learning, computer vision problems like object detection and segmentation have seen a great improvement in performance. However, in many real-world applications such as autonomous driving vehicles, the risk associated with incorrect predictions of objects is very high. Standard deep learning models for object detection such as YOLO models are often overconfident in their predictions and do not take into account the uncertainty in predictions on out-of-distribution data. In this work, we propose an efficient and effective approach to model uncertainty in object detection and segmentation tasks using Monte-Carlo DropBlock (MC-DropBlock) based inference. The proposed approach applies drop-block during training time and test time on the convolutional layer of the deep learning models such as YOLO. We show that this leads to a Bayesian convolutional neural network capable of capturing the epistemic uncertainty in the model. Additionally, we capture the aleatoric uncertainty using a Gaussian likelihood. We demonstrate the effectiveness of the proposed approach on modeling uncertainty in object detection and segmentation tasks using out-of-distribution experiments. Experimental results show that MC-DropBlock improves the generalization, calibration, and uncertainty modeling capabilities of YOLO models in object detection and segmentation.",0
"The performance of computer vision problems like object detection and segmentation has been greatly improved by the advancements in deep learning. However, in real-world applications like autonomous driving, incorrect predictions of objects can pose a significant risk. The standard deep learning models for object detection, such as YOLO, are often overconfident in their predictions and fail to consider the uncertainty in predictions on out-of-distribution data. To address this issue, we propose an efficient and effective approach to model uncertainty in object detection and segmentation tasks using Monte-Carlo DropBlock (MC-DropBlock) based inference. This approach applies drop-block during both training and test time on the convolutional layer of deep learning models like YOLO, resulting in a Bayesian convolutional neural network that captures the epistemic uncertainty in the model. Additionally, we capture the aleatoric uncertainty using a Gaussian likelihood. Through out-of-distribution experiments, we demonstrate the effectiveness of our proposed approach in improving the generalization, calibration, and uncertainty modeling capabilities of YOLO models in object detection and segmentation.",1
"We presented an optical system to perform imaging interested objects in complex scenes, like the creature easy see the interested prey in the hunt for complex environments. It utilized Deep-learning network to learn the interested objects's vision features and designed the corresponding ""imaging matrices"", furthermore the learned matrixes act as the measurement matrix to complete compressive imaging with a single-pixel camera, finally we can using the compressed image data to only image the interested objects without the rest objects and backgrounds of the scenes with the previous Deep-learning network. Our results demonstrate that no matter interested object is single feature or rich details, the interference can be successfully filtered out and this idea can be applied in some common applications that effectively improve the performance. This bio-inspired optical system can act as the creature eye to achieve success on interested-based object imaging, object detection, object recognition and object tracking, etc.",0
"In this study, we introduced an optical system that can capture images of objects of interest in complex scenes, similar to how creatures can easily spot and hunt prey in their environment. The system utilized a Deep-learning network to learn the visual features of the objects of interest and designed corresponding ""imaging matrices"". These learned matrices acted as measurement matrices for compressive imaging using a single-pixel camera. The compressed image data allowed us to isolate and image only the objects of interest without any interference from the background or other objects in the scene, thanks to the Deep-learning network. Our results demonstrated the system's effectiveness in filtering out interference and improving performance in both single-feature and rich-detail objects. This bio-inspired optical system has potential applications in object imaging, detection, recognition, tracking, and other fields.",1
"Class-agnostic object proposal generation is an important first step in many object detection pipelines. However, object proposals of modern systems are rather inaccurate in terms of segmentation and only roughly adhere to object boundaries. Since typical refinement steps are usually not applicable to thousands of proposals, we propose a superpixel-based refinement system for object proposal generation systems. Utilizing precise superpixels and superpixel pooling on deep features, we refine initial coarse proposals in an end-to-end learned system. Furthermore, we propose a novel DeepFH segmentation, which enriches the classic Felzenszwalb and Huttenlocher (FH) segmentation with deep features leading to improved segmentation results and better object proposal refinements. On the COCO dataset with LVIS annotations, we show that our refinement based on DeepFH superpixels outperforms state-of-the-art methods and leads to more precise object proposals.",0
"The initial step in numerous object detection workflows involves generating object proposals that are not dependent on the class. However, the segmentation accuracy of modern systems' object proposals is relatively low, and they only roughly conform to object boundaries. As it is unfeasible to apply typical refinement steps to thousands of proposals, we suggest a refinement system based on superpixels for object proposal generation systems. By utilizing precise superpixels and superpixel pooling on deep features, we refine initial coarse proposals in a learned system that is end-to-end. Additionally, we propose a new segmentation technique called DeepFH, which enhances the classic Felzenszwalb and Huttenlocher (FH) segmentation with deep features, resulting in improved segmentation outcomes and superior object proposal refinements. We demonstrate on the COCO dataset with LVIS annotations that our refinement based on DeepFH superpixels outperforms state-of-the-art methods and produces more accurate object proposals.",1
"Visual object localization is the key step in a series of object detection tasks. In the literature, high localization accuracy is achieved with the mainstream strongly supervised frameworks. However, such methods require object-level annotations and are unable to detect objects of unknown categories. Weakly supervised methods face similar difficulties. In this paper, a self-paced learning framework is proposed to achieve accurate object localization on the rank list returned by instance search. The proposed framework mines the target instance gradually from the queries and their corresponding top-ranked search results. Since a common instance is shared between the query and the images in the rank list, the target visual instance can be accurately localized even without knowing what the object category is. In addition to performing localization on instance search, the issue of few-shot object detection is also addressed under the same framework. Superior performance over state-of-the-art methods is observed on both tasks.",0
"The crucial step in detecting objects is localizing them visually. Existing supervised methods achieve high accuracy in localization, but they require object-level annotations and cannot detect unfamiliar objects. Similarly, weakly supervised methods are also faced with challenges. This paper proposes a self-paced learning framework that can accurately localize objects by gradually mining the target instance from the queries and their corresponding top-ranked search results. Since the query and the images in the rank list share a common instance, the proposed framework can localize the target visual instance even without knowing the object category. Additionally, the framework addresses the issue of few-shot object detection and demonstrates superior performance compared to state-of-the-art methods in both tasks.",1
"This paper presents an end-to-end semi-supervised object detection approach, in contrast to previous more complex multi-stage methods. The end-to-end training gradually improves pseudo label qualities during the curriculum, and the more and more accurate pseudo labels in turn benefit object detection training. We also propose two simple yet effective techniques within this framework: a soft teacher mechanism where the classification loss of each unlabeled bounding box is weighed by the classification score produced by the teacher network; a box jittering approach to select reliable pseudo boxes for the learning of box regression. On the COCO benchmark, the proposed approach outperforms previous methods by a large margin under various labeling ratios, i.e. 1\%, 5\% and 10\%. Moreover, our approach proves to perform also well when the amount of labeled data is relatively large. For example, it can improve a 40.9 mAP baseline detector trained using the full COCO training set by +3.6 mAP, reaching 44.5 mAP, by leveraging the 123K unlabeled images of COCO. On the state-of-the-art Swin Transformer based object detector (58.9 mAP on test-dev), it can still significantly improve the detection accuracy by +1.5 mAP, reaching 60.4 mAP, and improve the instance segmentation accuracy by +1.2 mAP, reaching 52.4 mAP. Further incorporating with the Object365 pre-trained model, the detection accuracy reaches 61.3 mAP and the instance segmentation accuracy reaches 53.0 mAP, pushing the new state-of-the-art.",0
"The present study introduces a semi-supervised object detection method that is end-to-end, in contrast to previous methods that are multi-stage and more complex. The training process gradually enhances the quality of pseudo labels during the curriculum, which, in turn, benefits the object detection training. Within this framework, two effective techniques are proposed: a soft teacher mechanism that weighs the classification loss of each unlabeled bounding box by the teacher network's classification score, and a box jittering approach that selects reliable pseudo boxes for the learning of box regression. The proposed approach outperforms previous methods significantly under various labeling ratios on the COCO benchmark, such as 1%, 5%, and 10%. Furthermore, it performs well even when the amount of labeled data is relatively large. For example, it enhances a 40.9 mAP baseline detector trained using the full COCO training set by +3.6 mAP, reaching 44.5 mAP, by leveraging the 123K unlabeled images of COCO. On the state-of-the-art Swin Transformer based object detector, it significantly improves the detection accuracy and instance segmentation accuracy. When incorporated with the Object365 pre-trained model, it pushes the new state-of-the-art, with detection accuracy reaching 61.3 mAP and instance segmentation accuracy reaching 53.0 mAP.",1
"Rotating object detection has wide applications in aerial photographs, remote sensing images, UAVs, etc. At present, most of the rotating object detection datasets focus on the field of remote sensing, and these images are usually shot in high-altitude scenes. However, image datasets captured at low-altitude areas also should be concerned, such as drone-based datasets. So we present a low-altitude dronebased dataset, named UAV-ROD, aiming to promote the research and development in rotating object detection and UAV applications. The UAV-ROD consists of 1577 images and 30,090 instances of car category annotated by oriented bounding boxes. In particular, The UAV-ROD can be utilized for the rotating object detection, vehicle orientation recognition and object counting tasks. Compared with horizontal object detection, the regression stage of the rotation detection is a tricky problem. In this paper, we propose a rotating object detector TS4Net, which contains anchor refinement module (ARM) and two-stage sample selective strategy (TS4). The ARM can convert preseted horizontal anchors into high-quality rotated anchors through twostage anchor refinement. The TS4 module utilizes different constrained sample selective strategies to allocate positive and negative samples, which is adaptive to the regression task in different stages. Benefiting from the ARM and TS4, the TS4Net can achieve superior performance for rotating object detection solely with one preseted horizontal anchor. Extensive experimental results on UAV-ROD dataset and three remote sensing datasets DOTA, HRSC2016 and UCAS-AOD demonstrate that our method achieves competitive performance against most state-of-the-art methods.",0
"Rotating object detection is widely applicable in aerial photography, remote sensing imagery, and unmanned aerial vehicles (UAVs). Currently, rotating object detection datasets predominantly focus on remote sensing, with high-altitude scenes being the norm. However, it is important to consider low-altitude areas, such as drone-based datasets. To address this gap, we introduce UAV-ROD, a low-altitude drone-based dataset aimed at advancing research and development in rotating object detection and UAV applications. The dataset comprises 1577 images and 30,090 instances of car categories annotated by oriented bounding boxes. The UAV-ROD can be used for rotating object detection, vehicle orientation recognition, and object counting tasks. Detecting rotating objects is challenging due to the regression stage, but our proposed method, TS4Net, overcomes this with an anchor refinement module (ARM) and a two-stage sample selective strategy (TS4). The ARM refines horizontal anchors into high-quality rotated anchors, while the TS4 module allocates positive and negative samples adaptively for different stages of regression. TS4Net achieves superior performance in rotating object detection with only one horizontal anchor, as evidenced by extensive experiments on UAV-ROD and three remote sensing datasets. Our proposed method competes well against most state-of-the-art methods.",1
"In recent years, many deep learning models have been adopted in autonomous driving. At the same time, these models introduce new vulnerabilities that may compromise the safety of autonomous vehicles. Specifically, recent studies have demonstrated that adversarial attacks can cause a significant decline in detection precision of deep learning-based 3D object detection models. Although driving safety is the ultimate concern for autonomous driving, there is no comprehensive study on the linkage between the performance of deep learning models and the driving safety of autonomous vehicles under adversarial attacks. In this paper, we investigate the impact of two primary types of adversarial attacks, perturbation attacks and patch attacks, on the driving safety of vision-based autonomous vehicles rather than the detection precision of deep learning models. In particular, we consider two state-of-the-art models in vision-based 3D object detection, Stereo R-CNN and DSGN. To evaluate driving safety, we propose an end-to-end evaluation framework with a set of driving safety performance metrics. By analyzing the results of our extensive evaluation experiments, we find that (1) the attack's impact on the driving safety of autonomous vehicles and the attack's impact on the precision of 3D object detectors are decoupled, and (2) the DSGN model demonstrates stronger robustness to adversarial attacks than the Stereo R-CNN model. In addition, we further investigate the causes behind the two findings with an ablation study. The findings of this paper provide a new perspective to evaluate adversarial attacks and guide the selection of deep learning models in autonomous driving.",0
"Autonomous driving has seen an upsurge in the adoption of deep learning models in recent years. However, these models have also introduced new vulnerabilities that can compromise the safety of autonomous vehicles. Adversarial attacks can lead to a significant decline in the detection precision of deep learning-based object detection models. Despite safety being the ultimate concern for autonomous driving, there is no comprehensive study on the relationship between deep learning models' performance and driving safety in the face of adversarial attacks. This study examines the impact of perturbation and patch attacks on the driving safety of vision-based autonomous vehicles, rather than just the detection precision of deep learning models. The study uses two state-of-the-art models in vision-based 3D object detection, Stereo R-CNN and DSGN, and proposes an end-to-end evaluation framework with driving safety performance metrics. The results of the extensive evaluation experiments show that the attack's impact on the driving safety of autonomous vehicles and the precision of 3D object detectors are not interconnected. Moreover, the DSGN model is more robust to adversarial attacks than the Stereo R-CNN model. The study's findings offer a new perspective on evaluating adversarial attacks and guiding the selection of deep learning models for autonomous driving.",1
"Colorectal cancer (CRC) is one of the most common types of cancer with a high mortality rate. Colonoscopy is the preferred procedure for CRC screening and has proven to be effective in reducing CRC mortality. Thus, a reliable computer-aided polyp detection and classification system can significantly increase the effectiveness of colonoscopy. In this paper, we create an endoscopic dataset collected from various sources and annotate the ground truth of polyp location and classification results with the help of experienced gastroenterologists. The dataset can serve as a benchmark platform to train and evaluate the machine learning models for polyp classification. We have also compared the performance of eight state-of-the-art deep learning-based object detection models. The results demonstrate that deep CNN models are promising in CRC screening. This work can serve as a baseline for future research in polyp detection and classification.",0
"CRC is a prevalent cancer type that has a high mortality rate. The preferred screening method for CRC is colonoscopy, which has been effective in reducing its mortality. A dependable computer-aided system for detecting and classifying polyps can greatly enhance the effectiveness of colonoscopy. This paper documents the creation of an endoscopic dataset from diverse sources and the annotation of polyp location and classification by experienced gastroenterologists. The dataset can serve as a benchmark for training and evaluating machine learning models for polyp classification. The performance of eight state-of-the-art deep learning-based object detection models was compared, and the results show that deep CNN models are promising in CRC screening. This work can be used as a starting point for future research on polyp detection and classification.",1
"The fully convolutional network (FCN) has dominated salient object detection for a long period. However, the locality of CNN requires the model deep enough to have a global receptive field and such a deep model always leads to the loss of local details. In this paper, we introduce a new attention-based encoder, vision transformer, into salient object detection to ensure the globalization of the representations from shallow to deep layers. With the global view in very shallow layers, the transformer encoder preserves more local representations to recover the spatial details in final saliency maps. Besides, as each layer can capture a global view of its previous layer, adjacent layers can implicitly maximize the representation differences and minimize the redundant features, making that every output feature of transformer layers contributes uniquely for final prediction. To decode features from the transformer, we propose a simple yet effective deeply-transformed decoder. The decoder densely decodes and upsamples the transformer features, generating the final saliency map with less noise injection. Experimental results demonstrate that our method significantly outperforms other FCN-based and transformer-based methods in five benchmarks by a large margin, with an average of 12.17% improvement in terms of Mean Absolute Error (MAE). Code will be available at https://github.com/OliverRensu/GLSTR.",0
"Salient object detection has long been dominated by the fully convolutional network (FCN), but the CNN's locality necessitates a deep model for a global receptive field, which can result in the loss of local details. This paper introduces an attention-based encoder, the vision transformer, to salient object detection, ensuring that representations are globalized from shallow to deep layers. With the global view in very shallow layers, the transformer encoder preserves more local representations to recover the spatial details in the final saliency maps. Additionally, adjacent layers can implicitly maximize the representation differences and minimize redundant features, ensuring that every output feature of transformer layers contributes uniquely to the final prediction. To decode features from the transformer, a simple yet effective deeply-transformed decoder is proposed. The decoder densely decodes and upsamples the transformer features, generating the final saliency map with less noise injection. Experimental results indicate that this method outperforms other FCN-based and transformer-based methods in five benchmarks by a large margin, with an average improvement of 12.17% in terms of Mean Absolute Error (MAE). Code will be available at https://github.com/OliverRensu/GLSTR.",1
"Unsupervised learning is just at a tipping point where it could really take off. Among these approaches, contrastive learning has seen tremendous progress and led to state-of-the-art performance. In this paper, we construct a novel probabilistic graphical model that effectively incorporates the low rank promoting prior into the framework of contrastive learning, referred to as LORAC. In contrast to the existing conventional self-supervised approaches that only considers independent learning, our hypothesis explicitly requires that all the samples belonging to the same instance class lie on the same subspace with small dimension. This heuristic poses particular joint learning constraints to reduce the degree of freedom of the problem during the search of the optimal network parameterization. Most importantly, we argue that the low rank prior employed here is not unique, and many different priors can be invoked in a similar probabilistic way, corresponding to different hypotheses about underlying truth behind the contrastive features. Empirical evidences show that the proposed algorithm clearly surpasses the state-of-the-art approaches on multiple benchmarks, including image classification, object detection, instance segmentation and keypoint detection.",0
"At present, unsupervised learning is on the verge of becoming highly successful. One promising approach is contrastive learning, which has made significant advancements and achieved state-of-the-art performance. In this article, we introduce a new probabilistic graphical model called LORAC, which incorporates a low rank promoting prior into the contrastive learning framework. Unlike traditional self-supervised techniques that focus on independent learning, our approach imposes joint learning constraints to reduce the degree of freedom when searching for the optimal network parameterization. We contend that the low rank prior used in LORAC is not the only one that can be employed in a probabilistic manner, as different priors can be used to represent various hypotheses about the contrastive features. Empirical data shows that our algorithm outperforms other state-of-the-art approaches on multiple benchmarks, including image classification, object detection, instance segmentation, and keypoint detection.",1
"Detection transformers have recently shown promising object detection results and attracted increasing attention. However, how to develop effective domain adaptation techniques to improve its cross-domain performance remains unexplored and unclear. In this paper, we delve into this topic and empirically find that direct feature distribution alignment on the CNN backbone only brings limited improvements, as it does not guarantee domain-invariant sequence features in the transformer for prediction. To address this issue, we propose a novel Sequence Feature Alignment (SFA) method that is specially designed for the adaptation of detection transformers. Technically, SFA consists of a domain query-based feature alignment (DQFA) module and a token-wise feature alignment (TDA) module. In DQFA, a novel domain query is used to aggregate and align global context from the token sequence of both domains. DQFA reduces the domain discrepancy in global feature representations and object relations when deploying in the transformer encoder and decoder, respectively. Meanwhile, TDA aligns token features in the sequence from both domains, which reduces the domain gaps in local and instance-level feature representations in the transformer encoder and decoder, respectively. Besides, a novel bipartite matching consistency loss is proposed to enhance the feature discriminability for robust object detection. Experiments on three challenging benchmarks show that SFA outperforms state-of-the-art domain adaptive object detection methods. Code has been made available at: https://github.com/encounter1997/SFA.",0
"Recently, object detection results using detection transformers have shown promise and garnered increased attention. However, effective techniques for developing domain adaptation to improve cross-domain performance have not been explored and are unclear. This paper addresses this issue by delving into the topic and discovering that direct feature distribution alignment on the CNN backbone only results in limited improvement, as it does not guarantee domain-invariant sequence features in the transformer for prediction. To overcome this limitation, a novel Sequence Feature Alignment (SFA) method is proposed, which is specifically designed for the adaptation of detection transformers. SFA comprises a domain query-based feature alignment (DQFA) module and a token-wise feature alignment (TDA) module. In DQFA, a novel domain query is employed to aggregate and align global context from the token sequence of both domains, thereby reducing the domain discrepancy in global feature representations and object relations when deployed in the transformer encoder and decoder, respectively. Meanwhile, TDA aligns token features in the sequence from both domains, thereby reducing domain gaps in local and instance-level feature representations in the transformer encoder and decoder, respectively. Additionally, a novel bipartite matching consistency loss is proposed to enhance feature discriminability for robust object detection. Experimental results on three challenging benchmarks demonstrate that SFA outperforms state-of-the-art domain adaptive object detection methods. The code for SFA is available at: https://github.com/encounter1997/SFA.",1
"Automated inspection and detection of foreign objects on railways is important for rail transportation safety as it helps prevent potential accidents and trains derailment. Most existing vision-based approaches focus on the detection of frontal intrusion objects with prior labels, such as categories and locations of the objects. In reality, foreign objects with unknown categories can appear anytime on railway tracks. In this paper, we develop a semi-supervised convolutional autoencoder based framework that only requires railway track images without prior knowledge on the foreign objects in the training process. It consists of three different modules, a bottleneck feature generator as encoder, a photographic image generator as decoder, and a reconstruction discriminator developed via adversarial learning. In the proposed framework, the problem of detecting the presence, location, and shape of foreign objects is addressed by comparing the input and reconstructed images as well as setting thresholds based on reconstruction errors. The proposed method is evaluated through comprehensive studies under different performance criteria. The results show that the proposed method outperforms some well-known benchmarking methods. The proposed framework is useful for data analytics via the train Internet-of-Things (IoT) systems",0
"Ensuring safety in rail transportation is crucial, and automated inspection and detection of foreign objects on railways can prevent accidents and derailments. Although most current vision-based methods focus on detecting labeled frontal intrusion objects, unknown foreign objects can appear on tracks at any time. This study proposes a semi-supervised convolutional autoencoder-based framework that only requires railway track images and does not need prior knowledge of foreign objects during training. The framework consists of three modules, including a bottleneck feature generator, a photographic image generator, and a reconstruction discriminator developed through adversarial learning. It addresses the issue of detecting the presence, location, and shape of foreign objects through image comparison and setting thresholds based on reconstruction errors. The proposed method outperforms some benchmarking methods and is useful for data analytics via train IoT systems.",1
"The recently proposed Detection Transformer (DETR) model successfully applies Transformer to objects detection and achieves comparable performance with two-stage object detection frameworks, such as Faster-RCNN. However, DETR suffers from its slow convergence. Training DETR from scratch needs 500 epochs to achieve a high accuracy. To accelerate its convergence, we propose a simple yet effective scheme for improving the DETR framework, namely Spatially Modulated Co-Attention (SMCA) mechanism. The core idea of SMCA is to conduct location-aware co-attention in DETR by constraining co-attention responses to be high near initially estimated bounding box locations. Our proposed SMCA increases DETR's convergence speed by replacing the original co-attention mechanism in the decoder while keeping other operations in DETR unchanged. Furthermore, by integrating multi-head and scale-selection attention designs into SMCA, our fully-fledged SMCA can achieve better performance compared to DETR with a dilated convolution-based backbone (45.6 mAP at 108 epochs vs. 43.3 mAP at 500 epochs). We perform extensive ablation studies on COCO dataset to validate SMCA. Code is released at https://github.com/gaopengcuhk/SMCA-DETR .",0
"The Detection Transformer (DETR) model has been proposed as a successful application of Transformer to objects detection, with performance comparable to that of two-stage object detection frameworks like Faster-RCNN. However, DETR's convergence is slow, requiring 500 epochs to achieve high accuracy when trained from scratch. To speed up convergence, we propose a Spatially Modulated Co-Attention (SMCA) mechanism that conducts location-aware co-attention in DETR, constraining co-attention responses to be high near initially estimated bounding box locations. SMCA replaces the original co-attention mechanism in the decoder, while keeping other operations in DETR unchanged. By integrating multi-head and scale-selection attention designs into SMCA, our fully-fledged SMCA outperforms DETR with a dilated convolution-based backbone (45.6 mAP at 108 epochs vs. 43.3 mAP at 500 epochs). We validate SMCA through extensive ablation studies on the COCO dataset, and our code is available at https://github.com/gaopengcuhk/SMCA-DETR.",1
"Current state-of-the-art object detectors can have significant performance drop when deployed in the wild due to domain gaps with training data. Unsupervised Domain Adaptation (UDA) is a promising approach to adapt models for new domains/environments without any expensive label cost. However, without ground truth labels, most prior works on UDA for object detection tasks can only perform coarse image-level and/or feature-level adaptation by using adversarial learning methods. In this work, we show that such adversarial-based methods can only reduce the domain style gap, but cannot address the domain content distribution gap that is shown to be important for object detectors. To overcome this limitation, we propose the Cross-Domain Semi-Supervised Learning (CDSSL) framework by leveraging high-quality pseudo labels to learn better representations from the target domain directly. To enable SSL for cross-domain object detection, we propose fine-grained domain transfer, progressive-confidence-based label sharpening and imbalanced sampling strategy to address two challenges: (i) non-identical distribution between source and target domain data, (ii) error amplification/accumulation due to noisy pseudo labeling on the target domain. Experiment results show that our proposed approach consistently achieves new state-of-the-art performance (2.2% - 9.5% better than prior best work on mAP) under various domain gap scenarios. The code will be released.",0
"Object detectors that are currently considered state-of-the-art may experience a significant decline in performance when implemented in real-world settings, as they may face domain gaps with the training data. Unsupervised Domain Adaptation (UDA) is a promising technique for adapting models to new domains or environments without incurring high label costs. However, most previous works on UDA for object detection tasks can only carry out coarse image-level and/or feature-level adaptation, as they don't have access to ground truth labels and instead rely on adversarial learning methodologies. This research demonstrates that such adversarial-based methods can only reduce the domain style gap, but can't address the domain content distribution gap, which is crucial for object detectors. To overcome this limitation, the Cross-Domain Semi-Supervised Learning (CDSSL) framework is proposed by leveraging high-quality pseudo labels to learn better representations from the target domain directly. To enable SSL for cross-domain object detection, the researchers propose a fine-grained domain transfer, a progressive-confidence-based label sharpening, and an imbalanced sampling strategy to address two challenges: non-identical distribution between source and target domain data, as well as error amplification/accumulation due to noisy pseudo labeling on the target domain. According to experimental results, this approach consistently achieves new state-of-the-art performance (2.2% - 9.5% better than prior best work on mAP) under various domain gap scenarios. The code for this approach will be released.",1
"Expensive bounding-box annotations have limited the development of object detection task. Thus, it is necessary to focus on more challenging task of few-shot object detection. It requires the detector to recognize objects of novel classes with only a few training samples. Nowadays, many existing popular methods based on meta-learning have achieved promising performance, such as Meta R-CNN series. However, only a single category of support data is used as the attention to guide the detecting of query images each time. Their relevance to each other remains unexploited. Moreover, a lot of recent works treat the support data and query images as independent branch without considering the relationship between them. To address this issue, we propose a dynamic relevance learning model, which utilizes the relationship between all support images and Region of Interest (RoI) on the query images to construct a dynamic graph convolutional network (GCN). By adjusting the prediction distribution of the base detector using the output of this GCN, the proposed model can guide the detector to improve the class representation implicitly. Comprehensive experiments have been conducted on Pascal VOC and MS-COCO dataset. The proposed model achieves the best overall performance, which shows its effectiveness of learning more generalized features. Our code is available at https://github.com/liuweijie19980216/DRL-for-FSOD.",0
"The limited development of object detection task has been hindered by expensive bounding-box annotations. To overcome this, it is necessary to focus on the more challenging task of few-shot object detection. This involves the detector recognizing objects of novel classes with only a few training samples. Meta-learning-based methods, such as the Meta R-CNN series, have shown promising performance. However, these methods only use a single category of support data to guide the detection of query images, without exploiting their relevance to each other. Furthermore, recent works treat support data and query images as independent branches, ignoring their relationship. To address this, we propose a dynamic relevance learning model that uses a dynamic graph convolutional network (GCN) to exploit the relationship between all support images and the Region of Interest (RoI) on query images. By adjusting the prediction distribution of the base detector using the GCN output, the proposed model improves the class representation implicitly. Our experiments on Pascal VOC and MS-COCO dataset show that our proposed model achieves the best overall performance and is effective in learning more generalized features. Our code is available at https://github.com/liuweijie19980216/DRL-for-FSOD.",1
"Recent progress on salient object detection (SOD) mainly benefits from multi-scale learning, where the high-level and low-level features collaborate in locating salient objects and discovering fine details, respectively. However, most efforts are devoted to low-level feature learning by fusing multi-scale features or enhancing boundary representations. High-level features, which although have long proven effective for many other tasks, yet have been barely studied for SOD. In this paper, we tap into this gap and show that enhancing high-level features is essential for SOD as well. To this end, we introduce an Extremely-Downsampled Network (EDN), which employs an extreme downsampling technique to effectively learn a global view of the whole image, leading to accurate salient object localization. To accomplish better multi-level feature fusion, we construct the Scale-Correlated Pyramid Convolution (SCPC) to build an elegant decoder for recovering object details from the above extreme downsampling. Extensive experiments demonstrate that EDN achieves state-of-the-art performance with real-time speed. Our efficient EDN-Lite also achieves competitive performance with a speed of 316fps. Hence, this work is expected to spark some new thinking in SOD. Full training and testing code will be available at https://github.com/yuhuan-wu/EDN.",0
"The advancement of salient object detection (SOD) has primarily resulted from multi-scale learning. This involves the combination of high-level and low-level features to locate salient objects and detect fine details, respectively. However, the majority of research has focused on low-level feature learning, such as merging multi-scale features or enhancing boundary representations. High-level features, which have proven successful in other tasks, have received minimal attention in SOD. This study aims to bridge this gap by demonstrating the importance of enhancing high-level features for SOD. To achieve this, an Extremely-Downsampled Network (EDN) is introduced. This network uses an extreme downsampling technique to learn a global view of the entire image, leading to precise salient object localization. Additionally, a Scale-Correlated Pyramid Convolution (SCPC) is constructed to develop an elegant decoder that retrieves object details from the aforementioned extreme downsampling, resulting in better multi-level feature fusion. Extensive experiments reveal that EDN achieves superior performance with real-time speed. The EDN-Lite version also obtains competitive results, with a speed of 316fps. Therefore, this study is expected to stimulate new ideas in SOD. The complete training and testing code is available at https://github.com/yuhuan-wu/EDN.",1
"The popularity and promotion of depth maps have brought new vigor and vitality into salient object detection (SOD), and a mass of RGB-D SOD algorithms have been proposed, mainly concentrating on how to better integrate cross-modality features from RGB image and depth map. For the cross-modality interaction in feature encoder, existing methods either indiscriminately treat RGB and depth modalities, or only habitually utilize depth cues as auxiliary information of the RGB branch. Different from them, we reconsider the status of two modalities and propose a novel Cross-modality Discrepant Interaction Network (CDINet) for RGB-D SOD, which differentially models the dependence of two modalities according to the feature representations of different layers. To this end, two components are designed to implement the effective cross-modality interaction: 1) the RGB-induced Detail Enhancement (RDE) module leverages RGB modality to enhance the details of the depth features in low-level encoder stage. 2) the Depth-induced Semantic Enhancement (DSE) module transfers the object positioning and internal consistency of depth features to the RGB branch in high-level encoder stage. Furthermore, we also design a Dense Decoding Reconstruction (DDR) structure, which constructs a semantic block by combining multi-level encoder features to upgrade the skip connection in the feature decoding. Extensive experiments on five benchmark datasets demonstrate that our network outperforms $15$ state-of-the-art methods both quantitatively and qualitatively. Our code is publicly available at: https://rmcong.github.io/proj_CDINet.html.",0
"The promotion and widespread use of depth maps has injected new energy into salient object detection (SOD), leading to a plethora of RGB-D SOD algorithms that focus on improving the integration of cross-modality features from RGB images and depth maps. However, existing methods have either treated the two modalities indiscriminately or have only used depth cues to supplement the RGB branch. In contrast, we propose a novel Cross-modality Discrepant Interaction Network (CDINet) for RGB-D SOD that takes into account the unique characteristics of each modality and models their dependence differentially based on the feature representations of different layers. Two components, the RGB-induced Detail Enhancement (RDE) module and the Depth-induced Semantic Enhancement (DSE) module, are designed to facilitate effective cross-modality interaction at different stages of the encoder. In addition, we introduce a Dense Decoding Reconstruction (DDR) structure that combines multi-level encoder features to upgrade the skip connection in the feature decoding. Our experiments on five benchmark datasets demonstrate that CDINet outperforms 15 state-of-the-art methods in both quantitative and qualitative measures. The code for our network is available at https://rmcong.github.io/proj_CDINet.html.",1
"This paper focuses on high-transferable adversarial attacks on detectors, which are hard to attack in a black-box manner, because of their multiple-output characteristics and the diversity across architectures. To pursue a high attack transferability, one plausible way is to find a common property across detectors, which facilitates the discovery of common weaknesses. We are the first to suggest that the relevance map from interpreters for detectors is such a property. Based on it, we design a Relevance Attack on Detectors (RAD), which achieves a state-of-the-art transferability, exceeding existing results by above 20%. On MS COCO, the detection mAPs for all 8 black-box architectures are more than halved and the segmentation mAPs are also significantly influenced. Given the great transferability of RAD, we generate the first adversarial dataset for object detection and instance segmentation, i.e., Adversarial Objects in COntext (AOCO), which helps to quickly evaluate and improve the robustness of detectors.",0
"The main focus of this paper is on creating high-transferable adversarial attacks on detectors that are difficult to attack using a black-box approach. This is due to the multiple-output characteristics and diversity across architectures. To achieve high attack transferability, a possible method is to identify a common property among detectors that can reveal common weaknesses. We propose that the relevance map from interpreters for detectors is such a property and use it to create a Relevance Attack on Detectors (RAD). This approach achieves a state-of-the-art transferability, surpassing existing results by over 20%. RAD significantly impacts the detection mAPs and segmentation mAPs for all 8 black-box architectures on MS COCO, reducing them by more than half. Due to the success of RAD, we generate the first adversarial dataset for object detection and instance segmentation, known as Adversarial Objects in COntext (AOCO), which enables the evaluation and enhancement of detector robustness.",1
"High performance person Re-Identification (Re-ID) requires the model to focus on both global silhouette and local details of pedestrian. To extract such more representative features, an effective way is to exploit deep models with multiple branches. However, most multi-branch based methods implemented by duplication of part backbone structure normally lead to severe increase of computational cost. In this paper, we propose a lightweight Feature Pyramid Branch (FPB) to extract features from different layers of networks and aggregate them in a bidirectional pyramid structure. Cooperated by attention modules and our proposed cross orthogonality regularization, FPB significantly prompts the performance of backbone network by only introducing less than 1.5M extra parameters. Extensive experimental results on standard benchmark datasets demonstrate that our proposed FPB based model outperforms state-of-the-art methods with obvious margin as well as much less model complexity. FPB borrows the idea of the Feature Pyramid Network (FPN) from prevailing object detection methods. To our best knowledge, it is the first successful application of similar structure in person Re-ID tasks, which empirically proves that pyramid network as affiliated branch could be a potential structure in related feature embedding models. The source code is publicly available at https://github.com/anocodetest1/FPB.git.",0
"For effective High Performance Person Re-Identification (Re-ID), the model must focus on both the global silhouette and local details of the pedestrian. To extract more representative features, deep models with multiple branches are used, but most of these methods lead to a severe increase in computational cost due to duplication of part backbone structure. In this paper, we propose a lightweight Feature Pyramid Branch (FPB) that extracts features from different layers of networks and aggregates them in a bidirectional pyramid structure, using attention modules and our proposed cross orthogonality regularization. By introducing less than 1.5M extra parameters, FPB significantly improves the performance of the backbone network. Our experimental results on standard benchmark datasets show that FPB-based models outperform state-of-the-art methods with less model complexity. FPB borrows the idea of the Feature Pyramid Network (FPN) from object detection methods, and is the first successful application of a similar structure in person Re-ID tasks. This proves that pyramid networks as affiliated branches have potential for related feature embedding models. The source code is publicly available at https://github.com/anocodetest1/FPB.git.",1
"Human vision is often adversely affected by complex environmental factors, especially in night vision scenarios. Thus, infrared cameras are often leveraged to help enhance the visual effects via detecting infrared radiation in the surrounding environment, but the infrared videos are undesirable due to the lack of detailed semantic information. In such a case, an effective video-to-video translation method from the infrared domain to the visible light counterpart is strongly needed by overcoming the intrinsic huge gap between infrared and visible fields. To address this challenging problem, we propose an infrared-to-visible (I2V) video translation method I2V-GAN to generate fine-grained and spatial-temporal consistent visible light videos by given unpaired infrared videos. Technically, our model capitalizes on three types of constraints: 1)adversarial constraint to generate synthetic frames that are similar to the real ones, 2)cyclic consistency with the introduced perceptual loss for effective content conversion as well as style preservation, and 3)similarity constraints across and within domains to enhance the content and motion consistency in both spatial and temporal spaces at a fine-grained level. Furthermore, the current public available infrared and visible light datasets are mainly used for object detection or tracking, and some are composed of discontinuous images which are not suitable for video tasks. Thus, we provide a new dataset for I2V video translation, which is named IRVI. Specifically, it has 12 consecutive video clips of vehicle and monitoring scenes, and both infrared and visible light videos could be apart into 24352 frames. Comprehensive experiments validate that I2V-GAN is superior to the compared SOTA methods in the translation of I2V videos with higher fluency and finer semantic details. The code and IRVI dataset are available at https://github.com/BIT-DA/I2V-GAN.",0
"In situations where complex environmental factors make it difficult for humans to see clearly, especially at night, infrared cameras can be used to detect infrared radiation and enhance visual effects. However, the resulting infrared videos lack detailed semantic information. To address this issue, an effective video-to-video translation method is needed to convert infrared videos to visible light videos. We propose I2V-GAN, an infrared-to-visible video translation method that generates fine-grained and spatial-temporal consistent visible light videos from unpaired infrared videos. Our model uses three types of constraints to ensure content and motion consistency at a fine-grained level. We also introduce a new dataset, IRVI, for I2V video translation. Our experiments show that I2V-GAN outperforms other state-of-the-art methods in terms of fluency and semantic details. The code and IRVI dataset are available at https://github.com/BIT-DA/I2V-GAN.",1
"Domain shift is a well known problem where a model trained on a particular domain (source) does not perform well when exposed to samples from a different domain (target). Unsupervised methods that can adapt to domain shift are highly desirable as they allow effective utilization of the source data without requiring additional annotated training data from the target. Practically, obtaining sufficient amount of annotated data from the target domain can be both infeasible and extremely expensive. In this work, we address the domain shift problem for the object detection task. Our approach relies on gradually removing the domain shift between the source and the target domains. The key ingredients to our approach are -- (a) mapping the source to the target domain on pixel-level; (b) training a teacher network on the mapped source and the unannotated target domain using adversarial feature alignment; and (c) finally training a student network using the pseudo-labels obtained from the teacher. Experimentally, when tested on challenging scenarios involving domain shift, we consistently obtain significantly large performance gains over various recent state of the art approaches.",0
"The issue of domain shift is widely recognized, wherein a model trained on a specific domain (source) fails to perform well when presented with samples from a different domain (target). Unsupervised methods that can adjust to domain shift are highly desirable as they enable effective utilization of the source data without necessitating additional annotated training data from the target, which can be both impractical and costly. This study focuses on addressing the domain shift problem for the object detection task by gradually eliminating the domain shift between the source and target domains. Our methodology involves (a) mapping the source to the target domain at the pixel level; (b) training a teacher network using adversarial feature alignment on the mapped source and the unannotated target domain; and (c) ultimately training a student network using the pseudo-labels produced by the teacher. Our experimental results consistently demonstrate significant performance improvements over various recent state-of-the-art approaches in challenging scenarios involving domain shift.",1
"Weakly-supervised object detection (WSOD) has emerged as an inspiring recent topic to avoid expensive instance-level object annotations. However, the bounding boxes of most existing WSOD methods are mainly determined by precomputed proposals, thereby being limited in precise object localization. In this paper, we defend the problem setting for improving localization performance by leveraging the bounding box regression knowledge from a well-annotated auxiliary dataset. First, we use the well-annotated auxiliary dataset to explore a series of learnable bounding box adjusters (LBBAs) in a multi-stage training manner, which is class-agnostic. Then, only LBBAs and a weakly-annotated dataset with non-overlapped classes are used for training LBBA-boosted WSOD. As such, our LBBAs are practically more convenient and economical to implement while avoiding the leakage of the auxiliary well-annotated dataset. In particular, we formulate learning bounding box adjusters as a bi-level optimization problem and suggest an EM-like multi-stage training algorithm. Then, a multi-stage scheme is further presented for LBBA-boosted WSOD. Additionally, a masking strategy is adopted to improve proposal classification. Experimental results verify the effectiveness of our method. Our method performs favorably against state-of-the-art WSOD methods and knowledge transfer model with similar problem setting. Code is publicly available at \url{https://github.com/DongSky/lbba_boosted_wsod}.",0
"Recently, Weakly-supervised object detection (WSOD) has gained popularity as it eliminates the need for expensive instance-level object annotations. However, most existing WSOD methods rely on precomputed proposals to determine bounding boxes, which limits their ability to accurately localize objects. This paper aims to improve localization performance by utilizing bounding box regression knowledge from a well-annotated auxiliary dataset. The approach involves developing learnable bounding box adjusters (LBBAs) in a multi-stage training process using the auxiliary dataset. The LBBA-boosted WSOD model is then trained using only LBBAs and a weakly-annotated dataset with non-overlapping classes. This approach is more practical and cost-effective as it avoids the leakage of the auxiliary dataset. The learning of bounding box adjusters is formulated as a bi-level optimization problem, and an EM-like multi-stage training algorithm is suggested. A multi-stage scheme is also presented for LBBA-boosted WSOD. Additionally, a masking strategy is adopted to improve proposal classification. Experimental results demonstrate the effectiveness of this method, which outperforms state-of-the-art WSOD methods and knowledge transfer models with similar problem settings. The code is publicly available at \url{https://github.com/DongSky/lbba_boosted_wsod}.",1
"Descriptive region features extracted by object detection networks have played an important role in the recent advancements of image captioning. However, they are still criticized for the lack of contextual information and fine-grained details, which in contrast are the merits of traditional grid features. In this paper, we introduce a novel Dual-Level Collaborative Transformer (DLCT) network to realize the complementary advantages of the two features. Concretely, in DLCT, these two features are first processed by a novelDual-way Self Attenion (DWSA) to mine their intrinsic properties, where a Comprehensive Relation Attention component is also introduced to embed the geometric information. In addition, we propose a Locality-Constrained Cross Attention module to address the semantic noises caused by the direct fusion of these two features, where a geometric alignment graph is constructed to accurately align and reinforce region and grid features. To validate our model, we conduct extensive experiments on the highly competitive MS-COCO dataset, and achieve new state-of-the-art performance on both local and online test sets, i.e., 133.8% CIDEr-D on Karpathy split and 135.4% CIDEr on the official split. Code is available at https://github.com/luo3300612/image-captioning-DLCT.",0
"The utilization of object detection networks for extracting descriptive region features has been crucial in the recent progress of image captioning. However, these features have been subjected to criticism due to their lack of contextual information and detailed intricacies, which are instead the strengths of conventional grid features. This paper presents a novel Dual-Level Collaborative Transformer (DLCT) network that incorporates both features to achieve complementary advantages. The DLCT network employs a Dual-way Self Attention (DWSA) to analyze the intrinsic properties of the two features, incorporating a Comprehensive Relation Attention component to include geometric information. Furthermore, to address semantic noise resulting from the direct fusion of the features, the Locality-Constrained Cross Attention module is proposed. This module utilizes a geometric alignment graph to accurately align and reinforce region and grid features. Our model is validated through extensive experiments conducted on the highly competitive MS-COCO dataset, resulting in new state-of-the-art performance on both local and online test sets, with 133.8% CIDEr-D on Karpathy split and 135.4% CIDEr on the official split. The code for our model is available at https://github.com/luo3300612/image-captioning-DLCT.",1
"General object detectors use powerful backbones that uniformly extract features from images for enabling detection of a vast amount of object types. However, utilization of such backbones in object detection applications developed for specific object types can unnecessarily over-process an extensive amount of background. In addition, they are agnostic to object scales, thus redundantly process all image regions at the same resolution. In this work we introduce BLT-net, a new low-computation two-stage object detection architecture designed to process images with a significant amount of background and objects of variate scales. BLT-net reduces computations by separating objects from background using a very lite first-stage. BLT-net then efficiently merges obtained proposals to further decrease processed background and then dynamically reduces their resolution to minimize computations. Resulting image proposals are then processed in the second-stage by a highly accurate model. We demonstrate our architecture on the pedestrian detection problem, where objects are of different sizes, images are of high resolution and object detection is required to run in real-time. We show that our design reduces computations by a factor of x4-x7 on the Citypersons and Caltech datasets with respect to leading pedestrian detectors, on account of a small accuracy degradation. This method can be applied on other object detection applications in scenes with a considerable amount of background and variate object sizes to reduce computations.",0
"Object detection systems typically rely on high-powered backbones to extract image features and enable the detection of a wide range of object types. However, using these backbones in object detection applications developed for specific object types can result in unnecessary processing of a large amount of background. Additionally, these systems do not account for object scales, resulting in redundant processing of all image regions at the same resolution. To address these issues, we introduce BLT-net, a new two-stage object detection architecture designed to process images with a significant amount of background and objects of varying scales while reducing computations. BLT-net uses a lightweight first stage to separate objects from background and merges proposals to decrease processed background. It then dynamically reduces proposal resolution to minimize computations before processing them in the highly accurate second stage. We demonstrate the effectiveness of our approach on pedestrian detection, where objects are of different sizes and real-time detection is required. Our design reduces computations by a factor of x4-x7 on the Citypersons and Caltech datasets with only a small accuracy degradation. This approach can be applied to other object detection applications in scenes with a significant amount of background and varying object sizes to reduce computations.",1
"The Annotated Germs for Automated Recognition (AGAR) dataset is an image database of microbial colonies cultured on agar plates. It contains 18000 photos of five different microorganisms as single or mixed cultures, taken under diverse lighting conditions with two different cameras. All the images are classified into ""countable"", ""uncountable"", and ""empty"", with the ""countable"" class labeled by microbiologists with colony location and species identification (336442 colonies in total). This study describes the dataset itself and the process of its development. In the second part, the performance of selected deep neural network architectures for object detection, namely Faster R-CNN and Cascade R-CNN, was evaluated on the AGAR dataset. The results confirmed the great potential of deep learning methods to automate the process of microbe localization and classification based on Petri dish photos. Moreover, AGAR is the first publicly available dataset of this kind and size and will facilitate the future development of machine learning models. The data used in these studies can be found at https://agar.neurosys.com/.",0
"The AGAR dataset is a collection of images showing microbial colonies grown on agar plates. It includes 18000 photos of five different microorganisms, either alone or mixed, captured using two cameras and in various lighting conditions. The images are categorized into ""countable"", ""uncountable"", and ""empty"" classes, with the ""countable"" category having 336442 colonies identified by microbiologists with their location and species. This article details the creation of the dataset and evaluates the suitability of two deep neural network architectures, Faster R-CNN and Cascade R-CNN, for object detection. The findings demonstrate the potential of deep learning techniques to automate microbe classification and localization using images of Petri dishes. This publicly available dataset, AGAR, is the first of its kind and size and will support the development of machine learning models. Access to the dataset is available at https://agar.neurosys.com/.",1
"We present BoTNet, a conceptually simple yet powerful backbone architecture that incorporates self-attention for multiple computer vision tasks including image classification, object detection and instance segmentation. By just replacing the spatial convolutions with global self-attention in the final three bottleneck blocks of a ResNet and no other changes, our approach improves upon the baselines significantly on instance segmentation and object detection while also reducing the parameters, with minimal overhead in latency. Through the design of BoTNet, we also point out how ResNet bottleneck blocks with self-attention can be viewed as Transformer blocks. Without any bells and whistles, BoTNet achieves 44.4% Mask AP and 49.7% Box AP on the COCO Instance Segmentation benchmark using the Mask R-CNN framework; surpassing the previous best published single model and single scale results of ResNeSt evaluated on the COCO validation set. Finally, we present a simple adaptation of the BoTNet design for image classification, resulting in models that achieve a strong performance of 84.7% top-1 accuracy on the ImageNet benchmark while being up to 1.64x faster in compute time than the popular EfficientNet models on TPU-v3 hardware. We hope our simple and effective approach will serve as a strong baseline for future research in self-attention models for vision",0
"BoTNet is a powerful backbone architecture for various computer vision tasks, such as image classification, object detection, and instance segmentation. It incorporates self-attention in the final three bottleneck blocks of a ResNet, replacing spatial convolutions with global self-attention. This approach significantly improves upon the baselines on instance segmentation and object detection, while also reducing parameters and minimizing latency. BoTNet's design highlights how ResNet bottleneck blocks with self-attention can be viewed as Transformer blocks. BoTNet achieves impressive results on the COCO Instance Segmentation benchmark, surpassing previous best published results. Furthermore, BoTNet's design can be adapted for image classification, achieving strong performance with faster compute time than popular EfficientNet models. We hope our simple and effective approach will serve as a strong baseline for future research in self-attention models for vision.",1
"In this paper, we present new feature encoding methods for Detection of 3D objects in point clouds. We used a graph neural network (GNN) for Detection of 3D objects namely cars, pedestrians, and cyclists. Feature encoding is one of the important steps in Detection of 3D objects. The dataset used is point cloud data which is irregular and unstructured and it needs to be encoded in such a way that ensures better feature encapsulation. Earlier works have used relative distance as one of the methods to encode the features. These methods are not resistant to rotation variance problems in Graph Neural Networks. We have included angular-based measures while performing feature encoding in graph neural networks. Along with that, we have performed a comparison between other methods like Absolute, Relative, Euclidean distances, and a combination of the Angle and Relative methods. The model is trained and evaluated on the subset of the KITTI object detection benchmark dataset under resource constraints. Our results demonstrate that a combination of angle measures and relative distance has performed better than other methods. In comparison to the baseline method(relative), it achieved better performance. We also performed time analysis of various feature encoding methods.",0
"The aim of this paper is to introduce novel methods for feature encoding in the Detection of 3D objects within point clouds. The study utilizes a graph neural network (GNN) to identify cars, pedestrians, and cyclists. Feature encoding is critical in this process, as point cloud data is irregular and unstructured, necessitating effective encapsulation of features. Prior research has used relative distance to encode features, which is not immune to rotation variance issues in Graph Neural Networks. Therefore, the study includes angular-based measures in feature encoding, comparing them with other methods like Absolute and Euclidean distances, as well as a combination of Angle and Relative methods. The model's performance is evaluated on a subset of the KITTI object detection benchmark dataset while adhering to resource constraints. The findings show that combining angle measures and relative distance outperforms other methods, including the baseline relative method. Moreover, the study conducts a time analysis of various feature encoding methods.",1
"Current perception models in autonomous driving have become notorious for greatly relying on a mass of annotated data to cover unseen cases and address the long-tail problem. On the other hand, learning from unlabeled large-scale collected data and incrementally self-training powerful recognition models have received increasing attention and may become the solutions of next-generation industry-level powerful and robust perception models in autonomous driving. However, the research community generally suffered from data inadequacy of those essential real-world scene data, which hampers the future exploration of fully/semi/self-supervised methods for 3D perception. In this paper, we introduce the ONCE (One millioN sCenEs) dataset for 3D object detection in the autonomous driving scenario. The ONCE dataset consists of 1 million LiDAR scenes and 7 million corresponding camera images. The data is selected from 144 driving hours, which is 20x longer than the largest 3D autonomous driving dataset available (e.g. nuScenes and Waymo), and it is collected across a range of different areas, periods and weather conditions. To facilitate future research on exploiting unlabeled data for 3D detection, we additionally provide a benchmark in which we reproduce and evaluate a variety of self-supervised and semi-supervised methods on the ONCE dataset. We conduct extensive analyses on those methods and provide valuable observations on their performance related to the scale of used data. Data, code, and more information are available at https://once-for-auto-driving.github.io/index.html.",0
"Autonomous driving perception models are often criticized for their heavy reliance on annotated data to cover various situations and address the long-tail issue. However, there is an increasing interest in learning from unlabeled large-scale data and incrementally self-training powerful recognition models as potential solutions for next-generation industry-level perception models. Unfortunately, the research community is plagued by inadequate real-world scene data, which makes it difficult to explore fully/semi/self-supervised methods for 3D perception. To address this issue, we present the ONCE dataset, which contains one million LiDAR scenes and seven million corresponding camera images from 144 driving hours. This dataset is 20 times longer than the largest 3D autonomous driving dataset available and covers a wide range of areas, periods, and weather conditions. We also provide a benchmark for future research on exploiting unlabeled data for 3D detection, along with a variety of self-supervised and semi-supervised methods that we reproduce and evaluate on the ONCE dataset. Our extensive analyses provide valuable insights on their performance in relation to the scale of the used data. For more information, data, and code, please visit https://once-for-auto-driving.github.io/index.html.",1
"Network compression has been widely studied since it is able to reduce the memory and computation cost during inference. However, previous methods seldom deal with complicated structures like residual connections, group/depth-wise convolution and feature pyramid network, where channels of multiple layers are coupled and need to be pruned simultaneously. In this paper, we present a general channel pruning approach that can be applied to various complicated structures. Particularly, we propose a layer grouping algorithm to find coupled channels automatically. Then we derive a unified metric based on Fisher information to evaluate the importance of a single channel and coupled channels. Moreover, we find that inference speedup on GPUs is more correlated with the reduction of memory rather than FLOPs, and thus we employ the memory reduction of each channel to normalize the importance. Our method can be used to prune any structures including those with coupled channels. We conduct extensive experiments on various backbones, including the classic ResNet and ResNeXt, mobile-friendly MobileNetV2, and the NAS-based RegNet, both on image classification and object detection which is under-explored. Experimental results validate that our method can effectively prune sophisticated networks, boosting inference speed without sacrificing accuracy.",0
"The reduction of memory and computation cost during inference can be achieved through network compression, which has been the subject of extensive research. However, prior approaches have not adequately addressed complex structures, such as residual connections, group/depth-wise convolution, and feature pyramid networks, which require simultaneous pruning of multiple layers. In this study, we present a channel pruning method that is applicable to various sophisticated structures. Our approach utilizes a layer grouping algorithm to automatically identify coupled channels and a unified metric based on Fisher information to assess the importance of single and coupled channels. We also discovered that memory reduction is more closely related to GPU inference speedup than FLOPs, and therefore, we normalized channel importance based on memory reduction. Our method is suitable for pruning any network architecture, including those with coupled channels, and has been tested on various backbone models, such as ResNet, ResNeXt, MobileNetV2, and RegNet, for image classification and object detection tasks. The experimental results show that our method effectively prunes advanced networks, improving inference speed without compromising accuracy.",1
"Object detection in three-dimensional (3D) space attracts much interest from academia and industry since it is an essential task in AI-driven applications such as robotics, autonomous driving, and augmented reality. As the basic format of 3D data, the point cloud can provide detailed geometric information about the objects in the original 3D space. However, due to 3D data's sparsity and unorderedness, specially designed networks and modules are needed to process this type of data. Attention mechanism has achieved impressive performance in diverse computer vision tasks; however, it is unclear how attention modules would affect the performance of 3D point cloud object detection and what sort of attention modules could fit with the inherent properties of 3D data. This work investigates the role of the attention mechanism in 3D point cloud object detection and provides insights into the potential of different attention modules. To achieve that, we comprehensively investigate classical 2D attentions, novel 3D attentions, including the latest point cloud transformers on SUN RGB-D and ScanNetV2 datasets. Based on the detailed experiments and analysis, we conclude the effects of different attention modules. This paper is expected to serve as a reference source for benefiting attention-embedded 3D point cloud object detection. The code and trained models are available at: https://github.com/ShiQiu0419/attentions_in_3D_detection.",0
"The detection of objects in 3D space is a highly sought-after area of research in both academia and industry, as it is a crucial task in the development of AI-driven applications like robotics, autonomous driving, and augmented reality. Point cloud data is the fundamental format of 3D data, providing detailed geometric information about objects in their original 3D space. However, due to the sparsity and unordered nature of 3D data, specialized networks and modules are required to process this type of data. While attention mechanisms have demonstrated impressive performance in various computer vision tasks, it is unclear how these modules impact 3D point cloud object detection, and which ones are suitable for the inherent properties of 3D data. This study explores the role of attention mechanisms in 3D point cloud object detection, offering insights into the potential of different attention modules. We comprehensively examine both classical 2D attentions and novel 3D attentions, including the latest point cloud transformers, on SUN RGB-D and ScanNetV2 datasets. Through detailed experiments and analysis, we provide conclusions on the effects of different attention modules. This paper will serve as a valuable reference for attention-embedded 3D point cloud object detection. The code and trained models can be found at: https://github.com/ShiQiu0419/attentions_in_3D_detection.",1
"Video captioning is one of the challenging problems at the intersection of vision and language, having many real-life applications in video retrieval, video surveillance, assisting visually challenged people, Human-machine interface, and many more. Recent deep learning-based methods have shown promising results but are still on the lower side than other vision tasks (such as image classification, object detection). A significant drawback with existing video captioning methods is that they are optimized over cross-entropy loss function, which is uncorrelated to the de facto evaluation metrics (BLEU, METEOR, CIDER, ROUGE).In other words, cross-entropy is not a proper surrogate of the true loss function for video captioning. This paper addresses the drawback by introducing a dynamic loss network (DLN), which provides an additional feedback signal that directly reflects the evaluation metrics. Our results on Microsoft Research Video Description Corpus (MSVD) and MSR-Video to Text (MSRVTT) datasets outperform previous methods.",0
"The intersection of vision and language presents a challenging problem in video captioning, with various practical applications such as video retrieval, video surveillance, and assisting visually challenged individuals. Although recent deep learning-based approaches have demonstrated promise, they still lag behind other vision tasks like image classification and object detection. A significant limitation of current video captioning methods is their optimization over the cross-entropy loss function, which does not correlate with the accurate evaluation metrics (BLEU, METEOR, CIDER, ROUGE). This paper proposes a dynamic loss network (DLN) to address this drawback by introducing an additional feedback signal that directly reflects the evaluation metrics. Our results on Microsoft Research Video Description Corpus (MSVD) and MSR-Video to Text (MSRVTT) datasets surpass previous methods.",1
"Latest CNN-based object detection models are quite accurate but require a high-performance GPU to run in real-time. They still are heavy in terms of memory size and speed for an embedded system with limited memory space. Since the object detection for autonomous system is run on an embedded processor, it is preferable to compress the detection network as light as possible while preserving the detection accuracy. There are several popular lightweight detection models but their accuracy is too low for safe driving applications. Therefore, this paper proposes a new object detection model, referred as YOffleNet, which is compressed at a high ratio while minimizing the accuracy loss for real-time and safe driving application on an autonomous system. The backbone network architecture is based on YOLOv4, but we could compress the network greatly by replacing the high-calculation-load CSP DenseNet with the lighter modules of ShuffleNet. Experiments with KITTI dataset showed that the proposed YOffleNet is compressed by 4.7 times than the YOLOv4-s that could achieve as fast as 46 FPS on an embedded GPU system(NVIDIA Jetson AGX Xavier). Compared to the high compression ratio, the accuracy is reduced slightly to 85.8% mAP, that is only 2.6% lower than YOLOv4-s. Thus, the proposed network showed a high potential to be deployed on the embedded system of the autonomous system for the real-time and accurate object detection applications.",0
"Although the latest CNN-based object detection models are quite accurate, they require a high-performance GPU to run in real-time and are often too heavy in terms of memory size and speed for use in an embedded system with limited memory space. To ensure safe driving in autonomous systems, it is preferable to compress the detection network as much as possible while maintaining accuracy. While there are several lightweight detection models available, their accuracy is too low for safe driving applications. Accordingly, this paper proposes a new object detection model called YOffleNet, which is highly compressed while minimizing the accuracy loss for real-time and safe driving applications on an autonomous system. The backbone network architecture is based on YOLOv4, but the network has been greatly compressed by replacing the high-calculation-load CSP DenseNet with the lighter modules of ShuffleNet. Experiments with KITTI dataset showed that the proposed YOffleNet is compressed by 4.7 times compared to YOLOv4-s and can achieve a speed of up to 46 FPS on an embedded GPU system (NVIDIA Jetson AGX Xavier). Despite the high compression ratio, the accuracy is only slightly reduced to 85.8% mAP, which is only 2.6% lower than YOLOv4-s. Therefore, the proposed network has a high potential for deployment on the embedded system of autonomous systems for real-time and accurate object detection applications.",1
"3D object detection is a key component of many robotic applications such as self-driving vehicles. While many approaches rely on expensive 3D sensors such as LiDAR to produce accurate 3D estimates, methods that exploit stereo cameras have recently shown promising results at a lower cost. Existing approaches tackle this problem in two steps: first depth estimation from stereo images is performed to produce a pseudo LiDAR point cloud, which is then used as input to a 3D object detector. However, this approach is suboptimal due to the representation mismatch, as the two tasks are optimized in two different metric spaces. In this paper we propose a model that unifies these two tasks and performs them in the same metric space. Specifically, we directly construct a pseudo LiDAR feature volume (PLUME) in 3D space, which is then used to solve both depth estimation and object detection tasks. Our approach achieves state-of-the-art performance with much faster inference times when compared to existing methods on the challenging KITTI benchmark.",0
"Many robotic applications, such as self-driving vehicles, require 3D object detection. However, accurate 3D estimates often rely on expensive 3D sensors like LiDAR. Recently, methods using stereo cameras have shown promising results at a lower cost. However, the existing approach of depth estimation from stereo images then using a 3D object detector is suboptimal due to a representation mismatch. Our paper proposes a model that unifies these two tasks in the same metric space by constructing a pseudo LiDAR feature volume (PLUME) in 3D space. This approach achieves state-of-the-art performance with faster inference times when compared to existing methods on the challenging KITTI benchmark.",1
"Transformers have made much progress in dealing with visual tasks. However, existing vision transformers still do not possess an ability that is important to visual input: building the attention among features of different scales. The reasons for this problem are two-fold: (1) Input embeddings of each layer are equal-scale without cross-scale features; (2) Some vision transformers sacrifice the small-scale features of embeddings to lower the cost of the self-attention module. To make up this defect, we propose Cross-scale Embedding Layer (CEL) and Long Short Distance Attention (LSDA). In particular, CEL blends each embedding with multiple patches of different scales, providing the model with cross-scale embeddings. LSDA splits the self-attention module into a short-distance and long-distance one, also lowering the cost but keeping both small-scale and large-scale features in embeddings. Through these two designs, we achieve cross-scale attention. Besides, we propose dynamic position bias for vision transformers to make the popular relative position bias apply to variable-sized images. Based on these proposed modules, we construct our vision architecture called CrossFormer. Experiments show that CrossFormer outperforms other transformers on several representative visual tasks, especially object detection and segmentation. The code has been released: https://github.com/cheerss/CrossFormer.",0
"Although transformers have made significant advancements in visual tasks, they still lack the ability to build attention among features of varying scales, which is crucial for visual input. This is due to two reasons: the input embeddings of each layer are equal-scale without cross-scale features, and some vision transformers sacrifice small-scale features to reduce the cost of the self-attention module. To address this issue, we introduce the Cross-scale Embedding Layer (CEL) and Long Short Distance Attention (LSDA) to provide cross-scale embeddings and maintain both small- and large-scale features in embeddings while reducing costs. Our proposed vision architecture, CrossFormer, uses these modules and dynamic position bias to achieve cross-scale attention. We conducted experiments that demonstrate CrossFormer's superior performance in several visual tasks, including object detection and segmentation. The code is available at https://github.com/cheerss/CrossFormer.",1
"LiDAR point clouds collected from a moving vehicle are functions of its trajectories, because the sensor motion needs to be compensated to avoid distortions. When autonomous vehicles are sending LiDAR point clouds to deep networks for perception and planning, could the motion compensation consequently become a wide-open backdoor in those networks, due to both the adversarial vulnerability of deep learning and GPS-based vehicle trajectory estimation that is susceptible to wireless spoofing? We demonstrate such possibilities for the first time: instead of directly attacking point cloud coordinates which requires tampering with the raw LiDAR readings, only adversarial spoofing of a self-driving car's trajectory with small perturbations is enough to make safety-critical objects undetectable or detected with incorrect positions. Moreover, polynomial trajectory perturbation is developed to achieve a temporally-smooth and highly-imperceptible attack. Extensive experiments on 3D object detection have shown that such attacks not only lower the performance of the state-of-the-art detectors effectively, but also transfer to other detectors, raising a red flag for the community. The code is available on https://ai4ce.github.io/FLAT/.",0
"The point clouds obtained from LiDAR while a vehicle is in motion are dependent on the vehicle's movement because the sensor must be adjusted to prevent distortions. When autonomous vehicles use LiDAR point clouds for perception and planning, the motion compensation could become a potential weakness in deep networks due to the susceptibility of GPS-based vehicle trajectory estimation to wireless spoofing and the adversarial vulnerability of deep learning. Our research presents the possibility that small perturbations in the trajectory of a self-driving car can make safety-critical objects undetectable or detected with incorrect positions, without tampering with the raw LiDAR readings. We have developed a polynomial trajectory perturbation that is smooth and imperceptible, and our experiments on 3D object detection demonstrate that such attacks significantly reduce the effectiveness of state-of-the-art detectors and are transferable to other detectors. This raises concerns for the community, and our code is available at https://ai4ce.github.io/FLAT/.",1
"Transformer has achieved great success in computer vision, while how to split patches in an image remains a problem. Existing methods usually use a fixed-size patch embedding which might destroy the semantics of objects. To address this problem, we propose a new Deformable Patch (DePatch) module which learns to adaptively split the images into patches with different positions and scales in a data-driven way rather than using predefined fixed patches. In this way, our method can well preserve the semantics in patches. The DePatch module can work as a plug-and-play module, which can easily be incorporated into different transformers to achieve an end-to-end training. We term this DePatch-embedded transformer as Deformable Patch-based Transformer (DPT) and conduct extensive evaluations of DPT on image classification and object detection. Results show DPT can achieve 81.9% top-1 accuracy on ImageNet classification, and 43.7% box mAP with RetinaNet, 44.3% with Mask R-CNN on MSCOCO object detection. Code has been made available at: https://github.com/CASIA-IVA-Lab/DPT .",0
"While Transformer has been successful in computer vision, the issue of patch splitting in images persists. Current methods employ fixed-size patch embedding, which can compromise object semantics. To overcome this, we suggest the use of the Deformable Patch (DePatch) module, which adaptively splits images into patches with varying positions and scales in a data-driven manner, preserving semantic integrity. The DePatch module can be integrated into various transformers for seamless training, resulting in the Deformable Patch-based Transformer (DPT). Our evaluations of DPT demonstrate an 81.9% top-1 accuracy on ImageNet classification and 43.7% box mAP with RetinaNet, 44.3% with Mask R-CNN on MSCOCO object detection. Our code is available at: https://github.com/CASIA-IVA-Lab/DPT.",1
"As an emerging data modal with precise distance sensing, LiDAR point clouds have been placed great expectations on 3D scene understanding. However, point clouds are always sparsely distributed in the 3D space, and with unstructured storage, which makes it difficult to represent them for effective 3D object detection. To this end, in this work, we regard point clouds as hollow-3D data and propose a new architecture, namely Hallucinated Hollow-3D R-CNN ($\text{H}^2$3D R-CNN), to address the problem of 3D object detection. In our approach, we first extract the multi-view features by sequentially projecting the point clouds into the perspective view and the bird-eye view. Then, we hallucinate the 3D representation by a novel bilaterally guided multi-view fusion block. Finally, the 3D objects are detected via a box refinement module with a novel Hierarchical Voxel RoI Pooling operation. The proposed $\text{H}^2$3D R-CNN provides a new angle to take full advantage of complementary information in the perspective view and the bird-eye view with an efficient framework. We evaluate our approach on the public KITTI Dataset and Waymo Open Dataset. Extensive experiments demonstrate the superiority of our method over the state-of-the-art algorithms with respect to both effectiveness and efficiency. The code will be made available at \url{https://github.com/djiajunustc/H-23D_R-CNN}.",0
"LiDAR point clouds are a promising data model for precise distance sensing and have great potential for 3D scene understanding. However, due to their sparsely distributed nature and unstructured storage, it is challenging to effectively represent them for 3D object detection. In this study, we propose a new architecture called Hallucinated Hollow-3D R-CNN ($\text{H}^2$3D R-CNN) that treats point clouds as hollow-3D data. Our approach involves extracting multi-view features by projecting the point clouds into the perspective and bird-eye views, followed by a novel bilaterally guided multi-view fusion block that hallucinates the 3D representation. Our method also includes a box refinement module with a Hierarchical Voxel RoI Pooling operation to detect 3D objects. By taking full advantage of complementary information in the perspective and bird-eye views, our proposed $\text{H}^2$3D R-CNN offers an efficient framework for 3D object detection. We evaluated our approach on the KITTI Dataset and Waymo Open Dataset, and our experiments showed that our method outperformed state-of-the-art algorithms in terms of both effectiveness and efficiency. The code for our approach can be found at \url{https://github.com/djiajunustc/H-23D_R-CNN}.",1
"Nowadays, plenty of deep learning technologies are being applied to all aspects of autonomous driving with promising results. Among them, object detection is the key to improve the ability of an autonomous agent to perceive its environment so that it can (re)act. However, previous vision-based object detectors cannot achieve satisfactory performance under real-time driving scenarios. To remedy this, we present the real-time steaming perception system in this paper, which is also the 2nd Place solution of Streaming Perception Challenge (Workshop on Autonomous Driving at CVPR 2021) for the detection-only track. Unlike traditional object detection challenges, which focus mainly on the absolute performance, streaming perception task requires achieving a balance of accuracy and latency, which is crucial for real-time autonomous driving. We adopt YOLOv5 as our basic framework, data augmentation, Bag-of-Freebies, and Transformer are adopted to improve streaming object detection performance with negligible extra inference cost. On the Argoverse-HD test set, our method achieves 33.2 streaming AP (34.6 streaming AP verified by the organizer) under the required hardware. Its performance significantly surpasses the fixed baseline of 13.6 (host team), demonstrating the potentiality of application.",0
"Currently, deep learning technologies are being utilized in various areas of autonomous driving, resulting in promising outcomes. Object detection plays a critical role in enhancing an autonomous agent's ability to perceive its surroundings and take necessary actions. However, previous vision-based object detectors have failed to produce satisfactory results in real-time driving scenarios. To address this issue, this paper introduces the real-time streaming perception system, which secured the 2nd Place in the Streaming Perception Challenge (Workshop on Autonomous Driving at CVPR 2021) for the detection-only track. Unlike conventional object detection challenges that prioritize absolute performance, the streaming perception task demands a balance of accuracy and latency, which is crucial for real-time autonomous driving. Our approach employs YOLOv5 as the basic framework, and data augmentation, Bag-of-Freebies, and Transformer are implemented to enhance streaming object detection results with negligible extra inference cost. On the Argoverse-HD test set, our method achieves 33.2 streaming AP (34.6 streaming AP verified by the organizer) with the required hardware, significantly outperforming the fixed baseline of 13.6 (host team) and demonstrating its potential application.",1
"Images acquired by computer vision systems under low light conditions have multiple characteristics like high noise, lousy illumination, reflectance, and bad contrast, which make object detection tasks difficult. Much work has been done to enhance images using various pixel manipulation techniques, as well as deep neural networks - some focused on improving the illumination, while some on reducing the noise. Similarly, considerable research has been done in object detection neural network models. In our work, we break down the problem into two phases: 1)First, we explore which image enhancement algorithm is more suited for object detection tasks, where accurate feature retrieval is more important than good image quality. Specifically, we look at basic histogram equalization techniques and unpaired image translation techniques. 2)In the second phase, we explore different object detection models that can be applied to the enhanced image. We conclude by comparing all results, calculating mean average precisions (mAP), and giving some directions for future work.",0
"Computer vision systems face several challenges when capturing images in low light conditions, including poor illumination, high noise, reflectance, and low contrast. To address these challenges, researchers have developed various techniques, including pixel manipulation and deep neural networks, to enhance images. However, object detection tasks remain difficult due to these characteristics. In this study, we divide the problem into two phases. Firstly, we evaluate which image enhancement algorithm is best suited for accurate feature retrieval in object detection tasks. We compare basic histogram equalization techniques and unpaired image translation techniques. Secondly, we explore different object detection models that can be applied to the enhanced images. Finally, we compare the results, calculating mean average precisions (mAP), and suggest directions for future research.",1
"In a typical multi-label setting, a picture contains on average few positive labels, and many negative ones. This positive-negative imbalance dominates the optimization process, and can lead to under-emphasizing gradients from positive labels during training, resulting in poor accuracy. In this paper, we introduce a novel asymmetric loss (""ASL""), which operates differently on positive and negative samples. The loss enables to dynamically down-weights and hard-thresholds easy negative samples, while also discarding possibly mislabeled samples. We demonstrate how ASL can balance the probabilities of different samples, and how this balancing is translated to better mAP scores. With ASL, we reach state-of-the-art results on multiple popular multi-label datasets: MS-COCO, Pascal-VOC, NUS-WIDE and Open Images. We also demonstrate ASL applicability for other tasks, such as single-label classification and object detection. ASL is effective, easy to implement, and does not increase the training time or complexity.   Implementation is available at: https://github.com/Alibaba-MIIL/ASL.",0
"Typically, in multi-label scenarios, images have few positive labels and many negative labels on average. This imbalance can hinder the optimization process, leading to inadequate accuracy due to the under-emphasis of positive label gradients during training. In this study, we propose an innovative asymmetric loss (""ASL"") that applies different operations to positive and negative samples. The loss allows for the dynamic down-weighting and hard-thresholding of easy negative samples, as well as the removal of potentially mislabeled samples. Our findings demonstrate that ASL balances sample probabilities, resulting in improved mAP scores. ASL also achieves state-of-the-art results on several popular multi-label datasets, including MS-COCO, Pascal-VOC, NUS-WIDE, and Open Images. Furthermore, we showcase how ASL can be utilized for single-label classification and object detection tasks. ASL is an effective and straightforward approach that does not increase training time or complexity. Our implementation is freely available at https://github.com/Alibaba-MIIL/ASL.",1
"As a crucial task of autonomous driving, 3D object detection has made great progress in recent years. However, monocular 3D object detection remains a challenging problem due to the unsatisfactory performance in depth estimation. Most existing monocular methods typically directly regress the scene depth while ignoring important relationships between the depth and various geometric elements (e.g. bounding box sizes, 3D object dimensions, and object poses). In this paper, we propose to learn geometry-guided depth estimation with projective modeling to advance monocular 3D object detection. Specifically, a principled geometry formula with projective modeling of 2D and 3D depth predictions in the monocular 3D object detection network is devised. We further implement and embed the proposed formula to enable geometry-aware deep representation learning, allowing effective 2D and 3D interactions for boosting the depth estimation. Moreover, we provide a strong baseline through addressing substantial misalignment between 2D annotation and projected boxes to ensure robust learning with the proposed geometric formula. Experiments on the KITTI dataset show that our method remarkably improves the detection performance of the state-of-the-art monocular-based method without extra data by 2.80% on the moderate test setting. The model and code will be released at https://github.com/YinminZhang/MonoGeo.",0
"Recent years have seen significant progress in 3D object detection, which is a critical aspect of autonomous driving. However, monocular 3D object detection remains challenging due to unsatisfactory depth estimation. Existing methods primarily focus on direct regression of scene depth, neglecting essential relationships between depth and geometric elements such as bounding box sizes, 3D object dimensions, and object poses. Our paper proposes an approach that involves learning geometry-guided depth estimation using projective modeling to advance monocular 3D object detection. We introduce a principled geometry formula with projective modeling of 2D and 3D depth predictions in the monocular 3D object detection network. We integrate the proposed formula to enable geometry-aware deep representation learning, allowing effective 2D and 3D interactions for boosting depth estimation. Additionally, we provide a strong baseline by addressing substantial misalignment between 2D annotation and projected boxes to ensure robust learning with the proposed geometric formula. Our experiments on the KITTI dataset show that our method improves the detection performance of state-of-the-art monocular-based methods by 2.80% on the moderate test setting without extra data. The model and code are available at https://github.com/YinminZhang/MonoGeo.",1
"The fact that there exists a gap between low-level features and semantic meanings of images, called the semantic gap, is known for decades. Resolution of the semantic gap is a long standing problem. The semantic gap problem is reviewed and a survey on recent efforts in bridging the gap is made in this work. Most importantly, we claim that the semantic gap is primarily bridged through supervised learning today. Experiences are drawn from two application domains to illustrate this point: 1) object detection and 2) metric learning for content-based image retrieval (CBIR). To begin with, this paper offers a historical retrospective on supervision, makes a gradual transition to the modern data-driven methodology and introduces commonly used datasets. Then, it summarizes various supervision methods to bridge the semantic gap in the context of object detection and metric learning.",0
"For decades, researchers have been aware of the semantic gap - the disconnect between low-level image features and their semantic meaning. Despite efforts to bridge this gap, it remains a persistent problem. This paper provides a review of recent attempts to address the semantic gap, with a focus on the role of supervised learning. Through case studies in object detection and content-based image retrieval, we demonstrate that supervised learning is the key to bridging the semantic gap. The paper begins with a historical overview of supervision, then transitions to modern data-driven approaches and introduces relevant datasets. The various methods of supervised learning for object detection and metric learning are also summarized.",1
"Modern top-performing object detectors depend heavily on backbone networks, whose advances bring consistent performance gains through exploring more effective network structures. In this paper, we propose a novel and flexible backbone framework, namely CBNetV2, to construct high-performance detectors using existing open-sourced pre-trained backbones under the pre-training fine-tuning paradigm. In particular, CBNetV2 architecture groups multiple identical backbones, which are connected through composite connections. Specifically, it integrates the high- and low-level features of multiple backbone networks and gradually expands the receptive field to more efficiently perform object detection. We also propose a better training strategy with assistant supervision for CBNet-based detectors. Without additional pre-training of the composite backbone, CBNetV2 can be adapted to various backbones (CNN-based vs. Transformer-based) and head designs of most mainstream detectors (one-stage vs. two-stage, anchor-based vs. anchor-free-based). Experiments provide strong evidence that, compared with simply increasing the depth and width of the network, CBNetV2 introduces a more efficient, effective, and resource-friendly way to build high-performance backbone networks. Particularly, our Dual-Swin-L achieves 59.4% box AP and 51.6% mask AP on COCO test-dev under the single-model and single-scale testing protocol, which is significantly better than the state-of-the-art result (57.7% box AP and 50.2% mask AP) achieved by Swin-L, while the training schedule is reduced by 6$\times$. With multi-scale testing, we push the current best single model result to a new record of 60.1% box AP and 52.3% mask AP without using extra training data. Code is available at https://github.com/VDIGPKU/CBNetV2.",0
"Current high-performing object detection systems rely heavily on backbone networks, whose advancements lead to consistent performance improvements by exploring more effective network structures. In this study, we introduce a novel and flexible backbone framework called CBNetV2, which utilizes existing open-sourced pre-trained backbones under the pre-training fine-tuning paradigm to construct high-performance detectors. CBNetV2 architecture groups multiple identical backbones through composite connections, integrating high- and low-level features of multiple backbone networks, and gradually expanding the receptive field to perform object detection more efficiently. We also propose a better training strategy with assistant supervision for CBNet-based detectors. CBNetV2 can be adapted to various backbones and head designs of most mainstream detectors without additional pre-training of the composite backbone. Our experiments demonstrate that CBNetV2 offers a more efficient, effective, and resource-friendly approach to building high-performance backbone networks compared to simply increasing the depth and width of the network. Our Dual-Swin-L model achieves significantly better results than the state-of-the-art model Swin-L, while reducing the training schedule by 6 times. We also set a new record for the best single-model result with multi-scale testing without using extra training data. The code is available at https://github.com/VDIGPKU/CBNetV2.",1
"The paper describes a method for measuring the similarity and symmetry of an image annotated with bounding boxes indicating image objects. The latter representation became popular recently due to the rapid development of fast and efficient deep-learning-based object-detection methods. The proposed approach allows for comparing sets of bounding boxes to estimate the degree of similarity of their underlying images. It is based on the fuzzy approach that uses the fuzzy mutual position (FMP) matrix to describe spatial composition and relations between bounding boxes within an image. A method of computing the similarity of two images described by their FMP matrices is proposed and the algorithm of its computation. It outputs the single scalar value describing the degree of content-based image similarity. By modifying the method`s parameters, instead of similarity, the reflectional symmetry of object composition may also be measured. The proposed approach allows for measuring differences in objects` composition of various intensities. It is also invariant to translation and scaling and - in case of symmetry detection - position and orientation of the symmetry axis. A couple of examples illustrate the method.",0
"The article details a technique for gauging the likeness and balance of an image that has been labeled with bounding boxes to indicate objects within it. This approach has gained traction recently because of the swift growth of object-detection methods that utilize deep learning and are highly efficient. Using this method, collections of bounding boxes can be compared to determine the similarity of their underlying images. The approach employs a fuzzy method that relies on a fuzzy mutual position (FMP) matrix to depict the spatial composition and connections between bounding boxes in an image. The article presents a method for determining the similarity of two images based on their FMP matrices, as well as the algorithm that must be used to calculate it. The output is a single scalar figure that conveys the degree of similarity between images based on their content. By altering the method's parameters, the reflective symmetry of object composition can also be measured rather than similarity. This methodology can detect differences in the composition of objects of varying strengths. It is also invariant to scaling and translation and, in the case of symmetry detection, to the position and orientation of the symmetry axis. The article presents a few examples to illustrate the method.",1
"Attempts of learning from hierarchical taxonomies in computer vision have been mostly focusing on image classification. Though ways of best harvesting learning improvements from hierarchies in classification are far from being solved, there is a need to target these problems in other vision tasks such as object detection. As progress on the classification side is often dependent on hierarchical cross-entropy losses, novel detection architectures using sigmoid as an output function instead of softmax cannot easily apply these advances, requiring novel methods in detection. In this work we establish a theoretical framework based on probability and set theory for extracting parent predictions and a hierarchical loss that can be used across tasks, showing results across classification and detection benchmarks and opening up the possibility of hierarchical learning for sigmoid-based detection architectures.",0
"Efforts to utilize hierarchical taxonomies in computer vision have primarily centered on image classification, but there is a need to extend this approach to other vision tasks like object detection. While the challenges of effectively leveraging hierarchies in classification persist, the use of sigmoid output functions in detection architectures presents new obstacles, as advances in classification are often reliant on hierarchical cross-entropy losses. To address this issue, we present a theoretical framework based on probability and set theory that enables the extraction of parent predictions and a hierarchical loss that can be applied across tasks. Our approach yields promising results in both classification and detection benchmarks, and offers a pathway for hierarchical learning in sigmoid-based detection architectures.",1
"Object recognition in unseen indoor environments remains a challenging problem for visual perception of mobile robots. In this letter, we propose the use of topologically persistent features, which rely on the objects' shape information, to address this challenge. In particular, we extract two kinds of features, namely, sparse persistence image (PI) and amplitude, by applying persistent homology to multi-directional height function-based filtrations of the cubical complexes representing the object segmentation maps. The features are then used to train a fully connected network for recognition. For performance evaluation, in addition to a widely used shape dataset and a benchmark indoor scenes dataset, we collect a new dataset, comprising scene images from two different environments, namely, a living room and a mock warehouse. The scenes are captured using varying camera poses under different illumination conditions and include up to five different objects from a given set of fourteen objects. On the benchmark indoor scenes dataset, sparse PI features show better recognition performance in unseen environments than the features learned using the widely used ResNetV2-56 and EfficientNet-B4 models. Further, they provide slightly higher recall and accuracy values than Faster R-CNN, an end-to-end object detection method, and its state-of-the-art variant, Domain Adaptive Faster R-CNN. The performance of our methods also remains relatively unchanged from the training environment (living room) to the unseen environment (mock warehouse) in the new dataset. In contrast, the performance of the object detection methods drops substantially. We also implement the proposed method on a real-world robot to demonstrate its usefulness.",0
"Mobile robots face a challenging task in recognizing objects in unfamiliar indoor settings. To tackle this problem, we suggest using topologically persistent features that depend on the shape information of the objects. Our approach involves extracting two types of features, namely, sparse persistence image (PI) and amplitude, by applying persistent homology to multi-directional height function-based filtrations of the cubical complexes representing the object segmentation maps. These features are then used to train a fully connected network for recognition. To evaluate the performance of our method, we collected a new dataset comprising scene images from two different environments, namely, a living room and a mock warehouse. The scenes were captured using varying camera poses under different illumination conditions and included up to five different objects from a set of fourteen objects. Our results show that sparse PI features outperform the features learned using widely used models such as ResNetV2-56 and EfficientNet-B4 in recognizing objects in unseen environments. Moreover, they provide slightly higher recall and accuracy values than end-to-end object detection methods such as Faster R-CNN and its state-of-the-art variant, Domain Adaptive Faster R-CNN. The performance of our method remains relatively consistent from the training environment (living room) to the unseen environment (mock warehouse) in the new dataset, whereas the performance of object detection methods drops significantly. We also demonstrate the utility of our proposed method on a real-world robot.",1
"This paper presents a Simple and effective unsupervised adaptation method for Robust Object Detection (SimROD). To overcome the challenging issues of domain shift and pseudo-label noise, our method integrates a novel domain-centric augmentation method, a gradual self-labeling adaptation procedure, and a teacher-guided fine-tuning mechanism. Using our method, target domain samples can be leveraged to adapt object detection models without changing the model architecture or generating synthetic data. When applied to image corruptions and high-level cross-domain adaptation benchmarks, our method outperforms prior baselines on multiple domain adaptation benchmarks. SimROD achieves new state-of-the-art on standard real-to-synthetic and cross-camera setup benchmarks. On the image corruption benchmark, models adapted with our method achieved a relative robustness improvement of 15-25% AP50 on Pascal-C and 5-6% AP on COCO-C and Cityscapes-C. On the cross-domain benchmark, our method outperformed the best baseline performance by up to 8% AP50 on Comic dataset and up to 4% on Watercolor dataset.",0
"In this article, we introduce SimROD, a straightforward and effective unsupervised adaptation strategy for robust object detection. Our method addresses the complex challenges posed by domain shift and pseudo-label noise by incorporating a domain-centric augmentation method, a gradual self-labeling adaptation process, and a teacher-guided fine-tuning mechanism. With our approach, object detection models can adapt to target domain samples without altering the model architecture or producing synthetic data. Our method surpasses previous benchmarks on various domain adaptation scenarios, including image corruptions and high-level cross-domain benchmarks. SimROD sets new state-of-the-art records on standard real-to-synthetic and cross-camera setup benchmarks. We observed a relative robustness enhancement of 15-25% AP50 on Pascal-C and 5-6% AP on COCO-C and Cityscapes-C on the image corruption benchmark with our adapted models. On the cross-domain benchmark, our method outperformed the best baseline performance by up to 8% AP50 on the Comic dataset and up to 4% on the Watercolor dataset.",1
"Current geometry-based monocular 3D object detection models can efficiently detect objects by leveraging perspective geometry, but their performance is limited due to the absence of accurate depth information. Though this issue can be alleviated in a depth-based model where a depth estimation module is plugged to predict depth information before 3D box reasoning, the introduction of such module dramatically reduces the detection speed. Instead of training a costly depth estimator, we propose a rendering module to augment the training data by synthesizing images with virtual-depths. The rendering module takes as input the RGB image and its corresponding sparse depth image, outputs a variety of photo-realistic synthetic images, from which the detection model can learn more discriminative features to adapt to the depth changes of the objects. Besides, we introduce an auxiliary module to improve the detection model by jointly optimizing it through a depth estimation task. Both modules are working in the training time and no extra computation will be introduced to the detection model. Experiments show that by working with our proposed modules, a geometry-based model can represent the leading accuracy on the KITTI 3D detection benchmark.",0
"Although current monocular 3D object detection models that utilize perspective geometry can efficiently detect objects, their accuracy is limited due to the lack of precise depth information. While depth-based models can address this issue by incorporating a depth estimation module, this often results in slower detection speeds. Instead of using a costly depth estimator, we propose a rendering module that can augment training data by generating images with virtual depths. This module takes an RGB image and its corresponding sparse depth image as input and produces various realistic synthetic images that can help the detection model learn more distinct features and adapt to depth changes. We also introduce an auxiliary module that can enhance the detection model's performance by jointly optimizing it through a depth estimation task. Both modules operate during training and do not add extra computational burden to the detection model. Our experiments show that our proposed modules can significantly improve the accuracy of geometry-based models on the KITTI 3D detection benchmark.",1
"Confluence is a novel non-Intersection over Union (IoU) alternative to Non-Maxima Suppression (NMS) in bounding box post-processing in object detection. It overcomes the inherent limitations of IoU-based NMS variants to provide a more stable, consistent predictor of bounding box clustering by using a normalized Manhattan Distance inspired proximity metric to represent bounding box clustering. Unlike Greedy and Soft NMS, it does not rely solely on classification confidence scores to select optimal bounding boxes, instead selecting the box which is closest to every other box within a given cluster and removing highly confluent neighboring boxes. Confluence is experimentally validated on the MS COCO and CrowdHuman benchmarks, improving Average Precision by up to 2.3-3.8% and Average Recall by up to 5.3-7.2% when compared against de-facto standard and state of the art NMS variants. Quantitative results are supported by extensive qualitative analysis and threshold sensitivity analysis experiments support the conclusion that Confluence is more robust than NMS variants. Confluence represents a paradigm shift in bounding box processing, with potential to replace IoU in bounding box regression processes.",0
"Confluence is an innovative alternative to Non-Maxima Suppression (NMS) in bounding box post-processing for object detection. It addresses the limitations of IoU-based NMS methods by utilizing a normalized Manhattan Distance proximity metric to accurately predict bounding box clustering. Unlike Greedy and Soft NMS, Confluence does not rely solely on classification confidence scores to select optimal bounding boxes. Instead, it selects the box closest to every other box in a given cluster and removes highly confluent neighboring boxes. Experimental validation on the MS COCO and CrowdHuman benchmarks shows that Confluence improves Average Precision by up to 2.3-3.8% and Average Recall by up to 5.3-7.2% when compared to standard and state-of-the-art NMS variants. Qualitative and threshold sensitivity analysis experiments further support that Confluence is more robust than NMS variants and has the potential to replace IoU in bounding box regression processes, representing a paradigm shift in bounding box processing.",1
"To boost the object grabbing capability of underwater robots for open-sea farming, we propose a new dataset (UDD) consisting of three categories (seacucumber, seaurchin, and scallop) with 2,227 images. To the best of our knowledge, it is the first 4K HD dataset collected in a real open-sea farm. We also propose a novel Poisson-blending Generative Adversarial Network (Poisson GAN) and an efficient object detection network (AquaNet) to address two common issues within related datasets: the class-imbalance problem and the problem of mass small object, respectively. Specifically, Poisson GAN combines Poisson blending into its generator and employs a new loss called Dual Restriction loss (DR loss), which supervises both implicit space features and image-level features during training to generate more realistic images. By utilizing Poisson GAN, objects of minority class like seacucumber or scallop could be added into an image naturally and annotated automatically, which could increase the loss of minority classes during training detectors to eliminate the class-imbalance problem; AquaNet is a high-efficiency detector to address the problem of detecting mass small objects from cloudy underwater pictures. Within it, we design two efficient components: a depth-wise-convolution-based Multi-scale Contextual Features Fusion (MFF) block and a Multi-scale Blursampling (MBP) module to reduce the parameters of the network to 1.3 million. Both two components could provide multi-scale features of small objects under a short backbone configuration without any loss of accuracy. In addition, we construct a large-scale augmented dataset (AUDD) and a pre-training dataset via Poisson GAN from UDD. Extensive experiments show the effectiveness of the proposed Poisson GAN, AquaNet, UDD, AUDD, and pre-training dataset.",0
"We propose a novel dataset (UDD) comprising 2,227 images of three categories (seacucumber, seaurchin, and scallop) to enhance the object grabbing capability of underwater robots for open-sea farming. Our dataset is the first 4K HD dataset collected in a real open-sea farm. To address the class-imbalance problem and the problem of detecting mass small objects, we introduce a Poisson-blending Generative Adversarial Network (Poisson GAN) and an efficient object detection network (AquaNet), respectively. Poisson GAN combines Poisson blending into its generator and employs a new loss called Dual Restriction loss (DR loss) to generate more realistic images. AquaNet is a high-efficiency detector that uses a depth-wise-convolution-based Multi-scale Contextual Features Fusion (MFF) block and a Multi-scale Blursampling (MBP) module to reduce the parameters of the network to 1.3 million, without any loss of accuracy. We also construct a large-scale augmented dataset (AUDD) and a pre-training dataset via Poisson GAN from UDD. Our experiments demonstrate the effectiveness of the proposed Poisson GAN, AquaNet, UDD, AUDD, and pre-training dataset.",1
"This paper revisits human-object interaction (HOI) recognition at image level without using supervisions of object location and human pose. We name it detection-free HOI recognition, in contrast to the existing detection-supervised approaches which rely on object and keypoint detections to achieve state of the art. With our method, not only the detection supervision is evitable, but superior performance can be achieved by properly using image-text pre-training (such as CLIP) and the proposed Log-Sum-Exp Sign (LSE-Sign) loss function. Specifically, using text embeddings of class labels to initialize the linear classifier is essential for leveraging the CLIP pre-trained image encoder. In addition, LSE-Sign loss facilitates learning from multiple labels on an imbalanced dataset by normalizing gradients over all classes in a softmax format. Surprisingly, our detection-free solution achieves 60.5 mAP on the HICO dataset, outperforming the detection-supervised state of the art by 13.4 mAP",0
"The aim of this research is to explore human-object interaction (HOI) recognition at image level, without relying on object location and human pose supervision. This approach is referred to as detection-free HOI recognition, which stands in contrast to current detection-supervised methods that require object and keypoint detections for optimal results. Detection supervision is not necessary with our method, and by utilizing image-text pre-training (such as CLIP) and the proposed Log-Sum-Exp Sign (LSE-Sign) loss function, we can achieve superior performance. To leverage the CLIP pre-trained image encoder, it is essential to use text embeddings of class labels for linear classifier initialization. Furthermore, the LSE-Sign loss function allows for learning from multiple labels on an imbalanced dataset by normalizing gradients over all classes in a softmax format. Our detection-free approach surprisingly outperforms the detection-supervised state of the art by 13.4 mAP, achieving 60.5 mAP on the HICO dataset.",1
"Advanced data augmentation strategies have widely been studied to improve the generalization ability of deep learning models. Regional dropout is one of the popular solutions that guides the model to focus on less discriminative parts by randomly removing image regions, resulting in improved regularization. However, such information removal is undesirable. On the other hand, recent strategies suggest to randomly cut and mix patches and their labels among training images, to enjoy the advantages of regional dropout without having any pointless pixel in the augmented images. We argue that such random selection strategies of the patches may not necessarily represent sufficient information about the corresponding object and thereby mixing the labels according to that uninformative patch enables the model to learn unexpected feature representation. Therefore, we propose SaliencyMix that carefully selects a representative image patch with the help of a saliency map and mixes this indicative patch with the target image, thus leading the model to learn more appropriate feature representation. SaliencyMix achieves the best known top-1 error of 21.26% and 20.09% for ResNet-50 and ResNet-101 architectures on ImageNet classification, respectively, and also improves the model robustness against adversarial perturbations. Furthermore, models that are trained with SaliencyMix help to improve the object detection performance. Source code is available at https://github.com/SaliencyMix/SaliencyMix.",0
"Numerous studies have examined advanced data augmentation methods to enhance the generalization capacity of deep learning models. One such solution is regional dropout, which boosts regularization by randomly removing image regions to focus on less informative aspects. However, this approach entails undesirable information removal. Alternatively, recent strategies propose randomly cutting and mixing patches and their labels among training images to achieve the benefits of regional dropout without pointless pixels in the augmented images. Nevertheless, we contend that randomly selecting patches may not adequately represent object information, and mixing labels based on uninformative patches may enable the model to learn unexpected feature representation. To address this, we introduce SaliencyMix, which selects a representative image patch with a saliency map's help and mixes it with the target image to encourage the model to learn more appropriate feature representation. SaliencyMix achieves superior top-1 error rates of 21.26% and 20.09% for ResNet-50 and ResNet-101 architectures, respectively, on ImageNet classification and improves model robustness against adversarial perturbations. Additionally, models trained with SaliencyMix enhance object detection performance. The source code is available at https://github.com/SaliencyMix/SaliencyMix.",1
"In this work, we propose a novel two-stage framework for the efficient 3D point cloud object detection. Instead of transforming point clouds into 2D bird eye view projections, we parse the raw point cloud data directly in the 3D space yet achieve impressive efficiency and accuracy. To achieve this goal, we propose dynamic voxelization, a method that voxellizes points at local scale on-the-fly. By doing so, we preserve the point cloud geometry with 3D voxels, and therefore waive the dependence on expensive MLPs to learn from point coordinates. On the other hand, we inherently still follow the same processing pattern as point-wise methods (e.g., PointNet) and no longer suffer from the quantization issue like conventional convolutions. For further speed optimization, we propose the grid-based downsampling and voxelization method, and provide different CUDA implementations to accommodate to the discrepant requirements during training and inference phases. We highlight our efficiency on KITTI 3D object detection dataset with 75 FPS and on Waymo Open dataset with 25 FPS inference speed with satisfactory accuracy.",0
"Our work presents a new framework for efficient 3D point cloud object detection. Instead of using 2D bird eye view projections, we directly analyze the raw point cloud data in 3D space, while maintaining impressive efficiency and accuracy. Our approach involves dynamic voxelization, which voxellizes points at a local scale on-the-fly. This preserves the point cloud geometry and eliminates the need for expensive MLPs to learn from point coordinates. Furthermore, our method follows the same processing pattern as point-wise methods, such as PointNet, and avoids the quantization issue of conventional convolutions. To optimize speed, we also propose a grid-based downsampling and voxelization method, with different CUDA implementations to meet varying requirements during training and inference phases. Our approach achieved an inference speed of 75 FPS on the KITTI 3D object detection dataset and 25 FPS on the Waymo Open dataset, with satisfactory accuracy.",1
"Object detection is a critical problem for the safe interaction between autonomous vehicles and road users. Deep-learning methodologies allowed the development of object detection approaches with better performance. However, there is still the challenge to obtain more characteristics from the objects detected in real-time. The main reason is that more information from the environment's objects can improve the autonomous vehicle capacity to face different urban situations. This paper proposes a new approach to detect static and dynamic objects in front of an autonomous vehicle. Our approach can also get other characteristics from the objects detected, like their position, velocity, and heading. We develop our proposal fusing results of the environment's interpretations achieved of YoloV3 and a Bayesian filter. To demonstrate our proposal's performance, we asses it through a benchmark dataset and real-world data obtained from an autonomous platform. We compared the results achieved with another approach.",0
"The safe interaction between autonomous vehicles and road users relies heavily on object detection, which remains a critical problem. Deep-learning methodologies have improved object detection approaches, but obtaining more real-time characteristics from detected objects remains a challenge. More information from objects in the environment can enhance an autonomous vehicle's ability to navigate different urban situations. This paper presents a novel approach for detecting static and dynamic objects in front of an autonomous vehicle, which can also extract additional characteristics such as position, velocity, and heading. Our approach combines the results of YoloV3 and a Bayesian filter to achieve better performance. We evaluate our proposal using benchmark datasets and real-world data from an autonomous platform, and compare our results with another approach.",1
"Transformers with remarkable global representation capacities achieve competitive results for visual tasks, but fail to consider high-level local pattern information in input images. In this paper, we present a generic Dual-stream Network (DS-Net) to fully explore the representation capacity of local and global pattern features for image classification. Our DS-Net can simultaneously calculate fine-grained and integrated features and efficiently fuse them. Specifically, we propose an Intra-scale Propagation module to process two different resolutions in each block and an Inter-Scale Alignment module to perform information interaction across features at dual scales. Besides, we also design a Dual-stream FPN (DS-FPN) to further enhance contextual information for downstream dense predictions. Without bells and whistles, the propsed DS-Net outperforms Deit-Small by 2.4% in terms of top-1 accuracy on ImageNet-1k and achieves state-of-the-art performance over other Vision Transformers and ResNets. For object detection and instance segmentation, DS-Net-Small respectively outperforms ResNet-50 by 6.4% and 5.5 % in terms of mAP on MSCOCO 2017, and surpasses the previous state-of-the-art scheme, which significantly demonstrates its potential to be a general backbone in vision tasks. The code will be released soon.",0
"Although Transformers are effective for visual tasks on a global scale, they do not take into account high-level local patterns in input images. This paper introduces a Dual-stream Network (DS-Net) that can fully explore the representation capacity of both local and global pattern features for image classification. The DS-Net can simultaneously calculate fine-grained and integrated features and efficiently fuse them. The paper proposes an Intra-scale Propagation module to process two different resolutions in each block and an Inter-Scale Alignment module to perform information interaction across features at dual scales. Additionally, a Dual-stream FPN (DS-FPN) is designed to enhance contextual information for downstream dense predictions. The proposed DS-Net outperforms Deit-Small by 2.4% in top-1 accuracy on ImageNet-1k and achieves state-of-the-art performance over other Vision Transformers and ResNets, without any extra features. For object detection and instance segmentation, DS-Net-Small respectively outperforms ResNet-50 by 6.4% and 5.5 % in mAP on MSCOCO 2017, surpassing the previous state-of-the-art scheme, which demonstrates its potential to be a general backbone in vision tasks. The code will be released soon.",1
"In this report, we introduce our real-time 2D object detection system for the realistic autonomous driving scenario. Our detector is built on a newly designed YOLO model, called YOLOX. On the Argoverse-HD dataset, our system achieves 41.0 streaming AP, which surpassed second place by 7.8/6.1 on detection-only track/fully track, respectively. Moreover, equipped with TensorRT, our model achieves the 30FPS inference speed with a high-resolution input size (e.g., 1440-2304). Code and models will be available at https://github.com/Megvii-BaseDetection/YOLOX",0
"This report presents our real-time 2D object detection system that is specifically designed for the practical autonomous driving scenario. Our detector is based on a newly created YOLO model, known as YOLOX. On the Argoverse-HD dataset, our system outperforms the second-place competitor by 7.8/6.1 on the detection-only track/fully track, respectively, achieving a 41.0 streaming AP. Furthermore, when equipped with TensorRT, our model achieves a 30FPS inference speed while maintaining high-resolution input size (e.g., 1440-2304). Our code and models will be accessible at https://github.com/Megvii-BaseDetection/YOLOX.",1
"There is a surge of interest in image scene graph generation (object, attribute and relationship detection) due to the need of building fine-grained image understanding models that go beyond object detection. Due to the lack of a good benchmark, the reported results of different scene graph generation models are not directly comparable, impeding the research progress. We have developed a much-needed scene graph generation benchmark based on the maskrcnn-benchmark and several popular models. This paper presents main features of our benchmark and a comprehensive ablation study of scene graph generation models using the Visual Genome and OpenImages Visual relationship detection datasets. Our codebase is made publicly available at https://github.com/microsoft/scene_graph_benchmark.",0
"The interest in generating image scene graphs, which includes object, attribute, and relationship detection, has surged as there is a need for more advanced image understanding models beyond object detection. However, there is currently no good benchmark available, making it difficult to compare the results of different scene graph generation models and impeding research progress. To address this issue, we have created a scene graph generation benchmark using the maskrcnn-benchmark and several popular models. In this paper, we detail the main features of our benchmark and provide a comprehensive ablation study of scene graph generation models using the Visual Genome and OpenImages Visual relationship detection datasets. The codebase for our benchmark is now publicly accessible on https://github.com/microsoft/scene_graph_benchmark.",1
"Effective fusion of different types of features is the key to salient object detection. The majority of existing network structure design is based on the subjective experience of scholars and the process of feature fusion does not consider the relationship between the fused features and highest-level features. In this paper, we focus on the feature relationship and propose a novel global attention unit, which we term the ""perception- and-regulation"" (PR) block, that adaptively regulates the feature fusion process by explicitly modeling interdependencies between features. The perception part uses the structure of fully-connected layers in classification networks to learn the size and shape of objects. The regulation part selectively strengthens and weakens the features to be fused. An imitating eye observation module (IEO) is further employed for improving the global perception ability of the network. The imitation of foveal vision and peripheral vision enables IEO to scrutinize highly detailed objects and to organize the broad spatial scene to better segment objects. Sufficient experiments conducted on SOD datasets demonstrate that the proposed method performs favorably against 22 state-of-the-art methods.",0
"Salient object detection hinges on the effective fusion of diverse features. However, most network designs rely on scholars' subjective experience, and the feature fusion process overlooks the relationship between fused and highest-level features. This paper proposes a novel ""perception-and-regulation"" (PR) block that models interdependencies between features. The PR block comprises a perception part that uses fully-connected layers to learn object size and shape and a regulation part that strengthens and weakens features for fusion. An ""imitating eye observation"" (IEO) module further enhances network perception. By mimicking foveal and peripheral vision, IEO scrutinizes detailed objects and better segments the wider scene. Our experiments show that our method outperforms 22 state-of-the-art techniques on SOD datasets.",1
"Transformer with self-attention has led to the revolutionizing of natural language processing field, and recently inspires the emergence of Transformer-style architecture design with competitive results in numerous computer vision tasks. Nevertheless, most of existing designs directly employ self-attention over a 2D feature map to obtain the attention matrix based on pairs of isolated queries and keys at each spatial location, but leave the rich contexts among neighbor keys under-exploited. In this work, we design a novel Transformer-style module, i.e., Contextual Transformer (CoT) block, for visual recognition. Such design fully capitalizes on the contextual information among input keys to guide the learning of dynamic attention matrix and thus strengthens the capacity of visual representation. Technically, CoT block first contextually encodes input keys via a $3\times3$ convolution, leading to a static contextual representation of inputs. We further concatenate the encoded keys with input queries to learn the dynamic multi-head attention matrix through two consecutive $1\times1$ convolutions. The learnt attention matrix is multiplied by input values to achieve the dynamic contextual representation of inputs. The fusion of the static and dynamic contextual representations are finally taken as outputs. Our CoT block is appealing in the view that it can readily replace each $3\times3$ convolution in ResNet architectures, yielding a Transformer-style backbone named as Contextual Transformer Networks (CoTNet). Through extensive experiments over a wide range of applications (e.g., image recognition, object detection and instance segmentation), we validate the superiority of CoTNet as a stronger backbone. Source code is available at \url{https://github.com/JDAI-CV/CoTNet}.",0
"The introduction of self-attention in transformers has had a profound impact on the field of natural language processing and has inspired the development of transformer-style architecture designs that have produced impressive results in a multitude of computer vision tasks. However, current designs that employ self-attention over a 2D feature map only consider isolated queries and keys at each spatial location, neglecting the rich contextual information among neighboring keys. To address this issue, we introduce a novel transformer-style module, the Contextual Transformer (CoT) block, for visual recognition. The CoT block fully exploits contextual information among input keys to enhance the learning of a dynamic attention matrix and strengthen the capacity of visual representation. First, the CoT block encodes input keys contextually with a $3\times3$ convolution, producing a static contextual representation. Then, input queries and encoded keys are concatenated to learn the dynamic multi-head attention matrix through two consecutive $1\times1$ convolutions. The resulting attention matrix is multiplied by input values to achieve a dynamic contextual representation of inputs. The fusion of the static and dynamic contextual representations is the output of the CoT block. Our CoT block can replace each $3\times3$ convolution in ResNet architectures, resulting in a transformer-style backbone called the Contextual Transformer Networks (CoTNet). Through extensive experiments in image recognition, object detection, and instance segmentation, we demonstrate the superiority of CoTNet as a stronger backbone. The source code for CoTNet is available at \url{https://github.com/JDAI-CV/CoTNet}.",1
"We study the challenging task of neural network quantization without end-to-end retraining, called Post-training Quantization (PTQ). PTQ usually requires a small subset of training data but produces less powerful quantized models than Quantization-Aware Training (QAT). In this work, we propose a novel PTQ framework, dubbed BRECQ, which pushes the limits of bitwidth in PTQ down to INT2 for the first time. BRECQ leverages the basic building blocks in neural networks and reconstructs them one-by-one. In a comprehensive theoretical study of the second-order error, we show that BRECQ achieves a good balance between cross-layer dependency and generalization error. To further employ the power of quantization, the mixed precision technique is incorporated in our framework by approximating the inter-layer and intra-layer sensitivity. Extensive experiments on various handcrafted and searched neural architectures are conducted for both image classification and object detection tasks. And for the first time we prove that, without bells and whistles, PTQ can attain 4-bit ResNet and MobileNetV2 comparable with QAT and enjoy 240 times faster production of quantized models. Codes are available at https://github.com/yhhhli/BRECQ.",0
"Our research focuses on Post-training Quantization (PTQ), which is a challenging task of neural network quantization that does not require end-to-end retraining. PTQ typically requires a small subset of training data and produces less powerful quantized models compared to Quantization-Aware Training (QAT). We introduce a new PTQ framework called BRECQ, which is the first to push the bitwidth limit in PTQ down to INT2. BRECQ utilizes the fundamental building blocks in neural networks and reconstructs them one-by-one. Our theoretical study shows that BRECQ achieves a good balance between cross-layer dependency and generalization error. To enhance the power of quantization, we incorporate the mixed precision technique by approximating the inter-layer and intra-layer sensitivity. We conduct extensive experiments on various handcrafted and searched neural architectures for image classification and object detection tasks. For the first time, we demonstrate that PTQ can achieve 4-bit ResNet and MobileNetV2, which are comparable to QAT, but with 240 times faster production of quantized models. Our codes are available at https://github.com/yhhhli/BRECQ.",1
"We propose ST-DETR, a Spatio-Temporal Transformer-based architecture for object detection from a sequence of temporal frames. We treat the temporal frames as sequences in both space and time and employ the full attention mechanisms to take advantage of the features correlations over both dimensions. This treatment enables us to deal with frames sequence as temporal object features traces over every location in the space. We explore two possible approaches; the early spatial features aggregation over the temporal dimension, and the late temporal aggregation of object query spatial features. Moreover, we propose a novel Temporal Positional Embedding technique to encode the time sequence information. To evaluate our approach, we choose the Moving Object Detection (MOD)task, since it is a perfect candidate to showcase the importance of the temporal dimension. Results show a significant 5% mAP improvement on the KITTI MOD dataset over the 1-step spatial baseline.",0
"ST-DETR is our proposed object detection architecture that utilizes a Spatio-Temporal Transformer approach to handle a sequence of temporal frames. We consider the frames as sequences in both space and time and employ full attention mechanisms to take advantage of feature correlations across both dimensions. This enables us to treat the frames sequence as temporal object feature traces for each location in space. We explore two possible approaches: early spatial features aggregation over the temporal dimension and late temporal aggregation of object query spatial features. Additionally, we introduce a novel Temporal Positional Embedding technique to encode time sequence information. Our evaluation focuses on the Moving Object Detection (MOD) task, which is an ideal candidate to highlight the significance of the temporal dimension. Our results show a noteworthy 5% mAP improvement on the KITTI MOD dataset compared to the 1-step spatial baseline.",1
"Automated Vehicle License Plate (VLP) detection and recognition have ended up being a significant research issue as of late. VLP localization and recognition are some of the most essential techniques for managing traffic using digital techniques. In this paper, four smart systems are developed to recognize Egyptian vehicles license plates. Two systems are based on character recognition, which are (System1, Characters Recognition with Classical Machine Learning) and (System2, Characters Recognition with Deep Machine Learning). The other two systems are based on the whole plate recognition which are (System3, Whole License Plate Recognition with Classical Machine Learning) and (System4, Whole License Plate Recognition with Deep Machine Learning). We use object detection algorithms, and machine learning based object recognition algorithms. The performance of the developed systems has been tested on real images, and the experimental results demonstrate that the best detection accuracy rate for VLP is provided by using the deep learning method. Where the VLP detection accuracy rate is better than the classical system by 32%. However, the best detection accuracy rate for Vehicle License Plate Arabic Character (VLPAC) is provided by using the classical method. Where VLPAC detection accuracy rate is better than the deep learning-based system by 6%. Also, the results show that deep learning is better than the classical technique used in VLP recognition processes. Where the recognition accuracy rate is better than the classical system by 8%. Finally, the paper output recommends a robust VLP recognition system based on both statistical and deep machine learning.",0
"Recently, the detection and recognition of Automated Vehicle License Plates (VLP) have become a significant research concern. VLP localization and recognition are crucial techniques for traffic management using digital methods. This paper develops four intelligent systems to identify Egyptian vehicle license plates. Two systems rely on character recognition: System1 uses Classical Machine Learning, while System2 uses Deep Machine Learning. The other two systems use whole plate recognition: System3 uses Classical Machine Learning, while System4 uses Deep Machine Learning. Object detection algorithms and machine learning-based object recognition algorithms are utilized. The systems are tested on real images, and the results reveal that the deep learning method provides the best detection accuracy rate for VLP, outperforming the classical system by 32%. The classical method provides better detection accuracy for Vehicle License Plate Arabic Character (VLPAC), outperforming the deep learning-based system by 6%. Furthermore, the deep learning technique is better than the classical technique for VLP recognition, with an accuracy rate of 8%. Ultimately, this paper recommends a robust VLP recognition system based on both statistical and deep machine learning.",1
"Exploring to what humans pay attention in dynamic panoramic scenes is useful for many fundamental applications, including augmented reality (AR) in retail, AR-powered recruitment, and visual language navigation. With this goal in mind, we propose PV-SOD, a new task that aims to segment salient objects from panoramic videos. In contrast to existing fixation-level or object-level saliency detection tasks, we focus on multi-modal salient object detection (SOD), which mimics human attention mechanism by segmenting salient objects with the guidance of audio-visual cues. To support this task, we collect the first large-scale dataset, named ASOD60K, which contains 4K-resolution video frames annotated with a six-level hierarchy, thus distinguishing itself with richness, diversity and quality. Specifically, each sequence is marked with both its super-/sub-class, with objects of each sub-class being further annotated with human eye fixations, bounding boxes, object-/instance-level masks, and associated attributes (e.g., geometrical distortion). These coarse-to-fine annotations enable detailed analysis for PV-SOD modeling, e.g., determining the major challenges for existing SOD models, and predicting scanpaths to study the long-term eye fixation behaviors of humans. We systematically benchmark 11 representative approaches on ASOD60K and derive several interesting findings. We hope this study could serve as a good starting point for advancing SOD research towards panoramic videos.",0
"Understanding human attention patterns in dynamic panoramic scenes has numerous applications, such as AR in retail, AR-based recruitment, and visual language navigation. To achieve this objective, we introduce PV-SOD, a task that focuses on multi-modal salient object detection (SOD) in panoramic videos. Unlike existing fixation or object-level SOD tasks, our approach segments salient objects based on audio-visual cues, mimicking human attention mechanisms. We have created the ASOD60K dataset to support PV-SOD, comprising 4K video frames annotated with a six-level hierarchy, allowing for detailed analysis of the challenges in existing SOD models. The dataset includes coarse-to-fine annotations, including human eye fixations, bounding boxes, object and instance-level masks, and associated attributes. We benchmarked 11 approaches on ASOD60K and identified key findings that could advance SOD research in panoramic videos.",1
"Salient object detection (SOD) is a long-standing research topic in computer vision and has drawn an increasing amount of research interest in the past decade. This paper provides the first comprehensive review and benchmark for light field SOD, which has long been lacking in the saliency community. Firstly, we introduce preliminary knowledge on light fields, including theory and data forms, and then review existing studies on light field SOD, covering ten traditional models, seven deep learning-based models, one comparative study, and one brief review. Existing datasets for light field SOD are also summarized with detailed information and statistical analyses. Secondly, we benchmark nine representative light field SOD models together with several cutting-edge RGB-D SOD models on four widely used light field datasets, from which insightful discussions and analyses, including a comparison between light field SOD and RGB-D SOD models, are achieved. Besides, due to the inconsistency of datasets in their current forms, we further generate complete data and supplement focal stacks, depth maps and multi-view images for the inconsistent datasets, making them consistent and unified. Our supplemental data makes a universal benchmark possible. Lastly, because light field SOD is quite a special problem attributed to its diverse data representations and high dependency on acquisition hardware, making it differ greatly from other saliency detection tasks, we provide nine hints into the challenges and future directions, and outline several open issues. We hope our review and benchmarking could help advance research in this field. All the materials including collected models, datasets, benchmarking results, and supplemented light field datasets will be publicly available on our project site https://github.com/kerenfu/LFSOD-Survey.",0
"The research topic of Salient Object Detection (SOD) has gained increasing interest in the past decade within the computer vision community. However, there has been a lack of comprehensive reviews and benchmarks for light field SOD. This paper addresses this gap by providing the first comprehensive review and benchmark for light field SOD. Firstly, preliminary knowledge on light fields, including theory and data forms, is introduced, and existing studies on light field SOD are reviewed, covering ten traditional models, seven deep learning-based models, one comparative study, and one brief review. Existing datasets for light field SOD are also summarized with detailed information and statistical analyses. Secondly, nine representative light field SOD models, along with several cutting-edge RGB-D SOD models, are benchmarked on four widely used light field datasets. The benchmarking results provide insightful discussions and analyses, including a comparison between light field SOD and RGB-D SOD models. Inconsistencies in the datasets are addressed by generating complete data and supplementing focal stacks, depth maps, and multi-view images, making them consistent and unified for a universal benchmark. Lastly, nine hints into the challenges and future directions, and several open issues are outlined. The hope is that this review and benchmarking will help advance research in this field. All materials, including collected models, datasets, benchmarking results, and supplemented light field datasets, will be publicly available on the project site https://github.com/kerenfu/LFSOD-Survey.",1
"The quantification of positively buoyant marine plastic debris is critical to understanding how concentrations of trash from across the world's ocean and identifying high concentration garbage hotspots in dire need of trash removal. Currently, the most common monitoring method to quantify floating plastic requires the use of a manta trawl. Techniques requiring manta trawls (or similar surface collection devices) utilize physical removal of marine plastic debris as the first step and then analyze collected samples as a second step. The need for physical removal before analysis incurs high costs and requires intensive labor preventing scalable deployment of a real-time marine plastic monitoring service across the entirety of Earth's ocean bodies. Without better monitoring and sampling methods, the total impact of plastic pollution on the environment as a whole, and details of impact within specific oceanic regions, will remain unknown. This study presents a highly scalable workflow that utilizes images captured within the epipelagic layer of the ocean as an input. It produces real-time quantification of marine plastic debris for accurate quantification and physical removal. The workflow includes creating and preprocessing a domain-specific dataset, building an object detection model utilizing a deep neural network, and evaluating the model's performance. YOLOv5-S was the best performing model, which operates at a Mean Average Precision (mAP) of 0.851 and an F1-Score of 0.89 while maintaining near-real-time speed.",0
"The accurate measurement of buoyant plastic waste in the ocean is crucial to comprehending the extent of trash concentrations found worldwide and identifying areas of high garbage accumulation that require immediate cleanup efforts. Presently, the primary method for monitoring floating plastic involves the use of a manta trawl, which entails physically removing plastic debris from the surface and subsequently analyzing collected samples. However, this approach is expensive, labor-intensive, and not conducive to real-time monitoring of the entire ocean. Consequently, the full impact of plastic pollution on the environment and specific oceanic regions remains unclear. This study proposes a scalable workflow that leverages images captured in the epipelagic zone of the ocean to quantify plastic waste in real-time and enable physical removal. The workflow involves generating and preprocessing a domain-specific dataset, constructing an object detection model using deep neural networks, and evaluating performance. YOLOv5-S emerged as the most efficient model, exhibiting an mAP of 0.851, an F1-Score of 0.89, and near-real-time speed.",1
"This paper proposes a Fully Spiking Hybrid Neural Network (FSHNN) for energy-efficient and robust object detection in resource-constrained platforms. The network architecture is based on Convolutional SNN using leaky-integrate-fire neuron models. The model combines unsupervised Spike Time-Dependent Plasticity (STDP) learning with back-propagation (STBP) learning methods and also uses Monte Carlo Dropout to get an estimate of the uncertainty error. FSHNN provides better accuracy compared to DNN based object detectors while being 150X energy-efficient. It also outperforms these object detectors, when subjected to noisy input data and less labeled training data with a lower uncertainty error.",0
"In this paper, a Fully Spiking Hybrid Neural Network (FSHNN) is proposed as a means of achieving energy-efficient and robust object detection on platforms with limited resources. The network's architecture is rooted in Convolutional SNN, which utilizes leaky-integrate-fire neuron models. The model makes use of unsupervised Spike Time-Dependent Plasticity (STDP) learning in conjunction with back-propagation (STBP) learning methods, while also incorporating Monte Carlo Dropout as a means of estimating uncertainty error. FSHNN offers superior accuracy compared to object detectors based on DNN, while utilizing only 1/150th of the energy. It also outperforms these object detectors when presented with noisy input data and less labeled training data, resulting in a lower uncertainty error.",1
"LiDAR sensors can be used to obtain a wide range of measurement signals other than a simple 3D point cloud, and those signals can be leveraged to improve perception tasks like 3D object detection. A single laser pulse can be partially reflected by multiple objects along its path, resulting in multiple measurements called echoes. Multi-echo measurement can provide information about object contours and semi-transparent surfaces which can be used to better identify and locate objects. LiDAR can also measure surface reflectance (intensity of laser pulse return), as well as ambient light of the scene (sunlight reflected by objects). These signals are already available in commercial LiDAR devices but have not been used in most LiDAR-based detection models. We present a 3D object detection model which leverages the full spectrum of measurement signals provided by LiDAR. First, we propose a multi-signal fusion (MSF) module to combine (1) the reflectance and ambient features extracted with a 2D CNN, and (2) point cloud features extracted using a 3D graph neural network (GNN). Second, we propose a multi-echo aggregation (MEA) module to combine the information encoded in different set of echo points. Compared with traditional single echo point cloud methods, our proposed Multi-Signal LiDAR Detector (MSLiD) extracts richer context information from a wider range of sensing measurements and achieves more accurate 3D object detection. Experiments show that by incorporating the multi-modality of LiDAR, our method outperforms the state-of-the-art by up to 9.1%.",0
"There are various measurement signals that can be obtained using LiDAR sensors, beyond a simple 3D point cloud, which can enhance perception tasks such as 3D object detection. When a laser pulse is emitted, it can reflect off multiple objects in its path, resulting in several measurements called echoes. Multi-echo measurement can provide additional information about object shapes and semi-transparent surfaces, which can improve object identification and location. LiDAR sensors can also measure the intensity of laser pulse return and the ambient light of the scene. Although these signals are available in commercial LiDAR devices, they have not been widely used in LiDAR-based detection models. This paper introduces a 3D object detection model that leverages the full range of measurement signals provided by LiDAR. The model includes a multi-signal fusion (MSF) module to combine reflectance and ambient features extracted with a 2D CNN and point cloud features extracted using a 3D graph neural network (GNN). It also includes a multi-echo aggregation (MEA) module to combine information from different sets of echo points. Compared to traditional single echo point cloud methods, the proposed Multi-Signal LiDAR Detector (MSLiD) extracts richer contextual information from a wider range of sensing measurements and achieves more accurate 3D object detection. Experimental results demonstrate that incorporating the multi-modality of LiDAR improves detection performance by up to 9.1% compared to the state-of-the-art.",1
"Unsupervised contrastive learning achieves great success in learning image representations with CNN. Unlike most recent methods that focused on improving accuracy of image classification, we present a novel contrastive learning approach, named DetCo, which fully explores the contrasts between global image and local image patches to learn discriminative representations for object detection. DetCo has several appealing benefits. (1) It is carefully designed by investigating the weaknesses of current self-supervised methods, which discard important representations for object detection. (2) DetCo builds hierarchical intermediate contrastive losses between global image and local patches to improve object detection, while maintaining global representations for image recognition. Theoretical analysis shows that the local patches actually remove the contextual information of an image, improving the lower bound of mutual information for better contrastive learning. (3) Extensive experiments on PASCAL VOC, COCO and Cityscapes demonstrate that DetCo not only outperforms state-of-the-art methods on object detection, but also on segmentation, pose estimation, and 3D shape prediction, while it is still competitive on image classification. For example, on PASCAL VOC, DetCo-100ep achieves 57.4 mAP, which is on par with the result of MoCov2-800ep. Moreover, DetCo consistently outperforms supervised method by 1.6/1.2/1.0 AP on Mask RCNN-C4/FPN/RetinaNet with 1x schedule. Code will be released at \href{https://github.com/xieenze/DetCo}{\color{blue}{\tt github.com/xieenze/DetCo}}.",0
"With CNN, unsupervised contrastive learning has proven to be highly effective in learning image representations. While most recent methods have focused on improving image classification accuracy, we introduce a novel contrastive learning approach called DetCo, which fully exploits the differences between global and local image patches to generate discriminative representations for object detection. DetCo has numerous benefits. Firstly, it was meticulously designed by identifying the weaknesses of current self-supervised techniques, which discard important representations for object detection. Secondly, DetCo creates intermediate contrastive losses between global images and local patches to enhance object detection while maintaining global representations for image recognition. Theoretical analysis suggests that local patches actually remove the contextual information of an image, which improves the lower bound of mutual information for better contrastive learning. Lastly, extensive experiments on PASCAL VOC, COCO, and Cityscapes demonstrate that DetCo not only surpasses state-of-the-art methods in object detection but also in segmentation, pose estimation, and 3D shape prediction, while remaining competitive in image classification. For example, on PASCAL VOC, DetCo-100ep achieves a 57.4 mAP, which is comparable to the results of MoCov2-800ep. Furthermore, DetCo consistently outperforms supervised methods by 1.6/1.2/1.0 AP on Mask RCNN-C4/FPN/RetinaNet with 1x schedule. The code for DetCo will be available on \href{https://github.com/xieenze/DetCo}{\color{blue}{\tt github.com/xieenze/DetCo}}.",1
"Attention mechanism, especially channel attention, has gained great success in the computer vision field. Many works focus on how to design efficient channel attention mechanisms while ignoring a fundamental problem, i.e., channel attention mechanism uses scalar to represent channel, which is difficult due to massive information loss. In this work, we start from a different view and regard the channel representation problem as a compression process using frequency analysis. Based on the frequency analysis, we mathematically prove that the conventional global average pooling is a special case of the feature decomposition in the frequency domain. With the proof, we naturally generalize the compression of the channel attention mechanism in the frequency domain and propose our method with multi-spectral channel attention, termed as FcaNet. FcaNet is simple but effective. We can change a few lines of code in the calculation to implement our method within existing channel attention methods. Moreover, the proposed method achieves state-of-the-art results compared with other channel attention methods on image classification, object detection, and instance segmentation tasks. Our method could consistently outperform the baseline SENet, with the same number of parameters and the same computational cost. Our code and models will are publicly available at https://github.com/cfzd/FcaNet.",0
"The computer vision field has experienced great success with attention mechanisms, particularly channel attention. However, many studies solely focus on designing efficient channel attention mechanisms, disregarding the issue of using scalar to represent channel, which causes massive information loss. Our approach differs as we view the channel representation problem as a compression process utilizing frequency analysis. Through mathematical proof, we establish that conventional global average pooling is a special case of feature decomposition in the frequency domain. This proof leads us to generalize the compression of the channel attention mechanism in the frequency domain and introduce our method, FcaNet, with multi-spectral channel attention. FcaNet is a simple yet effective method that can be easily implemented within existing channel attention methods with a few lines of code changes in the calculation. Furthermore, our proposed method outperforms other channel attention methods in image classification, object detection, and instance segmentation tasks, achieving state-of-the-art results. We consistently outperform the baseline SENet with the same number of parameters and computational cost. Our code and models are publicly available at https://github.com/cfzd/FcaNet.",1
"We present a new four-pronged approach to build firefighter's situational awareness for the first time in the literature. We construct a series of deep learning frameworks built on top of one another to enhance the safety, efficiency, and successful completion of rescue missions conducted by firefighters in emergency first response settings. First, we used a deep Convolutional Neural Network (CNN) system to classify and identify objects of interest from thermal imagery in real-time. Next, we extended this CNN framework for object detection, tracking, segmentation with a Mask RCNN framework, and scene description with a multimodal natural language processing(NLP) framework. Third, we built a deep Q-learning-based agent, immune to stress-induced disorientation and anxiety, capable of making clear navigation decisions based on the observed and stored facts in live-fire environments. Finally, we used a low computational unsupervised learning technique called tensor decomposition to perform meaningful feature extraction for anomaly detection in real-time. With these ad-hoc deep learning structures, we built the artificial intelligence system's backbone for firefighters' situational awareness. To bring the designed system into usage by firefighters, we designed a physical structure where the processed results are used as inputs in the creation of an augmented reality capable of advising firefighters of their location and key features around them, which are vital to the rescue operation at hand, as well as a path planning feature that acts as a virtual guide to assist disoriented first responders in getting back to safety. When combined, these four approaches present a novel approach to information understanding, transfer, and synthesis that could dramatically improve firefighter response and efficacy and reduce life loss.",0
"In this study, we introduce a novel approach to enhancing firefighters' situational awareness, which has not been previously explored in literature. Our approach involves constructing a series of deep learning frameworks that aim to improve the safety, efficiency, and success rate of rescue missions conducted by emergency first responders. Firstly, we utilize a deep Convolutional Neural Network (CNN) system to classify and identify objects of interest from thermal imagery in real-time. Secondly, we expand upon this CNN framework by incorporating object detection, tracking, segmentation with a Mask RCNN framework, and scene description with a multimodal natural language processing (NLP) framework. Thirdly, we develop a deep Q-learning-based agent that is impervious to stress-induced disorientation and anxiety, capable of making clear navigation decisions based on observed and stored facts in live-fire environments. Finally, we implement a low computational unsupervised learning technique called tensor decomposition to perform meaningful feature extraction for anomaly detection in real-time. These ad-hoc deep learning structures serve as the backbone of our artificial intelligence system for firefighters' situational awareness. To make this system accessible to firefighters, we design a physical structure that utilizes the processed results as inputs in the creation of an augmented reality capable of advising firefighters of their location and key features around them, which are crucial to the rescue operation at hand. Additionally, we include a path planning feature that acts as a virtual guide to assist disoriented first responders in getting back to safety. Together, our four-pronged approach provides a unique method for understanding, transferring, and synthesizing information that has the potential to greatly enhance firefighter response and efficacy, ultimately reducing life loss.",1
"Recently, it has been demonstrated that the performance of a deep convolutional neural network can be effectively improved by embedding an attention module into it. In this work, a novel lightweight and effective attention method named Pyramid Squeeze Attention (PSA) module is proposed. By replacing the 3x3 convolution with the PSA module in the bottleneck blocks of the ResNet, a novel representational block named Efficient Pyramid Squeeze Attention (EPSA) is obtained. The EPSA block can be easily added as a plug-and-play component into a well-established backbone network, and significant improvements on model performance can be achieved. Hence, a simple and efficient backbone architecture named EPSANet is developed in this work by stacking these ResNet-style EPSA blocks. Correspondingly, a stronger multi-scale representation ability can be offered by the proposed EPSANet for various computer vision tasks including but not limited to, image classification, object detection, instance segmentation, etc. Without bells and whistles, the performance of the proposed EPSANet outperforms most of the state-of-the-art channel attention methods. As compared to the SENet-50, the Top-1 accuracy is improved by 1.93% on ImageNet dataset, a larger margin of +2.7 box AP for object detection and an improvement of +1.7 mask AP for instance segmentation by using the Mask-RCNN on MS-COCO dataset are obtained. Our source code is available at:https://github.com/murufeng/EPSANet.",0
"A recent study has shown that embedding an attention module into a deep convolutional neural network can enhance its performance. This research proposes a new attention method called Pyramid Squeeze Attention (PSA) module, which is both lightweight and effective. By replacing the 3x3 convolution with the PSA module in the ResNet bottleneck blocks, an Efficient Pyramid Squeeze Attention (EPSA) block is created. This can be easily added to an established backbone network, resulting in significant improvements in model performance. The EPSANet is a simple and efficient backbone architecture that stacks ResNet-style EPSA blocks, providing a stronger multi-scale representation ability for various computer vision tasks such as image classification, object detection, and instance segmentation. The proposed EPSANet outperforms most state-of-the-art channel attention methods, with a Top-1 accuracy improvement of 1.93% on the ImageNet dataset compared to SENet-50. Moreover, object detection and instance segmentation show larger margins of improvement. The source code for EPSANet is available at: https://github.com/murufeng/EPSANet.",1
"Tunnel CCTVs are installed to low height and long-distance interval. However, because of the limitation of installation height, severe perspective effect in distance occurs, and it is almost impossible to detect vehicles in far distance from the CCTV in the existing tunnel CCTV-based accident detection system (Pflugfelder 2005). To overcome the limitation, a vehicle object is detected through an object detection algorithm based on an inverse perspective transform by re-setting the region of interest (ROI). It can detect vehicles that are far away from the CCTV. To verify this process, this paper creates each dataset consisting of images and bounding boxes based on the original and warped images of the CCTV at the same time, and then compares performance of the deep learning object detection models trained with the two datasets. As a result, the model that trained the warped image was able to detect vehicle objects more accurately at the position far from the CCTV compared to the model that trained the original image.",0
"The installation of tunnel CCTVs is limited by their low height and long-distance interval. This causes a severe perspective effect in the distance, making it difficult to detect vehicles that are far away from the CCTV in the existing tunnel CCTV-based accident detection system (Pflugfelder 2005). To address this issue, an object detection algorithm based on an inverse perspective transform is used to detect a vehicle object by resetting the region of interest (ROI). This approach is capable of detecting vehicles that are far away from the CCTV. To evaluate this process, this paper generates datasets consisting of images and bounding boxes based on both the original and warped images of the CCTV simultaneously. The performance of deep learning object detection models trained with the two datasets is compared. The results show that the model trained on the warped image was more accurate in detecting vehicle objects at positions far from the CCTV compared to the model trained on the original image.",1
"ImageNet has been arguably the most popular image classification benchmark, but it is also the one with a significant level of label noise. Recent studies have shown that many samples contain multiple classes, despite being assumed to be a single-label benchmark. They have thus proposed to turn ImageNet evaluation into a multi-label task, with exhaustive multi-label annotations per image. However, they have not fixed the training set, presumably because of a formidable annotation cost. We argue that the mismatch between single-label annotations and effectively multi-label images is equally, if not more, problematic in the training setup, where random crops are applied. With the single-label annotations, a random crop of an image may contain an entirely different object from the ground truth, introducing noisy or even incorrect supervision during training. We thus re-label the ImageNet training set with multi-labels. We address the annotation cost barrier by letting a strong image classifier, trained on an extra source of data, generate the multi-labels. We utilize the pixel-wise multi-label predictions before the final pooling layer, in order to exploit the additional location-specific supervision signals. Training on the re-labeled samples results in improved model performances across the board. ResNet-50 attains the top-1 classification accuracy of 78.9% on ImageNet with our localized multi-labels, which can be further boosted to 80.2% with the CutMix regularization. We show that the models trained with localized multi-labels also outperforms the baselines on transfer learning to object detection and instance segmentation tasks, and various robustness benchmarks. The re-labeled ImageNet training set, pre-trained weights, and the source code are available at {https://github.com/naver-ai/relabel_imagenet}.",0
"ImageNet is widely used for image classification benchmarking, but it has a high level of label noise due to many samples containing multiple classes. Recent studies suggest converting ImageNet evaluation into a multi-label task with exhaustive multi-label annotations per image to address this issue. However, the training set has not been fixed due to annotation costs. The single-label annotations mismatch with effectively multi-label images, leading to noisy or incorrect supervision during training when random crops are applied. Therefore, we re-labeled the ImageNet training set with multi-labels, utilizing a strong image classifier trained on an extra data source to generate the multi-labels. Our approach improved model performances and outperformed baselines on transfer learning to object detection and instance segmentation tasks and various robustness benchmarks. We offer the re-labeled ImageNet training set, pre-trained weights, and source code at {https://github.com/naver-ai/relabel_imagenet}. ResNet-50 achieved a top-1 classification accuracy of 78.9% on ImageNet with our localized multi-labels, which was further boosted to 80.2% with CutMix regularization.",1
"Modern convolutional neural networks (CNNs) have massive identical convolution blocks, and, hence, recursive sharing of parameters across these blocks has been proposed to reduce the amount of parameters. However, naive sharing of parameters poses many challenges such as limited representational power and the vanishing/exploding gradients problem of recursively shared parameters. In this paper, we present a recursive convolution block design and training method, in which a recursively shareable part, or a filter basis, is separated and learned while effectively avoiding the vanishing/exploding gradients problem during training. We show that the unwieldy vanishing/exploding gradients problem can be controlled by enforcing the elements of the filter basis orthonormal, and empirically demonstrate that the proposed orthogonality regularization improves the flow of gradients during training. Experimental results on image classification and object detection show that our approach, unlike previous parameter-sharing approaches, does not trade performance to save parameters and consistently outperforms overparameterized counterpart networks. This superior performance demonstrates that the proposed recursive convolution block design and the orthogonality regularization not only prevent performance degradation, but also consistently improve the representation capability while a significant amount of parameters are recursively shared.",0
"To reduce the number of parameters in modern convolutional neural networks (CNNs), recursive sharing of parameters across identical convolution blocks is proposed. However, this approach has issues such as limited representational power and the vanishing/exploding gradients problem. In this paper, we introduce a new design and training method for recursive convolution blocks. We separate and learn a recursively shareable filter basis, which avoids the vanishing/exploding gradients problem by enforcing orthonormality. We demonstrate that our approach consistently outperforms previous parameter-sharing methods in image classification and object detection, without sacrificing performance for parameter savings. Our proposed design and regularization not only improve representation capability but also prevent performance degradation while sharing a significant number of parameters.",1
"The development of digitization methods for line drawings (especially in the area of electrical engineering) relies on the availability of publicly available training and evaluation data. This paper presents such an image set along with annotations. The dataset consists of 1152 images of 144 circuits by 12 drafters and 48 563 annotations. Each of these images depicts an electrical circuit diagram, taken by consumer grade cameras under varying lighting conditions and perspectives. A variety of different pencil types and surface materials has been used. For each image, all individual electrical components are annotated with bounding boxes and one out of 45 class labels. In order to simplify a graph extraction process, different helper symbols like junction points and crossovers are introduced, while texts are annotated as well. The geometric and taxonomic problems arising from this task as well as the classes themselves and statistics of their appearances are stated. The performance of a standard Faster RCNN on the dataset is provided as an object detection baseline.",0
"Publicly available training and evaluation data are crucial for developing digitization methods for line drawings, particularly in the field of electrical engineering. This paper introduces an image set with annotations, consisting of 1152 images of 144 circuits by 12 drafters and 48,563 annotations. These images depict electrical circuit diagrams captured by consumer grade cameras and exhibit varying lighting conditions, perspectives, pencil types, and surface materials. Each image's individual electrical components are annotated with bounding boxes and one of 45 class labels, and helper symbols like junction points and crossovers, as well as text, are also annotated to facilitate graph extraction. The paper outlines the geometric and taxonomic challenges associated with this task, as well as the classes and statistics of their appearances. The object detection baseline on the dataset is provided using a standard Faster RCNN.",1
"This paper presents a simple MLP-like architecture, CycleMLP, which is a versatile backbone for visual recognition and dense predictions, unlike modern MLP architectures, e.g., MLP-Mixer, ResMLP, and gMLP, whose architectures are correlated to image size and thus are infeasible in object detection and segmentation. CycleMLP has two advantages compared to modern approaches. (1) It can cope with various image sizes. (2) It achieves linear computational complexity to image size by using local windows. In contrast, previous MLPs have quadratic computations because of their fully spatial connections. We build a family of models that surpass existing MLPs and achieve a comparable accuracy (83.2%) on ImageNet-1K classification compared to the state-of-the-art Transformer such as Swin Transformer (83.3%) but using fewer parameters and FLOPs. We expand the MLP-like models' applicability, making them a versatile backbone for dense prediction tasks. CycleMLP aims to provide a competitive baseline on object detection, instance segmentation, and semantic segmentation for MLP models. In particular, CycleMLP achieves 45.1 mIoU on ADE20K val, comparable to Swin (45.2 mIOU). Code is available at \url{https://github.com/ShoufaChen/CycleMLP}.",0
"The article introduces a new MLP-like architecture called CycleMLP that can be used as a flexible backbone for visual recognition and dense predictions. Unlike other modern MLP architectures, such as MLP-Mixer, ResMLP, and gMLP, which are limited by image size in object detection and segmentation, CycleMLP has the distinct advantage of being able to handle various image sizes and achieve linear computational complexity by using local windows. The authors demonstrate that CycleMLP can achieve comparable accuracy to state-of-the-art Transformers, such as Swin Transformer, while using fewer parameters and FLOPs. The authors suggest that CycleMLP can serve as a versatile backbone for dense prediction tasks and provide a competitive baseline for MLP models in object detection, instance segmentation, and semantic segmentation. The code for CycleMLP is available on GitHub at \url{https://github.com/ShoufaChen/CycleMLP}.",1
"In this paper, we propose a novel evaluation metric for performance evaluation of semantic segmentation. In recent years, many studies have tried to train pixel-level classifiers on large-scale image datasets to perform accurate semantic segmentation. The goal of semantic segmentation is to assign a class label of each pixel in the scene. It has various potential applications in computer vision fields e.g., object detection, classification, scene understanding and Etc. To validate the proposed wIoU evaluation metric, we tested state-of-the art methods on public benchmark datasets (e.g., KITTI) based on the proposed wIoU metric and compared with other conventional evaluation metrics.",0
"The aim of this study is to introduce a new method for evaluating semantic segmentation performance. Recent research has focused on training pixel-level classifiers on extensive image datasets to achieve precise semantic segmentation. This involves assigning a class label to each pixel in a scene, which has numerous potential applications in computer vision, such as object detection, classification, and scene comprehension. To assess the effectiveness of the new metric, wIoU, we evaluated leading techniques using public benchmark datasets like KITTI and compared the results to conventional evaluation metrics.",1
"Weakly-supervised object detection attempts to limit the amount of supervision by dispensing the need for bounding boxes, but still assumes image-level labels on the entire training set. In this work, we study the problem of training an object detector from one or few images with image-level labels and a larger set of completely unlabeled images. This is an extreme case of semi-supervised learning where the labeled data are not enough to bootstrap the learning of a detector. Our solution is to train a weakly-supervised student detector model from image-level pseudo-labels generated on the unlabeled set by a teacher classifier model, bootstrapped by region-level similarities to labeled images. Building upon the recent representative weakly-supervised pipeline PCL, our method can use more unlabeled images to achieve performance competitive or superior to many recent weakly-supervised detection solutions.",0
"The approach of weakly-supervised object detection aims to reduce the level of supervision required for bounding boxes, while still relying on image-level labels for the entire training dataset. This study investigates the challenge of training an object detector using only one or a few images with image-level labels, along with a much larger set of unlabeled images. This represents an extreme case of semi-supervised learning where the labeled data is insufficient to initiate the learning process. To address this issue, we propose training a weakly-supervised student detector model using image-level pseudo-labels generated from the unlabeled set by a teacher classifier model. This approach is bootstrapped by region-level similarities to labeled images. Our method builds upon the recent PCL weakly-supervised pipeline and can leverage a larger pool of unlabeled images to achieve competitive or superior performance compared to other weakly-supervised detection solutions.",1
"We present neural architectures that disentangle RGB-D images into objects' shapes and styles and a map of the background scene, and explore their applications for few-shot 3D object detection and few-shot concept classification. Our networks incorporate architectural biases that reflect the image formation process, 3D geometry of the world scene, and shape-style interplay. They are trained end-to-end self-supervised by predicting views in static scenes, alongside a small number of 3D object boxes. Objects and scenes are represented in terms of 3D feature grids in the bottleneck of the network. We show that the proposed 3D neural representations are compositional: they can generate novel 3D scene feature maps by mixing object shapes and styles, resizing and adding the resulting object 3D feature maps over background scene feature maps. We show that classifiers for object categories, color, materials, and spatial relationships trained over the disentangled 3D feature sub-spaces generalize better with dramatically fewer examples than the current state-of-the-art, and enable a visual question answering system that uses them as its modules to generalize one-shot to novel objects in the scene.",0
"Our research introduces neural structures that can separate RGB-D images into the shapes and styles of objects as well as the background scene map. We investigate their potential for few-shot 3D object detection and few-shot concept classification. These networks incorporate architectural inclinations which mirror the image formation process, the 3D geometry of the world scene, and the interplay between shape and style. They are trained end-to-end through self-supervision while forecasting perspectives in still scenes and a limited number of 3D object boxes. Objects and scenes are represented using 3D feature grids in the bottleneck of the network. We demonstrate that these 3D neural representations are compositional because they can formulate innovative 3D scene feature maps by mixing object shapes and styles, adjusting and merging the resulting object 3D feature maps with background scene feature maps. We prove that classifiers for object categories, color, materials, and spatial relationships trained on the segmented 3D feature subspaces generalize more effectively with significantly fewer examples than the current state-of-the-art. These structures enable a visual question answering system that uses them as modules to generalize one-shot to novel objects in the scene.",1
"This paper proposes anchor pruning for object detection in one-stage anchor-based detectors. While pruning techniques are widely used to reduce the computational cost of convolutional neural networks, they tend to focus on optimizing the backbone networks where often most computations are. In this work we demonstrate an additional pruning technique, specifically for object detection: anchor pruning. With more efficient backbone networks and a growing trend of deploying object detectors on embedded systems where post-processing steps such as non-maximum suppression can be a bottleneck, the impact of the anchors used in the detection head is becoming increasingly more important. In this work, we show that many anchors in the object detection head can be removed without any loss in accuracy. With additional retraining, anchor pruning can even lead to improved accuracy. Extensive experiments on SSD and MS COCO show that the detection head can be made up to 44% more efficient while simultaneously increasing accuracy. Further experiments on RetinaNet and PASCAL VOC show the general effectiveness of our approach. We also introduce `overanchorized' models that can be used together with anchor pruning to eliminate hyperparameters related to the initial shape of anchors.",0
"The proposal put forth in this paper is to use anchor pruning as a means of object detection in one-stage anchor-based detectors. Although pruning techniques are commonly utilized in order to decrease the computational cost of convolutional neural networks, they typically concentrate on optimizing the backbone networks since they involve the most computations. This study demonstrates a new pruning technique that is specifically geared towards object detection, known as anchor pruning. Due to the fact that post-processing steps like non-maximum suppression can present a bottleneck when deploying object detectors on embedded systems, the anchors utilized in the detection head are becoming more and more significant. The authors of this paper provide evidence that many anchors in the object detection head can be eliminated without sacrificing accuracy. With additional retraining, anchor pruning can even lead to improved accuracy. The results of experiments conducted on SSD and MS COCO show that the detection head can become up to 44% more efficient while simultaneously increasing accuracy. The general effectiveness of this approach is demonstrated by further experiments conducted on RetinaNet and PASCAL VOC. Additionally, `overanchorized' models are introduced that may be utilized in conjunction with anchor pruning to eliminate hyperparameters associated with the initial shape of anchors.",1
"Graph-based convolutional model such as non-local block has shown to be effective for strengthening the context modeling ability in convolutional neural networks (CNNs). However, its pixel-wise computational overhead is prohibitive which renders it unsuitable for high resolution imagery. In this paper, we explore the efficiency of context graph reasoning and propose a novel framework called Squeeze Reasoning. Instead of propagating information on the spatial map, we first learn to squeeze the input feature into a channel-wise global vector and perform reasoning within the single vector where the computation cost can be significantly reduced. Specifically, we build the node graph in the vector where each node represents an abstract semantic concept. The refined feature within the same semantic category results to be consistent, which is thus beneficial for downstream tasks. We show that our approach can be modularized as an end-to-end trained block and can be easily plugged into existing networks. {Despite its simplicity and being lightweight, the proposed strategy allows us to establish the considerable results on different semantic segmentation datasets and shows significant improvements with respect to strong baselines on various other scene understanding tasks including object detection, instance segmentation and panoptic segmentation.} Code is available at \url{https://github.com/lxtGH/SFSegNets}.",0
"Non-local block, a type of graph-based convolutional model, has been proven to be effective in enhancing the context modeling capabilities of convolutional neural networks (CNNs). However, its high pixel-wise computational overhead makes it unsuitable for high resolution imagery. This research explores the efficiency of context graph reasoning and proposes a new framework called Squeeze Reasoning. Rather than propagating information on the spatial map, our approach learns to compress the input feature into a channel-wise global vector and perform reasoning within the single vector, resulting in significantly reduced computation costs. Using a node graph in the vector, each node representing an abstract semantic concept, we refine features within the same semantic category, which is beneficial for downstream tasks. Our approach can be modularized as an end-to-end trained block and easily integrated into existing networks, displaying considerable results on various semantic segmentation datasets and showing significant improvements compared to strong baselines on scene understanding tasks such as object detection, instance segmentation, and panoptic segmentation. The code is available at the following link: \url{https://github.com/lxtGH/SFSegNets}.",1
"We introduce the Unity Perception package which aims to simplify and accelerate the process of generating synthetic datasets for computer vision tasks by offering an easy-to-use and highly customizable toolset. This open-source package extends the Unity Editor and engine components to generate perfectly annotated examples for several common computer vision tasks. Additionally, it offers an extensible Randomization framework that lets the user quickly construct and configure randomized simulation parameters in order to introduce variation into the generated datasets. We provide an overview of the provided tools and how they work, and demonstrate the value of the generated synthetic datasets by training a 2D object detection model. The model trained with mostly synthetic data outperforms the model trained using only real data.",0
"The Unity Perception package is presented as a means of simplifying and expediting the production of synthetic datasets for computer vision projects through the provision of user-friendly and highly adaptable tools. This open-source package expands upon the Unity Editor and engine components to create accurately labeled examples for various computer vision tasks. Furthermore, it introduces a versatile Randomization framework that enables users to quickly design and adjust randomized simulation parameters to introduce diversity into the generated datasets. We outline the available tools and their functions, and illustrate the usefulness of the synthetic datasets produced by training a 2D object detection model. The model that primarily employed synthetic data outperformed the model trained exclusively on real data.",1
"Automotive Cyber-Physical Systems (ACPS) have attracted a significant amount of interest in the past few decades, while one of the most critical operations in these systems is the perception of the environment. Deep learning and, especially, the use of Deep Neural Networks (DNNs) provides impressive results in analyzing and understanding complex and dynamic scenes from visual data. The prediction horizons for those perception systems are very short and inference must often be performed in real time, stressing the need of transforming the original large pre-trained networks into new smaller models, by utilizing Model Compression and Acceleration (MCA) techniques. Our goal in this work is to investigate best practices for appropriately applying novel weight sharing techniques, optimizing the available variables and the training procedures towards the significant acceleration of widely adopted DNNs. Extensive evaluation studies carried out using various state-of-the-art DNN models in object detection and tracking experiments, provide details about the type of errors that manifest after the application of weight sharing techniques, resulting in significant acceleration gains with negligible accuracy losses.",0
"Over the past few decades, Automotive Cyber-Physical Systems (ACPS) have generated considerable interest, and the perception of the environment is a crucial aspect of these systems. Deep Neural Networks (DNNs) have shown impressive results in analyzing complex and dynamic scenes from visual data. However, these perception systems require real-time inference with short prediction horizons, which necessitates the use of Model Compression and Acceleration (MCA) techniques to transform large pre-trained networks into smaller models. In this study, we aim to investigate optimal weight sharing techniques, variable optimization, and training procedures to accelerate widely adopted DNNs. We conducted extensive evaluation studies on various state-of-the-art DNN models in object detection and tracking experiments, which revealed negligible accuracy losses and significant acceleration gains after applying weight sharing techniques.",1
"The 3D visual perception for vehicles with the surround-view fisheye camera system is a critical and challenging task for low-cost urban autonomous driving. While existing monocular 3D object detection methods perform not well enough on the fisheye images for mass production, partly due to the lack of 3D datasets of such images. In this paper, we manage to overcome and avoid the difficulty of acquiring the large scale of accurate 3D labeled truth data, by breaking down the 3D object detection task into some sub-tasks, such as vehicle's contact point detection, type classification, re-identification and unit assembling, etc. Particularly, we propose the concept of Multidimensional Vector to include the utilizable information generated in different dimensions and stages, instead of the descriptive approach for the bird's eye view (BEV) or a cube of eight points. The experiments of real fisheye images demonstrate that our solution achieves state-of-the-art accuracy while being real-time in practice.",0
"Detecting 3D objects using a surround-view fisheye camera system for low-cost urban autonomous driving is a critical and challenging task. However, existing monocular 3D object detection methods do not perform well on fisheye images for mass production due to the lack of 3D datasets. In this study, we overcome this difficulty by breaking down the 3D object detection task into sub-tasks such as vehicle's contact point detection, type classification, re-identification, and unit assembling. To include information generated in different dimensions and stages, we propose the concept of Multidimensional Vector instead of the descriptive approach for bird's eye view (BEV) or a cube of eight points. Our real-time solution achieves state-of-the-art accuracy in detecting 3D objects in fisheye images.",1
"By the aid of attention mechanisms to weight the image features adaptively, recent advanced deep learning-based models encourage the predicted results to approximate the ground-truth masks with as large predictable areas as possible, thus achieving the state-of-the-art performance. However, these methods do not pay enough attention to small areas prone to misprediction. In this way, it is still tough to accurately locate salient objects due to the existence of regions with indistinguishable foreground and background and regions with complex or fine structures. To address these problems, we propose a novel convolutional neural network with purificatory mechanism and structural similarity loss. Specifically, in order to better locate preliminary salient objects, we first introduce the promotion attention, which is based on spatial and channel attention mechanisms to promote attention to salient regions. Subsequently, for the purpose of restoring the indistinguishable regions that can be regarded as error-prone regions of one model, we propose the rectification attention, which is learned from the areas of wrong prediction and guide the network to focus on error-prone regions thus rectifying errors. Through these two attentions, we use the Purificatory Mechanism to impose strict weights with different regions of the whole salient objects and purify results from hard-to-distinguish regions, thus accurately predicting the locations and details of salient objects. In addition to paying different attention to these hard-to-distinguish regions, we also consider the structural constraints on complex regions and propose the Structural Similarity Loss. In experiments, the proposed approach outperforms 19 state-of-the-art methods on six datasets with a notable margin at over 27FPS on a single NVIDIA 1080Ti GPU.",0
"Recent advanced deep learning-based models utilize attention mechanisms to adaptively weigh image features, resulting in predicted results that approximate the ground-truth masks with large predictable areas, leading to state-of-the-art performance. However, these methods neglect small areas that are prone to misprediction, making it difficult to accurately locate salient objects that feature regions with indistinguishable foreground and background or complex/fine structures. To combat these issues, we propose a convolutional neural network with a purificatory mechanism and structural similarity loss. Our approach introduces promotion attention, based on spatial and channel attention mechanisms, to better locate preliminary salient objects, as well as rectification attention to rectify errors in indistinguishable regions. By imposing strict weights with different regions of the whole salient objects and purifying results from hard-to-distinguish regions, our Purificatory Mechanism accurately predicts the locations and details of salient objects. Additionally, our approach considers the structural constraints on complex regions, resulting in improved performance over 19 state-of-the-art methods on six datasets with a notable margin at over 27FPS on a single NVIDIA 1080Ti GPU.",1
"By considering the spatial correspondence, dense self-supervised representation learning has achieved superior performance on various dense prediction tasks. However, the pixel-level correspondence tends to be noisy because of many similar misleading pixels, e.g., backgrounds. To address this issue, in this paper, we propose to explore \textbf{set} \textbf{sim}ilarity (SetSim) for dense self-supervised representation learning. We generalize pixel-wise similarity learning to set-wise one to improve the robustness because sets contain more semantic and structure information. Specifically, by resorting to attentional features of views, we establish corresponding sets, thus filtering out noisy backgrounds that may cause incorrect correspondences. Meanwhile, these attentional features can keep the coherence of the same image across different views to alleviate semantic inconsistency. We further search the cross-view nearest neighbours of sets and employ the structured neighbourhood information to enhance the robustness. Empirical evaluations demonstrate that SetSim is superior to state-of-the-art methods on object detection, keypoint detection, instance segmentation, and semantic segmentation.",0
"Dense self-supervised representation learning has achieved excellent results on various dense prediction tasks by taking into account the spatial correspondence. However, due to the presence of many similar and misleading pixels such as backgrounds, the pixel-level correspondence tends to be noisy. This paper proposes a solution to this issue by exploring set similarity (SetSim) for dense self-supervised representation learning. To improve robustness, we extend pixel-wise similarity learning to set-wise similarity learning as sets contain more semantic and structural information. We establish corresponding sets by utilizing attentional features of views, which filters out noisy backgrounds that may cause incorrect correspondences. Additionally, these attentional features maintain the coherence of the same image across different views to alleviate semantic inconsistency. We also employ structured neighborhood information by searching for cross-view nearest neighbors of sets to further enhance the robustness. Empirical evaluations demonstrate that SetSim outperforms state-of-the-art methods on object detection, keypoint detection, instance segmentation, and semantic segmentation.",1
"An Axial Shifted MLP architecture (AS-MLP) is proposed in this paper. Different from MLP-Mixer, where the global spatial feature is encoded for the information flow through matrix transposition and one token-mixing MLP, we pay more attention to the local features communication. By axially shifting channels of the feature map, AS-MLP is able to obtain the information flow from different axial directions, which captures the local dependencies. Such an operation enables us to utilize a pure MLP architecture to achieve the same local receptive field as CNN-like architecture. We can also design the receptive field size and dilation of blocks of AS-MLP, etc, just like designing those of convolution kernels. With the proposed AS-MLP architecture, our model obtains 83.3% Top-1 accuracy with 88M parameters and 15.2 GFLOPs on the ImageNet-1K dataset. Such a simple yet effective architecture outperforms all MLP-based architectures and achieves competitive performance compared to the transformer-based architectures (e.g., Swin Transformer) even with slightly lower FLOPs. In addition, AS-MLP is also the first MLP-based architecture to be applied to the downstream tasks (e.g., object detection and semantic segmentation). The experimental results are also impressive. Our proposed AS-MLP obtains 51.5 mAP on the COCO validation set and 49.5 MS mIoU on the ADE20K dataset, which is competitive compared to the transformer-based architectures. Code is available at https://github.com/svip-lab/AS-MLP.",0
"This paper proposes the Axial Shifted MLP (AS-MLP) architecture as an alternative to the MLP-Mixer. While the MLP-Mixer uses matrix transposition and one token-mixing MLP to encode global spatial features, AS-MLP focuses on local feature communication. By shifting channels of the feature map, AS-MLP captures information flow from different axial directions, allowing it to attain the same local receptive field as CNN-like architecture. AS-MLP also allows for the design of receptive field size and dilation of blocks, similar to convolution kernels. With 88M parameters and 15.2 GFLOPs, the proposed AS-MLP achieves 83.3% Top-1 accuracy on the ImageNet-1K dataset, surpassing MLP-based architectures and even competing with transformer-based architectures like Swin Transformer. AS-MLP is also the first MLP-based architecture applied to downstream tasks such as object detection and semantic segmentation, achieving impressive results of 51.5 mAP on the COCO validation set and 49.5 MS mIoU on the ADE20K dataset. The code for AS-MLP is available on https://github.com/svip-lab/AS-MLP.",1
"Recent convolutional neural network (CNN) development continues to advance the state-of-the-art model accuracy for various applications. However, the enhanced accuracy comes at the cost of substantial memory bandwidth and storage requirements and demanding computational resources. Although in the past the quantization methods have effectively reduced the deployment cost for edge devices, it suffers from significant information loss when processing the biased activations of contemporary CNNs. In this paper, we hence introduce an adaptive high-performance quantization method to resolve the issue of biased activation by dynamically adjusting the scaling and shifting factors based on the task loss. Our proposed method has been extensively evaluated on image classification models (ResNet-18/34/50, MobileNet-V2, EfficientNet-B0) with ImageNet dataset, object detection model (YOLO-V4) with COCO dataset, and language models with PTB dataset. The results show that our 4-bit integer (INT4) quantization models achieve better accuracy than the state-of-the-art 4-bit models, and in some cases, even surpass the golden full-precision models. The final designs have been successfully deployed onto extremely resource-constrained edge devices for many practical applications.",0
"The development of convolutional neural networks (CNNs) has resulted in improved accuracy for various applications. However, this increased accuracy comes at a cost of significant memory bandwidth and storage requirements, as well as the need for demanding computational resources. Although quantization methods have reduced deployment costs for edge devices in the past, they suffer from significant information loss when processing biased activations of contemporary CNNs. This paper proposes an adaptive high-performance quantization method that dynamically adjusts scaling and shifting factors based on task loss to resolve the issue of biased activation. The proposed method was evaluated on image classification models, object detection models, and language models and achieved better accuracy than the state-of-the-art 4-bit models and even surpassed full-precision models in some cases. The final designs were successfully deployed on resource-constrained edge devices for many practical applications.",1
"As one of the prevalent components, Feature Pyramid Network (FPN) is widely used in the current object detection models to improve the performance of multi-scale detection. However, its interaction is still in a local and lossy manner, thus limiting the representation power. In this paper, to simulate a global view of human vision in object detection and address the inherent defects of interaction mode in FPN, we construct a novel architecture termed Content-Augmented Feature Pyramid Network (CA-FPN). Unlike the vanilla FPN, which fuses features within a local receptive field, CA-FPN can adaptively aggregate similar features from a global view. It is equipped with a global content extraction module and light linear spatial transformers. The former allows to extract multi-scale context information and the latter can deeply combine the global content extraction module with the vanilla FPN using the linearized attention function, which is designed to reduce model complexity. Furthermore, CA-FPN can be readily plugged into existing FPN-based models. Extensive experiments on the challenging COCO and PASCAL VOC object detection datasets demonstrated that our CA-FPN significantly outperforms competitive FPN-based detectors without bells and whistles. When plugging CA-FPN into Cascade R-CNN framework built upon a standard ResNet-50 backbone, our method can achieve 44.8 AP on COCO mini-val. Its performance surpasses the previous state-of-the-art by 1.5 AP, demonstrating the potentiality of application.",0
"Feature Pyramid Network (FPN) is a commonly used component in current object detection models to improve multi-scale detection performance. However, its interaction is limited to a local and lossy manner, which restricts its representation power. To address this issue and simulate a global view of human vision in object detection, we introduce a novel architecture called Content-Augmented Feature Pyramid Network (CA-FPN). Unlike vanilla FPN, which fuses features locally, CA-FPN can adaptively aggregate similar features from a global perspective. It includes a global content extraction module and light linear spatial transformers, which allow for the extraction of multi-scale context information and the deep combination of the global content extraction module with vanilla FPN using a linearized attention function to reduce model complexity. Additionally, CA-FPN can be easily integrated into existing FPN-based models. Extensive experiments on the challenging COCO and PASCAL VOC object detection datasets demonstrate that our CA-FPN significantly outperforms competitive FPN-based detectors without additional features. When integrated into the Cascade R-CNN framework with a standard ResNet-50 backbone, our method achieves 44.8 AP on COCO mini-val, surpassing the previous state-of-the-art by 1.5 AP and demonstrating the potential for application.",1
"This survey paper specially analyzed computer vision-based object detection challenges and solutions by different techniques. We mainly highlighted object detection by three different trending strategies, i.e., 1) domain adaptive deep learning-based approaches (discrepancy-based, Adversarial-based, Reconstruction-based, Hybrid). We examined general as well as tiny object detection-related challenges and offered solutions by historical and comparative analysis. In part 2) we mainly focused on tiny object detection techniques (multi-scale feature learning, Data augmentation, Training strategy (TS), Context-based detection, GAN-based detection). In part 3), To obtain knowledge-able findings, we discussed different object detection methods, i.e., convolutions and convolutional neural networks (CNN), pooling operations with trending types. Furthermore, we explained results with the help of some object detection algorithms, i.e., R-CNN, Fast R-CNN, Faster R-CNN, YOLO, and SSD, which are generally considered the base bone of CV, CNN, and OD. We performed comparative analysis on different datasets such as MS-COCO, PASCAL VOC07,12, and ImageNet to analyze results and present findings. At the end, we showed future directions with existing challenges of the field. In the future, OD methods and models can be analyzed for real-time object detection, tracking strategies.",0
"The main focus of this survey paper was on the challenges and solutions of computer vision-based object detection using various techniques. The paper primarily covered three trending strategies for object detection, including domain adaptive deep learning-based approaches, tiny object detection techniques, and different methods for object detection such as convolutions and convolutional neural networks. The study examined the challenges faced in object detection and provided solutions through historical and comparative analysis. The paper also discussed the results of different object detection algorithms, including R-CNN, Fast R-CNN, Faster R-CNN, YOLO, and SSD, which are considered the base of CV, CNN, and OD. The findings of the study were presented by analyzing various datasets. Finally, the paper highlighted future directions and challenges in the field of OD, including real-time object detection and tracking strategies.",1
"In this paper we present a novel loss function, called class-agnostic segmentation (CAS) loss. With CAS loss the class descriptors are learned during training of the network. We don't require to define the label of a class a-priori, rather the CAS loss clusters regions with similar appearance together in a weakly-supervised manner. Furthermore, we show that the CAS loss function is sparse, bounded, and robust to class-imbalance. We first apply our CAS loss function with fully-convolutional ResNet101 and DeepLab-v3 architectures to the binary segmentation problem of salient object detection. We investigate the performance against the state-of-the-art methods in two settings of low and high-fidelity training data on seven salient object detection datasets. For low-fidelity training data (incorrect class label) class-agnostic segmentation loss outperforms the state-of-the-art methods on salient object detection datasets by staggering margins of around 50%. For high-fidelity training data (correct class labels) class-agnostic segmentation models perform as good as the state-of-the-art approaches while beating the state-of-the-art methods on most datasets. In order to show the utility of the loss function across different domains we then also test on general segmentation dataset, where class-agnostic segmentation loss outperforms competing losses by huge margins.",0
"This paper introduces a new loss function called class-agnostic segmentation (CAS) loss, which learns class descriptors during network training without the need for defining class labels beforehand. Instead, CAS loss groups together regions with similar appearances in a weakly-supervised manner. Additionally, the CAS loss function is sparse, bounded, and robust to class-imbalance. The study applies CAS loss with fully-convolutional ResNet101 and DeepLab-v3 architectures to the binary segmentation problem of salient object detection. The performance is evaluated against state-of-the-art methods on seven salient object detection datasets, under two settings of low and high-fidelity training data. Results show that class-agnostic segmentation loss outperforms the state-of-the-art methods by approximately 50% for low-fidelity training data, while performing as well as the state-of-the-art approaches for high-fidelity training data. The study also tests the loss function on a general segmentation dataset, where it outperforms competing losses by a significant margin.",1
"Deep neural networks (DNNs) have accomplished impressive success in various applications, including autonomous driving perception tasks, in recent years. On the other hand, current deep neural networks are easily fooled by adversarial attacks. This vulnerability raises significant concerns, particularly in safety-critical applications. As a result, research into attacking and defending DNNs has gained much coverage. In this work, detailed adversarial attacks are applied on a diverse multi-task visual perception deep network across distance estimation, semantic segmentation, motion detection, and object detection. The experiments consider both white and black box attacks for targeted and un-targeted cases, while attacking a task and inspecting the effect on all the others, in addition to inspecting the effect of applying a simple defense method. We conclude this paper by comparing and discussing the experimental results, proposing insights and future work. The visualizations of the attacks are available at https://youtu.be/R3JUV41aiPY.",0
"Recent years have seen remarkable achievements in diverse applications of Deep Neural Networks (DNNs), such as autonomous driving perception tasks. However, current DNNs are highly susceptible to adversarial attacks, which poses grave concerns, especially in safety-critical scenarios. Consequently, research on attacking and defending DNNs has gained significant attention. This study explores the effects of detailed adversarial attacks on a multi-task visual perception deep network, spanning distance estimation, semantic segmentation, motion detection, and object detection. The investigation covers both white and black box attacks, targeted and un-targeted cases, and the impact on all tasks, with a simple defense approach also being tested. The paper concludes by presenting experimental results, insights, and future directions. The attack visualizations can be accessed at https://youtu.be/R3JUV41aiPY.",1
"Up-to-date High-Definition (HD) maps are essential for self-driving cars. To achieve constantly updated HD maps, we present a deep neural network (DNN), Diff-Net, to detect changes in them. Compared to traditional methods based on object detectors, the essential design in our work is a parallel feature difference calculation structure that infers map changes by comparing features extracted from the camera and rasterized images. To generate these rasterized images, we project map elements onto images in the camera view, yielding meaningful map representations that can be consumed by a DNN accordingly. As we formulate the change detection task as an object detection problem, we leverage the anchor-based structure that predicts bounding boxes with different change status categories. Furthermore, rather than relying on single frame input, we introduce a spatio-temporal fusion module that fuses features from history frames into the current, thus improving the overall performance. Finally, we comprehensively validate our method's effectiveness using freshly collected datasets. Results demonstrate that our Diff-Net achieves better performance than the baseline methods and is ready to be integrated into a map production pipeline maintaining an up-to-date HD map.",0
"Having up-to-date High-Definition (HD) maps is crucial for the proper functioning of self-driving cars. Our solution to ensure the constant updating of these HD maps is the implementation of a deep neural network (DNN), known as Diff-Net, which is designed to detect changes in the maps. Unlike traditional methods that rely on object detectors, our approach employs a parallel feature difference calculation structure to identify changes in the maps by comparing features extracted from the camera and rasterized images. The rasterized images are generated by projecting map elements onto images from the camera view, creating meaningful map representations that the DNN can analyze. We formulate the change detection task as an object detection problem and use the anchor-based structure to predict bounding boxes with different change status categories. Additionally, we introduce a spatio-temporal fusion module that fuses features from previous frames into the current frame, improving the overall performance. Our method's effectiveness is validated using recently collected datasets, demonstrating that Diff-Net outperforms baseline methods and is suitable for integration into a map production pipeline for maintaining an up-to-date HD map.",1
"Lidar-based object detectors are critical parts of the 3D perception pipeline in autonomous navigation systems such as self-driving cars. However, they are known to be sensitive to adverse weather conditions such as rain, snow and fog due to reduced signal-to-noise ratio (SNR) and signal-to-background ratio (SBR). As a result, lidar-based object detectors trained on data captured in normal weather tend to perform poorly in such scenarios. However, collecting and labelling sufficient training data in a diverse range of adverse weather conditions is laborious and prohibitively expensive. To address this issue, we propose a physics-based approach to simulate lidar point clouds of scenes in adverse weather conditions. These augmented datasets can then be used to train lidar-based detectors to improve their all-weather reliability. Specifically, we introduce a hybrid Monte-Carlo based approach that treats (i) the effects of large particles by placing them randomly and comparing their back reflected power against the target, and (ii) attenuation effects on average through calculation of scattering efficiencies from the Mie theory and particle size distributions. Retraining networks with this augmented data improves mean average precision evaluated on real world rainy scenes and we observe greater improvement in performance with our model relative to existing models from the literature. Furthermore, we evaluate recent state-of-the-art detectors on the simulated weather conditions and present an in-depth analysis of their performance.",0
"In autonomous navigation systems like self-driving cars, lidar-based object detectors play a crucial role in the 3D perception pipeline. However, they are vulnerable to adverse weather conditions such as rain, snow, and fog, which can negatively affect their signal-to-noise ratio (SNR) and signal-to-background ratio (SBR). This can lead to poor performance of lidar-based object detectors trained on data captured in normal weather. Unfortunately, obtaining sufficient training data in different adverse weather conditions is a daunting and costly task. In response to this challenge, we propose a physics-based approach that simulates lidar point clouds of scenes in adverse weather conditions. This augmented dataset can then be used to train lidar-based detectors to improve their reliability in all weather conditions. Our approach uses a hybrid Monte-Carlo method that considers the effects of large particles by randomly placing them and comparing their back reflected power against the target. Additionally, we account for attenuation effects by calculating scattering efficiencies from the Mie theory and particle size distributions. We retrain networks with this augmented data and observe improved mean average precision on real-world rainy scenes compared to existing models. We also evaluate recent state-of-the-art detectors on the simulated weather conditions and provide a comprehensive analysis of their performance.",1
"A rising research challenge is running costly machine learning (ML) networks locally on resource-constrained edge devices. ML networks with large convolutional layers can easily exceed available memory, increasing latency due to excessive swapping. Previous memory reduction techniques such as pruning and quantization reduce model accuracy and often require retraining. Alternatively, distributed methods partition the convolutions into equivalent smaller sub-computations, but the implementations introduce communication costs and require a network of devices. However, a distributed partitioning approach can also be used to run in a reduced memory footprint on a single device by subdividing the network into smaller operations.   This report extends prior work on distributed partitioning using tiling and fusing of convolutional layers into a memory-aware execution on a single device. Our approach extends prior fusing strategies to allow for two groups of convolutional layers that are fused and tiled independently. This approach reduces overhead via data reuse, and reduces the memory footprint further. We also propose a memory usage predictor coupled with a search algorithm to provide fusing and tiling configurations for an arbitrary set of convolutional layers. When applied to the YOLOv2 object detection network, results show that our approach can run in less than half the memory, and with a speedup of up to 2.78 under severe memory constraints. Additionally, our algorithm will return a configuration with a latency that is within 6% of the best latency measured in a manual search.",0
"Running machine learning (ML) networks on edge devices with limited resources is becoming a more challenging area of research. Large convolutional layers in ML networks can cause memory constraints, leading to increased latency due to excessive swapping. While pruning and quantization have been used to reduce memory usage, they often result in decreased model accuracy and require retraining. Distributed methods can partition convolutions into smaller sub-computations, but they introduce additional communication costs and require multiple devices. However, a distributed partitioning approach can be adapted for use on a single device by subdividing the network into smaller operations to reduce the memory footprint. This report builds upon prior work on distributed partitioning by introducing a memory-aware execution approach that fuses and tiles two groups of convolutional layers independently, reducing memory usage and overhead. The report also proposes a memory usage predictor and search algorithm to provide optimal fusing and tiling configurations for any set of convolutional layers. Results show that the approach can reduce memory usage by half and improve speed by up to 2.78 under severe memory constraints, while also returning configurations with latencies within 6% of the best manually searched configuration.",1
"Object detection in images has reached unprecedented performances. The state-of-the-art methods rely on deep architectures that extract salient features and predict bounding boxes enclosing the objects of interest. These methods essentially run on RGB images. However, the RGB images are often compressed by the acquisition devices for storage purpose and transfer efficiency. Hence, their decompression is required for object detectors. To gain in efficiency, this paper proposes to take advantage of the compressed representation of images to carry out object detection usable in constrained resources conditions.   Specifically, we focus on JPEG images and propose a thorough analysis of detection architectures newly designed in regard of the peculiarities of the JPEG norm. This leads to a $\times 1.7$ speed up in comparison with a standard RGB-based architecture, while only reducing the detection performance by 5.5%. Additionally, our empirical findings demonstrate that only part of the compressed JPEG information, namely the luminance component, may be required to match detection accuracy of the full input methods.",0
"Unprecedented performances have been achieved in object detection in images through the use of state-of-the-art methods that rely on deep architectures to extract important features and predict bounding boxes around objects of interest. However, acquisition devices often compress RGB images for storage and transfer efficiency, requiring decompression for object detection. To improve efficiency in constrained resource conditions, this paper proposes utilizing the compressed representation of JPEG images for object detection. A thorough analysis of detection architectures designed specifically for the JPEG norm resulted in a 1.7x speed increase compared to standard RGB-based architecture, with only a 5.5% reduction in detection performance. Empirical findings show that only the luminance component of compressed JPEG information is necessary to achieve the same detection accuracy as full input methods.",1
"Roads are connecting line between different places, and used daily. Roads' periodic maintenance keeps them safe and functional. Detecting and reporting the existence of potholes to responsible departments can help in eliminating them. This study deployed and tested on different deep learning architecture to detect potholes. The images used for training were collected by cellphone mounted on the windshield of the car, in addition to many images downloaded from the internet to increase the size and variability of the database. Second, various object detection algorithms are employed and compared to detect potholes in real-time like SDD-TensorFlow, YOLOv3Darknet53 and YOLOv4Darknet53. YOLOv4 achieved the best performance with 81% recall, 85% precision and 85.39% mean Average Precision (mAP). The speed of processing was 20 frame per second. The system was able to detect potholes from a range on 100 meters away from the camera. The system can increase the safety of drivers and improve the performance of self-driving cars by detecting pothole time ahead.",0
"Roads serve as a means of transportation between different locations and are utilized on a daily basis. Periodic maintenance of roads is necessary to ensure their safety and functionality. Reporting the presence of potholes to the relevant authorities can aid in their eradication. This study utilized diverse deep learning architectures to detect potholes. Images obtained from a cellphone mounted on a car windshield, as well as various internet downloads, were utilized to increase the size and variability of the database. Additionally, several object detection algorithms were employed, including SDD-TensorFlow, YOLOv3Darknet53, and YOLOv4Darknet53, to detect potholes in real-time. YOLOv4 demonstrated the highest level of performance, with 81% recall, 85% precision, and 85.39% mean Average Precision (mAP), processing 20 frames per second. The system was able to detect potholes up to a distance of 100 meters away from the camera. By detecting potholes in advance, the technology can enhance driver safety and improve the performance of self-driving cars.",1
"Despite the remarkable successes of Convolutional Neural Networks (CNNs) in computer vision, it is time-consuming and error-prone to manually design a CNN. Among various Neural Architecture Search (NAS) methods that are motivated to automate designs of high-performance CNNs, the differentiable NAS and population-based NAS are attracting increasing interests due to their unique characters. To benefit from the merits while overcoming the deficiencies of both, this work proposes a novel NAS method, RelativeNAS. As the key to efficient search, RelativeNAS performs joint learning between fast-learners (i.e. networks with relatively higher accuracy) and slow-learners in a pairwise manner. Moreover, since RelativeNAS only requires low-fidelity performance estimation to distinguish each pair of fast-learner and slow-learner, it saves certain computation costs for training the candidate architectures. The proposed RelativeNAS brings several unique advantages: (1) it achieves state-of-the-art performance on ImageNet with top-1 error rate of 24.88%, i.e. outperforming DARTS and AmoebaNet-B by 1.82% and 1.12% respectively; (2) it spends only nine hours with a single 1080Ti GPU to obtain the discovered cells, i.e. 3.75x and 7875x faster than DARTS and AmoebaNet respectively; (3) it provides that the discovered cells obtained on CIFAR-10 can be directly transferred to object detection, semantic segmentation, and keypoint detection, yielding competitive results of 73.1% mAP on PASCAL VOC, 78.7% mIoU on Cityscapes, and 68.5% AP on MSCOCO, respectively. The implementation of RelativeNAS is available at https://github.com/EMI-Group/RelativeNAS",0
"Although Convolutional Neural Networks (CNNs) have been successful in computer vision, it is time-consuming and prone to errors to manually design a CNN. Differentiable NAS and population-based NAS are two NAS methods that are gaining popularity due to their unique characteristics. This paper proposes a new NAS method called RelativeNAS, which combines the advantages of both methods and performs joint learning between fast-learners and slow-learners in a pairwise manner. RelativeNAS only requires low-fidelity performance estimation to distinguish between each pair of fast-learner and slow-learner, saving computation costs for training candidate architectures. RelativeNAS has several advantages, including achieving state-of-the-art performance on ImageNet with a top-1 error rate of 24.88%, outperforming DARTS and AmoebaNet-B by 1.82% and 1.12% respectively. It spends only nine hours with a single 1080Ti GPU to obtain the discovered cells, which is 3.75x and 7875x faster than DARTS and AmoebaNet respectively. Additionally, the discovered cells obtained on CIFAR-10 can be directly transferred to object detection, semantic segmentation, and keypoint detection, yielding competitive results. The implementation of RelativeNAS is available at https://github.com/EMI-Group/RelativeNAS.",1
"Despite being widely used as a performance measure for visual detection tasks, Average Precision (AP) is limited in reflecting localisation quality, (ii) interpretability and (iii) robustness to the design choices regarding its computation, and its applicability to outputs without confidence scores. Panoptic Quality (PQ), a measure proposed for evaluating panoptic segmentation (Kirillov et al., 2019), does not suffer from these limitations but is limited to panoptic segmentation. In this paper, we propose Localisation Recall Precision (LRP) Error as the performance measure for all visual detection tasks. LRP Error, initially proposed only for object detection by Oksuz et al. (2018), does not suffer from the aforementioned limitations and is applicable to all visual detection tasks. We also introduce Optimal LRP (oLRP) Error as the minimum LRP error obtained over confidence scores to evaluate visual detectors and obtain optimal thresholds for deployment. We provide a detailed comparative analysis of LRP with AP and PQ, and use nearly 100 state-of-the-art visual detectors from seven visual detection tasks (i.e. object detection, keypoint detection, instance segmentation, panoptic segmentation, visual relationship detection, zero-shot detection and generalised zero-shot detection) using ten datasets (i.e. different COCO variants, LVIS, Open Images, Pascal, ILSVRC) to empirically show that LRP provides richer and more discriminative information than its counterparts. Code available at: https://github.com/kemaloksuz/LRP-Error",0
"Although Average Precision (AP) is widely used as a performance measure for visual detection tasks, it has limitations in reflecting localisation quality, interpretability, and robustness to design choices. Additionally, it cannot be used for outputs without confidence scores. On the other hand, Panoptic Quality (PQ) eliminates these limitations but is only applicable to panoptic segmentation. To address this issue, we propose Localisation Recall Precision (LRP) Error as the performance measure for all visual detection tasks. LRP Error, which was initially proposed for object detection in 2018, overcomes the aforementioned limitations and is applicable to all visual detection tasks. We also introduce Optimal LRP (oLRP) Error to evaluate visual detectors and determine optimal thresholds for deployment. We conducted a comparative analysis of LRP with AP and PQ, using nearly 100 state-of-the-art visual detectors from seven visual detection tasks and ten datasets. Our results show that LRP provides richer and more discriminative information than its counterparts. The code is available at https://github.com/kemaloksuz/LRP-Error.",1
"A high-performing object detection system plays a crucial role in autonomous driving (AD). The performance, typically evaluated in terms of mean Average Precision, does not take into account orientation and distance of the actors in the scene, which are important for the safe AD. It also ignores environmental context. Recently, Philion et al. proposed a neural planning metric (PKL), based on the KL divergence of a planner's trajectory and the groundtruth route, to accommodate these requirements. In this paper, we use this neural planning metric to score all submissions of the nuScenes detection challenge and analyze the results. We find that while somewhat correlated with mAP, the PKL metric shows different behavior to increased traffic density, ego velocity, road curvature and intersections. Finally, we propose ideas to extend the neural planning metric.",0
"A critical component of autonomous driving (AD) is a top-performing object detection system. The system's performance is commonly assessed through mean Average Precision, but this metric fails to consider the orientation and distance of actors in the scene, which are significant for safe AD. Additionally, environmental context is ignored. Recently, Philion et al. introduced a neural planning metric (PKL), which leverages the KL divergence of a planner's trajectory and the groundtruth route to address these requirements. This paper employs the PKL metric to evaluate all nuScenes detection challenge submissions and examine the outcomes. Our analysis reveals that while PKL has some correlation with mAP, its behavior differs with increasing traffic density, ego velocity, road curvature, and intersections. Lastly, we suggest expanding the neural planning metric to include more aspects.",1
"Human vision is able to capture the part-whole hierarchical information from the entire scene. This paper presents the Visual Parser (ViP) that explicitly constructs such a hierarchy with transformers. ViP divides visual representations into two levels, the part level and the whole level. Information of each part represents a combination of several independent vectors within the whole. To model the representations of the two levels, we first encode the information from the whole into part vectors through an attention mechanism, then decode the global information within the part vectors back into the whole representation. By iteratively parsing the two levels with the proposed encoder-decoder interaction, the model can gradually refine the features on both levels. Experimental results demonstrate that ViP can achieve very competitive performance on three major tasks e.g. classification, detection and instance segmentation. In particular, it can surpass the previous state-of-the-art CNN backbones by a large margin on object detection. The tiny model of the ViP family with $7.2\times$ fewer parameters and $10.9\times$ fewer FLOPS can perform comparably with the largest model ResNeXt-101-64$\times$4d of ResNe(X)t family. Visualization results also demonstrate that the learnt parts are highly informative of the predicting class, making ViP more explainable than previous fundamental architectures. Code is available at https://github.com/kevin-ssy/ViP.",0
"The Visual Parser (ViP) presented in this paper is a method for explicitly constructing a hierarchy of part-whole information in visual representations using transformers. ViP divides visual representations into two levels: the part level and the whole level. Each part represents a combination of several independent vectors within the whole. To model the two levels, the information from the whole is encoded into part vectors using an attention mechanism, and then the global information is decoded from the part vectors back into the whole representation. By iteratively parsing the two levels with the proposed encoder-decoder interaction, the model can refine the features on both levels. Experimental results show that ViP outperforms previous state-of-the-art CNN backbones on object detection and achieves competitive performance on classification and instance segmentation tasks. Even the tiny model of ViP family performs comparably with the largest model of ResNe(X)t family with significantly fewer parameters and FLOPS. Visualization results also show that ViP is more explainable than previous architectures. The code is available at https://github.com/kevin-ssy/ViP.",1
"Object detection has recently achieved a breakthrough for removing the last one non-differentiable component in the pipeline, Non-Maximum Suppression (NMS), and building up an end-to-end system. However, what makes for its one-to-one prediction has not been well understood. In this paper, we first point out that one-to-one positive sample assignment is the key factor, while, one-to-many assignment in previous detectors causes redundant predictions in inference. Second, we surprisingly find that even training with one-to-one assignment, previous detectors still produce redundant predictions. We identify that classification cost in matching cost is the main ingredient: (1) previous detectors only consider location cost, (2) by additionally introducing classification cost, previous detectors immediately produce one-to-one prediction during inference. We introduce the concept of score gap to explore the effect of matching cost. Classification cost enlarges the score gap by choosing positive samples as those of highest score in the training iteration and reducing noisy positive samples brought by only location cost. Finally, we demonstrate the advantages of end-to-end object detection on crowded scenes. The code is available at: \url{https://github.com/PeizeSun/OneNet}.",0
"Recently, there has been a significant breakthrough in object detection by eliminating the Non-Maximum Suppression (NMS) step, which was previously non-differentiable, and developing an end-to-end system. However, the reason for the one-to-one prediction capability of this method has not been well comprehended. In this study, we highlight that one-to-one positive sample assignment is the crucial factor, while one-to-many assignment in previous detectors leads to redundant predictions during inference. Surprisingly, we discovered that despite training with one-to-one assignment, previous detectors still generate redundant predictions. We attribute this to the classification cost in the matching cost, which was previously ignored, and only the location cost was considered. By introducing the classification cost, previous detectors immediately produce one-to-one prediction during inference. We introduce the score gap concept to investigate the impact of the matching cost, and we demonstrate the advantages of end-to-end object detection in crowded scenes. The code for this study is available at: \url{https://github.com/PeizeSun/OneNet}.",1
"The proliferation of remote sensing satellites has resulted in a massive amount of remote sensing images. However, due to human and material resource constraints, the vast majority of remote sensing images remain unlabeled. As a result, it cannot be applied to currently available deep learning methods. To fully utilize the remaining unlabeled images, we propose a Geographical Knowledge-driven Representation learning method for remote sensing images (GeoKR), improving network performance and reduce the demand for annotated data. The global land cover products and geographical location associated with each remote sensing image are regarded as geographical knowledge to provide supervision for representation learning and network pre-training. An efficient pre-training framework is proposed to eliminate the supervision noises caused by imaging times and resolutions difference between remote sensing images and geographical knowledge. A large scale pre-training dataset Levir-KR is proposed to support network pre-training. It contains 1,431,950 remote sensing images from Gaofen series satellites with various resolutions. Experimental results demonstrate that our proposed method outperforms ImageNet pre-training and self-supervised representation learning methods and significantly reduces the burden of data annotation on downstream tasks such as scene classification, semantic segmentation, object detection, and cloud / snow detection. It demonstrates that our proposed method can be used as a novel paradigm for pre-training neural networks. Codes will be available on https://github.com/flyakon/Geographical-Knowledge-driven-Representaion-Learning.",0
"The availability of remote sensing satellites has led to a surplus of remote sensing images. Nonetheless, due to a lack of human and material resources, the majority of remote sensing images remain untagged. As a result, they cannot be employed in current deep learning methods. To make the most of the remaining unlabeled images, we have proposed a novel approach called GeoKR, which is a Geographical Knowledge-driven Representation learning method for remote sensing images. This method enhances network performance and reduces the need for annotated data. The global land cover products and geographical location linked to each remote sensing image are seen as geographical knowledge, which provides guidance for representation learning and network pre-training. We have devised an effective pre-training framework to overcome the supervision issues caused by differences in imaging times and resolutions between remote sensing images and geographical knowledge. We have also created a vast pre-training dataset called Levir-KR, which includes 1,431,950 remote sensing images from Gaofen series satellites with varying resolutions. Our experimental results show that GeoKR outperforms ImageNet pre-training and self-supervised representation learning methods and significantly reduces the burden of data annotation on downstream tasks such as semantic segmentation, scene classification, object detection, and cloud/snow detection. This method can be used as an innovative approach for pre-training neural networks. The source code is available at https://github.com/flyakon/Geographical-Knowledge-driven-Representaion-Learning.",1
"Explainable Artificial Intelligence (XAI) is an emerging area of research in the field of Artificial Intelligence (AI). XAI can explain how AI obtained a particular solution (e.g., classification or object detection) and can also answer other ""wh"" questions. This explainability is not possible in traditional AI. Explainability is essential for critical applications, such as defense, health care, law and order, and autonomous driving vehicles, etc, where the know-how is required for trust and transparency. A number of XAI techniques so far have been purposed for such applications. This paper provides an overview of these techniques from a multimedia (i.e., text, image, audio, and video) point of view. The advantages and shortcomings of these techniques have been discussed, and pointers to some future directions have also been provided.",0
"The emerging field of Explainable Artificial Intelligence (XAI) in Artificial Intelligence (AI) research allows for explanations of how AI arrived at a solution, including answering ""wh"" questions that traditional AI cannot. XAI is crucial for trustworthy and transparent applications in defense, healthcare, law enforcement, and autonomous driving. Various techniques have been proposed for XAI, and this paper offers a multimedia perspective (text, image, audio, and video) on these techniques, including their advantages, shortcomings, and future directions.",1
"The past decade has witnessed significant progress on detecting objects in aerial images that are often distributed with large scale variations and arbitrary orientations. However most of existing methods rely on heuristically defined anchors with different scales, angles and aspect ratios and usually suffer from severe misalignment between anchor boxes and axis-aligned convolutional features, which leads to the common inconsistency between the classification score and localization accuracy. To address this issue, we propose a Single-shot Alignment Network (S$^2$A-Net) consisting of two modules: a Feature Alignment Module (FAM) and an Oriented Detection Module (ODM). The FAM can generate high-quality anchors with an Anchor Refinement Network and adaptively align the convolutional features according to the anchor boxes with a novel Alignment Convolution. The ODM first adopts active rotating filters to encode the orientation information and then produces orientation-sensitive and orientation-invariant features to alleviate the inconsistency between classification score and localization accuracy. Besides, we further explore the approach to detect objects in large-size images, which leads to a better trade-off between speed and accuracy. Extensive experiments demonstrate that our method can achieve state-of-the-art performance on two commonly used aerial objects datasets (i.e., DOTA and HRSC2016) while keeping high efficiency. The code is available at https://github.com/csuhan/s2anet.",0
"Over the past decade, there has been notable improvement in identifying objects in aerial images that typically have varying scales and orientations. However, current methods rely on anchors with different scales, angles, and aspect ratios, resulting in misalignment between anchor boxes and convolutional features, leading to inconsistencies between classification scores and localization accuracy. To tackle this issue, we propose the Single-shot Alignment Network (S$^2$A-Net), which comprises the Feature Alignment Module (FAM) and the Oriented Detection Module (ODM). The FAM generates high-quality anchors and aligns convolutional features with anchors using the Alignment Convolution. Meanwhile, the ODM encodes orientation information through active rotating filters, ultimately producing orientation-sensitive and orientation-invariant features to address the classification score and localization accuracy inconsistency. Moreover, we explore detecting objects in large images for better speed and accuracy balance. Our extensive experiments show that our method outperforms current ones on two commonly used aerial object datasets (DOTA and HRSC2016) and is efficient. Our code is available at https://github.com/csuhan/s2anet.",1
"3D multi-object tracking is a crucial component in the perception system of autonomous driving vehicles. Tracking all dynamic objects around the vehicle is essential for tasks such as obstacle avoidance and path planning. Autonomous vehicles are usually equipped with different sensor modalities to improve accuracy and reliability. While sensor fusion has been widely used in object detection networks in recent years, most existing multi-object tracking algorithms either rely on a single input modality, or do not fully exploit the information provided by multiple sensing modalities. In this work, we propose an end-to-end network for joint object detection and tracking based on radar and camera sensor fusion. Our proposed method uses a center-based radar-camera fusion algorithm for object detection and utilizes a greedy algorithm for object association. The proposed greedy algorithm uses the depth, velocity and 2D displacement of the detected objects to associate them through time. This makes our tracking algorithm very robust to occluded and overlapping objects, as the depth and velocity information can help the network in distinguishing them. We evaluate our method on the challenging nuScenes dataset, where it achieves 20.0 AMOTA and outperforms all vision-based 3D tracking methods in the benchmark, as well as the baseline LiDAR-based method. Our method is online with a runtime of 35ms per image, making it very suitable for autonomous driving applications.",0
"The ability to track multiple objects in 3D is a critical part of self-driving vehicles' perception systems. It is crucial for tasks such as avoiding obstacles and planning routes that all dynamic objects around the vehicle are monitored. To improve accuracy and reliability, autonomous vehicles are typically equipped with various sensor modalities. Although sensor fusion has been widely used in object detection networks, most existing multi-object tracking algorithms either rely on one input modality or fail to exploit the information provided by several sensing modalities. This study proposes an end-to-end network for joint object detection and tracking based on radar and camera sensor fusion. Our method employs a center-based radar-camera fusion algorithm for object detection and a greedy algorithm for object association. The proposed greedy algorithm uses the depth, velocity, and 2D displacement of the identified objects to associate them over time, making it very robust to occluded and overlapping objects. We evaluated our approach on the challenging nuScenes dataset, achieving 20.0 AMOTA and outperforming all vision-based 3D tracking methods in the benchmark, as well as the baseline LiDAR-based method. Our method is online, with a runtime of 35ms per image, making it suitable for autonomous driving applications.",1
"The continual learning problem has been widely studied in image classification, while rare work has been explored in object detection. Some recent works apply knowledge distillation to constrain the model to retain old knowledge, but this rigid constraint is detrimental for learning new knowledge. In our paper, we propose a new scheme for continual learning of object detection, namely Contrast R-CNN, an approach strikes a balance between retaining the old knowledge and learning the new knowledge. Furthermore, we design a Proposal Contrast to eliminate the ambiguity between old and new instance to make the continual learning more robust. Extensive evaluation on the PASCAL VOC dataset demonstrates the effectiveness of our approach.",0
"Extensive research has been conducted on the issue of continual learning in image classification, however, the exploration of this problem in object detection has been relatively scarce. Some recent studies have utilized knowledge distillation to restrict the model from forgetting previous knowledge, although this approach may hinder the acquisition of new knowledge due to its inflexibility. To tackle this challenge, we propose Contrast R-CNN, a novel approach for continual learning of object detection that strikes a balance between retaining old knowledge and acquiring new knowledge. We also introduce Proposal Contrast to eliminate any uncertainty between old and new instances, thereby ensuring the robustness of continual learning. Our approach has been evaluated on the PASCAL VOC dataset, and the results demonstrate its effectiveness.",1
"This study delves into semi-supervised object detection (SSOD) to improve detector performance with additional unlabeled data. State-of-the-art SSOD performance has been achieved recently by self-training, in which training supervision consists of ground truths and pseudo-labels. In current studies, we observe that class imbalance in SSOD severely impedes the effectiveness of self-training. To address the class imbalance, we propose adaptive class-rebalancing self-training (ACRST) with a novel memory module called CropBank. ACRST adaptively rebalances the training data with foreground instances extracted from the CropBank, thereby alleviating the class imbalance. Owing to the high complexity of detection tasks, we observe that both self-training and data-rebalancing suffer from noisy pseudo-labels in SSOD. Therefore, we propose a novel two-stage filtering algorithm to generate accurate pseudo-labels. Our method achieves satisfactory improvements on MS-COCO and VOC benchmarks. When using only 1\% labeled data in MS-COCO, our method achieves 17.02 mAP improvement over supervised baselines, and 5.32 mAP improvement compared with state-of-the-art methods.",0
"The aim of this research is to enhance the performance of detectors in semi-supervised object detection (SSOD) by utilizing unlabeled data. Self-training has proven to be an effective method for achieving state-of-the-art SSOD performance, wherein training supervision involves ground truths and pseudo-labels. However, current studies have revealed that class imbalance can hinder the effectiveness of self-training in SSOD. To address this issue, we propose adaptive class-rebalancing self-training (ACRST), which utilizes a memory module called CropBank to extract foreground instances and rebalance the training data. Despite the complexity of detection tasks, our approach overcomes the challenge of noisy pseudo-labels in SSOD by implementing a two-stage filtering algorithm to generate accurate pseudo-labels. Our method demonstrates significant improvements on MS-COCO and VOC benchmarks. Even when using only 1% labeled data in MS-COCO, our approach outperforms supervised baselines by 17.02 mAP and state-of-the-art methods by 5.32 mAP.",1
"Object detection in autonomous cars is commonly based on camera images and Lidar inputs, which are often used to train prediction models such as deep artificial neural networks for decision making for object recognition, adjusting speed, etc. A mistake in such decision making can be damaging; thus, it is vital to measure the reliability of decisions made by such prediction models via uncertainty measurement. Uncertainty, in deep learning models, is often measured for classification problems. However, deep learning models in autonomous driving are often multi-output regression models. Hence, we propose a novel method called PURE (Prediction sURface uncErtainty) for measuring prediction uncertainty of such regression models. We formulate the object recognition problem as a regression model with more than one outputs for finding object locations in a 2-dimensional camera view. For evaluation, we modified three widely-applied object recognition models (i.e., YoLo, SSD300 and SSD512) and used the KITTI, Stanford Cars, Berkeley DeepDrive, and NEXET datasets. Results showed the statistically significant negative correlation between prediction surface uncertainty and prediction accuracy suggesting that uncertainty significantly impacts the decisions made by autonomous driving.",0
"Autonomous vehicles typically rely on camera images and Lidar inputs to detect objects, and use deep artificial neural networks to train prediction models for decision making related to object recognition and speed adjustments. Since incorrect decision making can have serious consequences, it is crucial to evaluate the reliability of such models by measuring uncertainty. Although uncertainty is commonly measured for classification problems in deep learning models, autonomous driving models are typically multi-output regression models. To address this, we introduce a new method called PURE (Prediction sURface uncErtainty) for measuring prediction uncertainty in regression models. We treat object recognition as a regression problem with multiple outputs for locating objects in a 2D camera view and evaluate our method using modified versions of three commonly used object recognition models along with four datasets. Our findings indicate a significant negative correlation between prediction surface uncertainty and prediction accuracy, demonstrating that uncertainty has a significant impact on the decisions made by autonomous vehicles.",1
"Capturing uncertainty in object detection is indispensable for safe autonomous driving. In recent years, deep learning has become the de-facto approach for object detection, and many probabilistic object detectors have been proposed. However, there is no summary on uncertainty estimation in deep object detection, and existing methods are not only built with different network architectures and uncertainty estimation methods, but also evaluated on different datasets with a wide range of evaluation metrics. As a result, a comparison among methods remains challenging, as does the selection of a model that best suits a particular application. This paper aims to alleviate this problem by providing a review and comparative study on existing probabilistic object detection methods for autonomous driving applications. First, we provide an overview of generic uncertainty estimation in deep learning, and then systematically survey existing methods and evaluation metrics for probabilistic object detection. Next, we present a strict comparative study for probabilistic object detection based on an image detector and three public autonomous driving datasets. Finally, we present a discussion of the remaining challenges and future works. Code has been made available at https://github.com/asharakeh/pod_compare.git",0
"For autonomous driving to be safe, it is crucial to consider uncertainty in object detection. Deep learning has become the standard approach for object detection, leading to the development of numerous probabilistic object detectors. However, there is no comprehensive summary of uncertainty estimation in deep object detection, and existing methods vary in network architecture, uncertainty estimation techniques, and evaluation metrics. As a result, comparing methods and selecting the best model for a specific application is challenging. This paper addresses this issue by reviewing and comparing existing probabilistic object detection methods for autonomous driving. The paper begins with an overview of uncertainty estimation in deep learning and proceeds to survey existing methods and evaluation metrics for probabilistic object detection. The study then presents a rigorous comparison of probabilistic object detection based on an image detector and three public autonomous driving datasets. Finally, the paper discusses unresolved challenges and future research opportunities. Code for this study is available at https://github.com/asharakeh/pod_compare.git.",1
"Data augmentation has greatly contributed to improving the performance in image recognition tasks, and a lot of related studies have been conducted. However, data augmentation on 3D point cloud data has not been much explored. 3D label has more sophisticated and rich structural information than the 2D label, so it enables more diverse and effective data augmentation. In this paper, we propose part-aware data augmentation (PA-AUG) that can better utilize rich information of 3D label to enhance the performance of 3D object detectors. PA-AUG divides objects into partitions and stochastically applies five augmentation methods to each local region. It is compatible with existing point cloud data augmentation methods and can be used universally regardless of the detector's architecture. PA-AUG has improved the performance of state-of-the-art 3D object detector for all classes of the KITTI dataset and has the equivalent effect of increasing the train data by about 2.5$\times$. We also show that PA-AUG not only increases performance for a given dataset but also is robust to corrupted data. The code is available at https://github.com/sky77764/pa-aug.pytorch",0
"The usage of data augmentation has played a significant role in enhancing the efficiency of image recognition tasks, leading to numerous related studies. However, the exploration of data augmentation on 3D point cloud data has been limited. As 3D labels offer more detailed and complex structural information than 2D labels, they can facilitate more diverse and effective data augmentation. This article introduces the part-aware data augmentation (PA-AUG) approach, which can effectively exploit the rich information of 3D labels to improve the performance of 3D object detectors. PA-AUG partitions objects and applies five augmentation methods to each local region to achieve better results. It is compatible with various point cloud data augmentation techniques and can be used universally, regardless of the architecture of the detector. PA-AUG has demonstrated improved performance for all categories of the KITTI dataset, equivalent to increasing the training data by approximately 2.5 times. Additionally, the study shows that PA-AUG not only enhances the performance of a given dataset but also exhibits robustness to corrupted data. Access to the code is available at https://github.com/sky77764/pa-aug.pytorch.",1
"Detection of moving objects is a very important task in autonomous driving systems. After the perception phase, motion planning is typically performed in Bird's Eye View (BEV) space. This would require projection of objects detected on the image plane to top view BEV plane. Such a projection is prone to errors due to lack of depth information and noisy mapping in far away areas. CNNs can leverage the global context in the scene to project better. In this work, we explore end-to-end Moving Object Detection (MOD) on the BEV map directly using monocular images as input. To the best of our knowledge, such a dataset does not exist and we create an extended KITTI-raw dataset consisting of 12.9k images with annotations of moving object masks in BEV space for five classes. The dataset is intended to be used for class agnostic motion cue based object detection and classes are provided as meta-data for better tuning. We design and implement a two-stream RGB and optical flow fusion architecture which outputs motion segmentation directly in BEV space. We compare it with inverse perspective mapping of state-of-the-art motion segmentation predictions on the image plane. We observe a significant improvement of 13% in mIoU using the simple baseline implementation. This demonstrates the ability to directly learn motion segmentation output in BEV space. Qualitative results of our baseline and the dataset annotations can be found in https://sites.google.com/view/bev-modnet.",0
"In autonomous driving systems, the detection of moving objects is a crucial task. Typically, motion planning is carried out in Bird's Eye View (BEV) space following the perception phase. However, this requires the projection of detected objects onto the top view BEV plane, which can be prone to errors due to a lack of depth information and noisy mapping in distant areas. To address this issue, CNNs can be utilized to leverage global context and improve the projection. This study explores end-to-end Moving Object Detection (MOD) on the BEV map using monocular images as input. To achieve this, an extended KITTI-raw dataset consisting of 12.9k images with annotations of moving object masks in BEV space for five classes is created. The dataset is intended for class agnostic motion cue based object detection, and classes are provided as meta-data for better tuning. A two-stream RGB and optical flow fusion architecture is designed and implemented to output motion segmentation directly in BEV space. The results show a significant improvement of 13% in mIoU using the simple baseline implementation, demonstrating the ability to directly learn motion segmentation output in BEV space. Qualitative results of the baseline and the dataset annotations can be found at https://sites.google.com/view/bev-modnet.",1
"3D object detection based on LiDAR point clouds is a crucial module in autonomous driving particularly for long range sensing. Most of the research is focused on achieving higher accuracy and these models are not optimized for deployment on embedded systems from the perspective of latency and power efficiency. For high speed driving scenarios, latency is a crucial parameter as it provides more time to react to dangerous situations. Typically a voxel or point-cloud based 3D convolution approach is utilized for this module. Firstly, they are inefficient on embedded platforms as they are not suitable for efficient parallelization. Secondly, they have a variable runtime due to level of sparsity of the scene which is against the determinism needed in a safety system. In this work, we aim to develop a very low latency algorithm with fixed runtime. We propose a novel semantic segmentation architecture as a single unified model for object center detection using key points, box predictions and orientation prediction using binned classification in a simpler Bird's Eye View (BEV) 2D representation. The proposed architecture can be trivially extended to include semantic segmentation classes like road without any additional computation. The proposed model has a latency of 4 ms on the embedded Nvidia Xavier platform. The model is 5X faster than other top accuracy models with a minimal accuracy degradation of 2% in Average Precision at IoU=0.5 on KITTI dataset.",0
"The detection of 3D objects through LiDAR point clouds is a critical component of autonomous driving, particularly for long-range sensing. However, current research focuses on increasing accuracy and not optimizing for latency and power efficiency when deployed on embedded systems. For high-speed driving scenarios, latency is crucial for providing adequate time to react to dangerous situations. Current approaches, such as voxel or point-cloud-based 3D convolution, are inefficient on embedded platforms due to their unsuitability for efficient parallelization and variable runtime. In response, we propose a low-latency algorithm with a fixed runtime that uses a novel semantic segmentation architecture for object center detection, key points, box predictions, and orientation prediction using binned classification in a simplified Bird's Eye View (BEV) 2D representation. This architecture can easily incorporate semantic segmentation classes such as roads without additional computation. Our proposed model has a latency of only 4 ms on the embedded Nvidia Xavier platform, making it 5X faster than top accuracy models with a minimal accuracy degradation of only 2% in Average Precision at IoU=0.5 on the KITTI dataset.",1
"Moving object Detection (MOD) is a critical task in autonomous driving as moving agents around the ego-vehicle need to be accurately detected for safe trajectory planning. It also enables appearance agnostic detection of objects based on motion cues. There are geometric challenges like motion-parallax ambiguity which makes it a difficult problem. In this work, we aim to leverage the vehicle motion information and feed it into the model to have an adaptation mechanism based on ego-motion. The motivation is to enable the model to implicitly perform ego-motion compensation to improve performance. We convert the six degrees of freedom vehicle motion into a pixel-wise tensor which can be fed as input to the CNN model. The proposed model using Vehicle Motion Tensor (VMT) achieves an absolute improvement of 5.6% in mIoU over the baseline architecture. We also achieve state-of-the-art results on the public KITTI_MoSeg_Extended dataset even compared to methods which make use of LiDAR and additional input frames. Our model is also lightweight and runs at 85 fps on a TitanX GPU. Qualitative results are provided in https://youtu.be/ezbfjti-kTk.",0
"Accurately detecting moving agents around the ego-vehicle is crucial for safe trajectory planning in autonomous driving. It is a challenging task due to geometric obstacles like motion-parallax ambiguity. Additionally, it allows the detection of objects based on motion cues rather than appearance. In this study, we propose using the vehicle's motion information to adaptively improve performance and compensate for ego-motion. By converting the six degrees of freedom vehicle motion into a pixel-wise tensor, we feed it as input to the CNN model. Our proposed model, Vehicle Motion Tensor (VMT), achieves a 5.6% improvement in mIoU compared to the baseline architecture. We also outperform state-of-the-art methods on the KITTI_MoSeg_Extended dataset, including those that utilize LiDAR and extra input frames. Moreover, our model is lightweight and operates at 85 fps on a TitanX GPU. Qualitative results can be seen in https://youtu.be/ezbfjti-kTk.",1
"This paper jointly resolves two problems in vision transformer: i) the computation of Multi-Head Self-Attention (MHSA) has high computational/space complexity; ii) recent vision transformer networks are overly tuned for image classification, ignoring the difference between image classification (simple scenarios, more similar to NLP) and downstream scene understanding tasks (complicated scenarios, rich structural and contextual information). To this end, we note that pyramid pooling has been demonstrated to be effective in various vision tasks owing to its powerful ability in context abstraction, and its natural property of spatial invariance is also suitable to address the loss of structural information (problem ii)). Hence, we propose to adapt pyramid pooling to MHSA for alleviating its high requirement on computational resources (problem i)). In this way, this pooling-based MHSA can well address the above two problems and is thus flexible and powerful for downstream scene understanding tasks. Plugged with our pooling-based MHSA, we build a downstream-task-oriented transformer network, dubbed Pyramid Pooling Transformer (P2T). Extensive experiments demonstrate that, when applied P2T as the backbone network, it shows substantial superiority in various downstream scene understanding tasks such as semantic segmentation, object detection, instance segmentation, and visual saliency detection, compared to previous CNN- and transformer-based networks. The code will be released at https://github.com/yuhuan-wu/P2T.",0
"This paper offers a solution to two challenges faced by vision transformer networks. Firstly, the computation of Multi-Head Self-Attention (MHSA) requires a significant amount of computational and storage resources. Secondly, existing vision transformer networks are designed solely for image classification tasks and do not consider the complex scenarios and rich contextual information associated with downstream scene understanding tasks. To address these issues, the authors propose the use of pyramid pooling, which has proven effective in various vision tasks due to its ability to abstract context and maintain spatial invariance. By adapting pyramid pooling to MHSA, the authors develop a flexible and powerful pooling-based MHSA that is suitable for downstream scene understanding tasks. This approach is implemented in their Pyramid Pooling Transformer (P2T) network, which outperforms previous CNN- and transformer-based networks in various downstream scene understanding tasks such as semantic segmentation, object detection, instance segmentation, and visual saliency detection. The authors plan to make their code available at https://github.com/yuhuan-wu/P2T.",1
"The development of lightweight object detectors is essential due to the limited computation resources. To reduce the computation cost, how to generate redundant features plays a significant role. This paper proposes a new lightweight Convolution method Cross-Stage Lightweight (CSL) Module, to generate redundant features from cheap operations. In the intermediate expansion stage, we replaced Pointwise Convolution with Depthwise Convolution to produce candidate features. The proposed CSL-Module can reduce the computation cost significantly. Experiments conducted at MS-COCO show that the proposed CSL-Module can approximate the fitting ability of Convolution-3x3. Finally, we use the module to construct a lightweight detector CSL-YOLO, achieving better detection performance with only 43% FLOPs and 52% parameters than Tiny-YOLOv4.",0
"Generating redundant features is crucial in developing lightweight object detectors as computation resources are limited. A significant role in reducing computation costs is played by the way redundant features are generated. This paper introduces the Cross-Stage Lightweight (CSL) Module, a new lightweight Convolution method that produces redundant features from affordable operations. In the intermediate expansion stage, Pointwise Convolution is replaced with Depthwise Convolution to generate candidate features. The proposed CSL-Module can significantly reduce computation costs. MS-COCO experiments demonstrate that the proposed CSL-Module can approximate the fitting ability of Convolution-3x3. The module is then used to create a lightweight detector, CSL-YOLO, which achieves better detection performance with only 43% FLOPs and 52% parameters than Tiny-YOLOv4.",1
"Due to the vulnerability of deep neural networks to adversarial examples, numerous works on adversarial attacks and defenses have been burgeoning over the past several years. However, there seem to be some conventional views regarding adversarial attacks and object detection approaches that most researchers take for granted. In this work, we bring a fresh perspective on those procedures by evaluating the impact of universal perturbations on object detection at a class-level. We apply it to a carefully curated data set related to autonomous driving. We use Faster-RCNN object detector on images of five different categories: person, car, truck, stop sign and traffic light from the COCO data set, while carefully perturbing the images using Universal Dense Object Suppression algorithm. Our results indicate that person, car, traffic light, truck and stop sign are resilient in that order (most to least) to universal perturbations. To the best of our knowledge, this is the first time such a ranking has been established which is significant for the security of the data sets pertaining to autonomous vehicles and object detection in general.",0
"In recent years, many studies have focused on adversarial attacks and defenses due to the susceptibility of deep neural networks to adversarial examples. However, there are some commonly accepted views among researchers regarding adversarial attacks and object detection methods. This study aims to provide a novel perspective by investigating the effects of universal perturbations on object detection at a class-level. To achieve this, we used the Faster-RCNN object detector on images of five categories - person, car, truck, stop sign, and traffic light - from the COCO dataset. We perturbed the images using the Universal Dense Object Suppression algorithm and carefully evaluated the results. Our findings indicate that the resilience of the categories to universal perturbations follows this order: person, car, traffic light, truck, and stop sign. This ranking has not been established before and is significant for the security of datasets related to autonomous vehicles and object detection.",1
"Reliable epistemic uncertainty estimation is an essential component for backend applications of deep object detectors in safety-critical environments. Modern network architectures tend to give poorly calibrated confidences with limited predictive power. Here, we introduce novel gradient-based uncertainty metrics and investigate them for different object detection architectures. Experiments on the MS COCO, PASCAL VOC and the KITTI dataset show significant improvements in true positive / false positive discrimination and prediction of intersection over union as compared to network confidence. We also find improvement over Monte-Carlo dropout uncertainty metrics and further significant boosts by aggregating different sources of uncertainty metrics.The resulting uncertainty models generate well-calibrated confidences in all instances. Furthermore, we implement our uncertainty quantification models into object detection pipelines as a means to discern true against false predictions, replacing the ordinary score-threshold-based decision rule. In our experiments, we achieve a significant boost in detection performance in terms of mean average precision. With respect to computational complexity, we find that computing gradient uncertainty metrics results in floating point operation counts similar to those of Monte-Carlo dropout.",0
"Backend applications of deep object detectors in safety-critical environments require reliable estimation of epistemic uncertainty. However, modern network architectures often provide poorly calibrated confidences and limited predictive power. To address this issue, we introduce new gradient-based uncertainty metrics, which we test on various object detection architectures using datasets such as MS COCO, PASCAL VOC, and KITTI. Our experiments demonstrate significant improvements in true positive/false positive discrimination and prediction of intersection over union, compared to network confidence. We also observe further enhancements by combining different sources of uncertainty metrics. Our resulting models generate well-calibrated confidences in all cases and allow us to replace the ordinary score-threshold-based decision rule with a means of distinguishing true from false predictions. As a result, we achieve a significant boost in detection performance in terms of mean average precision. Furthermore, we find that computing gradient uncertainty metrics has similar computational complexity to Monte-Carlo dropout.",1
"Annotating user interfaces (UIs) that involves localization and classification of meaningful UI elements on a screen is a critical step for many mobile applications such as screen readers and voice control of devices. Annotating object icons, such as menu, search, and arrow backward, is especially challenging due to the lack of explicit labels on screens, their similarity to pictures, and their diverse shapes. Existing studies either use view hierarchy or pixel based methods to tackle the task. Pixel based approaches are more popular as view hierarchy features on mobile platforms are often incomplete or inaccurate, however it leaves out instructional information in the view hierarchy such as resource-ids or content descriptions. We propose a novel deep learning based multi-modal approach that combines the benefits of both pixel and view hierarchy features as well as leverages the state-of-the-art object detection techniques. In order to demonstrate the utility provided, we create a high quality UI dataset by manually annotating the most commonly used 29 icons in Rico, a large scale mobile design dataset consisting of 72k UI screenshots. The experimental results indicate the effectiveness of our multi-modal approach. Our model not only outperforms a widely used object classification baseline but also pixel based object detection models. Our study sheds light on how to combine view hierarchy with pixel features for annotating UI elements.",0
"Annotating meaningful UI elements on a screen is a crucial step for mobile applications such as screen readers and voice-controlled devices. Annotating object icons, like menu, search, and arrow backward, is particularly challenging due to their lack of explicit labels, diverse shapes, and similarity to pictures. Existing studies use either view hierarchy or pixel-based methods to address this issue. However, pixel-based approaches are more popular as view hierarchy features on mobile platforms are often incomplete or inaccurate, but it does not include instructional information in the view hierarchy such as resource-ids or content descriptions. To overcome this limitation, we propose a novel deep learning-based multi-modal approach that combines the benefits of both pixel and view hierarchy features and leverages state-of-the-art object detection techniques. We demonstrate the effectiveness of our approach by manually annotating the most commonly used 29 icons in Rico, a large-scale mobile design dataset consisting of 72k UI screenshots. Our experimental results show that our model outperforms a widely used object classification baseline and pixel-based object detection models. Our study highlights the significance of combining view hierarchy with pixel features for annotating UI elements.",1
"Point clouds and RGB images are naturally complementary modalities for 3D visual understanding - the former provides sparse but accurate locations of points on objects, while the latter contains dense color and texture information. Despite this potential for close sensor fusion, many methods train two models in isolation and use simple feature concatenation to represent 3D sensor data. This separated training scheme results in potentially sub-optimal performance and prevents 3D tasks from being used to benefit 2D tasks that are often useful on their own. To provide a more integrated approach, we propose a novel Multi-Modality Task Cascade network (MTC-RCNN) that leverages 3D box proposals to improve 2D segmentation predictions, which are then used to further refine the 3D boxes. We show that including a 2D network between two stages of 3D modules significantly improves both 2D and 3D task performance. Moreover, to prevent the 3D module from over-relying on the overfitted 2D predictions, we propose a dual-head 2D segmentation training and inference scheme, allowing the 2nd 3D module to learn to interpret imperfect 2D segmentation predictions. Evaluating our model on the challenging SUN RGB-D dataset, we improve upon state-of-the-art results of both single modality and fusion networks by a large margin ($\textbf{+3.8}$ mAP@0.5). Code will be released $\href{https://github.com/Divadi/MTC_RCNN}{\text{here.}}$",0
"The combination of point clouds and RGB images is ideal for comprehending 3D visuals, as the former provides accurate but sparse point locations, while the latter offers rich color and texture information. However, current methods often train two separate models and only concatenate features to represent 3D sensor data, resulting in sub-optimal performance and missed opportunities to use 3D tasks to enhance 2D tasks. To address this, we introduce the Multi-Modality Task Cascade network (MTC-RCNN), which employs 3D box proposals to improve 2D segmentation predictions and refine 3D boxes. By including a 2D network between two stages of 3D modules, we enhance the performance of both 2D and 3D tasks. Additionally, we propose a dual-head 2D segmentation training and inference scheme to avoid over-reliance on overfitted 2D predictions. Our approach significantly outperforms existing single-modality and fusion networks on the challenging SUN RGB-D dataset, achieving an improvement of $\textbf{+3.8}$ mAP@0.5. The code for our model can be accessed at $\href{https://github.com/Divadi/MTC_RCNN}{\text{here.}}$",1
"Conventional salient object detection models cannot differentiate the importance of different salient objects. Recently, two works have been proposed to detect saliency ranking by assigning different degrees of saliency to different objects. However, one of these models cannot differentiate object instances and the other focuses more on sequential attention shift order inference. In this paper, we investigate a practical problem setting that requires simultaneously segment salient instances and infer their relative saliency rank order. We present a novel unified model as the first end-to-end solution, where an improved Mask R-CNN is first used to segment salient instances and a saliency ranking branch is then added to infer the relative saliency. For relative saliency ranking, we build a new graph reasoning module by combining four graphs to incorporate the instance interaction relation, local contrast, global contrast, and a high-level semantic prior, respectively. A novel loss function is also proposed to effectively train the saliency ranking branch. Besides, a new dataset and an evaluation metric are proposed for this task, aiming at pushing forward this field of research. Finally, experimental results demonstrate that our proposed model is more effective than previous methods. We also show an example of its practical usage on adaptive image retargeting.",0
"Traditional models for detecting salient objects are unable to distinguish between the importance of different objects. Recently, two models have been introduced that attempt to detect saliency ranking by assigning varying degrees of saliency to different objects. However, one of these models is unable to differentiate between object instances, while the other places greater emphasis on sequential attention shift order inference. In this study, we examine a practical problem where it is necessary to simultaneously segment salient instances and determine their relative saliency rank order. To address this, we propose a novel unified model that utilizes an improved Mask R-CNN to segment salient instances, with a saliency ranking branch added to determine their relative saliency. Our model also includes a new graph reasoning module that combines four graphs to incorporate instance interaction relation, local and global contrast, as well as high-level semantic prior. We also introduce a novel loss function to effectively train the saliency ranking branch. To push the research forward, we propose a new dataset and evaluation metric for this task. Our experimental results demonstrate that our proposed model is more effective than previous methods, and we provide an example of its practical usage on adaptive image retargeting.",1
"There is a proliferation in the number of satellites launched each year, resulting in downlinking of terabytes of data each day. The data received by ground stations is often unprocessed, making this an expensive process considering the large data sizes and that not all of the data is useful. This, coupled with the increasing demand for real-time data processing, has led to a growing need for on-orbit processing solutions. In this work, we investigate the performance of CNN-based object detectors on constrained devices by applying different image compression techniques to satellite data. We examine the capabilities of the NVIDIA Jetson Nano and NVIDIA Jetson AGX Xavier; low-power, high-performance computers, with integrated GPUs, small enough to fit on-board a nanosatellite. We take a closer look at object detection networks, including the Single Shot MultiBox Detector (SSD) and Region-based Fully Convolutional Network (R-FCN) models that are pre-trained on DOTA - a Large Scale Dataset for Object Detection in Aerial Images. The performance is measured in terms of execution time, memory consumption, and accuracy, and are compared against a baseline containing a server with two powerful GPUs. The results show that by applying image compression techniques, we are able to improve the execution time and memory consumption, achieving a fully runnable dataset. A lossless compression technique achieves roughly a 10% reduction in execution time and about a 3% reduction in memory consumption, with no impact on the accuracy. While a lossy compression technique improves the execution time by up to 144% and the memory consumption is reduced by as much as 97%. However, it has a significant impact on accuracy, varying depending on the compression ratio. Thus the application and ratio of these compression techniques may differ depending on the required level of accuracy for a particular task.",0
"There has been a significant increase in the number of satellites launched each year, leading to the downloading of large amounts of data daily. However, much of the data received by ground stations is unprocessed, which is costly due to the size of the data and its limited usefulness. Additionally, the demand for real-time data processing has resulted in a need for on-orbit processing solutions. This study aims to investigate the effectiveness of CNN-based object detectors on devices with limited resources by applying various image compression techniques to satellite data. The NVIDIA Jetson Nano and NVIDIA Jetson AGX Xavier, which are small, low-power, high-performance computers with integrated GPUs, were examined, and object detection networks, including the SSD and R-FCN models pre-trained on DOTA, were evaluated. The study assessed performance in terms of execution time, memory consumption, and accuracy, and compared the results against a baseline containing a server with two powerful GPUs. The findings indicate that image compression techniques can improve execution time and memory consumption, resulting in a fully runnable dataset. Lossless compression reduced execution time by roughly 10% and memory consumption by about 3%, with no impact on accuracy. However, lossy compression significantly improved execution time by up to 144% and reduced memory consumption by up to 97%, but accuracy was affected, depending on the compression ratio. Therefore, the choice and ratio of compression techniques may vary depending on the accuracy level required for a specific task.",1
"A unified neural network structure is presented for joint 3D object detection and point cloud segmentation in this paper. We leverage rich supervision from both detection and segmentation labels rather than using just one of them. In addition, an extension based on single-stage object detectors is proposed based on the implicit function widely used in 3D scene and object understanding. The extension branch takes the final feature map from the object detection module as input, and produces an implicit function that generates semantic distribution for each point for its corresponding voxel center. We demonstrated the performance of our structure on nuScenes-lidarseg, a large-scale outdoor dataset. Our solution achieves competitive results against state-of-the-art methods in both 3D object detection and point cloud segmentation with little additional computation load compared with object detection solutions. The capability of efficient weakly supervision semantic segmentation of the proposed method is also validated by experiments.",0
"In this paper, we introduce a neural network structure that unifies joint 3D object detection and point cloud segmentation. Our approach utilizes supervision from both detection and segmentation labels to enhance its performance. Additionally, we propose an extension to the single-stage object detectors, which utilizes the implicit function, widely utilized in 3D scene and object understanding. The extension branch takes the final feature map from the object detection module as input and generates a semantic distribution for each point for its corresponding voxel center. Our proposed structure is evaluated on the nuScenes-lidarseg, a large-scale outdoor dataset, and achieves competitive results against state-of-the-art methods in both 3D object detection and point cloud segmentation with little additional computation load compared to existing object detection solutions. The proposed method also demonstrates efficient weakly supervised semantic segmentation, which is validated by experiments.",1
"3D object detection is an important yet demanding task that heavily relies on difficult to obtain 3D annotations. To reduce the required amount of supervision, we propose 3DIoUMatch, a novel semi-supervised method for 3D object detection applicable to both indoor and outdoor scenes. We leverage a teacher-student mutual learning framework to propagate information from the labeled to the unlabeled train set in the form of pseudo-labels. However, due to the high task complexity, we observe that the pseudo-labels suffer from significant noise and are thus not directly usable. To that end, we introduce a confidence-based filtering mechanism, inspired by FixMatch. We set confidence thresholds based upon the predicted objectness and class probability to filter low-quality pseudo-labels. While effective, we observe that these two measures do not sufficiently capture localization quality. We therefore propose to use the estimated 3D IoU as a localization metric and set category-aware self-adjusted thresholds to filter poorly localized proposals. We adopt VoteNet as our backbone detector on indoor datasets while we use PV-RCNN on the autonomous driving dataset, KITTI. Our method consistently improves state-of-the-art methods on both ScanNet and SUN-RGBD benchmarks by significant margins under all label ratios (including fully labeled setting). For example, when training using only 10\% labeled data on ScanNet, 3DIoUMatch achieves 7.7% absolute improvement on mAP@0.25 and 8.5% absolute improvement on mAP@0.5 upon the prior art. On KITTI, we are the first to demonstrate semi-supervised 3D object detection and our method surpasses a fully supervised baseline from 1.8% to 7.6% under different label ratios and categories.",0
"Detecting 3D objects is a challenging task that requires 3D annotations, which can be difficult to obtain. To address this issue and reduce the need for extensive supervision, we present a novel semi-supervised method called 3DIoUMatch, which can be applied to both indoor and outdoor scenes. Our approach uses a teacher-student mutual learning framework to transfer information from labeled to unlabeled data in the form of pseudo-labels. However, we found that the pseudo-labels were noisy and needed to be filtered. To address this, we developed a confidence-based filtering mechanism inspired by FixMatch, which sets confidence thresholds based on objectness and class probability to filter out low-quality pseudo-labels. However, we observed that these measures were not sufficient for evaluating localization quality, so we proposed a category-aware self-adjusted threshold for filtering poorly localized proposals based on estimated 3D IoU. We used VoteNet for indoor datasets and PV-RCNN for the KITTI autonomous driving dataset. Our method outperformed state-of-the-art methods in both ScanNet and SUN-RGBD benchmarks, achieving significant improvements in mAP@0.25 and mAP@0.5 at all label ratios, including fully labeled. On the KITTI dataset, we achieved semi-supervised 3D object detection, surpassing a fully supervised baseline by up to 7.6% under various label ratios and categories.",1
"As cameras are increasingly deployed in new application domains such as autonomous driving, performing 3D object detection on monocular images becomes an important task for visual scene understanding. Recent advances on monocular 3D object detection mainly rely on the ``pseudo-LiDAR'' generation, which performs monocular depth estimation and lifts the 2D pixels to pseudo 3D points. However, depth estimation from monocular images, due to its poor accuracy, leads to inevitable position shift of pseudo-LiDAR points within the object. Therefore, the predicted bounding boxes may suffer from inaccurate location and deformed shape. In this paper, we present a novel neighbor-voting method that incorporates neighbor predictions to ameliorate object detection from severely deformed pseudo-LiDAR point clouds. Specifically, each feature point around the object forms their own predictions, and then the ``consensus'' is achieved through voting. In this way, we can effectively combine the neighbors' predictions with local prediction and achieve more accurate 3D detection. To further enlarge the difference between the foreground region of interest (ROI) pseudo-LiDAR points and the background points, we also encode the ROI prediction scores of 2D foreground pixels into the corresponding pseudo-LiDAR points. We conduct extensive experiments on the KITTI benchmark to validate the merits of our proposed method. Our results on the bird's eye view detection outperform the state-of-the-art performance by a large margin, especially for the ``hard'' level detection.",0
"The use of cameras in various settings, such as autonomous driving, has made 3D object detection on monocular images an important task for visual scene understanding. However, the accuracy of depth estimation from monocular images is poor, leading to a shift in the position of pseudo-LiDAR points within the object and resulting in inaccurate location and deformed shape of predicted bounding boxes. To address this issue, we propose a novel neighbor-voting method that incorporates neighbor predictions to improve object detection from severely deformed pseudo-LiDAR point clouds. This method combines local and neighbor predictions through voting, achieving more accurate 3D detection. Additionally, we encode the ROI prediction scores of 2D foreground pixels into the corresponding pseudo-LiDAR points to further enhance the difference between foreground and background points. Our experiments on the KITTI benchmark demonstrate the superior performance of our proposed method, particularly for the ""hard"" level detection, outperforming the state-of-the-art performance by a large margin.",1
"While recent advancement of domain adaptation techniques is significant, most of methods only align a feature extractor and do not adapt a classifier to target domain, which would be a cause of performance degradation. We propose novel domain adaptation technique for object detection that aligns prediction output space. In addition to feature alignment, we aligned predictions of locations and class confidences of our vehicle detector for satellite images by adversarial training. The proposed method significantly improved AP score by over 5%, which shows effectivity of our method for object detection tasks in satellite images.",0
"Although there have been notable developments in domain adaptation techniques, the majority of these methods only focus on aligning a feature extractor and fail to adjust the classifier for the target domain. This can lead to a decrease in performance. To address this, we present a groundbreaking domain adaptation technique for object detection that centers on aligning the prediction output space. Through adversarial training, we aligned both feature and prediction output spaces, including locations and class confidences of our vehicle detector for satellite images. Our approach resulted in an AP score improvement of over 5%, demonstrating the effectiveness of our method for object detection in satellite images.",1
"RGB-D salient object detection (SOD) recently has attracted increasing research interest by benefiting conventional RGB SOD with extra depth information. However, existing RGB-D SOD models often fail to perform well in terms of both efficiency and accuracy, which hinders their potential applications on mobile devices and real-world problems. An underlying challenge is that the model accuracy usually degrades when the model is simplified to have few parameters. To tackle this dilemma and also inspired by the fact that depth quality is a key factor influencing the accuracy, we propose a novel depth quality-inspired feature manipulation (DQFM) process, which is efficient itself and can serve as a gating mechanism for filtering depth features to greatly boost the accuracy. DQFM resorts to the alignment of low-level RGB and depth features, as well as holistic attention of the depth stream to explicitly control and enhance cross-modal fusion. We embed DQFM to obtain an efficient light-weight model called DFM-Net, where we also design a tailored depth backbone and a two-stage decoder for further efficiency consideration. Extensive experimental results demonstrate that our DFM-Net achieves state-of-the-art accuracy when comparing to existing non-efficient models, and meanwhile runs at 140ms on CPU (2.2$\times$ faster than the prior fastest efficient model) with only $\sim$8.5Mb model size (14.9% of the prior lightest). Our code will be available at https://github.com/zwbx/DFM-Net.",0
"RGB-D salient object detection (SOD) has recently sparked growing interest in research as it supplements conventional RGB SOD with additional depth data. However, the current RGB-D SOD models are not efficient nor accurate enough, posing limitations on their application in real-world problems and on mobile devices. One of the main challenges is maintaining model accuracy while reducing the number of parameters. To address this challenge, we introduce a novel Depth Quality-inspired Feature Manipulation (DQFM) process that acts as an efficient gating mechanism to filter depth features and significantly improve accuracy. DQFM aligns low-level RGB and depth features and employs holistic attention of the depth stream to enhance cross-modal fusion. We incorporate DQFM into our lightweight DFM-Net model, which includes a customized depth backbone and a two-stage decoder to further improve efficiency. Our experiments demonstrate that DFM-Net outperforms existing non-efficient models, achieving state-of-the-art accuracy while running at 140ms on CPU (2.2x faster than the prior fastest efficient model) with a model size of only ~8.5Mb (14.9% of the prior lightest). The code for our model is available at https://github.com/zwbx/DFM-Net.",1
"Vanilla models for object detection and instance segmentation suffer from the heavy bias toward detecting frequent objects in the long-tailed setting. Existing methods address this issue mostly during training, e.g., by re-sampling or re-weighting. In this paper, we investigate a largely overlooked approach -- post-processing calibration of confidence scores. We propose NorCal, Normalized Calibration for long-tailed object detection and instance segmentation, a simple and straightforward recipe that reweighs the predicted scores of each class by its training sample size. We show that separately handling the background class and normalizing the scores over classes for each proposal are keys to achieving superior performance. On the LVIS dataset, NorCal can effectively improve nearly all the baseline models not only on rare classes but also on common and frequent classes. Finally, we conduct extensive analysis and ablation studies to offer insights into various modeling choices and mechanisms of our approach.",0
"Object detection and instance segmentation models based on vanilla methods are biased towards detecting frequently occurring objects in long-tailed settings. While current techniques address this bias during training, such as through re-sampling or re-weighting, post-processing calibration of confidence scores is often overlooked. This paper investigates this approach and introduces a straightforward method for long-tailed object detection and instance segmentation called NorCal, or Normalized Calibration. NorCal reweighs the predicted scores of each class by its training sample size, with specific attention given to handling the background class and normalizing scores over classes for each proposal. Our experiments on the LVIS dataset demonstrate that NorCal can improve the performance of nearly all baseline models, not only on rare classes but also on common and frequent classes. We also provide insights into the various modeling choices and mechanisms of our approach through extensive analysis and ablation studies.",1
"Retail scenes usually contain densely packed high number of objects in each image. Standard object detection techniques use fully supervised training methodology. This is highly costly as annotating a large dense retail object detection dataset involves an order of magnitude more effort compared to standard datasets. Hence, we propose semi-supervised learning to effectively use the large amount of unlabeled data available in the retail domain. We adapt a popular self supervised method called noisy student initially proposed for object classification to the task of dense object detection. We show that using unlabeled data with the noisy student training methodology, we can improve the state of the art on precise detection of objects in densely packed retail scenes. We also show that performance of the model increases as you increase the amount of unlabeled data.",0
"Typically, retail images are filled with a high quantity of densely packed items. Current object detection techniques rely on fully supervised training, which is a costly process due to the extensive effort required to annotate a substantial amount of data for dense retail object detection. To address this issue, we suggest using semi-supervised learning to effectively utilize the large quantity of unlabeled data available in the retail industry. We have adapted the popular self-supervised ""noisy student"" method, originally designed for object classification, to the task of detecting densely packed objects. Our study demonstrates that incorporating unlabeled data and utilizing the noisy student training method improves the current state-of-the-art in precise object detection of dense retail scenes. Additionally, we have observed that the model's performance increases as the quantity of unlabeled data increases.",1
"We introduce SCOD (Sensory Commutativity Object Detection), an active method for movable and immovable object detection. SCOD exploits the commutative properties of action sequences, in the scenario of an embodied agent equipped with first-person sensors and a continuous motor space with multiple degrees of freedom. SCOD is based on playing an action sequence in two different orders from the same starting point and comparing the two final observations obtained after each sequence. Our experiments on 3D realistic robotic setups (iGibson) demonstrate the accuracy of SCOD and its generalization to unseen environments and objects. We also successfully apply SCOD on a real robot to further illustrate its generalization properties. With SCOD, we aim at providing a novel way of approaching the problem of object discovery in the context of a naive embodied agent. We provide code and a supplementary video.",0
An innovative technique called SCOD (Sensory Commutativity Object Detection) is presented for detecting both movable and immovable objects. The method takes advantage of the commutative nature of action sequences in situations where an embodied agent is equipped with first-person sensors and a motor space with multiple degrees of freedom. SCOD involves playing an action sequence in two different orders from the same starting point and comparing the final observations obtained after each sequence. The accuracy of SCOD and its ability to generalize to unseen environments and objects are demonstrated through experiments on 3D realistic robotic setups (iGibson) and a real robot. SCOD offers a fresh approach to discovering objects for a novice embodied agent and is accompanied by code and a supplementary video.,1
"The detection of semantic relationships between objects represented in an image is one of the fundamental challenges in image interpretation. Neural-Symbolic techniques, such as Logic Tensor Networks (LTNs), allow the combination of semantic knowledge representation and reasoning with the ability to efficiently learn from examples typical of neural networks. We here propose Faster-LTN, an object detector composed of a convolutional backbone and an LTN. To the best of our knowledge, this is the first attempt to combine both frameworks in an end-to-end training setting. This architecture is trained by optimizing a grounded theory which combines labelled examples with prior knowledge, in the form of logical axioms. Experimental comparisons show competitive performance with respect to the traditional Faster R-CNN architecture.",0
"One of the main challenges in interpreting images is detecting semantic relationships between objects within them. Logic Tensor Networks (LTNs) provide a solution by combining semantic knowledge representation and reasoning with the learning capabilities of neural networks. Our proposal, Faster-LTN, is an object detector that utilizes a convolutional backbone and an LTN, representing the first attempt to integrate these frameworks in an end-to-end training setting. The architecture is trained by optimizing a grounded theory that incorporates labeled examples and logical axioms as prior knowledge. Our experimental comparisons demonstrate that Faster-LTN performs competitively with the traditional Faster R-CNN architecture.",1
"Deep learning-based object detection and instance segmentation have achieved unprecedented progress. In this paper, we propose Complete-IoU (CIoU) loss and Cluster-NMS for enhancing geometric factors in both bounding box regression and Non-Maximum Suppression (NMS), leading to notable gains of average precision (AP) and average recall (AR), without the sacrifice of inference efficiency. In particular, we consider three geometric factors, i.e., overlap area, normalized central point distance and aspect ratio, which are crucial for measuring bounding box regression in object detection and instance segmentation. The three geometric factors are then incorporated into CIoU loss for better distinguishing difficult regression cases. The training of deep models using CIoU loss results in consistent AP and AR improvements in comparison to widely adopted $\ell_n$-norm loss and IoU-based loss. Furthermore, we propose Cluster-NMS, where NMS during inference is done by implicitly clustering detected boxes and usually requires less iterations. Cluster-NMS is very efficient due to its pure GPU implementation, and geometric factors can be incorporated to improve both AP and AR. In the experiments, CIoU loss and Cluster-NMS have been applied to state-of-the-art instance segmentation (e.g., YOLACT and BlendMask-RT), and object detection (e.g., YOLO v3, SSD and Faster R-CNN) models. Taking YOLACT on MS COCO as an example, our method achieves performance gains as +1.7 AP and +6.2 AR$_{100}$ for object detection, and +0.9 AP and +3.5 AR$_{100}$ for instance segmentation, with 27.1 FPS on one NVIDIA GTX 1080Ti GPU. All the source code and trained models are available at https://github.com/Zzh-tju/CIoU",0
"Unprecedented progress has been made in deep learning-based object detection and instance segmentation. This paper proposes the use of Complete-IoU (CIoU) loss and Cluster-NMS to enhance geometric factors in bounding box regression and Non-Maximum Suppression (NMS), leading to significant improvements in average precision (AP) and average recall (AR), without sacrificing inference efficiency. Three crucial geometric factors, namely overlap area, normalized central point distance, and aspect ratio, are incorporated into the CIoU loss to distinguish difficult regression cases. Training deep models using CIoU loss consistently results in better AP and AR improvements compared to the widely adopted $\ell_n$-norm loss and IoU-based loss. The paper also introduces Cluster-NMS, which clusters detected boxes during inference and requires fewer iterations, making it highly efficient with pure GPU implementation. Geometric factors can be incorporated into Cluster-NMS to improve both AP and AR. The proposed method was applied to state-of-the-art instance segmentation and object detection models, resulting in significant performance gains. For example, using YOLACT on MS COCO, the method achieved performance gains of +1.7 AP and +6.2 AR$_{100}$ for object detection and +0.9 AP and +3.5 AR$_{100}$ for instance segmentation, with 27.1 FPS on one NVIDIA GTX 1080Ti GPU. The source code and trained models are available at https://github.com/Zzh-tju/CIoU.",1
"In this paper, we present Generic Object Detection (GenOD), one of the largest object detection systems deployed to a web-scale general visual search engine that can detect over 900 categories for all Microsoft Bing Visual Search queries in near real-time. It acts as a fundamental visual query understanding service that provides object-centric information and shows gains in multiple production scenarios, improving upon domain-specific models. We discuss the challenges of collecting data, training, deploying and updating such a large-scale object detection model with multiple dependencies. We discuss a data collection pipeline that reduces per-bounding box labeling cost by 81.5% and latency by 61.2% while improving on annotation quality. We show that GenOD can improve weighted average precision by over 20% compared to multiple domain-specific models. We also improve the model update agility by nearly 2 times with the proposed disjoint detector training compared to joint fine-tuning. Finally we demonstrate how GenOD benefits visual search applications by significantly improving object-level search relevance by 54.9% and user engagement by 59.9%.",0
"The following paper introduces GenOD, a web-scale object detection system that can detect over 900 categories for Microsoft Bing Visual Search queries in near real-time. This system serves as a crucial visual query understanding service that provides object-centric information and improves production scenarios, surpassing domain-specific models. The paper discusses the challenges faced during data collection, training, deployment, and updating of such a large-scale object detection model with multiple dependencies. The data collection pipeline is examined, which reduces the cost of per-bounding box labeling by 81.5%, latency by 61.2%, and improves annotation quality. Compared to multiple domain-specific models, GenOD enhances weighted average precision by over 20%. The proposed disjoint detector training also enhances model update agility by nearly 2 times, surpassing joint fine-tuning. Finally, the paper showcases how GenOD improves visual search applications by enhancing object-level search relevance by 54.9% and user engagement by 59.9%.",1
"In real applications, new object classes often emerge after the detection model has been trained on a prepared dataset with fixed classes. Due to the storage burden and the privacy of old data, sometimes it is impractical to train the model from scratch with both old and new data. Fine-tuning the old model with only new data will lead to a well-known phenomenon of catastrophic forgetting, which severely degrades the performance of modern object detectors. In this paper, we propose a novel \textbf{M}ulti-\textbf{V}iew \textbf{C}orrelation \textbf{D}istillation (MVCD) based incremental object detection method, which explores the correlations in the feature space of the two-stage object detector (Faster R-CNN). To better transfer the knowledge learned from the old classes and maintain the ability to learn new classes, we design correlation distillation losses from channel-wise, point-wise and instance-wise views to regularize the learning of the incremental model. A new metric named Stability-Plasticity-mAP is proposed to better evaluate both the stability for old classes and the plasticity for new classes in incremental object detection. The extensive experiments conducted on VOC2007 and COCO demonstrate that MVCD can effectively learn to detect objects of new classes and mitigate the problem of catastrophic forgetting.",0
"Often, new object classes arise in practical applications after a detection model has been trained on a fixed-class dataset. However, due to storage limitations and privacy concerns, it may be impractical to retrain the model from scratch with both old and new data. Fine-tuning the old model with only new data can result in catastrophic forgetting, severely impacting the performance of modern object detectors. In this study, we introduce a novel approach to incremental object detection called Multi-View Correlation Distillation (MVCD). This method leverages the correlations in the feature space of the Faster R-CNN two-stage detector to transfer knowledge from old classes and learn new ones. To maintain the ability to learn new classes while preserving stability for old ones, we design correlation distillation losses from channel-wise, point-wise, and instance-wise views. We introduce a new metric, Stability-Plasticity-mAP, to evaluate the performance of the incremental object detector. Our experiments on VOC2007 and COCO show that MVCD effectively detects objects of new classes and mitigates the problem of catastrophic forgetting.",1
"Clustering is an unsupervised machine learning method grouping data samples into clusters of similar objects. In practice, clustering has been used in numerous applications such as banking customers profiling, document retrieval, image segmentation, and e-commerce recommendation engines. However, the existing clustering techniques present significant limitations, from which is the dependability of their stability on the initialization parameters (e.g. number of clusters, centroids). Different solutions were presented in the literature to overcome this limitation (i.e. internal and external validation metrics). However, these solutions require high computational complexity and memory consumption, especially when dealing with big data. In this paper, we apply the recent object detection Deep Learning (DL) model, named YOLO-v5, to detect the initial clustering parameters such as the number of clusters with their sizes and centroids. Mainly, the proposed solution consists of adding a DL-based initialization phase making the clustering algorithms free of initialization. Two model solutions are provided in this work, one for isolated clusters and the other one for overlapping clusters. The features of the incoming dataset determine which model to use. Moreover, The results show that the proposed solution can provide near-optimal clusters initialization parameters with low computational and resources overhead compared to existing solutions.",0
"Clustering is a machine learning technique that groups similar data samples into clusters without the need for supervision. It has been widely used in various fields such as banking, document retrieval, image segmentation, and e-commerce recommendations. However, current clustering methods have significant limitations, particularly in terms of their dependence on initialization parameters like the number of clusters and centroids. To address this, previous studies have proposed internal and external validation metrics, but these require high computational resources, especially for large datasets. In this paper, we propose a solution that utilizes the YOLO-v5 Deep Learning model for object detection to detect the initial clustering parameters, such as the number of clusters, their sizes, and centroids. Our approach involves adding a DL-based initialization phase to make the clustering algorithm initialization-free. We provide two models for isolated and overlapping clusters, depending on the features of the incoming dataset. Our results demonstrate the effectiveness of our proposed solution, which provides near-optimal initialization parameters with low computational and resource overhead compared to existing methods.",1
"Object detection is essential to safe autonomous or assisted driving. Previous works usually utilize RGB images or LiDAR point clouds to identify and localize multiple objects in self-driving. However, cameras tend to fail in bad driving conditions, e.g. bad weather or weak lighting, while LiDAR scanners are too expensive to get widely deployed in commercial applications. Radar has been drawing more and more attention due to its robustness and low cost. In this paper, we propose a scene-aware radar learning framework for accurate and robust object detection. First, the learning framework contains branches conditioning on the scene category of the radar sequence; with each branch optimized for a specific type of scene. Second, three different 3D autoencoder-based architectures are proposed for radar object detection and ensemble learning is performed over the different architectures to further boost the final performance. Third, we propose novel scene-aware sequence mix augmentation (SceneMix) and scene-specific post-processing to generate more robust detection results. In the ROD2021 Challenge, we achieved a final result of average precision of 75.0% and an average recall of 81.0%. Moreover, in the parking lot scene, our framework ranks first with an average precision of 97.8% and an average recall of 98.6%, which demonstrates the effectiveness of our framework.",0
"Safe autonomous or assisted driving requires effective object detection. In the past, RGB images and LiDAR point clouds have been used to locate and identify objects in self-driving situations. However, cameras are not reliable in poor driving conditions, such as bad weather or weak lighting, and LiDAR scanners are too costly for wide-scale commercial use. Radar has recently gained popularity due to its low cost and robustness. This paper proposes a scene-aware radar learning framework for accurate and reliable object detection. The learning framework includes branches optimized for specific types of scenes, three different 3D autoencoder-based architectures for radar object detection, and ensemble learning to improve performance. Additionally, scene-aware sequence mix augmentation and scene-specific post-processing are used to produce more robust detection results. The framework achieved an average precision of 75.0% and an average recall of 81.0% in the ROD2021 Challenge. In the parking lot scene, it ranked first with an average precision of 97.8% and an average recall of 98.6%, demonstrating its effectiveness.",1
"Modeling implicit feature interaction patterns is of significant importance to object detection tasks. However, in the two-stage detectors, due to the excessive use of hand-crafted components, it is very difficult to reason about the implicit relationship of the instance features. To tackle this problem, we analyze three different levels of feature interaction relationships, namely, the dependency relationship between the cropped local features and global features, the feature autocorrelation within the instance, and the cross-correlation relationship between the instances. To this end, we propose a more compact object detector head network (CODH), which can not only preserve global context information and condense the information density, but also allows instance-wise feature enhancement and relational reasoning in a larger matrix space. Without bells and whistles, our method can effectively improve the detection performance while significantly reducing the parameters of the model, e.g., with our method, the parameters of the head network is 0.6 times smaller than the state-of-the-art Cascade R-CNN, yet the performance boost is 1.3% on COCO test-dev. Without losing generality, we can also build a more lighter head network for other multi-stage detectors by assembling our method.",0
"The importance of modeling implicit feature interaction patterns for object detection tasks cannot be understated. However, in two-stage detectors where hand-crafted components are excessively used, reasoning about the implicit relationship of instance features becomes a challenge. Consequently, we propose an approach that analyzes three different levels of feature interaction relationships, namely, the dependency relationship between cropped local features and global features, feature autocorrelation within an instance, and cross-correlation relationship between instances. Our approach is achieved through a more compact object detector head network (CODH) that preserves global context information, condenses information density, allows instance-wise feature enhancement, and relational reasoning in a larger matrix space. Importantly, our method significantly improves detection performance and reduces model parameters. For instance, the parameters of our head network are 0.6 times smaller than Cascade R-CNN's, yet we achieve a 1.3% performance boost on COCO test-dev. Our approach can also be used to build a lighter head network for other multi-stage detectors.",1
"Feature pyramid network (FPN) is a critical component in modern object detection frameworks. The performance gain in most of the existing FPN variants is mainly attributed to the increase of computational burden. An attempt to enhance the FPN is enriching the spatial information by expanding the receptive fields, which is promising to largely improve the detection accuracy. In this paper, we first investigate how expanding the receptive fields affect the accuracy and computational costs of FPN. We explore a baseline model called inception FPN in which each lateral connection contains convolution filters with different kernel sizes. Moreover, we point out that not all objects need such a complicated calculation and propose a new dynamic FPN (DyFPN). The output features of DyFPN will be calculated by using the adaptively selected branch according to a dynamic gating operation. Therefore, the proposed method can provide a more efficient dynamic inference for achieving a better trade-off between accuracy and computational cost. Extensive experiments conducted on MS-COCO benchmark demonstrate that the proposed DyFPN significantly improves performance with the optimal allocation of computation resources. For instance, replacing inception FPN with DyFPN reduces about 40% of its FLOPs while maintaining similar high performance.",0
"Modern object detection frameworks rely heavily on the Feature Pyramid Network (FPN). However, existing FPN variants that have shown performance improvements have also increased the computational burden. To improve FPN's accuracy, we explore the expansion of receptive fields to enrich spatial information. Our study investigates how this expansion impacts accuracy and computational costs. We propose a baseline model, Inception FPN, which utilizes different kernel sizes in each lateral connection. We also suggest a new Dynamic FPN (DyFPN) that adapts to each object's complexity through a dynamic gating operation. Our proposed method provides a more efficient inference process, achieving a better trade-off between accuracy and computational cost. Experiments on the MS-COCO benchmark show that DyFPN significantly improves performance while reducing FLOPs by 40% compared to Inception FPN.",1
"Nonlinear estimation in robotics and vision is typically plagued with outliers due to wrong data association, or to incorrect detections from signal processing and machine learning methods. This paper introduces two unifying formulations for outlier-robust estimation, Generalized Maximum Consensus (G-MC) and Generalized Truncated Least Squares (G-TLS), and investigates fundamental limits, practical algorithms, and applications. Our first contribution is a proof that outlier-robust estimation is inapproximable: in the worst case, it is impossible to (even approximately) find the set of outliers, even with slower-than-polynomial-time algorithms (particularly, algorithms running in quasi-polynomial time). As a second contribution, we review and extend two general-purpose algorithms. The first, Adaptive Trimming (ADAPT), is combinatorial, and is suitable for G-MC; the second, Graduated Non-Convexity (GNC), is based on homotopy methods, and is suitable for G-TLS. We extend ADAPT and GNC to the case where the user does not have prior knowledge of the inlier-noise statistics (or the statistics may vary over time) and is unable to guess a reasonable threshold to separate inliers from outliers (as the one commonly used in RANSAC). We propose the first minimally tuned algorithms for outlier rejection, that dynamically decide how to separate inliers from outliers. Our third contribution is an evaluation of the proposed algorithms on robot perception problems: mesh registration, image-based object detection (shape alignment), and pose graph optimization. ADAPT and GNC execute in real-time, are deterministic, outperform RANSAC, and are robust up to 80-90% outliers. Their minimally tuned versions also compare favorably with the state of the art, even though they do not rely on a noise bound for the inliers.",0
"Typically, nonlinear estimation in robotics and vision encounters issues with outliers due to incorrect data association or detections caused by signal processing and machine learning methods. This study presents two unified approaches for outlier-robust estimation, known as Generalized Maximum Consensus (G-MC) and Generalized Truncated Least Squares (G-TLS), and investigates their fundamental limits, practical algorithms, and applications. The research first demonstrates that outlier-robust estimation is unattainable, even with slower-than-polynomial-time algorithms. Secondly, the study extends two general-purpose algorithms, Adaptive Trimming (ADAPT) and Graduated Non-Convexity (GNC), to be suitable for G-MC and G-TLS respectively, even when the user lacks prior knowledge of the inlier-noise statistics or is unable to guess a reasonable threshold to separate inliers from outliers. The study evaluates the proposed algorithms on robot perception problems, including mesh registration, image-based object detection, and pose graph optimization, and finds that ADAPT and GNC execute in real-time, are deterministic, outperform RANSAC, and are robust up to 80-90% outliers. The minimally tuned versions of these algorithms also compare favorably with the state of the art, despite not relying on a noise bound for the inliers.",1
"Besides accuracy, the model size of convolutional neural networks (CNN) models is another important factor considering limited hardware resources in practical applications. For example, employing deep neural networks on mobile systems requires the design of accurate yet fast CNN for low latency in classification and object detection. To fulfill the need, we aim at obtaining CNN models with both high testing accuracy and small size to address resource constraints in many embedded devices. In particular, this paper focuses on proposing a generic reinforcement learning-based model compression approach in a two-stage compression pipeline: pruning and quantization. The first stage of compression, i.e., pruning, is achieved via exploiting deep reinforcement learning (DRL) to co-learn the accuracy and the FLOPs updated after layer-wise channel pruning and element-wise variational pruning via information dropout. The second stage, i.e., quantization, is achieved via a similar DRL approach but focuses on obtaining the optimal bits representation for individual layers. We further conduct experimental results on CIFAR-10 and ImageNet datasets. For the CIFAR-10 dataset, the proposed method can reduce the size of VGGNet by 9x from 20.04MB to 2.2MB with a slight accuracy increase. For the ImageNet dataset, the proposed method can reduce the size of VGG-16 by 33x from 138MB to 4.14MB with no accuracy loss.",0
"Considering the limited hardware resources in practical applications, the size of convolutional neural network (CNN) models, in addition to their accuracy, is an important factor. For instance, mobile systems require accurate and fast CNN models with low latency for classification and object detection. Therefore, our goal is to obtain CNN models with high testing accuracy and small size to cater to the resource constraints of embedded devices. This paper proposes a reinforcement learning-based approach for model compression, focusing on two stages: pruning and quantization. The first stage involves deep reinforcement learning (DRL) to balance accuracy and floating-point operations (FLOPs) after layer-wise channel pruning and element-wise variational pruning using information dropout. The second stage uses a similar DRL approach to determine the optimal bit representation for individual layers. We conducted experiments on both CIFAR-10 and ImageNet datasets, and the proposed method reduced the size of VGGNet by 9x to 2.2MB with a slight increase in accuracy for the CIFAR-10 dataset. For the ImageNet dataset, the proposed method reduced the size of VGG-16 by 33x to 4.14MB with no loss of accuracy.",1
"In this technical report, we briefly introduce the solution of our team ""TAL-ai"" for (Semi-) supervised Face detection in the low light condition in UG2+ Challenge in CVPR 2021. By conducting several experiments with popular image enhancement methods and image transfer methods, we pulled the low light image and the normal image to a more closer domain. And it is observed that using these data to training can achieve better performance. We also adapt several popular object detection frameworks, e.g., DetectoRS, Cascade-RCNN, and large backbone like Swin-transformer. Finally, we ensemble several models which achieved mAP 74.89 on the testing set, ranking 1st on the final leaderboard.",0
"Our team, ""TAL-ai,"" presents our solution for (semi-)supervised face detection in low light conditions in the UG2+ Challenge at CVPR 2021 in this technical report. We conducted various experiments using popular image enhancement and image transfer methods to bring low light images closer to normal images. These experiments showed that training with this data resulted in better performance. We also adapted popular object detection frameworks, such as DetectoRS, Cascade-RCNN, and utilized a large backbone like Swin-transformer. Finally, we combined multiple models, achieving an mAP of 74.89 on the testing set, securing the top spot on the final leaderboard.",1
"Recently, Vision Transformer and its variants have shown great promise on various computer vision tasks. The ability of capturing short- and long-range visual dependencies through self-attention is arguably the main source for the success. But it also brings challenges due to quadratic computational overhead, especially for the high-resolution vision tasks (e.g., object detection). In this paper, we present focal self-attention, a new mechanism that incorporates both fine-grained local and coarse-grained global interactions. Using this new mechanism, each token attends the closest surrounding tokens at fine granularity but the tokens far away at coarse granularity, and thus can capture both short- and long-range visual dependencies efficiently and effectively. With focal self-attention, we propose a new variant of Vision Transformer models, called Focal Transformer, which achieves superior performance over the state-of-the-art vision Transformers on a range of public image classification and object detection benchmarks. In particular, our Focal Transformer models with a moderate size of 51.1M and a larger size of 89.8M achieve 83.5 and 83.8 Top-1 accuracy, respectively, on ImageNet classification at 224x224 resolution. Using Focal Transformers as the backbones, we obtain consistent and substantial improvements over the current state-of-the-art Swin Transformers for 6 different object detection methods trained with standard 1x and 3x schedules. Our largest Focal Transformer yields 58.7/58.9 box mAPs and 50.9/51.3 mask mAPs on COCO mini-val/test-dev, and 55.4 mIoU on ADE20K for semantic segmentation, creating new SoTA on three of the most challenging computer vision tasks.",0
"The success of Vision Transformer and its variations in various computer vision tasks is mainly due to their ability to capture short- and long-range visual dependencies through self-attention. However, this also poses challenges, particularly for high-resolution vision tasks like object detection, due to the quadratic computational overhead. To address this, we introduce focal self-attention, a novel mechanism that combines fine-grained local and coarse-grained global interactions. With this mechanism, each token can attend to nearby tokens at a fine granularity but to distant tokens at a coarse granularity, enabling it to efficiently capture both short- and long-range visual dependencies. We also present a new variant of Vision Transformer models called Focal Transformer, which employs focal self-attention and outperforms existing state-of-the-art models on various image classification and object detection benchmarks. Using Focal Transformers as the backbone, we achieve significant improvements for six different object detection methods trained with standard schedules. Our largest Focal Transformer also sets new standards on three of the most challenging computer vision tasks, including COCO mini-val/test-dev and ADE20K for semantic segmentation.",1
"TANet is one of state-of-the-art 3D object detection method on KITTI and JRDB benchmark, the network contains a Triple Attention module and Coarse-to-Fine Regression module to improve the robustness and accuracy of 3D Detection. However, since the original input data (point clouds) contains a lot of noise during collecting the data, which will further affect the training of the model. For example, the object is far from the robot, the sensor is difficult to obtain enough pointcloud. If the objects only contains few point clouds, and the samples are fed into model with the normal samples together during training, the detector will be difficult to distinguish the individual with few pointcloud belong to object or background. In this paper, we propose TANet++ to improve the performance on 3D Detection, which adopt a novel training strategy on training the TANet. In order to reduce the negative impact by the weak samples, the training strategy previously filtered the training data, and then the TANet++ is trained by the rest of data. The experimental results shows that AP score of TANet++ is 8.98 higher than TANet on JRDB benchmark.",0
"TANet is a cutting-edge 3D object detection method utilized in KITTI and JRDB benchmark. It incorporates a Triple Attention module and Coarse-to-Fine Regression module, which enhances the precision and resilience of 3D Detection. However, the input data (point clouds) is susceptible to noise during collection, thereby affecting the model's training. For instance, if an object is far from the robot, obtaining enough point clouds becomes challenging. Moreover, feeding samples with only a few point clouds into the model during training alongside normal samples can make it arduous for the detector to differentiate between the object and the background. To overcome these limitations, we propose TANet++, which utilizes a novel training strategy to enhance 3D Detection performance. The training strategy filters out the weak samples, and the TANet++ is then trained using the remaining data, thus reducing the adverse effect of weak samples. Experimental results reveal that TANet++ outperforms TANet on JRDB benchmark with an 8.98 higher AP score.",1
"Driven by recent advances in object detection with deep neural networks, the tracking-by-detection paradigm has gained increasing prevalence in the research community of multi-object tracking (MOT). It has long been known that appearance information plays an essential role in the detection-to-track association, which lies at the core of the tracking-by-detection paradigm. While most existing works consider the appearance distances between the detections and the tracks, they ignore the statistical information implied by the historical appearance distance records in the tracks, which can be particularly useful when a detection has similar distances with two or more tracks. In this work, we propose a hybrid track association (HTA) algorithm that models the historical appearance distances of a track with an incremental Gaussian mixture model (IGMM) and incorporates the derived statistical information into the calculation of the detection-to-track association cost. Experimental results on three MOT benchmarks confirm that HTA effectively improves the target identification performance with a small compromise to the tracking speed. Additionally, compared to many state-of-the-art trackers, the DeepSORT tracker equipped with HTA achieves better or comparable performance in terms of the balance of tracking quality and speed.",0
"The tracking-by-detection paradigm has gained popularity in the multi-object tracking (MOT) research community due to advances in object detection with deep neural networks. Appearance information has long been recognized as crucial in the detection-to-track association that forms the foundation of this paradigm. Although most current works focus on appearance distances between detections and tracks, they overlook the statistical information conveyed by historical appearance distance records in tracks, which could be particularly useful when a detection has similar distances with several tracks. In this study, we propose a hybrid track association (HTA) algorithm that utilizes an incremental Gaussian mixture model (IGMM) to model the historical appearance distances of a track and employs the resulting statistical information to calculate the detection-to-track association cost. Our experiments on three MOT benchmarks demonstrate that HTA enhances target identification performance with minimal impact on tracking speed. Moreover, the DeepSORT tracker equipped with HTA outperforms or performs comparably to many state-of-the-art trackers in terms of the balance between tracking quality and speed.",1
"At the heart of all automated driving systems is the ability to sense the surroundings, e.g., through semantic segmentation of LiDAR sequences, which experienced a remarkable progress due to the release of large datasets such as SemanticKITTI and nuScenes-LidarSeg. While most previous works focus on sparse segmentation of the LiDAR input, dense output masks provide self-driving cars with almost complete environment information. In this paper, we introduce MASS - a Multi-Attentional Semantic Segmentation model specifically built for dense top-view understanding of the driving scenes. Our framework operates on pillar- and occupancy features and comprises three attention-based building blocks: (1) a keypoint-driven graph attention, (2) an LSTM-based attention computed from a vector embedding of the spatial input, and (3) a pillar-based attention, resulting in a dense 360-degree segmentation mask. With extensive experiments on both, SemanticKITTI and nuScenes-LidarSeg, we quantitatively demonstrate the effectiveness of our model, outperforming the state of the art by 19.0% on SemanticKITTI and reaching 32.7% in mIoU on nuScenes-LidarSeg, where MASS is the first work addressing the dense segmentation task. Furthermore, our multi-attention model is shown to be very effective for 3D object detection validated on the KITTI-3D dataset, showcasing its high generalizability to other tasks related to 3D vision.",0
"Automated driving systems rely on the ability to detect the surrounding environment, which can be achieved through semantic segmentation of LiDAR sequences. Recent advancements in this area are attributed to the availability of large datasets such as SemanticKITTI and nuScenes-LidarSeg. While previous works have focused on sparse segmentation of LiDAR input, dense output masks offer self-driving cars almost complete environmental information. This paper introduces MASS, a Multi-Attentional Semantic Segmentation model designed for dense top-view understanding of driving scenes. MASS operates on pillar and occupancy features and comprises three attention-based building blocks. These include a keypoint-driven graph attention, an LSTM-based attention computed from a vector embedding of the spatial input, and a pillar-based attention, resulting in a dense 360-degree segmentation mask. Extensive experiments on SemanticKITTI and nuScenes-LidarSeg demonstrate the effectiveness of the model, outperforming the state of the art by 19.0% on SemanticKITTI and achieving 32.7% in mIoU on nuScenes-LidarSeg. MASS is the first work to address the dense segmentation task in nuScenes-LidarSeg. Moreover, the multi-attention model is highly effective for 3D object detection validated on the KITTI-3D dataset, showcasing its high generalizability to other 3D vision tasks.",1
"The research of visual signal compression has a long history. Fueled by deep learning, exciting progress has been made recently. Despite achieving better compression performance, existing end-to-end compression algorithms are still designed towards better signal quality in terms of rate-distortion optimization. In this paper, we show that the design and optimization of network architecture could be further improved for compression towards machine vision. We propose an inverted bottleneck structure for end-to-end compression towards machine vision, which specifically accounts for efficient representation of the semantic information. Moreover, we quest the capability of optimization by incorporating the analytics accuracy into the optimization process, and the optimality is further explored with generalized rate-accuracy optimization in an iterative manner. We use object detection as a showcase for end-to-end compression towards machine vision, and extensive experiments show that the proposed scheme achieves significant BD-rate savings in terms of analysis performance. Moreover, the promise of the scheme is also demonstrated with strong generalization capability towards other machine vision tasks, due to the enabling of signal-level reconstruction.",0
"The study of compressing visual signals has a lengthy background. With the advancement of deep learning, there have been notable advancements. However, current end-to-end compression methods are still focused on improving the signal quality through rate-distortion optimization. Our research aims to enhance the network architecture design and optimization for compression in machine vision. We propose an inverted bottleneck structure that efficiently represents semantic information for end-to-end compression in machine vision. Additionally, we incorporate analytics accuracy into the optimization process and explore optimality with generalized rate-accuracy optimization in an iterative way. We use object detection as an example of end-to-end compression in machine vision and demonstrate that our proposed method achieves significant BD-rate savings in terms of analysis performance. Furthermore, our scheme's potential is exhibited with strong generalization capabilities for other machine vision tasks due to the signal-level reconstruction.",1
"RGB-D salient object detection(SOD) demonstrates its superiority on detecting in complex environments due to the additional depth information introduced in the data. Inevitably, an independent stream is introduced to extract features from depth images, leading to extra computation and parameters. This methodology which sacrifices the model size to improve the detection accuracy may impede the practical application of SOD problems. To tackle this dilemma, we propose a dynamic distillation method along with a lightweight framework, which significantly reduces the parameters. This method considers the factors of both teacher and student performance within the training stage and dynamically assigns the distillation weight instead of applying a fixed weight on the student model. Extensive experiments are conducted on five public datasets to demonstrate that our method can achieve competitive performance compared to 10 prior methods through a 78.2MB lightweight structure.",0
"The introduction of depth information in RGB-D salient object detection (SOD) has made it more effective in detecting objects in complex environments. However, this comes at the cost of increased computation and parameters as an independent stream is needed to extract features from the depth images. This approach, although improving detection accuracy, can limit the practical application of SOD. To address this issue, we propose a dynamic distillation method and a lightweight framework that significantly reduces parameters. Our method considers both teacher and student performance during training and assigns distillation weight dynamically instead of applying a fixed weight on the student model. We conducted extensive experiments on five public datasets and demonstrate that our method achieves competitive performance compared to 10 prior methods using a 78.2MB lightweight structure.",1
"The speed-accuracy Pareto curve of object detection systems have advanced through a combination of better model architectures, training and inference methods. In this paper, we methodically evaluate a variety of these techniques to understand where most of the improvements in modern detection systems come from. We benchmark these improvements on the vanilla ResNet-FPN backbone with RetinaNet and RCNN detectors. The vanilla detectors are improved by 7.7% in accuracy while being 30% faster in speed. We further provide simple scaling strategies to generate family of models that form two Pareto curves, named RetinaNet-RS and Cascade RCNN-RS. These simple rescaled detectors explore the speed-accuracy trade-off between the one-stage RetinaNet detectors and two-stage RCNN detectors. Our largest Cascade RCNN-RS models achieve 52.9% AP with a ResNet152-FPN backbone and 53.6% with a SpineNet143L backbone. Finally, we show the ResNet architecture, with three minor architectural changes, outperforms EfficientNet as the backbone for object detection and instance segmentation systems.",0
"A combination of improved model architectures, training and inference methods have led to advancements in the speed-accuracy Pareto curve of object detection systems. The aim of this paper is to systematically evaluate these techniques in order to determine where the majority of the improvements in modern detection systems come from. Our evaluation is conducted on vanilla ResNet-FPN backbone with RetinaNet and RCNN detectors. Through our experimentation, we were able to improve the accuracy of vanilla detectors by 7.7% whilst increasing their speed by 30%. Further, we provide simple scaling strategies that generate two Pareto curves, RetinaNet-RS and Cascade RCNN-RS, which explore the speed-accuracy trade-off between one-stage RetinaNet detectors and two-stage RCNN detectors. Our largest Cascade RCNN-RS models achieved 52.9% AP with a ResNet152-FPN backbone and 53.6% with a SpineNet143L backbone. Lastly, we demonstrate that with three minor architectural changes, the ResNet architecture outperforms EfficientNet as the backbone for object detection and instance segmentation systems.",1
"Compared to many other dense prediction tasks, e.g., semantic segmentation, it is the arbitrary number of instances that has made instance segmentation much more challenging. In order to predict a mask for each instance, mainstream approaches either follow the 'detect-then-segment' strategy (e.g., Mask R-CNN), or predict embedding vectors first then cluster pixels into individual instances. In this paper, we view the task of instance segmentation from a completely new perspective by introducing the notion of ""instance categories"", which assigns categories to each pixel within an instance according to the instance's location. With this notion, we propose segmenting objects by locations (SOLO), a simple, direct, and fast framework for instance segmentation with strong performance. We derive a few SOLO variants (e.g., Vanilla SOLO, Decoupled SOLO, Dynamic SOLO) following the basic principle. Our method directly maps a raw input image to the desired object categories and instance masks, eliminating the need for the grouping post-processing or the bounding box detection. Our approach achieves state-of-the-art results for instance segmentation in terms of both speed and accuracy, while being considerably simpler than the existing methods. Besides instance segmentation, our method yields state-of-the-art results in object detection (from our mask byproduct) and panoptic segmentation. We further demonstrate the flexibility and high-quality segmentation of SOLO by extending it to perform one-stage instance-level image matting. Code is available at: https://git.io/AdelaiDet",0
"Instance segmentation has proven to be a more challenging task compared to other dense prediction tasks like semantic segmentation due to the arbitrary number of instances. Predicting a mask for each instance can be achieved through mainstream approaches such as the 'detect-then-segment' strategy or predicting embedding vectors and clustering pixels into individual instances. However, in this paper, we introduce a new perspective on instance segmentation through the concept of ""instance categories."" This concept assigns categories to each pixel within an instance based on the instance's location, leading to the development of our segmenting objects by locations (SOLO) framework. Our approach eliminates the need for grouping post-processing or bounding box detection and achieves state-of-the-art results in terms of both speed and accuracy for instance segmentation. Additionally, our method yields state-of-the-art results in object detection and panoptic segmentation. We further demonstrate the flexibility and high-quality segmentation of SOLO by extending it to perform one-stage instance-level image matting. Code is available at: https://git.io/AdelaiDet.",1
"Monocular 3D object detection is an important task in autonomous driving. It can be easily intractable where there exists ego-car pose change w.r.t. ground plane. This is common due to the slight fluctuation of road smoothness and slope. Due to the lack of insight in industrial application, existing methods on open datasets neglect the camera pose information, which inevitably results in the detector being susceptible to camera extrinsic parameters. The perturbation of objects is very popular in most autonomous driving cases for industrial products. To this end, we propose a novel method to capture camera pose to formulate the detector free from extrinsic perturbation. Specifically, the proposed framework predicts camera extrinsic parameters by detecting vanishing point and horizon change. A converter is designed to rectify perturbative features in the latent space. By doing so, our 3D detector works independent of the extrinsic parameter variations and produces accurate results in realistic cases, e.g., potholed and uneven roads, where almost all existing monocular detectors fail to handle. Experiments demonstrate our method yields the best performance compared with the other state-of-the-arts by a large margin on both KITTI 3D and nuScenes datasets.",0
"Autonomous driving relies heavily on the ability to detect 3D objects using a single camera, however, this task can become difficult when there is a change in the ego-car pose with respect to the ground plane. This is often the case due to uneven road surfaces and slopes. Existing methods do not consider the camera pose information, making the detector susceptible to extrinsic perturbations. Our proposed method addresses this issue by predicting camera extrinsic parameters using vanishing point and horizon change detection. A converter rectifies perturbative features in the latent space, allowing our 3D detector to produce accurate results in realistic scenarios, such as potholed and uneven roads. Compared to other state-of-the-art methods, our approach performs significantly better on both KITTI 3D and nuScenes datasets.",1
"To reduce annotation labor associated with object detection, an increasing number of studies focus on transferring the learned knowledge from a labeled source domain to another unlabeled target domain. However, existing methods assume that the labeled data are sampled from a single source domain, which ignores a more generalized scenario, where labeled data are from multiple source domains. For the more challenging task, we propose a unified Faster R-CNN based framework, termed Divide-and-Merge Spindle Network (DMSN), which can simultaneously enhance domain invariance and preserve discriminative power. Specifically, the framework contains multiple source subnets and a pseudo target subnet. First, we propose a hierarchical feature alignment strategy to conduct strong and weak alignments for low- and high-level features, respectively, considering their different effects for object detection. Second, we develop a novel pseudo subnet learning algorithm to approximate optimal parameters of pseudo target subset by weighted combination of parameters in different source subnets. Finally, a consistency regularization for region proposal network is proposed to facilitate each subnet to learn more abstract invariances. Extensive experiments on different adaptation scenarios demonstrate the effectiveness of the proposed model.",0
"An increasing number of studies aim to reduce the annotation labor associated with object detection by transferring learned knowledge from a labeled source domain to an unlabeled target domain. However, existing methods assume that the labeled data are from a single source domain, which does not account for a more generalized scenario where the labeled data are from multiple source domains. To address this challenge, we propose a unified Faster R-CNN based framework, called Divide-and-Merge Spindle Network (DMSN), which enhances domain invariance and preserves discriminative power. The framework includes multiple source subnets and a pseudo target subnet. We propose a hierarchical feature alignment strategy to conduct strong and weak alignments for low- and high-level features, respectively, and we develop a novel pseudo subnet learning algorithm to approximate optimal parameters of the pseudo target subset. Finally, a consistency regularization for region proposal network is proposed to facilitate each subnet to learn more abstract invariances. Extensive experiments demonstrate the effectiveness of the proposed model in different adaptation scenarios.",1
"Self-supervised contrastive learning has demonstrated great potential in learning visual representations. Despite their success on various downstream tasks such as image classification and object detection, self-supervised pre-training for fine-grained scenarios is not fully explored. In this paper, we first point out that current contrastive methods are prone to memorizing background/foreground texture and therefore have a limitation in localizing the foreground object. Analysis suggests that learning to extract discriminative texture information and localization are equally crucial for self-supervised pre-training under fine-grained scenarios. Based on our findings, we introduce Cross-view Saliency Alignment (CVSA), a contrastive learning framework that first crops and swaps saliency regions of images as a novel view generation and then guides the model to localize on the foreground object via a cross-view alignment loss. Extensive experiments on four popular fine-grained classification benchmarks show that CVSA significantly improves the learned representation.",0
"Learning visual representations through self-supervised contrastive learning has exhibited remarkable potential. However, while these methods have shown success in various downstream tasks like object detection and image classification, their application in fine-grained scenarios has not been thoroughly explored. This paper highlights that current contrastive techniques tend to memorize background/foreground patterns, which restricts their ability to locate the foreground object. Our analysis indicates that for self-supervised pre-training under fine-grained scenarios, it is equally critical to learn discriminative texture information and localization. Consequently, we propose a contrastive learning framework, Cross-view Saliency Alignment (CVSA), that generates novel views by cropping and swapping saliency regions of images. CVSA then guides the model to localize the foreground object through a cross-view alignment loss. Our experiments on four popular fine-grained classification benchmarks show that the learned representation significantly improves with CVSA.",1
"Recently deep neural networks (DNNs) have achieved tremendous success for object detection in overhead (e.g., satellite) imagery. One ongoing challenge however is the acquisition of training data, due to high costs of obtaining satellite imagery and annotating objects in it. In this work we present a simple approach - termed Synthetic object IMPLantation (SIMPL) - to easily and rapidly generate large quantities of synthetic overhead training data for custom target objects. We demonstrate the effectiveness of using SIMPL synthetic imagery for training DNNs in zero-shot scenarios where no real imagery is available; and few-shot learning scenarios, where limited real-world imagery is available. We also conduct experiments to study the sensitivity of SIMPL's effectiveness to some key design parameters, providing users for insights when designing synthetic imagery for custom objects. We release a software implementation of our SIMPL approach so that others can build upon it, or use it for their own custom problems.",0
"Object detection in overhead imagery has been revolutionized by the success of deep neural networks (DNNs). However, the high costs associated with acquiring satellite imagery and annotating objects make training data acquisition a persistent challenge. To address this issue, we propose Synthetic object IMPLantation (SIMPL), a simple approach for generating large quantities of synthetic overhead training data for custom target objects quickly and easily. We demonstrate that using SIMPL synthetic imagery is effective for training DNNs in scenarios where no real imagery is available (zero-shot) as well as in scenarios where limited real-world imagery is available (few-shot). We also investigate the sensitivity of SIMPL's effectiveness to key design parameters, providing users with insights for designing synthetic imagery for custom objects. We have released a software implementation of our SIMPL approach to enable others to use it for their own custom problems or build upon it.",1
"Perceiving the world in terms of objects and tracking them through time is a crucial prerequisite for reasoning and scene understanding. Recently, several methods have been proposed for unsupervised learning of object-centric representations. However, since these models were evaluated on different downstream tasks, it remains unclear how they compare in terms of basic perceptual abilities such as detection, figure-ground segmentation and tracking of objects. To close this gap, we design a benchmark with four data sets of varying complexity and seven additional test sets featuring challenging tracking scenarios relevant for natural videos. Using this benchmark, we compare the perceptual abilities of four object-centric approaches: ViMON, a video-extension of MONet, based on recurrent spatial attention, OP3, which exploits clustering via spatial mixture models, as well as TBA and SCALOR, which use explicit factorization via spatial transformers. Our results suggest that the architectures with unconstrained latent representations learn more powerful representations in terms of object detection, segmentation and tracking than the spatial transformer based architectures. We also observe that none of the methods are able to gracefully handle the most challenging tracking scenarios despite their synthetic nature, suggesting that our benchmark may provide fruitful guidance towards learning more robust object-centric video representations.",0
"Having the ability to perceive the world in terms of objects and track them over time is essential for reasoning and understanding scenes. Several unsupervised learning methods have been proposed for object-centric representations, but it is unclear how they compare in basic perceptual abilities such as object detection, figure-ground segmentation, and object tracking since they were evaluated on different downstream tasks. To address this gap, we created a benchmark comprising four data sets of varying complexity and seven additional test sets with challenging tracking scenarios relevant to natural videos. We compared the perceptual abilities of four object-centric approaches using this benchmark: ViMON, which is a video-extension of MONet based on recurrent spatial attention; OP3, which uses clustering via spatial mixture models; and TBA and SCALOR, which utilize explicit factorization via spatial transformers. Our findings suggest that unconstrained latent representations in architectures learn more robust representations in object detection, segmentation, and tracking than spatial transformer-based architectures. However, none of the methods could handle the most challenging tracking scenarios effectively despite their synthetic nature, indicating that our benchmark could guide learning more sturdy object-centric video representations.",1
"In this paper, we introduce a new technique that combines two popular methods to estimate uncertainty in object detection. Quantifying uncertainty is critical in real-world robotic applications. Traditional detection models can be ambiguous even when they provide a high-probability output. Robot actions based on high-confidence, yet unreliable predictions, may result in serious repercussions. Our framework employs deep ensembles and Monte Carlo dropout for approximating predictive uncertainty, and it improves upon the uncertainty estimation quality of the baseline method. The proposed approach is evaluated on publicly available synthetic image datasets captured from sequences of video.",0
"The paper presents a novel approach that merges two well-known techniques for estimating uncertainty in object detection. Accurately measuring uncertainty is essential for practical robotic applications, as conventional detection models can produce uncertain results, despite high-probability outputs. Relying on such unreliable predictions can lead to severe consequences. To address this issue, the authors employ deep ensembles and Monte Carlo dropout to approximate predictive uncertainty, which enhances the baseline method's quality of uncertainty estimation. The proposed technique is assessed on publicly available synthetic image datasets acquired from video sequences.",1
"We present an object detection based approach to localize handwritten regions from documents, which initially aims to enhance the anonymization during the data transmission. The concatenated fusion of original and preprocessed images containing both printed texts and handwritten notes or signatures are fed into the convolutional neural network, where the bounding boxes are learned to detect the handwriting. Afterwards, the handwritten regions can be processed (e.g. replaced with redacted signatures) to conceal the personally identifiable information (PII). This processing pipeline based on the deep learning network Cascade R-CNN works at 10 fps on a GPU during the inference, which ensures the enhanced anonymization with minimal computational overheads. Furthermore, the impressive generalizability has been empirically showcased: the trained model based on the English-dominant dataset works well on the fictitious unseen invoices, even in Chinese. The proposed approach is also expected to facilitate other tasks such as handwriting recognition and signature verification.",0
"We introduce an approach for detecting handwritten regions in documents using object detection to improve anonymization during data transmission. The approach involves using concatenated original and preprocessed images with printed texts and handwritten notes or signatures as input for a convolutional neural network. The network learns to detect handwriting by identifying bounding boxes. The resulting handwritten regions can then be processed, such as by replacing signatures with redacted versions to protect personally identifiable information. Our approach, which employs the deep learning network Cascade R-CNN, operates at 10 fps on a GPU during inference, achieving enhanced anonymization with minimal computational overhead. Empirical evidence demonstrates impressive generalizability, as the model trained on an English-dominant dataset performs well on fictitious, unseen invoices, even in Chinese. Additionally, the proposed approach has potential applications in handwriting recognition and signature verification.",1
"Most deep learning models are data-driven and the excellent performance is highly dependent on the abundant and diverse datasets. However, it is very hard to obtain and label the datasets of some specific scenes or applications. If we train the detector using the data from one domain, it cannot perform well on the data from another domain due to domain shift, which is one of the big challenges of most object detection models. To address this issue, some image-to-image translation techniques have been employed to generate some fake data of some specific scenes to train the models. With the advent of Generative Adversarial Networks (GANs), we could realize unsupervised image-to-image translation in both directions from a source to a target domain and from the target to the source domain. In this study, we report a new approach to making use of the generated images. We propose to concatenate the original 3-channel images and their corresponding GAN-generated fake images to form 6-channel representations of the dataset, hoping to address the domain shift problem while exploiting the success of available detection models. The idea of augmented data representation may inspire further study on object detection and other applications.",0
"Deep learning models rely heavily on abundant and diverse datasets to achieve excellent performance. However, obtaining and labeling datasets for specific scenes or applications can be challenging. Object detection models face the problem of domain shift, making it difficult for detectors trained on one domain to perform well on another. To address this issue, image-to-image translation techniques have been used to generate fake data for specific scenes. Generative Adversarial Networks (GANs) enable unsupervised image-to-image translation in both directions. In this study, we propose concatenating the original 3-channel images and their corresponding GAN-generated fake images to form 6-channel representations of the dataset. By doing so, we hope to address the domain shift problem and improve the success of available detection models. This approach to augmented data representation may inspire further study on object detection and other applications.",1
"Object detection plays an important role in self-driving cars for security development. However, mobile systems on self-driving cars with limited computation resources lead to difficulties for object detection. To facilitate this, we propose a compiler-aware neural pruning search framework to achieve high-speed inference on autonomous vehicles for 2D and 3D object detection. The framework automatically searches the pruning scheme and rate for each layer to find a best-suited pruning for optimizing detection accuracy and speed performance under compiler optimization. Our experiments demonstrate that for the first time, the proposed method achieves (close-to) real-time, 55ms and 99ms inference times for YOLOv4 based 2D object detection and PointPillars based 3D detection, respectively, on an off-the-shelf mobile phone with minor (or no) accuracy loss.",0
"The importance of object detection in self-driving cars for safety development cannot be overstated. However, limited computation resources on mobile systems present challenges for object detection. To address this issue, we propose a compiler-aware neural pruning search framework that can achieve high-speed inference for 2D and 3D object detection on autonomous vehicles. The framework automatically searches for the best-suited pruning scheme and rate for each layer, optimizing both detection accuracy and speed performance under compiler optimization. Our experiments demonstrate that our method achieves (close-to) real-time inference times of 55ms and 99ms for YOLOv4 based 2D object detection and PointPillars based 3D detection, respectively, on an off-the-shelf mobile phone with minimal accuracy loss.",1
"Rotated object detection is a challenging issue of computer vision field. Loss of spatial information and confusion of parametric order have been the bottleneck for rotated detection accuracy. In this paper, we propose an orientation-sensitive keypoint based rotated detector OSKDet. We adopt a set of keypoints to characterize the target and predict the keypoint heatmap on ROI to form a rotated target. By proposing the orientation-sensitive heatmap, OSKDet could learn the shape and direction of rotated target implicitly and has stronger modeling capabilities for target representation, which improves the localization accuracy and acquires high quality detection results. To extract highly effective features at border areas, we design a rotation-aware deformable convolution module. Furthermore, we explore a new keypoint reorder algorithm and feature fusion module based on the angle distribution to eliminate the confusion of keypoint order. Experimental results on several public benchmarks show the state-of-the-art performance of OSKDet. Specifically, we achieve an AP of 77.81% on DOTA, 89.91% on HRSC2016, and 97.18% on UCAS-AOD, respectively.",0
"The detection of rotated objects poses a difficult challenge in the field of computer vision. The accuracy of rotated detection has been hindered by the loss of spatial information and confusion of parametric order. This paper introduces a new orientation-sensitive keypoint based rotated detector called OSKDet. OSKDet characterizes the target using a set of keypoints and predicts the keypoint heatmap on ROI to form a rotated target. The orientation-sensitive heatmap enables OSKDet to learn the shape and direction of rotated targets implicitly, resulting in stronger modeling capabilities for target representation and improved localization accuracy. A rotation-aware deformable convolution module is designed to extract highly effective features at border areas. Additionally, a new keypoint reorder algorithm and feature fusion module based on the angle distribution are explored to eliminate confusion of keypoint order. The state-of-the-art performance of OSKDet is demonstrated through experimental results on several public benchmarks, achieving an AP of 77.81% on DOTA, 89.91% on HRSC2016, and 97.18% on UCAS-AOD, respectively.",1
"When producing a model to object detection in a specific context, the first obstacle is to have a dataset labeling the desired classes. In RoboCup, some leagues already have more than one dataset to train and evaluate a model. However, in the Small Size League (SSL), there is not such dataset available yet. This paper presents an open-source dataset to be used as a benchmark for real-time object detection in SSL. This work also presented a pipeline to train, deploy, and evaluate Convolutional Neural Networks (CNNs) models in a low-power embedded system. This pipeline was used to evaluate the proposed dataset with state-of-art optimized models. In this dataset, the MobileNet SSD v1 achieves 44.88% AP (68.81% AP50) at 94 Frames Per Second (FPS) while running on an SSL robot.",0
"The primary challenge in developing a model for object detection in a specific context is obtaining a dataset that accurately labels the desired classes. Some RoboCup leagues have multiple datasets available for training and evaluating models, but the Small Size League (SSL) currently lacks such a resource. To address this, a new open-source dataset has been created to serve as a benchmark for real-time object detection in SSL. Additionally, a pipeline has been developed for training, deploying, and evaluating Convolutional Neural Networks (CNNs) models on a low-power embedded system. This pipeline was utilized to assess the proposed dataset using optimized state-of-the-art models. The MobileNet SSD v1 model achieved 44.88% AP (68.81% AP50) with 94 Frames Per Second (FPS) when tested on an SSL robot.",1
"Remote examination and job interviews have gained popularity and become indispensable because of both pandemics and the advantage of remote working circumstances. Most companies and academic institutions utilize these systems for their recruitment processes and also for online exams. However, one of the critical problems of the remote examination systems is conducting the exams in a reliable environment. In this work, we present a cheating analysis pipeline for online interviews and exams. The system only requires a video of the candidate, which is recorded during the exam. Then cheating detection pipeline is employed to detect another person, electronic device usage, and candidate absence status. The pipeline consists of face detection, face recognition, object detection, and face tracking algorithms. To evaluate the performance of the pipeline we collected a private video dataset. The video dataset includes both cheating activities and clean videos. Ultimately, our pipeline presents an efficient and fast guideline to detect and analyze cheating activities in an online interview and exam video.",0
"Due to the pandemic and the benefits of remote work, remote examinations and job interviews have become increasingly popular and necessary. Many companies and academic institutions are now using these systems for recruiting and online exams. However, one of the major challenges of remote examination systems is ensuring a reliable testing environment. To address this issue, we have developed a cheating analysis pipeline for online interviews and exams. This system only requires a video of the candidate taken during the exam, which is then analyzed using face detection, face recognition, object detection, and face tracking algorithms to detect cheating, electronic device usage, and candidate absence. To evaluate the pipeline's performance, we created a private video dataset that includes both clean and cheating videos. Our pipeline provides an efficient and quick way to detect and analyze cheating activities in online interviews and exam videos.",1
"Deep learning has proven to be a highly effective problem-solving tool for object detection and image segmentation across various domains such as healthcare and autonomous driving. At the heart of this performance lies neural architecture design which relies heavily on domain knowledge and prior experience on the researchers' behalf. More recently, this process of finding the most optimal architectures, given an initial search space of possible operations, was automated by Neural Architecture Search (NAS). In this paper, we evaluate the robustness of one such algorithm known as Efficient NAS (ENAS) against data agnostic poisoning attacks on the original search space with carefully designed ineffective operations. By evaluating algorithm performance on the CIFAR-10 dataset, we empirically demonstrate how our novel search space poisoning (SSP) approach and multiple-instance poisoning attacks exploit design flaws in the ENAS controller to result in inflated prediction error rates for child networks. Our results provide insights into the challenges to surmount in using NAS for more adversarially robust architecture search.",0
"In various fields such as healthcare and autonomous driving, deep learning has been successful in object detection and image segmentation. The effectiveness of this approach is largely due to the neural architecture design, which heavily relies on researchers' domain knowledge and experience. Recently, Neural Architecture Search (NAS) has automated the process of finding optimal architectures from a set of possible operations. In this study, we evaluate the performance of Efficient NAS (ENAS) against data agnostic poisoning attacks on the original search space. Our novel search space poisoning (SSP) approach and multiple-instance poisoning attacks exploit design flaws in the ENAS controller, resulting in higher prediction error rates for child networks. Our experiments on the CIFAR-10 dataset highlight the challenges of using NAS for more adversarially robust architecture search.",1
"In this study, we propose a novel approach to predict the distances of the detected objects in an observed scene. The proposed approach modifies the recently proposed Convolutional Support Estimator Networks (CSENs). CSENs are designed to compute a direct mapping for the Support Estimation (SE) task in a representation-based classification problem. We further propose and demonstrate that representation-based methods (sparse or collaborative representation) can be used in well-designed regression problems. To the best of our knowledge, this is the first representation-based method proposed for performing a regression task by utilizing the modified CSENs; and hence, we name this novel approach as Representation-based Regression (RbR). The initial version of CSENs has a proxy mapping stage (i.e., a coarse estimation for the support set) that is required for the input. In this study, we improve the CSEN model by proposing Compressive Learning CSEN (CL-CSEN) that has the ability to jointly optimize the so-called proxy mapping stage along with convolutional layers. The experimental evaluations using the KITTI 3D Object Detection distance estimation dataset show that the proposed method can achieve a significantly improved distance estimation performance over all competing methods. Finally, the software implementations of the methods are publicly shared at https://github.com/meteahishali/CSENDistance.",0
"The study introduces a new method for predicting the distances of detected objects in a scene. The approach modifies Convolutional Support Estimator Networks (CSENs), which were originally designed for Support Estimation (SE) in a representation-based classification problem. The study proposes that representation-based methods, such as sparse or collaborative representation, can be used in well-designed regression problems. This is the first representation-based method proposed for performing a regression task using modified CSENs, named Representation-based Regression (RbR). The study improves the CSEN model by proposing Compressive Learning CSEN (CL-CSEN), which optimizes the proxy mapping stage and convolutional layers jointly. Evaluations on the KITTI 3D Object Detection distance estimation dataset show that the proposed method significantly improves distance estimation performance compared to competing methods. The software implementations of the methods are publicly available at https://github.com/meteahishali/CSENDistance.",1
"There are many real-life use cases such as barcode scanning or billboard reading where people need to detect objects and read the object contents. Commonly existing methods are first trying to localize object regions, then determine layout and lastly classify content units. However, for simple fixed structured objects like license plates, this approach becomes overkill and lengthy to run. This work aims to solve this detect-and-read problem in a lightweight way by integrating multi-digit recognition into a one-stage object detection model. Our unified method not only eliminates the duplication in feature extraction (one for localizing, one again for classifying) but also provides useful contextual information around object regions for classification. Additionally, our choice of backbones and modifications in architecture, loss function, data augmentation and training make the method robust, efficient and speedy. Secondly, we made a public benchmark dataset of diverse real-life 1D barcodes for a reliable evaluation, which we collected, annotated and checked carefully. Eventually, experimental results prove the method's efficiency on the barcode problem by outperforming industrial tools in both detecting and decoding rates with a real-time fps at a VGA-similar resolution. It also did a great job expectedly on the license-plate recognition task (on the AOLP dataset) by outperforming the current state-of-the-art method significantly in terms of recognition rate and inference time.",0
"In real-life scenarios such as barcode scanning or billboard reading, detecting objects and reading their contents is crucial. Traditional methods involve localizing object regions, determining layout, and classifying content units. However, this process is cumbersome and excessive for simple, structured objects like license plates. This study aims to solve the detect-and-read problem efficiently by integrating multi-digit recognition into a one-stage object detection model. By doing so, the method eliminates feature extraction duplication and provides useful contextual information for classification. The approach's robustness, efficiency, and speed are achieved through modifications in architecture, loss function, data augmentation, and training, and a public benchmark dataset of diverse real-life 1D barcodes for reliable evaluation. Experimental results show that the method outperforms industrial tools in both detecting and decoding rates, with a real-time fps at a VGA-similar resolution on the barcode problem. Additionally, it significantly outperforms the current state-of-the-art method in terms of recognition rate and inference time on the AOLP dataset for license-plate recognition.",1
"In this paper, we present a real-time 3D detection approach considering time-spatial feature map aggregation from different time steps of deep neural model inference (named feature map flow, FMF). Proposed approach improves the quality of 3D detection center-based baseline and provides real-time performance on the nuScenes and Waymo benchmark. Code is available at https://github.com/YoushaaMurhij/FMFNet",0
"The focus of this paper is a real-time 3D detection method that utilizes feature map flow (FMF), which aggregates time-spatial features from various time steps in deep neural model inference. By utilizing FMF, our approach enhances the accuracy of 3D detection center-based baseline and achieves real-time performance on the nuScenes and Waymo benchmark. Interested parties can access the code on https://github.com/YoushaaMurhij/FMFNet.",1
"Automotive traffic scenes are complex due to the variety of possible scenarios, objects, and weather conditions that need to be handled. In contrast to more constrained environments, such as automated underground trains, automotive perception systems cannot be tailored to a narrow field of specific tasks but must handle an ever-changing environment with unforeseen events. As currently no single sensor is able to reliably perceive all relevant activity in the surroundings, sensor data fusion is applied to perceive as much information as possible. Data fusion of different sensors and sensor modalities on a low abstraction level enables the compensation of sensor weaknesses and misdetections among the sensors before the information-rich sensor data are compressed and thereby information is lost after a sensor-individual object detection. This paper develops a low-level sensor fusion network for 3D object detection, which fuses lidar, camera, and radar data. The fusion network is trained and evaluated on the nuScenes data set. On the test set, fusion of radar data increases the resulting AP (Average Precision) detection score by about 5.1% in comparison to the baseline lidar network. The radar sensor fusion proves especially beneficial in inclement conditions such as rain and night scenes. Fusing additional camera data contributes positively only in conjunction with the radar fusion, which shows that interdependencies of the sensors are important for the detection result. Additionally, the paper proposes a novel loss to handle the discontinuity of a simple yaw representation for object detection. Our updated loss increases the detection and orientation estimation performance for all sensor input configurations. The code for this research has been made available on GitHub.",0
"The complexity of automotive traffic scenes arises from the variety of scenarios, objects, and weather conditions that must be handled. Unlike more restricted environments, e.g., automated underground trains, automotive perception systems must deal with an ever-changing environment with unforeseeable events, making it impossible to personalize them for specific tasks. As no single sensor can perceive all relevant activity reliably, sensor data fusion is used to gather as much information as possible. By fusing different sensors and sensor modalities at a low abstraction level, sensor weaknesses and misdetections are compensated for before the information-rich sensor data are compressed, thus preventing information loss. This study develops a 3D object detection low-level sensor fusion network that integrates lidar, camera, and radar data. The network is trained and assessed on the nuScenes dataset. On the test set, radar data fusion improves the resulting AP (Average Precision) detection score by around 5.1% compared to the baseline lidar network, particularly in inclement conditions such as rain and night scenes. Fusing more camera data contributes positively only when combined with radar fusion, highlighting the importance of sensor interdependencies for detection outcomes. Additionally, the study proposes a new loss to handle the discontinuity of a simple yaw representation for object detection, enhancing detection and orientation estimation performance for all sensor input configurations. The code for this research is available on GitHub.",1
"The transformer networks are particularly good at modeling long-range dependencies within a long sequence. In this paper, we conduct research on applying the transformer networks for salient object detection (SOD). We adopt the dense transformer backbone for fully supervised RGB image based SOD, RGB-D image pair based SOD, and weakly supervised SOD within a unified framework based on the observation that the transformer backbone can provide accurate structure modeling, which makes it powerful in learning from weak labels with less structure information. Further, we find that the vision transformer architectures do not offer direct spatial supervision, instead encoding position as a feature. Therefore, we investigate the contributions of two strategies to provide stronger spatial supervision through the transformer layers within our unified framework, namely deep supervision and difficulty-aware learning. We find that deep supervision can get gradients back into the higher level features, thus leads to uniform activation within the same semantic object. Difficulty-aware learning on the other hand is capable of identifying the hard pixels for effective hard negative mining. We also visualize features of conventional backbone and transformer backbone before and after fine-tuning them for SOD, and find that transformer backbone encodes more accurate object structure information and more distinct semantic information within the lower and higher level features respectively. We also apply our model to camouflaged object detection (COD) and achieve similar observations as the above three SOD tasks. Extensive experimental results on various SOD and COD tasks illustrate that transformer networks can transform SOD and COD, leading to new benchmarks for each related task. The source code and experimental results are available via our project page: https://github.com/fupiao1998/TrasformerSOD.",0
"The transformer networks excel in modeling long sequences by capturing long-range dependencies. This study explores the application of transformer networks in salient object detection (SOD). The dense transformer backbone is utilized for fully supervised RGB image-based SOD, RGB-D image pair-based SOD, and weakly supervised SOD. The transformer backbone is powerful in learning from weak labels with less structure information due to its accurate structure modeling. However, the vision transformer architectures do not provide direct spatial supervision. To address this, two strategies are employed: deep supervision and difficulty-aware learning, to offer stronger spatial supervision through transformer layers. Deep supervision leads to uniform activation within the same semantic object, while difficulty-aware learning is effective in identifying hard pixels for hard negative mining. Compared to conventional backbone, the transformer backbone encodes more accurate object structure information and more distinct semantic information within lower and higher level features respectively. The model is also applied to camouflaged object detection (COD) with similar observations as in SOD. The experimental results on various SOD and COD tasks showcase the potential of transformer networks in transforming these tasks, leading to new benchmarks. The source code and experimental results are available on the project page: https://github.com/fupiao1998/TrasformerSOD.",1
"Camouflaged object detection (COD) aims to segment camouflaged objects hiding in the environment, which is challenging due to the similar appearance of camouflaged objects and their surroundings. Research in biology suggests that depth can provide useful object localization cues for camouflaged object discovery, as all the animals have 3D perception ability. However, the depth information has not been exploited for camouflaged object detection. To explore the contribution of depth for camouflage detection, we present a depth-guided camouflaged object detection network with pre-computed depth maps from existing monocular depth estimation methods. Due to the domain gap between the depth estimation dataset and our camouflaged object detection dataset, the generated depth may not be accurate enough to be directly used in our framework. We then introduce a depth quality assessment module to evaluate the quality of depth based on the model prediction from both RGB COD branch and RGB-D COD branch. During training, only high-quality depth is used to update the modal interaction module for multi-modal learning. During testing, our depth quality assessment module can effectively determine the contribution of depth and select the RGB branch or RGB-D branch for camouflage prediction. Extensive experiments on various camouflaged object detection datasets prove the effectiveness of our solution in exploring the depth information for camouflaged object detection. Our code and data is publicly available at: \url{https://github.com/JingZhang617/RGBD-COD}.",0
"The goal of Camouflaged Object Detection (COD) is to detect hidden objects in the environment that are camouflaged, which can be difficult due to their resemblance to their surroundings. Research in biology has shown that depth perception can be a useful cue for detecting camouflaged objects, as animals possess this ability. However, depth information has not yet been utilized for camouflaged object detection. In order to investigate the potential of depth for detecting camouflage, we have developed a depth-guided network for camouflaged object detection. We use pre-existing monocular depth estimation methods to generate depth maps, but due to differences between the depth estimation and camouflaged object detection datasets, the depth may not be completely accurate. To address this, we have included a depth quality assessment module that evaluates the accuracy of the depth based on predictions from both the RGB COD branch and the RGB-D COD branch. During training, only high-quality depth is used to update the modal interaction module for multi-modal learning, while during testing, the depth quality assessment module selects either the RGB branch or the RGB-D branch for camouflage prediction. Our solution has been extensively tested on various camouflaged object detection datasets, and the results demonstrate the effectiveness of using depth information for camouflaged object detection. The code and data used in this study are available at: \url{https://github.com/JingZhang617/RGBD-COD}.",1
"As a core problem in computer vision, the performance of object detection has improved drastically in the past few years. Despite their impressive performance, object detectors suffer from a lack of interpretability. Visualization techniques have been developed and widely applied to introspect the decisions made by other kinds of deep learning models; however, visualizing object detectors has been underexplored. In this paper, we propose using inversion as a primary tool to understand modern object detectors and develop an optimization-based approach to layout inversion, allowing us to generate synthetic images recognized by trained detectors as containing a desired configuration of objects. We reveal intriguing properties of detectors by applying our layout inversion technique to a variety of modern object detectors, and further investigate them via validation experiments: they rely on qualitatively different features for classification and regression; they learn canonical motifs of commonly co-occurring objects; they use diff erent visual cues to recognize objects of varying sizes. We hope our insights can help practitioners improve object detectors.",0
"Over the past few years, the performance of object detection in computer vision has significantly enhanced. Despite this progress, there is a lack of interpretability in object detectors. While visualization techniques have been developed for other deep learning models, they have not been extensively used for object detectors. This paper proposes an optimization-based approach to layout inversion as a means of understanding modern object detectors. By generating synthetic images recognized by trained detectors as containing a desired object configuration, we uncover the detectors' intriguing properties. Our investigation reveals that they rely on different features for classification and regression, learn common object motifs, and use distinct visual cues to recognize varying object sizes. These insights can help practitioners enhance object detectors.",1
"Ultrasound scanning is essential in several medical diagnostic and therapeutic applications. It is used to visualize and analyze anatomical features and structures that influence treatment plans. However, it is both labor intensive, and its effectiveness is operator dependent. Real-time accurate and robust automatic detection and tracking of anatomical structures while scanning would significantly impact diagnostic and therapeutic procedures to be consistent and efficient. In this paper, we propose a deep learning framework to automatically detect and track a specific anatomical target structure in ultrasound scans. Our framework is designed to be accurate and robust across subjects and imaging devices, to operate in real-time, and to not require a large training set. It maintains a localization precision and recall higher than 90% when trained on training sets that are as small as 20% in size of the original training set. The framework backbone is a weakly trained segmentation neural network based on U-Net. We tested the framework on two different ultrasound datasets with the aim to detect and track the Vagus nerve, where it outperformed current state-of-the-art real-time object detection networks.",0
"Several medical diagnostic and therapeutic applications rely on ultrasound scanning to visualize and analyze anatomical features and structures that affect treatment plans. However, this method is labor-intensive and operator-dependent. The ability to automatically detect and track anatomical structures while scanning would greatly improve the efficiency and consistency of diagnostic and therapeutic procedures. To address this need, we propose a deep learning framework that can accurately and robustly detect and track a specific anatomical structure in ultrasound scans. Our framework is designed to operate in real-time, work across subjects and imaging devices, and not require a large training set. It achieves a localization precision and recall higher than 90% with training sets as small as 20% of the original size. The framework's backbone is a weakly trained segmentation neural network based on U-Net. We tested the framework on two different ultrasound datasets to detect and track the Vagus nerve and found that it outperformed current state-of-the-art real-time object detection networks.",1
"In the past few years, we have witnessed rapid development of autonomous driving. However, achieving full autonomy remains a daunting task due to the complex and dynamic driving environment. As a result, self-driving cars are equipped with a suite of sensors to conduct robust and accurate environment perception. As the number and type of sensors keep increasing, combining them for better perception is becoming a natural trend. So far, there has been no indepth review that focuses on multi-sensor fusion based perception. To bridge this gap and motivate future research, this survey devotes to review recent fusion-based 3D detection deep learning models that leverage multiple sensor data sources, especially cameras and LiDARs. In this survey, we first introduce the background of popular sensors for autonomous cars, including their common data representations as well as object detection networks developed for each type of sensor data. Next, we discuss some popular datasets for multi-modal 3D object detection, with a special focus on the sensor data included in each dataset. Then we present in-depth reviews of recent multi-modal 3D detection networks by considering the following three aspects of the fusion: fusion location, fusion data representation, and fusion granularity. After a detailed review, we discuss open challenges and point out possible solutions. We hope that our detailed review can help researchers to embark investigations in the area of multi-modal 3D object detection.",0
"Autonomous driving has seen significant progress in recent years, yet achieving complete autonomy remains a challenging feat due to the highly complex and dynamic nature of the driving environment. To address this challenge, self-driving cars are equipped with an array of sensors to enable robust and accurate environment perception. With an increasing number and variety of sensors, combining them for better perception is a natural trend. However, there is currently a lack of comprehensive review that focuses on multi-sensor fusion based perception. To address this gap and encourage further research, this survey provides an overview of recent fusion-based 3D detection deep learning models that utilize multiple sensor data sources, particularly cameras and LiDARs. The survey includes an introduction to popular sensors and their data representations, as well as object detection networks developed for each sensor type. Additionally, popular datasets for multi-modal 3D object detection are discussed with emphasis on the sensor data involved. The survey then delves into in-depth reviews of recent multi-modal 3D detection networks, examining the fusion location, data representation, and granularity. Finally, open challenges are discussed, along with potential solutions. The hope is that this detailed review will assist researchers in exploring the area of multi-modal 3D object detection.",1
"In this paper we propose a novel data augmentation approach for visual content domains that have scarce training datasets, compositing synthetic 3D objects within real scenes. We show the performance of the proposed system in the context of object detection in thermal videos, a domain where 1) training datasets are very limited compared to visible spectrum datasets and 2) creating full realistic synthetic scenes is extremely cumbersome and expensive due to the difficulty in modeling the thermal properties of the materials of the scene. We compare different augmentation strategies, including state of the art approaches obtained through RL techniques, the injection of simulated data and the employment of a generative model, and study how to best combine our proposed augmentation with these other techniques.Experimental results demonstrate the effectiveness of our approach, and our single-modality detector achieves state-of-the-art results on the FLIR ADAS dataset.",0
"This paper introduces a new method of data augmentation for visual content domains with limited training datasets. Our approach involves integrating synthetic 3D objects into real scenes to enhance performance in object detection within thermal videos. This domain poses challenges due to the scarcity of training datasets and the difficulty of creating realistic synthetic scenes with accurate thermal properties. We evaluate and compare different augmentation strategies, including state of the art RL techniques, simulated data injection, and generative models. Our experimental results show that our proposed approach is highly effective and outperforms existing methods. Our single-modality detector achieves state-of-the-art results on the FLIR ADAS dataset.",1
"The detection of nuclei and cells in histology images is of great value in both clinical practice and pathological studies. However, multiple reasons such as morphological variations of nuclei or cells make it a challenging task where conventional object detection methods cannot obtain satisfactory performance in many cases. A detection task consists of two sub-tasks, classification and localization. Under the condition of dense object detection, classification is a key to boost the detection performance. Considering this, we propose similarity based region proposal networks (SRPN) for nuclei and cells detection in histology images. In particular, a customized convolution layer termed as embedding layer is designed for network building. The embedding layer is added into the region proposal networks, enabling the networks to learn discriminative features based on similarity learning. Features obtained by similarity learning can significantly boost the classification performance compared to conventional methods. SRPN can be easily integrated into standard convolutional neural networks architectures such as the Faster R-CNN and RetinaNet. We test the proposed approach on tasks of multi-organ nuclei detection and signet ring cells detection in histological images. Experimental results show that networks applying similarity learning achieved superior performance on both tasks when compared to their counterparts. In particular, the proposed SRPN achieve state-of-the-art performance on the MoNuSeg benchmark for nuclei segmentation and detection while compared to previous methods, and on the signet ring cell detection benchmark when compared with baselines. The sourcecode is publicly available at: https://github.com/sigma10010/nuclei_cells_det.",0
"Histology images can benefit from the detection of nuclei and cells for clinical practice and pathological studies. However, the task is challenging due to various factors such as morphological diversity, which conventional object detection methods struggle to handle effectively. A detection task comprises classification and localization, with classification being crucial for dense object detection. To address this, we propose similarity-based region proposal networks (SRPN) for detecting nuclei and cells in histology images. Our approach includes an embedding layer for network building, which facilitates similarity learning and discriminative feature extraction. We integrate SRPN into standard convolutional neural networks like Faster R-CNN and RetinaNet. Our experimental results show that SRPN outperforms previous methods significantly on multi-organ nuclei detection and signet ring cell detection. The proposed SRPN achieves state-of-the-art performance on the MoNuSeg benchmark for nuclei segmentation and detection and the signet ring cell detection benchmark. The source code for SRPN is available publicly on GitHub at https://github.com/sigma10010/nuclei_cells_det.",1
"3D object detection is vital for many robotics applications. For tasks where a 2D perspective range image exists, we propose to learn a 3D representation directly from this range image view. To this end, we designed a 2D convolutional network architecture that carries the 3D spherical coordinates of each pixel throughout the network. Its layers can consume any arbitrary convolution kernel in place of the default inner product kernel and exploit the underlying local geometry around each pixel. We outline four such kernels: a dense kernel according to the bag-of-words paradigm, and three graph kernels inspired by recent graph neural network advances: the Transformer, the PointNet, and the Edge Convolution. We also explore cross-modality fusion with the camera image, facilitated by operating in the perspective range image view. Our method performs competitively on the Waymo Open Dataset and improves the state-of-the-art AP for pedestrian detection from 69.7% to 75.5%. It is also efficient in that our smallest model, which still outperforms the popular PointPillars in quality, requires 180 times fewer FLOPS and model parameters",0
"The detection of 3D objects is crucial in various robotics applications. Our proposed approach involves learning a 3D representation directly from the existing 2D perspective range image. We accomplished this by creating a 2D convolutional network architecture that maintains the 3D spherical coordinates of each pixel throughout the network. This architecture allows for the use of any convolution kernel, and it can take advantage of the local geometry surrounding each pixel. We present four kernel options, including a dense kernel based on the bag-of-words paradigm, as well as three graph kernels inspired by recent advances in graph neural networks: the Transformer, PointNet, and Edge Convolution. We also explore the fusion of cross-modality with the camera image by operating in the perspective range image view. Our approach performs competitively on the Waymo Open Dataset, improving the AP for pedestrian detection from 69.7% to 75.5%. Our method is efficient, as our smallest model, which outperforms the popular PointPillars in quality, requires 180 times fewer FLOPS and model parameters.",1
"The detection of 3D objects from LiDAR data is a critical component in most autonomous driving systems. Safe, high speed driving needs larger detection ranges, which are enabled by new LiDARs. These larger detection ranges require more efficient and accurate detection models. Towards this goal, we propose Range Sparse Net (RSN), a simple, efficient, and accurate 3D object detector in order to tackle real time 3D object detection in this extended detection regime. RSN predicts foreground points from range images and applies sparse convolutions on the selected foreground points to detect objects. The lightweight 2D convolutions on dense range images results in significantly fewer selected foreground points, thus enabling the later sparse convolutions in RSN to efficiently operate. Combining features from the range image further enhance detection accuracy. RSN runs at more than 60 frames per second on a 150m x 150m detection region on Waymo Open Dataset (WOD) while being more accurate than previously published detectors. As of 11/2020, RSN is ranked first in the WOD leaderboard based on the APH/LEVEL 1 metrics for LiDAR-based pedestrian and vehicle detection, while being several times faster than alternatives.",0
"The identification of 3D objects using LiDAR data is a crucial element in many self-driving systems, as safe driving at high speeds necessitates wider detection ranges, which can be achieved through the use of new LiDARs. However, this also necessitates more efficient and precise detection models. To address this, we present Range Sparse Net (RSN), a straightforward, effective, and accurate 3D object detector that is capable of real-time detection in this extended detection range. RSN identifies foreground points from range images and applies sparse convolutions to these selected foreground points in order to detect objects. The use of lightweight 2D convolutions on dense range images results in significantly fewer foreground points being selected, allowing RSN to operate efficiently. Combining features from the range image further enhances detection accuracy. RSN is capable of running at over 60 frames per second on a 150m x 150m detection area on the Waymo Open Dataset (WOD), while also being more accurate than previously published detectors. As of November 2020, RSN is ranked first on the WOD leaderboard based on the APH/LEVEL 1 metrics for LiDAR-based pedestrian and vehicle detection, while also being several times faster than alternative models.",1
"Recently, query based deep networks catch lots of attention owing to their end-to-end pipeline and competitive results on several fundamental computer vision tasks, such as object detection, semantic segmentation, and instance segmentation. However, how to establish a query based video instance segmentation (VIS) framework with elegant architecture and strong performance remains to be settled. In this paper, we present \textbf{QueryTrack} (i.e., tracking instances as queries), a unified query based VIS framework fully leveraging the intrinsic one-to-one correspondence between instances and queries in QueryInst. The proposed method obtains 52.7 / 52.3 AP on YouTube-VIS-2019 / 2021 datasets, which wins the 2-nd place in the YouTube-VIS Challenge at CVPR 2021 \textbf{with a single online end-to-end model, single scale testing \& modest amount of training data}. We also provide QueryTrack-ResNet-50 baseline results on YouTube-VIS-2021 val set as references for the VIS community.",0
"In recent times, query based deep networks have received significant attention due to their end-to-end pipeline and impressive outcomes on various significant computer vision tasks including semantic segmentation, object detection, and instance segmentation. However, there is still a need to resolve the issue of creating an elegant and high-performing query based video instance segmentation (VIS) framework. In this study, we introduce QueryTrack, a comprehensive query based VIS framework called tracking instances as queries that fully capitalizes on the inherent one-to-one relationship between queries and instances in QueryInst. Using a single online end-to-end model, single scale testing, and a modest amount of training data, the proposed approach achieves remarkable results of 52.7 / 52.3 AP on YouTube-VIS-2019 / 2021 datasets, securing the second position in the YouTube-VIS Challenge at CVPR 2021. Additionally, we offer QueryTrack-ResNet-50 baseline outcomes on YouTube-VIS-2021 val set as a reference for the VIS community.",1
"While common image object detection tasks focus on bounding boxes or segmentation masks as object representations, we consider the problem of finding objects based on four arbitrary vertices. We propose a novel model, named TetraPackNet, to tackle this problem. TetraPackNet is based on CornerNet and uses similar algorithms and ideas. It is designated for applications requiring high-accuracy detection of regularly shaped objects, which is the case in the logistics use-case of packaging structure recognition. We evaluate our model on our specific real-world dataset for this use-case. Baselined against a previous solution, consisting of a Mask R-CNN model and suitable post-processing steps, TetraPackNet achieves superior results (9% higher in accuracy) in the sub-task of four-corner based transport unit side detection.",0
"While most image object detection tasks rely on bounding boxes or segmentation masks to represent objects, our focus is on finding objects using four arbitrary vertices. To address this issue, our proposed model, TetraPackNet, draws inspiration from CornerNet and leverages similar algorithms and ideas. TetraPackNet is designed to provide high-accuracy detection of regularly shaped objects, particularly in logistics scenarios where packaging structure recognition is crucial. Our evaluation is based on a real-world dataset specific to this use-case, and we compare TetraPackNet to a previous solution that involved a Mask R-CNN model and appropriate post-processing steps. Our results demonstrate that TetraPackNet outperforms the baseline solution by achieving a 9% higher accuracy in the sub-task of detecting transport unit sides based on four corners.",1
"Cross-domain object detection is challenging, because object detection model is often vulnerable to data variance, especially to the considerable domain shift between two distinctive domains. In this paper, we propose a new Unbiased Mean Teacher (UMT) model for cross-domain object detection. We reveal that there often exists a considerable model bias for the simple mean teacher (MT) model in cross-domain scenarios, and eliminate the model bias with several simple yet highly effective strategies. In particular, for the teacher model, we propose a cross-domain distillation method for MT to maximally exploit the expertise of the teacher model. Moreover, for the student model, we alleviate its bias by augmenting training samples with pixel-level adaptation. Finally, for the teaching process, we employ an out-of-distribution estimation strategy to select samples that most fit the current model to further enhance the cross-domain distillation process. By tackling the model bias issue with these strategies, our UMT model achieves mAPs of 44.1%, 58.1%, 41.7%, and 43.1% on benchmark datasets Clipart1k, Watercolor2k, Foggy Cityscapes, and Cityscapes, respectively, which outperforms the existing state-of-the-art results in notable margins. Our implementation is available at https://github.com/kinredon/umt.",0
"Detecting objects across different domains is a difficult task, as the variance in data can make object detection models vulnerable, especially when shifting between two distinct domains. In this paper, we introduce a new model called the Unbiased Mean Teacher (UMT) for cross-domain object detection. We found that the simple mean teacher (MT) model often has a significant bias in cross-domain scenarios, but we eliminate this bias by implementing several straightforward yet highly effective strategies. We propose a cross-domain distillation method for the teacher model, which maximizes the teacher model's expertise, and we alleviate the student model's bias by augmenting training samples with pixel-level adaptation. To further enhance the cross-domain distillation process, we employ an out-of-distribution estimation strategy to select samples that fit the current model best. By implementing these strategies to tackle the model bias issue, our UMT model achieves mAPs of 44.1%, 58.1%, 41.7%, and 43.1% on benchmark datasets Clipart1k, Watercolor2k, Foggy Cityscapes, and Cityscapes, respectively, outperforming the existing state-of-the-art results by notable margins. Our UMT model's implementation is available at https://github.com/kinredon/umt.",1
"Deep neural networks have reached very high accuracy on object detection but their success hinges on large amounts of labeled data. To reduce the dependency on labels, various active-learning strategies have been proposed, typically based on the confidence of the detector. However, these methods are biased towards best-performing classes and can lead to acquired datasets that are not good representatives of the data in the testing set. In this work, we propose a unified framework for active learning, that considers both the uncertainty and the robustness of the detector, ensuring that the network performs accurately in all classes. Furthermore, our method is able to pseudo-label the very confident predictions, suppressing a potential distribution drift while further boosting the performance of the model. Experiments show that our method comprehensively outperforms a wide range of active-learning methods on PASCAL VOC07+12 and MS-COCO, having up to a 7.7% relative improvement, or up to 82% reduction in labeling cost.",0
"Although deep neural networks have achieved high precision in object detection, their reliance on large amounts of labeled data is a major drawback. To overcome this issue, various active-learning methods have been proposed to reduce the need for labels, but they are often biased towards best-performing classes and may result in datasets that do not accurately represent the testing set. In this study, we introduce a unified framework for active learning that accounts for both uncertainty and robustness of the detector, ensuring accurate performance in all classes. Additionally, our approach can pseudo-label confident predictions, preventing distribution drift and enhancing model performance. Experiments on PASCAL VOC07+12 and MS-COCO demonstrate that our method outperforms a variety of active-learning methods, yielding up to a 7.7% relative improvement or an 82% reduction in labeling costs.",1
"With the introduction of new regulations in the European Union, the future of Beyond Visual Line Of Sight (BVLOS) drones is set to bloom. This led to the creation of the theBEAST project, which aims to create an autonomous security drone, with focus on those regulations and on safety. This technical paper describes the first steps of a module within this project, which revolves around detecting obstacles so they can be avoided in a fail-safe landing. A deep learning powered object detection method is the subject of our research, and various experiments are held to maximize its performance, such as comparing various data augmentation techniques or YOLOv3 and YOLOv5. According to the results of the experiments, we conclude that although object detection is a promising approach to resolve this problem, more volume of data is required for potential usage in a real-life application.",0
"The future of BVLOS drones is looking bright due to new regulations in the European Union. As a result, the theBEAST project was launched, with the goal of developing an autonomous security drone that adheres to these regulations and prioritizes safety. This technical paper focuses on the first steps of a module within the project, which involves obstacle detection for a fail-safe landing. Our research centers on a deep learning-based object detection method, and we conducted several experiments to enhance its performance, including comparing different data augmentation techniques and YOLOv3 and YOLOv5. Although object detection shows promise in solving this issue, our experiments reveal that more data is needed for potential usage in real-life applications.",1
"Deep neural networks designed for vision tasks are often prone to failure when they encounter environmental conditions not covered by the training data. Single-modal strategies are insufficient when the sensor fails to acquire information due to malfunction or its design limitations. Multi-sensor configurations are known to provide redundancy, increase reliability, and are crucial in achieving robustness against asymmetric sensor failures. To address the issue of changing lighting conditions and asymmetric sensor degradation in object detection, we develop a multi-modal 2D object detector, and propose deterministic and stochastic sensor-aware feature fusion strategies. The proposed fusion mechanisms are driven by the estimated sensor measurement reliability values/weights. Reliable object detection in harsh lighting conditions is essential for applications such as self-driving vehicles and human-robot interaction. We also propose a new ""r-blended"" hybrid depth modality for RGB-D sensors. Through extensive experimentation, we show that the proposed strategies outperform the existing state-of-the-art methods on the FLIR-Thermal dataset, and obtain promising results on the SUNRGB-D dataset. We additionally record a new RGB-Infra indoor dataset, namely L515-Indoors, and demonstrate that the proposed object detection methodologies are highly effective for a variety of lighting conditions.",0
"Vision tasks utilizing deep neural networks often encounter failures when environmental conditions not included in the training data are present. When a sensor fails to acquire information due to malfunction or design limitations, single-modal strategies are inadequate. Multi-sensor systems provide redundancy, improve reliability, and are essential in achieving robustness against asymmetric sensor failures. To address the challenges of changing lighting conditions and asymmetric sensor degradation in object detection, we have developed a multi-modal 2D object detector with deterministic and stochastic sensor-aware feature fusion strategies. These fusion mechanisms rely on the estimated sensor measurement reliability values/weights. Accurate object detection in harsh lighting conditions is essential for applications such as self-driving vehicles and human-robot interaction. We also propose a new ""r-blended"" hybrid depth modality for RGB-D sensors and demonstrate through extensive experimentation that our proposed methodologies outperform existing state-of-the-art methods on the FLIR-Thermal dataset and show promising results on the SUNRGB-D dataset. Additionally, we have recorded a new RGB-Infra indoor dataset, L515-Indoors, and have shown that our proposed object detection methodologies are highly effective for a variety of lighting conditions.",1
"Confidence-aware learning is proven as an effective solution to prevent networks becoming overconfident. We present a confidence-aware camouflaged object detection framework using dynamic supervision to produce both accurate camouflage map and meaningful ""confidence"" representing model awareness about the current prediction. A camouflaged object detection network is designed to produce our camouflage prediction. Then, we concatenate it with the input image and feed it to the confidence estimation network to produce an one channel confidence map.We generate dynamic supervision for the confidence estimation network, representing the agreement of camouflage prediction with the ground truth camouflage map. With the produced confidence map, we introduce confidence-aware learning with the confidence map as guidance to pay more attention to the hard/low-confidence pixels in the loss function. We claim that, once trained, our confidence estimation network can evaluate pixel-wise accuracy of the prediction without relying on the ground truth camouflage map. Extensive results on four camouflaged object detection testing datasets illustrate the superior performance of the proposed model in explaining the camouflage prediction.",0
"The effectiveness of confidence-aware learning is established in preventing networks from becoming overconfident. Our framework for camouflaged object detection employs dynamic supervision to generate an accurate camouflage map and meaningful ""confidence"" that reflects the model's awareness of the current prediction. Our approach involves a camouflaged object detection network that produces a camouflage prediction, which is concatenated with the input image and fed into a confidence estimation network to generate a one-channel confidence map. We introduce dynamic supervision to train the confidence estimation network using the agreement between the camouflage prediction and the ground truth camouflage map. Our approach utilizes the confidence map to prioritize hard/low-confidence pixels in the loss function, which enhances the network's ability to detect camouflaged objects. Furthermore, we demonstrate that our confidence estimation network can evaluate pixel-wise accuracy of the prediction without relying on the ground truth camouflage map. Our results on four camouflaged object detection testing datasets illustrate the superior performance of our model in explaining the camouflage prediction.",1
"With the recent developments in neural networks, there has been a resurgence in algorithms for the automatic generation of simulation ready electronic circuits from hand-drawn circuits. However, most of the approaches in literature were confined to classify different types of electrical components and only a few of those methods have shown a way to rebuild the circuit schematic from the scanned image, which is extremely important for further automation of netlist generation. This paper proposes a real-time algorithm for the automatic recognition of hand-drawn electrical circuits based on object detection and circuit node recognition. The proposed approach employs You Only Look Once version 5 (YOLOv5) for detection of circuit components and a novel Hough transform based approach for node recognition. Using YOLOv5 object detection algorithm, a mean average precision (mAP0.5) of 98.2% is achieved in detecting the components. The proposed method is also able to rebuild the circuit schematic with 80% accuracy.",0
"Recent advancements in neural networks have led to a renewed interest in algorithms that can automatically generate simulation-ready electronic circuits from hand-drawn designs. However, most existing methods in literature have focused solely on classifying different types of electrical components, with only a handful of approaches demonstrating the ability to rebuild the circuit schematic from a scanned image. This capability is crucial for achieving further automation of netlist generation. To address this gap, this paper presents a real-time algorithm that leverages object detection and circuit node recognition. The proposed method utilizes the You Only Look Once version 5 (YOLOv5) algorithm for component detection, achieving a mean average precision (mAP0.5) of 98.2%. For node recognition, a novel Hough transform-based approach is employed. Overall, the proposed method achieves an 80% accuracy rate in rebuilding the circuit schematic.",1
"Pseudo-LiDAR based 3D object detectors have gained popularity due to their high accuracy. However, these methods need dense depth supervision and suffer from inferior speed. To solve these two issues, a recently introduced RTS3D builds an efficient 4D Feature-Consistency Embedding (FCE) space for the intermediate representation of object without depth supervision. FCE space splits the entire object region into 3D uniform grid latent space for feature sampling point generation, which ignores the importance of different object regions. However, we argue that, compared with the inner region, the outer region plays a more important role for accurate 3D detection. To encode more information from the outer region, we propose a shape prior non-uniform sampling strategy that performs dense sampling in outer region and sparse sampling in inner region. As a result, more points are sampled from the outer region and more useful features are extracted for 3D detection. Further, to enhance the feature discrimination of each sampling point, we propose a high-level semantic enhanced FCE module to exploit more contextual information and suppress noise better. Experiments on the KITTI dataset are performed to show the effectiveness of the proposed method. Compared with the baseline RTS3D, our proposed method has 2.57% improvement on AP3d almost without extra network parameters. Moreover, our proposed method outperforms the state-of-the-art methods without extra supervision at a real-time speed.",0
"The accuracy of Pseudo-LiDAR based 3D object detectors has made them popular, but they require dense depth supervision and their speed is lacking. To address these issues, a new method called RTS3D has been introduced, which builds an efficient 4D Feature-Consistency Embedding (FCE) space that doesn't require depth supervision. However, the FCE space divides the object region into a uniform grid, which doesn't consider the importance of different regions. We argue that the outer region is more crucial for accurate 3D detection, so we propose a shape prior non-uniform sampling strategy that densely samples the outer region and sparsely samples the inner region. This approach extracts more useful features for 3D detection. We also suggest a high-level semantic enhanced FCE module to enhance feature discrimination and suppress noise. Our experiments on the KITTI dataset show that our proposed method outperforms the baseline RTS3D by 2.57% on AP3d without additional network parameters and achieves state-of-the-art results without extra supervision at real-time speed.",1
"Feature learning for 3D object detection from point clouds is very challenging due to the irregularity of 3D point cloud data. In this paper, we propose Pointformer, a Transformer backbone designed for 3D point clouds to learn features effectively. Specifically, a Local Transformer module is employed to model interactions among points in a local region, which learns context-dependent region features at an object level. A Global Transformer is designed to learn context-aware representations at the scene level. To further capture the dependencies among multi-scale representations, we propose Local-Global Transformer to integrate local features with global features from higher resolution. In addition, we introduce an efficient coordinate refinement module to shift down-sampled points closer to object centroids, which improves object proposal generation. We use Pointformer as the backbone for state-of-the-art object detection models and demonstrate significant improvements over original models on both indoor and outdoor datasets.",0
"The irregularity of 3D point cloud data poses a great challenge for feature learning in 3D object detection. In this study, we present Pointformer, a Transformer backbone that is specifically designed to effectively learn features from 3D point clouds. To achieve this, we employ a Local Transformer module that models interactions among points in a local region, enabling the learning of context-dependent region features at an object level. Additionally, we introduce a Global Transformer that learns context-aware representations at the scene level. To capture dependencies among multi-scale representations, we introduce the Local-Global Transformer, which integrates local features with global features from higher resolution. Furthermore, we propose an efficient coordinate refinement module that shifts down-sampled points closer to object centroids, resulting in improved object proposal generation. We demonstrate the effectiveness of Pointformer as the backbone for state-of-the-art object detection models, showing significant improvements over original models on indoor and outdoor datasets.",1
"Aiming at facilitating a real-world, ever-evolving and scalable autonomous driving system, we present a large-scale benchmark for standardizing the evaluation of different self-supervised and semi-supervised approaches by learning from raw data, which is the first and largest benchmark to date. Existing autonomous driving systems heavily rely on `perfect' visual perception models (e.g., detection) trained using extensive annotated data to ensure the safety. However, it is unrealistic to elaborately label instances of all scenarios and circumstances (e.g., night, extreme weather, cities) when deploying a robust autonomous driving system. Motivated by recent powerful advances of self-supervised and semi-supervised learning, a promising direction is to learn a robust detection model by collaboratively exploiting large-scale unlabeled data and few labeled data. Existing dataset (e.g., KITTI, Waymo) either provides only a small amount of data or covers limited domains with full annotation, hindering the exploration of large-scale pre-trained models. Here, we release a Large-Scale Object Detection benchmark for Autonomous driving, named as SODA10M, containing 10 million unlabeled images and 20K images labeled with 6 representative object categories. To improve diversity, the images are collected every ten seconds per frame within 32 different cities under different weather conditions, periods and location scenes. We provide extensive experiments and deep analyses of existing supervised state-of-the-art detection models, popular self-supervised and semi-supervised approaches, and some insights about how to develop future models. The data and more up-to-date information have been released at https://soda-2d.github.io.",0
"Our objective is to simplify the development of a practical, adaptable autonomous driving system. To this end, we have created the largest benchmark to date for evaluating various self-supervised and semi-supervised methods that learn from raw data. The current autonomous driving systems rely heavily on ""flawless"" visual perception models trained on annotated data to ensure safety. However, it is not feasible to label all possible scenarios and situations, such as extreme weather or city environments, for a robust autonomous driving system. With recent advances in self-supervised and semi-supervised learning, it is possible to create a robust detection model by leveraging large-scale unlabeled data and a small amount of labeled data. However, existing datasets like KITTI or Waymo have limited coverage or provide only a small amount of data, which limits the exploration of pre-trained models. Therefore, we introduce SODA10M, a Large-Scale Object Detection benchmark for Autonomous driving, which contains 10 million unlabeled images and 20K labeled images that cover 6 representative object categories. Our dataset is collected every ten seconds per frame across 32 different cities with various weather conditions, periods, and locations to increase diversity. We conduct extensive experiments and analysis on current supervised state-of-the-art detection models, self-supervised and semi-supervised methods, and provide insights for future model development. The data and up-to-date information are available at https://soda-2d.github.io.",1
"Moving Object Detection (MOD) is a crucial task for the Autonomous Driving pipeline. MOD is usually handled via 2-stream convolutional architectures that incorporates both appearance and motion cues, without considering the inter-relations between the spatial or motion features. In this paper, we tackle this problem through multi-head attention mechanisms, both across the spatial and motion streams. We propose MODETR; a Moving Object DEtection TRansformer network, comprised of multi-stream transformer encoders for both spatial and motion modalities, and an object transformer decoder that produces the moving objects bounding boxes using set predictions. The whole architecture is trained end-to-end using bi-partite loss. Several methods of incorporating motion cues with the Transformer model are explored, including two-stream RGB and Optical Flow (OF) methods, and multi-stream architectures that take advantage of sequence information. To incorporate the temporal information, we propose a new Temporal Positional Encoding (TPE) approach to extend the Spatial Positional Encoding(SPE) in DETR. We explore two architectural choices for that, balancing between speed and time. To evaluate the our network, we perform the MOD task on the KITTI MOD [6] data set. Results show significant 5% mAP of the Transformer network for MOD over the state-of-the art methods. Moreover, the proposed TPE encoding provides 10% mAP improvement over the SPE baseline.",0
"The Autonomous Driving pipeline relies heavily on Moving Object Detection (MOD), which is typically accomplished through 2-stream convolutional architectures using both appearance and motion cues. However, these methods fail to consider the relationships between spatial and motion features. In this paper, we introduce MODETR, a Moving Object Detection Transformer network, which addresses this issue using multi-head attention mechanisms across both spatial and motion streams. The network incorporates multi-stream transformer encoders for spatial and motion modalities and an object transformer decoder that produces moving object bounding boxes using set predictions. We train the entire architecture end-to-end using bi-partite loss and explore various methods of incorporating motion cues, including two-stream RGB and Optical Flow (OF) methods, and multi-stream architectures that use sequence information. To incorporate temporal information, we introduce a Temporal Positional Encoding (TPE) approach that extends the Spatial Positional Encoding (SPE) in DETR. We evaluate our network using the KITTI MOD [6] data set and observe a significant 5% mAP improvement over state-of-the-art methods. Additionally, the proposed TPE encoding provides a 10% mAP improvement over the SPE baseline.",1
